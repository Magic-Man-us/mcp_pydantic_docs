{"title": "Command Line Interface (CLI)", "anchor": "command-line-interface-cli", "heading_level": 1, "md_text": "**Pydantic AI** comes with a CLI, `clai` (pronounced \"clay\") which you can use to interact with various LLMs from the command line.\nIt provides a convenient way to chat with language models and quickly get answers right in the terminal.\n\nWe originally developed this CLI for our own use, but found ourselves using it so frequently that we decided to share it as part of the Pydantic AI package.\n\nWe plan to continue adding new features, such as interaction with MCP servers, access to tools, and more.", "url": "https://ai.pydantic.dev/docs/cli/#command-line-interface-cli", "page": "docs/cli", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 2, "md_text": "<!-- Keep this in sync with clai/README.md -->\n\nYou'll need to set an environment variable depending on the provider you intend to use.\n\nE.g. if you're using OpenAI, set the `OPENAI_API_KEY` environment variable:\n\n```bash\nexport OPENAI_API_KEY='your-api-key-here'\n```\n\nThen with [`uvx`](https://docs.astral.sh/uv/guides/tools/), run:\n\n```bash\nuvx clai\n```\n\nOr to install `clai` globally [with `uv`](https://docs.astral.sh/uv/guides/tools/#installing-tools), run:\n\n```bash\nuv tool install clai\n...\nclai\n```\n\nOr with `pip`, run:\n\n```bash\npip install clai\n...\nclai\n```\n\nEither way, running `clai` will start an interactive session where you can chat with the AI model. Special commands available in interactive mode:\n\n- `/exit`: Exit the session\n- `/markdown`: Show the last response in markdown format\n- `/multiline`: Toggle multiline input mode (use Ctrl+D to submit)\n- `/cp`: Copy the last response to clipboard", "url": "https://ai.pydantic.dev/docs/cli/#usage", "page": "docs/cli", "source_site": "pydantic_ai"}
{"title": "Help", "anchor": "help", "heading_level": 3, "md_text": "To get help on the CLI, use the `--help` flag:\n\n```bash\nuvx clai --help\n```", "url": "https://ai.pydantic.dev/docs/cli/#help", "page": "docs/cli", "source_site": "pydantic_ai"}
{"title": "Choose a model", "anchor": "choose-a-model", "heading_level": 3, "md_text": "You can specify which model to use with the `--model` flag:\n\n```bash\nuvx clai --model anthropic:claude-sonnet-4-0\n```\n\n(a full list of models available can be printed with `uvx clai --list-models`)", "url": "https://ai.pydantic.dev/docs/cli/#choose-a-model", "page": "docs/cli", "source_site": "pydantic_ai"}
{"title": "Custom Agents", "anchor": "custom-agents", "heading_level": 3, "md_text": "You can specify a custom agent using the `--agent` flag with a module path and variable name:\n\n```python {title=\"custom_agent.py\" test=\"skip\"}\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-5', instructions='You always respond in Italian.')\n```\n\nThen run:\n\n```bash\nuvx clai --agent custom_agent:agent \"What's the weather today?\"\n```\n\nThe format must be `module:variable` where:\n\n- `module` is the importable Python module path\n- `variable` is the name of the Agent instance in that module\n\nAdditionally, you can directly launch CLI mode from an `Agent` instance using `Agent.to_cli_sync()`:\n\n```python {title=\"agent_to_cli_sync.py\" test=\"skip\" hl_lines=4}\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-5', instructions='You always respond in Italian.')\nagent.to_cli_sync()\n```\n\nYou can also use the async interface with `Agent.to_cli()`:\n\n```python {title=\"agent_to_cli.py\" test=\"skip\" hl_lines=6}\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-5', instructions='You always respond in Italian.')\n\nasync def main():\n    await agent.to_cli()\n```\n\n_(You'll need to add `asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/cli/#custom-agents", "page": "docs/cli", "source_site": "pydantic_ai"}
{"title": "Message History", "anchor": "message-history", "heading_level": 3, "md_text": "Both `Agent.to_cli()` and `Agent.to_cli_sync()` support a `message_history` parameter, allowing you to continue an existing conversation or provide conversation context:\n\n```python {title=\"agent_with_history.py\" test=\"skip\"}\nfrom pydantic_ai import (\n    Agent,\n    ModelMessage,\n    ModelRequest,\n    ModelResponse,\n    TextPart,\n    UserPromptPart,\n)\n\nagent = Agent('openai:gpt-5')", "url": "https://ai.pydantic.dev/docs/cli/#message-history", "page": "docs/cli", "source_site": "pydantic_ai"}
{"title": "Create some conversation history", "anchor": "create-some-conversation-history", "heading_level": 1, "md_text": "message_history: list[ModelMessage] = [\n    ModelRequest([UserPromptPart(content='What is 2+2?')]),\n    ModelResponse([TextPart(content='2+2 equals 4.')])\n]", "url": "https://ai.pydantic.dev/docs/cli/#create-some-conversation-history", "page": "docs/cli", "source_site": "pydantic_ai"}
{"title": "Start CLI with existing conversation context", "anchor": "start-cli-with-existing-conversation-context", "heading_level": 1, "md_text": "agent.to_cli_sync(message_history=message_history)\n```\n\nThe CLI will start with the provided conversation history, allowing the agent to refer back to previous exchanges and maintain context throughout the session.", "url": "https://ai.pydantic.dev/docs/cli/#start-cli-with-existing-conversation-context", "page": "docs/cli", "source_site": "pydantic_ai"}
{"title": "Upgrade Guide", "anchor": "upgrade-guide", "heading_level": 1, "md_text": "In September 2025, Pydantic AI reached V1, which means we're committed to API stability: we will not introduce changes that break your code until V2. For more information, review our [Version Policy](version-policy.md).", "url": "https://ai.pydantic.dev/docs/changelog/#upgrade-guide", "page": "docs/changelog", "source_site": "pydantic_ai"}
{"title": "Breaking Changes", "anchor": "breaking-changes", "heading_level": 2, "md_text": "Here's a filtered list of the breaking changes for each version to help you upgrade Pydantic AI.", "url": "https://ai.pydantic.dev/docs/changelog/#breaking-changes", "page": "docs/changelog", "source_site": "pydantic_ai"}
{"title": "v1.0.1 (2025-09-05)", "anchor": "v101-2025-09-05", "heading_level": 3, "md_text": "The following breaking change was accidentally left out of v1.0.0:\n\n- See [#2808](https://github.com/pydantic/pydantic-ai/pull/2808) - Remove `Python` evaluator from `pydantic_evals` for security reasons", "url": "https://ai.pydantic.dev/docs/changelog/#v101-2025-09-05", "page": "docs/changelog", "source_site": "pydantic_ai"}
{"title": "v1.0.0 (2025-09-04)", "anchor": "v100-2025-09-04", "heading_level": 3, "md_text": "- See [#2725](https://github.com/pydantic/pydantic-ai/pull/2725) - Drop support for Python 3.9\n- See [#2738](https://github.com/pydantic/pydantic-ai/pull/2738) - Make many dataclasses require keyword arguments\n- See [#2715](https://github.com/pydantic/pydantic-ai/pull/2715) - Remove `cases` and `averages` attributes from `pydantic_evals` spans\n- See [#2798](https://github.com/pydantic/pydantic-ai/pull/2798) - Change `ModelRequest.parts` and `ModelResponse.parts` types from `list` to `Sequence`\n- See [#2726](https://github.com/pydantic/pydantic-ai/pull/2726) - Default `InstrumentationSettings` version to 2\n- See [#2717](https://github.com/pydantic/pydantic-ai/pull/2717) - Remove errors when passing `AsyncRetrying` or `Retrying` object to `AsyncTenacityTransport` or `TenacityTransport` instead of `RetryConfig`", "url": "https://ai.pydantic.dev/docs/changelog/#v100-2025-09-04", "page": "docs/changelog", "source_site": "pydantic_ai"}
{"title": "v0.x.x", "anchor": "v0xx", "heading_level": 3, "md_text": "Before V1, minor versions were used to introduce breaking changes:\n\n**v0.8.0 (2025-08-26)**\n\nSee [#2689](https://github.com/pydantic/pydantic-ai/pull/2689) - `AgentStreamEvent` was expanded to be a union of `ModelResponseStreamEvent` and `HandleResponseEvent`, simplifying the `event_stream_handler` function signature. Existing code accepting `AgentStreamEvent | HandleResponseEvent` will continue to work.\n\n**v0.7.6 (2025-08-26)**\n\nThe following breaking change was inadvertently released in a patch version rather than a minor version:\n\nSee [#2670](https://github.com/pydantic/pydantic-ai/pull/2670) - `TenacityTransport` and `AsyncTenacityTransport` now require the use of `pydantic_ai.retries.RetryConfig` (which is just a `TypedDict` containing the kwargs to `tenacity.retry`) instead of `tenacity.Retrying` or `tenacity.AsyncRetrying`.\n\n**v0.7.0 (2025-08-12)**\n\nSee [#2458](https://github.com/pydantic/pydantic-ai/pull/2458) - `pydantic_ai.models.StreamedResponse` now yields a `FinalResultEvent` along with the existing `PartStartEvent` and `PartDeltaEvent`. If you're using `pydantic_ai.direct.model_request_stream` or `pydantic_ai.direct.model_request_stream_sync`, you may need to update your code to account for this.\n\nSee [#2458](https://github.com/pydantic/pydantic-ai/pull/2458) - `pydantic_ai.models.Model.request_stream` now receives a `run_context` argument. If you've implemented a custom `Model` subclass, you will need to account for this.\n\nSee [#2458](https://github.com/pydantic/pydantic-ai/pull/2458) - `pydantic_ai.models.StreamedResponse` now requires a `model_request_parameters` field and constructor argument. If you've implemented a custom `Model` subclass and implemented `request_stream`, you will need to account for this.\n\n**v0.6.0 (2025-08-06)**\n\nThis release was meant to clean some old deprecated code, so we can get a step closer to V1.\n\nSee [#2440](https://github.com/pydantic/pydantic-ai/pull/2440) - The `next` method was removed from the `Graph` class. Use `async with graph.iter(...) as run:  run.next()` instead.\n\nSee [#2441](https://github.com/pydantic/pydantic-ai/pull/2441) - The `result_type`, `result_tool_name` and `result_tool_description` arguments were removed from the `Agent` class. Use `output_type` instead.\n\nSee [#2441](https://github.com/pydantic/pydantic-ai/pull/2441) - The `result_retries` argument was also removed from the `Agent` class. Use `output_retries` instead.\n\nSee [#2443](https://github.com/pydantic/pydantic-ai/pull/2443) - The `data` property was removed from the `FinalResult` class. Use `output` instead.\n\nSee [#2445](https://github.com/pydantic/pydantic-ai/pull/2445) - The `get_data` and `validate_structured_result` methods were removed from the\n`StreamedRunResult` class. Use `get_output` and `validate_structured_output` instead.\n\nSee [#2446](https://github.com/pydantic/pydantic-ai/pull/2446) - The `format_as_xml` function was moved to the `pydantic_ai.format_as_xml` module.\nImport it via `from pydantic_ai import format_as_xml` instead.\n\nSee [#2451](https://github.com/pydantic/pydantic-ai/pull/2451) - Removed deprecated `Agent.result_validator` method, `Agent.last_run_messages` property, `AgentRunResult.data` property, and `result_tool_return_content` parameters from result classes.\n\n**v0.5.0 (2025-08-04)**\n\nSee [#2388](https://github.com/pydantic/pydantic-ai/pull/2388) - The `source` field of an `EvaluationResult` is now of type `EvaluatorSpec` rather than the actual source `Evaluator` instance, to help with serialization/deserialization.\n\nSee [#2163](https://github.com/pydantic/pydantic-ai/pull/2163) - The `EvaluationReport.print` and `EvaluationReport.console_table` methods now require most arguments be passed by keyword.\n\n**v0.4.0 (2025-07-08)**\n\nSee [#1799](https://github.com/pydantic/pydantic-ai/pull/1799) - Pydantic Evals `EvaluationReport` and `ReportCase` are now generic dataclasses instead of Pydantic models. If you were serializing them using `model_dump()`, you will now need to use the `EvaluationReportAdapter` and `ReportCaseAdapter` type adapters instead.\n\nSee [#1507](https://github.com/pydantic/pydantic-ai/pull/1507) - The `ToolDefinition` `description` argument is now optional and the order of positional arguments has changed from `name, description, parameters_json_schema, ...` to `name, parameters_json_schema, description, ...` to account for this.", "url": "https://ai.pydantic.dev/docs/changelog/#v0xx", "page": "docs/changelog", "source_site": "pydantic_ai"}
{"title": "v0.x.x", "anchor": "v0xx", "heading_level": 3, "md_text": "**v0.3.0 (2025-06-18)**\n\nSee [#1142](https://github.com/pydantic/pydantic-ai/pull/1142) \u2014 Adds support for thinking parts.\n\nWe now convert the thinking blocks (`\"<think>...\"</think>\"`) in provider specific text parts to\nPydantic AI `ThinkingPart`s. Also, as part of this release, we made the choice to not send back the\n`ThinkingPart`s to the provider - the idea is to save costs on behalf of the user. In the future, we\nintend to add a setting to customize this behavior.\n\n**v0.2.0 (2025-05-12)**\n\nSee [#1647](https://github.com/pydantic/pydantic-ai/pull/1647) \u2014 usage makes sense as part of `ModelResponse`, and could be really useful in \"messages\" (really a sequence of requests and response). In this PR:\n\n- Adds `usage` to `ModelResponse` (field has a default factory of `Usage()` so it'll work to load data that doesn't have usage)\n- changes the return type of `Model.request` to just `ModelResponse` instead of `tuple[ModelResponse, Usage]`\n\n**v0.1.0 (2025-04-15)**\n\nSee [#1248](https://github.com/pydantic/pydantic-ai/pull/1248) \u2014 the attribute/parameter name `result` was renamed to `output` in many places. Hopefully all changes keep a deprecated attribute or parameter with the old name, so you should get many deprecation warnings.\n\nSee [#1484](https://github.com/pydantic/pydantic-ai/pull/1484) \u2014 `format_as_xml` was moved and made available to import from the package root, e.g. `from pydantic_ai import format_as_xml`.", "url": "https://ai.pydantic.dev/docs/changelog/#v0xx", "page": "docs/changelog", "source_site": "pydantic_ai"}
{"title": "Full Changelog", "anchor": "full-changelog", "heading_level": 2, "md_text": "<div id=\"display-changelog\">\n  For the full changelog, see <a href=\"https://github.com/pydantic/pydantic-ai/releases\">GitHub Releases</a>.\n</div>\n\n<script>\n  fetch('/changelog.html').then(r => {\n    if (r.ok) {\n      r.text().then(t => {\n        document.getElementById('display-changelog').innerHTML = t;\n      });\n    }\n  });\n</script>", "url": "https://ai.pydantic.dev/docs/changelog/#full-changelog", "page": "docs/changelog", "source_site": "pydantic_ai"}
{"title": "Image, Audio, Video & Document Input", "anchor": "image-audio-video-document-input", "heading_level": 1, "md_text": "Some LLMs are now capable of understanding audio, video, image and document content.", "url": "https://ai.pydantic.dev/docs/input/#image-audio-video-document-input", "page": "docs/input", "source_site": "pydantic_ai"}
{"title": "Image Input", "anchor": "image-input", "heading_level": 2, "md_text": "!!! info\n    Some models do not support image input. Please check the model's documentation to confirm whether it supports image input.\n\nIf you have a direct URL for the image, you can use [`ImageUrl`][pydantic_ai.ImageUrl]:\n\n```py {title=\"image_input.py\" test=\"skip\" lint=\"skip\"}\nfrom pydantic_ai import Agent, ImageUrl\n\nagent = Agent(model='openai:gpt-5')\nresult = agent.run_sync(\n    [\n        'What company is this logo from?',\n        ImageUrl(url='https://iili.io/3Hs4FMg.png'),\n    ]\n)\nprint(result.output)\n#> This is the logo for Pydantic, a data validation and settings management library in Python.\n```\n\nIf you have the image locally, you can also use [`BinaryContent`][pydantic_ai.BinaryContent]:\n\n```py {title=\"local_image_input.py\" test=\"skip\" lint=\"skip\"}\nimport httpx\n\nfrom pydantic_ai import Agent, BinaryContent\n\nimage_response = httpx.get('https://iili.io/3Hs4FMg.png')  # Pydantic logo\n\nagent = Agent(model='openai:gpt-5')\nresult = agent.run_sync(\n    [\n        'What company is this logo from?',\n        BinaryContent(data=image_response.content, media_type='image/png'),  # (1)!\n    ]\n)\nprint(result.output)\n#> This is the logo for Pydantic, a data validation and settings management library in Python.\n```\n\n1. To ensure the example is runnable we download this image from the web, but you can also use `Path().read_bytes()` to read a local file's contents.", "url": "https://ai.pydantic.dev/docs/input/#image-input", "page": "docs/input", "source_site": "pydantic_ai"}
{"title": "Audio Input", "anchor": "audio-input", "heading_level": 2, "md_text": "!!! info\n    Some models do not support audio input. Please check the model's documentation to confirm whether it supports audio input.\n\nYou can provide audio input using either [`AudioUrl`][pydantic_ai.AudioUrl] or [`BinaryContent`][pydantic_ai.BinaryContent]. The process is analogous to the examples above.", "url": "https://ai.pydantic.dev/docs/input/#audio-input", "page": "docs/input", "source_site": "pydantic_ai"}
{"title": "Video Input", "anchor": "video-input", "heading_level": 2, "md_text": "!!! info\n    Some models do not support video input. Please check the model's documentation to confirm whether it supports video input.\n\nYou can provide video input using either [`VideoUrl`][pydantic_ai.VideoUrl] or [`BinaryContent`][pydantic_ai.BinaryContent]. The process is analogous to the examples above.", "url": "https://ai.pydantic.dev/docs/input/#video-input", "page": "docs/input", "source_site": "pydantic_ai"}
{"title": "Document Input", "anchor": "document-input", "heading_level": 2, "md_text": "!!! info\n    Some models do not support document input. Please check the model's documentation to confirm whether it supports document input.\n\nYou can provide document input using either [`DocumentUrl`][pydantic_ai.DocumentUrl] or [`BinaryContent`][pydantic_ai.BinaryContent]. The process is similar to the examples above.\n\nIf you have a direct URL for the document, you can use [`DocumentUrl`][pydantic_ai.DocumentUrl]:\n\n```py {title=\"document_input.py\" test=\"skip\" lint=\"skip\"}\nfrom pydantic_ai import Agent, DocumentUrl\n\nagent = Agent(model='anthropic:claude-sonnet-4-5')\nresult = agent.run_sync(\n    [\n        'What is the main content of this document?',\n        DocumentUrl(url='https://storage.googleapis.com/cloud-samples-data/generative-ai/pdf/2403.05530.pdf'),\n    ]\n)\nprint(result.output)\n#> This document is the technical report introducing Gemini 1.5, Google's latest large language model...\n```\n\nThe supported document formats vary by model.\n\nYou can also use [`BinaryContent`][pydantic_ai.BinaryContent] to pass document data directly:\n\n```py {title=\"binary_content_input.py\" test=\"skip\" lint=\"skip\"}\nfrom pathlib import Path\nfrom pydantic_ai import Agent, BinaryContent\n\npdf_path = Path('document.pdf')\nagent = Agent(model='anthropic:claude-sonnet-4-5')\nresult = agent.run_sync(\n    [\n        'What is the main content of this document?',\n        BinaryContent(data=pdf_path.read_bytes(), media_type='application/pdf'),\n    ]\n)\nprint(result.output)\n#> The document discusses...\n```", "url": "https://ai.pydantic.dev/docs/input/#document-input", "page": "docs/input", "source_site": "pydantic_ai"}
{"title": "User-side download vs. direct file URL", "anchor": "user-side-download-vs-direct-file-url", "heading_level": 2, "md_text": "As a general rule, when you provide a URL using any of `ImageUrl`, `AudioUrl`, `VideoUrl` or `DocumentUrl`, Pydantic AI downloads the file content and then sends it as part of the API request.\n\nThe situation is different for certain models:\n\n- [`AnthropicModel`][pydantic_ai.models.anthropic.AnthropicModel]: if you provide a PDF document via `DocumentUrl`, the URL is sent directly in the API request, so no download happens on the user side.\n\n- [`GoogleModel`][pydantic_ai.models.google.GoogleModel] on Vertex AI: any URL provided using `ImageUrl`, `AudioUrl`, `VideoUrl`, or `DocumentUrl` is sent as-is in the API request and no data is downloaded beforehand.\n\n  See the [Gemini API docs for Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#filedata) to learn more about supported URLs, formats and limitations:\n\n  - Cloud Storage bucket URIs (with protocol `gs://`)\n  - Public HTTP(S) URLs\n  - Public YouTube video URL (maximum one URL per request)\n\n  However, because of crawling restrictions, it may happen that Gemini can't access certain URLs. In that case, you can instruct Pydantic AI to download the file content and send that instead of the URL by setting the boolean flag `force_download` to `True`. This attribute is available on all objects that inherit from [`FileUrl`][pydantic_ai.messages.FileUrl].\n\n- [`GoogleModel`][pydantic_ai.models.google.GoogleModel] on GLA: YouTube video URLs are sent directly in the request to the model.", "url": "https://ai.pydantic.dev/docs/input/#user-side-download-vs-direct-file-url", "page": "docs/input", "source_site": "pydantic_ai"}
{"title": "Dependencies", "anchor": "dependencies", "heading_level": 1, "md_text": "Pydantic AI uses a dependency injection system to provide data and services to your agent's [system prompts](agents.md#system-prompts), [tools](tools.md) and [output validators](output.md#output-validator-functions).\n\nMatching Pydantic AI's design philosophy, our dependency system tries to use existing best practice in Python development rather than inventing esoteric \"magic\", this should make dependencies type-safe, understandable easier to test and ultimately easier to deploy in production.", "url": "https://ai.pydantic.dev/docs/dependencies/#dependencies", "page": "docs/dependencies", "source_site": "pydantic_ai"}
{"title": "Defining Dependencies", "anchor": "defining-dependencies", "heading_level": 2, "md_text": "Dependencies can be any python type. While in simple cases you might be able to pass a single object as a dependency (e.g. an HTTP connection), [dataclasses][] are generally a convenient container when your dependencies included multiple objects.\n\nHere's an example of defining an agent that requires dependencies.\n\n(**Note:** dependencies aren't actually used in this example, see [Accessing Dependencies](#accessing-dependencies) below)\n\n```python {title=\"unused_dependencies.py\"}\nfrom dataclasses import dataclass\n\nimport httpx\n\nfrom pydantic_ai import Agent\n\n\n@dataclass\nclass MyDeps:  # (1)!\n    api_key: str\n    http_client: httpx.AsyncClient\n\n\nagent = Agent(\n    'openai:gpt-5',\n    deps_type=MyDeps,  # (2)!\n)\n\n\nasync def main():\n    async with httpx.AsyncClient() as client:\n        deps = MyDeps('foobar', client)\n        result = await agent.run(\n            'Tell me a joke.',\n            deps=deps,  # (3)!\n        )\n        print(result.output)\n        #> Did you hear about the toothpaste scandal? They called it Colgate.\n```\n\n1. Define a dataclass to hold dependencies.\n2. Pass the dataclass type to the `deps_type` argument of the [`Agent` constructor][pydantic_ai.Agent.__init__]. **Note**: we're passing the type here, NOT an instance, this parameter is not actually used at runtime, it's here so we can get full type checking of the agent.\n3. When running the agent, pass an instance of the dataclass to the `deps` parameter.\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/dependencies/#defining-dependencies", "page": "docs/dependencies", "source_site": "pydantic_ai"}
{"title": "Accessing Dependencies", "anchor": "accessing-dependencies", "heading_level": 2, "md_text": "Dependencies are accessed through the [`RunContext`][pydantic_ai.tools.RunContext] type, this should be the first parameter of system prompt functions etc.\n\n```python {title=\"system_prompt_dependencies.py\" hl_lines=\"20-27\"}\nfrom dataclasses import dataclass\n\nimport httpx\n\nfrom pydantic_ai import Agent, RunContext\n\n\n@dataclass\nclass MyDeps:\n    api_key: str\n    http_client: httpx.AsyncClient\n\n\nagent = Agent(\n    'openai:gpt-5',\n    deps_type=MyDeps,\n)\n\n\n@agent.system_prompt  # (1)!\nasync def get_system_prompt(ctx: RunContext[MyDeps]) -> str:  # (2)!\n    response = await ctx.deps.http_client.get(  # (3)!\n        'https://example.com',\n        headers={'Authorization': f'Bearer {ctx.deps.api_key}'},  # (4)!\n    )\n    response.raise_for_status()\n    return f'Prompt: {response.text}'\n\n\nasync def main():\n    async with httpx.AsyncClient() as client:\n        deps = MyDeps('foobar', client)\n        result = await agent.run('Tell me a joke.', deps=deps)\n        print(result.output)\n        #> Did you hear about the toothpaste scandal? They called it Colgate.\n```\n\n1. [`RunContext`][pydantic_ai.tools.RunContext] may optionally be passed to a [`system_prompt`][pydantic_ai.Agent.system_prompt] function as the only argument.\n2. [`RunContext`][pydantic_ai.tools.RunContext] is parameterized with the type of the dependencies, if this type is incorrect, static type checkers will raise an error.\n3. Access dependencies through the [`.deps`][pydantic_ai.tools.RunContext.deps] attribute.\n4. Access dependencies through the [`.deps`][pydantic_ai.tools.RunContext.deps] attribute.\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/dependencies/#accessing-dependencies", "page": "docs/dependencies", "source_site": "pydantic_ai"}
{"title": "Asynchronous vs. Synchronous dependencies", "anchor": "asynchronous-vs-synchronous-dependencies", "heading_level": 3, "md_text": "[System prompt functions](agents.md#system-prompts), [function tools](tools.md) and [output validators](output.md#output-validator-functions) are all run in the async context of an agent run.\n\nIf these functions are not coroutines (e.g. `async def`) they are called with\n[`run_in_executor`][asyncio.loop.run_in_executor] in a thread pool, it's therefore marginally preferable\nto use `async` methods where dependencies perform IO, although synchronous dependencies should work fine too.\n\n!!! note \"`run` vs. `run_sync` and Asynchronous vs. Synchronous dependencies\"\n    Whether you use synchronous or asynchronous dependencies, is completely independent of whether you use `run` or `run_sync` \u2014 `run_sync` is just a wrapper around `run` and agents are always run in an async context.\n\nHere's the same example as above, but with a synchronous dependency:\n\n```python {title=\"sync_dependencies.py\"}\nfrom dataclasses import dataclass\n\nimport httpx\n\nfrom pydantic_ai import Agent, RunContext\n\n\n@dataclass\nclass MyDeps:\n    api_key: str\n    http_client: httpx.Client  # (1)!\n\n\nagent = Agent(\n    'openai:gpt-5',\n    deps_type=MyDeps,\n)\n\n\n@agent.system_prompt\ndef get_system_prompt(ctx: RunContext[MyDeps]) -> str:  # (2)!\n    response = ctx.deps.http_client.get(\n        'https://example.com', headers={'Authorization': f'Bearer {ctx.deps.api_key}'}\n    )\n    response.raise_for_status()\n    return f'Prompt: {response.text}'\n\n\nasync def main():\n    deps = MyDeps('foobar', httpx.Client())\n    result = await agent.run(\n        'Tell me a joke.',\n        deps=deps,\n    )\n    print(result.output)\n    #> Did you hear about the toothpaste scandal? They called it Colgate.\n```\n\n1. Here we use a synchronous `httpx.Client` instead of an asynchronous `httpx.AsyncClient`.\n2. To match the synchronous dependency, the system prompt function is now a plain function, not a coroutine.\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/dependencies/#asynchronous-vs-synchronous-dependencies", "page": "docs/dependencies", "source_site": "pydantic_ai"}
{"title": "Full Example", "anchor": "full-example", "heading_level": 2, "md_text": "As well as system prompts, dependencies can be used in [tools](tools.md) and [output validators](output.md#output-validator-functions).\n\n```python {title=\"full_example.py\" hl_lines=\"27-35 38-48\"}\nfrom dataclasses import dataclass\n\nimport httpx\n\nfrom pydantic_ai import Agent, ModelRetry, RunContext\n\n\n@dataclass\nclass MyDeps:\n    api_key: str\n    http_client: httpx.AsyncClient\n\n\nagent = Agent(\n    'openai:gpt-5',\n    deps_type=MyDeps,\n)\n\n\n@agent.system_prompt\nasync def get_system_prompt(ctx: RunContext[MyDeps]) -> str:\n    response = await ctx.deps.http_client.get('https://example.com')\n    response.raise_for_status()\n    return f'Prompt: {response.text}'\n\n\n@agent.tool  # (1)!\nasync def get_joke_material(ctx: RunContext[MyDeps], subject: str) -> str:\n    response = await ctx.deps.http_client.get(\n        'https://example.com#jokes',\n        params={'subject': subject},\n        headers={'Authorization': f'Bearer {ctx.deps.api_key}'},\n    )\n    response.raise_for_status()\n    return response.text\n\n\n@agent.output_validator  # (2)!\nasync def validate_output(ctx: RunContext[MyDeps], output: str) -> str:\n    response = await ctx.deps.http_client.post(\n        'https://example.com#validate',\n        headers={'Authorization': f'Bearer {ctx.deps.api_key}'},\n        params={'query': output},\n    )\n    if response.status_code == 400:\n        raise ModelRetry(f'invalid response: {response.text}')\n    response.raise_for_status()\n    return output\n\n\nasync def main():\n    async with httpx.AsyncClient() as client:\n        deps = MyDeps('foobar', client)\n        result = await agent.run('Tell me a joke.', deps=deps)\n        print(result.output)\n        #> Did you hear about the toothpaste scandal? They called it Colgate.\n```\n\n1. To pass `RunContext` to a tool, use the [`tool`][pydantic_ai.Agent.tool] decorator.\n2. `RunContext` may optionally be passed to a [`output_validator`][pydantic_ai.Agent.output_validator] function as the first argument.\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/dependencies/#full-example", "page": "docs/dependencies", "source_site": "pydantic_ai"}
{"title": "Overriding Dependencies", "anchor": "overriding-dependencies", "heading_level": 2, "md_text": "When testing agents, it's useful to be able to customise dependencies.\n\nWhile this can sometimes be done by calling the agent directly within unit tests, we can also override dependencies\nwhile calling application code which in turn calls the agent.\n\nThis is done via the [`override`][pydantic_ai.Agent.override] method on the agent.\n\n```python {title=\"joke_app.py\"}\nfrom dataclasses import dataclass\n\nimport httpx\n\nfrom pydantic_ai import Agent, RunContext\n\n\n@dataclass\nclass MyDeps:\n    api_key: str\n    http_client: httpx.AsyncClient\n\n    async def system_prompt_factory(self) -> str:  # (1)!\n        response = await self.http_client.get('https://example.com')\n        response.raise_for_status()\n        return f'Prompt: {response.text}'\n\n\njoke_agent = Agent('openai:gpt-5', deps_type=MyDeps)\n\n\n@joke_agent.system_prompt\nasync def get_system_prompt(ctx: RunContext[MyDeps]) -> str:\n    return await ctx.deps.system_prompt_factory()  # (2)!\n\n\nasync def application_code(prompt: str) -> str:  # (3)!\n    ...\n    ...\n    # now deep within application code we call our agent\n    async with httpx.AsyncClient() as client:\n        app_deps = MyDeps('foobar', client)\n        result = await joke_agent.run(prompt, deps=app_deps)  # (4)!\n    return result.output\n```\n\n1. Define a method on the dependency to make the system prompt easier to customise.\n2. Call the system prompt factory from within the system prompt function.\n3. Application code that calls the agent, in a real application this might be an API endpoint.\n4. Call the agent from within the application code, in a real application this call might be deep within a call stack. Note `app_deps` here will NOT be used when deps are overridden.\n\n_(This example is complete, it can be run \"as is\")_\n\n```python {title=\"test_joke_app.py\" hl_lines=\"10-12\" call_name=\"test_application_code\" requires=\"joke_app.py\"}\nfrom joke_app import MyDeps, application_code, joke_agent\n\n\nclass TestMyDeps(MyDeps):  # (1)!\n    async def system_prompt_factory(self) -> str:\n        return 'test prompt'\n\n\nasync def test_application_code():\n    test_deps = TestMyDeps('test_key', None)  # (2)!\n    with joke_agent.override(deps=test_deps):  # (3)!\n        joke = await application_code('Tell me a joke.')  # (4)!\n    assert joke.startswith('Did you hear about the toothpaste scandal?')\n```\n\n1. Define a subclass of `MyDeps` in tests to customise the system prompt factory.\n2. Create an instance of the test dependency, we don't need to pass an `http_client` here as it's not used.\n3. Override the dependencies of the agent for the duration of the `with` block, `test_deps` will be used when the agent is run.\n4. Now we can safely call our application code, the agent will use the overridden dependencies.", "url": "https://ai.pydantic.dev/docs/dependencies/#overriding-dependencies", "page": "docs/dependencies", "source_site": "pydantic_ai"}
{"title": "Examples", "anchor": "examples", "heading_level": 2, "md_text": "The following examples demonstrate how to use dependencies in Pydantic AI:\n\n- [Weather Agent](examples/weather-agent.md)\n- [SQL Generation](examples/sql-gen.md)\n- [RAG](examples/rag.md)", "url": "https://ai.pydantic.dev/docs/dependencies/#examples", "page": "docs/dependencies", "source_site": "pydantic_ai"}
{"title": "Multi-agent Applications", "anchor": "multi-agent-applications", "heading_level": 1, "md_text": "There are roughly four levels of complexity when building applications with Pydantic AI:\n\n1. Single agent workflows \u2014 what most of the `pydantic_ai` documentation covers\n2. [Agent delegation](#agent-delegation) \u2014 agents using another agent via tools\n3. [Programmatic agent hand-off](#programmatic-agent-hand-off) \u2014 one agent runs, then application code calls another agent\n4. [Graph based control flow](graph.md) \u2014 for the most complex cases, a graph-based state machine can be used to control the execution of multiple agents\n\nOf course, you can combine multiple strategies in a single application.", "url": "https://ai.pydantic.dev/docs/multi-agent-applications/#multi-agent-applications", "page": "docs/multi-agent-applications", "source_site": "pydantic_ai"}
{"title": "Agent delegation", "anchor": "agent-delegation", "heading_level": 2, "md_text": "\"Agent delegation\" refers to the scenario where an agent delegates work to another agent, then takes back control when the delegate agent (the agent called from within a tool) finishes.\nIf you want to hand off control to another agent completely, without coming back to the first agent, you can use an [output function](output.md#output-functions).\n\nSince agents are stateless and designed to be global, you do not need to include the agent itself in agent [dependencies](dependencies.md).\n\nYou'll generally want to pass [`ctx.usage`][pydantic_ai.RunContext.usage] to the [`usage`][pydantic_ai.agent.AbstractAgent.run] keyword argument of the delegate agent run so usage within that run counts towards the total usage of the parent agent run.\n\n!!! note \"Multiple models\"\n    Agent delegation doesn't need to use the same model for each agent. If you choose to use different models within a run, calculating the monetary cost from the final [`result.usage()`][pydantic_ai.agent.AgentRunResult.usage] of the run will not be possible, but you can still use [`UsageLimits`][pydantic_ai.usage.UsageLimits] \u2014 including `request_limit`, `total_tokens_limit`, and `tool_calls_limit` \u2014 to avoid unexpected costs or runaway tool loops.\n\n```python {title=\"agent_delegation_simple.py\"}\nfrom pydantic_ai import Agent, RunContext, UsageLimits\n\njoke_selection_agent = Agent(  # (1)!\n    'openai:gpt-5',\n    system_prompt=(\n        'Use the `joke_factory` to generate some jokes, then choose the best. '\n        'You must return just a single joke.'\n    ),\n)\njoke_generation_agent = Agent(  # (2)!\n    'google-gla:gemini-2.5-flash', output_type=list[str]\n)\n\n\n@joke_selection_agent.tool\nasync def joke_factory(ctx: RunContext[None], count: int) -> list[str]:\n    r = await joke_generation_agent.run(  # (3)!\n        f'Please generate {count} jokes.',\n        usage=ctx.usage,  # (4)!\n    )\n    return r.output  # (5)!\n\n\nresult = joke_selection_agent.run_sync(\n    'Tell me a joke.',\n    usage_limits=UsageLimits(request_limit=5, total_tokens_limit=500),\n)\nprint(result.output)\n#> Did you hear about the toothpaste scandal? They called it Colgate.\nprint(result.usage())\n#> RunUsage(input_tokens=204, output_tokens=24, requests=3, tool_calls=1)\n```\n\n1. The \"parent\" or controlling agent.\n2. The \"delegate\" agent, which is called from within a tool of the parent agent.\n3. Call the delegate agent from within a tool of the parent agent.\n4. Pass the usage from the parent agent to the delegate agent so the final [`result.usage()`][pydantic_ai.agent.AgentRunResult.usage] includes the usage from both agents.\n5. Since the function returns `#!python list[str]`, and the `output_type` of `joke_generation_agent` is also `#!python list[str]`, we can simply return `#!python r.output` from the tool.\n\n_(This example is complete, it can be run \"as is\")_\n\nThe control flow for this example is pretty simple and can be summarised as follows:\n\n```mermaid\ngraph TD\n  START --> joke_selection_agent\n  joke_selection_agent --> joke_factory[\"joke_factory (tool)\"]\n  joke_factory --> joke_generation_agent\n  joke_generation_agent --> joke_factory\n  joke_factory --> joke_selection_agent\n  joke_selection_agent --> END\n```", "url": "https://ai.pydantic.dev/docs/multi-agent-applications/#agent-delegation", "page": "docs/multi-agent-applications", "source_site": "pydantic_ai"}
{"title": "Agent delegation and dependencies", "anchor": "agent-delegation-and-dependencies", "heading_level": 3, "md_text": "Generally the delegate agent needs to either have the same [dependencies](dependencies.md) as the calling agent, or dependencies which are a subset of the calling agent's dependencies.\n\n!!! info \"Initializing dependencies\"\n    We say \"generally\" above since there's nothing to stop you initializing dependencies within a tool call and therefore using interdependencies in a delegate agent that are not available on the parent, this should often be avoided since it can be significantly slower than reusing connections etc. from the parent agent.\n\n```python {title=\"agent_delegation_deps.py\"}\nfrom dataclasses import dataclass\n\nimport httpx\n\nfrom pydantic_ai import Agent, RunContext\n\n\n@dataclass\nclass ClientAndKey:  # (1)!\n    http_client: httpx.AsyncClient\n    api_key: str\n\n\njoke_selection_agent = Agent(\n    'openai:gpt-5',\n    deps_type=ClientAndKey,  # (2)!\n    system_prompt=(\n        'Use the `joke_factory` tool to generate some jokes on the given subject, '\n        'then choose the best. You must return just a single joke.'\n    ),\n)\njoke_generation_agent = Agent(\n    'google-gla:gemini-2.5-flash',\n    deps_type=ClientAndKey,  # (4)!\n    output_type=list[str],\n    system_prompt=(\n        'Use the \"get_jokes\" tool to get some jokes on the given subject, '\n        'then extract each joke into a list.'\n    ),\n)\n\n\n@joke_selection_agent.tool\nasync def joke_factory(ctx: RunContext[ClientAndKey], count: int) -> list[str]:\n    r = await joke_generation_agent.run(\n        f'Please generate {count} jokes.',\n        deps=ctx.deps,  # (3)!\n        usage=ctx.usage,\n    )\n    return r.output\n\n\n@joke_generation_agent.tool  # (5)!\nasync def get_jokes(ctx: RunContext[ClientAndKey], count: int) -> str:\n    response = await ctx.deps.http_client.get(\n        'https://example.com',\n        params={'count': count},\n        headers={'Authorization': f'Bearer {ctx.deps.api_key}'},\n    )\n    response.raise_for_status()\n    return response.text\n\n\nasync def main():\n    async with httpx.AsyncClient() as client:\n        deps = ClientAndKey(client, 'foobar')\n        result = await joke_selection_agent.run('Tell me a joke.', deps=deps)\n        print(result.output)\n        #> Did you hear about the toothpaste scandal? They called it Colgate.\n        print(result.usage())  # (6)!\n        #> RunUsage(input_tokens=309, output_tokens=32, requests=4, tool_calls=2)\n```\n\n1. Define a dataclass to hold the client and API key dependencies.\n2. Set the `deps_type` of the calling agent \u2014 `joke_selection_agent` here.\n3. Pass the dependencies to the delegate agent's run method within the tool call.\n4. Also set the `deps_type` of the delegate agent \u2014 `joke_generation_agent` here.\n5. Define a tool on the delegate agent that uses the dependencies to make an HTTP request.\n6. Usage now includes 4 requests \u2014 2 from the calling agent and 2 from the delegate agent.\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_\n\nThis example shows how even a fairly simple agent delegation can lead to a complex control flow:\n\n```mermaid\ngraph TD\n  START --> joke_selection_agent\n  joke_selection_agent --> joke_factory[\"joke_factory (tool)\"]\n  joke_factory --> joke_generation_agent\n  joke_generation_agent --> get_jokes[\"get_jokes (tool)\"]\n  get_jokes --> http_request[\"HTTP request\"]\n  http_request --> get_jokes\n  get_jokes --> joke_generation_agent\n  joke_generation_agent --> joke_factory\n  joke_factory --> joke_selection_agent\n  joke_selection_agent --> END\n```", "url": "https://ai.pydantic.dev/docs/multi-agent-applications/#agent-delegation-and-dependencies", "page": "docs/multi-agent-applications", "source_site": "pydantic_ai"}
{"title": "Programmatic agent hand-off", "anchor": "programmatic-agent-hand-off", "heading_level": 2, "md_text": "\"Programmatic agent hand-off\" refers to the scenario where multiple agents are called in succession, with application code and/or a human in the loop responsible for deciding which agent to call next.\n\nHere agents don't need to use the same deps.\n\nHere we show two agents used in succession, the first to find a flight and the second to extract the user's seat preference.\n\n```python {title=\"programmatic_handoff.py\"}\nfrom typing import Literal\n\nfrom pydantic import BaseModel, Field\nfrom rich.prompt import Prompt\n\nfrom pydantic_ai import Agent, ModelMessage, RunContext, RunUsage, UsageLimits\n\n\nclass FlightDetails(BaseModel):\n    flight_number: str\n\n\nclass Failed(BaseModel):\n    \"\"\"Unable to find a satisfactory choice.\"\"\"\n\n\nflight_search_agent = Agent[None, FlightDetails | Failed](  # (1)!\n    'openai:gpt-5',\n    output_type=FlightDetails | Failed,  # type: ignore\n    system_prompt=(\n        'Use the \"flight_search\" tool to find a flight '\n        'from the given origin to the given destination.'\n    ),\n)\n\n\n@flight_search_agent.tool  # (2)!\nasync def flight_search(\n    ctx: RunContext[None], origin: str, destination: str\n) -> FlightDetails | None:\n    # in reality, this would call a flight search API or\n    # use a browser to scrape a flight search website\n    return FlightDetails(flight_number='AK456')\n\n\nusage_limits = UsageLimits(request_limit=15)  # (3)!\n\n\nasync def find_flight(usage: RunUsage) -> FlightDetails | None:  # (4)!\n    message_history: list[ModelMessage] | None = None\n    for _ in range(3):\n        prompt = Prompt.ask(\n            'Where would you like to fly from and to?',\n        )\n        result = await flight_search_agent.run(\n            prompt,\n            message_history=message_history,\n            usage=usage,\n            usage_limits=usage_limits,\n        )\n        if isinstance(result.output, FlightDetails):\n            return result.output\n        else:\n            message_history = result.all_messages(\n                output_tool_return_content='Please try again.'\n            )\n\n\nclass SeatPreference(BaseModel):\n    row: int = Field(ge=1, le=30)\n    seat: Literal['A', 'B', 'C', 'D', 'E', 'F']", "url": "https://ai.pydantic.dev/docs/multi-agent-applications/#programmatic-agent-hand-off", "page": "docs/multi-agent-applications", "source_site": "pydantic_ai"}
{"title": "This agent is responsible for extracting the user's seat selection", "anchor": "this-agent-is-responsible-for-extracting-the-users-seat-selection", "heading_level": 1, "md_text": "seat_preference_agent = Agent[None, SeatPreference | Failed](  # (5)!\n    'openai:gpt-5',\n    output_type=SeatPreference | Failed,  # type: ignore\n    system_prompt=(\n        \"Extract the user's seat preference. \"\n        'Seats A and F are window seats. '\n        'Row 1 is the front row and has extra leg room. '\n        'Rows 14, and 20 also have extra leg room. '\n    ),\n)\n\n\nasync def find_seat(usage: RunUsage) -> SeatPreference:  # (6)!\n    message_history: list[ModelMessage] | None = None\n    while True:\n        answer = Prompt.ask('What seat would you like?')\n\n        result = await seat_preference_agent.run(\n            answer,\n            message_history=message_history,\n            usage=usage,\n            usage_limits=usage_limits,\n        )\n        if isinstance(result.output, SeatPreference):\n            return result.output\n        else:\n            print('Could not understand seat preference. Please try again.')\n            message_history = result.all_messages()\n\n\nasync def main():  # (7)!\n    usage: RunUsage = RunUsage()\n\n    opt_flight_details = await find_flight(usage)\n    if opt_flight_details is not None:\n        print(f'Flight found: {opt_flight_details.flight_number}')\n        #> Flight found: AK456\n        seat_preference = await find_seat(usage)\n        print(f'Seat preference: {seat_preference}')\n        #> Seat preference: row=1 seat='A'\n```\n\n1. Define the first agent, which finds a flight. We use an explicit type annotation until [PEP-747](https://peps.python.org/pep-0747/) lands, see [structured output](output.md#structured-output). We use a union as the output type so the model can communicate if it's unable to find a satisfactory choice; internally, each member of the union will be registered as a separate tool.\n2. Define a tool on the agent to find a flight. In this simple case we could dispense with the tool and just define the agent to return structured data, then search for a flight, but in more complex scenarios the tool would be necessary.\n3. Define usage limits for the entire app.\n4. Define a function to find a flight, which asks the user for their preferences and then calls the agent to find a flight.\n5. As with `flight_search_agent` above, we use an explicit type annotation to define the agent.\n6. Define a function to find the user's seat preference, which asks the user for their seat preference and then calls the agent to extract the seat preference.\n7. Now that we've put our logic for running each agent into separate functions, our main app becomes very simple.\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_\n\nThe control flow for this example can be summarised as follows:\n\n```mermaid\ngraph TB\n  START --> ask_user_flight[\"ask user for flight\"]\n\n  subgraph find_flight\n    flight_search_agent --> ask_user_flight\n    ask_user_flight --> flight_search_agent\n  end\n\n  flight_search_agent --> ask_user_seat[\"ask user for seat\"]\n  flight_search_agent --> END\n\n  subgraph find_seat\n    seat_preference_agent --> ask_user_seat\n    ask_user_seat --> seat_preference_agent\n  end\n\n  seat_preference_agent --> END\n```", "url": "https://ai.pydantic.dev/docs/multi-agent-applications/#this-agent-is-responsible-for-extracting-the-users-seat-selection", "page": "docs/multi-agent-applications", "source_site": "pydantic_ai"}
{"title": "Pydantic Graphs", "anchor": "pydantic-graphs", "heading_level": 2, "md_text": "See the [graph](graph.md) documentation on when and how to use graphs.", "url": "https://ai.pydantic.dev/docs/multi-agent-applications/#pydantic-graphs", "page": "docs/multi-agent-applications", "source_site": "pydantic_ai"}
{"title": "Examples", "anchor": "examples", "heading_level": 2, "md_text": "The following examples demonstrate how to use dependencies in Pydantic AI:\n\n- [Flight booking](examples/flight-booking.md)", "url": "https://ai.pydantic.dev/docs/multi-agent-applications/#examples", "page": "docs/multi-agent-applications", "source_site": "pydantic_ai"}
{"title": "output", "anchor": null, "heading_level": 0, "md_text": "\"Output\" refers to the final value returned from [running an agent](agents.md#running-agents). This can be either plain text, [structured data](#structured-output), an [image](#image-output), or the result of a [function](#output-functions) called with arguments provided by the model.\n\nThe output is wrapped in [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] or [`StreamedRunResult`][pydantic_ai.result.StreamedRunResult] so that you can access other data, like [usage][pydantic_ai.usage.RunUsage] of the run and [message history](message-history.md#accessing-messages-from-results).\n\nBoth `AgentRunResult` and `StreamedRunResult` are generic in the data they wrap, so typing information about the data returned by the agent is preserved.\n\nA run ends when the model responds with one of the output types, or, if no output type is specified or `str` is one of the allowed options, when a plain text response is received. A run can also be cancelled if usage limits are exceeded, see [Usage Limits](agents.md#usage-limits).\n\nHere's an example using a Pydantic model as the `output_type`, forcing the model to respond with data matching our specification:\n\n```python {title=\"olympics.py\" line_length=\"90\"}\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent\n\n\nclass CityLocation(BaseModel):\n    city: str\n    country: str\n\n\nagent = Agent('google-gla:gemini-2.5-flash', output_type=CityLocation)\nresult = agent.run_sync('Where were the olympics held in 2012?')\nprint(result.output)\n#> city='London' country='United Kingdom'\nprint(result.usage())\n#> RunUsage(input_tokens=57, output_tokens=8, requests=1)\n```\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/output/", "page": "docs/output", "source_site": "pydantic_ai"}
{"title": "Structured output data {#structured-output}", "anchor": "structured-output-data-structured-output", "heading_level": 2, "md_text": "The [`Agent`][pydantic_ai.Agent] class constructor takes an `output_type` argument that takes one or more types or [output functions](#output-functions). It supports simple scalar types, list and dict types (including `TypedDict`s and [`StructuredDict`s](#structured-dict)), dataclasses and Pydantic models, as well as type unions -- generally everything supported as type hints in a Pydantic model. You can also pass a list of multiple choices.\n\nBy default, Pydantic AI leverages the model's tool calling capability to make it return structured data. When multiple output types are specified (in a union or list), each member is registered with the model as a separate output tool in order to reduce the complexity of the schema and maximise the chances a model will respond correctly. This has been shown to work well across a wide range of models. If you'd like to change the names of the output tools, use a model's native structured output feature, or pass the output schema to the model in its [instructions](agents.md#instructions), you can use an [output mode](#output-modes) marker class.\n\nWhen no output type is specified, or when `str` is among the output types, any plain text response from the model will be used as the output data.\nIf `str` is not among the output types, the model is forced to return structured data or call an output function.\n\nIf the output type schema is not of type `\"object\"` (e.g. it's `int` or `list[int]`), the output type is wrapped in a single element object, so the schema of all tools registered with the model are object schemas.\n\nStructured outputs (like tools) use Pydantic to build the JSON schema used for the tool, and to validate the data returned by the model.\n\n!!! note \"Type checking considerations\"\n    The Agent class is generic in its output type, and this type is carried through to `AgentRunResult.output` and `StreamedRunResult.output` so that your IDE or static type checker can warn you when your code doesn't properly take into account all the possible values those outputs could have.\n\n    Static type checkers like pyright and mypy will do their best to infer the agent's output type from the `output_type` you've specified, but they're not always able to do so correctly when you provide functions or multiple types in a union or list, even though Pydantic AI will behave correctly. When this happens, your type checker will complain even when you're confident you've passed a valid `output_type`, and you'll need to help the type checker by explicitly specifying the generic parameters on the `Agent` constructor. This is shown in the second example below and the output functions example further down.\n\n    Specifically, there are three valid uses of `output_type` where you'll need to do this:\n\n    1. When using a union of types, e.g. `output_type=Foo | Bar`. Until [PEP-747](https://peps.python.org/pep-0747/) \"Annotating Type Forms\" lands in Python 3.15, type checkers do not consider these a valid value for `output_type`. In addition to the generic parameters on the `Agent` constructor, you'll need to add `# type: ignore` to the line that passes the union to `output_type`. Alternatively, you can use a list: `output_type=[Foo, Bar]`.\n    2. With mypy: When using a list, as a functionally equivalent alternative to a union, or because you're passing in [output functions](#output-functions). Pyright does handle this correctly, and we've filed [an issue](https://github.com/python/mypy/issues/19142) with mypy to try and get this fixed.\n    3. With mypy: when using an async output function. Pyright does handle this correctly, and we've filed [an issue](https://github.com/python/mypy/issues/19143) with mypy to try and get this fixed.\n\nHere's an example of returning either text or structured data:\n\n```python {title=\"box_or_error.py\"}\n\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent\n\n\nclass Box(BaseModel):\n    width: int\n    height: int\n    depth: int\n    units: str\n\n\nagent = Agent(\n    'openai:gpt-5-mini',\n    output_type=[Box, str], # (1)!\n    system_prompt=(\n        \"Extract me the dimensions of a box, \"\n        \"if you can't extract all data, ask the user to try again.\"\n    ),\n)\n\nresult = agent.run_sync('The box is 10x20x30')\nprint(result.output)\n#> Please provide the units for the dimensions (e.g., cm, in, m).\n\nresult = agent.run_sync('The box is 10x20x30 cm')\nprint(result.output)\n#> width=10 height=20 depth=30 units='cm'\n```\n\n1. This could also have been a union: `output_type=Box | str`. However, as explained in the \"Type checking considerations\" section above, that would've required explicitly specifying the generic parameters on the `Agent` constructor and adding `# type: ignore` to this line in order to be type checked correctly.\n\n_(This example is complete, it can be run \"as is\")_\n\nHere's an example of using a union return type, which will register multiple output tools and wrap non-object schemas in an object:\n\n```python {title=\"colors_or_sizes.py\"}\nfrom pydantic_ai import Agent", "url": "https://ai.pydantic.dev/docs/output/#structured-output-data-structured-output", "page": "docs/output", "source_site": "pydantic_ai"}
{"title": "Structured output data {#structured-output}", "anchor": "structured-output-data-structured-output", "heading_level": 2, "md_text": "agent = Agent[None, list[str] | list[int]](\n    'openai:gpt-5-mini',\n    output_type=list[str] | list[int],  # type: ignore # (1)!\n    system_prompt='Extract either colors or sizes from the shapes provided.',\n)\n\nresult = agent.run_sync('red square, blue circle, green triangle')\nprint(result.output)\n#> ['red', 'blue', 'green']\n\nresult = agent.run_sync('square size 10, circle size 20, triangle size 30')\nprint(result.output)\n#> [10, 20, 30]\n```\n\n1. As explained in the \"Type checking considerations\" section above, using a union rather than a list requires explicitly specifying the generic parameters on the `Agent` constructor and adding `# type: ignore` to this line in order to be type checked correctly.\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/output/#structured-output-data-structured-output", "page": "docs/output", "source_site": "pydantic_ai"}
{"title": "Output functions", "anchor": "output-functions", "heading_level": 3, "md_text": "Instead of plain text or structured data, you may want the output of your agent run to be the result of a function called with arguments provided by the model, for example to further process or validate the data provided through the arguments (with the option to tell the model to try again), or to hand off to another agent.\n\nOutput functions are similar to [function tools](tools.md), but the model is forced to call one of them, the call ends the agent run, and the result is not passed back to the model.\n\nAs with tool functions, output function arguments provided by the model are validated using Pydantic, they can optionally take [`RunContext`][pydantic_ai.tools.RunContext] as the first argument, and they can raise [`ModelRetry`][pydantic_ai.exceptions.ModelRetry] to ask the model to try again with modified arguments (or with a different output type).\n\nTo specify output functions, you set the agent's `output_type` to either a single function (or bound instance method), or a list of functions. The list can also contain other output types like simple scalars or entire Pydantic models.\nYou typically do not want to also register your output function as a tool (using the `@agent.tool` decorator or `tools` argument), as this could confuse the model about which it should be calling.\n\nHere's an example of all of these features in action:\n\n```python {title=\"output_functions.py\"}\nimport re\n\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, ModelRetry, RunContext, UnexpectedModelBehavior\n\n\nclass Row(BaseModel):\n    name: str\n    country: str\n\n\ntables = {\n    'capital_cities': [\n        Row(name='Amsterdam', country='Netherlands'),\n        Row(name='Mexico City', country='Mexico'),\n    ]\n}\n\n\nclass SQLFailure(BaseModel):\n    \"\"\"An unrecoverable failure. Only use this when you can't change the query to make it work.\"\"\"\n\n    explanation: str\n\n\ndef run_sql_query(query: str) -> list[Row]:\n    \"\"\"Run a SQL query on the database.\"\"\"\n\n    select_table = re.match(r'SELECT (.+) FROM (\\w+)', query)\n    if select_table:\n        column_names = select_table.group(1)\n        if column_names != '*':\n            raise ModelRetry(\"Only 'SELECT *' is supported, you'll have to do column filtering manually.\")\n\n        table_name = select_table.group(2)\n        if table_name not in tables:\n            raise ModelRetry(\n                f\"Unknown table '{table_name}' in query '{query}'. Available tables: {', '.join(tables.keys())}.\"\n            )\n\n        return tables[table_name]\n\n    raise ModelRetry(f\"Unsupported query: '{query}'.\")\n\n\nsql_agent = Agent[None, list[Row] | SQLFailure](\n    'openai:gpt-5',\n    output_type=[run_sql_query, SQLFailure],\n    instructions='You are a SQL agent that can run SQL queries on a database.',\n)\n\n\nasync def hand_off_to_sql_agent(ctx: RunContext, query: str) -> list[Row]:\n    \"\"\"I take natural language queries, turn them into SQL, and run them on a database.\"\"\"\n\n    # Drop the final message with the output tool call, as it shouldn't be passed on to the SQL agent\n    messages = ctx.messages[:-1]\n    try:\n        result = await sql_agent.run(query, message_history=messages)\n        output = result.output\n        if isinstance(output, SQLFailure):\n            raise ModelRetry(f'SQL agent failed: {output.explanation}')\n        return output\n    except UnexpectedModelBehavior as e:\n        # Bubble up potentially retryable errors to the router agent\n        if (cause := e.__cause__) and isinstance(cause, ModelRetry):\n            raise ModelRetry(f'SQL agent failed: {cause.message}') from e\n        else:\n            raise\n\n\nclass RouterFailure(BaseModel):\n    \"\"\"Use me when no appropriate agent is found or the used agent failed.\"\"\"\n\n    explanation: str\n\n\nrouter_agent = Agent[None, list[Row] | RouterFailure](\n    'openai:gpt-5',\n    output_type=[hand_off_to_sql_agent, RouterFailure],\n    instructions='You are a router to other agents. Never try to solve a problem yourself, just pass it on.',\n)\n\nresult = router_agent.run_sync('Select the names and countries of all capitals')\nprint(result.output)\n\"\"\"\n[\n    Row(name='Amsterdam', country='Netherlands'),\n    Row(name='Mexico City', country='Mexico'),\n]\n\"\"\"\n\nresult = router_agent.run_sync('Select all pets')\nprint(repr(result.output))\n\"\"\"\nRouterFailure(explanation=\"The requested table 'pets' does not exist in the database. The only available table is 'capital_cities', which does not contain data about pets.\")\n\"\"\"\n\nresult = router_agent.run_sync('How do I fly from Amsterdam to Mexico City?')\nprint(repr(result.output))\n\"\"\"\nRouterFailure(explanation='I am not equipped to provide travel information, such as flights from Amsterdam to Mexico City.')\n\"\"\"\n```", "url": "https://ai.pydantic.dev/docs/output/#output-functions", "page": "docs/output", "source_site": "pydantic_ai"}
{"title": "Text output", "anchor": "text-output", "heading_level": 4, "md_text": "If you provide an output function that takes a string, Pydantic AI will by default create an output tool like for any other output function. If instead you'd like the model to provide the string using plain text output, you can wrap the function in the [`TextOutput`][pydantic_ai.output.TextOutput] marker class. If desired, this marker class can be used alongside one or more [`ToolOutput`](#tool-output) marker classes (or unmarked types or functions) in a list provided to `output_type`.\n\n```python {title=\"text_output_function.py\"}\nfrom pydantic_ai import Agent, TextOutput\n\n\ndef split_into_words(text: str) -> list[str]:\n    return text.split()\n\n\nagent = Agent(\n    'openai:gpt-5',\n    output_type=TextOutput(split_into_words),\n)\nresult = agent.run_sync('Who was Albert Einstein?')\nprint(result.output)\n#> ['Albert', 'Einstein', 'was', 'a', 'German-born', 'theoretical', 'physicist.']\n```\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/output/#text-output", "page": "docs/output", "source_site": "pydantic_ai"}
{"title": "Output modes", "anchor": "output-modes", "heading_level": 3, "md_text": "Pydantic AI implements three different methods to get a model to output structured data:\n\n1. [Tool Output](#tool-output), where tool calls are used to produce the output.\n2. [Native Output](#native-output), where the model is required to produce text content compliant with a provided JSON schema.\n3. [Prompted Output](#prompted-output), where a prompt is injected into the model instructions including the desired JSON schema, and we attempt to parse the model's plain-text response as appropriate.", "url": "https://ai.pydantic.dev/docs/output/#output-modes", "page": "docs/output", "source_site": "pydantic_ai"}
{"title": "Tool Output", "anchor": "tool-output", "heading_level": 4, "md_text": "In the default Tool Output mode, the output JSON schema of each output type (or function) is provided to the model as the parameters schema of a special output tool. This is the default as it's supported by virtually all models and has been shown to work very well.\n\nIf you'd like to change the name of the output tool, pass a custom description to aid the model, or turn on or off strict mode, you can wrap the type(s) in the [`ToolOutput`][pydantic_ai.output.ToolOutput] marker class and provide the appropriate arguments. Note that by default, the description is taken from the docstring specified on a Pydantic model or output function, so specifying it using the marker class is typically not necessary.\n\nTo dynamically modify or filter the available output tools during an agent run, you can define an agent-wide `prepare_output_tools` function that will be called ahead of each step of a run. This function should be of type [`ToolsPrepareFunc`][pydantic_ai.tools.ToolsPrepareFunc], which takes the [`RunContext`][pydantic_ai.tools.RunContext] and a list of [`ToolDefinition`][pydantic_ai.tools.ToolDefinition], and returns a new list of tool definitions (or `None` to disable all tools for that step). This is analogous to the [`prepare_tools` function](tools-advanced.md#prepare-tools) for non-output tools.\n\n```python {title=\"tool_output.py\"}\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, ToolOutput\n\n\nclass Fruit(BaseModel):\n    name: str\n    color: str\n\n\nclass Vehicle(BaseModel):\n    name: str\n    wheels: int\n\n\nagent = Agent(\n    'openai:gpt-5',\n    output_type=[ # (1)!\n        ToolOutput(Fruit, name='return_fruit'),\n        ToolOutput(Vehicle, name='return_vehicle'),\n    ],\n)\nresult = agent.run_sync('What is a banana?')\nprint(repr(result.output))\n#> Fruit(name='banana', color='yellow')\n```\n\n1. If we were passing just `Fruit` and `Vehicle` without custom tool names, we could have used a union: `output_type=Fruit | Vehicle`. However, as `ToolOutput` is an object rather than a type, we have to use a list.\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/output/#tool-output", "page": "docs/output", "source_site": "pydantic_ai"}
{"title": "Native Output", "anchor": "native-output", "heading_level": 4, "md_text": "Native Output mode uses a model's native \"Structured Outputs\" feature (aka \"JSON Schema response format\"), where the model is forced to only output text matching the provided JSON schema. Note that this is not supported by all models, and sometimes comes with restrictions. For example, Anthropic does not support this at all, and Gemini cannot use tools at the same time as structured output, and attempting to do so will result in an error.\n\nTo use this mode, you can wrap the output type(s) in the [`NativeOutput`][pydantic_ai.output.NativeOutput] marker class that also lets you specify a `name` and `description` if the name and docstring of the type or function are not sufficient.\n\n```python {title=\"native_output.py\" requires=\"tool_output.py\"}\nfrom pydantic_ai import Agent, NativeOutput\n\nfrom tool_output import Fruit, Vehicle\n\nagent = Agent(\n    'openai:gpt-5',\n    output_type=NativeOutput(\n        [Fruit, Vehicle], # (1)!\n        name='Fruit_or_vehicle',\n        description='Return a fruit or vehicle.'\n    ),\n)\nresult = agent.run_sync('What is a Ford Explorer?')\nprint(repr(result.output))\n#> Vehicle(name='Ford Explorer', wheels=4)\n```\n\n1. This could also have been a union: `output_type=Fruit | Vehicle`. However, as explained in the \"Type checking considerations\" section above, that would've required explicitly specifying the generic parameters on the `Agent` constructor and adding `# type: ignore` to this line in order to be type checked correctly.\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/output/#native-output", "page": "docs/output", "source_site": "pydantic_ai"}
{"title": "Prompted Output", "anchor": "prompted-output", "heading_level": 4, "md_text": "In this mode, the model is prompted to output text matching the provided JSON schema through its [instructions](agents.md#instructions) and it's up to the model to interpret those instructions correctly. This is usable with all models, but is often the least reliable approach as the model is not forced to match the schema.\n\nWhile we would generally suggest starting with tool or native output, in some cases this mode may result in higher quality outputs, and for models without native tool calling or structured output support it is the only option for producing structured outputs.\n\nIf the model API supports the \"JSON Mode\" feature (aka \"JSON Object response format\") to force the model to output valid JSON, this is enabled, but it's still up to the model to abide by the schema. Pydantic AI will validate the returned structured data and tell the model to try again if validation fails, but if the model is not intelligent enough this may not be sufficient.\n\nTo use this mode, you can wrap the output type(s) in the [`PromptedOutput`][pydantic_ai.output.PromptedOutput] marker class that also lets you specify a `name` and `description` if the name and docstring of the type or function are not sufficient. Additionally, it supports an `template` argument lets you specify a custom instructions template to be used instead of the [default][pydantic_ai.profiles.ModelProfile.prompted_output_template].\n\n```python {title=\"prompted_output.py\" requires=\"tool_output.py\"}\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, PromptedOutput\n\nfrom tool_output import Vehicle\n\n\nclass Device(BaseModel):\n    name: str\n    kind: str\n\n\nagent = Agent(\n    'openai:gpt-5',\n    output_type=PromptedOutput(\n        [Vehicle, Device], # (1)!\n        name='Vehicle or device',\n        description='Return a vehicle or device.'\n    ),\n)\nresult = agent.run_sync('What is a MacBook?')\nprint(repr(result.output))\n#> Device(name='MacBook', kind='laptop')\n\nagent = Agent(\n    'openai:gpt-5',\n    output_type=PromptedOutput(\n        [Vehicle, Device],\n        template='Gimme some JSON: {schema}'\n    ),\n)\nresult = agent.run_sync('What is a Ford Explorer?')\nprint(repr(result.output))\n#> Vehicle(name='Ford Explorer', wheels=4)\n```\n\n1. This could also have been a union: `output_type=Vehicle | Device`. However, as explained in the \"Type checking considerations\" section above, that would've required explicitly specifying the generic parameters on the `Agent` constructor and adding `# type: ignore` to this line in order to be type checked correctly.\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/output/#prompted-output", "page": "docs/output", "source_site": "pydantic_ai"}
{"title": "Custom JSON schema {#structured-dict}", "anchor": "custom-json-schema-structured-dict", "heading_level": 3, "md_text": "If it's not feasible to define your desired structured output object using a Pydantic `BaseModel`, dataclass, or `TypedDict`, for example when you get a JSON schema from an external source or generate it dynamically, you can use the [`StructuredDict()`][pydantic_ai.output.StructuredDict] helper function to generate a `dict[str, Any]` subclass with a JSON schema attached that Pydantic AI will pass to the model.\n\nNote that Pydantic AI will not perform any validation of the received JSON object and it's up to the model to correctly interpret the schema and any constraints expressed in it, like required fields or integer value ranges.\n\nThe output type will be a `dict[str, Any]` and it's up to your code to defensively read from it in case the model made a mistake. You can use an [output validator](#output-validator-functions) to reflect validation errors back to the model and get it to try again.\n\nAlong with the JSON schema, you can optionally pass `name` and `description` arguments to provide additional context to the model:\n\n```python\nfrom pydantic_ai import Agent, StructuredDict\n\nHumanDict = StructuredDict(\n    {\n        'type': 'object',\n        'properties': {\n            'name': {'type': 'string'},\n            'age': {'type': 'integer'}\n        },\n        'required': ['name', 'age']\n    },\n    name='Human',\n    description='A human with a name and age',\n)\n\nagent = Agent('openai:gpt-5', output_type=HumanDict)\nresult = agent.run_sync('Create a person')\n#> {'name': 'John Doe', 'age': 30}\n```", "url": "https://ai.pydantic.dev/docs/output/#custom-json-schema-structured-dict", "page": "docs/output", "source_site": "pydantic_ai"}
{"title": "Output validators {#output-validator-functions}", "anchor": "output-validators-output-validator-functions", "heading_level": 3, "md_text": "Some validation is inconvenient or impossible to do in Pydantic validators, in particular when the validation requires IO and is asynchronous. Pydantic AI provides a way to add validation functions via the [`agent.output_validator`][pydantic_ai.Agent.output_validator] decorator.\n\nIf you want to implement separate validation logic for different output types, it's recommended to use [output functions](#output-functions) instead, to save you from having to do `isinstance` checks inside the output validator.\nIf you want the model to output plain text, do your own processing or validation, and then have the agent's final output be the result of your function, it's recommended to use an [output function](#output-functions) with the [`TextOutput` marker class](#text-output).\n\nHere's a simplified variant of the [SQL Generation example](examples/sql-gen.md):\n\n```python {title=\"sql_gen.py\"}\nfrom fake_database import DatabaseConn, QueryError\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, RunContext, ModelRetry\n\n\nclass Success(BaseModel):\n    sql_query: str\n\n\nclass InvalidRequest(BaseModel):\n    error_message: str\n\n\nOutput = Success | InvalidRequest\nagent = Agent[DatabaseConn, Output](\n    'google-gla:gemini-2.5-flash',\n    output_type=Output,  # type: ignore\n    deps_type=DatabaseConn,\n    system_prompt='Generate PostgreSQL flavored SQL queries based on user input.',\n)\n\n\n@agent.output_validator\nasync def validate_sql(ctx: RunContext[DatabaseConn], output: Output) -> Output:\n    if isinstance(output, InvalidRequest):\n        return output\n    try:\n        await ctx.deps.execute(f'EXPLAIN {output.sql_query}')\n    except QueryError as e:\n        raise ModelRetry(f'Invalid query: {e}') from e\n    else:\n        return output\n\n\nresult = agent.run_sync(\n    'get me users who were last active yesterday.', deps=DatabaseConn()\n)\nprint(result.output)\n#> sql_query='SELECT * FROM users WHERE last_active::date = today() - interval 1 day'\n```\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/output/#output-validators-output-validator-functions", "page": "docs/output", "source_site": "pydantic_ai"}
{"title": "Image output", "anchor": "image-output", "heading_level": 2, "md_text": "Some models can generate images as part of their response, for example those that support the [Image Generation built-in tool](builtin-tools.md#image-generation-tool) and OpenAI models using the [Code Execution built-in tool](builtin-tools.md#code-execution-tool) when told to generate a chart.\n\nTo use the generated image as the output of the agent run, you can set `output_type` to [`BinaryImage`][pydantic_ai.messages.BinaryImage]. If no image-generating built-in tool is explicitly specified, the [`ImageGenerationTool`][pydantic_ai.builtin_tools.ImageGenerationTool] will be enabled automatically.\n\n```py {title=\"image_output.py\"}\nfrom pydantic_ai import Agent, BinaryImage\n\nagent = Agent('openai-responses:gpt-5', output_type=BinaryImage)\n\nresult = agent.run_sync('Generate an image of an axolotl.')\nassert isinstance(result.output, BinaryImage)\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nIf an agent does not need to always generate an image, you can use a union of `BinaryImage` and `str`. If the model generates both, the image will take precedence as output and the text will be available on [`ModelResponse.text`][pydantic_ai.messages.ModelResponse.text]:\n\n```py {title=\"image_output_union.py\"}\nfrom pydantic_ai import Agent, BinaryImage\n\nagent = Agent('openai-responses:gpt-5', output_type=BinaryImage | str)\n\nresult = agent.run_sync('Tell me a two-sentence story about an axolotl, no image please.')\nprint(result.output)\n\"\"\"\nOnce upon a time, in a hidden underwater cave, lived a curious axolotl named Pip who loved to explore. One day, while venturing further than usual, Pip discovered a shimmering, ancient coin that granted wishes!\n\"\"\"\n\nresult = agent.run_sync('Tell me a two-sentence story about an axolotl with an illustration.')\nassert isinstance(result.output, BinaryImage)\nprint(result.response.text)\n\"\"\"\nOnce upon a time, in a hidden underwater cave, lived a curious axolotl named Pip who loved to explore. One day, while venturing further than usual, Pip discovered a shimmering, ancient coin that granted wishes!\n\"\"\"\n```", "url": "https://ai.pydantic.dev/docs/output/#image-output", "page": "docs/output", "source_site": "pydantic_ai"}
{"title": "Streamed Results", "anchor": "streamed-results", "heading_level": 2, "md_text": "There two main challenges with streamed results:\n\n1. Validating structured responses before they're complete, this is achieved by \"partial validation\" which was recently added to Pydantic in [pydantic/pydantic#10748](https://github.com/pydantic/pydantic/pull/10748).\n2. When receiving a response, we don't know if it's the final response without starting to stream it and peeking at the content. Pydantic AI streams just enough of the response to sniff out if it's a tool call or an output, then streams the whole thing and calls tools, or returns the stream as a [`StreamedRunResult`][pydantic_ai.result.StreamedRunResult].\n\n!!! note\n    As the `run_stream()` method will consider the first output matching the `output_type` to be the final output,\n    it will stop running the agent graph and will not execute any tool calls made by the model after this \"final\" output.\n\n    If you want to always run the agent graph to completion and stream all events from the model's streaming response and the agent's execution of tools,\n    use [`agent.run_stream_events()`][pydantic_ai.agent.AbstractAgent.run_stream_events] ([docs](agents.md#streaming-all-events)) or [`agent.iter()`][pydantic_ai.agent.AbstractAgent.iter] ([docs](agents.md#streaming-all-events-and-output)) instead.", "url": "https://ai.pydantic.dev/docs/output/#streamed-results", "page": "docs/output", "source_site": "pydantic_ai"}
{"title": "Streaming Text", "anchor": "streaming-text", "heading_level": 3, "md_text": "Example of streamed text output:\n\n```python {title=\"streamed_hello_world.py\" line_length=\"120\"}\nfrom pydantic_ai import Agent\n\nagent = Agent('google-gla:gemini-2.5-flash')  # (1)!\n\n\nasync def main():\n    async with agent.run_stream('Where does \"hello world\" come from?') as result:  # (2)!\n        async for message in result.stream_text():  # (3)!\n            print(message)\n            #> The first known\n            #> The first known use of \"hello,\n            #> The first known use of \"hello, world\" was in\n            #> The first known use of \"hello, world\" was in a 1974 textbook\n            #> The first known use of \"hello, world\" was in a 1974 textbook about the C\n            #> The first known use of \"hello, world\" was in a 1974 textbook about the C programming language.\n```\n\n1. Streaming works with the standard [`Agent`][pydantic_ai.Agent] class, and doesn't require any special setup, just a model that supports streaming (currently all models support streaming).\n2. The [`Agent.run_stream()`][pydantic_ai.agent.AbstractAgent.run_stream] method is used to start a streamed run, this method returns a context manager so the connection can be closed when the stream completes.\n3. Each item yield by [`StreamedRunResult.stream_text()`][pydantic_ai.result.StreamedRunResult.stream_text] is the complete text response, extended as new data is received.\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_\n\nWe can also stream text as deltas rather than the entire text in each item:\n\n```python {title=\"streamed_delta_hello_world.py\"}\nfrom pydantic_ai import Agent\n\nagent = Agent('google-gla:gemini-2.5-flash')\n\n\nasync def main():\n    async with agent.run_stream('Where does \"hello world\" come from?') as result:\n        async for message in result.stream_text(delta=True):  # (1)!\n            print(message)\n            #> The first known\n            #> use of \"hello,\n            #> world\" was in\n            #> a 1974 textbook\n            #> about the C\n            #> programming language.\n```\n\n1. [`stream_text`][pydantic_ai.result.StreamedRunResult.stream_text] will error if the response is not text.\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_\n\n!!! warning \"Output message not included in `messages`\"\n    The final output message will **NOT** be added to result messages if you use `.stream_text(delta=True)`,\n    see [Messages and chat history](message-history.md) for more information.", "url": "https://ai.pydantic.dev/docs/output/#streaming-text", "page": "docs/output", "source_site": "pydantic_ai"}
{"title": "Streaming Structured Output", "anchor": "streaming-structured-output", "heading_level": 3, "md_text": "Here's an example of streaming a user profile as it's built:\n\n```python {title=\"streamed_user_profile.py\" line_length=\"120\"}\nfrom datetime import date\n\nfrom typing_extensions import NotRequired, TypedDict\n\nfrom pydantic_ai import Agent\n\n\nclass UserProfile(TypedDict):\n    name: str\n    dob: NotRequired[date]\n    bio: NotRequired[str]\n\n\nagent = Agent(\n    'openai:gpt-5',\n    output_type=UserProfile,\n    system_prompt='Extract a user profile from the input',\n)\n\n\nasync def main():\n    user_input = 'My name is Ben, I was born on January 28th 1990, I like the chain the dog and the pyramid.'\n    async with agent.run_stream(user_input) as result:\n        async for profile in result.stream_output():\n            print(profile)\n            #> {'name': 'Ben'}\n            #> {'name': 'Ben'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the '}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyr'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_\n\nAs setting an `output_type` uses the [Tool Output](#tool-output) mode by default, this will only work if the model supports streaming tool arguments. For models that don't, like Gemini, try [Native Output](#native-output) or [Prompted Output](#prompted-output) instead.", "url": "https://ai.pydantic.dev/docs/output/#streaming-structured-output", "page": "docs/output", "source_site": "pydantic_ai"}
{"title": "Streaming Model Responses", "anchor": "streaming-model-responses", "heading_level": 3, "md_text": "If you want fine-grained control of validation, you can use the following pattern to get the entire partial [`ModelResponse`][pydantic_ai.messages.ModelResponse]:\n\n```python {title=\"streamed_user_profile.py\" line_length=\"120\"}\nfrom datetime import date\n\nfrom pydantic import ValidationError\nfrom typing_extensions import TypedDict\n\nfrom pydantic_ai import Agent\n\n\nclass UserProfile(TypedDict, total=False):\n    name: str\n    dob: date\n    bio: str\n\n\nagent = Agent('openai:gpt-5', output_type=UserProfile)\n\n\nasync def main():\n    user_input = 'My name is Ben, I was born on January 28th 1990, I like the chain the dog and the pyramid.'\n    async with agent.run_stream(user_input) as result:\n        async for message, last in result.stream_responses(debounce_by=0.01):  # (1)!\n            try:\n                profile = await result.validate_response_output(  # (2)!\n                    message,\n                    allow_partial=not last,\n                )\n            except ValidationError:\n                continue\n            print(profile)\n            #> {'name': 'Ben'}\n            #> {'name': 'Ben'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the '}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyr'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}\n            #> {'name': 'Ben', 'dob': date(1990, 1, 28), 'bio': 'Likes the chain the dog and the pyramid'}\n```\n\n1. [`stream_responses`][pydantic_ai.result.StreamedRunResult.stream_responses] streams the data as [`ModelResponse`][pydantic_ai.messages.ModelResponse] objects, thus iteration can't fail with a `ValidationError`.\n2. [`validate_response_output`][pydantic_ai.result.StreamedRunResult.validate_response_output] validates the data, `allow_partial=True` enables pydantic's [`experimental_allow_partial` flag on `TypeAdapter`][pydantic.type_adapter.TypeAdapter.validate_json].\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/output/#streaming-model-responses", "page": "docs/output", "source_site": "pydantic_ai"}
{"title": "Examples", "anchor": "examples", "heading_level": 2, "md_text": "The following examples demonstrate how to use streamed responses in Pydantic AI:\n\n- [Stream markdown](examples/stream-markdown.md)\n- [Stream Whales](examples/stream-whales.md)", "url": "https://ai.pydantic.dev/docs/output/#examples", "page": "docs/output", "source_site": "pydantic_ai"}
{"title": "Pydantic Logfire Debugging and Monitoring", "anchor": "pydantic-logfire-debugging-and-monitoring", "heading_level": 1, "md_text": "Applications that use LLMs have some challenges that are well known and understood: LLMs are **slow**, **unreliable** and **expensive**.\n\nThese applications also have some challenges that most developers have encountered much less often: LLMs are **fickle** and **non-deterministic**. Subtle changes in a prompt can completely change a model's performance, and there's no `EXPLAIN` query you can run to understand why.\n\n!!! danger \"Warning\"\n    From a software engineers point of view, you can think of LLMs as the worst database you've ever heard of, but worse.\n\n    If LLMs weren't so bloody useful, we'd never touch them.\n\nTo build successful applications with LLMs, we need new tools to understand both model performance, and the behavior of applications that rely on them.\n\nLLM Observability tools that just let you understand how your model is performing are useless: making API calls to an LLM is easy, it's building that into an application that's hard.", "url": "https://ai.pydantic.dev/docs/logfire/#pydantic-logfire-debugging-and-monitoring", "page": "docs/logfire", "source_site": "pydantic_ai"}
{"title": "Pydantic Logfire", "anchor": "pydantic-logfire", "heading_level": 2, "md_text": "[Pydantic Logfire](https://pydantic.dev/logfire) is an observability platform developed by the team who created and maintain Pydantic Validation and Pydantic AI. Logfire aims to let you understand your entire application: Gen AI, classic predictive AI, HTTP traffic, database queries and everything else a modern application needs, all using OpenTelemetry.\n\n!!! tip \"Pydantic Logfire is a commercial product\"\n    Logfire is a commercially supported, hosted platform with an extremely generous and perpetual [free tier](https://pydantic.dev/pricing/).\n    You can sign up and start using Logfire in a couple of minutes. Logfire can also be self-hosted on the enterprise tier.\n\nPydantic AI has built-in (but optional) support for Logfire. That means if the `logfire` package is installed and configured and agent instrumentation is enabled then detailed information about agent runs is sent to Logfire. Otherwise there's virtually no overhead and nothing is sent.\n\nHere's an example showing details of running the [Weather Agent](examples/weather-agent.md) in Logfire:\n\n![Weather Agent Logfire](img/logfire-weather-agent.png)\n\nA trace is generated for the agent run, and spans are emitted for each model request and tool call.", "url": "https://ai.pydantic.dev/docs/logfire/#pydantic-logfire", "page": "docs/logfire", "source_site": "pydantic_ai"}
{"title": "Using Logfire", "anchor": "using-logfire", "heading_level": 2, "md_text": "To use Logfire, you'll need a Logfire [account](https://logfire.pydantic.dev). The Logfire Python SDK is included with `pydantic-ai`:\n\n```bash\npip/uv-add pydantic-ai\n```\n\nOr if you're using the slim package, you can install it with the `logfire` optional group:\n\n```bash\npip/uv-add \"pydantic-ai-slim[logfire]\"\n```\n\nThen authenticate your local environment with Logfire:\n\n```bash\npy-cli logfire auth\n```\n\nAnd configure a project to send data to:\n\n```bash\npy-cli logfire projects new\n```\n\n(Or use an existing project with `logfire projects use`)\n\nThis will write to a `.logfire` directory in the current working directory, which the Logfire SDK will use for configuration at run time.\n\nWith that, you can start using Logfire to instrument Pydantic AI code:\n\n```python {title=\"instrument_pydantic_ai.py\" hl_lines=\"1 5 6\"}\nimport logfire\n\nfrom pydantic_ai import Agent\n\nlogfire.configure()  # (1)!\nlogfire.instrument_pydantic_ai()  # (2)!\n\nagent = Agent('openai:gpt-5', instructions='Be concise, reply with one sentence.')\nresult = agent.run_sync('Where does \"hello world\" come from?')  # (3)!\nprint(result.output)\n\"\"\"\nThe first known use of \"hello, world\" was in a 1974 textbook about the C programming language.\n\"\"\"\n```\n\n1. [`logfire.configure()`][logfire.configure] configures the SDK, by default it will find the write token from the `.logfire` directory, but you can also pass a token directly.\n2. [`logfire.instrument_pydantic_ai()`][logfire.Logfire.instrument_pydantic_ai] enables instrumentation of Pydantic AI.\n3. Since we've enabled instrumentation, a trace will be generated for each run, with spans emitted for models calls and tool function execution\n\n_(This example is complete, it can be run \"as is\")_\n\nWhich will display in Logfire thus:\n\n![Logfire Simple Agent Run](img/logfire-simple-agent.png)\n\nThe [Logfire documentation](https://logfire.pydantic.dev/docs/) has more details on how to use Logfire,\nincluding how to instrument other libraries like [HTTPX](https://logfire.pydantic.dev/docs/integrations/http-clients/httpx/) and [FastAPI](https://logfire.pydantic.dev/docs/integrations/web-frameworks/fastapi/).\n\nSince Logfire is built on [OpenTelemetry](https://opentelemetry.io/), you can use the Logfire Python SDK to send data to any OpenTelemetry collector, see [below](#using-opentelemetry).", "url": "https://ai.pydantic.dev/docs/logfire/#using-logfire", "page": "docs/logfire", "source_site": "pydantic_ai"}
{"title": "Debugging", "anchor": "debugging", "heading_level": 3, "md_text": "To demonstrate how Logfire can let you visualise the flow of a Pydantic AI run, here's the view you get from Logfire while running the [chat app examples](examples/chat-app.md):\n\n{{ video('a764aff5840534dc77eba7d028707bfa', 25) }}", "url": "https://ai.pydantic.dev/docs/logfire/#debugging", "page": "docs/logfire", "source_site": "pydantic_ai"}
{"title": "Monitoring Performance", "anchor": "monitoring-performance", "heading_level": 3, "md_text": "We can also query data with SQL in Logfire to monitor the performance of an application. Here's a real world example of using Logfire to monitor Pydantic AI runs inside Logfire itself:\n\n![Logfire monitoring Pydantic AI](img/logfire-monitoring-pydanticai.png)", "url": "https://ai.pydantic.dev/docs/logfire/#monitoring-performance", "page": "docs/logfire", "source_site": "pydantic_ai"}
{"title": "Monitoring HTTP Requests", "anchor": "monitoring-http-requests", "heading_level": 3, "md_text": "!!! tip \"\\\"F**k you, show me the prompt.\\\"\"\n    As per Hamel Husain's influential 2024 blog post [\"Fuck You, Show Me The Prompt.\"](https://hamel.dev/blog/posts/prompt/)\n    (bear with the capitalization, the point is valid), it's often useful to be able to view the raw HTTP requests and responses made to model providers.\n\n    To observe raw HTTP requests made to model providers, you can use Logfire's [HTTPX instrumentation](https://logfire.pydantic.dev/docs/integrations/http-clients/httpx/) since all provider SDKs use the [HTTPX](https://www.python-httpx.org/) library internally.\n\n=== \"With HTTP instrumentation\"\n\n    ```py {title=\"with_logfire_instrument_httpx.py\" hl_lines=\"7\"}\n    import logfire\n\n    from pydantic_ai import Agent\n\n    logfire.configure()\n    logfire.instrument_pydantic_ai()\n    logfire.instrument_httpx(capture_all=True)  # (1)!\n    agent = Agent('openai:gpt-5')\n    result = agent.run_sync('What is the capital of France?')\n    print(result.output)\n    #> The capital of France is Paris.\n    ```\n\n    1. See the [`logfire.instrument_httpx` docs][logfire.Logfire.instrument_httpx] more details, `capture_all=True` means both headers and body are captured for both the request and response.\n\n    ![Logfire with HTTPX instrumentation](img/logfire-with-httpx.png)\n\n=== \"Without HTTP instrumentation\"\n\n    ```py {title=\"without_logfire_instrument_httpx.py\"}\n    import logfire\n\n    from pydantic_ai import Agent\n\n    logfire.configure()\n    logfire.instrument_pydantic_ai()\n\n    agent = Agent('openai:gpt-5')\n    result = agent.run_sync('What is the capital of France?')\n    print(result.output)\n    #> The capital of France is Paris.\n    ```\n\n    ![Logfire without HTTPX instrumentation](img/logfire-without-httpx.png)", "url": "https://ai.pydantic.dev/docs/logfire/#monitoring-http-requests", "page": "docs/logfire", "source_site": "pydantic_ai"}
{"title": "Using OpenTelemetry", "anchor": "using-opentelemetry", "heading_level": 2, "md_text": "Pydantic AI's instrumentation uses [OpenTelemetry](https://opentelemetry.io/) (OTel), which Logfire is based on.\n\nThis means you can debug and monitor Pydantic AI with any OpenTelemetry backend.\n\nPydantic AI follows the [OpenTelemetry Semantic Conventions for Generative AI systems](https://opentelemetry.io/docs/specs/semconv/gen-ai/), so while we think you'll have the best experience using the Logfire platform :wink:, you should be able to use any OTel service with GenAI support.", "url": "https://ai.pydantic.dev/docs/logfire/#using-opentelemetry", "page": "docs/logfire", "source_site": "pydantic_ai"}
{"title": "Logfire with an alternative OTel backend", "anchor": "logfire-with-an-alternative-otel-backend", "heading_level": 3, "md_text": "You can use the Logfire SDK completely freely and send the data to any OpenTelemetry backend.\n\nHere's an example of configuring the Logfire library to send data to the excellent [otel-tui](https://github.com/ymtdzzz/otel-tui) \u2014 an open source terminal based OTel backend and viewer (no association with Pydantic Validation).\n\nRun `otel-tui` with docker (see [the otel-tui readme](https://github.com/ymtdzzz/otel-tui) for more instructions):\n\n```txt title=\"Terminal\"\ndocker run --rm -it -p 4318:4318 --name otel-tui ymtdzzz/otel-tui:latest\n```\n\nthen run,\n\n```python {title=\"otel_tui.py\" hl_lines=\"7 8\" test=\"skip\"}\nimport os\n\nimport logfire\n\nfrom pydantic_ai import Agent\n\nos.environ['OTEL_EXPORTER_OTLP_ENDPOINT'] = 'http://localhost:4318'  # (1)!\nlogfire.configure(send_to_logfire=False)  # (2)!\nlogfire.instrument_pydantic_ai()\nlogfire.instrument_httpx(capture_all=True)\n\nagent = Agent('openai:gpt-5')\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> Paris\n```\n\n1. Set the `OTEL_EXPORTER_OTLP_ENDPOINT` environment variable to the URL of your OpenTelemetry backend. If you're using a backend that requires authentication, you may need to set [other environment variables](https://opentelemetry.io/docs/languages/sdk-configuration/otlp-exporter/). Of course, these can also be set outside the process, e.g. with `export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318`.\n2. We [configure][logfire.configure] Logfire to disable sending data to the Logfire OTel backend itself. If you removed `send_to_logfire=False`, data would be sent to both Logfire and your OpenTelemetry backend.\n\nRunning the above code will send tracing data to `otel-tui`, which will display like this:\n\n![otel tui simple](img/otel-tui-simple.png)\n\nRunning the [weather agent](examples/weather-agent.md) example connected to `otel-tui` shows how it can be used to visualise a more complex trace:\n\n![otel tui weather agent](img/otel-tui-weather.png)\n\nFor more information on using the Logfire SDK to send data to alternative backends, see\n[the Logfire documentation](https://logfire.pydantic.dev/docs/how-to-guides/alternative-backends/).", "url": "https://ai.pydantic.dev/docs/logfire/#logfire-with-an-alternative-otel-backend", "page": "docs/logfire", "source_site": "pydantic_ai"}
{"title": "OTel without Logfire", "anchor": "otel-without-logfire", "heading_level": 3, "md_text": "You can also emit OpenTelemetry data from Pydantic AI without using Logfire at all.\n\nTo do this, you'll need to install and configure the OpenTelemetry packages you need. To run the following examples, use\n\n```txt title=\"Terminal\"\nuv run \\\n  --with 'pydantic-ai-slim[openai]' \\\n  --with opentelemetry-sdk \\\n  --with opentelemetry-exporter-otlp \\\n  raw_otel.py\n```\n\n```python {title=\"raw_otel.py\" test=\"skip\"}\nimport os\n\nfrom opentelemetry.exporter.otlp.proto.http.trace_exporter import OTLPSpanExporter\nfrom opentelemetry.sdk.trace import TracerProvider\nfrom opentelemetry.sdk.trace.export import BatchSpanProcessor\nfrom opentelemetry.trace import set_tracer_provider\n\nfrom pydantic_ai import Agent\n\nos.environ['OTEL_EXPORTER_OTLP_ENDPOINT'] = 'http://localhost:4318'\nexporter = OTLPSpanExporter()\nspan_processor = BatchSpanProcessor(exporter)\ntracer_provider = TracerProvider()\ntracer_provider.add_span_processor(span_processor)\n\nset_tracer_provider(tracer_provider)\n\nAgent.instrument_all()\nagent = Agent('openai:gpt-5')\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> Paris\n```", "url": "https://ai.pydantic.dev/docs/logfire/#otel-without-logfire", "page": "docs/logfire", "source_site": "pydantic_ai"}
{"title": "Alternative Observability backends", "anchor": "alternative-observability-backends", "heading_level": 3, "md_text": "Because Pydantic AI uses OpenTelemetry for observability, you can easily configure it to send data to any OpenTelemetry-compatible backend, not just our observability platform [Pydantic Logfire](#pydantic-logfire).\n\nThe following providers have dedicated documentation on Pydantic AI:\n\n<!--Feel free to add other platforms here. They MUST be added to the bottom of the list, and may only be a name with link.-->\n\n- [Langfuse](https://langfuse.com/docs/integrations/pydantic-ai)\n- [W&B Weave](https://weave-docs.wandb.ai/guides/integrations/pydantic_ai/)\n- [Arize](https://arize.com/docs/ax/observe/tracing-integrations-auto/pydantic-ai)\n- [Openlayer](https://www.openlayer.com/docs/integrations/pydantic-ai)\n- [OpenLIT](https://docs.openlit.io/latest/integrations/pydantic)\n- [LangWatch](https://docs.langwatch.ai/integration/python/integrations/pydantic-ai)\n- [Patronus AI](https://docs.patronus.ai/docs/percival/pydantic)\n- [Opik](https://www.comet.com/docs/opik/tracing/integrations/pydantic-ai)\n- [mlflow](https://mlflow.org/docs/latest/genai/tracing/integrations/listing/pydantic_ai)\n- [Agenta](https://docs.agenta.ai/observability/integrations/pydanticai)\n- [Confident AI](https://documentation.confident-ai.com/docs/llm-tracing/integrations/pydanticai)\n- [LangWatch](https://docs.langwatch.ai/integration/python/integrations/pydantic-ai)", "url": "https://ai.pydantic.dev/docs/logfire/#alternative-observability-backends", "page": "docs/logfire", "source_site": "pydantic_ai"}
{"title": "Configuring data format", "anchor": "configuring-data-format", "heading_level": 3, "md_text": "Pydantic AI follows the [OpenTelemetry Semantic Conventions for Generative AI systems](https://opentelemetry.io/docs/specs/semconv/gen-ai/). Specifically, it follows version 1.37.0 of the conventions by default, with a few exceptions. Certain span and attribute names are not spec compliant by default for compatibility reasons, but can be made compliant by passing [`InstrumentationSettings(version=3)`][pydantic_ai.models.instrumented.InstrumentationSettings] (the default is currently `version=2`). This will change the following:\n\n- The span name `agent run` becomes `invoke_agent {gen_ai.agent.name}` (with the agent name filled in)\n- The span name `running tool` becomes `execute_tool {gen_ai.tool.name}` (with the tool name filled in)\n- The attribute name `tool_arguments` becomes `gen_ai.tool.call.arguments`\n- The attribute name `tool_response` becomes `gen_ai.tool.call.result`\n\nTo use [OpenTelemetry semantic conventions version 1.36.0](https://github.com/open-telemetry/semantic-conventions/blob/v1.36.0/docs/gen-ai/README.md) or older, pass [`InstrumentationSettings(version=1)`][pydantic_ai.models.instrumented.InstrumentationSettings]. Moreover, those semantic conventions specify that messages should be captured as individual events (logs) that are children of the request span, whereas by default, Pydantic AI instead collects these events into a JSON array which is set as a single large attribute called `events` on the request span. To change this, use `event_mode='logs'`:\n\n```python {title=\"instrumentation_settings_event_mode.py\"}\nimport logfire\n\nfrom pydantic_ai import Agent\n\nlogfire.configure()\nlogfire.instrument_pydantic_ai(version=1, event_mode='logs')\nagent = Agent('openai:gpt-5')\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n```\n\nThis won't look as good in the Logfire UI, and will also be removed from Pydantic AI in a future release, but may be useful for backwards compatibility.\n\nNote that the OpenTelemetry Semantic Conventions are still experimental and are likely to change.", "url": "https://ai.pydantic.dev/docs/logfire/#configuring-data-format", "page": "docs/logfire", "source_site": "pydantic_ai"}
{"title": "Setting OpenTelemetry SDK providers", "anchor": "setting-opentelemetry-sdk-providers", "heading_level": 3, "md_text": "By default, the global `TracerProvider` and `EventLoggerProvider` are used. These are set automatically by `logfire.configure()`. They can also be set by the `set_tracer_provider` and `set_event_logger_provider` functions in the OpenTelemetry Python SDK. You can set custom providers with [`InstrumentationSettings`][pydantic_ai.models.instrumented.InstrumentationSettings].\n\n```python {title=\"instrumentation_settings_providers.py\"}\nfrom opentelemetry.sdk._events import EventLoggerProvider\nfrom opentelemetry.sdk.trace import TracerProvider\n\nfrom pydantic_ai import Agent, InstrumentationSettings\n\ninstrumentation_settings = InstrumentationSettings(\n    tracer_provider=TracerProvider(),\n    event_logger_provider=EventLoggerProvider(),\n)\n\nagent = Agent('openai:gpt-5', instrument=instrumentation_settings)", "url": "https://ai.pydantic.dev/docs/logfire/#setting-opentelemetry-sdk-providers", "page": "docs/logfire", "source_site": "pydantic_ai"}
{"title": "or to instrument all agents:", "anchor": "or-to-instrument-all-agents", "heading_level": 1, "md_text": "Agent.instrument_all(instrumentation_settings)\n```", "url": "https://ai.pydantic.dev/docs/logfire/#or-to-instrument-all-agents", "page": "docs/logfire", "source_site": "pydantic_ai"}
{"title": "Instrumenting a specific `Model`", "anchor": "instrumenting-a-specific-model", "heading_level": 3, "md_text": "```python {title=\"instrumented_model_example.py\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.instrumented import InstrumentationSettings, InstrumentedModel\n\nsettings = InstrumentationSettings()\nmodel = InstrumentedModel('openai:gpt-5', settings)\nagent = Agent(model)\n```", "url": "https://ai.pydantic.dev/docs/logfire/#instrumenting-a-specific-model", "page": "docs/logfire", "source_site": "pydantic_ai"}
{"title": "Excluding binary content", "anchor": "excluding-binary-content", "heading_level": 3, "md_text": "```python {title=\"excluding_binary_content.py\"}\nfrom pydantic_ai import Agent, InstrumentationSettings\n\ninstrumentation_settings = InstrumentationSettings(include_binary_content=False)\n\nagent = Agent('openai:gpt-5', instrument=instrumentation_settings)", "url": "https://ai.pydantic.dev/docs/logfire/#excluding-binary-content", "page": "docs/logfire", "source_site": "pydantic_ai"}
{"title": "or to instrument all agents:", "anchor": "or-to-instrument-all-agents", "heading_level": 1, "md_text": "Agent.instrument_all(instrumentation_settings)\n```", "url": "https://ai.pydantic.dev/docs/logfire/#or-to-instrument-all-agents", "page": "docs/logfire", "source_site": "pydantic_ai"}
{"title": "Excluding prompts and completions", "anchor": "excluding-prompts-and-completions", "heading_level": 3, "md_text": "For privacy and security reasons, you may want to monitor your agent's behavior and performance without exposing sensitive user data or proprietary prompts in your observability platform. Pydantic AI allows you to exclude the actual content from instrumentation events while preserving the structural information needed for debugging and monitoring.\n\nWhen `include_content=False` is set, Pydantic AI will exclude sensitive content from OpenTelemetry events, including user prompts and model completions, tool call arguments and responses, and any other message content.\n\n```python {title=\"excluding_sensitive_content.py\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.instrumented import InstrumentationSettings\n\ninstrumentation_settings = InstrumentationSettings(include_content=False)\n\nagent = Agent('openai:gpt-5', instrument=instrumentation_settings)", "url": "https://ai.pydantic.dev/docs/logfire/#excluding-prompts-and-completions", "page": "docs/logfire", "source_site": "pydantic_ai"}
{"title": "or to instrument all agents:", "anchor": "or-to-instrument-all-agents", "heading_level": 1, "md_text": "Agent.instrument_all(instrumentation_settings)\n```\n\nThis setting is particularly useful in production environments where compliance requirements or data sensitivity concerns make it necessary to limit what content is sent to your observability platform.", "url": "https://ai.pydantic.dev/docs/logfire/#or-to-instrument-all-agents", "page": "docs/logfire", "source_site": "pydantic_ai"}
{"title": "Getting Help", "anchor": "getting-help", "heading_level": 1, "md_text": "If you need help getting started with Pydantic AI or with advanced usage, the following sources may be useful.", "url": "https://ai.pydantic.dev/docs/help/#getting-help", "page": "docs/help", "source_site": "pydantic_ai"}
{"title": ":simple-slack: Slack", "anchor": "simple-slack-slack", "heading_level": 2, "md_text": "Join the `#pydantic-ai` channel in the [Pydantic Slack][slack] to ask questions, get help, and chat about Pydantic AI. There's also channels for Pydantic, Logfire, and FastUI.\n\nIf you're on a [Logfire][logfire] Pro plan, you can also get a dedicated private slack collab channel with us.", "url": "https://ai.pydantic.dev/docs/help/#simple-slack-slack", "page": "docs/help", "source_site": "pydantic_ai"}
{"title": ":simple-github: GitHub Issues", "anchor": "simple-github-github-issues", "heading_level": 2, "md_text": "The [Pydantic AI GitHub Issues][github-issues] are a great place to ask questions and give us feedback.\n\n[slack]: https://logfire.pydantic.dev/docs/join-slack/\n[github-issues]: https://github.com/pydantic/pydantic-ai/issues\n[logfire]: https://pydantic.dev/logfire", "url": "https://ai.pydantic.dev/docs/help/#simple-github-github-issues", "page": "docs/help", "source_site": "pydantic_ai"}
{"title": "Direct Model Requests", "anchor": "direct-model-requests", "heading_level": 1, "md_text": "The `direct` module provides low-level methods for making imperative requests to LLMs where the only abstraction is input and output schema translation, enabling you to use all models with the same API.\n\nThese methods are thin wrappers around the [`Model`][pydantic_ai.models.Model] implementations, offering a simpler interface when you don't need the full functionality of an [`Agent`][pydantic_ai.Agent].\n\nThe following functions are available:\n\n- [`model_request`][pydantic_ai.direct.model_request]: Make a non-streamed async request to a model\n- [`model_request_sync`][pydantic_ai.direct.model_request_sync]: Make a non-streamed synchronous request to a model\n- [`model_request_stream`][pydantic_ai.direct.model_request_stream]: Make a streamed async request to a model\n- [`model_request_stream_sync`][pydantic_ai.direct.model_request_stream_sync]: Make a streamed sync request to a model", "url": "https://ai.pydantic.dev/docs/direct/#direct-model-requests", "page": "docs/direct", "source_site": "pydantic_ai"}
{"title": "Basic Example", "anchor": "basic-example", "heading_level": 2, "md_text": "Here's a simple example demonstrating how to use the direct API to make a basic request:\n\n```python title=\"direct_basic.py\"\nfrom pydantic_ai import ModelRequest\nfrom pydantic_ai.direct import model_request_sync", "url": "https://ai.pydantic.dev/docs/direct/#basic-example", "page": "docs/direct", "source_site": "pydantic_ai"}
{"title": "Make a synchronous request to the model", "anchor": "make-a-synchronous-request-to-the-model", "heading_level": 1, "md_text": "model_response = model_request_sync(\n    'anthropic:claude-haiku-4-5',\n    [ModelRequest.user_text_prompt('What is the capital of France?')]\n)\n\nprint(model_response.parts[0].content)\n#> The capital of France is Paris.\nprint(model_response.usage)\n#> RequestUsage(input_tokens=56, output_tokens=7)\n```\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/direct/#make-a-synchronous-request-to-the-model", "page": "docs/direct", "source_site": "pydantic_ai"}
{"title": "Advanced Example with Tool Calling", "anchor": "advanced-example-with-tool-calling", "heading_level": 2, "md_text": "You can also use the direct API to work with function/tool calling.\n\nEven here we can use Pydantic to generate the JSON schema for the tool:\n\n```python\nfrom typing import Literal\n\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import ModelRequest, ToolDefinition\nfrom pydantic_ai.direct import model_request\nfrom pydantic_ai.models import ModelRequestParameters\n\n\nclass Divide(BaseModel):\n    \"\"\"Divide two numbers.\"\"\"\n\n    numerator: float\n    denominator: float\n    on_inf: Literal['error', 'infinity'] = 'infinity'\n\n\nasync def main():\n    # Make a request to the model with tool access\n    model_response = await model_request(\n        'openai:gpt-5-nano',\n        [ModelRequest.user_text_prompt('What is 123 / 456?')],\n        model_request_parameters=ModelRequestParameters(\n            function_tools=[\n                ToolDefinition(\n                    name=Divide.__name__.lower(),\n                    description=Divide.__doc__,\n                    parameters_json_schema=Divide.model_json_schema(),\n                )\n            ],\n            allow_text_output=True,  # Allow model to either use tools or respond directly\n        ),\n    )\n    print(model_response)\n    \"\"\"\n    ModelResponse(\n        parts=[\n            ToolCallPart(\n                tool_name='divide',\n                args={'numerator': '123', 'denominator': '456'},\n                tool_call_id='pyd_ai_2e0e396768a14fe482df90a29a78dc7b',\n            )\n        ],\n        usage=RequestUsage(input_tokens=55, output_tokens=7),\n        model_name='gpt-5-nano',\n        timestamp=datetime.datetime(...),\n    )\n    \"\"\"\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/direct/#advanced-example-with-tool-calling", "page": "docs/direct", "source_site": "pydantic_ai"}
{"title": "When to Use the direct API vs Agent", "anchor": "when-to-use-the-direct-api-vs-agent", "heading_level": 2, "md_text": "The direct API is ideal when:\n\n1. You need more direct control over model interactions\n2. You want to implement custom behavior around model requests\n3. You're building your own abstractions on top of model interactions\n\nFor most application use cases, the higher-level [`Agent`][pydantic_ai.Agent] API provides a more convenient interface with additional features such as built-in tool execution, retrying, structured output parsing, and more.", "url": "https://ai.pydantic.dev/docs/direct/#when-to-use-the-direct-api-vs-agent", "page": "docs/direct", "source_site": "pydantic_ai"}
{"title": "OpenTelemetry or Logfire Instrumentation", "anchor": "opentelemetry-or-logfire-instrumentation", "heading_level": 2, "md_text": "As with [agents][pydantic_ai.Agent], you can enable OpenTelemetry/Logfire instrumentation with just a few extra lines\n\n```python {title=\"direct_instrumented.py\" hl_lines=\"1 6 7\"}\nimport logfire\n\nfrom pydantic_ai import ModelRequest\nfrom pydantic_ai.direct import model_request_sync\n\nlogfire.configure()\nlogfire.instrument_pydantic_ai()", "url": "https://ai.pydantic.dev/docs/direct/#opentelemetry-or-logfire-instrumentation", "page": "docs/direct", "source_site": "pydantic_ai"}
{"title": "Make a synchronous request to the model", "anchor": "make-a-synchronous-request-to-the-model", "heading_level": 1, "md_text": "model_response = model_request_sync(\n    'anthropic:claude-haiku-4-5',\n    [ModelRequest.user_text_prompt('What is the capital of France?')],\n)\n\nprint(model_response.parts[0].content)\n#> The capital of France is Paris.\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nYou can also enable OpenTelemetry on a per call basis:\n\n```python {title=\"direct_instrumented.py\" hl_lines=\"1 6 12\"}\nimport logfire\n\nfrom pydantic_ai import ModelRequest\nfrom pydantic_ai.direct import model_request_sync\n\nlogfire.configure()", "url": "https://ai.pydantic.dev/docs/direct/#make-a-synchronous-request-to-the-model", "page": "docs/direct", "source_site": "pydantic_ai"}
{"title": "Make a synchronous request to the model", "anchor": "make-a-synchronous-request-to-the-model", "heading_level": 1, "md_text": "model_response = model_request_sync(\n    'anthropic:claude-haiku-4-5',\n    [ModelRequest.user_text_prompt('What is the capital of France?')],\n    instrument=True\n)\n\nprint(model_response.parts[0].content)\n#> The capital of France is Paris.\n```\n\nSee [Debugging and Monitoring](logfire.md) for more details, including how to instrument with plain OpenTelemetry without Logfire.", "url": "https://ai.pydantic.dev/docs/direct/#make-a-synchronous-request-to-the-model", "page": "docs/direct", "source_site": "pydantic_ai"}
{"title": "Function Tools", "anchor": "function-tools", "heading_level": 1, "md_text": "Function tools provide a mechanism for models to perform actions and retrieve extra information to help them generate a response.\n\nThey're useful when you want to enable the model to take some action and use the result, when it is impractical or impossible to put all the context an agent might need into the instructions, or when you want to make agents' behavior more deterministic or reliable by deferring some of the logic required to generate a response to another (not necessarily AI-powered) tool.\n\nIf you want a model to be able to call a function as its final action, without the result being sent back to the model, you can use an [output function](output.md#output-functions) instead.\n\nThere are a number of ways to register tools with an agent:\n\n- via the [`@agent.tool`][pydantic_ai.Agent.tool] decorator \u2014 for tools that need access to the agent [context][pydantic_ai.tools.RunContext]\n- via the [`@agent.tool_plain`][pydantic_ai.Agent.tool_plain] decorator \u2014 for tools that do not need access to the agent [context][pydantic_ai.tools.RunContext]\n- via the [`tools`][pydantic_ai.Agent.__init__] keyword argument to `Agent` which can take either plain functions, or instances of [`Tool`][pydantic_ai.tools.Tool]\n\nFor more advanced use cases, the [toolsets](toolsets.md) feature lets you manage collections of tools (built by you or provided by an [MCP server](mcp/client.md) or other [third party](third-party-tools.md#third-party-tools)) and register them with an agent in one go via the [`toolsets`][pydantic_ai.Agent.__init__] keyword argument to `Agent`. Internally, all `tools` and `toolsets` are gathered into a single [combined toolset](toolsets.md#combining-toolsets) that's made available to the model.\n\n!!! info \"Function tools vs. RAG\"\n    Function tools are basically the \"R\" of RAG (Retrieval-Augmented Generation) \u2014 they augment what the model can do by letting it request extra information.\n\n    The main semantic difference between Pydantic AI Tools and RAG is RAG is synonymous with vector search, while Pydantic AI tools are more general-purpose. (Note: we may add support for vector search functionality in the future, particularly an API for generating embeddings. See [#58](https://github.com/pydantic/pydantic-ai/issues/58))\n\n!!! info \"Function Tools vs. Structured Outputs\"\n    As the name suggests, function tools use the model's \"tools\" or \"functions\" API to let the model know what is available to call. Tools or functions are also used to define the schema(s) for [structured output](output.md) when using the default [tool output mode](output.md#tool-output), thus a model might have access to many tools, some of which call function tools while others end the run and produce a final output.", "url": "https://ai.pydantic.dev/docs/tools/#function-tools", "page": "docs/tools", "source_site": "pydantic_ai"}
{"title": "Registering via Decorator {#registering-function-tools-via-decorator}", "anchor": "registering-via-decorator-registering-function-tools-via-decorator", "heading_level": 2, "md_text": "`@agent.tool` is considered the default decorator since in the majority of cases tools will need access to the agent [context][pydantic_ai.tools.RunContext].\n\nHere's an example using both:\n\n```python {title=\"dice_game.py\"}\nimport random\n\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent(\n    'google-gla:gemini-2.5-flash',  # (1)!\n    deps_type=str,  # (2)!\n    system_prompt=(\n        \"You're a dice game, you should roll the die and see if the number \"\n        \"you get back matches the user's guess. If so, tell them they're a winner. \"\n        \"Use the player's name in the response.\"\n    ),\n)\n\n\n@agent.tool_plain  # (3)!\ndef roll_dice() -> str:\n    \"\"\"Roll a six-sided die and return the result.\"\"\"\n    return str(random.randint(1, 6))\n\n\n@agent.tool  # (4)!\ndef get_player_name(ctx: RunContext[str]) -> str:\n    \"\"\"Get the player's name.\"\"\"\n    return ctx.deps\n\n\ndice_result = agent.run_sync('My guess is 4', deps='Anne')  # (5)!\nprint(dice_result.output)\n#> Congratulations Anne, you guessed correctly! You're a winner!\n```\n\n1. This is a pretty simple task, so we can use the fast and cheap Gemini flash model.\n2. We pass the user's name as the dependency, to keep things simple we use just the name as a string as the dependency.\n3. This tool doesn't need any context, it just returns a random number. You could probably use dynamic instructions in this case.\n4. This tool needs the player's name, so it uses `RunContext` to access dependencies which are just the player's name in this case.\n5. Run the agent, passing the player's name as the dependency.\n\n_(This example is complete, it can be run \"as is\")_\n\nLet's print the messages from that game to see what happened:\n\n```python {title=\"dice_game_messages.py\" requires=\"dice_game.py\"}\nfrom dice_game import dice_result\n\nprint(dice_result.all_messages())\n\"\"\"\n[\n    ModelRequest(\n        parts=[\n            SystemPromptPart(\n                content=\"You're a dice game, you should roll the die and see if the number you get back matches the user's guess. If so, tell them they're a winner. Use the player's name in the response.\",\n                timestamp=datetime.datetime(...),\n            ),\n            UserPromptPart(\n                content='My guess is 4',\n                timestamp=datetime.datetime(...),\n            ),\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            ToolCallPart(\n                tool_name='roll_dice', args={}, tool_call_id='pyd_ai_tool_call_id'\n            )\n        ],\n        usage=RequestUsage(input_tokens=90, output_tokens=2),\n        model_name='gemini-2.5-flash',\n        timestamp=datetime.datetime(...),\n    ),\n    ModelRequest(\n        parts=[\n            ToolReturnPart(\n                tool_name='roll_dice',\n                content='4',\n                tool_call_id='pyd_ai_tool_call_id',\n                timestamp=datetime.datetime(...),\n            )\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            ToolCallPart(\n                tool_name='get_player_name', args={}, tool_call_id='pyd_ai_tool_call_id'\n            )\n        ],\n        usage=RequestUsage(input_tokens=91, output_tokens=4),\n        model_name='gemini-2.5-flash',\n        timestamp=datetime.datetime(...),\n    ),\n    ModelRequest(\n        parts=[\n            ToolReturnPart(\n                tool_name='get_player_name',\n                content='Anne',\n                tool_call_id='pyd_ai_tool_call_id',\n                timestamp=datetime.datetime(...),\n            )\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content=\"Congratulations Anne, you guessed correctly! You're a winner!\"\n            )\n        ],\n        usage=RequestUsage(input_tokens=92, output_tokens=12),\n        model_name='gemini-2.5-flash',\n        timestamp=datetime.datetime(...),\n    ),\n]\n\"\"\"\n```\n\nWe can represent this with a diagram:\n\n```mermaid\nsequenceDiagram\n    participant Agent\n    participant LLM\n\n    Note over Agent: Send prompts\n    Agent ->> LLM: System: \"You're a dice game...\"<br>User: \"My guess is 4\"\n    activate LLM\n    Note over LLM: LLM decides to use<br>a tool\n\n    LLM ->> Agent: Call tool<br>roll_dice()\n    deactivate LLM\n    activate Agent\n    Note over Agent: Rolls a six-sided die\n\n    Agent -->> LLM: ToolReturn<br>\"4\"\n    deactivate Agent\n    activate LLM\n    Note over LLM: LLM decides to use<br>another tool\n\n    LLM ->> Agent: Call tool<br>get_player_name()\n    deactivate LLM\n    activate Agent\n    Note over Agent: Retrieves player name\n    Agent -->> LLM: ToolReturn<br>\"Anne\"\n    deactivate Agent\n    activate LLM\n    Note over LLM: LLM constructs final response\n\n    LLM ->> Agent: ModelResponse<br>\"Congratulations Anne, ...\"\n    deactivate LLM\n    Note over Agent: Game session complete\n```", "url": "https://ai.pydantic.dev/docs/tools/#registering-via-decorator-registering-function-tools-via-decorator", "page": "docs/tools", "source_site": "pydantic_ai"}
{"title": "Registering via Agent Argument {#registering-function-tools-via-agent-argument}", "anchor": "registering-via-agent-argument-registering-function-tools-via-agent-argument", "heading_level": 2, "md_text": "As well as using the decorators, we can register tools via the `tools` argument to the [`Agent` constructor][pydantic_ai.Agent.__init__]. This is useful when you want to reuse tools, and can also give more fine-grained control over the tools.\n\n```python {title=\"dice_game_tool_kwarg.py\"}\nimport random\n\nfrom pydantic_ai import Agent, RunContext, Tool\n\nsystem_prompt = \"\"\"\\\nYou're a dice game, you should roll the die and see if the number\nyou get back matches the user's guess. If so, tell them they're a winner.\nUse the player's name in the response.\n\"\"\"\n\n\ndef roll_dice() -> str:\n    \"\"\"Roll a six-sided die and return the result.\"\"\"\n    return str(random.randint(1, 6))\n\n\ndef get_player_name(ctx: RunContext[str]) -> str:\n    \"\"\"Get the player's name.\"\"\"\n    return ctx.deps\n\n\nagent_a = Agent(\n    'google-gla:gemini-2.5-flash',\n    deps_type=str,\n    tools=[roll_dice, get_player_name],  # (1)!\n    system_prompt=system_prompt,\n)\nagent_b = Agent(\n    'google-gla:gemini-2.5-flash',\n    deps_type=str,\n    tools=[  # (2)!\n        Tool(roll_dice, takes_ctx=False),\n        Tool(get_player_name, takes_ctx=True),\n    ],\n    system_prompt=system_prompt,\n)\n\ndice_result = {}\ndice_result['a'] = agent_a.run_sync('My guess is 6', deps='Yashar')\ndice_result['b'] = agent_b.run_sync('My guess is 4', deps='Anne')\nprint(dice_result['a'].output)\n#> Tough luck, Yashar, you rolled a 4. Better luck next time.\nprint(dice_result['b'].output)\n#> Congratulations Anne, you guessed correctly! You're a winner!\n```\n\n1. The simplest way to register tools via the `Agent` constructor is to pass a list of functions, the function signature is inspected to determine if the tool takes [`RunContext`][pydantic_ai.tools.RunContext].\n2. `agent_a` and `agent_b` are identical \u2014 but we can use [`Tool`][pydantic_ai.tools.Tool] to reuse tool definitions and give more fine-grained control over how tools are defined, e.g. setting their name or description, or using a custom [`prepare`](tools-advanced.md#tool-prepare) method.\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/tools/#registering-via-agent-argument-registering-function-tools-via-agent-argument", "page": "docs/tools", "source_site": "pydantic_ai"}
{"title": "Tool Output {#function-tool-output}", "anchor": "tool-output-function-tool-output", "heading_level": 2, "md_text": "Tools can return anything that Pydantic can serialize to JSON. For advanced output options including multi-modal content and metadata, see [Advanced Tool Features](tools-advanced.md#function-tool-output).", "url": "https://ai.pydantic.dev/docs/tools/#tool-output-function-tool-output", "page": "docs/tools", "source_site": "pydantic_ai"}
{"title": "Tool Schema {#function-tools-and-schema}", "anchor": "tool-schema-function-tools-and-schema", "heading_level": 2, "md_text": "Function parameters are extracted from the function signature, and all parameters except `RunContext` are used to build the schema for that tool call.\n\nEven better, Pydantic AI extracts the docstring from functions and (thanks to [griffe](https://mkdocstrings.github.io/griffe/)) extracts parameter descriptions from the docstring and adds them to the schema.\n\n[Griffe supports](https://mkdocstrings.github.io/griffe/reference/docstrings/#docstrings) extracting parameter descriptions from `google`, `numpy`, and `sphinx` style docstrings. Pydantic AI will infer the format to use based on the docstring, but you can explicitly set it using [`docstring_format`][pydantic_ai.tools.DocstringFormat]. You can also enforce parameter requirements by setting `require_parameter_descriptions=True`. This will raise a [`UserError`][pydantic_ai.exceptions.UserError] if a parameter description is missing.\n\nTo demonstrate a tool's schema, here we use [`FunctionModel`][pydantic_ai.models.function.FunctionModel] to print the schema a model would receive:\n\n```python {title=\"tool_schema.py\"}\nfrom pydantic_ai import Agent, ModelMessage, ModelResponse, TextPart\nfrom pydantic_ai.models.function import AgentInfo, FunctionModel\n\nagent = Agent()\n\n\n@agent.tool_plain(docstring_format='google', require_parameter_descriptions=True)\ndef foobar(a: int, b: str, c: dict[str, list[float]]) -> str:\n    \"\"\"Get me foobar.\n\n    Args:\n        a: apple pie\n        b: banana cake\n        c: carrot smoothie\n    \"\"\"\n    return f'{a} {b} {c}'\n\n\ndef print_schema(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:\n    tool = info.function_tools[0]\n    print(tool.description)\n    #> Get me foobar.\n    print(tool.parameters_json_schema)\n    \"\"\"\n    {\n        'additionalProperties': False,\n        'properties': {\n            'a': {'description': 'apple pie', 'type': 'integer'},\n            'b': {'description': 'banana cake', 'type': 'string'},\n            'c': {\n                'additionalProperties': {'items': {'type': 'number'}, 'type': 'array'},\n                'description': 'carrot smoothie',\n                'type': 'object',\n            },\n        },\n        'required': ['a', 'b', 'c'],\n        'type': 'object',\n    }\n    \"\"\"\n    return ModelResponse(parts=[TextPart('foobar')])\n\n\nagent.run_sync('hello', model=FunctionModel(print_schema))\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nIf a tool has a single parameter that can be represented as an object in JSON schema (e.g. dataclass, TypedDict, pydantic model), the schema for the tool is simplified to be just that object.\n\nHere's an example where we use [`TestModel.last_model_request_parameters`][pydantic_ai.models.test.TestModel.last_model_request_parameters] to inspect the tool schema that would be passed to the model.\n\n```python {title=\"single_parameter_tool.py\"}\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.test import TestModel\n\nagent = Agent()\n\n\nclass Foobar(BaseModel):\n    \"\"\"This is a Foobar\"\"\"\n\n    x: int\n    y: str\n    z: float = 3.14\n\n\n@agent.tool_plain\ndef foobar(f: Foobar) -> str:\n    return str(f)\n\n\ntest_model = TestModel()\nresult = agent.run_sync('hello', model=test_model)\nprint(result.output)\n#> {\"foobar\":\"x=0 y='a' z=3.14\"}\nprint(test_model.last_model_request_parameters.function_tools)\n\"\"\"\n[\n    ToolDefinition(\n        name='foobar',\n        parameters_json_schema={\n            'properties': {\n                'x': {'type': 'integer'},\n                'y': {'type': 'string'},\n                'z': {'default': 3.14, 'type': 'number'},\n            },\n            'required': ['x', 'y'],\n            'title': 'Foobar',\n            'type': 'object',\n        },\n        description='This is a Foobar',\n    )\n]\n\"\"\"\n```\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/tools/#tool-schema-function-tools-and-schema", "page": "docs/tools", "source_site": "pydantic_ai"}
{"title": "See Also", "anchor": "see-also", "heading_level": 2, "md_text": "For more tool features and integrations, see:\n\n- [Advanced Tool Features](tools-advanced.md) - Custom schemas, dynamic tools, tool execution and retries\n- [Toolsets](toolsets.md) - Managing collections of tools\n- [Builtin Tools](builtin-tools.md) - Native tools provided by LLM providers\n- [Common Tools](common-tools.md) - Ready-to-use tool implementations\n- [Third-Party Tools](third-party-tools.md) - Integrations with MCP, LangChain, ACI.dev and other tool libraries\n- [Deferred Tools](deferred-tools.md) - Tools requiring approval or external execution", "url": "https://ai.pydantic.dev/docs/tools/#see-also", "page": "docs/tools", "source_site": "pydantic_ai"}
{"title": "Installation", "anchor": "installation", "heading_level": 1, "md_text": "Pydantic AI is available on PyPI as [`pydantic-ai`](https://pypi.org/project/pydantic-ai/) so installation is as simple as:\n\n```bash\npip/uv-add pydantic-ai\n```\n\n(Requires Python 3.10+)\n\nThis installs the `pydantic_ai` package, core dependencies, and libraries required to use all the models included in Pydantic AI.\nIf you want to install only those dependencies required to use a specific model, you can install the [\"slim\"](#slim-install) version of Pydantic AI.", "url": "https://ai.pydantic.dev/docs/install/#installation", "page": "docs/install", "source_site": "pydantic_ai"}
{"title": "Use with Pydantic Logfire", "anchor": "use-with-pydantic-logfire", "heading_level": 2, "md_text": "Pydantic AI has an excellent (but completely optional) integration with [Pydantic Logfire](https://pydantic.dev/logfire) to help you view and understand agent runs.\n\nLogfire comes included with `pydantic-ai` (but not the [\"slim\" version](#slim-install)), so you can typically start using it immediately by following the [Logfire setup docs](logfire.md#using-logfire).", "url": "https://ai.pydantic.dev/docs/install/#use-with-pydantic-logfire", "page": "docs/install", "source_site": "pydantic_ai"}
{"title": "Running Examples", "anchor": "running-examples", "heading_level": 2, "md_text": "We distribute the [`pydantic_ai_examples`](https://github.com/pydantic/pydantic-ai/tree/main/examples/pydantic_ai_examples) directory as a separate PyPI package ([`pydantic-ai-examples`](https://pypi.org/project/pydantic-ai-examples/)) to make examples extremely easy to customize and run.\n\nTo install examples, use the `examples` optional group:\n\n```bash\npip/uv-add \"pydantic-ai[examples]\"\n```\n\nTo run the examples, follow instructions in the [examples docs](examples/setup.md).", "url": "https://ai.pydantic.dev/docs/install/#running-examples", "page": "docs/install", "source_site": "pydantic_ai"}
{"title": "Slim Install", "anchor": "slim-install", "heading_level": 2, "md_text": "If you know which model you're going to use and want to avoid installing superfluous packages, you can use the [`pydantic-ai-slim`](https://pypi.org/project/pydantic-ai-slim/) package.\nFor example, if you're using just [`OpenAIChatModel`][pydantic_ai.models.openai.OpenAIChatModel], you would run:\n\n```bash\npip/uv-add \"pydantic-ai-slim[openai]\"\n```\n\n`pydantic-ai-slim` has the following optional groups:", "url": "https://ai.pydantic.dev/docs/install/#slim-install", "page": "docs/install", "source_site": "pydantic_ai"}
{"title": "Slim Install", "anchor": "slim-install", "heading_level": 2, "md_text": "* `logfire` \u2014 installs [Pydantic Logfire](logfire.md) dependency `logfire` [PyPI \u2197](https://pypi.org/project/logfire){:target=\"_blank\"}\n* `evals` \u2014 installs [Pydantic Evals](evals.md) dependency `pydantic-evals` [PyPI \u2197](https://pypi.org/project/pydantic-evals){:target=\"_blank\"}\n* `openai` \u2014 installs [OpenAI Model](models/openai.md) dependency `openai` [PyPI \u2197](https://pypi.org/project/openai){:target=\"_blank\"}\n* `vertexai` \u2014 installs `GoogleVertexProvider` dependencies `google-auth` [PyPI \u2197](https://pypi.org/project/google-auth){:target=\"_blank\"} and `requests` [PyPI \u2197](https://pypi.org/project/requests){:target=\"_blank\"}\n* `google` \u2014 installs [Google Model](models/google.md) dependency `google-genai` [PyPI \u2197](https://pypi.org/project/google-genai){:target=\"_blank\"}\n* `anthropic` \u2014 installs [Anthropic Model](models/anthropic.md) dependency `anthropic` [PyPI \u2197](https://pypi.org/project/anthropic){:target=\"_blank\"}\n* `groq` \u2014 installs [Groq Model](models/groq.md) dependency `groq` [PyPI \u2197](https://pypi.org/project/groq){:target=\"_blank\"}\n* `mistral` \u2014 installs [Mistral Model](models/mistral.md) dependency `mistralai` [PyPI \u2197](https://pypi.org/project/mistralai){:target=\"_blank\"}\n* `cohere` - installs [Cohere Model](models/cohere.md) dependency `cohere` [PyPI \u2197](https://pypi.org/project/cohere){:target=\"_blank\"}\n* `bedrock` - installs [Bedrock Model](models/bedrock.md) dependency `boto3` [PyPI \u2197](https://pypi.org/project/boto3){:target=\"_blank\"}\n* `huggingface` - installs [Hugging Face Model](models/huggingface.md) dependency `huggingface-hub[inference]` [PyPI \u2197](https://pypi.org/project/huggingface-hub){:target=\"_blank\"}\n* `outlines-transformers` - installs [Outlines Model](models/outlines.md) dependency `outlines[transformers]` [PyPI \u2197](https://pypi.org/project/outlines){:target=\"_blank\"}\n* `outlines-llamacpp` - installs [Outlines Model](models/outlines.md) dependency `outlines[llamacpp]` [PyPI \u2197](https://pypi.org/project/outlines){:target=\"_blank\"}\n* `outlines-mlxlm` - installs [Outlines Model](models/outlines.md) dependency `outlines[mlxlm]` [PyPI \u2197](https://pypi.org/project/outlines){:target=\"_blank\"}\n* `outlines-sglang` - installs [Outlines Model](models/outlines.md) dependency `outlines[sglang]` [PyPI \u2197](https://pypi.org/project/outlines){:target=\"_blank\"}\n* `outlines-vllm-offline` - installs [Outlines Model](models/outlines.md) dependencies `outlines` [PyPI \u2197](https://pypi.org/project/outlines){:target=\"_blank\"} and `vllm` [PyPI \u2197](https://pypi.org/project/vllm){:target=\"_blank\"}\n* `duckduckgo` - installs [DuckDuckGo Search Tool](common-tools.md#duckduckgo-search-tool) dependency `ddgs` [PyPI \u2197](https://pypi.org/project/ddgs){:target=\"_blank\"}\n* `tavily` - installs [Tavily Search Tool](common-tools.md#tavily-search-tool) dependency `tavily-python` [PyPI \u2197](https://pypi.org/project/tavily-python){:target=\"_blank\"}\n* `cli` - installs [CLI](cli.md) dependencies `rich` [PyPI \u2197](https://pypi.org/project/rich){:target=\"_blank\"}, `prompt-toolkit` [PyPI \u2197](https://pypi.org/project/prompt-toolkit){:target=\"_blank\"}, and `argcomplete` [PyPI \u2197](https://pypi.org/project/argcomplete){:target=\"_blank\"}\n* `mcp` - installs [MCP](mcp/client.md) dependency `mcp` [PyPI \u2197](https://pypi.org/project/mcp){:target=\"_blank\"}\n* `fastmcp` - installs [FastMCP](mcp/fastmcp-client.md) dependency `fastmcp` [PyPI \u2197](https://pypi.org/project/fastmcp){:target=\"_blank\"}\n* `a2a` - installs [A2A](a2a.md) dependency `fasta2a` [PyPI \u2197](https://pypi.org/project/fasta2a){:target=\"_blank\"}\n* `ui` - installs [UI Event Streams](ui/overview.md) dependency `starlette` [PyPI \u2197](https://pypi.org/project/starlette){:target=\"_blank\"}\n* `ag-ui` - installs [AG-UI Event Stream Protocol](ui/ag-ui.md) dependencies `ag-ui-protocol` [PyPI \u2197](https://pypi.org/project/ag-ui-protocol){:target=\"_blank\"} and `starlette` [PyPI \u2197](https://pypi.org/project/starlette){:target=\"_blank\"}\n* `dbos` - installs [DBOS Durable Execution](durable_execution/dbos.md) dependency `dbos` [PyPI \u2197](https://pypi.org/project/dbos){:target=\"_blank\"}\n* `prefect` - installs [Prefect Durable Execution](durable_execution/prefect.md) dependency `prefect` [PyPI \u2197](https://pypi.org/project/prefect){:target=\"_blank\"}", "url": "https://ai.pydantic.dev/docs/install/#slim-install", "page": "docs/install", "source_site": "pydantic_ai"}
{"title": "Slim Install", "anchor": "slim-install", "heading_level": 2, "md_text": "You can also install dependencies for multiple models and use cases, for example:\n\n```bash\npip/uv-add \"pydantic-ai-slim[openai,google,logfire]\"\n```", "url": "https://ai.pydantic.dev/docs/install/#slim-install", "page": "docs/install", "source_site": "pydantic_ai"}
{"title": "contributing", "anchor": null, "heading_level": 0, "md_text": "We'd love you to contribute to Pydantic AI!", "url": "https://ai.pydantic.dev/docs/contributing/", "page": "docs/contributing", "source_site": "pydantic_ai"}
{"title": "Installation and Setup", "anchor": "installation-and-setup", "heading_level": 2, "md_text": "Clone your fork and cd into the repo directory\n\n```bash\ngit clone git@github.com:<your username>/pydantic-ai.git\ncd pydantic-ai\n```\n\nInstall `uv` (version 0.4.30 or later), `pre-commit` and `deno`:\n\n- [`uv` install docs](https://docs.astral.sh/uv/getting-started/installation/)\n- [`pre-commit` install docs](https://pre-commit.com/#install)\n- [`deno` install docs](https://docs.deno.com/runtime/getting_started/installation/)\n\nTo install `pre-commit` you can run the following command:\n\n```bash\nuv tool install pre-commit\n```\n\nFor `deno`, you can run the following, or check\n[their documentation](https://docs.deno.com/runtime/getting_started/installation/) for alternative\ninstallation methods:\n\n```bash\ncurl -fsSL https://deno.land/install.sh | sh\n```\n\nInstall `pydantic-ai`, all dependencies and pre-commit hooks\n\n```bash\nmake install\n```", "url": "https://ai.pydantic.dev/docs/contributing/#installation-and-setup", "page": "docs/contributing", "source_site": "pydantic_ai"}
{"title": "Running Tests etc.", "anchor": "running-tests-etc", "heading_level": 2, "md_text": "We use `make` to manage most commands you'll need to run.\n\nFor details on available commands, run:\n\n```bash\nmake help\n```\n\nTo run code formatting, linting, static type checks, and tests with coverage report generation, run:\n\n```bash\nmake\n```", "url": "https://ai.pydantic.dev/docs/contributing/#running-tests-etc", "page": "docs/contributing", "source_site": "pydantic_ai"}
{"title": "Documentation Changes", "anchor": "documentation-changes", "heading_level": 2, "md_text": "To run the documentation page locally, run:\n\n```bash\nuv run mkdocs serve\n```", "url": "https://ai.pydantic.dev/docs/contributing/#documentation-changes", "page": "docs/contributing", "source_site": "pydantic_ai"}
{"title": "Rules for adding new models to Pydantic AI {#new-model-rules}", "anchor": "rules-for-adding-new-models-to-pydantic-ai-new-model-rules", "heading_level": 2, "md_text": "To avoid an excessive workload for the maintainers of Pydantic AI, we can't accept all model contributions, so we're setting the following rules for when we'll accept new models and when we won't. This should hopefully reduce the chances of disappointment and wasted work.\n\n- To add a new model with an extra dependency, that dependency needs > 500k monthly downloads from PyPI consistently over 3 months or more\n- To add a new model which uses another models logic internally and has no extra dependencies, that model's GitHub org needs > 20k stars in total\n- For any other model that's just a custom URL and API key, we're happy to add a one-paragraph description with a link and instructions on the URL to use\n- For any other model that requires more logic, we recommend you release your own Python package `pydantic-ai-xxx`, which depends on [`pydantic-ai-slim`](install.md#slim-install) and implements a model that inherits from our [`Model`][pydantic_ai.models.Model] ABC\n\nIf you're unsure about adding a model, please [create an issue](https://github.com/pydantic/pydantic-ai/issues).", "url": "https://ai.pydantic.dev/docs/contributing/#rules-for-adding-new-models-to-pydantic-ai-new-model-rules", "page": "docs/contributing", "source_site": "pydantic_ai"}
{"title": "Pydantic AI {.hide}", "anchor": "pydantic-ai-hide", "heading_level": 1, "md_text": "--8<-- \"docs/.partials/index-header.html\"\n\nFastAPI revolutionized web development by offering an innovative and ergonomic design, built on the foundation of [Pydantic Validation](https://docs.pydantic.dev) and modern Python features like type hints.\n\nYet despite virtually every Python agent framework and LLM library using Pydantic Validation, when we began to use LLMs in [Pydantic Logfire](https://pydantic.dev/logfire), we couldn't find anything that gave us the same feeling.\n\nWe built Pydantic AI with one simple aim: to bring that FastAPI feeling to GenAI app and agent development.", "url": "https://ai.pydantic.dev/docs/#pydantic-ai-hide", "page": "docs/", "source_site": "pydantic_ai"}
{"title": "Why use Pydantic AI", "anchor": "why-use-pydantic-ai", "heading_level": 2, "md_text": "1. **Built by the Pydantic Team**:\n[Pydantic Validation](https://docs.pydantic.dev/latest/) is the validation layer of the OpenAI SDK, the Google ADK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more. _Why use the derivative when you can go straight to the source?_ :smiley:\n\n2. **Model-agnostic**:\nSupports virtually every [model](models/overview.md) and provider: OpenAI, Anthropic, Gemini, DeepSeek, Grok, Cohere, Mistral, and Perplexity; Azure AI Foundry, Amazon Bedrock, Google Vertex AI, Ollama, LiteLLM, Groq, OpenRouter, Together AI, Fireworks AI, Cerebras, Hugging Face, GitHub, Heroku, Vercel, Nebius, OVHcloud, and Outlines. If your favorite model or provider is not listed, you can easily implement a [custom model](models/overview.md#custom-models).\n\n3. **Seamless Observability**:\nTightly [integrates](logfire.md) with [Pydantic Logfire](https://pydantic.dev/logfire), our general-purpose OpenTelemetry observability platform, for real-time debugging, evals-based performance monitoring, and behavior, tracing, and cost tracking. If you already have an observability platform that supports OTel, you can [use that too](logfire.md#alternative-observability-backends).\n\n4. **Fully Type-safe**:\nDesigned to give your IDE or AI coding agent as much context as possible for auto-completion and [type checking](agents.md#static-type-checking), moving entire classes of errors from runtime to write-time for a bit of that Rust \"if it compiles, it works\" feel.\n\n5. **Powerful Evals**:\nEnables you to systematically test and [evaluate](evals.md) the performance and accuracy of the agentic systems you build, and monitor the performance over time in Pydantic Logfire.\n\n6. **MCP, A2A, and UI**:\nIntegrates the [Model Context Protocol](mcp/overview.md), [Agent2Agent](a2a.md), and various [UI event stream](ui/overview.md) standards to give your agent access to external tools and data, let it interoperate with other agents, and build interactive applications with streaming event-based communication.\n\n7. **Human-in-the-Loop Tool Approval**:\nEasily lets you flag that certain tool calls [require approval](deferred-tools.md#human-in-the-loop-tool-approval) before they can proceed, possibly depending on tool call arguments, conversation history, or user preferences.\n\n8. **Durable Execution**:\nEnables you to build [durable agents](durable_execution/overview.md) that can preserve their progress across transient API failures and application errors or restarts, and handle long-running, asynchronous, and human-in-the-loop workflows with production-grade reliability.\n\n9. **Streamed Outputs**:\nProvides the ability to [stream](output.md#streamed-results) structured output continuously, with immediate validation, ensuring real time access to generated data.\n\n10. **Graph Support**:\nProvides a powerful way to define [graphs](graph.md) using type hints, for use in complex applications where standard control flow can degrade to spaghetti code.\n\nRealistically though, no list is going to be as convincing as [giving it a try](#next-steps) and seeing how it makes you feel!\n\n**Sign up for our newsletter, *The Pydantic Stack*, with updates & tutorials on Pydantic AI, Logfire, and Pydantic:**\n\n  <form method=\"POST\" action=\"https://eu.customerioforms.com/forms/submit_action?site_id=53d2086c3c4214eaecaa&form_id=14b22611745b458&success_url=https://ai.pydantic.dev/\" class=\"md-typeset\" style=\"display: flex; align-items: center; gap: 0.5rem; width: 100%;\">\n      <input\n      type=\"email\"\n      id=\"email_input\"\n      name=\"email\"\n      class=\"md-input md-input--stretch\"\n      style=\"flex: 1; background: var(--md-default-bg-color); color: var(--md-default-fg-color);\"\n      required\n      placeholder=\"Email\"\n      data-1p-ignore\n      data-lpignore=\"true\"\n      data-protonpass-ignore=\"true\"\n      data-bwignore=\"true\"\n      />\n      <input type=\"hidden\" id=\"source_input\" name=\"source\" value=\"pydantic-ai\" />\n      <button type=\"submit\" class=\"md-button md-button--primary\">Subscribe</button>\n  </form>", "url": "https://ai.pydantic.dev/docs/#why-use-pydantic-ai", "page": "docs/", "source_site": "pydantic_ai"}
{"title": "Hello World Example", "anchor": "hello-world-example", "heading_level": 2, "md_text": "Here's a minimal example of Pydantic AI:\n\n```python {title=\"hello_world.py\"}\nfrom pydantic_ai import Agent\n\nagent = Agent(  # (1)!\n    'anthropic:claude-sonnet-4-0',\n    instructions='Be concise, reply with one sentence.',  # (2)!\n)\n\nresult = agent.run_sync('Where does \"hello world\" come from?')  # (3)!\nprint(result.output)\n\"\"\"\nThe first known use of \"hello, world\" was in a 1974 textbook about the C programming language.\n\"\"\"\n```\n\n1. We configure the agent to use [Anthropic's Claude Sonnet 4.0](api/models/anthropic.md) model, but you can also set the model when running the agent.\n2. Register static [instructions](agents.md#instructions) using a keyword argument to the agent.\n3. [Run the agent](agents.md#running-agents) synchronously, starting a conversation with the LLM.\n\n_(This example is complete, it can be run \"as is\", assuming you've [installed the `pydantic_ai` package](install.md))_\n\nThe exchange will be very short: Pydantic AI will send the instructions and the user prompt to the LLM, and the model will return a text response.\n\nNot very interesting yet, but we can easily add [tools](tools.md), [dynamic instructions](agents.md#instructions), and [structured outputs](output.md) to build more powerful agents.", "url": "https://ai.pydantic.dev/docs/#hello-world-example", "page": "docs/", "source_site": "pydantic_ai"}
{"title": "Tools & Dependency Injection Example", "anchor": "tools-dependency-injection-example", "heading_level": 2, "md_text": "Here is a concise example using Pydantic AI to build a support agent for a bank:\n\n```python {title=\"bank_support.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic import BaseModel, Field\nfrom pydantic_ai import Agent, RunContext\n\nfrom bank_database import DatabaseConn\n\n\n@dataclass\nclass SupportDependencies:  # (3)!\n    customer_id: int\n    db: DatabaseConn  # (12)!\n\n\nclass SupportOutput(BaseModel):  # (13)!\n    support_advice: str = Field(description='Advice returned to the customer')\n    block_card: bool = Field(description=\"Whether to block the customer's card\")\n    risk: int = Field(description='Risk level of query', ge=0, le=10)\n\n\nsupport_agent = Agent(  # (1)!\n    'openai:gpt-5',  # (2)!\n    deps_type=SupportDependencies,\n    output_type=SupportOutput,  # (9)!\n    instructions=(  # (4)!\n        'You are a support agent in our bank, give the '\n        'customer support and judge the risk level of their query.'\n    ),\n)\n\n\n@support_agent.instructions  # (5)!\nasync def add_customer_name(ctx: RunContext[SupportDependencies]) -> str:\n    customer_name = await ctx.deps.db.customer_name(id=ctx.deps.customer_id)\n    return f\"The customer's name is {customer_name!r}\"\n\n\n@support_agent.tool  # (6)!\nasync def customer_balance(\n    ctx: RunContext[SupportDependencies], include_pending: bool\n) -> float:\n    \"\"\"Returns the customer's current account balance.\"\"\"  # (7)!\n    return await ctx.deps.db.customer_balance(\n        id=ctx.deps.customer_id,\n        include_pending=include_pending,\n    )\n\n\n...  # (11)!\n\n\nasync def main():\n    deps = SupportDependencies(customer_id=123, db=DatabaseConn())\n    result = await support_agent.run('What is my balance?', deps=deps)  # (8)!\n    print(result.output)  # (10)!\n    \"\"\"\n    support_advice='Hello John, your current account balance, including pending transactions, is $123.45.' block_card=False risk=1\n    \"\"\"\n\n    result = await support_agent.run('I just lost my card!', deps=deps)\n    print(result.output)\n    \"\"\"\n    support_advice=\"I'm sorry to hear that, John. We are temporarily blocking your card to prevent unauthorized transactions.\" block_card=True risk=8\n    \"\"\"\n```", "url": "https://ai.pydantic.dev/docs/#tools-dependency-injection-example", "page": "docs/", "source_site": "pydantic_ai"}
{"title": "Tools & Dependency Injection Example", "anchor": "tools-dependency-injection-example", "heading_level": 2, "md_text": "1. This [agent](agents.md) will act as first-tier support in a bank. Agents are generic in the type of dependencies they accept and the type of output they return. In this case, the support agent has type `#!python Agent[SupportDependencies, SupportOutput]`.\n2. Here we configure the agent to use [OpenAI's GPT-5 model](api/models/openai.md), you can also set the model when running the agent.\n3. The `SupportDependencies` dataclass is used to pass data, connections, and logic into the model that will be needed when running [instructions](agents.md#instructions) and [tool](tools.md) functions. Pydantic AI's system of dependency injection provides a [type-safe](agents.md#static-type-checking) way to customise the behavior of your agents, and can be especially useful when running [unit tests](testing.md) and evals.\n4. Static [instructions](agents.md#instructions) can be registered with the [`instructions` keyword argument][pydantic_ai.Agent.__init__] to the agent.\n5. Dynamic [instructions](agents.md#instructions) can be registered with the [`@agent.instructions`][pydantic_ai.Agent.instructions] decorator, and can make use of dependency injection. Dependencies are carried via the [`RunContext`][pydantic_ai.tools.RunContext] argument, which is parameterized with the `deps_type` from above. If the type annotation here is wrong, static type checkers will catch it.\n6. The [`@agent.tool`](tools.md) decorator let you register functions which the LLM may call while responding to a user. Again, dependencies are carried via [`RunContext`][pydantic_ai.tools.RunContext], any other arguments become the tool schema passed to the LLM. Pydantic is used to validate these arguments, and errors are passed back to the LLM so it can retry.\n7. The docstring of a tool is also passed to the LLM as the description of the tool. Parameter descriptions are [extracted](tools.md#function-tools-and-schema) from the docstring and added to the parameter schema sent to the LLM.\n8. [Run the agent](agents.md#running-agents) asynchronously, conducting a conversation with the LLM until a final response is reached. Even in this fairly simple case, the agent will exchange multiple messages with the LLM as tools are called to retrieve an output.\n9. The response from the agent will be guaranteed to be a `SupportOutput`. If validation fails [reflection](agents.md#reflection-and-self-correction), the agent is prompted to try again.\n10. The output will be validated with Pydantic to guarantee it is a `SupportOutput`, since the agent is generic, it'll also be typed as a `SupportOutput` to aid with static type checking.\n11. In a real use case, you'd add more tools and longer instructions to the agent to extend the context it's equipped with and support it can provide.\n12. This is a simple sketch of a database connection, used to keep the example short and readable. In reality, you'd be connecting to an external database (e.g. PostgreSQL) to get information about customers.\n13. This [Pydantic](https://docs.pydantic.dev) model is used to constrain the structured data returned by the agent. From this simple definition, Pydantic builds the JSON Schema that tells the LLM how to return the data, and performs validation to guarantee the data is correct at the end of the run.\n\n!!! tip \"Complete `bank_support.py` example\"\n    The code included here is incomplete for the sake of brevity (the definition of `DatabaseConn` is missing); you can find the complete `bank_support.py` example [here](examples/bank-support.md).", "url": "https://ai.pydantic.dev/docs/#tools-dependency-injection-example", "page": "docs/", "source_site": "pydantic_ai"}
{"title": "Instrumentation with Pydantic Logfire", "anchor": "instrumentation-with-pydantic-logfire", "heading_level": 2, "md_text": "Even a simple agent with just a handful of tools can result in a lot of back-and-forth with the LLM, making it nearly impossible to be confident of what's going on just from reading the code.\nTo understand the flow of the above runs, we can watch the agent in action using Pydantic Logfire.\n\nTo do this, we need to [set up Logfire](logfire.md#using-logfire), and add the following to our code:\n\n```python {title=\"bank_support_with_logfire.py\" hl_lines=\"6-10\" test=\"skip\" lint=\"skip\"}\n...\nfrom pydantic_ai import Agent, RunContext\n\nfrom bank_database import DatabaseConn\n\nimport logfire\n\nlogfire.configure()  # (1)!\nlogfire.instrument_pydantic_ai()  # (2)!\nlogfire.instrument_asyncpg()  # (3)!\n\n...\n\nsupport_agent = Agent(\n    'openai:gpt-5',\n    deps_type=SupportDependencies,\n    output_type=SupportOutput,\n    system_prompt=(\n        'You are a support agent in our bank, give the '\n        'customer support and judge the risk level of their query.'\n    ),\n)\n```\n\n1. Configure the Logfire SDK, this will fail if project is not set up.\n2. This will instrument all Pydantic AI agents used from here on out. If you want to instrument only a specific agent, you can pass the [`instrument=True` keyword argument][pydantic_ai.Agent.__init__] to the agent.\n3. In our demo, `DatabaseConn` uses [`asyncpg`]() to connect to a PostgreSQL database, so [`logfire.instrument_asyncpg()`](https://magicstack.github.io/asyncpg/current/) is used to log the database queries.\n\nThat's enough to get the following view of your agent in action:\n\n{{ video('9078b98c4f75d01f912a0368bbbdb97a', 25, 55) }}\n\nSee [Monitoring and Performance](logfire.md) to learn more.", "url": "https://ai.pydantic.dev/docs/#instrumentation-with-pydantic-logfire", "page": "docs/", "source_site": "pydantic_ai"}
{"title": "`llms.txt`", "anchor": "llmstxt", "heading_level": 2, "md_text": "The Pydantic AI documentation is available in the [llms.txt](https://llmstxt.org/) format.\nThis format is defined in Markdown and suited for LLMs and AI coding assistants and agents.\n\nTwo formats are available:\n\n- [`llms.txt`](https://ai.pydantic.dev/llms.txt): a file containing a brief description\n  of the project, along with links to the different sections of the documentation. The structure\n  of this file is described in details [here](https://llmstxt.org/#format).\n- [`llms-full.txt`](https://ai.pydantic.dev/llms-full.txt): Similar to the `llms.txt` file,\n  but every link content is included. Note that this file may be too large for some LLMs.\n\nAs of today, these files are not automatically leveraged by IDEs or coding agents, but they will use it if you provide a link or the full text.", "url": "https://ai.pydantic.dev/docs/#llmstxt", "page": "docs/", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "To try Pydantic AI for yourself, [install it](install.md) and follow the instructions [in the examples](examples/setup.md).\n\nRead the [docs](agents.md) to learn more about building applications with Pydantic AI.\n\nRead the [API Reference](api/agent.md) to understand Pydantic AI's interface.\n\nJoin [:simple-slack: Slack](https://logfire.pydantic.dev/docs/join-slack/) or file an issue on [:simple-github: GitHub](https://github.com/pydantic/pydantic-ai/issues) if you have any questions.", "url": "https://ai.pydantic.dev/docs/#next-steps", "page": "docs/", "source_site": "pydantic_ai"}
{"title": "Messages and chat history", "anchor": "messages-and-chat-history", "heading_level": 1, "md_text": "Pydantic AI provides access to messages exchanged during an agent run. These messages can be used both to continue a coherent conversation, and to understand how an agent performed.", "url": "https://ai.pydantic.dev/docs/message-history/#messages-and-chat-history", "page": "docs/message-history", "source_site": "pydantic_ai"}
{"title": "Accessing Messages from Results", "anchor": "accessing-messages-from-results", "heading_level": 3, "md_text": "After running an agent, you can access the messages exchanged during that run from the `result` object.\n\nBoth [`RunResult`][pydantic_ai.agent.AgentRunResult]\n(returned by [`Agent.run`][pydantic_ai.agent.AbstractAgent.run], [`Agent.run_sync`][pydantic_ai.agent.AbstractAgent.run_sync])\nand [`StreamedRunResult`][pydantic_ai.result.StreamedRunResult] (returned by [`Agent.run_stream`][pydantic_ai.agent.AbstractAgent.run_stream]) have the following methods:\n\n- [`all_messages()`][pydantic_ai.agent.AgentRunResult.all_messages]: returns all messages, including messages from prior runs. There's also a variant that returns JSON bytes, [`all_messages_json()`][pydantic_ai.agent.AgentRunResult.all_messages_json].\n- [`new_messages()`][pydantic_ai.agent.AgentRunResult.new_messages]: returns only the messages from the current run. There's also a variant that returns JSON bytes, [`new_messages_json()`][pydantic_ai.agent.AgentRunResult.new_messages_json].\n\n!!! info \"StreamedRunResult and complete messages\"\n    On [`StreamedRunResult`][pydantic_ai.result.StreamedRunResult], the messages returned from these methods will only include the final result message once the stream has finished.\n\n    E.g. you've awaited one of the following coroutines:\n\n    * [`StreamedRunResult.stream_output()`][pydantic_ai.result.StreamedRunResult.stream_output]\n    * [`StreamedRunResult.stream_text()`][pydantic_ai.result.StreamedRunResult.stream_text]\n    * [`StreamedRunResult.stream_responses()`][pydantic_ai.result.StreamedRunResult.stream_responses]\n    * [`StreamedRunResult.get_output()`][pydantic_ai.result.StreamedRunResult.get_output]\n\n    **Note:** The final result message will NOT be added to result messages if you use [`.stream_text(delta=True)`][pydantic_ai.result.StreamedRunResult.stream_text] since in this case the result content is never built as one string.\n\nExample of accessing methods on a [`RunResult`][pydantic_ai.agent.AgentRunResult] :\n\n```python {title=\"run_result_messages.py\" hl_lines=\"10\"}\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-5', system_prompt='Be a helpful assistant.')\n\nresult = agent.run_sync('Tell me a joke.')\nprint(result.output)\n#> Did you hear about the toothpaste scandal? They called it Colgate.", "url": "https://ai.pydantic.dev/docs/message-history/#accessing-messages-from-results", "page": "docs/message-history", "source_site": "pydantic_ai"}
{"title": "all messages from the run", "anchor": "all-messages-from-the-run", "heading_level": 1, "md_text": "print(result.all_messages())\n\"\"\"\n[\n    ModelRequest(\n        parts=[\n            SystemPromptPart(\n                content='Be a helpful assistant.',\n                timestamp=datetime.datetime(...),\n            ),\n            UserPromptPart(\n                content='Tell me a joke.',\n                timestamp=datetime.datetime(...),\n            ),\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='Did you hear about the toothpaste scandal? They called it Colgate.'\n            )\n        ],\n        usage=RequestUsage(input_tokens=60, output_tokens=12),\n        model_name='gpt-5',\n        timestamp=datetime.datetime(...),\n    ),\n]\n\"\"\"\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nExample of accessing methods on a [`StreamedRunResult`][pydantic_ai.result.StreamedRunResult] :\n\n```python {title=\"streamed_run_result_messages.py\" hl_lines=\"9 40\"}\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-5', system_prompt='Be a helpful assistant.')\n\n\nasync def main():\n    async with agent.run_stream('Tell me a joke.') as result:\n        # incomplete messages before the stream finishes\n        print(result.all_messages())\n        \"\"\"\n        [\n            ModelRequest(\n                parts=[\n                    SystemPromptPart(\n                        content='Be a helpful assistant.',\n                        timestamp=datetime.datetime(...),\n                    ),\n                    UserPromptPart(\n                        content='Tell me a joke.',\n                        timestamp=datetime.datetime(...),\n                    ),\n                ]\n            )\n        ]\n        \"\"\"\n\n        async for text in result.stream_text():\n            print(text)\n            #> Did you hear\n            #> Did you hear about the toothpaste\n            #> Did you hear about the toothpaste scandal? They called\n            #> Did you hear about the toothpaste scandal? They called it Colgate.\n\n        # complete messages once the stream finishes\n        print(result.all_messages())\n        \"\"\"\n        [\n            ModelRequest(\n                parts=[\n                    SystemPromptPart(\n                        content='Be a helpful assistant.',\n                        timestamp=datetime.datetime(...),\n                    ),\n                    UserPromptPart(\n                        content='Tell me a joke.',\n                        timestamp=datetime.datetime(...),\n                    ),\n                ]\n            ),\n            ModelResponse(\n                parts=[\n                    TextPart(\n                        content='Did you hear about the toothpaste scandal? They called it Colgate.'\n                    )\n                ],\n                usage=RequestUsage(input_tokens=50, output_tokens=12),\n                model_name='gpt-5',\n                timestamp=datetime.datetime(...),\n            ),\n        ]\n        \"\"\"\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/message-history/#all-messages-from-the-run", "page": "docs/message-history", "source_site": "pydantic_ai"}
{"title": "Using Messages as Input for Further Agent Runs", "anchor": "using-messages-as-input-for-further-agent-runs", "heading_level": 3, "md_text": "The primary use of message histories in Pydantic AI is to maintain context across multiple agent runs.\n\nTo use existing messages in a run, pass them to the `message_history` parameter of\n[`Agent.run`][pydantic_ai.agent.AbstractAgent.run], [`Agent.run_sync`][pydantic_ai.agent.AbstractAgent.run_sync] or\n[`Agent.run_stream`][pydantic_ai.agent.AbstractAgent.run_stream].\n\nIf `message_history` is set and not empty, a new system prompt is not generated \u2014 we assume the existing message history includes a system prompt.\n\n```python {title=\"Reusing messages in a conversation\" hl_lines=\"9 13\"}\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-5', system_prompt='Be a helpful assistant.')\n\nresult1 = agent.run_sync('Tell me a joke.')\nprint(result1.output)\n#> Did you hear about the toothpaste scandal? They called it Colgate.\n\nresult2 = agent.run_sync('Explain?', message_history=result1.new_messages())\nprint(result2.output)\n#> This is an excellent joke invented by Samuel Colvin, it needs no explanation.\n\nprint(result2.all_messages())\n\"\"\"\n[\n    ModelRequest(\n        parts=[\n            SystemPromptPart(\n                content='Be a helpful assistant.',\n                timestamp=datetime.datetime(...),\n            ),\n            UserPromptPart(\n                content='Tell me a joke.',\n                timestamp=datetime.datetime(...),\n            ),\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='Did you hear about the toothpaste scandal? They called it Colgate.'\n            )\n        ],\n        usage=RequestUsage(input_tokens=60, output_tokens=12),\n        model_name='gpt-5',\n        timestamp=datetime.datetime(...),\n    ),\n    ModelRequest(\n        parts=[\n            UserPromptPart(\n                content='Explain?',\n                timestamp=datetime.datetime(...),\n            )\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='This is an excellent joke invented by Samuel Colvin, it needs no explanation.'\n            )\n        ],\n        usage=RequestUsage(input_tokens=61, output_tokens=26),\n        model_name='gpt-5',\n        timestamp=datetime.datetime(...),\n    ),\n]\n\"\"\"\n```\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/message-history/#using-messages-as-input-for-further-agent-runs", "page": "docs/message-history", "source_site": "pydantic_ai"}
{"title": "Storing and loading messages (to JSON)", "anchor": "storing-and-loading-messages-to-json", "heading_level": 2, "md_text": "While maintaining conversation state in memory is enough for many applications, often times you may want to store the messages history of an agent run on disk or in a database. This might be for evals, for sharing data between Python and JavaScript/TypeScript, or any number of other use cases.\n\nThe intended way to do this is using a `TypeAdapter`.\n\nWe export [`ModelMessagesTypeAdapter`][pydantic_ai.messages.ModelMessagesTypeAdapter] that can be used for this, or you can create your own.\n\nHere's an example showing how:\n\n```python {title=\"serialize messages to json\"}\nfrom pydantic_core import to_jsonable_python\n\nfrom pydantic_ai import (\n    Agent,\n    ModelMessagesTypeAdapter,  # (1)!\n)\n\nagent = Agent('openai:gpt-5', system_prompt='Be a helpful assistant.')\n\nresult1 = agent.run_sync('Tell me a joke.')\nhistory_step_1 = result1.all_messages()\nas_python_objects = to_jsonable_python(history_step_1)  # (2)!\nsame_history_as_step_1 = ModelMessagesTypeAdapter.validate_python(as_python_objects)\n\nresult2 = agent.run_sync(  # (3)!\n    'Tell me a different joke.', message_history=same_history_as_step_1\n)\n```\n\n1. Alternatively, you can create a `TypeAdapter` from scratch:\n   ```python {lint=\"skip\" format=\"skip\"}\n   from pydantic import TypeAdapter\n   from pydantic_ai import ModelMessage\n   ModelMessagesTypeAdapter = TypeAdapter(list[ModelMessage])\n   ```\n2. Alternatively you can serialize to/from JSON directly:\n   ```python {test=\"skip\" lint=\"skip\" format=\"skip\"}\n   from pydantic_core import to_json\n   ...\n   as_json_objects = to_json(history_step_1)\n   same_history_as_step_1 = ModelMessagesTypeAdapter.validate_json(as_json_objects)\n   ```\n3. You can now continue the conversation with history `same_history_as_step_1` despite creating a new agent run.\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/message-history/#storing-and-loading-messages-to-json", "page": "docs/message-history", "source_site": "pydantic_ai"}
{"title": "Other ways of using messages", "anchor": "other-ways-of-using-messages", "heading_level": 2, "md_text": "Since messages are defined by simple dataclasses, you can manually create and manipulate, e.g. for testing.\n\nThe message format is independent of the model used, so you can use messages in different agents, or the same agent with different models.\n\nIn the example below, we reuse the message from the first agent run, which uses the `openai:gpt-5` model, in a second agent run using the `google-gla:gemini-2.5-pro` model.\n\n```python {title=\"Reusing messages with a different model\" hl_lines=\"17\"}\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-5', system_prompt='Be a helpful assistant.')\n\nresult1 = agent.run_sync('Tell me a joke.')\nprint(result1.output)\n#> Did you hear about the toothpaste scandal? They called it Colgate.\n\nresult2 = agent.run_sync(\n    'Explain?',\n    model='google-gla:gemini-2.5-pro',\n    message_history=result1.new_messages(),\n)\nprint(result2.output)\n#> This is an excellent joke invented by Samuel Colvin, it needs no explanation.\n\nprint(result2.all_messages())\n\"\"\"\n[\n    ModelRequest(\n        parts=[\n            SystemPromptPart(\n                content='Be a helpful assistant.',\n                timestamp=datetime.datetime(...),\n            ),\n            UserPromptPart(\n                content='Tell me a joke.',\n                timestamp=datetime.datetime(...),\n            ),\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='Did you hear about the toothpaste scandal? They called it Colgate.'\n            )\n        ],\n        usage=RequestUsage(input_tokens=60, output_tokens=12),\n        model_name='gpt-5',\n        timestamp=datetime.datetime(...),\n    ),\n    ModelRequest(\n        parts=[\n            UserPromptPart(\n                content='Explain?',\n                timestamp=datetime.datetime(...),\n            )\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='This is an excellent joke invented by Samuel Colvin, it needs no explanation.'\n            )\n        ],\n        usage=RequestUsage(input_tokens=61, output_tokens=26),\n        model_name='gemini-2.5-pro',\n        timestamp=datetime.datetime(...),\n    ),\n]\n\"\"\"\n```", "url": "https://ai.pydantic.dev/docs/message-history/#other-ways-of-using-messages", "page": "docs/message-history", "source_site": "pydantic_ai"}
{"title": "Processing Message History", "anchor": "processing-message-history", "heading_level": 2, "md_text": "Sometimes you may want to modify the message history before it's sent to the model. This could be for privacy\nreasons (filtering out sensitive information), to save costs on tokens, to give less context to the LLM, or\ncustom processing logic.\n\nPydantic AI provides a `history_processors` parameter on `Agent` that allows you to intercept and modify\nthe message history before each model request.\n\n!!! warning \"History processors replace the message history\"\n    History processors replace the message history in the state with the processed messages, including the new user prompt part.\n    This means that if you want to keep the original message history, you need to make a copy of it.", "url": "https://ai.pydantic.dev/docs/message-history/#processing-message-history", "page": "docs/message-history", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 3, "md_text": "The `history_processors` is a list of callables that take a list of\n[`ModelMessage`][pydantic_ai.messages.ModelMessage] and return a modified list of the same type.\n\nEach processor is applied in sequence, and processors can be either synchronous or asynchronous.\n\n```python {title=\"simple_history_processor.py\"}\nfrom pydantic_ai import (\n    Agent,\n    ModelMessage,\n    ModelRequest,\n    ModelResponse,\n    TextPart,\n    UserPromptPart,\n)\n\n\ndef filter_responses(messages: list[ModelMessage]) -> list[ModelMessage]:\n    \"\"\"Remove all ModelResponse messages, keeping only ModelRequest messages.\"\"\"\n    return [msg for msg in messages if isinstance(msg, ModelRequest)]", "url": "https://ai.pydantic.dev/docs/message-history/#usage", "page": "docs/message-history", "source_site": "pydantic_ai"}
{"title": "Create agent with history processor", "anchor": "create-agent-with-history-processor", "heading_level": 1, "md_text": "agent = Agent('openai:gpt-5', history_processors=[filter_responses])", "url": "https://ai.pydantic.dev/docs/message-history/#create-agent-with-history-processor", "page": "docs/message-history", "source_site": "pydantic_ai"}
{"title": "Example: Create some conversation history", "anchor": "example-create-some-conversation-history", "heading_level": 1, "md_text": "message_history = [\n    ModelRequest(parts=[UserPromptPart(content='What is 2+2?')]),\n    ModelResponse(parts=[TextPart(content='2+2 equals 4')]),  # This will be filtered out\n]", "url": "https://ai.pydantic.dev/docs/message-history/#example-create-some-conversation-history", "page": "docs/message-history", "source_site": "pydantic_ai"}
{"title": "result = agent.run_sync('What about 3+3?', message_history=message_history)", "anchor": "result-agentrun_syncwhat-about-33-message_historymessage_history", "heading_level": 1, "md_text": "```", "url": "https://ai.pydantic.dev/docs/message-history/#result-agentrun_syncwhat-about-33-message_historymessage_history", "page": "docs/message-history", "source_site": "pydantic_ai"}
{"title": "Keep Only Recent Messages", "anchor": "keep-only-recent-messages", "heading_level": 4, "md_text": "You can use the `history_processor` to only keep the recent messages:\n\n```python {title=\"keep_recent_messages.py\"}\nfrom pydantic_ai import Agent, ModelMessage\n\n\nasync def keep_recent_messages(messages: list[ModelMessage]) -> list[ModelMessage]:\n    \"\"\"Keep only the last 5 messages to manage token usage.\"\"\"\n    return messages[-5:] if len(messages) > 5 else messages\n\nagent = Agent('openai:gpt-5', history_processors=[keep_recent_messages])", "url": "https://ai.pydantic.dev/docs/message-history/#keep-only-recent-messages", "page": "docs/message-history", "source_site": "pydantic_ai"}
{"title": "Example: Even with a long conversation history, only the last 5 messages are sent to the model", "anchor": "example-even-with-a-long-conversation-history-only-the-last-5-messages-are-sent-to-the-model", "heading_level": 1, "md_text": "long_conversation_history: list[ModelMessage] = []  # Your long conversation history here", "url": "https://ai.pydantic.dev/docs/message-history/#example-even-with-a-long-conversation-history-only-the-last-5-messages-are-sent-to-the-model", "page": "docs/message-history", "source_site": "pydantic_ai"}
{"title": "result = agent.run_sync('What did we discuss?', message_history=long_conversation_history)", "anchor": "result-agentrun_syncwhat-did-we-discuss-message_historylong_conversation_history", "heading_level": 1, "md_text": "```\n\n!!! warning \"Be careful when slicing the message history\"\n    When slicing the message history, you need to make sure that tool calls and returns are paired, otherwise the LLM may return an error. For more details, refer to [this GitHub issue](https://github.com/pydantic/pydantic-ai/issues/2050#issuecomment-3019976269).", "url": "https://ai.pydantic.dev/docs/message-history/#result-agentrun_syncwhat-did-we-discuss-message_historylong_conversation_history", "page": "docs/message-history", "source_site": "pydantic_ai"}
{"title": "`RunContext` parameter", "anchor": "runcontext-parameter", "heading_level": 4, "md_text": "History processors can optionally accept a [`RunContext`][pydantic_ai.tools.RunContext] parameter to access\nadditional information about the current run, such as dependencies, model information, and usage statistics:\n\n```python {title=\"context_aware_processor.py\"}\nfrom pydantic_ai import Agent, ModelMessage, RunContext\n\n\ndef context_aware_processor(\n    ctx: RunContext[None],\n    messages: list[ModelMessage],\n) -> list[ModelMessage]:\n    # Access current usage\n    current_tokens = ctx.usage.total_tokens\n\n    # Filter messages based on context\n    if current_tokens > 1000:\n        return messages[-3:]  # Keep only recent messages when token usage is high\n    return messages\n\nagent = Agent('openai:gpt-5', history_processors=[context_aware_processor])\n```\n\nThis allows for more sophisticated message processing based on the current state of the agent run.", "url": "https://ai.pydantic.dev/docs/message-history/#runcontext-parameter", "page": "docs/message-history", "source_site": "pydantic_ai"}
{"title": "Summarize Old Messages", "anchor": "summarize-old-messages", "heading_level": 4, "md_text": "Use an LLM to summarize older messages to preserve context while reducing tokens.\n\n```python {title=\"summarize_old_messages.py\"}\nfrom pydantic_ai import Agent, ModelMessage", "url": "https://ai.pydantic.dev/docs/message-history/#summarize-old-messages", "page": "docs/message-history", "source_site": "pydantic_ai"}
{"title": "Use a cheaper model to summarize old messages.", "anchor": "use-a-cheaper-model-to-summarize-old-messages", "heading_level": 1, "md_text": "summarize_agent = Agent(\n    'openai:gpt-5-mini',\n    instructions=\"\"\"\nSummarize this conversation, omitting small talk and unrelated topics.\nFocus on the technical discussion and next steps.\n\"\"\",\n)\n\n\nasync def summarize_old_messages(messages: list[ModelMessage]) -> list[ModelMessage]:\n    # Summarize the oldest 10 messages\n    if len(messages) > 10:\n        oldest_messages = messages[:10]\n        summary = await summarize_agent.run(message_history=oldest_messages)\n        # Return the last message and the summary\n        return summary.new_messages() + messages[-1:]\n\n    return messages\n\n\nagent = Agent('openai:gpt-5', history_processors=[summarize_old_messages])\n```\n\n!!! warning \"Be careful when summarizing the message history\"\n    When summarizing the message history, you need to make sure that tool calls and returns are paired, otherwise the LLM may return an error. For more details, refer to [this GitHub issue](https://github.com/pydantic/pydantic-ai/issues/2050#issuecomment-3019976269), where you can find examples of summarizing the message history.", "url": "https://ai.pydantic.dev/docs/message-history/#use-a-cheaper-model-to-summarize-old-messages", "page": "docs/message-history", "source_site": "pydantic_ai"}
{"title": "Testing History Processors", "anchor": "testing-history-processors", "heading_level": 3, "md_text": "You can test what messages are actually sent to the model provider using\n[`FunctionModel`][pydantic_ai.models.function.FunctionModel]:\n\n```python {title=\"test_history_processor.py\"}\nimport pytest\n\nfrom pydantic_ai import (\n    Agent,\n    ModelMessage,\n    ModelRequest,\n    ModelResponse,\n    TextPart,\n    UserPromptPart,\n)\nfrom pydantic_ai.models.function import AgentInfo, FunctionModel\n\n\n@pytest.fixture\ndef received_messages() -> list[ModelMessage]:\n    return []\n\n\n@pytest.fixture\ndef function_model(received_messages: list[ModelMessage]) -> FunctionModel:\n    def capture_model_function(messages: list[ModelMessage], info: AgentInfo) -> ModelResponse:\n        # Capture the messages that the provider actually receives\n        received_messages.clear()\n        received_messages.extend(messages)\n        return ModelResponse(parts=[TextPart(content='Provider response')])\n\n    return FunctionModel(capture_model_function)\n\n\ndef test_history_processor(function_model: FunctionModel, received_messages: list[ModelMessage]):\n    def filter_responses(messages: list[ModelMessage]) -> list[ModelMessage]:\n        return [msg for msg in messages if isinstance(msg, ModelRequest)]\n\n    agent = Agent(function_model, history_processors=[filter_responses])\n\n    message_history = [\n        ModelRequest(parts=[UserPromptPart(content='Question 1')]),\n        ModelResponse(parts=[TextPart(content='Answer 1')]),\n    ]\n\n    agent.run_sync('Question 2', message_history=message_history)\n    assert received_messages == [\n        ModelRequest(parts=[UserPromptPart(content='Question 1')]),\n        ModelRequest(parts=[UserPromptPart(content='Question 2')]),\n    ]\n```", "url": "https://ai.pydantic.dev/docs/message-history/#testing-history-processors", "page": "docs/message-history", "source_site": "pydantic_ai"}
{"title": "Multiple Processors", "anchor": "multiple-processors", "heading_level": 3, "md_text": "You can also use multiple processors:\n\n```python {title=\"multiple_history_processors.py\"}\nfrom pydantic_ai import Agent, ModelMessage, ModelRequest\n\n\ndef filter_responses(messages: list[ModelMessage]) -> list[ModelMessage]:\n    return [msg for msg in messages if isinstance(msg, ModelRequest)]\n\n\ndef summarize_old_messages(messages: list[ModelMessage]) -> list[ModelMessage]:\n    return messages[-5:]\n\n\nagent = Agent('openai:gpt-5', history_processors=[filter_responses, summarize_old_messages])\n```\n\nIn this case, the `filter_responses` processor will be applied first, and the\n`summarize_old_messages` processor will be applied second.", "url": "https://ai.pydantic.dev/docs/message-history/#multiple-processors", "page": "docs/message-history", "source_site": "pydantic_ai"}
{"title": "Examples", "anchor": "examples", "heading_level": 2, "md_text": "For a more complete example of using messages in conversations, see the [chat app](examples/chat-app.md) example.", "url": "https://ai.pydantic.dev/docs/message-history/#examples", "page": "docs/message-history", "source_site": "pydantic_ai"}
{"title": "Introduction", "anchor": "introduction", "heading_level": 2, "md_text": "Agents are Pydantic AI's primary interface for interacting with LLMs.\n\nIn some use cases a single Agent will control an entire application or component,\nbut multiple agents can also interact to embody more complex workflows.\n\nThe [`Agent`][pydantic_ai.Agent] class has full API documentation, but conceptually you can think of an agent as a container for:\n\n| **Component**                                             | **Description**                                                                                           |\n| --------------------------------------------------------- | --------------------------------------------------------------------------------------------------------- |\n| [Instructions](#instructions)                             | A set of instructions for the LLM written by the developer.                                               |\n| [Function tool(s)](tools.md) and [toolsets](toolsets.md)  | Functions that the LLM may call to get information while generating a response.                           |\n| [Structured output type](output.md)                       | The structured datatype the LLM must return at the end of a run, if specified.                            |\n| [Dependency type constraint](dependencies.md)             | Dynamic instructions functions, tools, and output functions may all use dependencies when they're run.          |\n| [LLM model](api/models/base.md)                           | Optional default LLM model associated with the agent. Can also be specified when running the agent.       |\n| [Model Settings](#additional-configuration)               | Optional default model settings to help fine tune requests. Can also be specified when running the agent. |\n\nIn typing terms, agents are generic in their dependency and output types, e.g., an agent which required dependencies of type `#!python Foobar` and produced outputs of type `#!python list[str]` would have type `Agent[Foobar, list[str]]`. In practice, you shouldn't need to care about this, it should just mean your IDE can tell you when you have the right type, and if you choose to use [static type checking](#static-type-checking) it should work well with Pydantic AI.\n\nHere's a toy example of an agent that simulates a roulette wheel:\n\n```python {title=\"roulette_wheel.py\"}\nfrom pydantic_ai import Agent, RunContext\n\nroulette_agent = Agent(  # (1)!\n    'openai:gpt-5',\n    deps_type=int,\n    output_type=bool,\n    system_prompt=(\n        'Use the `roulette_wheel` function to see if the '\n        'customer has won based on the number they provide.'\n    ),\n)\n\n\n@roulette_agent.tool\nasync def roulette_wheel(ctx: RunContext[int], square: int) -> str:  # (2)!\n    \"\"\"check if the square is a winner\"\"\"\n    return 'winner' if square == ctx.deps else 'loser'", "url": "https://ai.pydantic.dev/docs/agents/#introduction", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "Run the agent", "anchor": "run-the-agent", "heading_level": 1, "md_text": "success_number = 18  # (3)!\nresult = roulette_agent.run_sync('Put my money on square eighteen', deps=success_number)\nprint(result.output)  # (4)!\n#> True\n\nresult = roulette_agent.run_sync('I bet five is the winner', deps=success_number)\nprint(result.output)\n#> False\n```\n\n1. Create an agent, which expects an integer dependency and produces a boolean output. This agent will have type `#!python Agent[int, bool]`.\n2. Define a tool that checks if the square is a winner. Here [`RunContext`][pydantic_ai.tools.RunContext] is parameterized with the dependency type `int`; if you got the dependency type wrong you'd get a typing error.\n3. In reality, you might want to use a random number here e.g. `random.randint(0, 36)`.\n4. `result.output` will be a boolean indicating if the square is a winner. Pydantic performs the output validation, and it'll be typed as a `bool` since its type is derived from the `output_type` generic parameter of the agent.\n\n!!! tip \"Agents are designed for reuse, like FastAPI Apps\"\n    Agents are intended to be instantiated once (frequently as module globals) and reused throughout your application, similar to a small [FastAPI][fastapi.FastAPI] app or an [APIRouter][fastapi.APIRouter].", "url": "https://ai.pydantic.dev/docs/agents/#run-the-agent", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "Running Agents", "anchor": "running-agents", "heading_level": 2, "md_text": "There are five ways to run an agent:\n\n1. [`agent.run()`][pydantic_ai.agent.AbstractAgent.run] \u2014 an async function which returns a [`RunResult`][pydantic_ai.agent.AgentRunResult] containing a completed response.\n2. [`agent.run_sync()`][pydantic_ai.agent.AbstractAgent.run_sync] \u2014 a plain, synchronous function which returns a [`RunResult`][pydantic_ai.agent.AgentRunResult] containing a completed response (internally, this just calls `loop.run_until_complete(self.run())`).\n3. [`agent.run_stream()`][pydantic_ai.agent.AbstractAgent.run_stream] \u2014 an async context manager which returns a [`StreamedRunResult`][pydantic_ai.result.StreamedRunResult], which contains methods to stream text and structured output as an async iterable.\n4. [`agent.run_stream_events()`][pydantic_ai.agent.AbstractAgent.run_stream_events] \u2014 a function which returns an async iterable of [`AgentStreamEvent`s][pydantic_ai.messages.AgentStreamEvent] and a [`AgentRunResultEvent`][pydantic_ai.run.AgentRunResultEvent] containing the final run result.\n5. [`agent.iter()`][pydantic_ai.Agent.iter] \u2014 a context manager which returns an [`AgentRun`][pydantic_ai.agent.AgentRun], an async iterable over the nodes of the agent's underlying [`Graph`][pydantic_graph.graph.Graph].\n\nHere's a simple example demonstrating the first four:\n\n```python {title=\"run_agent.py\"}\nfrom pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent\n\nagent = Agent('openai:gpt-5')\n\nresult_sync = agent.run_sync('What is the capital of Italy?')\nprint(result_sync.output)\n#> The capital of Italy is Rome.\n\n\nasync def main():\n    result = await agent.run('What is the capital of France?')\n    print(result.output)\n    #> The capital of France is Paris.\n\n    async with agent.run_stream('What is the capital of the UK?') as response:\n        async for text in response.stream_text():\n            print(text)\n            #> The capital of\n            #> The capital of the UK is\n            #> The capital of the UK is London.\n\n    events: list[AgentStreamEvent | AgentRunResultEvent] = []\n    async for event in agent.run_stream_events('What is the capital of Mexico?'):\n        events.append(event)\n    print(events)\n    \"\"\"\n    [\n        PartStartEvent(index=0, part=TextPart(content='The capital of ')),\n        FinalResultEvent(tool_name=None, tool_call_id=None),\n        PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='Mexico is Mexico ')),\n        PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='City.')),\n        PartEndEvent(\n            index=0, part=TextPart(content='The capital of Mexico is Mexico City.')\n        ),\n        AgentRunResultEvent(\n            result=AgentRunResult(output='The capital of Mexico is Mexico City.')\n        ),\n    ]\n    \"\"\"\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_\n\nYou can also pass messages from previous runs to continue a conversation or provide context, as described in [Messages and Chat History](message-history.md).", "url": "https://ai.pydantic.dev/docs/agents/#running-agents", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "Streaming Events and Final Output", "anchor": "streaming-events-and-final-output", "heading_level": 3, "md_text": "As shown in the example above, [`run_stream()`][pydantic_ai.agent.AbstractAgent.run_stream] makes it easy to stream the agent's final output as it comes in.\nIt also takes an optional `event_stream_handler` argument that you can use to gain insight into what is happening during the run before the final output is produced.\n\nThe example below shows how to stream events and text output. You can also [stream structured output](output.md#streaming-structured-output).\n\n!!! note\n    As the `run_stream()` method will consider the first output matching the [output type](output.md#structured-output) to be the final output,\n    it will stop running the agent graph and will not execute any tool calls made by the model after this \"final\" output.\n\n    If you want to always run the agent graph to completion and stream all events from the model's streaming response and the agent's execution of tools,\n    use [`agent.run_stream_events()`][pydantic_ai.agent.AbstractAgent.run_stream_events] or [`agent.iter()`][pydantic_ai.agent.AbstractAgent.iter] instead, as described in the following sections.\n\n```python {title=\"run_stream_event_stream_handler.py\"}\nimport asyncio\nfrom collections.abc import AsyncIterable\nfrom datetime import date\n\nfrom pydantic_ai import (\n    Agent,\n    AgentStreamEvent,\n    FinalResultEvent,\n    FunctionToolCallEvent,\n    FunctionToolResultEvent,\n    PartDeltaEvent,\n    PartStartEvent,\n    RunContext,\n    TextPartDelta,\n    ThinkingPartDelta,\n    ToolCallPartDelta,\n)\n\nweather_agent = Agent(\n    'openai:gpt-5',\n    system_prompt='Providing a weather forecast at the locations the user provides.',\n)\n\n\n@weather_agent.tool\nasync def weather_forecast(\n    ctx: RunContext,\n    location: str,\n    forecast_date: date,\n) -> str:\n    return f'The forecast in {location} on {forecast_date} is 24\u00b0C and sunny.'\n\n\noutput_messages: list[str] = []\n\nasync def handle_event(event: AgentStreamEvent):\n    if isinstance(event, PartStartEvent):\n        output_messages.append(f'[Request] Starting part {event.index}: {event.part!r}')\n    elif isinstance(event, PartDeltaEvent):\n        if isinstance(event.delta, TextPartDelta):\n            output_messages.append(f'[Request] Part {event.index} text delta: {event.delta.content_delta!r}')\n        elif isinstance(event.delta, ThinkingPartDelta):\n            output_messages.append(f'[Request] Part {event.index} thinking delta: {event.delta.content_delta!r}')\n        elif isinstance(event.delta, ToolCallPartDelta):\n            output_messages.append(f'[Request] Part {event.index} args delta: {event.delta.args_delta}')\n    elif isinstance(event, FunctionToolCallEvent):\n        output_messages.append(\n            f'[Tools] The LLM calls tool={event.part.tool_name!r} with args={event.part.args} (tool_call_id={event.part.tool_call_id!r})'\n        )\n    elif isinstance(event, FunctionToolResultEvent):\n        output_messages.append(f'[Tools] Tool call {event.tool_call_id!r} returned => {event.result.content}')\n    elif isinstance(event, FinalResultEvent):\n        output_messages.append(f'[Result] The model starting producing a final result (tool_name={event.tool_name})')\n\n\nasync def event_stream_handler(\n    ctx: RunContext,\n    event_stream: AsyncIterable[AgentStreamEvent],\n):\n    async for event in event_stream:\n        await handle_event(event)\n\nasync def main():\n    user_prompt = 'What will the weather be like in Paris on Tuesday?'\n\n    async with weather_agent.run_stream(user_prompt, event_stream_handler=event_stream_handler) as run:\n        async for output in run.stream_text():\n            output_messages.append(f'[Output] {output}')\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n    print(output_messages)\n    \"\"\"\n    [\n        \"[Request] Starting part 0: ToolCallPart(tool_name='weather_forecast', tool_call_id='0001')\",\n        '[Request] Part 0 args delta: {\"location\":\"Pa',\n        '[Request] Part 0 args delta: ris\",\"forecast_',\n        '[Request] Part 0 args delta: date\":\"2030-01-',\n        '[Request] Part 0 args delta: 01\"}',\n        '[Tools] The LLM calls tool=\\'weather_forecast\\' with args={\"location\":\"Paris\",\"forecast_date\":\"2030-01-01\"} (tool_call_id=\\'0001\\')',\n        \"[Tools] Tool call '0001' returned => The forecast in Paris on 2030-01-01 is 24\u00b0C and sunny.\",\n        \"[Request] Starting part 0: TextPart(content='It will be ')\",\n        '[Result] The model starting producing a final result (tool_name=None)',\n        '[Output] It will be ',\n        '[Output] It will be warm and sunny ',\n        '[Output] It will be warm and sunny in Paris on ',\n        '[Output] It will be warm and sunny in Paris on Tuesday.',\n    ]\n    \"\"\"\n```", "url": "https://ai.pydantic.dev/docs/agents/#streaming-events-and-final-output", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "Streaming All Events", "anchor": "streaming-all-events", "heading_level": 3, "md_text": "Like `agent.run_stream()`, [`agent.run()`][pydantic_ai.agent.AbstractAgent.run_stream] takes an optional `event_stream_handler`\nargument that lets you stream all events from the model's streaming response and the agent's execution of tools.\nUnlike `run_stream()`, it always runs the agent graph to completion even if text was received ahead of tool calls that looked like it could've been the final result.\n\nFor convenience, a [`agent.run_stream_events()`][pydantic_ai.agent.AbstractAgent.run_stream_events] method is also available as a wrapper around `run(event_stream_handler=...)`, which returns an async iterable of [`AgentStreamEvent`s][pydantic_ai.messages.AgentStreamEvent] and a [`AgentRunResultEvent`][pydantic_ai.run.AgentRunResultEvent] containing the final run result.\n\n!!! note\n    As they return raw events as they come in, the `run_stream_events()` and `run(event_stream_handler=...)` methods require you to piece together the streamed text and structured output yourself from the `PartStartEvent` and subsequent `PartDeltaEvent`s.\n\n    To get the best of both worlds, at the expense of some additional complexity, you can use [`agent.iter()`][pydantic_ai.agent.AbstractAgent.iter] as described in the next section, which lets you [iterate over the agent graph](#iterating-over-an-agents-graph) and [stream both events and output](#streaming-all-events-and-output) at every step.\n\n```python {title=\"run_events.py\" requires=\"run_stream_event_stream_handler.py\"}\nimport asyncio\n\nfrom pydantic_ai import AgentRunResultEvent\n\nfrom run_stream_event_stream_handler import handle_event, output_messages, weather_agent\n\n\nasync def main():\n    user_prompt = 'What will the weather be like in Paris on Tuesday?'\n\n    async for event in weather_agent.run_stream_events(user_prompt):\n        if isinstance(event, AgentRunResultEvent):\n            output_messages.append(f'[Final Output] {event.result.output}')\n        else:\n            await handle_event(event)\n\nif __name__ == '__main__':\n    asyncio.run(main())\n\n    print(output_messages)\n    \"\"\"\n    [\n        \"[Request] Starting part 0: ToolCallPart(tool_name='weather_forecast', tool_call_id='0001')\",\n        '[Request] Part 0 args delta: {\"location\":\"Pa',\n        '[Request] Part 0 args delta: ris\",\"forecast_',\n        '[Request] Part 0 args delta: date\":\"2030-01-',\n        '[Request] Part 0 args delta: 01\"}',\n        '[Tools] The LLM calls tool=\\'weather_forecast\\' with args={\"location\":\"Paris\",\"forecast_date\":\"2030-01-01\"} (tool_call_id=\\'0001\\')',\n        \"[Tools] Tool call '0001' returned => The forecast in Paris on 2030-01-01 is 24\u00b0C and sunny.\",\n        \"[Request] Starting part 0: TextPart(content='It will be ')\",\n        '[Result] The model starting producing a final result (tool_name=None)',\n        \"[Request] Part 0 text delta: 'warm and sunny '\",\n        \"[Request] Part 0 text delta: 'in Paris on '\",\n        \"[Request] Part 0 text delta: 'Tuesday.'\",\n        '[Final Output] It will be warm and sunny in Paris on Tuesday.',\n    ]\n    \"\"\"\n```\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/agents/#streaming-all-events", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "Iterating Over an Agent's Graph", "anchor": "iterating-over-an-agents-graph", "heading_level": 3, "md_text": "Under the hood, each `Agent` in Pydantic AI uses **pydantic-graph** to manage its execution flow. **pydantic-graph** is a generic, type-centric library for building and running finite state machines in Python. It doesn't actually depend on Pydantic AI \u2014 you can use it standalone for workflows that have nothing to do with GenAI \u2014 but Pydantic AI makes use of it to orchestrate the handling of model requests and model responses in an agent's run.\n\nIn many scenarios, you don't need to worry about pydantic-graph at all; calling `agent.run(...)` simply traverses the underlying graph from start to finish. However, if you need deeper insight or control \u2014 for example to inject your own logic at specific stages \u2014 Pydantic AI exposes the lower-level iteration process via [`Agent.iter`][pydantic_ai.Agent.iter]. This method returns an [`AgentRun`][pydantic_ai.agent.AgentRun], which you can async-iterate over, or manually drive node-by-node via the [`next`][pydantic_ai.agent.AgentRun.next] method. Once the agent's graph returns an [`End`][pydantic_graph.nodes.End], you have the final result along with a detailed history of all steps.", "url": "https://ai.pydantic.dev/docs/agents/#iterating-over-an-agents-graph", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "`async for` iteration", "anchor": "async-for-iteration", "heading_level": 4, "md_text": "Here's an example of using `async for` with `iter` to record each node the agent executes:\n\n```python {title=\"agent_iter_async_for.py\"}\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-5')\n\n\nasync def main():\n    nodes = []\n    # Begin an AgentRun, which is an async-iterable over the nodes of the agent's graph\n    async with agent.iter('What is the capital of France?') as agent_run:\n        async for node in agent_run:\n            # Each node represents a step in the agent's execution\n            nodes.append(node)\n    print(nodes)\n    \"\"\"\n    [\n        UserPromptNode(\n            user_prompt='What is the capital of France?',\n            instructions_functions=[],\n            system_prompts=(),\n            system_prompt_functions=[],\n            system_prompt_dynamic_functions={},\n        ),\n        ModelRequestNode(\n            request=ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='What is the capital of France?',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            )\n        ),\n        CallToolsNode(\n            model_response=ModelResponse(\n                parts=[TextPart(content='The capital of France is Paris.')],\n                usage=RequestUsage(input_tokens=56, output_tokens=7),\n                model_name='gpt-5',\n                timestamp=datetime.datetime(...),\n            )\n        ),\n        End(data=FinalResult(output='The capital of France is Paris.')),\n    ]\n    \"\"\"\n    print(agent_run.result.output)\n    #> The capital of France is Paris.\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_\n\n- The `AgentRun` is an async iterator that yields each node (`BaseNode` or `End`) in the flow.\n- The run ends when an `End` node is returned.", "url": "https://ai.pydantic.dev/docs/agents/#async-for-iteration", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "Using `.next(...)` manually", "anchor": "using-next-manually", "heading_level": 4, "md_text": "You can also drive the iteration manually by passing the node you want to run next to the `AgentRun.next(...)` method. This allows you to inspect or modify the node before it executes or skip nodes based on your own logic, and to catch errors in `next()` more easily:\n\n```python {title=\"agent_iter_next.py\"}\nfrom pydantic_ai import Agent\nfrom pydantic_graph import End\n\nagent = Agent('openai:gpt-5')\n\n\nasync def main():\n    async with agent.iter('What is the capital of France?') as agent_run:\n        node = agent_run.next_node  # (1)!\n\n        all_nodes = [node]\n\n        # Drive the iteration manually:\n        while not isinstance(node, End):  # (2)!\n            node = await agent_run.next(node)  # (3)!\n            all_nodes.append(node)  # (4)!\n\n        print(all_nodes)\n        \"\"\"\n        [\n            UserPromptNode(\n                user_prompt='What is the capital of France?',\n                instructions_functions=[],\n                system_prompts=(),\n                system_prompt_functions=[],\n                system_prompt_dynamic_functions={},\n            ),\n            ModelRequestNode(\n                request=ModelRequest(\n                    parts=[\n                        UserPromptPart(\n                            content='What is the capital of France?',\n                            timestamp=datetime.datetime(...),\n                        )\n                    ]\n                )\n            ),\n            CallToolsNode(\n                model_response=ModelResponse(\n                    parts=[TextPart(content='The capital of France is Paris.')],\n                    usage=RequestUsage(input_tokens=56, output_tokens=7),\n                    model_name='gpt-5',\n                    timestamp=datetime.datetime(...),\n                )\n            ),\n            End(data=FinalResult(output='The capital of France is Paris.')),\n        ]\n        \"\"\"\n```\n\n1. We start by grabbing the first node that will be run in the agent's graph.\n2. The agent run is finished once an `End` node has been produced; instances of `End` cannot be passed to `next`.\n3. When you call `await agent_run.next(node)`, it executes that node in the agent's graph, updates the run's history, and returns the _next_ node to run.\n4. You could also inspect or mutate the new `node` here as needed.\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/agents/#using-next-manually", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "Accessing usage and final output", "anchor": "accessing-usage-and-final-output", "heading_level": 4, "md_text": "You can retrieve usage statistics (tokens, requests, etc.) at any time from the [`AgentRun`][pydantic_ai.agent.AgentRun] object via `agent_run.usage()`. This method returns a [`RunUsage`][pydantic_ai.usage.RunUsage] object containing the usage data.\n\nOnce the run finishes, `agent_run.result` becomes a [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] object containing the final output (and related metadata).", "url": "https://ai.pydantic.dev/docs/agents/#accessing-usage-and-final-output", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "Streaming All Events and Output", "anchor": "streaming-all-events-and-output", "heading_level": 4, "md_text": "Here is an example of streaming an agent run in combination with `async for` iteration:\n\n```python {title=\"streaming_iter.py\"}\nimport asyncio\nfrom dataclasses import dataclass\nfrom datetime import date\n\nfrom pydantic_ai import (\n    Agent,\n    FinalResultEvent,\n    FunctionToolCallEvent,\n    FunctionToolResultEvent,\n    PartDeltaEvent,\n    PartStartEvent,\n    RunContext,\n    TextPartDelta,\n    ThinkingPartDelta,\n    ToolCallPartDelta,\n)\n\n\n@dataclass\nclass WeatherService:\n    async def get_forecast(self, location: str, forecast_date: date) -> str:\n        # In real code: call weather API, DB queries, etc.\n        return f'The forecast in {location} on {forecast_date} is 24\u00b0C and sunny.'\n\n    async def get_historic_weather(self, location: str, forecast_date: date) -> str:\n        # In real code: call a historical weather API or DB\n        return f'The weather in {location} on {forecast_date} was 18\u00b0C and partly cloudy.'\n\n\nweather_agent = Agent[WeatherService, str](\n    'openai:gpt-5',\n    deps_type=WeatherService,\n    output_type=str,  # We'll produce a final answer as plain text\n    system_prompt='Providing a weather forecast at the locations the user provides.',\n)\n\n\n@weather_agent.tool\nasync def weather_forecast(\n    ctx: RunContext[WeatherService],\n    location: str,\n    forecast_date: date,\n) -> str:\n    if forecast_date >= date.today():\n        return await ctx.deps.get_forecast(location, forecast_date)\n    else:\n        return await ctx.deps.get_historic_weather(location, forecast_date)\n\n\noutput_messages: list[str] = []\n\n\nasync def main():\n    user_prompt = 'What will the weather be like in Paris on Tuesday?'\n\n    # Begin a node-by-node, streaming iteration\n    async with weather_agent.iter(user_prompt, deps=WeatherService()) as run:\n        async for node in run:\n            if Agent.is_user_prompt_node(node):\n                # A user prompt node => The user has provided input\n                output_messages.append(f'=== UserPromptNode: {node.user_prompt} ===')\n            elif Agent.is_model_request_node(node):\n                # A model request node => We can stream tokens from the model's request\n                output_messages.append('=== ModelRequestNode: streaming partial request tokens ===')\n                async with node.stream(run.ctx) as request_stream:\n                    final_result_found = False\n                    async for event in request_stream:\n                        if isinstance(event, PartStartEvent):\n                            output_messages.append(f'[Request] Starting part {event.index}: {event.part!r}')\n                        elif isinstance(event, PartDeltaEvent):\n                            if isinstance(event.delta, TextPartDelta):\n                                output_messages.append(\n                                    f'[Request] Part {event.index} text delta: {event.delta.content_delta!r}'\n                                )\n                            elif isinstance(event.delta, ThinkingPartDelta):\n                                output_messages.append(\n                                    f'[Request] Part {event.index} thinking delta: {event.delta.content_delta!r}'\n                                )\n                            elif isinstance(event.delta, ToolCallPartDelta):\n                                output_messages.append(\n                                    f'[Request] Part {event.index} args delta: {event.delta.args_delta}'\n                                )\n                        elif isinstance(event, FinalResultEvent):\n                            output_messages.append(\n                                f'[Result] The model started producing a final result (tool_name={event.tool_name})'\n                            )\n                            final_result_found = True\n                            break\n\n                    if final_result_found:\n                        # Once the final result is found, we can call `AgentStream.stream_text()` to stream the text.\n                        # A similar `AgentStream.stream_output()` method is available to stream structured output.\n                        async for output in request_stream.stream_text():\n                            output_messages.append(f'[Output] {output}')\n            elif Agent.is_call_tools_node(node):\n                # A handle-response node => The model returned some data, potentially calls a tool\n                output_messages.append('=== CallToolsNode: streaming partial response & tool usage ===')\n                async with node.stream(run.ctx) as handle_stream:\n                    async for event in handle_stream:\n                        if isinstance(event, FunctionToolCallEvent):\n                            output_messages.append(\n                                f'[Tools] The LLM calls tool={event.part.tool_name!r} with args={event.part.args} (tool_call_id={event.part.tool_call_id!r})'\n                            )\n                        elif isinstance(event, FunctionToolResultEvent):\n                            output_messages.append(\n                                f'[Tools] Tool call {event.tool_call_id!r} returned => {event.result.content}'\n                            )\n            elif Agent.is_end_node(node):\n                # Once an End node is reached, the agent run is complete\n                assert run.result is not None\n                assert run.result.output == node.data.output\n                output_messages.append(f'=== Final Agent Output: {run.result.output} ===')\n\n\nif __name__ == '__main__':\n    asyncio.run(main())", "url": "https://ai.pydantic.dev/docs/agents/#streaming-all-events-and-output", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "Streaming All Events and Output", "anchor": "streaming-all-events-and-output", "heading_level": 4, "md_text": "    print(output_messages)\n    \"\"\"\n    [\n        '=== UserPromptNode: What will the weather be like in Paris on Tuesday? ===',\n        '=== ModelRequestNode: streaming partial request tokens ===',\n        \"[Request] Starting part 0: ToolCallPart(tool_name='weather_forecast', tool_call_id='0001')\",\n        '[Request] Part 0 args delta: {\"location\":\"Pa',\n        '[Request] Part 0 args delta: ris\",\"forecast_',\n        '[Request] Part 0 args delta: date\":\"2030-01-',\n        '[Request] Part 0 args delta: 01\"}',\n        '=== CallToolsNode: streaming partial response & tool usage ===',\n        '[Tools] The LLM calls tool=\\'weather_forecast\\' with args={\"location\":\"Paris\",\"forecast_date\":\"2030-01-01\"} (tool_call_id=\\'0001\\')',\n        \"[Tools] Tool call '0001' returned => The forecast in Paris on 2030-01-01 is 24\u00b0C and sunny.\",\n        '=== ModelRequestNode: streaming partial request tokens ===',\n        \"[Request] Starting part 0: TextPart(content='It will be ')\",\n        '[Result] The model started producing a final result (tool_name=None)',\n        '[Output] It will be ',\n        '[Output] It will be warm and sunny ',\n        '[Output] It will be warm and sunny in Paris on ',\n        '[Output] It will be warm and sunny in Paris on Tuesday.',\n        '=== CallToolsNode: streaming partial response & tool usage ===',\n        '=== Final Agent Output: It will be warm and sunny in Paris on Tuesday. ===',\n    ]\n    \"\"\"\n```\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/agents/#streaming-all-events-and-output", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "Usage Limits", "anchor": "usage-limits", "heading_level": 4, "md_text": "Pydantic AI offers a [`UsageLimits`][pydantic_ai.usage.UsageLimits] structure to help you limit your\nusage (tokens, requests, and tool calls) on model runs.\n\nYou can apply these settings by passing the `usage_limits` argument to the `run{_sync,_stream}` functions.\n\nConsider the following example, where we limit the number of response tokens:\n\n```py\nfrom pydantic_ai import Agent, UsageLimitExceeded, UsageLimits\n\nagent = Agent('anthropic:claude-sonnet-4-5')\n\nresult_sync = agent.run_sync(\n    'What is the capital of Italy? Answer with just the city.',\n    usage_limits=UsageLimits(response_tokens_limit=10),\n)\nprint(result_sync.output)\n#> Rome\nprint(result_sync.usage())\n#> RunUsage(input_tokens=62, output_tokens=1, requests=1)\n\ntry:\n    result_sync = agent.run_sync(\n        'What is the capital of Italy? Answer with a paragraph.',\n        usage_limits=UsageLimits(response_tokens_limit=10),\n    )\nexcept UsageLimitExceeded as e:\n    print(e)\n    #> Exceeded the output_tokens_limit of 10 (output_tokens=32)\n```\n\nRestricting the number of requests can be useful in preventing infinite loops or excessive tool calling:\n\n```py\nfrom typing_extensions import TypedDict\n\nfrom pydantic_ai import Agent, ModelRetry, UsageLimitExceeded, UsageLimits\n\n\nclass NeverOutputType(TypedDict):\n    \"\"\"\n    Never ever coerce data to this type.\n    \"\"\"\n\n    never_use_this: str\n\n\nagent = Agent(\n    'anthropic:claude-sonnet-4-5',\n    retries=3,\n    output_type=NeverOutputType,\n    system_prompt='Any time you get a response, call the `infinite_retry_tool` to produce another response.',\n)\n\n\n@agent.tool_plain(retries=5)  # (1)!\ndef infinite_retry_tool() -> int:\n    raise ModelRetry('Please try again.')\n\n\ntry:\n    result_sync = agent.run_sync(\n        'Begin infinite retry loop!', usage_limits=UsageLimits(request_limit=3)  # (2)!\n    )\nexcept UsageLimitExceeded as e:\n    print(e)\n    #> The next request would exceed the request_limit of 3\n```\n\n1. This tool has the ability to retry 5 times before erroring, simulating a tool that might get stuck in a loop.\n2. This run will error after 3 requests, preventing the infinite tool calling.", "url": "https://ai.pydantic.dev/docs/agents/#usage-limits", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "Capping tool calls", "anchor": "capping-tool-calls", "heading_level": 5, "md_text": "If you need a limit on the number of successful tool invocations within a single run, use `tool_calls_limit`:\n\n```py\nfrom pydantic_ai import Agent\nfrom pydantic_ai.exceptions import UsageLimitExceeded\nfrom pydantic_ai.usage import UsageLimits\n\nagent = Agent('anthropic:claude-sonnet-4-5')\n\n@agent.tool_plain\ndef do_work() -> str:\n    return 'ok'\n\ntry:\n    # Allow at most one executed tool call in this run\n    agent.run_sync('Please call the tool twice', usage_limits=UsageLimits(tool_calls_limit=1))\nexcept UsageLimitExceeded as e:\n    print(e)\n    #> The next tool call(s) would exceed the tool_calls_limit of 1 (tool_calls=2).\n```\n\n!!! note\n    - Usage limits are especially relevant if you've registered many tools. Use `request_limit` to bound the number of model turns, and `tool_calls_limit` to cap the number of successful tool executions within a run.\n    - The `tool_calls_limit` is checked before executing tool calls. If the model returns parallel tool calls that would exceed the limit, no tools will be executed.", "url": "https://ai.pydantic.dev/docs/agents/#capping-tool-calls", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "Model (Run) Settings", "anchor": "model-run-settings", "heading_level": 4, "md_text": "Pydantic AI offers a [`settings.ModelSettings`][pydantic_ai.settings.ModelSettings] structure to help you fine tune your requests.\nThis structure allows you to configure common parameters that influence the model's behavior, such as `temperature`, `max_tokens`,\n`timeout`, and more.\n\nThere are three ways to apply these settings, with a clear precedence order:\n\n1. **Model-level defaults** - Set when creating a model instance via the `settings` parameter. These serve as the base defaults for that model.\n2. **Agent-level defaults** - Set during [`Agent`][pydantic_ai.agent.Agent] initialization via the `model_settings` argument. These are merged with model defaults, with agent settings taking precedence.\n3. **Run-time overrides** - Passed to `run{_sync,_stream}` functions via the `model_settings` argument. These have the highest priority and are merged with the combined agent and model defaults.\n\nFor example, if you'd like to set the `temperature` setting to `0.0` to ensure less random behavior,\nyou can do the following:\n\n```py\nfrom pydantic_ai import Agent, ModelSettings\nfrom pydantic_ai.models.openai import OpenAIChatModel", "url": "https://ai.pydantic.dev/docs/agents/#model-run-settings", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "1. Model-level defaults", "anchor": "1-model-level-defaults", "heading_level": 1, "md_text": "model = OpenAIChatModel(\n    'gpt-5',\n    settings=ModelSettings(temperature=0.8, max_tokens=500)  # Base defaults\n)", "url": "https://ai.pydantic.dev/docs/agents/#1-model-level-defaults", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "2. Agent-level defaults (overrides model defaults by merging)", "anchor": "2-agent-level-defaults-overrides-model-defaults-by-merging", "heading_level": 1, "md_text": "agent = Agent(model, model_settings=ModelSettings(temperature=0.5))", "url": "https://ai.pydantic.dev/docs/agents/#2-agent-level-defaults-overrides-model-defaults-by-merging", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "3. Run-time overrides (highest priority)", "anchor": "3-run-time-overrides-highest-priority", "heading_level": 1, "md_text": "result_sync = agent.run_sync(\n    'What is the capital of Italy?',\n    model_settings=ModelSettings(temperature=0.0)  # Final temperature: 0.0\n)\nprint(result_sync.output)\n#> The capital of Italy is Rome.\n```\n\nThe final request uses `temperature=0.0` (run-time), `max_tokens=500` (from model), demonstrating how settings merge with run-time taking precedence.\n\n!!! note \"Model Settings Support\"\n    Model-level settings are supported by all concrete model implementations (OpenAI, Anthropic, Google, etc.). Wrapper models like `FallbackModel`, `WrapperModel`, and `InstrumentedModel` don't have their own settings - they use the settings of their underlying models.", "url": "https://ai.pydantic.dev/docs/agents/#3-run-time-overrides-highest-priority", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "Model specific settings", "anchor": "model-specific-settings", "heading_level": 3, "md_text": "If you wish to further customize model behavior, you can use a subclass of [`ModelSettings`][pydantic_ai.settings.ModelSettings], like\n[`GoogleModelSettings`][pydantic_ai.models.google.GoogleModelSettings], associated with your model of choice.\n\nFor example:\n\n```py\nfrom pydantic_ai import Agent, UnexpectedModelBehavior\nfrom pydantic_ai.models.google import GoogleModelSettings\n\nagent = Agent('google-gla:gemini-2.5-flash')\n\ntry:\n    result = agent.run_sync(\n        'Write a list of 5 very rude things that I might say to the universe after stubbing my toe in the dark:',\n        model_settings=GoogleModelSettings(\n            temperature=0.0,  # general model settings can also be specified\n            gemini_safety_settings=[\n                {\n                    'category': 'HARM_CATEGORY_HARASSMENT',\n                    'threshold': 'BLOCK_LOW_AND_ABOVE',\n                },\n                {\n                    'category': 'HARM_CATEGORY_HATE_SPEECH',\n                    'threshold': 'BLOCK_LOW_AND_ABOVE',\n                },\n            ],\n        ),\n    )\nexcept UnexpectedModelBehavior as e:\n    print(e)  # (1)!\n    \"\"\"\n    Content filter 'SAFETY' triggered, body:\n    <safety settings details>\n    \"\"\"\n```\n\n1. This error is raised because the safety thresholds were exceeded.", "url": "https://ai.pydantic.dev/docs/agents/#model-specific-settings", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "Runs vs. Conversations", "anchor": "runs-vs-conversations", "heading_level": 2, "md_text": "An agent **run** might represent an entire conversation \u2014 there's no limit to how many messages can be exchanged in a single run. However, a **conversation** might also be composed of multiple runs, especially if you need to maintain state between separate interactions or API calls.\n\nHere's an example of a conversation comprised of multiple runs:\n\n```python {title=\"conversation_example.py\" hl_lines=\"13\"}\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-5')", "url": "https://ai.pydantic.dev/docs/agents/#runs-vs-conversations", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "First run", "anchor": "first-run", "heading_level": 1, "md_text": "result1 = agent.run_sync('Who was Albert Einstein?')\nprint(result1.output)\n#> Albert Einstein was a German-born theoretical physicist.", "url": "https://ai.pydantic.dev/docs/agents/#first-run", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "Second run, passing previous messages", "anchor": "second-run-passing-previous-messages", "heading_level": 1, "md_text": "result2 = agent.run_sync(\n    'What was his most famous equation?',\n    message_history=result1.new_messages(),  # (1)!\n)\nprint(result2.output)\n#> Albert Einstein's most famous equation is (E = mc^2).\n```\n\n1. Continue the conversation; without `message_history` the model would not know who \"his\" was referring to.\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/agents/#second-run-passing-previous-messages", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "Type safe by design {#static-type-checking}", "anchor": "type-safe-by-design-static-type-checking", "heading_level": 2, "md_text": "Pydantic AI is designed to work well with static type checkers, like mypy and pyright.\n\n!!! tip \"Typing is (somewhat) optional\"\n    Pydantic AI is designed to make type checking as useful as possible for you if you choose to use it, but you don't have to use types everywhere all the time.\n\n    That said, because Pydantic AI uses Pydantic, and Pydantic uses type hints as the definition for schema and validation, some types (specifically type hints on parameters to tools, and the `output_type` arguments to [`Agent`][pydantic_ai.Agent]) are used at runtime.\n\n    We (the library developers) have messed up if type hints are confusing you more than helping you, if you find this, please create an [issue](https://github.com/pydantic/pydantic-ai/issues) explaining what's annoying you!\n\nIn particular, agents are generic in both the type of their dependencies and the type of the outputs they return, so you can use the type hints to ensure you're using the right types.\n\nConsider the following script with type mistakes:\n\n```python {title=\"type_mistakes.py\" hl_lines=\"18 28\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_ai import Agent, RunContext\n\n\n@dataclass\nclass User:\n    name: str\n\n\nagent = Agent(\n    'test',\n    deps_type=User,  # (1)!\n    output_type=bool,\n)\n\n\n@agent.system_prompt\ndef add_user_name(ctx: RunContext[str]) -> str:  # (2)!\n    return f\"The user's name is {ctx.deps}.\"\n\n\ndef foobar(x: bytes) -> None:\n    pass\n\n\nresult = agent.run_sync('Does their name start with \"A\"?', deps=User('Anne'))\nfoobar(result.output)  # (3)!\n```\n\n1. The agent is defined as expecting an instance of `User` as `deps`.\n2. But here `add_user_name` is defined as taking a `str` as the dependency, not a `User`.\n3. Since the agent is defined as returning a `bool`, this will raise a type error since `foobar` expects `bytes`.\n\nRunning `mypy` on this will give the following output:\n\n```bash\n\u27a4 uv run mypy type_mistakes.py\ntype_mistakes.py:18: error: Argument 1 to \"system_prompt\" of \"Agent\" has incompatible type \"Callable[[RunContext[str]], str]\"; expected \"Callable[[RunContext[User]], str]\"  [arg-type]\ntype_mistakes.py:28: error: Argument 1 to \"foobar\" has incompatible type \"bool\"; expected \"bytes\"  [arg-type]\nFound 2 errors in 1 file (checked 1 source file)\n```\n\nRunning `pyright` would identify the same issues.", "url": "https://ai.pydantic.dev/docs/agents/#type-safe-by-design-static-type-checking", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "System Prompts", "anchor": "system-prompts", "heading_level": 2, "md_text": "System prompts might seem simple at first glance since they're just strings (or sequences of strings that are concatenated), but crafting the right system prompt is key to getting the model to behave as you want.\n\n!!! tip\n    For most use cases, you should use `instructions` instead of \"system prompts\".\n\n    If you know what you are doing though and want to preserve system prompt messages in the message history sent to the\n    LLM in subsequent completions requests, you can achieve this using the `system_prompt` argument/decorator.\n\n    See the section below on [Instructions](#instructions) for more information.\n\nGenerally, system prompts fall into two categories:\n\n1. **Static system prompts**: These are known when writing the code and can be defined via the `system_prompt` parameter of the [`Agent` constructor][pydantic_ai.Agent.__init__].\n2. **Dynamic system prompts**: These depend in some way on context that isn't known until runtime, and should be defined via functions decorated with [`@agent.system_prompt`][pydantic_ai.Agent.system_prompt].\n\nYou can add both to a single agent; they're appended in the order they're defined at runtime.\n\nHere's an example using both types of system prompts:\n\n```python {title=\"system_prompts.py\"}\nfrom datetime import date\n\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent(\n    'openai:gpt-5',\n    deps_type=str,  # (1)!\n    system_prompt=\"Use the customer's name while replying to them.\",  # (2)!\n)\n\n\n@agent.system_prompt  # (3)!\ndef add_the_users_name(ctx: RunContext[str]) -> str:\n    return f\"The user's name is {ctx.deps}.\"\n\n\n@agent.system_prompt\ndef add_the_date() -> str:  # (4)!\n    return f'The date is {date.today()}.'\n\n\nresult = agent.run_sync('What is the date?', deps='Frank')\nprint(result.output)\n#> Hello Frank, the date today is 2032-01-02.\n```\n\n1. The agent expects a string dependency.\n2. Static system prompt defined at agent creation time.\n3. Dynamic system prompt defined via a decorator with [`RunContext`][pydantic_ai.tools.RunContext], this is called just after `run_sync`, not when the agent is created, so can benefit from runtime information like the dependencies used on that run.\n4. Another dynamic system prompt, system prompts don't have to have the `RunContext` parameter.\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/agents/#system-prompts", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "Instructions", "anchor": "instructions", "heading_level": 2, "md_text": "Instructions are similar to system prompts. The main difference is that when an explicit `message_history` is provided\nin a call to `Agent.run` and similar methods, _instructions_ from any existing messages in the history are not included\nin the request to the model \u2014 only the instructions of the _current_ agent are included.\n\nYou should use:\n\n- `instructions` when you want your request to the model to only include system prompts for the _current_ agent\n- `system_prompt` when you want your request to the model to _retain_ the system prompts used in previous requests (possibly made using other agents)\n\nIn general, we recommend using `instructions` instead of `system_prompt` unless you have a specific reason to use `system_prompt`.\n\nInstructions, like system prompts, fall into two categories:\n\n1. **Static instructions**: These are known when writing the code and can be defined via the `instructions` parameter of the [`Agent` constructor][pydantic_ai.Agent.__init__].\n2. **Dynamic instructions**: These rely on context that is only available at runtime and should be defined using functions decorated with [`@agent.instructions`][pydantic_ai.Agent.instructions]. Unlike dynamic system prompts, which may be reused when `message_history` is present, dynamic instructions are always reevaluated.\n\nBoth static and dynamic instructions can be added to a single agent, and they are appended in the order they are defined at runtime.\n\nHere's an example using both types of instructions:\n\n```python {title=\"instructions.py\"}\nfrom datetime import date\n\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent(\n    'openai:gpt-5',\n    deps_type=str,  # (1)!\n    instructions=\"Use the customer's name while replying to them.\",  # (2)!\n)\n\n\n@agent.instructions  # (3)!\ndef add_the_users_name(ctx: RunContext[str]) -> str:\n    return f\"The user's name is {ctx.deps}.\"\n\n\n@agent.instructions\ndef add_the_date() -> str:  # (4)!\n    return f'The date is {date.today()}.'\n\n\nresult = agent.run_sync('What is the date?', deps='Frank')\nprint(result.output)\n#> Hello Frank, the date today is 2032-01-02.\n```\n\n1. The agent expects a string dependency.\n2. Static instructions defined at agent creation time.\n3. Dynamic instructions defined via a decorator with [`RunContext`][pydantic_ai.tools.RunContext],\n   this is called just after `run_sync`, not when the agent is created, so can benefit from runtime\n   information like the dependencies used on that run.\n4. Another dynamic instruction, instructions don't have to have the `RunContext` parameter.\n\n_(This example is complete, it can be run \"as is\")_\n\nNote that returning an empty string will result in no instruction message added.", "url": "https://ai.pydantic.dev/docs/agents/#instructions", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "Reflection and self-correction", "anchor": "reflection-and-self-correction", "heading_level": 2, "md_text": "Validation errors from both function tool parameter validation and [structured output validation](output.md#structured-output) can be passed back to the model with a request to retry.\n\nYou can also raise [`ModelRetry`][pydantic_ai.exceptions.ModelRetry] from within a [tool](tools.md) or [output function](output.md#output-functions) to tell the model it should retry generating a response.\n\n- The default retry count is **1** but can be altered for the [entire agent][pydantic_ai.Agent.__init__], a [specific tool][pydantic_ai.Agent.tool], or [outputs][pydantic_ai.Agent.__init__].\n- You can access the current retry count from within a tool or output function via [`ctx.retry`][pydantic_ai.tools.RunContext].\n\nHere's an example:\n\n```python {title=\"tool_retry.py\"}\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, RunContext, ModelRetry\n\nfrom fake_database import DatabaseConn\n\n\nclass ChatResult(BaseModel):\n    user_id: int\n    message: str\n\n\nagent = Agent(\n    'openai:gpt-5',\n    deps_type=DatabaseConn,\n    output_type=ChatResult,\n)\n\n\n@agent.tool(retries=2)\ndef get_user_by_name(ctx: RunContext[DatabaseConn], name: str) -> int:\n    \"\"\"Get a user's ID from their full name.\"\"\"\n    print(name)\n    #> John\n    #> John Doe\n    user_id = ctx.deps.users.get(name=name)\n    if user_id is None:\n        raise ModelRetry(\n            f'No user found with name {name!r}, remember to provide their full name'\n        )\n    return user_id\n\n\nresult = agent.run_sync(\n    'Send a message to John Doe asking for coffee next week', deps=DatabaseConn()\n)\nprint(result.output)\n\"\"\"\nuser_id=123 message='Hello John, would you be free for coffee sometime next week? Let me know what works for you!'\n\"\"\"\n```", "url": "https://ai.pydantic.dev/docs/agents/#reflection-and-self-correction", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "Model errors", "anchor": "model-errors", "heading_level": 2, "md_text": "If models behave unexpectedly (e.g., the retry limit is exceeded, or their API returns `503`), agent runs will raise [`UnexpectedModelBehavior`][pydantic_ai.exceptions.UnexpectedModelBehavior].\n\nIn these cases, [`capture_run_messages`][pydantic_ai.capture_run_messages] can be used to access the messages exchanged during the run to help diagnose the issue.\n\n```python {title=\"agent_model_errors.py\"}\nfrom pydantic_ai import Agent, ModelRetry, UnexpectedModelBehavior, capture_run_messages\n\nagent = Agent('openai:gpt-5')\n\n\n@agent.tool_plain\ndef calc_volume(size: int) -> int:  # (1)!\n    if size == 42:\n        return size**3\n    else:\n        raise ModelRetry('Please try again.')\n\n\nwith capture_run_messages() as messages:  # (2)!\n    try:\n        result = agent.run_sync('Please get me the volume of a box with size 6.')\n    except UnexpectedModelBehavior as e:\n        print('An error occurred:', e)\n        #> An error occurred: Tool 'calc_volume' exceeded max retries count of 1\n        print('cause:', repr(e.__cause__))\n        #> cause: ModelRetry('Please try again.')\n        print('messages:', messages)\n        \"\"\"\n        messages:\n        [\n            ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='Please get me the volume of a box with size 6.',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            ),\n            ModelResponse(\n                parts=[\n                    ToolCallPart(\n                        tool_name='calc_volume',\n                        args={'size': 6},\n                        tool_call_id='pyd_ai_tool_call_id',\n                    )\n                ],\n                usage=RequestUsage(input_tokens=62, output_tokens=4),\n                model_name='gpt-5',\n                timestamp=datetime.datetime(...),\n            ),\n            ModelRequest(\n                parts=[\n                    RetryPromptPart(\n                        content='Please try again.',\n                        tool_name='calc_volume',\n                        tool_call_id='pyd_ai_tool_call_id',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            ),\n            ModelResponse(\n                parts=[\n                    ToolCallPart(\n                        tool_name='calc_volume',\n                        args={'size': 6},\n                        tool_call_id='pyd_ai_tool_call_id',\n                    )\n                ],\n                usage=RequestUsage(input_tokens=72, output_tokens=8),\n                model_name='gpt-5',\n                timestamp=datetime.datetime(...),\n            ),\n        ]\n        \"\"\"\n    else:\n        print(result.output)\n```\n\n1. Define a tool that will raise `ModelRetry` repeatedly in this case.\n2. [`capture_run_messages`][pydantic_ai.capture_run_messages] is used to capture the messages exchanged during the run.\n\n_(This example is complete, it can be run \"as is\")_\n\n!!! note\n    If you call [`run`][pydantic_ai.agent.AbstractAgent.run], [`run_sync`][pydantic_ai.agent.AbstractAgent.run_sync], or [`run_stream`][pydantic_ai.agent.AbstractAgent.run_stream] more than once within a single `capture_run_messages` context, `messages` will represent the messages exchanged during the first call only.", "url": "https://ai.pydantic.dev/docs/agents/#model-errors", "page": "docs/agents", "source_site": "pydantic_ai"}
{"title": "Graphs", "anchor": "graphs", "heading_level": 1, "md_text": "!!! danger \"Don't use a nail gun unless you need a nail gun\"\n    If Pydantic AI [agents](agents.md) are a hammer, and [multi-agent workflows](multi-agent-applications.md) are a sledgehammer, then graphs are a nail gun:\n\n    * sure, nail guns look cooler than hammers\n    * but nail guns take a lot more setup than hammers\n    * and nail guns don't make you a better builder, they make you a builder with a nail gun\n    * Lastly, (and at the risk of torturing this metaphor), if you're a fan of medieval tools like mallets and untyped Python, you probably won't like nail guns or our approach to graphs. (But then again, if you're not a fan of type hints in Python, you've probably already bounced off Pydantic AI to use one of the toy agent frameworks \u2014 good luck, and feel free to borrow my sledgehammer when you realize you need it)\n\n    In short, graphs are a powerful tool, but they're not the right tool for every job. Please consider other [multi-agent approaches](multi-agent-applications.md) before proceeding.\n\n    If you're not confident a graph-based approach is a good idea, it might be unnecessary.\n\nGraphs and finite state machines (FSMs) are a powerful abstraction to model, execute, control and visualize complex workflows.\n\nAlongside Pydantic AI, we've developed `pydantic-graph` \u2014 an async graph and state machine library for Python where nodes and edges are defined using type hints.\n\nWhile this library is developed as part of Pydantic AI; it has no dependency on `pydantic-ai` and can be considered as a pure graph-based state machine library. You may find it useful whether or not you're using Pydantic AI or even building with GenAI.\n\n`pydantic-graph` is designed for advanced users and makes heavy use of Python generics and type hints. It is not designed to be as beginner-friendly as Pydantic AI.", "url": "https://ai.pydantic.dev/docs/graph/#graphs", "page": "docs/graph", "source_site": "pydantic_ai"}
{"title": "Installation", "anchor": "installation", "heading_level": 2, "md_text": "`pydantic-graph` is a required dependency of `pydantic-ai`, and an optional dependency of `pydantic-ai-slim`, see [installation instructions](install.md#slim-install) for more information. You can also install it directly:\n\n```bash\npip/uv-add pydantic-graph\n```", "url": "https://ai.pydantic.dev/docs/graph/#installation", "page": "docs/graph", "source_site": "pydantic_ai"}
{"title": "Graph Types", "anchor": "graph-types", "heading_level": 2, "md_text": "`pydantic-graph` is made up of a few key components:", "url": "https://ai.pydantic.dev/docs/graph/#graph-types", "page": "docs/graph", "source_site": "pydantic_ai"}
{"title": "GraphRunContext", "anchor": "graphruncontext", "heading_level": 3, "md_text": "[`GraphRunContext`][pydantic_graph.nodes.GraphRunContext] \u2014 The context for the graph run, similar to Pydantic AI's [`RunContext`][pydantic_ai.tools.RunContext]. This holds the state of the graph and dependencies and is passed to nodes when they're run.\n\n`GraphRunContext` is generic in the state type of the graph it's used in, [`StateT`][pydantic_graph.nodes.StateT].", "url": "https://ai.pydantic.dev/docs/graph/#graphruncontext", "page": "docs/graph", "source_site": "pydantic_ai"}
{"title": "End", "anchor": "end", "heading_level": 3, "md_text": "[`End`][pydantic_graph.nodes.End] \u2014 return value to indicate the graph run should end.\n\n`End` is generic in the graph return type of the graph it's used in, [`RunEndT`][pydantic_graph.nodes.RunEndT].", "url": "https://ai.pydantic.dev/docs/graph/#end", "page": "docs/graph", "source_site": "pydantic_ai"}
{"title": "Nodes", "anchor": "nodes", "heading_level": 3, "md_text": "Subclasses of [`BaseNode`][pydantic_graph.nodes.BaseNode] define nodes for execution in the graph.\n\nNodes, which are generally [`dataclass`es][dataclasses.dataclass], generally consist of:\n\n- fields containing any parameters required/optional when calling the node\n- the business logic to execute the node, in the [`run`][pydantic_graph.nodes.BaseNode.run] method\n- return annotations of the [`run`][pydantic_graph.nodes.BaseNode.run] method, which are read by `pydantic-graph` to determine the outgoing edges of the node\n\nNodes are generic in:\n\n- **state**, which must have the same type as the state of graphs they're included in, [`StateT`][pydantic_graph.nodes.StateT] has a default of `None`, so if you're not using state you can omit this generic parameter, see [stateful graphs](#stateful-graphs) for more information\n- **deps**, which must have the same type as the deps of the graph they're included in, [`DepsT`][pydantic_graph.nodes.DepsT] has a default of `None`, so if you're not using deps you can omit this generic parameter, see [dependency injection](#dependency-injection) for more information\n- **graph return type** \u2014 this only applies if the node returns [`End`][pydantic_graph.nodes.End]. [`RunEndT`][pydantic_graph.nodes.RunEndT] has a default of [Never][typing.Never] so this generic parameter can be omitted if the node doesn't return `End`, but must be included if it does.\n\nHere's an example of a start or intermediate node in a graph \u2014 it can't end the run as it doesn't return [`End`][pydantic_graph.nodes.End]:\n\n```py {title=\"intermediate_node.py\" noqa=\"F821\" test=\"skip\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph import BaseNode, GraphRunContext\n\n\n@dataclass\nclass MyNode(BaseNode[MyState]):  # (1)!\n    foo: int  # (2)!\n\n    async def run(\n        self,\n        ctx: GraphRunContext[MyState],  # (3)!\n    ) -> AnotherNode:  # (4)!\n        ...\n        return AnotherNode()\n```\n\n1. State in this example is `MyState` (not shown), hence `BaseNode` is parameterized with `MyState`. This node can't end the run, so the `RunEndT` generic parameter is omitted and defaults to `Never`.\n2. `MyNode` is a dataclass and has a single field `foo`, an `int`.\n3. The `run` method takes a `GraphRunContext` parameter, again parameterized with state `MyState`.\n4. The return type of the `run` method is `AnotherNode` (not shown), this is used to determine the outgoing edges of the node.\n\nWe could extend `MyNode` to optionally end the run if `foo` is divisible by 5:\n\n```py {title=\"intermediate_or_end_node.py\" hl_lines=\"7 13 15\" noqa=\"F821\" test=\"skip\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph import BaseNode, End, GraphRunContext\n\n\n@dataclass\nclass MyNode(BaseNode[MyState, None, int]):  # (1)!\n    foo: int\n\n    async def run(\n        self,\n        ctx: GraphRunContext[MyState],\n    ) -> AnotherNode | End[int]:  # (2)!\n        if self.foo % 5 == 0:\n            return End(self.foo)\n        else:\n            return AnotherNode()\n```\n\n1. We parameterize the node with the return type (`int` in this case) as well as state. Because generic parameters are positional-only, we have to include `None` as the second parameter representing deps.\n2. The return type of the `run` method is now a union of `AnotherNode` and `End[int]`, this allows the node to end the run if `foo` is divisible by 5.", "url": "https://ai.pydantic.dev/docs/graph/#nodes", "page": "docs/graph", "source_site": "pydantic_ai"}
{"title": "Graph", "anchor": "graph", "heading_level": 3, "md_text": "[`Graph`][pydantic_graph.graph.Graph] \u2014 this is the execution graph itself, made up of a set of [node classes](#nodes) (i.e., `BaseNode` subclasses).\n\n`Graph` is generic in:\n\n- **state** the state type of the graph, [`StateT`][pydantic_graph.nodes.StateT]\n- **deps** the deps type of the graph, [`DepsT`][pydantic_graph.nodes.DepsT]\n- **graph return type** the return type of the graph run, [`RunEndT`][pydantic_graph.nodes.RunEndT]\n\nHere's an example of a simple graph:\n\n```py {title=\"graph_example.py\"}\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\n\nfrom pydantic_graph import BaseNode, End, Graph, GraphRunContext\n\n\n@dataclass\nclass DivisibleBy5(BaseNode[None, None, int]):  # (1)!\n    foo: int\n\n    async def run(\n        self,\n        ctx: GraphRunContext,\n    ) -> Increment | End[int]:\n        if self.foo % 5 == 0:\n            return End(self.foo)\n        else:\n            return Increment(self.foo)\n\n\n@dataclass\nclass Increment(BaseNode):  # (2)!\n    foo: int\n\n    async def run(self, ctx: GraphRunContext) -> DivisibleBy5:\n        return DivisibleBy5(self.foo + 1)\n\n\nfives_graph = Graph(nodes=[DivisibleBy5, Increment])  # (3)!\nresult = fives_graph.run_sync(DivisibleBy5(4))  # (4)!\nprint(result.output)\n#> 5\n```\n\n1. The `DivisibleBy5` node is parameterized with `None` for the state param and `None` for the deps param as this graph doesn't use state or deps, and `int` as it can end the run.\n2. The `Increment` node doesn't return `End`, so the `RunEndT` generic parameter is omitted, state can also be omitted as the graph doesn't use state.\n3. The graph is created with a sequence of nodes.\n4. The graph is run synchronously with [`run_sync`][pydantic_graph.graph.Graph.run_sync]. The initial node is `DivisibleBy5(4)`. Because the graph doesn't use external state or deps, we don't pass `state` or `deps`.\n\n_(This example is complete, it can be run \"as is\")_\n\nA [mermaid diagram](#mermaid-diagrams) for this graph can be generated with the following code:\n\n```py {title=\"graph_example_diagram.py\" requires=\"graph_example.py\"}\nfrom graph_example import DivisibleBy5, fives_graph\n\nfives_graph.mermaid_code(start_node=DivisibleBy5)\n```\n\n```mermaid\n---\ntitle: fives_graph\n---\nstateDiagram-v2\n  [*] --> DivisibleBy5\n  DivisibleBy5 --> Increment\n  DivisibleBy5 --> [*]\n  Increment --> DivisibleBy5\n```\n\nIn order to visualize a graph within a `jupyter-notebook`, `IPython.display` needs to be used:\n\n```python {title=\"jupyter_display_mermaid.py\"  test=\"skip\"}\nfrom graph_example import DivisibleBy5, fives_graph\nfrom IPython.display import Image, display\n\ndisplay(Image(fives_graph.mermaid_image(start_node=DivisibleBy5)))\n```", "url": "https://ai.pydantic.dev/docs/graph/#graph", "page": "docs/graph", "source_site": "pydantic_ai"}
{"title": "Stateful Graphs", "anchor": "stateful-graphs", "heading_level": 2, "md_text": "The \"state\" concept in `pydantic-graph` provides an optional way to access and mutate an object (often a `dataclass` or Pydantic model) as nodes run in a graph. If you think of Graphs as a production line, then your state is the engine being passed along the line and built up by each node as the graph is run.\n\n`pydantic-graph` provides state persistence, with the state recorded after each node is run. (See [State Persistence](#state-persistence).)\n\nHere's an example of a graph which represents a vending machine where the user may insert coins and select a product to purchase.\n\n```python {title=\"vending_machine.py\"}\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\n\nfrom rich.prompt import Prompt\n\nfrom pydantic_graph import BaseNode, End, Graph, GraphRunContext\n\n\n@dataclass\nclass MachineState:  # (1)!\n    user_balance: float = 0.0\n    product: str | None = None\n\n\n@dataclass\nclass InsertCoin(BaseNode[MachineState]):  # (3)!\n    async def run(self, ctx: GraphRunContext[MachineState]) -> CoinsInserted:  # (16)!\n        return CoinsInserted(float(Prompt.ask('Insert coins')))  # (4)!\n\n\n@dataclass\nclass CoinsInserted(BaseNode[MachineState]):\n    amount: float  # (5)!\n\n    async def run(\n        self, ctx: GraphRunContext[MachineState]\n    ) -> SelectProduct | Purchase:  # (17)!\n        ctx.state.user_balance += self.amount  # (6)!\n        if ctx.state.product is not None:  # (7)!\n            return Purchase(ctx.state.product)\n        else:\n            return SelectProduct()\n\n\n@dataclass\nclass SelectProduct(BaseNode[MachineState]):\n    async def run(self, ctx: GraphRunContext[MachineState]) -> Purchase:\n        return Purchase(Prompt.ask('Select product'))\n\n\nPRODUCT_PRICES = {  # (2)!\n    'water': 1.25,\n    'soda': 1.50,\n    'crisps': 1.75,\n    'chocolate': 2.00,\n}\n\n\n@dataclass\nclass Purchase(BaseNode[MachineState, None, None]):  # (18)!\n    product: str\n\n    async def run(\n        self, ctx: GraphRunContext[MachineState]\n    ) -> End | InsertCoin | SelectProduct:\n        if price := PRODUCT_PRICES.get(self.product):  # (8)!\n            ctx.state.product = self.product  # (9)!\n            if ctx.state.user_balance >= price:  # (10)!\n                ctx.state.user_balance -= price\n                return End(None)\n            else:\n                diff = price - ctx.state.user_balance\n                print(f'Not enough money for {self.product}, need {diff:0.2f} more')\n                #> Not enough money for crisps, need 0.75 more\n                return InsertCoin()  # (11)!\n        else:\n            print(f'No such product: {self.product}, try again')\n            return SelectProduct()  # (12)!\n\n\nvending_machine_graph = Graph(  # (13)!\n    nodes=[InsertCoin, CoinsInserted, SelectProduct, Purchase]\n)\n\n\nasync def main():\n    state = MachineState()  # (14)!\n    await vending_machine_graph.run(InsertCoin(), state=state)  # (15)!\n    print(f'purchase successful item={state.product} change={state.user_balance:0.2f}')\n    #> purchase successful item=crisps change=0.25\n```", "url": "https://ai.pydantic.dev/docs/graph/#stateful-graphs", "page": "docs/graph", "source_site": "pydantic_ai"}
{"title": "Stateful Graphs", "anchor": "stateful-graphs", "heading_level": 2, "md_text": "1. The state of the vending machine is defined as a dataclass with the user's balance and the product they've selected, if any.\n2. A dictionary of products mapped to prices.\n3. The `InsertCoin` node, [`BaseNode`][pydantic_graph.nodes.BaseNode] is parameterized with `MachineState` as that's the state used in this graph.\n4. The `InsertCoin` node prompts the user to insert coins. We keep things simple by just entering a monetary amount as a float. Before you start thinking this is a toy too since it's using [rich's `Prompt.ask`][rich.prompt.PromptBase.ask] within nodes, see [below](#example-human-in-the-loop) for how control flow can be managed when nodes require external input.\n5. The `CoinsInserted` node; again this is a [`dataclass`][dataclasses.dataclass] with one field `amount`.\n6. Update the user's balance with the amount inserted.\n7. If the user has already selected a product, go to `Purchase`, otherwise go to `SelectProduct`.\n8. In the `Purchase` node, look up the price of the product if the user entered a valid product.\n9. If the user did enter a valid product, set the product in the state so we don't revisit `SelectProduct`.\n10. If the balance is enough to purchase the product, adjust the balance to reflect the purchase and return [`End`][pydantic_graph.nodes.End] to end the graph. We're not using the run return type, so we call `End` with `None`.\n11. If the balance is insufficient, go to `InsertCoin` to prompt the user to insert more coins.\n12. If the product is invalid, go to `SelectProduct` to prompt the user to select a product again.\n13. The graph is created by passing a list of nodes to [`Graph`][pydantic_graph.graph.Graph]. Order of nodes is not important, but it can affect how [diagrams](#mermaid-diagrams) are displayed.\n14. Initialize the state. This will be passed to the graph run and mutated as the graph runs.\n15. Run the graph with the initial state. Since the graph can be run from any node, we must pass the start node \u2014 in this case, `InsertCoin`. [`Graph.run`][pydantic_graph.graph.Graph.run] returns a [`GraphRunResult`][pydantic_graph.graph.GraphRunResult] that provides the final data and a history of the run.\n16. The return type of the node's [`run`][pydantic_graph.nodes.BaseNode.run] method is important as it is used to determine the outgoing edges of the node. This information in turn is used to render [mermaid diagrams](#mermaid-diagrams) and is enforced at runtime to detect misbehavior as soon as possible.\n17. The return type of `CoinsInserted`'s [`run`][pydantic_graph.nodes.BaseNode.run] method is a union, meaning multiple outgoing edges are possible.\n18. Unlike other nodes, `Purchase` can end the run, so the [`RunEndT`][pydantic_graph.nodes.RunEndT] generic parameter must be set. In this case it's `None` since the graph run return type is `None`.\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_\n\nA [mermaid diagram](#mermaid-diagrams) for this graph can be generated with the following code:\n\n```py {title=\"vending_machine_diagram.py\" requires=\"vending_machine.py\"}\nfrom vending_machine import InsertCoin, vending_machine_graph\n\nvending_machine_graph.mermaid_code(start_node=InsertCoin)\n```\n\nThe diagram generated by the above code is:\n\n```mermaid\n---\ntitle: vending_machine_graph\n---\nstateDiagram-v2\n  [*] --> InsertCoin\n  InsertCoin --> CoinsInserted\n  CoinsInserted --> SelectProduct\n  CoinsInserted --> Purchase\n  SelectProduct --> Purchase\n  Purchase --> InsertCoin\n  Purchase --> SelectProduct\n  Purchase --> [*]\n```\n\nSee [below](#mermaid-diagrams) for more information on generating diagrams.", "url": "https://ai.pydantic.dev/docs/graph/#stateful-graphs", "page": "docs/graph", "source_site": "pydantic_ai"}
{"title": "GenAI Example", "anchor": "genai-example", "heading_level": 2, "md_text": "So far we haven't shown an example of a Graph that actually uses Pydantic AI or GenAI at all.\n\nIn this example, one agent generates a welcome email to a user and the other agent provides feedback on the email.\n\nThis graph has a very simple structure:\n\n```mermaid\n---\ntitle: feedback_graph\n---\nstateDiagram-v2\n  [*] --> WriteEmail\n  WriteEmail --> Feedback\n  Feedback --> WriteEmail\n  Feedback --> [*]\n```\n\n```python {title=\"genai_email_feedback.py\"}\nfrom __future__ import annotations as _annotations\n\nfrom dataclasses import dataclass, field\n\nfrom pydantic import BaseModel, EmailStr\n\nfrom pydantic_ai import Agent, ModelMessage, format_as_xml\nfrom pydantic_graph import BaseNode, End, Graph, GraphRunContext\n\n\n@dataclass\nclass User:\n    name: str\n    email: EmailStr\n    interests: list[str]\n\n\n@dataclass\nclass Email:\n    subject: str\n    body: str\n\n\n@dataclass\nclass State:\n    user: User\n    write_agent_messages: list[ModelMessage] = field(default_factory=list)\n\n\nemail_writer_agent = Agent(\n    'google-gla:gemini-2.5-pro',\n    output_type=Email,\n    system_prompt='Write a welcome email to our tech blog.',\n)\n\n\n@dataclass\nclass WriteEmail(BaseNode[State]):\n    email_feedback: str | None = None\n\n    async def run(self, ctx: GraphRunContext[State]) -> Feedback:\n        if self.email_feedback:\n            prompt = (\n                f'Rewrite the email for the user:\\n'\n                f'{format_as_xml(ctx.state.user)}\\n'\n                f'Feedback: {self.email_feedback}'\n            )\n        else:\n            prompt = (\n                f'Write a welcome email for the user:\\n'\n                f'{format_as_xml(ctx.state.user)}'\n            )\n\n        result = await email_writer_agent.run(\n            prompt,\n            message_history=ctx.state.write_agent_messages,\n        )\n        ctx.state.write_agent_messages += result.new_messages()\n        return Feedback(result.output)\n\n\nclass EmailRequiresWrite(BaseModel):\n    feedback: str\n\n\nclass EmailOk(BaseModel):\n    pass\n\n\nfeedback_agent = Agent[None, EmailRequiresWrite | EmailOk](\n    'openai:gpt-5',\n    output_type=EmailRequiresWrite | EmailOk,  # type: ignore\n    system_prompt=(\n        'Review the email and provide feedback, email must reference the users specific interests.'\n    ),\n)\n\n\n@dataclass\nclass Feedback(BaseNode[State, None, Email]):\n    email: Email\n\n    async def run(\n        self,\n        ctx: GraphRunContext[State],\n    ) -> WriteEmail | End[Email]:\n        prompt = format_as_xml({'user': ctx.state.user, 'email': self.email})\n        result = await feedback_agent.run(prompt)\n        if isinstance(result.output, EmailRequiresWrite):\n            return WriteEmail(email_feedback=result.output.feedback)\n        else:\n            return End(self.email)\n\n\nasync def main():\n    user = User(\n        name='John Doe',\n        email='john.joe@example.com',\n        interests=['Haskel', 'Lisp', 'Fortran'],\n    )\n    state = State(user)\n    feedback_graph = Graph(nodes=(WriteEmail, Feedback))\n    result = await feedback_graph.run(WriteEmail(), state=state)\n    print(result.output)\n    \"\"\"\n    Email(\n        subject='Welcome to our tech blog!',\n        body='Hello John, Welcome to our tech blog! ...',\n    )\n    \"\"\"\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/#genai-example", "page": "docs/graph", "source_site": "pydantic_ai"}
{"title": "Using `Graph.iter` for `async for` iteration", "anchor": "using-graphiter-for-async-for-iteration", "heading_level": 3, "md_text": "Sometimes you want direct control or insight into each node as the graph executes. The easiest way to do that is with the [`Graph.iter`][pydantic_graph.graph.Graph.iter] method, which returns a **context manager** that yields a [`GraphRun`][pydantic_graph.graph.GraphRun] object. The `GraphRun` is an async-iterable over the nodes of your graph, allowing you to record or modify them as they execute.\n\nHere's an example:\n\n```python {title=\"count_down.py\" noqa=\"I001\"}\nfrom __future__ import annotations as _annotations\n\nfrom dataclasses import dataclass\nfrom pydantic_graph import Graph, BaseNode, End, GraphRunContext\n\n\n@dataclass\nclass CountDownState:\n    counter: int\n\n\n@dataclass\nclass CountDown(BaseNode[CountDownState, None, int]):\n    async def run(self, ctx: GraphRunContext[CountDownState]) -> CountDown | End[int]:\n        if ctx.state.counter <= 0:\n            return End(ctx.state.counter)\n        ctx.state.counter -= 1\n        return CountDown()\n\n\ncount_down_graph = Graph(nodes=[CountDown])\n\n\nasync def main():\n    state = CountDownState(counter=3)\n    async with count_down_graph.iter(CountDown(), state=state) as run:  # (1)!\n        async for node in run:  # (2)!\n            print('Node:', node)\n            #> Node: CountDown()\n            #> Node: CountDown()\n            #> Node: CountDown()\n            #> Node: CountDown()\n            #> Node: End(data=0)\n    print('Final output:', run.result.output)  # (3)!\n    #> Final output: 0\n```\n\n1. `Graph.iter(...)` returns a [`GraphRun`][pydantic_graph.graph.GraphRun].\n2. Here, we step through each node as it is executed.\n3. Once the graph returns an [`End`][pydantic_graph.nodes.End], the loop ends, and `run.result` becomes a [`GraphRunResult`][pydantic_graph.graph.GraphRunResult] containing the final outcome (`0` here).", "url": "https://ai.pydantic.dev/docs/graph/#using-graphiter-for-async-for-iteration", "page": "docs/graph", "source_site": "pydantic_ai"}
{"title": "Using `GraphRun.next(node)` manually", "anchor": "using-graphrunnextnode-manually", "heading_level": 3, "md_text": "Alternatively, you can drive iteration manually with the [`GraphRun.next`][pydantic_graph.graph.GraphRun.next] method, which allows you to pass in whichever node you want to run next. You can modify or selectively skip nodes this way.\n\nBelow is a contrived example that stops whenever the counter is at 2, ignoring any node runs beyond that:\n\n```python {title=\"count_down_next.py\" noqa=\"I001\" requires=\"count_down.py\"}\nfrom pydantic_graph import End, FullStatePersistence\nfrom count_down import CountDown, CountDownState, count_down_graph\n\n\nasync def main():\n    state = CountDownState(counter=5)\n    persistence = FullStatePersistence()  # (7)!\n    async with count_down_graph.iter(\n        CountDown(), state=state, persistence=persistence\n    ) as run:\n        node = run.next_node  # (1)!\n        while not isinstance(node, End):  # (2)!\n            print('Node:', node)\n            #> Node: CountDown()\n            #> Node: CountDown()\n            #> Node: CountDown()\n            #> Node: CountDown()\n            if state.counter == 2:\n                break  # (3)!\n            node = await run.next(node)  # (4)!\n\n        print(run.result)  # (5)!\n        #> None\n\n        for step in persistence.history:  # (6)!\n            print('History Step:', step.state, step.state)\n            #> History Step: CountDownState(counter=5) CountDownState(counter=5)\n            #> History Step: CountDownState(counter=4) CountDownState(counter=4)\n            #> History Step: CountDownState(counter=3) CountDownState(counter=3)\n            #> History Step: CountDownState(counter=2) CountDownState(counter=2)\n```\n\n1. We start by grabbing the first node that will be run in the agent's graph.\n2. The agent run is finished once an `End` node has been produced; instances of `End` cannot be passed to `next`.\n3. If the user decides to stop early, we break out of the loop. The graph run won't have a real final result in that case (`run.result` remains `None`).\n4. At each step, we call `await run.next(node)` to run it and get the next node (or an `End`).\n5. Because we did not continue the run until it finished, the `result` is not set.\n6. The run's history is still populated with the steps we executed so far.\n7. Use [`FullStatePersistence`][pydantic_graph.FullStatePersistence] so we can show the history of the run, see [State Persistence](#state-persistence) below for more information.", "url": "https://ai.pydantic.dev/docs/graph/#using-graphrunnextnode-manually", "page": "docs/graph", "source_site": "pydantic_ai"}
{"title": "State Persistence", "anchor": "state-persistence", "heading_level": 2, "md_text": "One of the biggest benefits of finite state machine (FSM) graphs is how they simplify the handling of interrupted execution. This might happen for a variety of reasons:\n\n- the state machine logic might fundamentally need to be paused \u2014 e.g. the returns workflow for an e-commerce order needs to wait for the item to be posted to the returns center or because execution of the next node needs input from a user so needs to wait for a new http request,\n- the execution takes so long that the entire graph can't reliably be executed in a single continuous run \u2014 e.g. a deep research agent that might take hours to run,\n- you want to run multiple graph nodes in parallel in different processes / hardware instances (note: parallel node execution is not yet supported in `pydantic-graph`, see [#704](https://github.com/pydantic/pydantic-ai/issues/704)).\n\nTrying to make a conventional control flow (i.e., boolean logic and nested function calls) implementation compatible with these usage scenarios generally results in brittle and over-complicated spaghetti code, with the logic required to interrupt and resume execution dominating the implementation.\n\nTo allow graph runs to be interrupted and resumed, `pydantic-graph` provides state persistence \u2014 a system for snapshotting the state of a graph run before and after each node is run, allowing a graph run to be resumed from any point in the graph.\n\n`pydantic-graph` includes three state persistence implementations:\n\n- [`SimpleStatePersistence`][pydantic_graph.SimpleStatePersistence] \u2014 Simple in memory state persistence that just hold the latest snapshot. If no state persistence implementation is provided when running a graph, this is used by default.\n- [`FullStatePersistence`][pydantic_graph.FullStatePersistence] \u2014 In memory state persistence that hold a list of snapshots.\n- [`FileStatePersistence`][pydantic_graph.persistence.file.FileStatePersistence] \u2014 File-based state persistence that saves snapshots to a JSON file.\n\nIn production applications, developers should implement their own state persistence by subclassing [`BaseStatePersistence`][pydantic_graph.persistence.BaseStatePersistence] abstract base class, which might persist runs in a relational database like PostgresQL.\n\nAt a high level the role of `StatePersistence` implementations is to store and retrieve [`NodeSnapshot`][pydantic_graph.persistence.NodeSnapshot] and [`EndSnapshot`][pydantic_graph.persistence.EndSnapshot] objects.\n\n[`graph.iter_from_persistence()`][pydantic_graph.graph.Graph.iter_from_persistence] may be used to run the graph based on the state stored in persistence.\n\nWe can run the `count_down_graph` from [above](#iterating-over-a-graph), using [`graph.iter_from_persistence()`][pydantic_graph.graph.Graph.iter_from_persistence] and [`FileStatePersistence`][pydantic_graph.persistence.file.FileStatePersistence].\n\nAs you can see in this code, `run_node` requires no external application state (apart from state persistence) to be run, meaning graphs can easily be executed by distributed execution and queueing systems.\n\n```python {title=\"count_down_from_persistence.py\" noqa=\"I001\" requires=\"count_down.py\"}\nfrom pathlib import Path\n\nfrom pydantic_graph import End\nfrom pydantic_graph.persistence.file import FileStatePersistence\n\nfrom count_down import CountDown, CountDownState, count_down_graph\n\n\nasync def main():\n    run_id = 'run_abc123'\n    persistence = FileStatePersistence(Path(f'count_down_{run_id}.json'))  # (1)!\n    state = CountDownState(counter=5)\n    await count_down_graph.initialize(  # (2)!\n        CountDown(), state=state, persistence=persistence\n    )\n\n    done = False\n    while not done:\n        done = await run_node(run_id)\n\n\nasync def run_node(run_id: str) -> bool:  # (3)!\n    persistence = FileStatePersistence(Path(f'count_down_{run_id}.json'))\n    async with count_down_graph.iter_from_persistence(persistence) as run:  # (4)!\n        node_or_end = await run.next()  # (5)!\n\n    print('Node:', node_or_end)\n    #> Node: CountDown()\n    #> Node: CountDown()\n    #> Node: CountDown()\n    #> Node: CountDown()\n    #> Node: CountDown()\n    #> Node: End(data=0)\n    return isinstance(node_or_end, End)  # (6)!\n```\n\n1. Create a [`FileStatePersistence`][pydantic_graph.persistence.file.FileStatePersistence] to use to start the graph.\n2. Call [`graph.initialize()`][pydantic_graph.graph.Graph.initialize] to set the initial graph state in the persistence object.\n3. `run_node` is a pure function that doesn't need access to any other process state to run the next node of the graph, except the ID of the run.\n4. Call [`graph.iter_from_persistence()`][pydantic_graph.graph.Graph.iter_from_persistence] create a [`GraphRun`][pydantic_graph.graph.GraphRun] object that will run the next node of the graph from the state stored in persistence. This will return either a node or an `End` object.\n5. [`graph.run()`][pydantic_graph.graph.Graph.run] will return either a [node][pydantic_graph.nodes.BaseNode] or an [`End`][pydantic_graph.nodes.End] object.\n6. Check if the node is an [`End`][pydantic_graph.nodes.End] object, if it is, the graph run is complete.\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/#state-persistence", "page": "docs/graph", "source_site": "pydantic_ai"}
{"title": "Example: Human in the loop.", "anchor": "example-human-in-the-loop", "heading_level": 3, "md_text": "As noted above, state persistence allows graphs to be interrupted and resumed. One use case of this is to allow user input to continue.\n\nIn this example, an AI asks the user a question, the user provides an answer, the AI evaluates the answer and ends if the user got it right or asks another question if they got it wrong.\n\nInstead of running the entire graph in a single process invocation, we run the graph by running the process repeatedly, optionally providing an answer to the question as a command line argument.\n\n??? example \"`ai_q_and_a_graph.py` \u2014 `question_graph` definition\"\n```python {title=\"ai_q_and_a_graph.py\" noqa=\"I001\"}\nfrom __future__ import annotations as _annotations\n\nfrom typing import Annotated\nfrom pydantic_graph import Edge\nfrom dataclasses import dataclass, field\nfrom pydantic import BaseModel\nfrom pydantic_graph import (\n    BaseNode,\n    End,\n    Graph,\n    GraphRunContext,\n)\nfrom pydantic_ai import Agent, format_as_xml\nfrom pydantic_ai import ModelMessage\n\nask_agent = Agent('openai:gpt-5', output_type=str, instrument=True)\n\n\n@dataclass\nclass QuestionState:\n    question: str | None = None\n    ask_agent_messages: list[ModelMessage] = field(default_factory=list)\n    evaluate_agent_messages: list[ModelMessage] = field(default_factory=list)\n\n\n@dataclass\nclass Ask(BaseNode[QuestionState]):\n    \"\"\"Generate question using GPT-5.\"\"\"\n    docstring_notes = True\n    async def run(\n        self, ctx: GraphRunContext[QuestionState]\n    ) -> Annotated[Answer, Edge(label='Ask the question')]:\n        result = await ask_agent.run(\n            'Ask a simple question with a single correct answer.',\n            message_history=ctx.state.ask_agent_messages,\n        )\n        ctx.state.ask_agent_messages += result.new_messages()\n        ctx.state.question = result.output\n        return Answer(result.output)\n\n\n@dataclass\nclass Answer(BaseNode[QuestionState]):\n    question: str\n\n    async def run(self, ctx: GraphRunContext[QuestionState]) -> Evaluate:\n        answer = input(f'{self.question}: ')\n        return Evaluate(answer)\n\n\nclass EvaluationResult(BaseModel, use_attribute_docstrings=True):\n    correct: bool\n    \"\"\"Whether the answer is correct.\"\"\"\n    comment: str\n    \"\"\"Comment on the answer, reprimand the user if the answer is wrong.\"\"\"\n\n\nevaluate_agent = Agent(\n    'openai:gpt-5',\n    output_type=EvaluationResult,\n    system_prompt='Given a question and answer, evaluate if the answer is correct.',\n)\n\n\n@dataclass\nclass Evaluate(BaseNode[QuestionState, None, str]):\n    answer: str\n\n    async def run(\n        self,\n        ctx: GraphRunContext[QuestionState],\n    ) -> Annotated[End[str], Edge(label='success')] | Reprimand:\n        assert ctx.state.question is not None\n        result = await evaluate_agent.run(\n            format_as_xml({'question': ctx.state.question, 'answer': self.answer}),\n            message_history=ctx.state.evaluate_agent_messages,\n        )\n        ctx.state.evaluate_agent_messages += result.new_messages()\n        if result.output.correct:\n            return End(result.output.comment)\n        else:\n            return Reprimand(result.output.comment)\n\n\n@dataclass\nclass Reprimand(BaseNode[QuestionState]):\n    comment: str\n\n    async def run(self, ctx: GraphRunContext[QuestionState]) -> Ask:\n        print(f'Comment: {self.comment}')\n        ctx.state.question = None\n        return Ask()\n\n\nquestion_graph = Graph(\n    nodes=(Ask, Answer, Evaluate, Reprimand), state_type=QuestionState\n)\n```\n\n_(This example is complete, it can be run \"as is\")_\n\n```python {title=\"ai_q_and_a_run.py\" noqa=\"I001\" requires=\"ai_q_and_a_graph.py\"}\nimport sys\nfrom pathlib import Path\n\nfrom pydantic_graph import End\nfrom pydantic_graph.persistence.file import FileStatePersistence\nfrom pydantic_ai import ModelMessage  # noqa: F401\n\nfrom ai_q_and_a_graph import Ask, question_graph, Evaluate, QuestionState, Answer\n\n\nasync def main():\n    answer: str | None = sys.argv[1] if len(sys.argv) > 1 else None  # (1)!\n    persistence = FileStatePersistence(Path('question_graph.json'))  # (2)!\n    persistence.set_graph_types(question_graph)  # (3)!\n\n    if snapshot := await persistence.load_next():  # (4)!\n        state = snapshot.state\n        assert answer is not None\n        node = Evaluate(answer)\n    else:\n        state = QuestionState()\n        node = Ask()  # (5)!\n\n    async with question_graph.iter(node, state=state, persistence=persistence) as run:\n        while True:\n            node = await run.next()  # (6)!\n            if isinstance(node, End):  # (7)!\n                print('END:', node.data)\n                history = await persistence.load_all()  # (8)!\n                print([e.node for e in history])\n                break\n            elif isinstance(node, Answer):  # (9)!\n                print(node.question)\n                #> What is the capital of France?\n                break\n            # otherwise just continue\n```", "url": "https://ai.pydantic.dev/docs/graph/#example-human-in-the-loop", "page": "docs/graph", "source_site": "pydantic_ai"}
{"title": "Example: Human in the loop.", "anchor": "example-human-in-the-loop", "heading_level": 3, "md_text": "1. Get the user's answer from the command line, if provided. See [question graph example](examples/question-graph.md) for a complete example.\n2. Create a state persistence instance the `'question_graph.json'` file may or may not already exist.\n3. Since we're using the [persistence interface][pydantic_graph.persistence.BaseStatePersistence] outside a graph, we need to call [`set_graph_types`][pydantic_graph.persistence.BaseStatePersistence.set_graph_types] to set the graph generic types `StateT` and `RunEndT` for the persistence instance. This is necessary to allow the persistence instance to know how to serialize and deserialize graph nodes.\n4. If we're run the graph before, [`load_next`][pydantic_graph.persistence.BaseStatePersistence.load_next] will return a snapshot of the next node to run, here we use `state` from that snapshot, and create a new `Evaluate` node with the answer provided on the command line.\n5. If the graph hasn't been run before, we create a new `QuestionState` and start with the `Ask` node.\n6. Call [`GraphRun.next()`][pydantic_graph.graph.GraphRun.next] to run the node. This will return either a node or an `End` object.\n7. If the node is an `End` object, the graph run is complete. The `data` field of the `End` object contains the comment returned by the `evaluate_agent` about the correct answer.\n8. To demonstrate the state persistence, we call [`load_all`][pydantic_graph.persistence.BaseStatePersistence.load_all] to get all the snapshots from the persistence instance. This will return a list of [`Snapshot`][pydantic_graph.persistence.Snapshot] objects.\n9. If the node is an `Answer` object, we print the question and break out of the loop to end the process and wait for user input.\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_\n\nFor a complete example of this graph, see the [question graph example](examples/question-graph.md).", "url": "https://ai.pydantic.dev/docs/graph/#example-human-in-the-loop", "page": "docs/graph", "source_site": "pydantic_ai"}
{"title": "Dependency Injection", "anchor": "dependency-injection", "heading_level": 2, "md_text": "As with Pydantic AI, `pydantic-graph` supports dependency injection via a generic parameter on [`Graph`][pydantic_graph.graph.Graph] and [`BaseNode`][pydantic_graph.nodes.BaseNode], and the [`GraphRunContext.deps`][pydantic_graph.nodes.GraphRunContext.deps] field.\n\nAs an example of dependency injection, let's modify the `DivisibleBy5` example [above](#graph) to use a [`ProcessPoolExecutor`][concurrent.futures.ProcessPoolExecutor] to run the compute load in a separate process (this is a contrived example, `ProcessPoolExecutor` wouldn't actually improve performance in this example):\n\n```py {title=\"deps_example.py\" test=\"skip\" hl_lines=\"4 10-12 35-37 48-49\"}\nfrom __future__ import annotations\n\nimport asyncio\nfrom concurrent.futures import ProcessPoolExecutor\nfrom dataclasses import dataclass\n\nfrom pydantic_graph import BaseNode, End, FullStatePersistence, Graph, GraphRunContext\n\n\n@dataclass\nclass GraphDeps:\n    executor: ProcessPoolExecutor\n\n\n@dataclass\nclass DivisibleBy5(BaseNode[None, GraphDeps, int]):\n    foo: int\n\n    async def run(\n        self,\n        ctx: GraphRunContext[None, GraphDeps],\n    ) -> Increment | End[int]:\n        if self.foo % 5 == 0:\n            return End(self.foo)\n        else:\n            return Increment(self.foo)\n\n\n@dataclass\nclass Increment(BaseNode[None, GraphDeps]):\n    foo: int\n\n    async def run(self, ctx: GraphRunContext[None, GraphDeps]) -> DivisibleBy5:\n        loop = asyncio.get_running_loop()\n        compute_result = await loop.run_in_executor(\n            ctx.deps.executor,\n            self.compute,\n        )\n        return DivisibleBy5(compute_result)\n\n    def compute(self) -> int:\n        return self.foo + 1\n\n\nfives_graph = Graph(nodes=[DivisibleBy5, Increment])\n\n\nasync def main():\n    with ProcessPoolExecutor() as executor:\n        deps = GraphDeps(executor)\n        result = await fives_graph.run(DivisibleBy5(3), deps=deps, persistence=FullStatePersistence())\n    print(result.output)\n    #> 5\n    # the full history is quite verbose (see below), so we'll just print the summary\n    print([item.node for item in result.persistence.history])\n    \"\"\"\n    [\n        DivisibleBy5(foo=3),\n        Increment(foo=3),\n        DivisibleBy5(foo=4),\n        Increment(foo=4),\n        DivisibleBy5(foo=5),\n        End(data=5),\n    ]\n    \"\"\"\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/#dependency-injection", "page": "docs/graph", "source_site": "pydantic_ai"}
{"title": "Mermaid Diagrams", "anchor": "mermaid-diagrams", "heading_level": 2, "md_text": "Pydantic Graph can generate [mermaid](https://mermaid.js.org/) [`stateDiagram-v2`](https://mermaid.js.org/syntax/stateDiagram.html) diagrams for graphs, as shown above.\n\nThese diagrams can be generated with:\n\n- [`Graph.mermaid_code`][pydantic_graph.graph.Graph.mermaid_code] to generate the mermaid code for a graph\n- [`Graph.mermaid_image`][pydantic_graph.graph.Graph.mermaid_image] to generate an image of the graph using [mermaid.ink](https://mermaid.ink/)\n- [`Graph.mermaid_save`][pydantic_graph.graph.Graph.mermaid_save] to generate an image of the graph using [mermaid.ink](https://mermaid.ink/) and save it to a file\n\nBeyond the diagrams shown above, you can also customize mermaid diagrams with the following options:\n\n- [`Edge`][pydantic_graph.nodes.Edge] allows you to apply a label to an edge\n- [`BaseNode.docstring_notes`][pydantic_graph.nodes.BaseNode.docstring_notes] and [`BaseNode.get_note`][pydantic_graph.nodes.BaseNode.get_note] allows you to add notes to nodes\n- The [`highlighted_nodes`][pydantic_graph.graph.Graph.mermaid_code] parameter allows you to highlight specific node(s) in the diagram\n\nPutting that together, we can edit the last [`ai_q_and_a_graph.py`](#example-human-in-the-loop) example to:\n\n- add labels to some edges\n- add a note to the `Ask` node\n- highlight the `Answer` node\n- save the diagram as a `PNG` image to file\n\n```python {title=\"ai_q_and_a_graph_extra.py\" test=\"skip\" lint=\"skip\" hl_lines=\"2 4 10-11 14 26 31\"}\nfrom typing import Annotated\n\nfrom pydantic_graph import BaseNode, End, Graph, GraphRunContext, Edge\n\nask_agent = Agent('openai:gpt-5', output_type=str, instrument=True)\n\n\n@dataclass\nclass QuestionState:\n    question: str | None = None\n    ask_agent_messages: list[ModelMessage] = field(default_factory=list)\n    evaluate_agent_messages: list[ModelMessage] = field(default_factory=list)\n\n\n@dataclass\nclass Ask(BaseNode[QuestionState]):\n    \"\"\"Generate question using GPT-5.\"\"\"\n    docstring_notes = True\n    async def run(\n        self, ctx: GraphRunContext[QuestionState]\n    ) -> Annotated[Answer, Edge(label='Ask the question')]:\n        result = await ask_agent.run(\n            'Ask a simple question with a single correct answer.',\n            message_history=ctx.state.ask_agent_messages,\n        )\n        ctx.state.ask_agent_messages += result.new_messages()\n        ctx.state.question = result.output\n        return Answer(result.output)\n\n\n@dataclass\nclass Answer(BaseNode[QuestionState]):\n    question: str\n\n    async def run(self, ctx: GraphRunContext[QuestionState]) -> Evaluate:\n        answer = input(f'{self.question}: ')\n        return Evaluate(answer)\n\n\nclass EvaluationResult(BaseModel, use_attribute_docstrings=True):\n    correct: bool\n    \"\"\"Whether the answer is correct.\"\"\"\n    comment: str\n    \"\"\"Comment on the answer, reprimand the user if the answer is wrong.\"\"\"\n\n\nevaluate_agent = Agent(\n    'openai:gpt-5',\n    output_type=EvaluationResult,\n    system_prompt='Given a question and answer, evaluate if the answer is correct.',\n)\n\n\n@dataclass\nclass Evaluate(BaseNode[QuestionState, None, str]):\n    answer: str\n\n    async def run(\n        self,\n        ctx: GraphRunContext[QuestionState],\n    ) -> Annotated[End[str], Edge(label='success')] | Reprimand:\n        assert ctx.state.question is not None\n        result = await evaluate_agent.run(\n            format_as_xml({'question': ctx.state.question, 'answer': self.answer}),\n            message_history=ctx.state.evaluate_agent_messages,\n        )\n        ctx.state.evaluate_agent_messages += result.new_messages()\n        if result.output.correct:\n            return End(result.output.comment)\n        else:\n            return Reprimand(result.output.comment)\n\n\n@dataclass\nclass Reprimand(BaseNode[QuestionState]):\n    comment: str\n\n    async def run(self, ctx: GraphRunContext[QuestionState]) -> Ask:\n        print(f'Comment: {self.comment}')\n        ctx.state.question = None\n        return Ask()\n\n\nquestion_graph = Graph(\n    nodes=(Ask, Answer, Evaluate, Reprimand), state_type=QuestionState\n)\n```\n\n_(This example is not complete and cannot be run directly)_\n\nThis would generate an image that looks like this:\n\n```mermaid\n---\ntitle: question_graph\n---\nstateDiagram-v2\n  Ask --> Answer: Ask the question\n  note right of Ask\n    Judge the answer.\n    Decide on next step.\n  end note\n  Answer --> Evaluate\n  Evaluate --> Reprimand\n  Evaluate --> [*]: success\n  Reprimand --> Ask\n\nclassDef highlighted fill:#fdff32\nclass Answer highlighted\n```", "url": "https://ai.pydantic.dev/docs/graph/#mermaid-diagrams", "page": "docs/graph", "source_site": "pydantic_ai"}
{"title": "Setting Direction of the State Diagram", "anchor": "setting-direction-of-the-state-diagram", "heading_level": 3, "md_text": "You can specify the direction of the state diagram using one of the following values:\n\n- `'TB'`: Top to bottom, the diagram flows vertically from top to bottom.\n- `'LR'`: Left to right, the diagram flows horizontally from left to right.\n- `'RL'`: Right to left, the diagram flows horizontally from right to left.\n- `'BT'`: Bottom to top, the diagram flows vertically from bottom to top.\n\nHere is an example of how to do this using 'Left to Right' (LR) instead of the default 'Top to Bottom' (TB):\n\n```py {title=\"vending_machine_diagram.py\" requires=\"vending_machine.py\"}\nfrom vending_machine import InsertCoin, vending_machine_graph\n\nvending_machine_graph.mermaid_code(start_node=InsertCoin, direction='LR')\n```\n\n```mermaid\n---\ntitle: vending_machine_graph\n---\nstateDiagram-v2\n  direction LR\n  [*] --> InsertCoin\n  InsertCoin --> CoinsInserted\n  CoinsInserted --> SelectProduct\n  CoinsInserted --> Purchase\n  SelectProduct --> Purchase\n  Purchase --> InsertCoin\n  Purchase --> SelectProduct\n  Purchase --> [*]\n```", "url": "https://ai.pydantic.dev/docs/graph/#setting-direction-of-the-state-diagram", "page": "docs/graph", "source_site": "pydantic_ai"}
{"title": "Pydantic Evals", "anchor": "pydantic-evals", "heading_level": 1, "md_text": "**Pydantic Evals** is a powerful evaluation framework for systematically testing and evaluating AI systems, from simple LLM calls to complex multi-agent applications.", "url": "https://ai.pydantic.dev/docs/evals/#pydantic-evals", "page": "docs/evals", "source_site": "pydantic_ai"}
{"title": "Design Philosophy", "anchor": "design-philosophy", "heading_level": 2, "md_text": "!!! note \"Code-First Approach\"\n    Pydantic Evals follows a code-first philosophy where all evaluation components are defined in Python. This differs from platforms with web-based configuration. You write and run evals in code, and can write the results to disk or view them in your terminal or in [Pydantic Logfire](https://logfire.pydantic.dev/docs/guides/web-ui/evals/).\n\n!!! danger \"Evals are an Emerging Practice\"\n    Unlike unit tests, evals are an emerging art/science. Anyone who claims to know exactly how your evals should be defined can safely be ignored. We've designed Pydantic Evals to be flexible and useful without being too opinionated.", "url": "https://ai.pydantic.dev/docs/evals/#design-philosophy", "page": "docs/evals", "source_site": "pydantic_ai"}
{"title": "Quick Navigation", "anchor": "quick-navigation", "heading_level": 2, "md_text": "**Getting Started:**\n\n- [Installation](#installation)\n- [Quick Start](evals/quick-start.md)\n- [Core Concepts](evals/core-concepts.md)\n\n**Evaluators:**\n\n- [Evaluators Overview](evals/evaluators/overview.md) - Compare evaluator types and learn when to use each approach\n- [Built-in Evaluators](evals/evaluators/built-in.md) - Complete reference for exact match, instance checks, and other ready-to-use evaluators\n- [LLM as a Judge](evals/evaluators/llm-judge.md) - Use LLMs to evaluate subjective qualities, complex criteria, and natural language outputs\n- [Custom Evaluators](evals/evaluators/custom.md) - Implement domain-specific scoring logic and custom evaluation metrics\n- [Span-Based Evaluation](evals/evaluators/span-based.md) - Evaluate internal agent behavior (tool calls, execution flow) using OpenTelemetry traces. Essential for complex agents where correctness depends on _how_ the answer was reached, not just the final output. Also ensures eval assertions align with production telemetry.\n\n**How-To Guides:**\n\n- [Logfire Integration](evals/how-to/logfire-integration.md) - Visualize results\n- [Dataset Management](evals/how-to/dataset-management.md) - Save, load, generate\n- [Concurrency & Performance](evals/how-to/concurrency.md) - Control parallel execution\n- [Retry Strategies](evals/how-to/retry-strategies.md) - Handle transient failures\n- [Metrics & Attributes](evals/how-to/metrics-attributes.md) - Track custom data\n\n**Examples:**\n\n- [Simple Validation](evals/examples/simple-validation.md) - Basic example\n\n**Reference:**\n\n- [API Documentation](api/pydantic_evals/dataset.md)", "url": "https://ai.pydantic.dev/docs/evals/#quick-navigation", "page": "docs/evals", "source_site": "pydantic_ai"}
{"title": "Code-First Evaluation", "anchor": "code-first-evaluation", "heading_level": 2, "md_text": "Pydantic Evals follows a **code-first approach** where you define all evaluation components (datasets, experiments, tasks, cases and evaluators) in Python code, or as serialized data loaded by Python code. This differs from platforms with fully web-based configuration.\n\nWhen you run an _Experiment_ you'll see a progress indicator and can print the results wherever you run your python code (IDE, terminal, etc). You also get a report object back that you can serialize and store or send to a notebook or other application for further visualization and analysis.\n\nIf you are using [Pydantic Logfire](https://logfire.pydantic.dev/docs/guides/web-ui/evals/), your experiment results automatically appear in the Logfire web interface for visualization, comparison, and collaborative analysis. Logfire serves as a observability layer - you write and run evals in code, then view and analyze results in the web UI.", "url": "https://ai.pydantic.dev/docs/evals/#code-first-evaluation", "page": "docs/evals", "source_site": "pydantic_ai"}
{"title": "Installation", "anchor": "installation", "heading_level": 2, "md_text": "To install the Pydantic Evals package, run:\n\n```bash\npip/uv-add pydantic-evals\n```\n\n`pydantic-evals` does not depend on `pydantic-ai`, but has an optional dependency on `logfire` if you'd like to\nuse OpenTelemetry traces in your evals, or send evaluation results to [logfire](https://pydantic.dev/logfire).\n\n```bash\npip/uv-add 'pydantic-evals[logfire]'\n```", "url": "https://ai.pydantic.dev/docs/evals/#installation", "page": "docs/evals", "source_site": "pydantic_ai"}
{"title": "Pydantic Evals Data Model", "anchor": "pydantic-evals-data-model", "heading_level": 2, "md_text": "Pydantic Evals is built around a simple data model:", "url": "https://ai.pydantic.dev/docs/evals/#pydantic-evals-data-model", "page": "docs/evals", "source_site": "pydantic_ai"}
{"title": "Data Model Diagram", "anchor": "data-model-diagram", "heading_level": 3, "md_text": "```\nDataset (1) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 (Many) Case\n\u2502                        \u2502\n\u2502                        \u2502\n\u2514\u2500\u2500\u2500 (Many) Experiment \u2500\u2500\u2534\u2500\u2500\u2500 (Many) Case results\n     \u2502\n     \u2514\u2500\u2500\u2500 (1) Task\n     \u2502\n     \u2514\u2500\u2500\u2500 (Many) Evaluator\n```", "url": "https://ai.pydantic.dev/docs/evals/#data-model-diagram", "page": "docs/evals", "source_site": "pydantic_ai"}
{"title": "Key Relationships", "anchor": "key-relationships", "heading_level": 3, "md_text": "1. **Dataset \u2192 Cases**: One Dataset contains many Cases\n2. **Dataset \u2192 Experiments**: One Dataset can be used across many Experiments over time\n3. **Experiment \u2192 Case results**: One Experiment generates results by executing each Case\n4. **Experiment \u2192 Task**: One Experiment evaluates one defined Task\n5. **Experiment \u2192 Evaluators**: One Experiment uses multiple Evaluators. Dataset-wide Evaluators are run against all Cases, and Case-specific Evaluators against their respective Cases", "url": "https://ai.pydantic.dev/docs/evals/#key-relationships", "page": "docs/evals", "source_site": "pydantic_ai"}
{"title": "Data Flow", "anchor": "data-flow", "heading_level": 3, "md_text": "1. **Dataset creation**: Define cases and evaluators in YAML/JSON, or directly in Python\n2. **Experiment execution**: Run `dataset.evaluate_sync(task_function)`\n3. **Cases run**: Each Case is executed against the Task\n4. **Evaluation**: Evaluators score the Task outputs for each Case\n5. **Results**: All Case results are collected into a summary report\n\n!!! note \"A metaphor\"\n\n    A useful metaphor (although not perfect) is to think of evals like a **Unit Testing** framework:\n\n    - **Cases + Evaluators** are your individual unit tests - each one\n    defines a specific scenario you want to test, complete with inputs\n    and expected outcomes. Just like a unit test, a case asks: _\"Given\n    this input, does my system produce the right output?\"_\n\n    -  **Datasets** are like test suites - they are the scaffolding that holds your unit\n    tests together. They group related cases and define shared\n    evaluation criteria that should apply across all tests in the suite.\n\n    - **Experiments** are like running your entire test suite and getting a\n    report. When you execute `dataset.evaluate_sync(my_ai_function)`,\n    you're running all your cases against your AI system and\n    collecting the results - just like running `pytest` and getting a\n    summary of passes, failures, and performance metrics.\n\n    The key difference from traditional unit testing is that AI systems are\n    probabilistic. If you're type checking you'll still get a simple pass/fail,\n    but scores for text outputs are likely qualitative and/or categorical,\n    and more open to interpretation.\n\nFor a deeper understanding, see [Core Concepts](evals/core-concepts.md).", "url": "https://ai.pydantic.dev/docs/evals/#data-flow", "page": "docs/evals", "source_site": "pydantic_ai"}
{"title": "Datasets and Cases", "anchor": "datasets-and-cases", "heading_level": 2, "md_text": "In Pydantic Evals, everything begins with [`Dataset`][pydantic_evals.Dataset]s and [`Case`][pydantic_evals.Case]s:\n\n- **[`Dataset`][pydantic_evals.Dataset]**: A collection of test Cases designed for the evaluation of a specific task or function\n- **[`Case`][pydantic_evals.Case]**: A single test scenario corresponding to Task inputs, with optional expected outputs, metadata, and case-specific evaluators\n\n```python {title=\"simple_eval_dataset.py\"}\nfrom pydantic_evals import Case, Dataset\n\ncase1 = Case(\n    name='simple_case',\n    inputs='What is the capital of France?',\n    expected_output='Paris',\n    metadata={'difficulty': 'easy'},\n)\n\ndataset = Dataset(cases=[case1])\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nSee [Dataset Management](evals/how-to/dataset-management.md) to learn about saving, loading, and generating datasets.", "url": "https://ai.pydantic.dev/docs/evals/#datasets-and-cases", "page": "docs/evals", "source_site": "pydantic_ai"}
{"title": "Evaluators", "anchor": "evaluators", "heading_level": 2, "md_text": "[`Evaluator`][pydantic_evals.evaluators.Evaluator]s analyze and score the results of your Task when tested against a Case.\n\nThese can be deterministic, code-based checks (such as testing model output format with a regex, or checking for the appearance of PII or sensitive data), or they can assess non-deterministic model outputs for qualities like accuracy, precision/recall, hallucinations, or instruction-following.\n\nWhile both kinds of testing are useful in LLM systems, classical code-based tests are cheaper and easier than tests which require either human or machine review of model outputs.\n\nPydantic Evals includes several [built-in evaluators](evals/evaluators/built-in.md) and allows you to define [custom evaluators](evals/evaluators/custom.md):\n\n```python {title=\"simple_eval_evaluator.py\" requires=\"simple_eval_dataset.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\nfrom pydantic_evals.evaluators.common import IsInstance\n\nfrom simple_eval_dataset import dataset\n\ndataset.add_evaluator(IsInstance(type_name='str'))  # (1)!\n\n\n@dataclass\nclass MyEvaluator(Evaluator):\n    async def evaluate(self, ctx: EvaluatorContext[str, str]) -> float:  # (2)!\n        if ctx.output == ctx.expected_output:\n            return 1.0\n        elif (\n            isinstance(ctx.output, str)\n            and ctx.expected_output.lower() in ctx.output.lower()\n        ):\n            return 0.8\n        else:\n            return 0.0\n\n\ndataset.add_evaluator(MyEvaluator())\n```\n\n1. You can add built-in evaluators to a dataset using the [`add_evaluator`][pydantic_evals.Dataset.add_evaluator] method.\n2. This custom evaluator returns a simple score based on whether the output matches the expected output.\n\n_(This example is complete, it can be run \"as is\")_\n\nLearn more:\n\n- [Evaluators Overview](evals/evaluators/overview.md) - When to use different types\n- [Built-in Evaluators](evals/evaluators/built-in.md) - Complete reference\n- [LLM Judge](evals/evaluators/llm-judge.md) - Using LLMs as evaluators\n- [Custom Evaluators](evals/evaluators/custom.md) - Write your own logic\n- [Span-Based Evaluation](evals/evaluators/span-based.md) - Analyze execution traces", "url": "https://ai.pydantic.dev/docs/evals/#evaluators", "page": "docs/evals", "source_site": "pydantic_ai"}
{"title": "Running Experiments", "anchor": "running-experiments", "heading_level": 2, "md_text": "Performing evaluations involves running a task against all cases in a dataset, also known as running an \"experiment\".\n\nPutting the above two examples together and using the more declarative `evaluators` kwarg to [`Dataset`][pydantic_evals.Dataset]:\n\n```python {title=\"simple_eval_complete.py\"}\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext, IsInstance\n\ncase1 = Case(  # (1)!\n    name='simple_case',\n    inputs='What is the capital of France?',\n    expected_output='Paris',\n    metadata={'difficulty': 'easy'},\n)\n\n\nclass MyEvaluator(Evaluator[str, str]):\n    def evaluate(self, ctx: EvaluatorContext[str, str]) -> float:\n        if ctx.output == ctx.expected_output:\n            return 1.0\n        elif (\n            isinstance(ctx.output, str)\n            and ctx.expected_output.lower() in ctx.output.lower()\n        ):\n            return 0.8\n        else:\n            return 0.0\n\n\ndataset = Dataset(\n    cases=[case1],\n    evaluators=[IsInstance(type_name='str'), MyEvaluator()],  # (2)!\n)\n\n\nasync def guess_city(question: str) -> str:  # (3)!\n    return 'Paris'\n\n\nreport = dataset.evaluate_sync(guess_city)  # (4)!\nreport.print(include_input=True, include_output=True, include_durations=False)  # (5)!\n\"\"\"\n                              Evaluation Summary: guess_city\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Case ID     \u2503 Inputs                         \u2503 Outputs \u2503 Scores            \u2503 Assertions \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 simple_case \u2502 What is the capital of France? \u2502 Paris   \u2502 MyEvaluator: 1.00 \u2502 \u2714          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Averages    \u2502                                \u2502         \u2502 MyEvaluator: 1.00 \u2502 100.0% \u2714   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\"\"\"\n```\n\n1. Create a [test case][pydantic_evals.Case] as above\n2. Create a [`Dataset`][pydantic_evals.Dataset] with test cases and [`evaluators`][pydantic_evals.Dataset.evaluators]\n3. Our function to evaluate.\n4. Run the evaluation with [`evaluate_sync`][pydantic_evals.Dataset.evaluate_sync], which runs the function against all test cases in the dataset, and returns an [`EvaluationReport`][pydantic_evals.reporting.EvaluationReport] object.\n5. Print the report with [`print`][pydantic_evals.reporting.EvaluationReport.print], which shows the results of the evaluation. We have omitted duration here just to keep the printed output from changing from run to run.\n\n_(This example is complete, it can be run \"as is\")_\n\nSee [Quick Start](evals/quick-start.md) for more examples and [Concurrency & Performance](evals/how-to/concurrency.md) to learn about controlling parallel execution.", "url": "https://ai.pydantic.dev/docs/evals/#running-experiments", "page": "docs/evals", "source_site": "pydantic_ai"}
{"title": "API Reference", "anchor": "api-reference", "heading_level": 2, "md_text": "For comprehensive coverage of all classes, methods, and configuration options, see the detailed [API Reference documentation](https://ai.pydantic.dev/api/pydantic_evals/dataset/).", "url": "https://ai.pydantic.dev/docs/evals/#api-reference", "page": "docs/evals", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "<!-- TODO - this would be the perfect place for a full tutorial or case study  -->\n1. **Start with simple evaluations** using [Quick Start](evals/quick-start.md)\n2. **Understand the data model** with [Core Concepts](evals/core-concepts.md)\n3. **Explore built-in evaluators** in [Built-in Evaluators](evals/evaluators/built-in.md)\n4. **Integrate with Logfire** for visualization: [Logfire Integration](evals/how-to/logfire-integration.md)\n5. **Build comprehensive test suites** with [Dataset Management](evals/how-to/dataset-management.md)\n6. **Implement custom evaluators** for domain-specific metrics: [Custom Evaluators](evals/evaluators/custom.md)", "url": "https://ai.pydantic.dev/docs/evals/#next-steps", "page": "docs/evals", "source_site": "pydantic_ai"}
{"title": "Deferred Tools", "anchor": "deferred-tools", "heading_level": 1, "md_text": "There are a few scenarios where the model should be able to call a tool that should not or cannot be executed during the same agent run inside the same Python process:\n\n- it may need to be approved by the user first\n- it may depend on an upstream service, frontend, or user to provide the result\n- the result could take longer to generate than it's reasonable to keep the agent process running\n\nTo support these use cases, Pydantic AI provides the concept of deferred tools, which come in two flavors documented below:\n\n- tools that [require approval](#human-in-the-loop-tool-approval)\n- tools that are [executed externally](#external-tool-execution)\n\nWhen the model calls a deferred tool, the agent run will end with a [`DeferredToolRequests`][pydantic_ai.output.DeferredToolRequests] output object containing information about the deferred tool calls. Once the approvals and/or results are ready, a new agent run can then be started with the original run's [message history](message-history.md) plus a [`DeferredToolResults`][pydantic_ai.tools.DeferredToolResults] object holding results for each tool call in `DeferredToolRequests`, which will continue the original run where it left off.\n\nNote that handling deferred tool calls requires `DeferredToolRequests` to be in the `Agent`'s [`output_type`](output.md#structured-output) so that the possible types of the agent run output are correctly inferred. If your agent can also be used in a context where no deferred tools are available and you don't want to deal with that type everywhere you use the agent, you can instead pass the `output_type` argument when you run the agent using [`agent.run()`][pydantic_ai.agent.AbstractAgent.run], [`agent.run_sync()`][pydantic_ai.agent.AbstractAgent.run_sync], [`agent.run_stream()`][pydantic_ai.agent.AbstractAgent.run_stream], or [`agent.iter()`][pydantic_ai.Agent.iter]. Note that the run-time `output_type` overrides the one specified at construction time (for type inference reasons), so you'll need to include the original output type explicitly.", "url": "https://ai.pydantic.dev/docs/deferred-tools/#deferred-tools", "page": "docs/deferred-tools", "source_site": "pydantic_ai"}
{"title": "Human-in-the-Loop Tool Approval", "anchor": "human-in-the-loop-tool-approval", "heading_level": 2, "md_text": "If a tool function always requires approval, you can pass the `requires_approval=True` argument to the [`@agent.tool`][pydantic_ai.Agent.tool] decorator, [`@agent.tool_plain`][pydantic_ai.Agent.tool_plain] decorator, [`Tool`][pydantic_ai.tools.Tool] class, [`FunctionToolset.tool`][pydantic_ai.toolsets.FunctionToolset.tool] decorator, or [`FunctionToolset.add_function()`][pydantic_ai.toolsets.FunctionToolset.add_function] method. Inside the function, you can then assume that the tool call has been approved.\n\nIf whether a tool function requires approval depends on the tool call arguments or the agent [run context][pydantic_ai.tools.RunContext] (e.g. [dependencies](dependencies.md) or message history), you can raise the [`ApprovalRequired`][pydantic_ai.exceptions.ApprovalRequired] exception from the tool function. The [`RunContext.tool_call_approved`][pydantic_ai.tools.RunContext.tool_call_approved] property will be `True` if the tool call has already been approved.\n\nTo require approval for calls to tools provided by a [toolset](toolsets.md) (like an [MCP server](mcp/client.md)), see the [`ApprovalRequiredToolset` documentation](toolsets.md#requiring-tool-approval).\n\nWhen the model calls a tool that requires approval, the agent run will end with a [`DeferredToolRequests`][pydantic_ai.output.DeferredToolRequests] output object with an `approvals` list holding [`ToolCallPart`s][pydantic_ai.messages.ToolCallPart] containing the tool name, validated arguments, and a unique tool call ID.\n\nOnce you've gathered the user's approvals or denials, you can build a [`DeferredToolResults`][pydantic_ai.tools.DeferredToolResults] object with an `approvals` dictionary that maps each tool call ID to a boolean, a [`ToolApproved`][pydantic_ai.tools.ToolApproved] object (with optional `override_args`), or a [`ToolDenied`][pydantic_ai.tools.ToolDenied] object (with an optional custom `message` to provide to the model). This `DeferredToolResults` object can then be provided to one of the agent run methods as `deferred_tool_results`, alongside the original run's [message history](message-history.md).\n\nHere's an example that shows how to require approval for all file deletions, and for updates of specific protected files:\n\n```python {title=\"tool_requires_approval.py\"}\nfrom pydantic_ai import (\n    Agent,\n    ApprovalRequired,\n    DeferredToolRequests,\n    DeferredToolResults,\n    RunContext,\n    ToolDenied,\n)\n\nagent = Agent('openai:gpt-5', output_type=[str, DeferredToolRequests])\n\nPROTECTED_FILES = {'.env'}\n\n\n@agent.tool\ndef update_file(ctx: RunContext, path: str, content: str) -> str:\n    if path in PROTECTED_FILES and not ctx.tool_call_approved:\n        raise ApprovalRequired\n    return f'File {path!r} updated: {content!r}'\n\n\n@agent.tool_plain(requires_approval=True)\ndef delete_file(path: str) -> str:\n    return f'File {path!r} deleted'\n\n\nresult = agent.run_sync('Delete `__init__.py`, write `Hello, world!` to `README.md`, and clear `.env`')\nmessages = result.all_messages()\n\nassert isinstance(result.output, DeferredToolRequests)\nrequests = result.output\nprint(requests)\n\"\"\"\nDeferredToolRequests(\n    calls=[],\n    approvals=[\n        ToolCallPart(\n            tool_name='update_file',\n            args={'path': '.env', 'content': ''},\n            tool_call_id='update_file_dotenv',\n        ),\n        ToolCallPart(\n            tool_name='delete_file',\n            args={'path': '__init__.py'},\n            tool_call_id='delete_file',\n        ),\n    ],\n)\n\"\"\"\n\nresults = DeferredToolResults()\nfor call in requests.approvals:\n    result = False\n    if call.tool_name == 'update_file':\n        # Approve all updates\n        result = True\n    elif call.tool_name == 'delete_file':\n        # deny all deletes\n        result = ToolDenied('Deleting files is not allowed')\n\n    results.approvals[call.tool_call_id] = result", "url": "https://ai.pydantic.dev/docs/deferred-tools/#human-in-the-loop-tool-approval", "page": "docs/deferred-tools", "source_site": "pydantic_ai"}
{"title": "Human-in-the-Loop Tool Approval", "anchor": "human-in-the-loop-tool-approval", "heading_level": 2, "md_text": "result = agent.run_sync(message_history=messages, deferred_tool_results=results)\nprint(result.output)\n\"\"\"\nI successfully updated `README.md` and cleared `.env`, but was not able to delete `__init__.py`.\n\"\"\"\nprint(result.all_messages())\n\"\"\"\n[\n    ModelRequest(\n        parts=[\n            UserPromptPart(\n                content='Delete `__init__.py`, write `Hello, world!` to `README.md`, and clear `.env`',\n                timestamp=datetime.datetime(...),\n            )\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            ToolCallPart(\n                tool_name='delete_file',\n                args={'path': '__init__.py'},\n                tool_call_id='delete_file',\n            ),\n            ToolCallPart(\n                tool_name='update_file',\n                args={'path': 'README.md', 'content': 'Hello, world!'},\n                tool_call_id='update_file_readme',\n            ),\n            ToolCallPart(\n                tool_name='update_file',\n                args={'path': '.env', 'content': ''},\n                tool_call_id='update_file_dotenv',\n            ),\n        ],\n        usage=RequestUsage(input_tokens=63, output_tokens=21),\n        model_name='gpt-5',\n        timestamp=datetime.datetime(...),\n    ),\n    ModelRequest(\n        parts=[\n            ToolReturnPart(\n                tool_name='update_file',\n                content=\"File 'README.md' updated: 'Hello, world!'\",\n                tool_call_id='update_file_readme',\n                timestamp=datetime.datetime(...),\n            )\n        ]\n    ),\n    ModelRequest(\n        parts=[\n            ToolReturnPart(\n                tool_name='delete_file',\n                content='Deleting files is not allowed',\n                tool_call_id='delete_file',\n                timestamp=datetime.datetime(...),\n            ),\n            ToolReturnPart(\n                tool_name='update_file',\n                content=\"File '.env' updated: ''\",\n                tool_call_id='update_file_dotenv',\n                timestamp=datetime.datetime(...),\n            ),\n        ]\n    ),\n    ModelResponse(\n        parts=[\n            TextPart(\n                content='I successfully updated `README.md` and cleared `.env`, but was not able to delete `__init__.py`.'\n            )\n        ],\n        usage=RequestUsage(input_tokens=79, output_tokens=39),\n        model_name='gpt-5',\n        timestamp=datetime.datetime(...),\n    ),\n]\n\"\"\"\n```\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/deferred-tools/#human-in-the-loop-tool-approval", "page": "docs/deferred-tools", "source_site": "pydantic_ai"}
{"title": "External Tool Execution", "anchor": "external-tool-execution", "heading_level": 2, "md_text": "When the result of a tool call cannot be generated inside the same agent run in which it was called, the tool is considered to be external.\nExamples of external tools are client-side tools implemented by a web or app frontend, and slow tasks that are passed off to a background worker or external service instead of keeping the agent process running.\n\nIf whether a tool call should be executed externally depends on the tool call arguments, the agent [run context][pydantic_ai.tools.RunContext] (e.g. [dependencies](dependencies.md) or message history), or how long the task is expected to take, you can define a tool function and conditionally raise the [`CallDeferred`][pydantic_ai.exceptions.CallDeferred] exception. Before raising the exception, the tool function would typically schedule some background task and pass along the [`RunContext.tool_call_id`][pydantic_ai.tools.RunContext.tool_call_id] so that the result can be matched to the deferred tool call later.\n\nIf a tool is always executed externally and its definition is provided to your code along with a JSON schema for its arguments, you can use an [`ExternalToolset`](toolsets.md#external-toolset). If the external tools are known up front and you don't have the arguments JSON schema handy, you can also define a tool function with the appropriate signature that does nothing but raise the [`CallDeferred`][pydantic_ai.exceptions.CallDeferred] exception.\n\nWhen the model calls an external tool, the agent run will end with a [`DeferredToolRequests`][pydantic_ai.output.DeferredToolRequests] output object with a `calls` list holding [`ToolCallPart`s][pydantic_ai.messages.ToolCallPart] containing the tool name, validated arguments, and a unique tool call ID.\n\nOnce the tool call results are ready, you can build a [`DeferredToolResults`][pydantic_ai.tools.DeferredToolResults] object with a `calls` dictionary that maps each tool call ID to an arbitrary value to be returned to the model, a [`ToolReturn`](tools-advanced.md#advanced-tool-returns) object, or a [`ModelRetry`][pydantic_ai.exceptions.ModelRetry] exception in case the tool call failed and the model should [try again](tools-advanced.md#tool-retries). This `DeferredToolResults` object can then be provided to one of the agent run methods as `deferred_tool_results`, alongside the original run's [message history](message-history.md).\n\nHere's an example that shows how to move a task that takes a while to complete to the background and return the result to the model once the task is complete:\n\n```python {title=\"external_tool.py\"}\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Any\n\nfrom pydantic_ai import (\n    Agent,\n    CallDeferred,\n    DeferredToolRequests,\n    DeferredToolResults,\n    ModelRetry,\n    RunContext,\n)\n\n\n@dataclass\nclass TaskResult:\n    tool_call_id: str\n    result: Any\n\n\nasync def calculate_answer_task(tool_call_id: str, question: str) -> TaskResult:\n    await asyncio.sleep(1)\n    return TaskResult(tool_call_id=tool_call_id, result=42)\n\n\nagent = Agent('openai:gpt-5', output_type=[str, DeferredToolRequests])\n\ntasks: list[asyncio.Task[TaskResult]] = []\n\n\n@agent.tool\nasync def calculate_answer(ctx: RunContext, question: str) -> str:\n    assert ctx.tool_call_id is not None\n\n    task = asyncio.create_task(calculate_answer_task(ctx.tool_call_id, question))  # (1)!\n    tasks.append(task)\n\n    raise CallDeferred\n\n\nasync def main():\n    result = await agent.run('Calculate the answer to the ultimate question of life, the universe, and everything')\n    messages = result.all_messages()\n\n    assert isinstance(result.output, DeferredToolRequests)\n    requests = result.output\n    print(requests)\n    \"\"\"\n    DeferredToolRequests(\n        calls=[\n            ToolCallPart(\n                tool_name='calculate_answer',\n                args={\n                    'question': 'the ultimate question of life, the universe, and everything'\n                },\n                tool_call_id='pyd_ai_tool_call_id',\n            )\n        ],\n        approvals=[],\n    )\n    \"\"\"\n\n    done, _ = await asyncio.wait(tasks)  # (2)!\n    task_results = [task.result() for task in done]\n    task_results_by_tool_call_id = {result.tool_call_id: result.result for result in task_results}\n\n    results = DeferredToolResults()\n    for call in requests.calls:\n        try:\n            result = task_results_by_tool_call_id[call.tool_call_id]\n        except KeyError:\n            result = ModelRetry('No result for this tool call was found.')\n\n        results.calls[call.tool_call_id] = result", "url": "https://ai.pydantic.dev/docs/deferred-tools/#external-tool-execution", "page": "docs/deferred-tools", "source_site": "pydantic_ai"}
{"title": "External Tool Execution", "anchor": "external-tool-execution", "heading_level": 2, "md_text": "    result = await agent.run(message_history=messages, deferred_tool_results=results)\n    print(result.output)\n    #> The answer to the ultimate question of life, the universe, and everything is 42.\n    print(result.all_messages())\n    \"\"\"\n    [\n        ModelRequest(\n            parts=[\n                UserPromptPart(\n                    content='Calculate the answer to the ultimate question of life, the universe, and everything',\n                    timestamp=datetime.datetime(...),\n                )\n            ]\n        ),\n        ModelResponse(\n            parts=[\n                ToolCallPart(\n                    tool_name='calculate_answer',\n                    args={\n                        'question': 'the ultimate question of life, the universe, and everything'\n                    },\n                    tool_call_id='pyd_ai_tool_call_id',\n                )\n            ],\n            usage=RequestUsage(input_tokens=63, output_tokens=13),\n            model_name='gpt-5',\n            timestamp=datetime.datetime(...),\n        ),\n        ModelRequest(\n            parts=[\n                ToolReturnPart(\n                    tool_name='calculate_answer',\n                    content=42,\n                    tool_call_id='pyd_ai_tool_call_id',\n                    timestamp=datetime.datetime(...),\n                )\n            ]\n        ),\n        ModelResponse(\n            parts=[\n                TextPart(\n                    content='The answer to the ultimate question of life, the universe, and everything is 42.'\n                )\n            ],\n            usage=RequestUsage(input_tokens=64, output_tokens=28),\n            model_name='gpt-5',\n            timestamp=datetime.datetime(...),\n        ),\n    ]\n    \"\"\"\n```\n\n1. In reality, you'd likely use Celery or a similar task queue to run the task in the background.\n2. In reality, this would typically happen in a separate process that polls for the task status or is notified when all pending tasks are complete.\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/deferred-tools/#external-tool-execution", "page": "docs/deferred-tools", "source_site": "pydantic_ai"}
{"title": "See Also", "anchor": "see-also", "heading_level": 2, "md_text": "- [Function Tools](tools.md) - Basic tool concepts and registration\n- [Advanced Tool Features](tools-advanced.md) - Custom schemas, dynamic tools, and execution details\n- [Toolsets](toolsets.md) - Managing collections of tools, including `ExternalToolset` for external tools\n- [Message History](message-history.md) - Understanding how to work with message history for deferred tools", "url": "https://ai.pydantic.dev/docs/deferred-tools/#see-also", "page": "docs/deferred-tools", "source_site": "pydantic_ai"}
{"title": "Built-in Tools", "anchor": "built-in-tools", "heading_level": 1, "md_text": "Built-in tools are native tools provided by LLM providers that can be used to enhance your agent's capabilities. Unlike [common tools](common-tools.md), which are custom implementations that Pydantic AI executes, built-in tools are executed directly by the model provider.", "url": "https://ai.pydantic.dev/docs/builtin-tools/#built-in-tools", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "Pydantic AI supports the following built-in tools:\n\n- **[`WebSearchTool`][pydantic_ai.builtin_tools.WebSearchTool]**: Allows agents to search the web\n- **[`CodeExecutionTool`][pydantic_ai.builtin_tools.CodeExecutionTool]**: Enables agents to execute code in a secure environment\n- **[`ImageGenerationTool`][pydantic_ai.builtin_tools.ImageGenerationTool]**: Enables agents to generate images\n- **[`UrlContextTool`][pydantic_ai.builtin_tools.UrlContextTool]**: Enables agents to pull URL contents into their context\n- **[`MemoryTool`][pydantic_ai.builtin_tools.MemoryTool]**: Enables agents to use memory\n- **[`MCPServerTool`][pydantic_ai.builtin_tools.MCPServerTool]**: Enables agents to use remote MCP servers with communication handled by the model provider\n\nThese tools are passed to the agent via the `builtin_tools` parameter and are executed by the model provider's infrastructure.\n\n!!! warning \"Provider Support\"\n    Not all model providers support built-in tools. If you use a built-in tool with an unsupported provider, Pydantic AI will raise a [`UserError`][pydantic_ai.exceptions.UserError] when you try to run the agent.\n\n    If a provider supports a built-in tool that is not currently supported by Pydantic AI, please file an issue.", "url": "https://ai.pydantic.dev/docs/builtin-tools/#overview", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "Web Search Tool", "anchor": "web-search-tool", "heading_level": 2, "md_text": "The [`WebSearchTool`][pydantic_ai.builtin_tools.WebSearchTool] allows your agent to search the web,\nmaking it ideal for queries that require up-to-date data.", "url": "https://ai.pydantic.dev/docs/builtin-tools/#web-search-tool", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "Provider Support", "anchor": "provider-support", "heading_level": 3, "md_text": "| Provider | Supported | Notes |\n|----------|-----------|-------|\n| OpenAI Responses | \u2705 | Full feature support. To include search results on the [`BuiltinToolReturnPart`][pydantic_ai.messages.BuiltinToolReturnPart] that's available via [`ModelResponse.builtin_tool_calls`][pydantic_ai.messages.ModelResponse.builtin_tool_calls], enable the [`OpenAIResponsesModelSettings.openai_include_web_search_sources`][pydantic_ai.models.openai.OpenAIResponsesModelSettings.openai_include_web_search_sources] [model setting](agents.md#model-run-settings). |\n| Anthropic | \u2705 | Full feature support |\n| Google | \u2705 | No parameter support. No [`BuiltinToolCallPart`][pydantic_ai.messages.BuiltinToolCallPart] or [`BuiltinToolReturnPart`][pydantic_ai.messages.BuiltinToolReturnPart] is generated when streaming. Using built-in tools and user tools (including [output tools](output.md#tool-output)) at the same time is not supported; to use structured output, use [`PromptedOutput`](output.md#prompted-output) instead. |\n| Groq | \u2705 | Limited parameter support. To use web search capabilities with Groq, you need to use the [compound models](https://console.groq.com/docs/compound). |\n| OpenAI Chat Completions | \u274c | Not supported |\n| Bedrock | \u274c | Not supported |\n| Mistral | \u274c | Not supported |\n| Cohere | \u274c | Not supported |\n| HuggingFace | \u274c | Not supported |\n| Outlines | \u274c | Not supported |", "url": "https://ai.pydantic.dev/docs/builtin-tools/#provider-support", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 3, "md_text": "```py {title=\"web_search_anthropic.py\"}\nfrom pydantic_ai import Agent, WebSearchTool\n\nagent = Agent('anthropic:claude-sonnet-4-0', builtin_tools=[WebSearchTool()])\n\nresult = agent.run_sync('Give me a sentence with the biggest news in AI this week.')\nprint(result.output)\n#> Scientists have developed a universal AI detector that can identify deepfake videos.\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nWith OpenAI, you must use their Responses API to access the web search tool.\n\n```py {title=\"web_search_openai.py\"}\nfrom pydantic_ai import Agent, WebSearchTool\n\nagent = Agent('openai-responses:gpt-5', builtin_tools=[WebSearchTool()])\n\nresult = agent.run_sync('Give me a sentence with the biggest news in AI this week.')\nprint(result.output)\n#> Scientists have developed a universal AI detector that can identify deepfake videos.\n```\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/builtin-tools/#usage", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "Configuration Options", "anchor": "configuration-options", "heading_level": 3, "md_text": "The `WebSearchTool` supports several configuration parameters:\n\n```py {title=\"web_search_configured.py\"}\nfrom pydantic_ai import Agent, WebSearchTool, WebSearchUserLocation\n\nagent = Agent(\n    'anthropic:claude-sonnet-4-0',\n    builtin_tools=[\n        WebSearchTool(\n            search_context_size='high',\n            user_location=WebSearchUserLocation(\n                city='San Francisco',\n                country='US',\n                region='CA',\n                timezone='America/Los_Angeles',\n            ),\n            blocked_domains=['example.com', 'spam-site.net'],\n            allowed_domains=None,  # Cannot use both blocked_domains and allowed_domains with Anthropic\n            max_uses=5,  # Anthropic only: limit tool usage\n        )\n    ],\n)\n\nresult = agent.run_sync('Use the web to get the current time.')\nprint(result.output)\n#> In San Francisco, it's 8:21:41 pm PDT on Wednesday, August 6, 2025.\n```\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/builtin-tools/#configuration-options", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "Provider Support", "anchor": "provider-support", "heading_level": 4, "md_text": "| Parameter | OpenAI | Anthropic | Groq |\n|-----------|--------|-----------|------|\n| `search_context_size` | \u2705 | \u274c | \u274c |\n| `user_location` | \u2705 | \u2705 | \u274c |\n| `blocked_domains` | \u274c | \u2705 | \u2705 |\n| `allowed_domains` | \u274c | \u2705 | \u2705 |\n| `max_uses` | \u274c | \u2705 | \u274c |\n\n!!! note \"Anthropic Domain Filtering\"\n    With Anthropic, you can only use either `blocked_domains` or `allowed_domains`, not both.", "url": "https://ai.pydantic.dev/docs/builtin-tools/#provider-support", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "Code Execution Tool", "anchor": "code-execution-tool", "heading_level": 2, "md_text": "The [`CodeExecutionTool`][pydantic_ai.builtin_tools.CodeExecutionTool] enables your agent to execute code\nin a secure environment, making it perfect for computational tasks, data analysis, and mathematical operations.", "url": "https://ai.pydantic.dev/docs/builtin-tools/#code-execution-tool", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "Provider Support", "anchor": "provider-support", "heading_level": 3, "md_text": "| Provider | Supported | Notes |\n|----------|-----------|-------|\n| OpenAI | \u2705 | To include code execution output on the [`BuiltinToolReturnPart`][pydantic_ai.messages.BuiltinToolReturnPart] that's available via [`ModelResponse.builtin_tool_calls`][pydantic_ai.messages.ModelResponse.builtin_tool_calls], enable the [`OpenAIResponsesModelSettings.openai_include_code_execution_outputs`][pydantic_ai.models.openai.OpenAIResponsesModelSettings.openai_include_code_execution_outputs] [model setting](agents.md#model-run-settings). If the code execution generated images, like charts, they will be available on [`ModelResponse.images`][pydantic_ai.messages.ModelResponse.images] as [`BinaryImage`][pydantic_ai.messages.BinaryImage] objects. The generated image can also be used as [image output](output.md#image-output) for the agent run. |\n| Google | \u2705 | Using built-in tools and user tools (including [output tools](output.md#tool-output)) at the same time is not supported; to use structured output, use [`PromptedOutput`](output.md#prompted-output) instead. |\n| Anthropic | \u2705 | |\n| Groq | \u274c | |\n| Bedrock | \u274c | |\n| Mistral | \u274c | |\n| Cohere | \u274c | |\n| HuggingFace | \u274c | |\n| Outlines | \u274c | |", "url": "https://ai.pydantic.dev/docs/builtin-tools/#provider-support", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 3, "md_text": "```py {title=\"code_execution_basic.py\"}\nfrom pydantic_ai import Agent, CodeExecutionTool\n\nagent = Agent('anthropic:claude-sonnet-4-0', builtin_tools=[CodeExecutionTool()])\n\nresult = agent.run_sync('Calculate the factorial of 15.')\nprint(result.output)\n#> The factorial of 15 is **1,307,674,368,000**.\nprint(result.response.builtin_tool_calls)\n\"\"\"\n[\n    (\n        BuiltinToolCallPart(\n            tool_name='code_execution',\n            args={\n                'code': 'import math\\n\\n# Calculate factorial of 15\\nresult = math.factorial(15)\\nprint(f\"15! = {result}\")\\n\\n# Let\\'s also show it in a more readable format with commas\\nprint(f\"15! = {result:,}\")'\n            },\n            tool_call_id='srvtoolu_017qRH1J3XrhnpjP2XtzPCmJ',\n            provider_name='anthropic',\n        ),\n        BuiltinToolReturnPart(\n            tool_name='code_execution',\n            content={\n                'content': [],\n                'return_code': 0,\n                'stderr': '',\n                'stdout': '15! = 1307674368000\\n15! = 1,307,674,368,000',\n                'type': 'code_execution_result',\n            },\n            tool_call_id='srvtoolu_017qRH1J3XrhnpjP2XtzPCmJ',\n            timestamp=datetime.datetime(...),\n            provider_name='anthropic',\n        ),\n    )\n]\n\"\"\"\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nIn addition to text output, code execution with OpenAI can generate images as part of their response. Accessing this image via [`ModelResponse.images`][pydantic_ai.messages.ModelResponse.images] or [image output](output.md#image-output) requires the [`OpenAIResponsesModelSettings.openai_include_code_execution_outputs`][pydantic_ai.models.openai.OpenAIResponsesModelSettings.openai_include_code_execution_outputs] [model setting](agents.md#model-run-settings) to be enabled.\n\n```py {title=\"code_execution_openai.py\"}\nfrom pydantic_ai import Agent, BinaryImage, CodeExecutionTool\nfrom pydantic_ai.models.openai import OpenAIResponsesModelSettings\n\nagent = Agent(\n    'openai-responses:gpt-5',\n    builtin_tools=[CodeExecutionTool()],\n    output_type=BinaryImage,\n    model_settings=OpenAIResponsesModelSettings(openai_include_code_execution_outputs=True),\n)\n\nresult = agent.run_sync('Generate a chart of y=x^2 for x=-5 to 5.')\nassert isinstance(result.output, BinaryImage)\n```\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/builtin-tools/#usage", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "Image Generation Tool", "anchor": "image-generation-tool", "heading_level": 2, "md_text": "The [`ImageGenerationTool`][pydantic_ai.builtin_tools.ImageGenerationTool] enables your agent to generate images.", "url": "https://ai.pydantic.dev/docs/builtin-tools/#image-generation-tool", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "Provider Support", "anchor": "provider-support", "heading_level": 3, "md_text": "| Provider | Supported | Notes |\n|----------|-----------|-------|\n| OpenAI Responses | \u2705 | Full feature support. Only supported by models newer than `gpt-5`. Metadata about the generated image, like the [`revised_prompt`](https://platform.openai.com/docs/guides/tools-image-generation#revised-prompt) sent to the underlying image model, is available on the [`BuiltinToolReturnPart`][pydantic_ai.messages.BuiltinToolReturnPart] that's available via [`ModelResponse.builtin_tool_calls`][pydantic_ai.messages.ModelResponse.builtin_tool_calls]. |\n| Google | \u2705 | No parameter support. Only supported by [image generation models](https://ai.google.dev/gemini-api/docs/image-generation) like `gemini-2.5-flash-image`. These models do not support [structured output](output.md) or [function tools](tools.md). These models will always generate images, even if this built-in tool is not explicitly specified. |\n| Anthropic | \u274c | |\n| Groq | \u274c | |\n| Bedrock | \u274c | |\n| Mistral | \u274c | |\n| Cohere | \u274c | |\n| HuggingFace | \u274c | |", "url": "https://ai.pydantic.dev/docs/builtin-tools/#provider-support", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 3, "md_text": "Generated images are available on [`ModelResponse.images`][pydantic_ai.messages.ModelResponse.images] as [`BinaryImage`][pydantic_ai.messages.BinaryImage] objects:\n\n```py {title=\"image_generation_openai.py\"}\nfrom pydantic_ai import Agent, BinaryImage, ImageGenerationTool\n\nagent = Agent('openai-responses:gpt-5', builtin_tools=[ImageGenerationTool()])\n\nresult = agent.run_sync('Tell me a two-sentence story about an axolotl with an illustration.')\nprint(result.output)\n\"\"\"\nOnce upon a time, in a hidden underwater cave, lived a curious axolotl named Pip who loved to explore. One day, while venturing further than usual, Pip discovered a shimmering, ancient coin that granted wishes!\n\"\"\"\n\nassert isinstance(result.response.images[0], BinaryImage)\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nImage generation with Google [image generation models](https://ai.google.dev/gemini-api/docs/image-generation) does not require the `ImageGenerationTool` built-in tool to be explicitly specified:\n\n```py {title=\"image_generation_google.py\"}\nfrom pydantic_ai import Agent, BinaryImage\n\nagent = Agent('google-gla:gemini-2.5-flash-image')\n\nresult = agent.run_sync('Tell me a two-sentence story about an axolotl with an illustration.')\nprint(result.output)\n\"\"\"\nOnce upon a time, in a hidden underwater cave, lived a curious axolotl named Pip who loved to explore. One day, while venturing further than usual, Pip discovered a shimmering, ancient coin that granted wishes!\n\"\"\"\n\nassert isinstance(result.response.images[0], BinaryImage)\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nThe `ImageGenerationTool` can be used together with `output_type=BinaryImage` to get [image output](output.md#image-output). If the `ImageGenerationTool` built-in tool is not explicitly specified, it will be enabled automatically:\n\n```py {title=\"image_generation_output.py\"}\nfrom pydantic_ai import Agent, BinaryImage\n\nagent = Agent('openai-responses:gpt-5', output_type=BinaryImage)\n\nresult = agent.run_sync('Generate an image of an axolotl.')\nassert isinstance(result.output, BinaryImage)\n```\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/builtin-tools/#usage", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "Configuration Options", "anchor": "configuration-options", "heading_level": 3, "md_text": "The `ImageGenerationTool` supports several configuration parameters:\n\n```py {title=\"image_generation_configured.py\"}\nfrom pydantic_ai import Agent, BinaryImage, ImageGenerationTool\n\nagent = Agent(\n    'openai-responses:gpt-5',\n    builtin_tools=[\n        ImageGenerationTool(\n            background='transparent',\n            input_fidelity='high',\n            moderation='low',\n            output_compression=100,\n            output_format='png',\n            partial_images=3,\n            quality='high',\n            size='1024x1024',\n        )\n    ],\n    output_type=BinaryImage,\n)\n\nresult = agent.run_sync('Generate an image of an axolotl.')\nassert isinstance(result.output, BinaryImage)\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nFor more details, check the [API documentation][pydantic_ai.builtin_tools.ImageGenerationTool].", "url": "https://ai.pydantic.dev/docs/builtin-tools/#configuration-options", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "Provider Support", "anchor": "provider-support", "heading_level": 4, "md_text": "| Parameter | OpenAI | Google |\n|-----------|--------|--------|\n| `background` | \u2705 | \u274c |\n| `input_fidelity` | \u2705 | \u274c |\n| `moderation` | \u2705 | \u274c |\n| `output_compression` | \u2705 | \u274c |\n| `output_format` | \u2705 | \u274c |\n| `partial_images` | \u2705 | \u274c |\n| `quality` | \u2705 | \u274c |\n| `size` | \u2705 | \u274c |", "url": "https://ai.pydantic.dev/docs/builtin-tools/#provider-support", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "URL Context Tool", "anchor": "url-context-tool", "heading_level": 2, "md_text": "The [`UrlContextTool`][pydantic_ai.builtin_tools.UrlContextTool] enables your agent to pull URL contents into its context,\nallowing it to pull up-to-date information from the web.", "url": "https://ai.pydantic.dev/docs/builtin-tools/#url-context-tool", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "Provider Support", "anchor": "provider-support", "heading_level": 3, "md_text": "| Provider | Supported | Notes |\n|----------|-----------|-------|\n| Google | \u2705 | No [`BuiltinToolCallPart`][pydantic_ai.messages.BuiltinToolCallPart] or [`BuiltinToolReturnPart`][pydantic_ai.messages.BuiltinToolReturnPart] is currently generated; please submit an issue if you need this. Using built-in tools and user tools (including [output tools](output.md#tool-output)) at the same time is not supported; to use structured output, use [`PromptedOutput`](output.md#prompted-output) instead. |\n| OpenAI | \u274c | |\n| Anthropic | \u274c | |\n| Groq | \u274c | |\n| Bedrock | \u274c | |\n| Mistral | \u274c | |\n| Cohere | \u274c | |\n| HuggingFace | \u274c | |\n| Outlines | \u274c | |", "url": "https://ai.pydantic.dev/docs/builtin-tools/#provider-support", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 3, "md_text": "```py {title=\"url_context_basic.py\"}\nfrom pydantic_ai import Agent, UrlContextTool\n\nagent = Agent('google-gla:gemini-2.5-flash', builtin_tools=[UrlContextTool()])\n\nresult = agent.run_sync('What is this? https://ai.pydantic.dev')\nprint(result.output)\n#> A Python agent framework for building Generative AI applications.\n```\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/builtin-tools/#usage", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "Memory Tool", "anchor": "memory-tool", "heading_level": 2, "md_text": "The [`MemoryTool`][pydantic_ai.builtin_tools.MemoryTool] enables your agent to use memory.", "url": "https://ai.pydantic.dev/docs/builtin-tools/#memory-tool", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "Provider Support", "anchor": "provider-support", "heading_level": 3, "md_text": "| Provider | Supported | Notes |\n|----------|-----------|-------|\n| Anthropic | \u2705 | Requires a tool named `memory` to be defined that implements [specific sub-commands](https://docs.claude.com/en/docs/agents-and-tools/tool-use/memory-tool#tool-commands). You can use a subclass of [`anthropic.lib.tools.BetaAbstractMemoryTool`](https://github.com/anthropics/anthropic-sdk-python/blob/main/src/anthropic/lib/tools/_beta_builtin_memory_tool.py) as documented below. |\n| Google | \u274c | |\n| OpenAI | \u274c | |\n| Groq | \u274c | |\n| Bedrock | \u274c | |\n| Mistral | \u274c | |\n| Cohere | \u274c | |\n| HuggingFace | \u274c | |", "url": "https://ai.pydantic.dev/docs/builtin-tools/#provider-support", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 3, "md_text": "The Anthropic SDK provides an abstract [`BetaAbstractMemoryTool`](https://github.com/anthropics/anthropic-sdk-python/blob/main/src/anthropic/lib/tools/_beta_builtin_memory_tool.py) class that you can subclass to create your own memory storage solution (e.g., database, cloud storage, encrypted files, etc.). Their [`LocalFilesystemMemoryTool`](https://github.com/anthropics/anthropic-sdk-python/blob/main/examples/memory/basic.py) example can serve as a starting point.\n\nThe following example uses a subclass that hard-codes a specific memory. The bits specific to Pydantic AI are the `MemoryTool` built-in tool and the `memory` tool definition that forwards commands to the `call` method of the `BetaAbstractMemoryTool` subclass.\n\n```py {title=\"anthropic_memory.py\"}\nfrom typing import Any\n\nfrom anthropic.lib.tools import BetaAbstractMemoryTool\nfrom anthropic.types.beta import (\n    BetaMemoryTool20250818CreateCommand,\n    BetaMemoryTool20250818DeleteCommand,\n    BetaMemoryTool20250818InsertCommand,\n    BetaMemoryTool20250818RenameCommand,\n    BetaMemoryTool20250818StrReplaceCommand,\n    BetaMemoryTool20250818ViewCommand,\n)\n\nfrom pydantic_ai import Agent, MemoryTool\n\n\nclass FakeMemoryTool(BetaAbstractMemoryTool):\n    def view(self, command: BetaMemoryTool20250818ViewCommand) -> str:\n        return 'The user lives in Mexico City.'\n\n    def create(self, command: BetaMemoryTool20250818CreateCommand) -> str:\n        return f'File created successfully at {command.path}'\n\n    def str_replace(self, command: BetaMemoryTool20250818StrReplaceCommand) -> str:\n        return f'File {command.path} has been edited'\n\n    def insert(self, command: BetaMemoryTool20250818InsertCommand) -> str:\n        return f'Text inserted at line {command.insert_line} in {command.path}'\n\n    def delete(self, command: BetaMemoryTool20250818DeleteCommand) -> str:\n        return f'File deleted: {command.path}'\n\n    def rename(self, command: BetaMemoryTool20250818RenameCommand) -> str:\n        return f'Renamed {command.old_path} to {command.new_path}'\n\n    def clear_all_memory(self) -> str:\n        return 'All memory cleared'\n\nfake_memory = FakeMemoryTool()\n\nagent = Agent('anthropic:claude-sonnet-4-5', builtin_tools=[MemoryTool()])\n\n\n@agent.tool_plain\ndef memory(**command: Any) -> Any:\n    return fake_memory.call(command)\n\n\nresult = agent.run_sync('Remember that I live in Mexico City')\nprint(result.output)\n\"\"\"\nGot it! I've recorded that you live in Mexico City. I'll remember this for future reference.\n\"\"\"\n\nresult = agent.run_sync('Where do I live?')\nprint(result.output)\n#> You live in Mexico City.\n```\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/builtin-tools/#usage", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "MCP Server Tool", "anchor": "mcp-server-tool", "heading_level": 2, "md_text": "The [`MCPServerTool`][pydantic_ai.builtin_tools.MCPServerTool] allows your agent to use remote MCP servers with communication handled by the model provider.\n\nThis requires the MCP server to live at a public URL the provider can reach and does not support many of the advanced features of Pydantic AI's agent-side [MCP support](mcp/client.md),\nbut can result in optimized context use and caching, and faster performance due to the lack of a round-trip back to Pydantic AI.", "url": "https://ai.pydantic.dev/docs/builtin-tools/#mcp-server-tool", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "Provider Support", "anchor": "provider-support", "heading_level": 3, "md_text": "| Provider | Supported | Notes                 |\n|----------|-----------|-----------------------|\n| OpenAI Responses | \u2705 | Full feature support. [Connectors](https://platform.openai.com/docs/guides/tools-connectors-mcp#connectors) can be used by specifying a special `x-openai-connector:<connector_id>` URL.  |\n| Anthropic | \u2705 | Full feature support |\n| Google  | \u274c | Not supported |\n| Groq  | \u274c | Not supported |\n| OpenAI Chat Completions | \u274c | Not supported |\n| Bedrock | \u274c | Not supported |\n| Mistral | \u274c | Not supported |\n| Cohere | \u274c | Not supported |\n| HuggingFace | \u274c | Not supported |", "url": "https://ai.pydantic.dev/docs/builtin-tools/#provider-support", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 3, "md_text": "```py {title=\"mcp_server_anthropic.py\"}\nfrom pydantic_ai import Agent, MCPServerTool\n\nagent = Agent(\n    'anthropic:claude-sonnet-4-5',\n    builtin_tools=[\n        MCPServerTool(\n            id='deepwiki',\n            url='https://mcp.deepwiki.com/mcp',  # (1)\n        )\n    ]\n)\n\nresult = agent.run_sync('Tell me about the pydantic/pydantic-ai repo.')\nprint(result.output)\n\"\"\"\nThe pydantic/pydantic-ai repo is a Python agent framework for building Generative AI applications.\n\"\"\"\n```\n\n1. The [DeepWiki MCP server](https://docs.devin.ai/work-with-devin/deepwiki-mcp) does not require authorization.\n\n_(This example is complete, it can be run \"as is\")_\n\nWith OpenAI, you must use their Responses API to access the MCP server tool:\n\n```py {title=\"mcp_server_openai.py\"}\nfrom pydantic_ai import Agent, MCPServerTool\n\nagent = Agent(\n    'openai-responses:gpt-5',\n    builtin_tools=[\n        MCPServerTool(\n            id='deepwiki',\n            url='https://mcp.deepwiki.com/mcp',  # (1)\n        )\n    ]\n)\n\nresult = agent.run_sync('Tell me about the pydantic/pydantic-ai repo.')\nprint(result.output)\n\"\"\"\nThe pydantic/pydantic-ai repo is a Python agent framework for building Generative AI applications.\n\"\"\"\n```\n\n1. The [DeepWiki MCP server](https://docs.devin.ai/work-with-devin/deepwiki-mcp) does not require authorization.\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/builtin-tools/#usage", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "Configuration Options", "anchor": "configuration-options", "heading_level": 3, "md_text": "The `MCPServerTool` supports several configuration parameters for custom MCP servers:\n\n```py {title=\"mcp_server_configured_url.py\"}\nimport os\n\nfrom pydantic_ai import Agent, MCPServerTool\n\nagent = Agent(\n    'openai-responses:gpt-5',\n    builtin_tools=[\n        MCPServerTool(\n            id='github',\n            url='https://api.githubcopilot.com/mcp/',\n            authorization_token=os.getenv('GITHUB_ACCESS_TOKEN', 'mock-access-token'),  # (1)\n            allowed_tools=['search_repositories', 'list_commits'],\n            description='GitHub MCP server',\n            headers={'X-Custom-Header': 'custom-value'},\n        )\n    ]\n)\n\nresult = agent.run_sync('Tell me about the pydantic/pydantic-ai repo.')\nprint(result.output)\n\"\"\"\nThe pydantic/pydantic-ai repo is a Python agent framework for building Generative AI applications.\n\"\"\"\n```\n\n1. The [GitHub MCP server](https://github.com/github/github-mcp-server) requires an authorization token.\n\nFor OpenAI Responses, you can use a [connector](https://platform.openai.com/docs/guides/tools-connectors-mcp#connectors) by specifying a special `x-openai-connector:` URL:\n\n_(This example is complete, it can be run \"as is\")_\n\n```py {title=\"mcp_server_configured_connector_id.py\"}\nimport os\n\nfrom pydantic_ai import Agent, MCPServerTool\n\nagent = Agent(\n    'openai-responses:gpt-5',\n    builtin_tools=[\n        MCPServerTool(\n            id='google-calendar',\n            url='x-openai-connector:connector_googlecalendar',\n            authorization_token=os.getenv('GOOGLE_API_KEY', 'mock-api-key'), # (1)\n        )\n    ]\n)\n\nresult = agent.run_sync('What do I have on my calendar today?')\nprint(result.output)\n#> You're going to spend all day playing with Pydantic AI.\n```\n\n1. OpenAI's Google Calendar connector requires an [authorization token](https://platform.openai.com/docs/guides/tools-connectors-mcp#authorizing-a-connector).\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/builtin-tools/#configuration-options", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "Provider Support", "anchor": "provider-support", "heading_level": 4, "md_text": "| Parameter             | OpenAI | Anthropic |\n|-----------------------|--------|-----------|\n| `authorization_token` | \u2705 | \u2705 |\n| `allowed_tools`       | \u2705 | \u2705 |\n| `description`         | \u2705 | \u274c |\n| `headers`             | \u2705 | \u274c |", "url": "https://ai.pydantic.dev/docs/builtin-tools/#provider-support", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "API Reference", "anchor": "api-reference", "heading_level": 2, "md_text": "For complete API documentation, see the [API Reference](api/builtin_tools.md).", "url": "https://ai.pydantic.dev/docs/builtin-tools/#api-reference", "page": "docs/builtin-tools", "source_site": "pydantic_ai"}
{"title": "Third-Party Tools", "anchor": "third-party-tools", "heading_level": 1, "md_text": "Pydantic AI supports integration with various third-party tool libraries, allowing you to leverage existing tool ecosystems in your agents.", "url": "https://ai.pydantic.dev/docs/third-party-tools/#third-party-tools", "page": "docs/third-party-tools", "source_site": "pydantic_ai"}
{"title": "MCP Tools {#mcp-tools}", "anchor": "mcp-tools-mcp-tools", "heading_level": 2, "md_text": "See the [MCP Client](./mcp/client.md) documentation for how to use MCP servers with Pydantic AI as [toolsets](toolsets.md).", "url": "https://ai.pydantic.dev/docs/third-party-tools/#mcp-tools-mcp-tools", "page": "docs/third-party-tools", "source_site": "pydantic_ai"}
{"title": "LangChain Tools {#langchain-tools}", "anchor": "langchain-tools-langchain-tools", "heading_level": 2, "md_text": "If you'd like to use a tool from LangChain's [community tool library](https://python.langchain.com/docs/integrations/tools/) with Pydantic AI, you can use the [`tool_from_langchain`][pydantic_ai.ext.langchain.tool_from_langchain] convenience method. Note that Pydantic AI will not validate the arguments in this case -- it's up to the model to provide arguments matching the schema specified by the LangChain tool, and up to the LangChain tool to raise an error if the arguments are invalid.\n\nYou will need to install the `langchain-community` package and any others required by the tool in question.\n\nHere is how you can use the LangChain `DuckDuckGoSearchRun` tool, which requires the `ddgs` package:\n\n```python {test=\"skip\"}\nfrom langchain_community.tools import DuckDuckGoSearchRun\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ext.langchain import tool_from_langchain\n\nsearch = DuckDuckGoSearchRun()\nsearch_tool = tool_from_langchain(search)\n\nagent = Agent(\n    'google-gla:gemini-2.5-flash',\n    tools=[search_tool],\n)\n\nresult = agent.run_sync('What is the release date of Elden Ring Nightreign?')  # (1)!\nprint(result.output)\n#> Elden Ring Nightreign is planned to be released on May 30, 2025.\n```\n\n1. The release date of this game is the 30th of May 2025, which is after the knowledge cutoff for Gemini 2.0 (August 2024).\n\nIf you'd like to use multiple LangChain tools or a LangChain [toolkit](https://python.langchain.com/docs/concepts/tools/#toolkits), you can use the [`LangChainToolset`][pydantic_ai.ext.langchain.LangChainToolset] [toolset](toolsets.md) which takes a list of LangChain tools:\n\n```python {test=\"skip\"}\nfrom langchain_community.agent_toolkits import SlackToolkit\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ext.langchain import LangChainToolset\n\ntoolkit = SlackToolkit()\ntoolset = LangChainToolset(toolkit.get_tools())\n\nagent = Agent('openai:gpt-5', toolsets=[toolset])", "url": "https://ai.pydantic.dev/docs/third-party-tools/#langchain-tools-langchain-tools", "page": "docs/third-party-tools", "source_site": "pydantic_ai"}
{"title": "...", "anchor": "", "heading_level": 1, "md_text": "```", "url": "https://ai.pydantic.dev/docs/third-party-tools/", "page": "docs/third-party-tools", "source_site": "pydantic_ai"}
{"title": "ACI.dev Tools {#aci-tools}", "anchor": "acidev-tools-aci-tools", "heading_level": 2, "md_text": "If you'd like to use a tool from the [ACI.dev tool library](https://www.aci.dev/tools) with Pydantic AI, you can use the [`tool_from_aci`][pydantic_ai.ext.aci.tool_from_aci] convenience method. Note that Pydantic AI will not validate the arguments in this case -- it's up to the model to provide arguments matching the schema specified by the ACI tool, and up to the ACI tool to raise an error if the arguments are invalid.\n\nYou will need to install the `aci-sdk` package, set your ACI API key in the `ACI_API_KEY` environment variable, and pass your ACI \"linked account owner ID\" to the function.\n\nHere is how you can use the ACI.dev `TAVILY__SEARCH` tool:\n\n```python {test=\"skip\"}\nimport os\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ext.aci import tool_from_aci\n\ntavily_search = tool_from_aci(\n    'TAVILY__SEARCH',\n    linked_account_owner_id=os.getenv('LINKED_ACCOUNT_OWNER_ID'),\n)\n\nagent = Agent(\n    'google-gla:gemini-2.5-flash',\n    tools=[tavily_search],\n)\n\nresult = agent.run_sync('What is the release date of Elden Ring Nightreign?')  # (1)!\nprint(result.output)\n#> Elden Ring Nightreign is planned to be released on May 30, 2025.\n```\n\n1. The release date of this game is the 30th of May 2025, which is after the knowledge cutoff for Gemini 2.0 (August 2024).\n\nIf you'd like to use multiple ACI.dev tools, you can use the [`ACIToolset`][pydantic_ai.ext.aci.ACIToolset] [toolset](toolsets.md) which takes a list of ACI tool names as well as the `linked_account_owner_id`:\n\n```python {test=\"skip\"}\nimport os\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ext.aci import ACIToolset\n\ntoolset = ACIToolset(\n    [\n        'OPEN_WEATHER_MAP__CURRENT_WEATHER',\n        'OPEN_WEATHER_MAP__FORECAST',\n    ],\n    linked_account_owner_id=os.getenv('LINKED_ACCOUNT_OWNER_ID'),\n)\n\nagent = Agent('openai:gpt-5', toolsets=[toolset])\n```", "url": "https://ai.pydantic.dev/docs/third-party-tools/#acidev-tools-aci-tools", "page": "docs/third-party-tools", "source_site": "pydantic_ai"}
{"title": "See Also", "anchor": "see-also", "heading_level": 2, "md_text": "- [Function Tools](tools.md) - Basic tool concepts and registration\n- [Toolsets](toolsets.md) - Managing collections of tools\n- [MCP Client](mcp/client.md) - Using MCP servers with Pydantic AI\n- [LangChain Toolsets](toolsets.md#langchain-tools) - Using LangChain toolsets\n- [ACI.dev Toolsets](toolsets.md#aci-tools) - Using ACI.dev toolsets", "url": "https://ai.pydantic.dev/docs/third-party-tools/#see-also", "page": "docs/third-party-tools", "source_site": "pydantic_ai"}
{"title": "Troubleshooting", "anchor": "troubleshooting", "heading_level": 1, "md_text": "Below are suggestions on how to fix some common errors you might encounter while using Pydantic AI. If the issue you're experiencing is not listed below or addressed in the documentation, please feel free to ask in the [Pydantic Slack](help.md) or create an issue on [GitHub](https://github.com/pydantic/pydantic-ai/issues).", "url": "https://ai.pydantic.dev/docs/troubleshooting/#troubleshooting", "page": "docs/troubleshooting", "source_site": "pydantic_ai"}
{"title": "`RuntimeError: This event loop is already running`", "anchor": "runtimeerror-this-event-loop-is-already-running", "heading_level": 3, "md_text": "This error is caused by conflicts between the event loops in Jupyter notebook and Pydantic AI's. One way to manage these conflicts is by using [`nest-asyncio`](https://pypi.org/project/nest-asyncio/). Namely, before you execute any agent runs, do the following:\n\n```python {test=\"skip\"}\nimport nest_asyncio\n\nnest_asyncio.apply()\n```\n\nNote: This fix also applies to Google Colab and [Marimo](https://github.com/marimo-team/marimo).", "url": "https://ai.pydantic.dev/docs/troubleshooting/#runtimeerror-this-event-loop-is-already-running", "page": "docs/troubleshooting", "source_site": "pydantic_ai"}
{"title": "`UserError: API key must be provided or set in the [MODEL]_API_KEY environment variable`", "anchor": "usererror-api-key-must-be-provided-or-set-in-the-model_api_key-environment-variable", "heading_level": 3, "md_text": "If you're running into issues with setting the API key for your model, visit the [Models](models/overview.md) page to learn more about how to set an environment variable and/or pass in an `api_key` argument.", "url": "https://ai.pydantic.dev/docs/troubleshooting/#usererror-api-key-must-be-provided-or-set-in-the-model_api_key-environment-variable", "page": "docs/troubleshooting", "source_site": "pydantic_ai"}
{"title": "Monitoring HTTPX Requests", "anchor": "monitoring-httpx-requests", "heading_level": 2, "md_text": "You can use custom `httpx` clients in your models in order to access specific requests, responses, and headers at runtime.\n\nIt's particularly helpful to use `logfire`'s [HTTPX integration](logfire.md#monitoring-http-requests) to monitor the above.", "url": "https://ai.pydantic.dev/docs/troubleshooting/#monitoring-httpx-requests", "page": "docs/troubleshooting", "source_site": "pydantic_ai"}
{"title": "Advanced Tool Features", "anchor": "advanced-tool-features", "heading_level": 1, "md_text": "This page covers advanced features for function tools in Pydantic AI. For basic tool usage, see the [Function Tools](tools.md) documentation.", "url": "https://ai.pydantic.dev/docs/tools-advanced/#advanced-tool-features", "page": "docs/tools-advanced", "source_site": "pydantic_ai"}
{"title": "Tool Output {#function-tool-output}", "anchor": "tool-output-function-tool-output", "heading_level": 2, "md_text": "Tools can return anything that Pydantic can serialize to JSON, as well as audio, video, image or document content depending on the types of [multi-modal input](input.md) the model supports:\n\n```python {title=\"function_tool_output.py\"}\nfrom datetime import datetime\n\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, DocumentUrl, ImageUrl\nfrom pydantic_ai.models.openai import OpenAIResponsesModel\n\n\nclass User(BaseModel):\n    name: str\n    age: int\n\n\nagent = Agent(model=OpenAIResponsesModel('gpt-5'))\n\n\n@agent.tool_plain\ndef get_current_time() -> datetime:\n    return datetime.now()\n\n\n@agent.tool_plain\ndef get_user() -> User:\n    return User(name='John', age=30)\n\n\n@agent.tool_plain\ndef get_company_logo() -> ImageUrl:\n    return ImageUrl(url='https://iili.io/3Hs4FMg.png')\n\n\n@agent.tool_plain\ndef get_document() -> DocumentUrl:\n    return DocumentUrl(url='https://www.w3.org/WAI/ER/tests/xhtml/testfiles/resources/pdf/dummy.pdf')\n\n\nresult = agent.run_sync('What time is it?')\nprint(result.output)\n#> The current time is 10:45 PM on April 17, 2025.\n\nresult = agent.run_sync('What is the user name?')\nprint(result.output)\n#> The user's name is John.\n\nresult = agent.run_sync('What is the company name in the logo?')\nprint(result.output)\n#> The company name in the logo is \"Pydantic.\"\n\nresult = agent.run_sync('What is the main content of the document?')\nprint(result.output)\n#> The document contains just the text \"Dummy PDF file.\"\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nSome models (e.g. Gemini) natively support semi-structured return values, while some expect text (OpenAI) but seem to be just as good at extracting meaning from the data. If a Python object is returned and the model expects a string, the value will be serialized to JSON.", "url": "https://ai.pydantic.dev/docs/tools-advanced/#tool-output-function-tool-output", "page": "docs/tools-advanced", "source_site": "pydantic_ai"}
{"title": "Advanced Tool Returns", "anchor": "advanced-tool-returns", "heading_level": 3, "md_text": "For scenarios where you need more control over both the tool's return value and the content sent to the model, you can use [`ToolReturn`][pydantic_ai.messages.ToolReturn]. This is particularly useful when you want to:\n\n- Provide rich multi-modal content (images, documents, etc.) to the model as context\n- Separate the programmatic return value from the model's context\n- Include additional metadata that shouldn't be sent to the LLM\n\nHere's an example of a computer automation tool that captures screenshots and provides visual feedback:\n\n```python {title=\"advanced_tool_return.py\" test=\"skip\" lint=\"skip\"}\nimport time\nfrom pydantic_ai import Agent\nfrom pydantic_ai import ToolReturn, BinaryContent\n\nagent = Agent('openai:gpt-5')\n\n@agent.tool_plain\ndef click_and_capture(x: int, y: int) -> ToolReturn:\n    \"\"\"Click at coordinates and show before/after screenshots.\"\"\"\n    # Take screenshot before action\n    before_screenshot = capture_screen()\n\n    # Perform click operation\n    perform_click(x, y)\n    time.sleep(0.5)  # Wait for UI to update\n\n    # Take screenshot after action\n    after_screenshot = capture_screen()\n\n    return ToolReturn(\n        return_value=f\"Successfully clicked at ({x}, {y})\",\n        content=[\n            f\"Clicked at coordinates ({x}, {y}). Here's the comparison:\",\n            \"Before:\",\n            BinaryContent(data=before_screenshot, media_type=\"image/png\"),\n            \"After:\",\n            BinaryContent(data=after_screenshot, media_type=\"image/png\"),\n            \"Please analyze the changes and suggest next steps.\"\n        ],\n        metadata={\n            \"coordinates\": {\"x\": x, \"y\": y},\n            \"action_type\": \"click_and_capture\",\n            \"timestamp\": time.time()\n        }\n    )", "url": "https://ai.pydantic.dev/docs/tools-advanced/#advanced-tool-returns", "page": "docs/tools-advanced", "source_site": "pydantic_ai"}
{"title": "while your application can access the structured return_value and metadata", "anchor": "while-your-application-can-access-the-structured-return_value-and-metadata", "heading_level": 1, "md_text": "result = agent.run_sync(\"Click on the submit button and tell me what happened\")\nprint(result.output)", "url": "https://ai.pydantic.dev/docs/tools-advanced/#while-your-application-can-access-the-structured-return_value-and-metadata", "page": "docs/tools-advanced", "source_site": "pydantic_ai"}
{"title": "The model can analyze the screenshots and provide detailed feedback", "anchor": "the-model-can-analyze-the-screenshots-and-provide-detailed-feedback", "heading_level": 1, "md_text": "```\n\n- **`return_value`**: The actual return value used in the tool response. This is what gets serialized and sent back to the model as the tool's result.\n- **`content`**: A sequence of content (text, images, documents, etc.) that provides additional context to the model. This appears as a separate user message.\n- **`metadata`**: Optional metadata that your application can access but is not sent to the LLM. Useful for logging, debugging, or additional processing. Some other AI frameworks call this feature \"artifacts\".\n\nThis separation allows you to provide rich context to the model while maintaining clean, structured return values for your application logic.", "url": "https://ai.pydantic.dev/docs/tools-advanced/#the-model-can-analyze-the-screenshots-and-provide-detailed-feedback", "page": "docs/tools-advanced", "source_site": "pydantic_ai"}
{"title": "Custom Tool Schema", "anchor": "custom-tool-schema", "heading_level": 2, "md_text": "If you have a function that lacks appropriate documentation (i.e. poorly named, no type information, poor docstring, use of \\*args or \\*\\*kwargs and suchlike) then you can still turn it into a tool that can be effectively used by the agent with the [`Tool.from_schema`][pydantic_ai.Tool.from_schema] function. With this you provide the name, description, JSON schema, and whether the function takes a `RunContext` for the function directly:\n\n```python\nfrom pydantic_ai import Agent, Tool\nfrom pydantic_ai.models.test import TestModel\n\n\ndef foobar(**kwargs) -> str:\n    return kwargs['a'] + kwargs['b']\n\ntool = Tool.from_schema(\n    function=foobar,\n    name='sum',\n    description='Sum two numbers.',\n    json_schema={\n        'additionalProperties': False,\n        'properties': {\n            'a': {'description': 'the first number', 'type': 'integer'},\n            'b': {'description': 'the second number', 'type': 'integer'},\n        },\n        'required': ['a', 'b'],\n        'type': 'object',\n    },\n    takes_ctx=False,\n)\n\ntest_model = TestModel()\nagent = Agent(test_model, tools=[tool])\n\nresult = agent.run_sync('testing...')\nprint(result.output)\n#> {\"sum\":0}\n```\n\nPlease note that validation of the tool arguments will not be performed, and this will pass all arguments as keyword arguments.", "url": "https://ai.pydantic.dev/docs/tools-advanced/#custom-tool-schema", "page": "docs/tools-advanced", "source_site": "pydantic_ai"}
{"title": "Dynamic Tools {#tool-prepare}", "anchor": "dynamic-tools-tool-prepare", "heading_level": 2, "md_text": "Tools can optionally be defined with another function: `prepare`, which is called at each step of a run to\ncustomize the definition of the tool passed to the model, or omit the tool completely from that step.\n\nA `prepare` method can be registered via the `prepare` kwarg to any of the tool registration mechanisms:\n\n- [`@agent.tool`][pydantic_ai.Agent.tool] decorator\n- [`@agent.tool_plain`][pydantic_ai.Agent.tool_plain] decorator\n- [`Tool`][pydantic_ai.tools.Tool] dataclass\n\nThe `prepare` method, should be of type [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc], a function which takes [`RunContext`][pydantic_ai.tools.RunContext] and a pre-built [`ToolDefinition`][pydantic_ai.tools.ToolDefinition], and should either return that `ToolDefinition` with or without modifying it, return a new `ToolDefinition`, or return `None` to indicate this tools should not be registered for that step.\n\nHere's a simple `prepare` method that only includes the tool if the value of the dependency is `42`.\n\nAs with the previous example, we use [`TestModel`][pydantic_ai.models.test.TestModel] to demonstrate the behavior without calling a real model.\n\n```python {title=\"tool_only_if_42.py\"}\n\nfrom pydantic_ai import Agent, RunContext, ToolDefinition\n\nagent = Agent('test')\n\n\nasync def only_if_42(\n    ctx: RunContext[int], tool_def: ToolDefinition\n) -> ToolDefinition | None:\n    if ctx.deps == 42:\n        return tool_def\n\n\n@agent.tool(prepare=only_if_42)\ndef hitchhiker(ctx: RunContext[int], answer: str) -> str:\n    return f'{ctx.deps} {answer}'\n\n\nresult = agent.run_sync('testing...', deps=41)\nprint(result.output)\n#> success (no tool calls)\nresult = agent.run_sync('testing...', deps=42)\nprint(result.output)\n#> {\"hitchhiker\":\"42 a\"}\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nHere's a more complex example where we change the description of the `name` parameter to based on the value of `deps`\n\nFor the sake of variation, we create this tool using the [`Tool`][pydantic_ai.tools.Tool] dataclass.\n\n```python {title=\"customize_name.py\"}\nfrom __future__ import annotations\n\nfrom typing import Literal\n\nfrom pydantic_ai import Agent, RunContext, Tool, ToolDefinition\nfrom pydantic_ai.models.test import TestModel\n\n\ndef greet(name: str) -> str:\n    return f'hello {name}'\n\n\nasync def prepare_greet(\n    ctx: RunContext[Literal['human', 'machine']], tool_def: ToolDefinition\n) -> ToolDefinition | None:\n    d = f'Name of the {ctx.deps} to greet.'\n    tool_def.parameters_json_schema['properties']['name']['description'] = d\n    return tool_def\n\n\ngreet_tool = Tool(greet, prepare=prepare_greet)\ntest_model = TestModel()\nagent = Agent(test_model, tools=[greet_tool], deps_type=Literal['human', 'machine'])\n\nresult = agent.run_sync('testing...', deps='human')\nprint(result.output)\n#> {\"greet\":\"hello a\"}\nprint(test_model.last_model_request_parameters.function_tools)\n\"\"\"\n[\n    ToolDefinition(\n        name='greet',\n        parameters_json_schema={\n            'additionalProperties': False,\n            'properties': {\n                'name': {'type': 'string', 'description': 'Name of the human to greet.'}\n            },\n            'required': ['name'],\n            'type': 'object',\n        },\n    )\n]\n\"\"\"\n```\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/tools-advanced/#dynamic-tools-tool-prepare", "page": "docs/tools-advanced", "source_site": "pydantic_ai"}
{"title": "Agent-wide Dynamic Tools {#prepare-tools}", "anchor": "agent-wide-dynamic-tools-prepare-tools", "heading_level": 3, "md_text": "In addition to per-tool `prepare` methods, you can also define an agent-wide `prepare_tools` function. This function is called at each step of a run and allows you to filter or modify the list of all tool definitions available to the agent for that step. This is especially useful if you want to enable or disable multiple tools at once, or apply global logic based on the current context.\n\nThe `prepare_tools` function should be of type [`ToolsPrepareFunc`][pydantic_ai.tools.ToolsPrepareFunc], which takes the [`RunContext`][pydantic_ai.tools.RunContext] and a list of [`ToolDefinition`][pydantic_ai.tools.ToolDefinition], and returns a new list of tool definitions (or `None` to disable all tools for that step).\n\n!!! note\n    The list of tool definitions passed to `prepare_tools` includes both regular function tools and tools from any [toolsets](toolsets.md) registered on the agent, but not [output tools](output.md#tool-output).\nTo modify output tools, you can set a `prepare_output_tools` function instead.\n\nHere's an example that makes all tools strict if the model is an OpenAI model:\n\n```python {title=\"agent_prepare_tools_customize.py\" noqa=\"I001\"}\nfrom dataclasses import replace\n\nfrom pydantic_ai import Agent, RunContext, ToolDefinition\nfrom pydantic_ai.models.test import TestModel\n\n\nasync def turn_on_strict_if_openai(\n    ctx: RunContext[None], tool_defs: list[ToolDefinition]\n) -> list[ToolDefinition] | None:\n    if ctx.model.system == 'openai':\n        return [replace(tool_def, strict=True) for tool_def in tool_defs]\n    return tool_defs\n\n\ntest_model = TestModel()\nagent = Agent(test_model, prepare_tools=turn_on_strict_if_openai)\n\n\n@agent.tool_plain\ndef echo(message: str) -> str:\n    return message\n\n\nagent.run_sync('testing...')\nassert test_model.last_model_request_parameters.function_tools[0].strict is None", "url": "https://ai.pydantic.dev/docs/tools-advanced/#agent-wide-dynamic-tools-prepare-tools", "page": "docs/tools-advanced", "source_site": "pydantic_ai"}
{"title": "Set the system attribute of the test_model to 'openai'", "anchor": "set-the-system-attribute-of-the-test_model-to-openai", "heading_level": 1, "md_text": "test_model._system = 'openai'\n\nagent.run_sync('testing with openai...')\nassert test_model.last_model_request_parameters.function_tools[0].strict\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nHere's another example that conditionally filters out the tools by name if the dependency (`ctx.deps`) is `True`:\n\n```python {title=\"agent_prepare_tools_filter_out.py\" noqa=\"I001\"}\n\nfrom pydantic_ai import Agent, RunContext, Tool, ToolDefinition\n\n\ndef launch_potato(target: str) -> str:\n    return f'Potato launched at {target}!'\n\n\nasync def filter_out_tools_by_name(\n    ctx: RunContext[bool], tool_defs: list[ToolDefinition]\n) -> list[ToolDefinition] | None:\n    if ctx.deps:\n        return [tool_def for tool_def in tool_defs if tool_def.name != 'launch_potato']\n    return tool_defs\n\n\nagent = Agent(\n    'test',\n    tools=[Tool(launch_potato)],\n    prepare_tools=filter_out_tools_by_name,\n    deps_type=bool,\n)\n\nresult = agent.run_sync('testing...', deps=False)\nprint(result.output)\n#> {\"launch_potato\":\"Potato launched at a!\"}\nresult = agent.run_sync('testing...', deps=True)\nprint(result.output)\n#> success (no tool calls)\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nYou can use `prepare_tools` to:\n\n- Dynamically enable or disable tools based on the current model, dependencies, or other context\n- Modify tool definitions globally (e.g., set all tools to strict mode, change descriptions, etc.)\n\nIf both per-tool `prepare` and agent-wide `prepare_tools` are used, the per-tool `prepare` is applied first to each tool, and then `prepare_tools` is called with the resulting list of tool definitions.", "url": "https://ai.pydantic.dev/docs/tools-advanced/#set-the-system-attribute-of-the-test_model-to-openai", "page": "docs/tools-advanced", "source_site": "pydantic_ai"}
{"title": "Tool Execution and Retries {#tool-retries}", "anchor": "tool-execution-and-retries-tool-retries", "heading_level": 2, "md_text": "When a tool is executed, its arguments (provided by the LLM) are first validated against the function's signature using Pydantic. If validation fails (e.g., due to incorrect types or missing required arguments), a `ValidationError` is raised, and the framework automatically generates a [`RetryPromptPart`][pydantic_ai.messages.RetryPromptPart] containing the validation details. This prompt is sent back to the LLM, informing it of the error and allowing it to correct the parameters and retry the tool call.\n\nBeyond automatic validation errors, the tool's own internal logic can also explicitly request a retry by raising the [`ModelRetry`][pydantic_ai.exceptions.ModelRetry] exception. This is useful for situations where the parameters were technically valid, but an issue occurred during execution (like a transient network error, or the tool determining the initial attempt needs modification).\n\n```python\nfrom pydantic_ai import ModelRetry\n\n\ndef my_flaky_tool(query: str) -> str:\n    if query == 'bad':\n        # Tell the LLM the query was bad and it should try again\n        raise ModelRetry(\"The query 'bad' is not allowed. Please provide a different query.\")\n    # ... process query ...\n    return 'Success!'\n```\n\nRaising `ModelRetry` also generates a `RetryPromptPart` containing the exception message, which is sent back to the LLM to guide its next attempt. Both `ValidationError` and `ModelRetry` respect the `retries` setting configured on the `Tool` or `Agent`.", "url": "https://ai.pydantic.dev/docs/tools-advanced/#tool-execution-and-retries-tool-retries", "page": "docs/tools-advanced", "source_site": "pydantic_ai"}
{"title": "Parallel tool calls & concurrency", "anchor": "parallel-tool-calls-concurrency", "heading_level": 3, "md_text": "When a model returns multiple tool calls in one response, Pydantic AI schedules them concurrently using `asyncio.create_task`.\nIf a tool requires sequential/serial execution, you can pass the [`sequential`][pydantic_ai.tools.ToolDefinition.sequential] flag when registering the tool, or wrap the agent run in the [`with agent.sequential_tool_calls()`][pydantic_ai.agent.AbstractAgent.sequential_tool_calls] context manager.\n\nAsync functions are run on the event loop, while sync functions are offloaded to threads. To get the best performance, _always_ use an async function _unless_ you're doing blocking I/O (and there's no way to use a non-blocking library instead) or CPU-bound work (like `numpy` or `scikit-learn` operations), so that simple functions are not offloaded to threads unnecessarily.\n\n!!! note \"Limiting tool executions\"\n    You can cap tool executions within a run using [`UsageLimits(tool_calls_limit=...)`](agents.md#usage-limits). The counter increments only after a successful tool invocation. Output tools (used for [structured output](output.md)) are not counted in the `tool_calls` metric.", "url": "https://ai.pydantic.dev/docs/tools-advanced/#parallel-tool-calls-concurrency", "page": "docs/tools-advanced", "source_site": "pydantic_ai"}
{"title": "See Also", "anchor": "see-also", "heading_level": 2, "md_text": "- [Function Tools](tools.md) - Basic tool concepts and registration\n- [Toolsets](toolsets.md) - Managing collections of tools\n- [Deferred Tools](deferred-tools.md) - Tools requiring approval or external execution\n- [Third-Party Tools](third-party-tools.md) - Integrations with external tool libraries", "url": "https://ai.pydantic.dev/docs/tools-advanced/#see-also", "page": "docs/tools-advanced", "source_site": "pydantic_ai"}
{"title": "Toolsets", "anchor": "toolsets", "heading_level": 1, "md_text": "A toolset represents a collection of [tools](tools.md) that can be registered with an agent in one go. They can be reused by different agents, swapped out at runtime or during testing, and composed in order to dynamically filter which tools are available, modify tool definitions, or change tool execution behavior. A toolset can contain locally defined functions, depend on an external service to provide them, or implement custom logic to list available tools and handle them being called.\n\nToolsets are used (among many other things) to define [MCP servers](mcp/client.md) available to an agent. Pydantic AI includes many kinds of toolsets which are described below, and you can define a [custom toolset](#building-a-custom-toolset) by inheriting from the [`AbstractToolset`][pydantic_ai.toolsets.AbstractToolset] class.\n\nThe toolsets that will be available during an agent run can be specified in four different ways:\n\n* at agent construction time, via the [`toolsets`][pydantic_ai.Agent.__init__] keyword argument to `Agent`, which takes toolset instances as well as functions that generate toolsets [dynamically](#dynamically-building-a-toolset) based on the agent [run context][pydantic_ai.tools.RunContext]\n* at agent run time, via the `toolsets` keyword argument to [`agent.run()`][pydantic_ai.agent.AbstractAgent.run], [`agent.run_sync()`][pydantic_ai.agent.AbstractAgent.run_sync], [`agent.run_stream()`][pydantic_ai.agent.AbstractAgent.run_stream], or [`agent.iter()`][pydantic_ai.Agent.iter]. These toolsets will be additional to those registered on the `Agent`\n* [dynamically](#dynamically-building-a-toolset), via the [`@agent.toolset`][pydantic_ai.Agent.toolset] decorator which lets you build a toolset based on the agent [run context][pydantic_ai.tools.RunContext]\n* as a contextual override, via the `toolsets` keyword argument to the [`agent.override()`][pydantic_ai.Agent.iter] context manager. These toolsets will replace those provided at agent construction or run time during the life of the context manager\n\n```python {title=\"toolsets.py\"}\nfrom pydantic_ai import Agent, FunctionToolset\nfrom pydantic_ai.models.test import TestModel\n\n\ndef agent_tool():\n    return \"I'm registered directly on the agent\"\n\n\ndef extra_tool():\n    return \"I'm passed as an extra tool for a specific run\"\n\n\ndef override_tool():\n    return 'I override all other tools'\n\n\nagent_toolset = FunctionToolset(tools=[agent_tool]) # (1)!\nextra_toolset = FunctionToolset(tools=[extra_tool])\noverride_toolset = FunctionToolset(tools=[override_tool])\n\ntest_model = TestModel() # (2)!\nagent = Agent(test_model, toolsets=[agent_toolset])\n\nresult = agent.run_sync('What tools are available?')\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n#> ['agent_tool']\n\nresult = agent.run_sync('What tools are available?', toolsets=[extra_toolset])\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n#> ['agent_tool', 'extra_tool']\n\nwith agent.override(toolsets=[override_toolset]):\n    result = agent.run_sync('What tools are available?', toolsets=[extra_toolset]) # (3)!\n    print([t.name for t in test_model.last_model_request_parameters.function_tools])\n    #> ['override_tool']\n```\n\n1. The [`FunctionToolset`][pydantic_ai.toolsets.FunctionToolset] will be explained in detail in the next section.\n2. We're using [`TestModel`][pydantic_ai.models.test.TestModel] here because it makes it easy to see which tools were available on each run.\n3. This `extra_toolset` will be ignored because we're inside an override context.\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/toolsets/#toolsets", "page": "docs/toolsets", "source_site": "pydantic_ai"}
{"title": "Function Toolset", "anchor": "function-toolset", "heading_level": 2, "md_text": "As the name suggests, a [`FunctionToolset`][pydantic_ai.toolsets.FunctionToolset] makes locally defined functions available as tools.\n\nFunctions can be added as tools in three different ways:\n\n* via the [`@toolset.tool`][pydantic_ai.toolsets.FunctionToolset.tool] decorator\n* via the [`tools`][pydantic_ai.toolsets.FunctionToolset.__init__] keyword argument to the constructor which can take either plain functions, or instances of [`Tool`][pydantic_ai.tools.Tool]\n* via the [`toolset.add_function()`][pydantic_ai.toolsets.FunctionToolset.add_function] and [`toolset.add_tool()`][pydantic_ai.toolsets.FunctionToolset.add_tool] methods which can take a plain function or an instance of [`Tool`][pydantic_ai.tools.Tool] respectively\n\nFunctions registered in any of these ways can define an initial `ctx: RunContext` argument in order to receive the agent [run context][pydantic_ai.tools.RunContext]. The `add_function()` and `add_tool()` methods can also be used from a tool function to dynamically register new tools during a run to be available in future run steps.\n\n```python {title=\"function_toolset.py\"}\nfrom datetime import datetime\n\nfrom pydantic_ai import Agent, FunctionToolset, RunContext\nfrom pydantic_ai.models.test import TestModel\n\n\ndef temperature_celsius(city: str) -> float:\n    return 21.0\n\n\ndef temperature_fahrenheit(city: str) -> float:\n    return 69.8\n\n\nweather_toolset = FunctionToolset(tools=[temperature_celsius, temperature_fahrenheit])\n\n\n@weather_toolset.tool\ndef conditions(ctx: RunContext, city: str) -> str:\n    if ctx.run_step % 2 == 0:\n        return \"It's sunny\"\n    else:\n        return \"It's raining\"\n\n\ndatetime_toolset = FunctionToolset()\ndatetime_toolset.add_function(lambda: datetime.now(), name='now')\n\ntest_model = TestModel()  # (1)!\nagent = Agent(test_model)\n\nresult = agent.run_sync('What tools are available?', toolsets=[weather_toolset])\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n#> ['temperature_celsius', 'temperature_fahrenheit', 'conditions']\n\nresult = agent.run_sync('What tools are available?', toolsets=[datetime_toolset])\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n#> ['now']\n```\n\n1. We're using [`TestModel`][pydantic_ai.models.test.TestModel] here because it makes it easy to see which tools were available on each run.\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/toolsets/#function-toolset", "page": "docs/toolsets", "source_site": "pydantic_ai"}
{"title": "Toolset Composition", "anchor": "toolset-composition", "heading_level": 2, "md_text": "Toolsets can be composed to dynamically filter which tools are available, modify tool definitions, or change tool execution behavior. Multiple toolsets can also be combined into one.", "url": "https://ai.pydantic.dev/docs/toolsets/#toolset-composition", "page": "docs/toolsets", "source_site": "pydantic_ai"}
{"title": "Combining Toolsets", "anchor": "combining-toolsets", "heading_level": 3, "md_text": "[`CombinedToolset`][pydantic_ai.toolsets.CombinedToolset] takes a list of toolsets and lets them be used as one.\n\n```python {title=\"combined_toolset.py\" requires=\"function_toolset.py\"}\nfrom pydantic_ai import Agent, CombinedToolset\nfrom pydantic_ai.models.test import TestModel\n\nfrom function_toolset import datetime_toolset, weather_toolset\n\ncombined_toolset = CombinedToolset([weather_toolset, datetime_toolset])\n\ntest_model = TestModel() # (1)!\nagent = Agent(test_model, toolsets=[combined_toolset])\nresult = agent.run_sync('What tools are available?')\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n#> ['temperature_celsius', 'temperature_fahrenheit', 'conditions', 'now']\n```\n\n1. We're using [`TestModel`][pydantic_ai.models.test.TestModel] here because it makes it easy to see which tools were available on each run.\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/toolsets/#combining-toolsets", "page": "docs/toolsets", "source_site": "pydantic_ai"}
{"title": "Filtering Tools", "anchor": "filtering-tools", "heading_level": 3, "md_text": "[`FilteredToolset`][pydantic_ai.toolsets.FilteredToolset] wraps a toolset and filters available tools ahead of each step of the run based on a user-defined function that is passed the agent [run context][pydantic_ai.tools.RunContext] and each tool's [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] and returns a boolean to indicate whether or not a given tool should be available.\n\nTo easily chain different modifications, you can also call [`filtered()`][pydantic_ai.toolsets.AbstractToolset.filtered] on any toolset instead of directly constructing a `FilteredToolset`.\n\n```python {title=\"filtered_toolset.py\" requires=\"function_toolset.py,combined_toolset.py\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.test import TestModel\n\nfrom combined_toolset import combined_toolset\n\nfiltered_toolset = combined_toolset.filtered(lambda ctx, tool_def: 'fahrenheit' not in tool_def.name)\n\ntest_model = TestModel() # (1)!\nagent = Agent(test_model, toolsets=[filtered_toolset])\nresult = agent.run_sync('What tools are available?')\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n#> ['weather_temperature_celsius', 'weather_conditions', 'datetime_now']\n```\n\n1. We're using [`TestModel`][pydantic_ai.models.test.TestModel] here because it makes it easy to see which tools were available on each run.\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/toolsets/#filtering-tools", "page": "docs/toolsets", "source_site": "pydantic_ai"}
{"title": "Prefixing Tool Names", "anchor": "prefixing-tool-names", "heading_level": 3, "md_text": "[`PrefixedToolset`][pydantic_ai.toolsets.PrefixedToolset] wraps a toolset and adds a prefix to each tool name to prevent tool name conflicts between different toolsets.\n\nTo easily chain different modifications, you can also call [`prefixed()`][pydantic_ai.toolsets.AbstractToolset.prefixed] on any toolset instead of directly constructing a `PrefixedToolset`.\n\n```python {title=\"combined_toolset.py\" requires=\"function_toolset.py\"}\nfrom pydantic_ai import Agent, CombinedToolset\nfrom pydantic_ai.models.test import TestModel\n\nfrom function_toolset import datetime_toolset, weather_toolset\n\ncombined_toolset = CombinedToolset(\n    [\n        weather_toolset.prefixed('weather'),\n        datetime_toolset.prefixed('datetime')\n    ]\n)\n\ntest_model = TestModel() # (1)!\nagent = Agent(test_model, toolsets=[combined_toolset])\nresult = agent.run_sync('What tools are available?')\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n\"\"\"\n[\n    'weather_temperature_celsius',\n    'weather_temperature_fahrenheit',\n    'weather_conditions',\n    'datetime_now',\n]\n\"\"\"\n```\n\n1. We're using [`TestModel`][pydantic_ai.models.test.TestModel] here because it makes it easy to see which tools were available on each run.\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/toolsets/#prefixing-tool-names", "page": "docs/toolsets", "source_site": "pydantic_ai"}
{"title": "Renaming Tools", "anchor": "renaming-tools", "heading_level": 3, "md_text": "[`RenamedToolset`][pydantic_ai.toolsets.RenamedToolset] wraps a toolset and lets you rename tools using a dictionary mapping new names to original names. This is useful when the names provided by a toolset are ambiguous or would conflict with tools defined by other toolsets, but [prefixing them](#prefixing-tool-names) creates a name that is unnecessarily long or could be confusing to the model.\n\nTo easily chain different modifications, you can also call [`renamed()`][pydantic_ai.toolsets.AbstractToolset.renamed] on any toolset instead of directly constructing a `RenamedToolset`.\n\n```python {title=\"renamed_toolset.py\" requires=\"function_toolset.py,combined_toolset.py\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.test import TestModel\n\nfrom combined_toolset import combined_toolset\n\nrenamed_toolset = combined_toolset.renamed(\n    {\n        'current_time': 'datetime_now',\n        'temperature_celsius': 'weather_temperature_celsius',\n        'temperature_fahrenheit': 'weather_temperature_fahrenheit'\n    }\n)\n\ntest_model = TestModel() # (1)!\nagent = Agent(test_model, toolsets=[renamed_toolset])\nresult = agent.run_sync('What tools are available?')\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n\"\"\"\n['temperature_celsius', 'temperature_fahrenheit', 'weather_conditions', 'current_time']\n\"\"\"\n```\n\n1. We're using [`TestModel`][pydantic_ai.models.test.TestModel] here because it makes it easy to see which tools were available on each run.\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/toolsets/#renaming-tools", "page": "docs/toolsets", "source_site": "pydantic_ai"}
{"title": "Dynamic Tool Definitions {#preparing-tool-definitions}", "anchor": "dynamic-tool-definitions-preparing-tool-definitions", "heading_level": 3, "md_text": "[`PreparedToolset`][pydantic_ai.toolsets.PreparedToolset] lets you modify the entire list of available tools ahead of each step of the agent run using a user-defined function that takes the  agent [run context][pydantic_ai.tools.RunContext] and a list of [`ToolDefinition`s][pydantic_ai.tools.ToolDefinition] and returns a list of modified `ToolDefinition`s.\n\nThis is the toolset-specific equivalent of the [`prepare_tools`](tools-advanced.md#prepare-tools) argument to `Agent` that prepares all tool definitions registered on an agent across toolsets.\n\nNote that it is not possible to add or rename tools using `PreparedToolset`. Instead, you can use [`FunctionToolset.add_function()`](#function-toolset) or [`RenamedToolset`](#renaming-tools).\n\nTo easily chain different modifications, you can also call [`prepared()`][pydantic_ai.toolsets.AbstractToolset.prepared] on any toolset instead of directly constructing a `PreparedToolset`.\n\n```python {title=\"prepared_toolset.py\" requires=\"function_toolset.py,combined_toolset.py,renamed_toolset.py\"}\nfrom dataclasses import replace\n\nfrom pydantic_ai import Agent, RunContext, ToolDefinition\nfrom pydantic_ai.models.test import TestModel\n\nfrom renamed_toolset import renamed_toolset\n\ndescriptions = {\n    'temperature_celsius': 'Get the temperature in degrees Celsius',\n    'temperature_fahrenheit': 'Get the temperature in degrees Fahrenheit',\n    'weather_conditions': 'Get the current weather conditions',\n    'current_time': 'Get the current time',\n}\n\nasync def add_descriptions(ctx: RunContext, tool_defs: list[ToolDefinition]) -> list[ToolDefinition] | None:\n    return [\n        replace(tool_def, description=description)\n        if (description := descriptions.get(tool_def.name, None))\n        else tool_def\n        for tool_def\n        in tool_defs\n    ]\n\nprepared_toolset = renamed_toolset.prepared(add_descriptions)\n\ntest_model = TestModel() # (1)!\nagent = Agent(test_model, toolsets=[prepared_toolset])\nresult = agent.run_sync('What tools are available?')\nprint(test_model.last_model_request_parameters.function_tools)\n\"\"\"\n[\n    ToolDefinition(\n        name='temperature_celsius',\n        parameters_json_schema={\n            'additionalProperties': False,\n            'properties': {'city': {'type': 'string'}},\n            'required': ['city'],\n            'type': 'object',\n        },\n        description='Get the temperature in degrees Celsius',\n    ),\n    ToolDefinition(\n        name='temperature_fahrenheit',\n        parameters_json_schema={\n            'additionalProperties': False,\n            'properties': {'city': {'type': 'string'}},\n            'required': ['city'],\n            'type': 'object',\n        },\n        description='Get the temperature in degrees Fahrenheit',\n    ),\n    ToolDefinition(\n        name='weather_conditions',\n        parameters_json_schema={\n            'additionalProperties': False,\n            'properties': {'city': {'type': 'string'}},\n            'required': ['city'],\n            'type': 'object',\n        },\n        description='Get the current weather conditions',\n    ),\n    ToolDefinition(\n        name='current_time',\n        parameters_json_schema={\n            'additionalProperties': False,\n            'properties': {},\n            'type': 'object',\n        },\n        description='Get the current time',\n    ),\n]\n\"\"\"\n```\n\n1. We're using [`TestModel`][pydantic_ai.models.test.TestModel] here because it makes it easy to see which tools were available on each run.", "url": "https://ai.pydantic.dev/docs/toolsets/#dynamic-tool-definitions-preparing-tool-definitions", "page": "docs/toolsets", "source_site": "pydantic_ai"}
{"title": "Requiring Tool Approval", "anchor": "requiring-tool-approval", "heading_level": 3, "md_text": "[`ApprovalRequiredToolset`][pydantic_ai.toolsets.ApprovalRequiredToolset] wraps a toolset and lets you dynamically [require approval](deferred-tools.md#human-in-the-loop-tool-approval) for a given tool call based on a user-defined function that is passed the agent [run context][pydantic_ai.tools.RunContext], the tool's [`ToolDefinition`][pydantic_ai.tools.ToolDefinition], and the validated tool call arguments. If no function is provided, all tool calls will require approval.\n\nTo easily chain different modifications, you can also call [`approval_required()`][pydantic_ai.toolsets.AbstractToolset.approval_required] on any toolset instead of directly constructing a `ApprovalRequiredToolset`.\n\nSee the [Human-in-the-Loop Tool Approval](deferred-tools.md#human-in-the-loop-tool-approval) documentation for more information on how to handle agent runs that call tools that require approval and how to pass in the results.\n\n```python {title=\"approval_required_toolset.py\" requires=\"function_toolset.py,combined_toolset.py,renamed_toolset.py,prepared_toolset.py\"}\nfrom pydantic_ai import Agent, DeferredToolRequests, DeferredToolResults\nfrom pydantic_ai.models.test import TestModel\n\nfrom prepared_toolset import prepared_toolset\n\napproval_required_toolset = prepared_toolset.approval_required(lambda ctx, tool_def, tool_args: tool_def.name.startswith('temperature'))\n\ntest_model = TestModel(call_tools=['temperature_celsius', 'temperature_fahrenheit']) # (1)!\nagent = Agent(\n    test_model,\n    toolsets=[approval_required_toolset],\n    output_type=[str, DeferredToolRequests],\n)\nresult = agent.run_sync('Call the temperature tools')\nmessages = result.all_messages()\nprint(result.output)\n\"\"\"\nDeferredToolRequests(\n    calls=[],\n    approvals=[\n        ToolCallPart(\n            tool_name='temperature_celsius',\n            args={'city': 'a'},\n            tool_call_id='pyd_ai_tool_call_id__temperature_celsius',\n        ),\n        ToolCallPart(\n            tool_name='temperature_fahrenheit',\n            args={'city': 'a'},\n            tool_call_id='pyd_ai_tool_call_id__temperature_fahrenheit',\n        ),\n    ],\n)\n\"\"\"\n\nresult = agent.run_sync(\n    message_history=messages,\n    deferred_tool_results=DeferredToolResults(\n        approvals={\n            'pyd_ai_tool_call_id__temperature_celsius': True,\n            'pyd_ai_tool_call_id__temperature_fahrenheit': False,\n        }\n    )\n)\nprint(result.output)\n#> {\"temperature_celsius\":21.0,\"temperature_fahrenheit\":\"The tool call was denied.\"}\n```\n\n1. We're using [`TestModel`][pydantic_ai.models.test.TestModel] here because it makes it easy to specify which tools to call.\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/toolsets/#requiring-tool-approval", "page": "docs/toolsets", "source_site": "pydantic_ai"}
{"title": "Changing Tool Execution", "anchor": "changing-tool-execution", "heading_level": 3, "md_text": "[`WrapperToolset`][pydantic_ai.toolsets.WrapperToolset] wraps another toolset and delegates all responsibility to it.\n\nIt is is a no-op by default, but you can subclass `WrapperToolset` to change the wrapped toolset's tool execution behavior by overriding the [`call_tool()`][pydantic_ai.toolsets.AbstractToolset.call_tool] method.\n\n```python {title=\"logging_toolset.py\" requires=\"function_toolset.py,combined_toolset.py,renamed_toolset.py,prepared_toolset.py\"}\nimport asyncio\n\nfrom typing_extensions import Any\n\nfrom pydantic_ai import Agent, RunContext, ToolsetTool, WrapperToolset\nfrom pydantic_ai.models.test import TestModel\n\nfrom prepared_toolset import prepared_toolset\n\nLOG = []\n\nclass LoggingToolset(WrapperToolset):\n    async def call_tool(self, name: str, tool_args: dict[str, Any], ctx: RunContext, tool: ToolsetTool) -> Any:\n        LOG.append(f'Calling tool {name!r} with args: {tool_args!r}')\n        try:\n            await asyncio.sleep(0.1 * len(LOG)) # (1)!\n\n            result = await super().call_tool(name, tool_args, ctx, tool)\n            LOG.append(f'Finished calling tool {name!r} with result: {result!r}')\n        except Exception as e:\n            LOG.append(f'Error calling tool {name!r}: {e}')\n            raise e\n        else:\n            return result\n\n\nlogging_toolset = LoggingToolset(prepared_toolset)\n\nagent = Agent(TestModel(), toolsets=[logging_toolset]) # (2)!\nresult = agent.run_sync('Call all the tools')\nprint(LOG)\n\"\"\"\n[\n    \"Calling tool 'temperature_celsius' with args: {'city': 'a'}\",\n    \"Calling tool 'temperature_fahrenheit' with args: {'city': 'a'}\",\n    \"Calling tool 'weather_conditions' with args: {'city': 'a'}\",\n    \"Calling tool 'current_time' with args: {}\",\n    \"Finished calling tool 'temperature_celsius' with result: 21.0\",\n    \"Finished calling tool 'temperature_fahrenheit' with result: 69.8\",\n    'Finished calling tool \\'weather_conditions\\' with result: \"It\\'s raining\"',\n    \"Finished calling tool 'current_time' with result: datetime.datetime(...)\",\n]\n\"\"\"\n```\n\n1. All docs examples are tested in CI and their their output is verified, so we need `LOG` to always have the same order whenever this code is run. Since the tools could finish in any order, we sleep an increasing amount of time based on which number tool call we are to ensure that they finish (and log) in the same order they were called in.\n2. We use [`TestModel`][pydantic_ai.models.test.TestModel] here as it will automatically call each tool.\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/toolsets/#changing-tool-execution", "page": "docs/toolsets", "source_site": "pydantic_ai"}
{"title": "External Toolset", "anchor": "external-toolset", "heading_level": 2, "md_text": "If your agent needs to be able to call [external tools](deferred-tools.md#external-tool-execution) that are provided and executed by an upstream service or frontend, you can build an [`ExternalToolset`][pydantic_ai.toolsets.ExternalToolset] from a list of [`ToolDefinition`s][pydantic_ai.tools.ToolDefinition] containing the tool names, arguments JSON schemas, and descriptions.\n\nWhen the model calls an external tool, the call is considered to be [\"deferred\"](deferred-tools.md#deferred-tools), and the agent run will end with a [`DeferredToolRequests`][pydantic_ai.output.DeferredToolRequests] output object with a `calls` list holding [`ToolCallPart`s][pydantic_ai.messages.ToolCallPart] containing the tool name, validated arguments, and a unique tool call ID, which are expected to be passed to the upstream service or frontend that will produce the results.\n\nWhen the tool call results are received from the upstream service or frontend, you can build a [`DeferredToolResults`][pydantic_ai.tools.DeferredToolResults] object with a `calls` dictionary that maps each tool call ID to an arbitrary value to be returned to the model, a [`ToolReturn`](tools-advanced.md#advanced-tool-returns) object, or a [`ModelRetry`][pydantic_ai.exceptions.ModelRetry] exception in case the tool call failed and the model should [try again](tools-advanced.md#tool-retries). This `DeferredToolResults` object can then be provided to one of the agent run methods as `deferred_tool_results`, alongside the original run's [message history](message-history.md).\n\nNote that you need to add `DeferredToolRequests` to the `Agent`'s or `agent.run()`'s [`output_type`](output.md#structured-output) so that the possible types of the agent run output are correctly inferred. For more information, see the [Deferred Tools](deferred-tools.md#deferred-tools) documentation.\n\nTo demonstrate, let us first define a simple agent _without_ deferred tools:\n\n```python {title=\"deferred_toolset_agent.py\"}\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, FunctionToolset\n\ntoolset = FunctionToolset()\n\n\n@toolset.tool\ndef get_default_language():\n    return 'en-US'\n\n\n@toolset.tool\ndef get_user_name():\n    return 'David'\n\n\nclass PersonalizedGreeting(BaseModel):\n    greeting: str\n    language_code: str\n\n\nagent = Agent('openai:gpt-5', toolsets=[toolset], output_type=PersonalizedGreeting)\n\nresult = agent.run_sync('Greet the user in a personalized way')\nprint(repr(result.output))\n#> PersonalizedGreeting(greeting='Hello, David!', language_code='en-US')\n```\n\nNext, let's define a function that represents a hypothetical \"run agent\" API endpoint that can be called by the frontend and takes a list of messages to send to the model, a list of frontend tool definitions, and optional deferred tool results. This is where `ExternalToolset`, `DeferredToolRequests`, and `DeferredToolResults` come in:\n\n```python {title=\"deferred_toolset_api.py\" requires=\"deferred_toolset_agent.py\"}\nfrom pydantic_ai import (\n    DeferredToolRequests,\n    DeferredToolResults,\n    ExternalToolset,\n    ModelMessage,\n    ToolDefinition,\n)\n\nfrom deferred_toolset_agent import PersonalizedGreeting, agent\n\n\ndef run_agent(\n    messages: list[ModelMessage] = [],\n    frontend_tools: list[ToolDefinition] = {},\n    deferred_tool_results: DeferredToolResults | None = None,\n) -> tuple[PersonalizedGreeting | DeferredToolRequests, list[ModelMessage]]:\n    deferred_toolset = ExternalToolset(frontend_tools)\n    result = agent.run_sync(\n        toolsets=[deferred_toolset], # (1)!\n        output_type=[agent.output_type, DeferredToolRequests], # (2)!\n        message_history=messages, # (3)!\n        deferred_tool_results=deferred_tool_results,\n    )\n    return result.output, result.new_messages()\n```\n\n1. As mentioned in the [Deferred Tools](deferred-tools.md#deferred-tools) documentation, these `toolsets` are additional to those provided to the `Agent` constructor\n2. As mentioned in the [Deferred Tools](deferred-tools.md#deferred-tools) documentation, this `output_type` overrides the one provided to the `Agent` constructor, so we have to make sure to not lose it\n3. We don't include an `user_prompt` keyword argument as we expect the frontend to provide it via `messages`\n\nNow, imagine that the code below is implemented on the frontend, and `run_agent` stands in for an API call to the backend that runs the agent. This is where we actually execute the deferred tool calls and start a new run with the new result included:\n\n```python {title=\"deferred_tools.py\" requires=\"deferred_toolset_agent.py,deferred_toolset_api.py\"}\nfrom pydantic_ai import (\n    DeferredToolRequests,\n    DeferredToolResults,\n    ModelMessage,\n    ModelRequest,\n    ModelRetry,\n    ToolDefinition,\n    UserPromptPart,\n)\n\nfrom deferred_toolset_api import run_agent\n\nfrontend_tool_definitions = [\n    ToolDefinition(\n        name='get_preferred_language',\n        parameters_json_schema={'type': 'object', 'properties': {'default_language': {'type': 'string'}}},\n        description=\"Get the user's preferred language from their browser\",\n    )\n]\n\ndef get_preferred_language(default_language: str) -> str:\n    return 'es-MX' # (1)!\n\nfrontend_tool_functions = {'get_preferred_language': get_preferred_language}", "url": "https://ai.pydantic.dev/docs/toolsets/#external-toolset", "page": "docs/toolsets", "source_site": "pydantic_ai"}
{"title": "External Toolset", "anchor": "external-toolset", "heading_level": 2, "md_text": "messages: list[ModelMessage] = [\n    ModelRequest(\n        parts=[\n            UserPromptPart(content='Greet the user in a personalized way')\n        ]\n    )\n]\n\ndeferred_tool_results: DeferredToolResults | None = None\n\nfinal_output = None\nwhile True:\n    output, new_messages = run_agent(messages, frontend_tool_definitions, deferred_tool_results)\n    messages += new_messages\n\n    if not isinstance(output, DeferredToolRequests):\n        final_output = output\n        break\n\n    print(output.calls)\n    \"\"\"\n    [\n        ToolCallPart(\n            tool_name='get_preferred_language',\n            args={'default_language': 'en-US'},\n            tool_call_id='pyd_ai_tool_call_id',\n        )\n    ]\n    \"\"\"\n    deferred_tool_results = DeferredToolResults()\n    for tool_call in output.calls:\n        if function := frontend_tool_functions.get(tool_call.tool_name):\n            result = function(**tool_call.args_as_dict())\n        else:\n            result = ModelRetry(f'Unknown tool {tool_call.tool_name!r}')\n        deferred_tool_results.calls[tool_call.tool_call_id] = result\n\nprint(repr(final_output))\n\"\"\"\nPersonalizedGreeting(greeting='Hola, David! Espero que tengas un gran d\u00eda!', language_code='es-MX')\n\"\"\"\n```\n\n1. Imagine that this returns the frontend [`navigator.language`](https://developer.mozilla.org/en-US/docs/Web/API/Navigator/language).\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/toolsets/#external-toolset", "page": "docs/toolsets", "source_site": "pydantic_ai"}
{"title": "Dynamically Building a Toolset", "anchor": "dynamically-building-a-toolset", "heading_level": 2, "md_text": "Toolsets can be built dynamically ahead of each agent run or run step using a function that takes the agent [run context][pydantic_ai.tools.RunContext] and returns a toolset or `None`. This is useful when a toolset (like an MCP server) depends on information specific to an agent run, like its [dependencies](./dependencies.md).\n\nTo register a dynamic toolset, you can pass a function that takes [`RunContext`][pydantic_ai.tools.RunContext] to the `toolsets` argument of the `Agent` constructor, or you can wrap a compliant function in the [`@agent.toolset`][pydantic_ai.Agent.toolset] decorator.\n\nBy default, the function will be called again ahead of each agent run step. If you are using the decorator, you can optionally provide a `per_run_step=False` argument to indicate that the toolset only needs to be built once for the entire run.\n\n```python {title=\"dynamic_toolset.py\", requires=\"function_toolset.py\"}\nfrom dataclasses import dataclass\nfrom typing import Literal\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.models.test import TestModel\n\nfrom function_toolset import datetime_toolset, weather_toolset\n\n\n@dataclass\nclass ToggleableDeps:\n    active: Literal['weather', 'datetime']\n\n    def toggle(self):\n        if self.active == 'weather':\n            self.active = 'datetime'\n        else:\n            self.active = 'weather'\n\ntest_model = TestModel()  # (1)!\nagent = Agent(\n    test_model,\n    deps_type=ToggleableDeps  # (2)!\n)\n\n@agent.toolset\ndef toggleable_toolset(ctx: RunContext[ToggleableDeps]):\n    if ctx.deps.active == 'weather':\n        return weather_toolset\n    else:\n        return datetime_toolset\n\n@agent.tool\ndef toggle(ctx: RunContext[ToggleableDeps]):\n    ctx.deps.toggle()\n\ndeps = ToggleableDeps('weather')\n\nresult = agent.run_sync('Toggle the toolset', deps=deps)\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])  # (3)!\n#> ['toggle', 'now']\n\nresult = agent.run_sync('Toggle the toolset', deps=deps)\nprint([t.name for t in test_model.last_model_request_parameters.function_tools])\n#> ['toggle', 'temperature_celsius', 'temperature_fahrenheit', 'conditions']\n```\n\n1. We're using [`TestModel`][pydantic_ai.models.test.TestModel] here because it makes it easy to see which tools were available on each run.\n2. We're using the agent's dependencies to give the `toggle` tool access to the `active` via the `RunContext` argument.\n3. This shows the available tools _after_ the `toggle` tool was executed, as the \"last model request\" was the one that returned the `toggle` tool result to the model.\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/toolsets/#dynamically-building-a-toolset", "page": "docs/toolsets", "source_site": "pydantic_ai"}
{"title": "Building a Custom Toolset", "anchor": "building-a-custom-toolset", "heading_level": 2, "md_text": "To define a fully custom toolset with its own logic to list available tools and handle them being called, you can subclass [`AbstractToolset`][pydantic_ai.toolsets.AbstractToolset] and implement the [`get_tools()`][pydantic_ai.toolsets.AbstractToolset.get_tools] and [`call_tool()`][pydantic_ai.toolsets.AbstractToolset.call_tool] methods.\n\nIf you want to reuse a network connection or session across tool listings and calls during an agent run, you can implement [`__aenter__()`][pydantic_ai.toolsets.AbstractToolset.__aenter__] and [`__aexit__()`][pydantic_ai.toolsets.AbstractToolset.__aexit__].", "url": "https://ai.pydantic.dev/docs/toolsets/#building-a-custom-toolset", "page": "docs/toolsets", "source_site": "pydantic_ai"}
{"title": "MCP Servers", "anchor": "mcp-servers", "heading_level": 3, "md_text": "Pydantic AI provides two toolsets that allow an agent to connect to and call tools on local and remote MCP Servers:\n\n1. `MCPServer`: the [MCP SDK-based Client](./mcp/client.md) which offers more direct control by leveraging the MCP SDK directly\n2. `FastMCPToolset`: the [FastMCP-based Client](./mcp/fastmcp-client.md) which offers additional capabilities like Tool Transformation, simpler OAuth configuration, and more.", "url": "https://ai.pydantic.dev/docs/toolsets/#mcp-servers", "page": "docs/toolsets", "source_site": "pydantic_ai"}
{"title": "LangChain Tools {#langchain-tools}", "anchor": "langchain-tools-langchain-tools", "heading_level": 3, "md_text": "If you'd like to use tools or a [toolkit](https://python.langchain.com/docs/concepts/tools/#toolkits) from LangChain's [community tool library](https://python.langchain.com/docs/integrations/tools/) with Pydantic AI, you can use the [`LangChainToolset`][pydantic_ai.ext.langchain.LangChainToolset] which takes a list of LangChain tools. Note that Pydantic AI will not validate the arguments in this case -- it's up to the model to provide arguments matching the schema specified by the LangChain tool, and up to the LangChain tool to raise an error if the arguments are invalid.\n\nYou will need to install the `langchain-community` package and any others required by the tools in question.\n\n```python {test=\"skip\"}\nfrom langchain_community.agent_toolkits import SlackToolkit\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ext.langchain import LangChainToolset\n\ntoolkit = SlackToolkit()\ntoolset = LangChainToolset(toolkit.get_tools())\n\nagent = Agent('openai:gpt-5', toolsets=[toolset])", "url": "https://ai.pydantic.dev/docs/toolsets/#langchain-tools-langchain-tools", "page": "docs/toolsets", "source_site": "pydantic_ai"}
{"title": "...", "anchor": "", "heading_level": 1, "md_text": "```", "url": "https://ai.pydantic.dev/docs/toolsets/", "page": "docs/toolsets", "source_site": "pydantic_ai"}
{"title": "ACI.dev Tools {#aci-tools}", "anchor": "acidev-tools-aci-tools", "heading_level": 3, "md_text": "If you'd like to use tools from the [ACI.dev tool library](https://www.aci.dev/tools) with Pydantic AI, you can use the [`ACIToolset`][pydantic_ai.ext.aci.ACIToolset] [toolset](toolsets.md) which takes a list of ACI tool names as well as the `linked_account_owner_id`. Note that Pydantic AI will not validate the arguments in this case -- it's up to the model to provide arguments matching the schema specified by the ACI tool, and up to the ACI tool to raise an error if the arguments are invalid.\n\nYou will need to install the `aci-sdk` package, set your ACI API key in the `ACI_API_KEY` environment variable, and pass your ACI \"linked account owner ID\" to the function.\n\n```python {test=\"skip\"}\nimport os\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ext.aci import ACIToolset\n\ntoolset = ACIToolset(\n    [\n        'OPEN_WEATHER_MAP__CURRENT_WEATHER',\n        'OPEN_WEATHER_MAP__FORECAST',\n    ],\n    linked_account_owner_id=os.getenv('LINKED_ACCOUNT_OWNER_ID'),\n)\n\nagent = Agent('openai:gpt-5', toolsets=[toolset])\n```", "url": "https://ai.pydantic.dev/docs/toolsets/#acidev-tools-aci-tools", "page": "docs/toolsets", "source_site": "pydantic_ai"}
{"title": "Unit testing", "anchor": "unit-testing", "heading_level": 1, "md_text": "Writing unit tests for Pydantic AI code is just like unit tests for any other Python code.\n\nBecause for the most part they're nothing new, we have pretty well established tools and patterns for writing and running these kinds of tests.\n\nUnless you're really sure you know better, you'll probably want to follow roughly this strategy:\n\n- Use [`pytest`](https://docs.pytest.org/en/stable/) as your test harness\n- If you find yourself typing out long assertions, use [inline-snapshot](https://15r10nk.github.io/inline-snapshot/latest/)\n- Similarly, [dirty-equals](https://dirty-equals.helpmanual.io/latest/) can be useful for comparing large data structures\n- Use [`TestModel`][pydantic_ai.models.test.TestModel] or [`FunctionModel`][pydantic_ai.models.function.FunctionModel] in place of your actual model to avoid the usage, latency and variability of real LLM calls\n- Use [`Agent.override`][pydantic_ai.agent.Agent.override] to replace an agent's model, dependencies, or toolsets inside your application logic\n- Set [`ALLOW_MODEL_REQUESTS=False`][pydantic_ai.models.ALLOW_MODEL_REQUESTS] globally to block any requests from being made to non-test models accidentally", "url": "https://ai.pydantic.dev/docs/testing/#unit-testing", "page": "docs/testing", "source_site": "pydantic_ai"}
{"title": "Unit testing with `TestModel`", "anchor": "unit-testing-with-testmodel", "heading_level": 3, "md_text": "The simplest and fastest way to exercise most of your application code is using [`TestModel`][pydantic_ai.models.test.TestModel], this will (by default) call all tools in the agent, then return either plain text or a structured response depending on the return type of the agent.\n\n!!! note \"`TestModel` is not magic\"\n    The \"clever\" (but not too clever) part of `TestModel` is that it will attempt to generate valid structured data for [function tools](tools.md) and [output types](output.md#structured-output) based on the schema of the registered tools.\n\n    There's no ML or AI in `TestModel`, it's just plain old procedural Python code that tries to generate data that satisfies the JSON schema of a tool.\n\n    The resulting data won't look pretty or relevant, but it should pass Pydantic's validation in most cases.\n    If you want something more sophisticated, use [`FunctionModel`][pydantic_ai.models.function.FunctionModel] and write your own data generation logic.\n\nLet's write unit tests for the following application code:\n\n```python {title=\"weather_app.py\"}\nimport asyncio\nfrom datetime import date\n\nfrom pydantic_ai import Agent, RunContext\n\nfrom fake_database import DatabaseConn  # (1)!\nfrom weather_service import WeatherService  # (2)!\n\nweather_agent = Agent(\n    'openai:gpt-5',\n    deps_type=WeatherService,\n    system_prompt='Providing a weather forecast at the locations the user provides.',\n)\n\n\n@weather_agent.tool\ndef weather_forecast(\n    ctx: RunContext[WeatherService], location: str, forecast_date: date\n) -> str:\n    if forecast_date < date.today():  # (3)!\n        return ctx.deps.get_historic_weather(location, forecast_date)\n    else:\n        return ctx.deps.get_forecast(location, forecast_date)\n\n\nasync def run_weather_forecast(  # (4)!\n    user_prompts: list[tuple[str, int]], conn: DatabaseConn\n):\n    \"\"\"Run weather forecast for a list of user prompts and save.\"\"\"\n    async with WeatherService() as weather_service:\n\n        async def run_forecast(prompt: str, user_id: int):\n            result = await weather_agent.run(prompt, deps=weather_service)\n            await conn.store_forecast(user_id, result.output)\n\n        # run all prompts in parallel\n        await asyncio.gather(\n            *(run_forecast(prompt, user_id) for (prompt, user_id) in user_prompts)\n        )\n```\n\n1. `DatabaseConn` is a class that holds a database connection\n2. `WeatherService` has methods to get weather forecasts and historic data about the weather\n3. We need to call a different endpoint depending on whether the date is in the past or the future, you'll see why this nuance is important below\n4. This function is the code we want to test, together with the agent it uses\n\nHere we have a function that takes a list of `#!python (user_prompt, user_id)` tuples, gets a weather forecast for each prompt, and stores the result in the database.\n\n**We want to test this code without having to mock certain objects or modify our code so we can pass test objects in.**\n\nHere's how we would write tests using [`TestModel`][pydantic_ai.models.test.TestModel]:\n\n```python {title=\"test_weather_app.py\" call_name=\"test_forecast\" requires=\"weather_app.py\"}\nfrom datetime import timezone\nimport pytest\n\nfrom dirty_equals import IsNow, IsStr\n\nfrom pydantic_ai import models, capture_run_messages, RequestUsage\nfrom pydantic_ai.models.test import TestModel\nfrom pydantic_ai import (\n    ModelResponse,\n    SystemPromptPart,\n    TextPart,\n    ToolCallPart,\n    ToolReturnPart,\n    UserPromptPart,\n    ModelRequest,\n)\n\nfrom fake_database import DatabaseConn\nfrom weather_app import run_weather_forecast, weather_agent\n\npytestmark = pytest.mark.anyio  # (1)!\nmodels.ALLOW_MODEL_REQUESTS = False  # (2)!\n\n\nasync def test_forecast():\n    conn = DatabaseConn()\n    user_id = 1\n    with capture_run_messages() as messages:\n        with weather_agent.override(model=TestModel()):  # (3)!\n            prompt = 'What will the weather be like in London on 2024-11-28?'\n            await run_weather_forecast([(prompt, user_id)], conn)  # (4)!\n\n    forecast = await conn.get_forecast(user_id)\n    assert forecast == '{\"weather_forecast\":\"Sunny with a chance of rain\"}'  # (5)!", "url": "https://ai.pydantic.dev/docs/testing/#unit-testing-with-testmodel", "page": "docs/testing", "source_site": "pydantic_ai"}
{"title": "Unit testing with `TestModel`", "anchor": "unit-testing-with-testmodel", "heading_level": 3, "md_text": "    assert messages == [  # (6)!\n        ModelRequest(\n            parts=[\n                SystemPromptPart(\n                    content='Providing a weather forecast at the locations the user provides.',\n                    timestamp=IsNow(tz=timezone.utc),\n                ),\n                UserPromptPart(\n                    content='What will the weather be like in London on 2024-11-28?',\n                    timestamp=IsNow(tz=timezone.utc),  # (7)!\n                ),\n            ]\n        ),\n        ModelResponse(\n            parts=[\n                ToolCallPart(\n                    tool_name='weather_forecast',\n                    args={\n                        'location': 'a',\n                        'forecast_date': '2024-01-01',  # (8)!\n                    },\n                    tool_call_id=IsStr(),\n                )\n            ],\n            usage=RequestUsage(\n                input_tokens=71,\n                output_tokens=7,\n            ),\n            model_name='test',\n            timestamp=IsNow(tz=timezone.utc),\n        ),\n        ModelRequest(\n            parts=[\n                ToolReturnPart(\n                    tool_name='weather_forecast',\n                    content='Sunny with a chance of rain',\n                    tool_call_id=IsStr(),\n                    timestamp=IsNow(tz=timezone.utc),\n                ),\n            ],\n        ),\n        ModelResponse(\n            parts=[\n                TextPart(\n                    content='{\"weather_forecast\":\"Sunny with a chance of rain\"}',\n                )\n            ],\n            usage=RequestUsage(\n                input_tokens=77,\n                output_tokens=16,\n            ),\n            model_name='test',\n            timestamp=IsNow(tz=timezone.utc),\n        ),\n    ]\n```\n\n1. We're using [anyio](https://anyio.readthedocs.io/en/stable/) to run async tests.\n2. This is a safety measure to make sure we don't accidentally make real requests to the LLM while testing, see [`ALLOW_MODEL_REQUESTS`][pydantic_ai.models.ALLOW_MODEL_REQUESTS] for more details.\n3. We're using [`Agent.override`][pydantic_ai.agent.Agent.override] to replace the agent's model with [`TestModel`][pydantic_ai.models.test.TestModel], the nice thing about `override` is that we can replace the model inside agent without needing access to the agent `run*` methods call site.\n4. Now we call the function we want to test inside the `override` context manager.\n5. But default, `TestModel` will return a JSON string summarising the tools calls made, and what was returned. If you wanted to customise the response to something more closely aligned with the domain, you could add [`custom_output_text='Sunny'`][pydantic_ai.models.test.TestModel.custom_output_text] when defining `TestModel`.\n6. So far we don't actually know which tools were called and with which values, we can use [`capture_run_messages`][pydantic_ai.capture_run_messages] to inspect messages from the most recent run and assert the exchange between the agent and the model occurred as expected.\n7. The [`IsNow`][dirty_equals.IsNow] helper allows us to use declarative asserts even with data which will contain timestamps that change over time.\n8. `TestModel` isn't doing anything clever to extract values from the prompt, so these values are hardcoded.", "url": "https://ai.pydantic.dev/docs/testing/#unit-testing-with-testmodel", "page": "docs/testing", "source_site": "pydantic_ai"}
{"title": "Unit testing with `FunctionModel`", "anchor": "unit-testing-with-functionmodel", "heading_level": 3, "md_text": "The above tests are a great start, but careful readers will notice that the `WeatherService.get_forecast` is never called since `TestModel` calls `weather_forecast` with a date in the past.\n\nTo fully exercise `weather_forecast`, we need to use [`FunctionModel`][pydantic_ai.models.function.FunctionModel] to customise how the tools is called.\n\nHere's an example of using `FunctionModel` to test the `weather_forecast` tool with custom inputs\n\n```python {title=\"test_weather_app2.py\" call_name=\"test_forecast_future\" requires=\"weather_app.py\"}\nimport re\n\nimport pytest\n\nfrom pydantic_ai import models\nfrom pydantic_ai import (\n    ModelMessage,\n    ModelResponse,\n    TextPart,\n    ToolCallPart,\n)\nfrom pydantic_ai.models.function import AgentInfo, FunctionModel\n\nfrom fake_database import DatabaseConn\nfrom weather_app import run_weather_forecast, weather_agent\n\npytestmark = pytest.mark.anyio\nmodels.ALLOW_MODEL_REQUESTS = False\n\n\ndef call_weather_forecast(  # (1)!\n    messages: list[ModelMessage], info: AgentInfo\n) -> ModelResponse:\n    if len(messages) == 1:\n        # first call, call the weather forecast tool\n        user_prompt = messages[0].parts[-1]\n        m = re.search(r'\\d{4}-\\d{2}-\\d{2}', user_prompt.content)\n        assert m is not None\n        args = {'location': 'London', 'forecast_date': m.group()}  # (2)!\n        return ModelResponse(parts=[ToolCallPart('weather_forecast', args)])\n    else:\n        # second call, return the forecast\n        msg = messages[-1].parts[0]\n        assert msg.part_kind == 'tool-return'\n        return ModelResponse(parts=[TextPart(f'The forecast is: {msg.content}')])\n\n\nasync def test_forecast_future():\n    conn = DatabaseConn()\n    user_id = 1\n    with weather_agent.override(model=FunctionModel(call_weather_forecast)):  # (3)!\n        prompt = 'What will the weather be like in London on 2032-01-01?'\n        await run_weather_forecast([(prompt, user_id)], conn)\n\n    forecast = await conn.get_forecast(user_id)\n    assert forecast == 'The forecast is: Rainy with a chance of sun'\n```\n\n1. We define a function `call_weather_forecast` that will be called by `FunctionModel` in place of the LLM, this function has access to the list of [`ModelMessage`][pydantic_ai.messages.ModelMessage]s that make up the run, and [`AgentInfo`][pydantic_ai.models.function.AgentInfo] which contains information about the agent and the function tools and return tools.\n2. Our function is slightly intelligent in that it tries to extract a date from the prompt, but just hard codes the location.\n3. We use [`FunctionModel`][pydantic_ai.models.function.FunctionModel] to replace the agent's model with our custom function.", "url": "https://ai.pydantic.dev/docs/testing/#unit-testing-with-functionmodel", "page": "docs/testing", "source_site": "pydantic_ai"}
{"title": "Overriding model via pytest fixtures", "anchor": "overriding-model-via-pytest-fixtures", "heading_level": 3, "md_text": "If you're writing lots of tests that all require model to be overridden, you can use [pytest fixtures](https://docs.pytest.org/en/6.2.x/fixture.html) to override the model with [`TestModel`][pydantic_ai.models.test.TestModel] or [`FunctionModel`][pydantic_ai.models.function.FunctionModel] in a reusable way.\n\nHere's an example of a fixture that overrides the model with `TestModel`:\n\n```python {title=\"test_agent.py\" requires=\"weather_app.py\"}\nimport pytest\n\nfrom pydantic_ai.models.test import TestModel\n\nfrom weather_app import weather_agent\n\n\n@pytest.fixture\ndef override_weather_agent():\n    with weather_agent.override(model=TestModel()):\n        yield\n\n\nasync def test_forecast(override_weather_agent: None):\n    ...\n    # test code here\n```", "url": "https://ai.pydantic.dev/docs/testing/#overriding-model-via-pytest-fixtures", "page": "docs/testing", "source_site": "pydantic_ai"}
{"title": "HTTP Request Retries", "anchor": "http-request-retries", "heading_level": 1, "md_text": "Pydantic AI provides retry functionality for HTTP requests made by model providers through custom HTTP transports.\nThis is particularly useful for handling transient failures like rate limits, network timeouts, or temporary server errors.", "url": "https://ai.pydantic.dev/docs/retries/#http-request-retries", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "The retry functionality is built on top of the [tenacity](https://github.com/jd/tenacity) library and integrates\nseamlessly with httpx clients. You can configure retry behavior for any provider that accepts a custom HTTP client.", "url": "https://ai.pydantic.dev/docs/retries/#overview", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "Installation", "anchor": "installation", "heading_level": 2, "md_text": "To use the retry transports, you need to install `tenacity`, which you can do via the `retries` dependency group:\n\n```bash\npip/uv-add 'pydantic-ai-slim[retries]'\n```", "url": "https://ai.pydantic.dev/docs/retries/#installation", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "Usage Example", "anchor": "usage-example", "heading_level": 2, "md_text": "Here's an example of adding retry functionality with smart retry handling:\n\n```python {title=\"smart_retry_example.py\"}\nfrom httpx import AsyncClient, HTTPStatusError\nfrom tenacity import retry_if_exception_type, stop_after_attempt, wait_exponential\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\nfrom pydantic_ai.retries import AsyncTenacityTransport, RetryConfig, wait_retry_after\n\n\ndef create_retrying_client():\n    \"\"\"Create a client with smart retry handling for multiple error types.\"\"\"\n\n    def should_retry_status(response):\n        \"\"\"Raise exceptions for retryable HTTP status codes.\"\"\"\n        if response.status_code in (429, 502, 503, 504):\n            response.raise_for_status()  # This will raise HTTPStatusError\n\n    transport = AsyncTenacityTransport(\n        config=RetryConfig(\n            # Retry on HTTP errors and connection issues\n            retry=retry_if_exception_type((HTTPStatusError, ConnectionError)),\n            # Smart waiting: respects Retry-After headers, falls back to exponential backoff\n            wait=wait_retry_after(\n                fallback_strategy=wait_exponential(multiplier=1, max=60),\n                max_wait=300\n            ),\n            # Stop after 5 attempts\n            stop=stop_after_attempt(5),\n            # Re-raise the last exception if all retries fail\n            reraise=True\n        ),\n        validate_response=should_retry_status\n    )\n    return AsyncClient(transport=transport)", "url": "https://ai.pydantic.dev/docs/retries/#usage-example", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "Use the retrying client with a model", "anchor": "use-the-retrying-client-with-a-model", "heading_level": 1, "md_text": "client = create_retrying_client()\nmodel = OpenAIChatModel('gpt-5', provider=OpenAIProvider(http_client=client))\nagent = Agent(model)\n```", "url": "https://ai.pydantic.dev/docs/retries/#use-the-retrying-client-with-a-model", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "wait_retry_after", "anchor": "wait_retry_after", "heading_level": 3, "md_text": "The `wait_retry_after` function is a smart wait strategy that automatically respects HTTP `Retry-After` headers:\n\n```python {title=\"wait_strategy_example.py\"}\nfrom tenacity import wait_exponential\n\nfrom pydantic_ai.retries import wait_retry_after", "url": "https://ai.pydantic.dev/docs/retries/#wait_retry_after", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "Basic usage - respects Retry-After headers, falls back to exponential backoff", "anchor": "basic-usage-respects-retry-after-headers-falls-back-to-exponential-backoff", "heading_level": 1, "md_text": "wait_strategy_1 = wait_retry_after()", "url": "https://ai.pydantic.dev/docs/retries/#basic-usage-respects-retry-after-headers-falls-back-to-exponential-backoff", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "Custom configuration", "anchor": "custom-configuration", "heading_level": 1, "md_text": "wait_strategy_2 = wait_retry_after(\n    fallback_strategy=wait_exponential(multiplier=2, max=120),\n    max_wait=600  # Never wait more than 10 minutes\n)\n```\n\nThis wait strategy:\n\n- Automatically parses `Retry-After` headers from HTTP 429 responses\n- Supports both seconds format (`\"30\"`) and HTTP date format (`\"Wed, 21 Oct 2015 07:28:00 GMT\"`)\n- Falls back to your chosen strategy when no header is present\n- Respects the `max_wait` limit to prevent excessive delays", "url": "https://ai.pydantic.dev/docs/retries/#custom-configuration", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "AsyncTenacityTransport", "anchor": "asynctenacitytransport", "heading_level": 3, "md_text": "For asynchronous HTTP clients (recommended for most use cases):\n\n```python {title=\"async_transport_example.py\"}\nfrom httpx import AsyncClient\nfrom tenacity import stop_after_attempt\n\nfrom pydantic_ai.retries import AsyncTenacityTransport, RetryConfig\n\n\ndef validator(response):\n    \"\"\"Treat responses with HTTP status 4xx/5xx as failures that need to be retried.\n    Without a response validator, only network errors and timeouts will result in a retry.\n    \"\"\"\n    response.raise_for_status()", "url": "https://ai.pydantic.dev/docs/retries/#asynctenacitytransport", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "Create the transport", "anchor": "create-the-transport", "heading_level": 1, "md_text": "transport = AsyncTenacityTransport(\n    config=RetryConfig(stop=stop_after_attempt(3), reraise=True),\n    validate_response=validator\n)", "url": "https://ai.pydantic.dev/docs/retries/#create-the-transport", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "Create a client using the transport:", "anchor": "create-a-client-using-the-transport", "heading_level": 1, "md_text": "client = AsyncClient(transport=transport)\n```", "url": "https://ai.pydantic.dev/docs/retries/#create-a-client-using-the-transport", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "TenacityTransport", "anchor": "tenacitytransport", "heading_level": 3, "md_text": "For synchronous HTTP clients:\n\n```python {title=\"sync_transport_example.py\"}\nfrom httpx import Client\nfrom tenacity import stop_after_attempt\n\nfrom pydantic_ai.retries import RetryConfig, TenacityTransport\n\n\ndef validator(response):\n    \"\"\"Treat responses with HTTP status 4xx/5xx as failures that need to be retried.\n    Without a response validator, only network errors and timeouts will result in a retry.\n    \"\"\"\n    response.raise_for_status()", "url": "https://ai.pydantic.dev/docs/retries/#tenacitytransport", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "Create the transport", "anchor": "create-the-transport", "heading_level": 1, "md_text": "transport = TenacityTransport(\n    config=RetryConfig(stop=stop_after_attempt(3), reraise=True),\n    validate_response=validator\n)", "url": "https://ai.pydantic.dev/docs/retries/#create-the-transport", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "Create a client using the transport", "anchor": "create-a-client-using-the-transport", "heading_level": 1, "md_text": "client = Client(transport=transport)\n```", "url": "https://ai.pydantic.dev/docs/retries/#create-a-client-using-the-transport", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "Rate Limit Handling with Retry-After Support", "anchor": "rate-limit-handling-with-retry-after-support", "heading_level": 3, "md_text": "```python {title=\"rate_limit_handling.py\"}\nfrom httpx import AsyncClient, HTTPStatusError\nfrom tenacity import retry_if_exception_type, stop_after_attempt, wait_exponential\n\nfrom pydantic_ai.retries import AsyncTenacityTransport, RetryConfig, wait_retry_after\n\n\ndef create_rate_limit_client():\n    \"\"\"Create a client that respects Retry-After headers from rate limiting responses.\"\"\"\n    transport = AsyncTenacityTransport(\n        config=RetryConfig(\n            retry=retry_if_exception_type(HTTPStatusError),\n            wait=wait_retry_after(\n                fallback_strategy=wait_exponential(multiplier=1, max=60),\n                max_wait=300  # Don't wait more than 5 minutes\n            ),\n            stop=stop_after_attempt(10),\n            reraise=True\n        ),\n        validate_response=lambda r: r.raise_for_status()  # Raises HTTPStatusError for 4xx/5xx\n    )\n    return AsyncClient(transport=transport)", "url": "https://ai.pydantic.dev/docs/retries/#rate-limit-handling-with-retry-after-support", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "Example usage", "anchor": "example-usage", "heading_level": 1, "md_text": "client = create_rate_limit_client()", "url": "https://ai.pydantic.dev/docs/retries/#example-usage", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "Client is now ready to use with any HTTP requests and will respect Retry-After headers", "anchor": "client-is-now-ready-to-use-with-any-http-requests-and-will-respect-retry-after-headers", "heading_level": 1, "md_text": "```\n\nThe `wait_retry_after` function automatically detects `Retry-After` headers in 429 (rate limit) responses and waits for the specified time. If no header is present, it falls back to exponential backoff.", "url": "https://ai.pydantic.dev/docs/retries/#client-is-now-ready-to-use-with-any-http-requests-and-will-respect-retry-after-headers", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "Network Error Handling", "anchor": "network-error-handling", "heading_level": 3, "md_text": "```python {title=\"network_error_handling.py\"}\nimport httpx\nfrom tenacity import retry_if_exception_type, stop_after_attempt, wait_exponential\n\nfrom pydantic_ai.retries import AsyncTenacityTransport, RetryConfig\n\n\ndef create_network_resilient_client():\n    \"\"\"Create a client that handles network errors with retries.\"\"\"\n    transport = AsyncTenacityTransport(\n        config=RetryConfig(\n            retry=retry_if_exception_type((\n                httpx.TimeoutException,\n                httpx.ConnectError,\n                httpx.ReadError\n            )),\n            wait=wait_exponential(multiplier=1, max=10),\n            stop=stop_after_attempt(3),\n            reraise=True\n        )\n    )\n    return httpx.AsyncClient(transport=transport)", "url": "https://ai.pydantic.dev/docs/retries/#network-error-handling", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "Example usage", "anchor": "example-usage", "heading_level": 1, "md_text": "client = create_network_resilient_client()", "url": "https://ai.pydantic.dev/docs/retries/#example-usage", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "Client will now retry on timeout, connection, and read errors", "anchor": "client-will-now-retry-on-timeout-connection-and-read-errors", "heading_level": 1, "md_text": "```", "url": "https://ai.pydantic.dev/docs/retries/#client-will-now-retry-on-timeout-connection-and-read-errors", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "Custom Retry Logic", "anchor": "custom-retry-logic", "heading_level": 3, "md_text": "```python {title=\"custom_retry_logic.py\"}\nimport httpx\nfrom tenacity import retry_if_exception, stop_after_attempt, wait_exponential\n\nfrom pydantic_ai.retries import AsyncTenacityTransport, RetryConfig, wait_retry_after\n\n\ndef create_custom_retry_client():\n    \"\"\"Create a client with custom retry logic.\"\"\"\n    def custom_retry_condition(exception):\n        \"\"\"Custom logic to determine if we should retry.\"\"\"\n        if isinstance(exception, httpx.HTTPStatusError):\n            # Retry on server errors but not client errors\n            return 500 <= exception.response.status_code < 600\n        return isinstance(exception, httpx.TimeoutException | httpx.ConnectError)\n\n    transport = AsyncTenacityTransport(\n        config=RetryConfig(\n            retry=retry_if_exception(custom_retry_condition),\n            # Use wait_retry_after for smart waiting on rate limits,\n            # with custom exponential backoff as fallback\n            wait=wait_retry_after(\n                fallback_strategy=wait_exponential(multiplier=2, max=30),\n                max_wait=120\n            ),\n            stop=stop_after_attempt(5),\n            reraise=True\n        ),\n        validate_response=lambda r: r.raise_for_status()\n    )\n    return httpx.AsyncClient(transport=transport)\n\nclient = create_custom_retry_client()", "url": "https://ai.pydantic.dev/docs/retries/#custom-retry-logic", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "Client will retry server errors (5xx) and network errors, but not client errors (4xx)", "anchor": "client-will-retry-server-errors-5xx-and-network-errors-but-not-client-errors-4xx", "heading_level": 1, "md_text": "```", "url": "https://ai.pydantic.dev/docs/retries/#client-will-retry-server-errors-5xx-and-network-errors-but-not-client-errors-4xx", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "Using with Different Providers", "anchor": "using-with-different-providers", "heading_level": 2, "md_text": "The retry transports work with any provider that accepts a custom HTTP client:", "url": "https://ai.pydantic.dev/docs/retries/#using-with-different-providers", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "OpenAI", "anchor": "openai", "heading_level": 3, "md_text": "```python {title=\"openai_with_retries.py\" requires=\"smart_retry_example.py\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nfrom smart_retry_example import create_retrying_client\n\nclient = create_retrying_client()\nmodel = OpenAIChatModel('gpt-5', provider=OpenAIProvider(http_client=client))\nagent = Agent(model)\n```", "url": "https://ai.pydantic.dev/docs/retries/#openai", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "Anthropic", "anchor": "anthropic", "heading_level": 3, "md_text": "```python {title=\"anthropic_with_retries.py\" requires=\"smart_retry_example.py\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.providers.anthropic import AnthropicProvider\n\nfrom smart_retry_example import create_retrying_client\n\nclient = create_retrying_client()\nmodel = AnthropicModel('claude-sonnet-4-5-20250929', provider=AnthropicProvider(http_client=client))\nagent = Agent(model)\n```", "url": "https://ai.pydantic.dev/docs/retries/#anthropic", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "Any OpenAI-Compatible Provider", "anchor": "any-openai-compatible-provider", "heading_level": 3, "md_text": "```python {title=\"openai_compatible_with_retries.py\" requires=\"smart_retry_example.py\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nfrom smart_retry_example import create_retrying_client\n\nclient = create_retrying_client()\nmodel = OpenAIChatModel(\n    'your-model-name',  # Replace with actual model name\n    provider=OpenAIProvider(\n        base_url='https://api.example.com/v1',  # Replace with actual API URL\n        api_key='your-api-key',  # Replace with actual API key\n        http_client=client\n    )\n)\nagent = Agent(model)\n```", "url": "https://ai.pydantic.dev/docs/retries/#any-openai-compatible-provider", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "Best Practices", "anchor": "best-practices", "heading_level": 2, "md_text": "1. **Start Conservative**: Begin with a small number of retries (3-5) and reasonable wait times.\n\n2. **Use Exponential Backoff**: This helps avoid overwhelming servers during outages.\n\n3. **Set Maximum Wait Times**: Prevent indefinite delays with reasonable maximum wait times.\n\n4. **Handle Rate Limits Properly**: Respect `Retry-After` headers when possible.\n\n5. **Log Retry Attempts**: Add logging to monitor retry behavior in production. (This will be picked up by Logfire automatically if you instrument httpx.)\n\n6. **Consider Circuit Breakers**: For high-traffic applications, consider implementing circuit breaker patterns.", "url": "https://ai.pydantic.dev/docs/retries/#best-practices", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "Error Handling", "anchor": "error-handling", "heading_level": 2, "md_text": "The retry transports will re-raise the last exception if all retry attempts fail. Make sure to handle these appropriately in your application:\n\n```python {title=\"error_handling_example.py\" requires=\"smart_retry_example.py\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nfrom smart_retry_example import create_retrying_client\n\nclient = create_retrying_client()\nmodel = OpenAIChatModel('gpt-5', provider=OpenAIProvider(http_client=client))\nagent = Agent(model)\n```", "url": "https://ai.pydantic.dev/docs/retries/#error-handling", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "Performance Considerations", "anchor": "performance-considerations", "heading_level": 2, "md_text": "- Retries add latency to requests, especially with exponential backoff\n- Consider the total timeout for your application when configuring retry behavior\n- Monitor retry rates to detect systemic issues\n- Use async transports for better concurrency when handling multiple requests\n\nFor more advanced retry configurations, refer to the [tenacity documentation](https://tenacity.readthedocs.io/).", "url": "https://ai.pydantic.dev/docs/retries/#performance-considerations", "page": "docs/retries", "source_site": "pydantic_ai"}
{"title": "Common Tools", "anchor": "common-tools", "heading_level": 1, "md_text": "Pydantic AI ships with native tools that can be used to enhance your agent's capabilities.", "url": "https://ai.pydantic.dev/docs/common-tools/#common-tools", "page": "docs/common-tools", "source_site": "pydantic_ai"}
{"title": "DuckDuckGo Search Tool", "anchor": "duckduckgo-search-tool", "heading_level": 2, "md_text": "The DuckDuckGo search tool allows you to search the web for information. It is built on top of the\n[DuckDuckGo API](https://github.com/deedy5/ddgs).", "url": "https://ai.pydantic.dev/docs/common-tools/#duckduckgo-search-tool", "page": "docs/common-tools", "source_site": "pydantic_ai"}
{"title": "Installation", "anchor": "installation", "heading_level": 3, "md_text": "To use [`duckduckgo_search_tool`][pydantic_ai.common_tools.duckduckgo.duckduckgo_search_tool], you need to install\n[`pydantic-ai-slim`](install.md#slim-install) with the `duckduckgo` optional group:\n\n```bash\npip/uv-add \"pydantic-ai-slim[duckduckgo]\"\n```", "url": "https://ai.pydantic.dev/docs/common-tools/#installation", "page": "docs/common-tools", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 3, "md_text": "Here's an example of how you can use the DuckDuckGo search tool with an agent:\n\n```py {title=\"duckduckgo_search.py\" test=\"skip\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.common_tools.duckduckgo import duckduckgo_search_tool\n\nagent = Agent(\n    'openai:o3-mini',\n    tools=[duckduckgo_search_tool()],\n    system_prompt='Search DuckDuckGo for the given query and return the results.',\n)\n\nresult = agent.run_sync(\n    'Can you list the top five highest-grossing animated films of 2025?'\n)\nprint(result.output)\n\"\"\"\nI looked into several sources on animated box\u2010office performance in 2025, and while detailed\nrankings can shift as more money is tallied, multiple independent reports have already\nhighlighted a couple of record\u2010breaking shows. For example:\n\n\u2022 Ne Zha 2 \u2013 News outlets (Variety, Wikipedia's \"List of animated feature films of 2025\", and others)\n    have reported that this Chinese title not only became the highest\u2011grossing animated film of 2025\n    but also broke records as the highest\u2011grossing non\u2011English animated film ever. One article noted\n    its run exceeded US$1.7 billion.\n\u2022 Inside Out 2 \u2013 According to data shared on Statista and in industry news, this Pixar sequel has been\n    on pace to set new records (with some sources even noting it as the highest\u2011grossing animated film\n    ever, as of January 2025).\n\nBeyond those two, some entertainment trade sites (for example, a Just Jared article titled\n\"Top 10 Highest-Earning Animated Films at the Box Office Revealed\") have begun listing a broader\ntop\u201110. Although full consolidated figures can sometimes differ by source and are updated daily during\na box\u2011office run, many of the industry trackers have begun to single out five films as the biggest\nearners so far in 2025.\n\nUnfortunately, although multiple articles discuss the \"top animated films\" of 2025, there isn't yet a\nsingle, universally accepted list with final numbers that names the complete top five. (Box\u2011office\nrankings, especially mid\u2011year, can be fluid as films continue to add to their totals.)\n\nBased on what several sources note so far, the two undisputed leaders are:\n1. Ne Zha 2\n2. Inside Out 2\n\nThe remaining top spots (3\u20135) are reported by some outlets in their \"Top\u201110 Animated Films\"\nlists for 2025 but the titles and order can vary depending on the source and the exact cut\u2011off\ndate of the data. For the most up\u2011to\u2011date and detailed ranking (including the 3rd, 4th, and 5th\nhighest\u2011grossing films), I recommend checking resources like:\n\u2022 Wikipedia's \"List of animated feature films of 2025\" page\n\u2022 Box\u2011office tracking sites (such as Box Office Mojo or The Numbers)\n\u2022 Trade articles like the one on Just Jared\n\nTo summarize with what is clear from the current reporting:\n1. Ne Zha 2\n2. Inside Out 2\n3\u20135. Other animated films (yet to be definitively finalized across all reporting outlets)\n\nIf you're looking for a final, consensus list of the top five, it may be best to wait until\nthe 2025 year\u2011end box\u2011office tallies are in or to consult a regularly updated entertainment industry source.\n\nWould you like help finding a current source or additional details on where to look for the complete updated list?\n\"\"\"\n```", "url": "https://ai.pydantic.dev/docs/common-tools/#usage", "page": "docs/common-tools", "source_site": "pydantic_ai"}
{"title": "Tavily Search Tool", "anchor": "tavily-search-tool", "heading_level": 2, "md_text": "!!! info\n    Tavily is a paid service, but they have free credits to explore their product.\n\n    You need to [sign up for an account](https://app.tavily.com/home) and get an API key to use the Tavily search tool.\n\nThe Tavily search tool allows you to search the web for information. It is built on top of the [Tavily API](https://tavily.com/).", "url": "https://ai.pydantic.dev/docs/common-tools/#tavily-search-tool", "page": "docs/common-tools", "source_site": "pydantic_ai"}
{"title": "Installation", "anchor": "installation", "heading_level": 3, "md_text": "To use [`tavily_search_tool`][pydantic_ai.common_tools.tavily.tavily_search_tool], you need to install\n[`pydantic-ai-slim`](install.md#slim-install) with the `tavily` optional group:\n\n```bash\npip/uv-add \"pydantic-ai-slim[tavily]\"\n```", "url": "https://ai.pydantic.dev/docs/common-tools/#installation", "page": "docs/common-tools", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 3, "md_text": "Here's an example of how you can use the Tavily search tool with an agent:\n\n```py {title=\"tavily_search.py\" test=\"skip\"}\nimport os\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.common_tools.tavily import tavily_search_tool\n\napi_key = os.getenv('TAVILY_API_KEY')\nassert api_key is not None\n\nagent = Agent(\n    'openai:o3-mini',\n    tools=[tavily_search_tool(api_key)],\n    system_prompt='Search Tavily for the given query and return the results.',\n)\n\nresult = agent.run_sync('Tell me the top news in the GenAI world, give me links.')\nprint(result.output)\n\"\"\"\nHere are some of the top recent news articles related to GenAI:\n\n1. How CLEAR users can improve risk analysis with GenAI \u2013 Thomson Reuters\n   Read more: https://legal.thomsonreuters.com/blog/how-clear-users-can-improve-risk-analysis-with-genai/\n   (This article discusses how CLEAR's new GenAI-powered tool streamlines risk analysis by quickly summarizing key information from various public data sources.)\n\n2. TELUS Digital Survey Reveals Enterprise Employees Are Entering Sensitive Data Into AI Assistants More Than You Think \u2013 FT.com\n   Read more: https://markets.ft.com/data/announce/detail?dockey=600-202502260645BIZWIRE_USPRX____20250226_BW490609-1\n   (This news piece highlights findings from a TELUS Digital survey showing that many enterprise employees use public GenAI tools and sometimes even enter sensitive data.)\n\n3. The Essential Guide to Generative AI \u2013 Virtualization Review\n   Read more: https://virtualizationreview.com/Whitepapers/2025/02/SNOWFLAKE-The-Essential-Guide-to-Generative-AI.aspx\n   (This guide provides insights into how GenAI is revolutionizing enterprise strategies and productivity, with input from industry leaders.)\n\nFeel free to click on the links to dive deeper into each story!\n\"\"\"\n```", "url": "https://ai.pydantic.dev/docs/common-tools/#usage", "page": "docs/common-tools", "source_site": "pydantic_ai"}
{"title": "Version Policy", "anchor": "version-policy", "heading_level": 2, "md_text": "We will not intentionally make breaking changes in minor releases of V1. V2 will be released in April 2026 at the earliest, 6 months after the release of V1 in September 2025.\n\nOnce we release V2, we'll continue to provide security fixes for V1 for another 6 months minimum, so you have time to upgrade your applications.\n\nFunctionality marked as deprecated will not be removed until V2.\n\nOf course, some apparently safe changes and bug fixes will inevitably break some users' code &mdash; obligatory link to [xkcd](https://xkcd.com/1172/).\n\nThe following changes will **NOT** be considered breaking changes, and may occur in minor releases:\n\n* Bug fixes that may result in existing code breaking, provided that such code was relying on undocumented features/constructs/assumptions.\n* Adding new [message parts][pydantic_ai.messages], [stream events][pydantic_ai.messages.AgentStreamEvent], or optional fields on existing message (part) and event types. Always code defensively when consuming message parts or event streams, and use the [`ModelMessagesTypeAdapter`][pydantic_ai.messages.ModelMessagesTypeAdapter] to (de)serialize message histories.\n* Changing OpenTelemetry span attributes. Because different [observability platforms](logfire.md#using-opentelemetry) support different versions of the [OpenTelemetry Semantic Conventions for Generative AI systems](https://opentelemetry.io/docs/specs/semconv/gen-ai/), Pydantic AI lets you configure the [instrumentation version](logfire.md#configuring-data-format), but the default version may change in a minor release. Span attributes for [Pydantic Evals](evals.md) may also change as we iterate on Evals support in [Pydantic Logfire](https://logfire.pydantic.dev/docs/guides/web-ui/evals/).\n* Changing how `__repr__` behaves, even of public classes.\n\nIn all cases we will aim to minimize churn and do so only when justified by the increase of quality of Pydantic AI for users.", "url": "https://ai.pydantic.dev/docs/version-policy/#version-policy", "page": "docs/version-policy", "source_site": "pydantic_ai"}
{"title": "Beta Features", "anchor": "beta-features", "heading_level": 2, "md_text": "At Pydantic, we like to move quickly and innovate! To that end, minor releases may introduce beta features (indicated by a `beta` module) that are active works in progress. While in its beta phase, a feature's API and behaviors may not be stable, and it's very possible that changes made to the feature will not be backward-compatible. We aim to move beta features out of beta within a few months after initial release, once users have had a chance to provide feedback and test the feature in production.", "url": "https://ai.pydantic.dev/docs/version-policy/#beta-features", "page": "docs/version-policy", "source_site": "pydantic_ai"}
{"title": "Support for Python versions", "anchor": "support-for-python-versions", "heading_level": 2, "md_text": "Pydantic will drop support for a Python version when the following conditions are met:\n\n* The Python version has reached its [expected end of life](https://devguide.python.org/versions/).\n* less than 5% of downloads of the most recent minor release are using that version.", "url": "https://ai.pydantic.dev/docs/version-policy/#support-for-python-versions", "page": "docs/version-policy", "source_site": "pydantic_ai"}
{"title": "Agent2Agent (A2A) Protocol", "anchor": "agent2agent-a2a-protocol", "heading_level": 1, "md_text": "The [Agent2Agent (A2A) Protocol](https://google.github.io/A2A/) is an open standard introduced by Google that enables\ncommunication and interoperability between AI agents, regardless of the framework or vendor they are built on.\n\nAt Pydantic, we built the [FastA2A](#fasta2a) library to make it easier to implement the A2A protocol in Python.\n\nWe also built a convenience method that expose Pydantic AI agents as A2A servers - let's have a quick look at how to use it:\n\n```py {title=\"agent_to_a2a.py\" hl_lines=\"4\"}\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-5', instructions='Be fun!')\napp = agent.to_a2a()\n```\n\n_You can run the example with `uvicorn agent_to_a2a:app --host 0.0.0.0 --port 8000`_\n\nThis will expose the agent as an A2A server, and you can start sending requests to it.\n\nSee more about [exposing Pydantic AI agents as A2A servers](#pydantic-ai-agent-to-a2a-server).", "url": "https://ai.pydantic.dev/docs/a2a/#agent2agent-a2a-protocol", "page": "docs/a2a", "source_site": "pydantic_ai"}
{"title": "FastA2A", "anchor": "fasta2a", "heading_level": 2, "md_text": "**FastA2A** is an agentic framework agnostic implementation of the A2A protocol in Python.\nThe library is designed to be used with any agentic framework, and is **not exclusive to Pydantic AI**.", "url": "https://ai.pydantic.dev/docs/a2a/#fasta2a", "page": "docs/a2a", "source_site": "pydantic_ai"}
{"title": "Design", "anchor": "design", "heading_level": 3, "md_text": "**FastA2A** is built on top of [Starlette](https://www.starlette.io), which means it's fully compatible with any ASGI server.\n\nGiven the nature of the A2A protocol, it's important to understand the design before using it, as a developer\nyou'll need to provide some components:\n\n- [`Storage`][fasta2a.Storage]: to save and load tasks, as well as store context for conversations\n- [`Broker`][fasta2a.Broker]: to schedule tasks\n- [`Worker`][fasta2a.Worker]: to execute tasks\n\nLet's have a look at how those components fit together:\n\n```mermaid\nflowchart TB\n    Server[\"HTTP Server\"] <--> |Sends Requests/<br>Receives Results| TM\n\n    subgraph CC[Core Components]\n        direction RL\n        TM[\"TaskManager<br>(coordinates)\"] --> |Schedules Tasks| Broker\n        TM <--> Storage\n        Broker[\"Broker<br>(queues & schedules)\"] <--> Storage[\"Storage<br>(persistence)\"]\n        Broker --> |Delegates Execution| Worker\n    end\n\n    Worker[\"Worker<br>(implementation)\"]\n```\n\nFastA2A allows you to bring your own [`Storage`][fasta2a.Storage], [`Broker`][fasta2a.Broker] and [`Worker`][fasta2a.Worker].", "url": "https://ai.pydantic.dev/docs/a2a/#design", "page": "docs/a2a", "source_site": "pydantic_ai"}
{"title": "Understanding Tasks and Context", "anchor": "understanding-tasks-and-context", "heading_level": 4, "md_text": "In the A2A protocol:\n\n- **Task**: Represents one complete execution of an agent. When a client sends a message to the agent, a new task is created. The agent runs until completion (or failure), and this entire execution is considered one task. The final output is stored as a task artifact.\n\n- **Context**: Represents a conversation thread that can span multiple tasks. The A2A protocol uses a `context_id` to maintain conversation continuity:\n  - When a new message is sent without a `context_id`, the server generates a new one\n  - Subsequent messages can include the same `context_id` to continue the conversation\n  - All tasks sharing the same `context_id` have access to the complete message history", "url": "https://ai.pydantic.dev/docs/a2a/#understanding-tasks-and-context", "page": "docs/a2a", "source_site": "pydantic_ai"}
{"title": "Storage Architecture", "anchor": "storage-architecture", "heading_level": 4, "md_text": "The [`Storage`][fasta2a.Storage] component serves two purposes:\n\n1. **Task Storage**: Stores tasks in A2A protocol format, including their status, artifacts, and message history\n2. **Context Storage**: Stores conversation context in a format optimized for the specific agent implementation\n\nThis design allows for agents to store rich internal state (e.g., tool calls, reasoning traces) as well as store task-specific A2A-formatted messages and artifacts.\n\nFor example, a Pydantic AI agent might store its complete internal message format (including tool calls and responses) in the context storage, while storing only the A2A-compliant messages in the task history.", "url": "https://ai.pydantic.dev/docs/a2a/#storage-architecture", "page": "docs/a2a", "source_site": "pydantic_ai"}
{"title": "Installation", "anchor": "installation", "heading_level": 3, "md_text": "FastA2A is available on PyPI as [`fasta2a`](https://pypi.org/project/fasta2a/) so installation is as simple as:\n\n```bash\npip/uv-add fasta2a\n```\n\nThe only dependencies are:\n\n- [starlette](https://www.starlette.io): to expose the A2A server as an [ASGI application](https://asgi.readthedocs.io/en/latest/)\n- [pydantic](https://pydantic.dev): to validate the request/response messages\n- [opentelemetry-api](https://opentelemetry-python.readthedocs.io/en/latest): to provide tracing capabilities\n\nYou can install Pydantic AI with the `a2a` extra to include **FastA2A**:\n\n```bash\npip/uv-add 'pydantic-ai-slim[a2a]'\n```", "url": "https://ai.pydantic.dev/docs/a2a/#installation", "page": "docs/a2a", "source_site": "pydantic_ai"}
{"title": "Pydantic AI Agent to A2A Server", "anchor": "pydantic-ai-agent-to-a2a-server", "heading_level": 3, "md_text": "To expose a Pydantic AI agent as an A2A server, you can use the `to_a2a` method:\n\n```python {title=\"agent_to_a2a.py\"}\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-5', instructions='Be fun!')\napp = agent.to_a2a()\n```\n\nSince `app` is an ASGI application, it can be used with any ASGI server.\n\n```bash\nuvicorn agent_to_a2a:app --host 0.0.0.0 --port 8000\n```\n\nSince the goal of `to_a2a` is to be a convenience method, it accepts the same arguments as the [`FastA2A`][fasta2a.FastA2A] constructor.\n\nWhen using `to_a2a()`, Pydantic AI automatically:\n\n- Stores the complete conversation history (including tool calls and responses) in the context storage\n- Ensures that subsequent messages with the same `context_id` have access to the full conversation history\n- Persists agent results as A2A artifacts:\n  - String results become `TextPart` artifacts and also appear in the message history\n  - Structured data (Pydantic models, dataclasses, tuples, etc.) become `DataPart` artifacts with the data wrapped as `{\"result\": <your_data>}`\n  - Artifacts include metadata with type information and JSON schema when available", "url": "https://ai.pydantic.dev/docs/a2a/#pydantic-ai-agent-to-a2a-server", "page": "docs/a2a", "source_site": "pydantic_ai"}
{"title": "Thinking", "anchor": "thinking", "heading_level": 1, "md_text": "Thinking (or reasoning) is the process by which a model works through a problem step-by-step before\nproviding its final answer.\n\nThis capability is typically disabled by default and depends on the specific model being used.\nSee the sections below for how to enable thinking for each provider.", "url": "https://ai.pydantic.dev/docs/thinking/#thinking", "page": "docs/thinking", "source_site": "pydantic_ai"}
{"title": "OpenAI", "anchor": "openai", "heading_level": 2, "md_text": "When using the [`OpenAIChatModel`][pydantic_ai.models.openai.OpenAIChatModel], text output inside `<think>` tags are converted to [`ThinkingPart`][pydantic_ai.messages.ThinkingPart] objects.\nYou can customize the tags using the [`thinking_tags`][pydantic_ai.profiles.ModelProfile.thinking_tags] field on the [model profile](models/openai.md#model-profile).", "url": "https://ai.pydantic.dev/docs/thinking/#openai", "page": "docs/thinking", "source_site": "pydantic_ai"}
{"title": "OpenAI Responses", "anchor": "openai-responses", "heading_level": 3, "md_text": "The [`OpenAIResponsesModel`][pydantic_ai.models.openai.OpenAIResponsesModel] can generate native thinking parts.\nTo enable this functionality, you need to set the\n[`OpenAIResponsesModelSettings.openai_reasoning_effort`][pydantic_ai.models.openai.OpenAIResponsesModelSettings.openai_reasoning_effort] and [`OpenAIResponsesModelSettings.openai_reasoning_summary`][pydantic_ai.models.openai.OpenAIResponsesModelSettings.openai_reasoning_summary] [model settings](agents.md#model-run-settings).\n\nBy default, the unique IDs of reasoning, text, and function call parts from the message history are sent to the model, which can result in errors like `\"Item 'rs_123' of type 'reasoning' was provided without its required following item.\"`\nif the message history you're sending does not match exactly what was received from the Responses API in a previous response, for example if you're using a [history processor](message-history.md#processing-message-history).\nTo disable this, you can disable the [`OpenAIResponsesModelSettings.openai_send_reasoning_ids`][pydantic_ai.models.openai.OpenAIResponsesModelSettings.openai_send_reasoning_ids] [model setting](agents.md#model-run-settings).\n\n```python {title=\"openai_thinking_part.py\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIResponsesModel, OpenAIResponsesModelSettings\n\nmodel = OpenAIResponsesModel('gpt-5')\nsettings = OpenAIResponsesModelSettings(\n    openai_reasoning_effort='low',\n    openai_reasoning_summary='detailed',\n)\nagent = Agent(model, model_settings=settings)\n...\n```", "url": "https://ai.pydantic.dev/docs/thinking/#openai-responses", "page": "docs/thinking", "source_site": "pydantic_ai"}
{"title": "Anthropic", "anchor": "anthropic", "heading_level": 2, "md_text": "To enable thinking, use the [`AnthropicModelSettings.anthropic_thinking`][pydantic_ai.models.anthropic.AnthropicModelSettings.anthropic_thinking] [model setting](agents.md#model-run-settings).\n\n```python {title=\"anthropic_thinking_part.py\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel, AnthropicModelSettings\n\nmodel = AnthropicModel('claude-sonnet-4-0')\nsettings = AnthropicModelSettings(\n    anthropic_thinking={'type': 'enabled', 'budget_tokens': 1024},\n)\nagent = Agent(model, model_settings=settings)\n...\n```", "url": "https://ai.pydantic.dev/docs/thinking/#anthropic", "page": "docs/thinking", "source_site": "pydantic_ai"}
{"title": "Google", "anchor": "google", "heading_level": 2, "md_text": "To enable thinking, use the [`GoogleModelSettings.google_thinking_config`][pydantic_ai.models.google.GoogleModelSettings.google_thinking_config] [model setting](agents.md#model-run-settings).\n\n```python {title=\"google_thinking_part.py\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel, GoogleModelSettings\n\nmodel = GoogleModel('gemini-2.5-pro')\nsettings = GoogleModelSettings(google_thinking_config={'include_thoughts': True})\nagent = Agent(model, model_settings=settings)\n...\n```", "url": "https://ai.pydantic.dev/docs/thinking/#google", "page": "docs/thinking", "source_site": "pydantic_ai"}
{"title": "Groq", "anchor": "groq", "heading_level": 2, "md_text": "Groq supports different formats to receive thinking parts:\n\n- `\"raw\"`: The thinking part is included in the text content inside `<think>` tags, which are automatically converted to [`ThinkingPart`][pydantic_ai.messages.ThinkingPart] objects.\n- `\"hidden\"`: The thinking part is not included in the text content.\n- `\"parsed\"`: The thinking part has its own structured part in the response which is converted into a [`ThinkingPart`][pydantic_ai.messages.ThinkingPart] object.\n\nTo enable thinking, use the [`GroqModelSettings.groq_reasoning_format`][pydantic_ai.models.groq.GroqModelSettings.groq_reasoning_format] [model setting](agents.md#model-run-settings):\n\n```python {title=\"groq_thinking_part.py\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.groq import GroqModel, GroqModelSettings\n\nmodel = GroqModel('qwen-qwq-32b')\nsettings = GroqModelSettings(groq_reasoning_format='parsed')\nagent = Agent(model, model_settings=settings)\n...\n```", "url": "https://ai.pydantic.dev/docs/thinking/#groq", "page": "docs/thinking", "source_site": "pydantic_ai"}
{"title": "Mistral", "anchor": "mistral", "heading_level": 2, "md_text": "Thinking is supported by the `magistral` family of models. It does not need to be specifically enabled.", "url": "https://ai.pydantic.dev/docs/thinking/#mistral", "page": "docs/thinking", "source_site": "pydantic_ai"}
{"title": "Cohere", "anchor": "cohere", "heading_level": 2, "md_text": "Thinking is supported by the `command-a-reasoning-08-2025` model. It does not need to be specifically enabled.", "url": "https://ai.pydantic.dev/docs/thinking/#cohere", "page": "docs/thinking", "source_site": "pydantic_ai"}
{"title": "Hugging Face", "anchor": "hugging-face", "heading_level": 2, "md_text": "Text output inside `<think>` tags is automatically converted to [`ThinkingPart`][pydantic_ai.messages.ThinkingPart] objects.\nYou can customize the tags using the [`thinking_tags`][pydantic_ai.profiles.ModelProfile.thinking_tags] field on the [model profile](models/openai.md#model-profile).", "url": "https://ai.pydantic.dev/docs/thinking/#hugging-face", "page": "docs/thinking", "source_site": "pydantic_ai"}
{"title": "Outlines", "anchor": "outlines", "heading_level": 2, "md_text": "Some local models run through Outlines include in their text output a thinking part delimited by tags. In that case, it will be handled by Pydantic AI that will separate the thinking part from the final answer without the need to specifically enable it. The thinking tags used by default are `\"<think>\"` and `\"</think>\"`. If your model uses different tags, you can specify them in the [model profile](models/openai.md#model-profile) using the [`thinking_tags`][pydantic_ai.profiles.ModelProfile.thinking_tags] field.\n\nOutlines currently does not support thinking along with structured output. If you provide an `output_type`, the model text output will not contain a thinking part with the associated tags, and you may experience degraded performance.", "url": "https://ai.pydantic.dev/docs/thinking/#outlines", "page": "docs/thinking", "source_site": "pydantic_ai"}
{"title": "Google", "anchor": "google", "heading_level": 1, "md_text": "The `GoogleModel` is a model that uses the [`google-genai`](https://pypi.org/project/google-genai/) package under the hood to\naccess Google's Gemini models via both the Generative Language API and Vertex AI.", "url": "https://ai.pydantic.dev/docs/models/google/#google", "page": "docs/models/google", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "heading_level": 2, "md_text": "To use `GoogleModel`, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `google` optional group:\n\n```bash\npip/uv-add \"pydantic-ai-slim[google]\"\n```", "url": "https://ai.pydantic.dev/docs/models/google/#install", "page": "docs/models/google", "source_site": "pydantic_ai"}
{"title": "Configuration", "anchor": "configuration", "heading_level": 2, "md_text": "`GoogleModel` lets you use Google's Gemini models through their [Generative Language API](https://ai.google.dev/api/all-methods) (`generativelanguage.googleapis.com`) or [Vertex AI API](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models) (`*-aiplatform.googleapis.com`).", "url": "https://ai.pydantic.dev/docs/models/google/#configuration", "page": "docs/models/google", "source_site": "pydantic_ai"}
{"title": "API Key (Generative Language API)", "anchor": "api-key-generative-language-api", "heading_level": 3, "md_text": "To use Gemini via the Generative Language API, go to [aistudio.google.com](https://aistudio.google.com/apikey) and create an API key.\n\nOnce you have the API key, set it as an environment variable:\n\n```bash\nexport GOOGLE_API_KEY=your-api-key\n```\n\nYou can then use `GoogleModel` by name (where GLA stands for Generative Language API):\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('google-gla:gemini-2.5-pro')\n...\n```\n\nOr you can explicitly create the provider:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\nprovider = GoogleProvider(api_key='your-api-key')\nmodel = GoogleModel('gemini-2.5-pro', provider=provider)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/google/#api-key-generative-language-api", "page": "docs/models/google", "source_site": "pydantic_ai"}
{"title": "Vertex AI (Enterprise/Cloud)", "anchor": "vertex-ai-enterprisecloud", "heading_level": 3, "md_text": "If you are an enterprise user, you can also use `GoogleModel` to access Gemini via Vertex AI.\n\nThis interface has a number of advantages over the Generative Language API:\n\n1. The VertexAI API comes with more enterprise readiness guarantees.\n2. You can [purchase provisioned throughput](https://cloud.google.com/vertex-ai/generative-ai/docs/provisioned-throughput#purchase-provisioned-throughput) with Vertex AI to guarantee capacity.\n3. If you're running Pydantic AI inside GCP, you don't need to set up authentication, it should \"just work\".\n4. You can decide which region to use, which might be important from a regulatory perspective, and might improve latency.\n\nYou can authenticate using [application default credentials](https://cloud.google.com/docs/authentication/application-default-credentials), a service account, or an [API key](https://cloud.google.com/vertex-ai/generative-ai/docs/start/api-keys?usertype=expressmode).\n\nWhichever way you authenticate, you'll need to have Vertex AI enabled in your GCP account.", "url": "https://ai.pydantic.dev/docs/models/google/#vertex-ai-enterprisecloud", "page": "docs/models/google", "source_site": "pydantic_ai"}
{"title": "Application Default Credentials", "anchor": "application-default-credentials", "heading_level": 4, "md_text": "If you have the [`gcloud` CLI](https://cloud.google.com/sdk/gcloud) installed and configured, you can use `GoogleProvider` in Vertex AI mode by name:\n\n```python {test=\"ci_only\"}\nfrom pydantic_ai import Agent\n\nagent = Agent('google-vertex:gemini-2.5-pro')\n...\n```\n\nOr you can explicitly create the provider and model:\n\n```python {test=\"ci_only\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\nprovider = GoogleProvider(vertexai=True)\nmodel = GoogleModel('gemini-2.5-pro', provider=provider)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/google/#application-default-credentials", "page": "docs/models/google", "source_site": "pydantic_ai"}
{"title": "Service Account", "anchor": "service-account", "heading_level": 4, "md_text": "To use a service account JSON file, explicitly create the provider and model:\n\n```python {title=\"google_model_service_account.py\" test=\"skip\"}\nfrom google.oauth2 import service_account\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\ncredentials = service_account.Credentials.from_service_account_file(\n    'path/to/service-account.json',\n    scopes=['https://www.googleapis.com/auth/cloud-platform'],\n)\nprovider = GoogleProvider(credentials=credentials, project='your-project-id')\nmodel = GoogleModel('gemini-2.5-flash', provider=provider)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/google/#service-account", "page": "docs/models/google", "source_site": "pydantic_ai"}
{"title": "API Key", "anchor": "api-key", "heading_level": 4, "md_text": "To use Vertex AI with an API key, [create a key](https://cloud.google.com/vertex-ai/generative-ai/docs/start/api-keys?usertype=expressmode) and set it as an environment variable:\n\n```bash\nexport GOOGLE_API_KEY=your-api-key\n```\n\nYou can then use `GoogleModel` in Vertex AI mode by name:\n\n```python {test=\"ci_only\"}\nfrom pydantic_ai import Agent\n\nagent = Agent('google-vertex:gemini-2.5-pro')\n...\n```\n\nOr you can explicitly create the provider and model:\n\n```python {test=\"skip\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\nprovider = GoogleProvider(vertexai=True, api_key='your-api-key')\nmodel = GoogleModel('gemini-2.5-pro', provider=provider)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/google/#api-key", "page": "docs/models/google", "source_site": "pydantic_ai"}
{"title": "Customizing Location or Project", "anchor": "customizing-location-or-project", "heading_level": 4, "md_text": "You can specify the location and/or project when using Vertex AI:\n\n```python {title=\"google_model_location.py\" test=\"skip\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\nprovider = GoogleProvider(vertexai=True, location='asia-east1', project='your-gcp-project-id')\nmodel = GoogleModel('gemini-2.5-pro', provider=provider)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/google/#customizing-location-or-project", "page": "docs/models/google", "source_site": "pydantic_ai"}
{"title": "Model Garden", "anchor": "model-garden", "heading_level": 4, "md_text": "You can access models from the [Model Garden](https://cloud.google.com/model-garden?hl=en) that support the `generateContent` API and are available under your GCP project, including but not limited to Gemini, using one of the following `model_name` patterns:\n\n- `{model_id}` for Gemini models\n- `{publisher}/{model_id}`\n- `publishers/{publisher}/models/{model_id}`\n- `projects/{project}/locations/{location}/publishers/{publisher}/models/{model_id}`\n\n```python {test=\"skip\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\nprovider = GoogleProvider(\n    project='your-gcp-project-id',\n    location='us-central1',  # the region where the model is available\n)\nmodel = GoogleModel('meta/llama-3.3-70b-instruct-maas', provider=provider)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/google/#model-garden", "page": "docs/models/google", "source_site": "pydantic_ai"}
{"title": "Custom HTTP Client", "anchor": "custom-http-client", "heading_level": 2, "md_text": "You can customize the `GoogleProvider` with a custom `httpx.AsyncClient`:\n\n```python\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\ncustom_http_client = AsyncClient(timeout=30)\nmodel = GoogleModel(\n    'gemini-2.5-pro',\n    provider=GoogleProvider(api_key='your-api-key', http_client=custom_http_client),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/google/#custom-http-client", "page": "docs/models/google", "source_site": "pydantic_ai"}
{"title": "Document, Image, Audio, and Video Input", "anchor": "document-image-audio-and-video-input", "heading_level": 2, "md_text": "`GoogleModel` supports multi-modal input, including documents, images, audio, and video. See the [input documentation](../input.md) for details and examples.", "url": "https://ai.pydantic.dev/docs/models/google/#document-image-audio-and-video-input", "page": "docs/models/google", "source_site": "pydantic_ai"}
{"title": "Model settings", "anchor": "model-settings", "heading_level": 2, "md_text": "You can customize model behavior using [`GoogleModelSettings`][pydantic_ai.models.google.GoogleModelSettings]:\n\n```python\nfrom google.genai.types import HarmBlockThreshold, HarmCategory\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel, GoogleModelSettings\n\nsettings = GoogleModelSettings(\n    temperature=0.2,\n    max_tokens=1024,\n    google_thinking_config={'thinking_budget': 2048},\n    google_safety_settings=[\n        {\n            'category': HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n            'threshold': HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n        }\n    ]\n)\nmodel = GoogleModel('gemini-2.5-flash')\nagent = Agent(model, model_settings=settings)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/google/#model-settings", "page": "docs/models/google", "source_site": "pydantic_ai"}
{"title": "Disable thinking", "anchor": "disable-thinking", "heading_level": 3, "md_text": "You can disable thinking by setting the `thinking_budget` to `0` on the `google_thinking_config`:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel, GoogleModelSettings\n\nmodel_settings = GoogleModelSettings(google_thinking_config={'thinking_budget': 0})\nmodel = GoogleModel('gemini-2.5-flash')\nagent = Agent(model, model_settings=model_settings)\n...\n```\n\nCheck out the [Gemini API docs](https://ai.google.dev/gemini-api/docs/thinking) for more on thinking.", "url": "https://ai.pydantic.dev/docs/models/google/#disable-thinking", "page": "docs/models/google", "source_site": "pydantic_ai"}
{"title": "Safety settings", "anchor": "safety-settings", "heading_level": 3, "md_text": "You can customize the safety settings by setting the `google_safety_settings` field.\n\n```python\nfrom google.genai.types import HarmBlockThreshold, HarmCategory\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel, GoogleModelSettings\n\nmodel_settings = GoogleModelSettings(\n    google_safety_settings=[\n        {\n            'category': HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n            'threshold': HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n        }\n    ]\n)\nmodel = GoogleModel('gemini-2.5-flash')\nagent = Agent(model, model_settings=model_settings)\n...\n```\n\nSee the [Gemini API docs](https://ai.google.dev/gemini-api/docs/safety-settings) for more on safety settings.", "url": "https://ai.pydantic.dev/docs/models/google/#safety-settings", "page": "docs/models/google", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "heading_level": 2, "md_text": "To use `GroqModel`, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `groq` optional group:\n\n```bash\npip/uv-add \"pydantic-ai-slim[groq]\"\n```", "url": "https://ai.pydantic.dev/docs/models/groq/#install", "page": "docs/models/groq", "source_site": "pydantic_ai"}
{"title": "Configuration", "anchor": "configuration", "heading_level": 2, "md_text": "To use [Groq](https://groq.com/) through their API, go to [console.groq.com/keys](https://console.groq.com/keys) and follow your nose until you find the place to generate an API key.\n\n`GroqModelName` contains a list of available Groq models.", "url": "https://ai.pydantic.dev/docs/models/groq/#configuration", "page": "docs/models/groq", "source_site": "pydantic_ai"}
{"title": "Environment variable", "anchor": "environment-variable", "heading_level": 2, "md_text": "Once you have the API key, you can set it as an environment variable:\n\n```bash\nexport GROQ_API_KEY='your-api-key'\n```\n\nYou can then use `GroqModel` by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('groq:llama-3.3-70b-versatile')\n...\n```\n\nOr initialise the model directly with just the model name:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.groq import GroqModel\n\nmodel = GroqModel('llama-3.3-70b-versatile')\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/groq/#environment-variable", "page": "docs/models/groq", "source_site": "pydantic_ai"}
{"title": "`provider` argument", "anchor": "provider-argument", "heading_level": 2, "md_text": "You can provide a custom `Provider` via the `provider` argument:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.groq import GroqModel\nfrom pydantic_ai.providers.groq import GroqProvider\n\nmodel = GroqModel(\n    'llama-3.3-70b-versatile', provider=GroqProvider(api_key='your-api-key')\n)\nagent = Agent(model)\n...\n```\n\nYou can also customize the `GroqProvider` with a custom `httpx.AsyncHTTPClient`:\n\n```python\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.groq import GroqModel\nfrom pydantic_ai.providers.groq import GroqProvider\n\ncustom_http_client = AsyncClient(timeout=30)\nmodel = GroqModel(\n    'llama-3.3-70b-versatile',\n    provider=GroqProvider(api_key='your-api-key', http_client=custom_http_client),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/groq/#provider-argument", "page": "docs/models/groq", "source_site": "pydantic_ai"}
{"title": "Hugging Face", "anchor": "hugging-face", "heading_level": 1, "md_text": "[Hugging Face](https://huggingface.co/) is an AI platform with all major open source models, datasets, MCPs, and demos. You can use [Inference Providers](https://huggingface.co/docs/inference-providers) to run open source models like DeepSeek R1 on scalable serverless infrastructure.", "url": "https://ai.pydantic.dev/docs/models/huggingface/#hugging-face", "page": "docs/models/huggingface", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "heading_level": 2, "md_text": "To use `HuggingFaceModel`, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `huggingface` optional group:\n\n```bash\npip/uv-add \"pydantic-ai-slim[huggingface]\"\n```", "url": "https://ai.pydantic.dev/docs/models/huggingface/#install", "page": "docs/models/huggingface", "source_site": "pydantic_ai"}
{"title": "Configuration", "anchor": "configuration", "heading_level": 2, "md_text": "To use [Hugging Face](https://huggingface.co/) inference, you'll need to set up an account which will give you [free tier](https://huggingface.co/docs/inference-providers/pricing) allowance on [Inference Providers](https://huggingface.co/docs/inference-providers). To setup inference, follow these steps:\n\n1. Go to [Hugging Face](https://huggingface.co/join) and sign up for an account.\n2. Create a new access token in [Hugging Face](https://huggingface.co/settings/tokens).\n3. Set the `HF_TOKEN` environment variable to the token you just created.\n\nOnce you have a Hugging Face access token, you can set it as an environment variable:\n\n```bash\nexport HF_TOKEN='hf_token'\n```", "url": "https://ai.pydantic.dev/docs/models/huggingface/#configuration", "page": "docs/models/huggingface", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 2, "md_text": "You can then use [`HuggingFaceModel`][pydantic_ai.models.huggingface.HuggingFaceModel] by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('huggingface:Qwen/Qwen3-235B-A22B')\n...\n```\n\nOr initialise the model directly with just the model name:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.huggingface import HuggingFaceModel\n\nmodel = HuggingFaceModel('Qwen/Qwen3-235B-A22B')\nagent = Agent(model)\n...\n```\n\nBy default, the [`HuggingFaceModel`][pydantic_ai.models.huggingface.HuggingFaceModel] uses the\n[`HuggingFaceProvider`][pydantic_ai.providers.huggingface.HuggingFaceProvider] that will select automatically\nthe first of the inference providers (Cerebras, Together AI, Cohere..etc) available for the model, sorted by your\npreferred order in https://hf.co/settings/inference-providers.", "url": "https://ai.pydantic.dev/docs/models/huggingface/#usage", "page": "docs/models/huggingface", "source_site": "pydantic_ai"}
{"title": "Configure the provider", "anchor": "configure-the-provider", "heading_level": 2, "md_text": "If you want to pass parameters in code to the provider, you can programmatically instantiate the\n[`HuggingFaceProvider`][pydantic_ai.providers.huggingface.HuggingFaceProvider] and pass it to the model:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.huggingface import HuggingFaceModel\nfrom pydantic_ai.providers.huggingface import HuggingFaceProvider\n\nmodel = HuggingFaceModel('Qwen/Qwen3-235B-A22B', provider=HuggingFaceProvider(api_key='hf_token', provider_name='nebius'))\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/huggingface/#configure-the-provider", "page": "docs/models/huggingface", "source_site": "pydantic_ai"}
{"title": "Custom Hugging Face client", "anchor": "custom-hugging-face-client", "heading_level": 2, "md_text": "[`HuggingFaceProvider`][pydantic_ai.providers.huggingface.HuggingFaceProvider] also accepts a custom\n[`AsyncInferenceClient`](https://huggingface.co/docs/huggingface_hub/v0.29.3/en/package_reference/inference_client#huggingface_hub.AsyncInferenceClient) client via the `hf_client` parameter, so you can customise\nthe `headers`, `bill_to` (billing to an HF organization you're a member of), `base_url` etc. as defined in the\n[Hugging Face Hub python library docs](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client).\n\n```python\nfrom huggingface_hub import AsyncInferenceClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.huggingface import HuggingFaceModel\nfrom pydantic_ai.providers.huggingface import HuggingFaceProvider\n\nclient = AsyncInferenceClient(\n    bill_to='openai',\n    api_key='hf_token',\n    provider='fireworks-ai',\n)\n\nmodel = HuggingFaceModel(\n    'Qwen/Qwen3-235B-A22B',\n    provider=HuggingFaceProvider(hf_client=client),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/huggingface/#custom-hugging-face-client", "page": "docs/models/huggingface", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "heading_level": 2, "md_text": "To use `MistralModel`, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `mistral` optional group:\n\n```bash\npip/uv-add \"pydantic-ai-slim[mistral]\"\n```", "url": "https://ai.pydantic.dev/docs/models/mistral/#install", "page": "docs/models/mistral", "source_site": "pydantic_ai"}
{"title": "Configuration", "anchor": "configuration", "heading_level": 2, "md_text": "To use [Mistral](https://mistral.ai) through their API, go to [console.mistral.ai/api-keys/](https://console.mistral.ai/api-keys/) and follow your nose until you find the place to generate an API key.\n\n`LatestMistralModelNames` contains a list of the most popular Mistral models.", "url": "https://ai.pydantic.dev/docs/models/mistral/#configuration", "page": "docs/models/mistral", "source_site": "pydantic_ai"}
{"title": "Environment variable", "anchor": "environment-variable", "heading_level": 2, "md_text": "Once you have the API key, you can set it as an environment variable:\n\n```bash\nexport MISTRAL_API_KEY='your-api-key'\n```\n\nYou can then use `MistralModel` by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('mistral:mistral-large-latest')\n...\n```\n\nOr initialise the model directly with just the model name:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.mistral import MistralModel\n\nmodel = MistralModel('mistral-small-latest')\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/mistral/#environment-variable", "page": "docs/models/mistral", "source_site": "pydantic_ai"}
{"title": "`provider` argument", "anchor": "provider-argument", "heading_level": 2, "md_text": "You can provide a custom `Provider` via the `provider` argument:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.mistral import MistralModel\nfrom pydantic_ai.providers.mistral import MistralProvider\n\nmodel = MistralModel(\n    'mistral-large-latest', provider=MistralProvider(api_key='your-api-key', base_url='https://<mistral-provider-endpoint>')\n)\nagent = Agent(model)\n...\n```\n\nYou can also customize the provider with a custom `httpx.AsyncHTTPClient`:\n\n```python\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.mistral import MistralModel\nfrom pydantic_ai.providers.mistral import MistralProvider\n\ncustom_http_client = AsyncClient(timeout=30)\nmodel = MistralModel(\n    'mistral-large-latest',\n    provider=MistralProvider(api_key='your-api-key', http_client=custom_http_client),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/mistral/#provider-argument", "page": "docs/models/mistral", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "heading_level": 2, "md_text": "To use `AnthropicModel` models, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `anthropic` optional group:\n\n```bash\npip/uv-add \"pydantic-ai-slim[anthropic]\"\n```", "url": "https://ai.pydantic.dev/docs/models/anthropic/#install", "page": "docs/models/anthropic", "source_site": "pydantic_ai"}
{"title": "Configuration", "anchor": "configuration", "heading_level": 2, "md_text": "To use [Anthropic](https://anthropic.com) through their API, go to [console.anthropic.com/settings/keys](https://console.anthropic.com/settings/keys) to generate an API key.\n\n`AnthropicModelName` contains a list of available Anthropic models.", "url": "https://ai.pydantic.dev/docs/models/anthropic/#configuration", "page": "docs/models/anthropic", "source_site": "pydantic_ai"}
{"title": "Environment variable", "anchor": "environment-variable", "heading_level": 2, "md_text": "Once you have the API key, you can set it as an environment variable:\n\n```bash\nexport ANTHROPIC_API_KEY='your-api-key'\n```\n\nYou can then use `AnthropicModel` by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('anthropic:claude-sonnet-4-5')\n...\n```\n\nOr initialise the model directly with just the model name:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel\n\nmodel = AnthropicModel('claude-sonnet-4-5')\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/anthropic/#environment-variable", "page": "docs/models/anthropic", "source_site": "pydantic_ai"}
{"title": "`provider` argument", "anchor": "provider-argument", "heading_level": 2, "md_text": "You can provide a custom `Provider` via the `provider` argument:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.providers.anthropic import AnthropicProvider\n\nmodel = AnthropicModel(\n    'claude-sonnet-4-5', provider=AnthropicProvider(api_key='your-api-key')\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/anthropic/#provider-argument", "page": "docs/models/anthropic", "source_site": "pydantic_ai"}
{"title": "Custom HTTP Client", "anchor": "custom-http-client", "heading_level": 2, "md_text": "You can customize the `AnthropicProvider` with a custom `httpx.AsyncClient`:\n\n```python\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.providers.anthropic import AnthropicProvider\n\ncustom_http_client = AsyncClient(timeout=30)\nmodel = AnthropicModel(\n    'claude-sonnet-4-5',\n    provider=AnthropicProvider(api_key='your-api-key', http_client=custom_http_client),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/anthropic/#custom-http-client", "page": "docs/models/anthropic", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "heading_level": 2, "md_text": "To use `BedrockConverseModel`, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `bedrock` optional group:\n\n```bash\npip/uv-add \"pydantic-ai-slim[bedrock]\"\n```", "url": "https://ai.pydantic.dev/docs/models/bedrock/#install", "page": "docs/models/bedrock", "source_site": "pydantic_ai"}
{"title": "Configuration", "anchor": "configuration", "heading_level": 2, "md_text": "To use [AWS Bedrock](https://aws.amazon.com/bedrock/), you'll need an AWS account with Bedrock enabled and appropriate credentials. You can use either AWS credentials directly or a pre-configured boto3 client.\n\n`BedrockModelName` contains a list of available Bedrock models, including models from Anthropic, Amazon, Cohere, Meta, and Mistral.", "url": "https://ai.pydantic.dev/docs/models/bedrock/#configuration", "page": "docs/models/bedrock", "source_site": "pydantic_ai"}
{"title": "Environment variables", "anchor": "environment-variables", "heading_level": 2, "md_text": "You can set your AWS credentials as environment variables ([among other options](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#using-environment-variables)):\n\n```bash\nexport AWS_BEARER_TOKEN_BEDROCK='your-api-key'", "url": "https://ai.pydantic.dev/docs/models/bedrock/#environment-variables", "page": "docs/models/bedrock", "source_site": "pydantic_ai"}
{"title": "or:", "anchor": "or", "heading_level": 1, "md_text": "export AWS_ACCESS_KEY_ID='your-access-key'\nexport AWS_SECRET_ACCESS_KEY='your-secret-key'\nexport AWS_DEFAULT_REGION='us-east-1'  # or your preferred region\n```\n\nYou can then use `BedrockConverseModel` by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('bedrock:anthropic.claude-3-sonnet-20240229-v1:0')\n...\n```\n\nOr initialize the model directly with just the model name:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.bedrock import BedrockConverseModel\n\nmodel = BedrockConverseModel('anthropic.claude-3-sonnet-20240229-v1:0')\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/bedrock/#or", "page": "docs/models/bedrock", "source_site": "pydantic_ai"}
{"title": "Customizing Bedrock Runtime API", "anchor": "customizing-bedrock-runtime-api", "heading_level": 2, "md_text": "You can customize the Bedrock Runtime API calls by adding additional parameters, such as [guardrail\nconfigurations](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html) and [performance settings](https://docs.aws.amazon.com/bedrock/latest/userguide/latency-optimized-inference.html). For a complete list of configurable parameters, refer to the\ndocumentation for [`BedrockModelSettings`][pydantic_ai.models.bedrock.BedrockModelSettings].\n\n```python {title=\"customize_bedrock_model_settings.py\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.bedrock import BedrockConverseModel, BedrockModelSettings", "url": "https://ai.pydantic.dev/docs/models/bedrock/#customizing-bedrock-runtime-api", "page": "docs/models/bedrock", "source_site": "pydantic_ai"}
{"title": "Define Bedrock model settings with guardrail and performance configurations", "anchor": "define-bedrock-model-settings-with-guardrail-and-performance-configurations", "heading_level": 1, "md_text": "bedrock_model_settings = BedrockModelSettings(\n    bedrock_guardrail_config={\n        'guardrailIdentifier': 'v1',\n        'guardrailVersion': 'v1',\n        'trace': 'enabled'\n    },\n    bedrock_performance_configuration={\n        'latency': 'optimized'\n    }\n)\n\n\nmodel = BedrockConverseModel(model_name='us.amazon.nova-pro-v1:0')\n\nagent = Agent(model=model, model_settings=bedrock_model_settings)\n```", "url": "https://ai.pydantic.dev/docs/models/bedrock/#define-bedrock-model-settings-with-guardrail-and-performance-configurations", "page": "docs/models/bedrock", "source_site": "pydantic_ai"}
{"title": "`provider` argument", "anchor": "provider-argument", "heading_level": 2, "md_text": "You can provide a custom `BedrockProvider` via the `provider` argument. This is useful when you want to specify credentials directly or use a custom boto3 client:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.bedrock import BedrockConverseModel\nfrom pydantic_ai.providers.bedrock import BedrockProvider", "url": "https://ai.pydantic.dev/docs/models/bedrock/#provider-argument", "page": "docs/models/bedrock", "source_site": "pydantic_ai"}
{"title": "Using AWS credentials directly", "anchor": "using-aws-credentials-directly", "heading_level": 1, "md_text": "model = BedrockConverseModel(\n    'anthropic.claude-3-sonnet-20240229-v1:0',\n    provider=BedrockProvider(\n        region_name='us-east-1',\n        aws_access_key_id='your-access-key',\n        aws_secret_access_key='your-secret-key',\n    ),\n)\nagent = Agent(model)\n...\n```\n\nYou can also pass a pre-configured boto3 client:\n\n```python\nimport boto3\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.bedrock import BedrockConverseModel\nfrom pydantic_ai.providers.bedrock import BedrockProvider", "url": "https://ai.pydantic.dev/docs/models/bedrock/#using-aws-credentials-directly", "page": "docs/models/bedrock", "source_site": "pydantic_ai"}
{"title": "Using a pre-configured boto3 client", "anchor": "using-a-pre-configured-boto3-client", "heading_level": 1, "md_text": "bedrock_client = boto3.client('bedrock-runtime', region_name='us-east-1')\nmodel = BedrockConverseModel(\n    'anthropic.claude-3-sonnet-20240229-v1:0',\n    provider=BedrockProvider(bedrock_client=bedrock_client),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/bedrock/#using-a-pre-configured-boto3-client", "page": "docs/models/bedrock", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "heading_level": 2, "md_text": "As Outlines is a library allowing you to run models from various different providers, it does not include the necessary dependencies for any provider by default. As a result, to use the [`OutlinesModel`][pydantic_ai.models.outlines.OutlinesModel], you must install `pydantic-ai-slim` with an optional group composed of outlines, a dash, and the name of the specific model provider you would use through Outlines. For instance:\n\n```bash\npip/uv-add \"pydantic-ai-slim[outlines-transformers]\"\n```\n\nOr\n\n```bash\npip/uv-add \"pydantic-ai-slim[outlines-mlxlm]\"\n```\n\nThere are 5 optional groups for the 5 model providers supported through Outlines:\n\n- `outlines-transformers`\n- `outlines-llamacpp`\n- `outlines-mlxlm`\n- `outlines-sglang`\n- `outlines-vllm-offline`", "url": "https://ai.pydantic.dev/docs/models/outlines/#install", "page": "docs/models/outlines", "source_site": "pydantic_ai"}
{"title": "Model Initialization", "anchor": "model-initialization", "heading_level": 2, "md_text": "As Outlines is not an inference provider, but instead a library allowing you to run both local and API-based models, instantiating the model is a bit different from the other models available on Pydantic AI.\n\nTo initialize the `OutlinesModel` through the `__init__` method, the first argument you must provide has to be an `outlines.Model` or an `outlines.AsyncModel` instance.\n\nFor instance:\n\n```python {test=\"skip\"}\nimport outlines\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nfrom pydantic_ai.models.outlines import OutlinesModel\n\noutlines_model = outlines.from_transformers(\n    AutoModelForCausalLM.from_pretrained('erwanf/gpt2-mini'),\n    AutoTokenizer.from_pretrained('erwanf/gpt2-mini')\n)\nmodel = OutlinesModel(outlines_model)\n```\n\nAs you already providing an Outlines model instance, there is no need to provide an `OutlinesProvider` yourself.", "url": "https://ai.pydantic.dev/docs/models/outlines/#model-initialization", "page": "docs/models/outlines", "source_site": "pydantic_ai"}
{"title": "Model Loading Methods", "anchor": "model-loading-methods", "heading_level": 3, "md_text": "Alternatively, you can use some `OutlinesModel` class methods made to load a specific type of Outlines model directly. To do so, you must provide as arguments the same arguments you would have given to the associated Outlines model loading function (except in the case of SGLang).\n\nThere are methods for the 5 Outlines models that are officially supported in the integration into Pydantic AI:\n\n- [`from_transformers`][pydantic_ai.models.outlines.OutlinesModel.from_transformers]\n- [`from_llamacpp`][pydantic_ai.models.outlines.OutlinesModel.from_llamacpp]\n- [`from_mlxlm`][pydantic_ai.models.outlines.OutlinesModel.from_mlxlm]\n- [`from_sglang`][pydantic_ai.models.outlines.OutlinesModel.from_sglang]\n- [`from_vllm_offline`][pydantic_ai.models.outlines.OutlinesModel.from_vllm_offline]", "url": "https://ai.pydantic.dev/docs/models/outlines/#model-loading-methods", "page": "docs/models/outlines", "source_site": "pydantic_ai"}
{"title": "Transformers", "anchor": "transformers", "heading_level": 4, "md_text": "```python {test=\"skip\"}\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nfrom pydantic_ai.models.outlines import OutlinesModel\n\nmodel = OutlinesModel.from_transformers(\n    AutoModelForCausalLM.from_pretrained('microsoft/Phi-3-mini-4k-instruct'),\n    AutoTokenizer.from_pretrained('microsoft/Phi-3-mini-4k-instruct')\n)\n```", "url": "https://ai.pydantic.dev/docs/models/outlines/#transformers", "page": "docs/models/outlines", "source_site": "pydantic_ai"}
{"title": "LlamaCpp", "anchor": "llamacpp", "heading_level": 4, "md_text": "```python {test=\"skip\"}\nfrom llama_cpp import Llama\n\nfrom pydantic_ai.models.outlines import OutlinesModel\n\nmodel = OutlinesModel.from_llamacpp(\n    Llama.from_pretrained(\n        repo_id='TheBloke/Mistral-7B-Instruct-v0.2-GGUF',\n        filename='mistral-7b-instruct-v0.2.Q5_K_M.gguf',\n    )\n)\n```", "url": "https://ai.pydantic.dev/docs/models/outlines/#llamacpp", "page": "docs/models/outlines", "source_site": "pydantic_ai"}
{"title": "MLXLM", "anchor": "mlxlm", "heading_level": 4, "md_text": "```python {test=\"skip\"}\nfrom mlx_lm import load\n\nfrom pydantic_ai.models.outlines import OutlinesModel\n\nmodel = OutlinesModel.from_mlxlm(\n    *load('mlx-community/TinyLlama-1.1B-Chat-v1.0-4bit')\n)\n```", "url": "https://ai.pydantic.dev/docs/models/outlines/#mlxlm", "page": "docs/models/outlines", "source_site": "pydantic_ai"}
{"title": "SGLang", "anchor": "sglang", "heading_level": 4, "md_text": "```python {test=\"skip\"}\nfrom pydantic_ai.models.outlines import OutlinesModel\n\nmodel = OutlinesModel.from_sglang(\n    'http://localhost:11434',\n    'api_key',\n    'meta-llama/Llama-3.1-8B'\n)\n```", "url": "https://ai.pydantic.dev/docs/models/outlines/#sglang", "page": "docs/models/outlines", "source_site": "pydantic_ai"}
{"title": "vLLM Offline", "anchor": "vllm-offline", "heading_level": 4, "md_text": "```python {test=\"skip\"}\nfrom vllm import LLM\n\nfrom pydantic_ai.models.outlines import OutlinesModel\n\nmodel = OutlinesModel.from_vllm_offline(\n    LLM('microsoft/Phi-3-mini-4k-instruct')\n)\n```", "url": "https://ai.pydantic.dev/docs/models/outlines/#vllm-offline", "page": "docs/models/outlines", "source_site": "pydantic_ai"}
{"title": "Running the model", "anchor": "running-the-model", "heading_level": 2, "md_text": "Once you have initialized an `OutlinesModel`, you can use it with an Agent as with all other Pydantic AI models.\n\nAs Outlines is focused on structured output, this provider supports the `output_type` component through the [`NativeOutput`][pydantic_ai.output.NativeOutput] format. There is not need to include information on the required output format in your prompt, instructions based on the `output_type` will be included automatically.\n\n```python {test=\"skip\"}\nfrom pydantic import BaseModel\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.outlines import OutlinesModel\nfrom pydantic_ai.settings import ModelSettings\n\n\nclass Box(BaseModel):\n    \"\"\"Class representing a box\"\"\"\n    width: int\n    height: int\n    depth: int\n    units: str\n\nmodel = OutlinesModel.from_transformers(\n    AutoModelForCausalLM.from_pretrained('microsoft/Phi-3-mini-4k-instruct'),\n    AutoTokenizer.from_pretrained('microsoft/Phi-3-mini-4k-instruct')\n)\nagent = Agent(model, output_type=Box)\n\nresult = agent.run_sync(\n    'Give me the dimensions of a box',\n    model_settings=ModelSettings(extra_body={'max_new_tokens': 100})\n)\nprint(result.output) # width=20 height=30 depth=40 units='cm'\n```\n\nOutlines does not support tools yet, but support for that feature will be added in the near future.", "url": "https://ai.pydantic.dev/docs/models/outlines/#running-the-model", "page": "docs/models/outlines", "source_site": "pydantic_ai"}
{"title": "Multimodal models", "anchor": "multimodal-models", "heading_level": 2, "md_text": "If the model you are running through Outlines and the provider selected supports it, you can include images in your prompts using [`ImageUrl`][pydantic_ai.messages.ImageUrl] or [`BinaryImage`][pydantic_ai.messages.BinaryImage]. In that case, the prompt you provide when running the agent should be a list containing a string and one or several images. See the [input documentation](../input.md) for details and examples on using assets in model inputs.\n\nThis feature is supported in Outlines for the `SGLang` and `Transformers` models. If you want to run a multimodal model through `transformers`, you must provide a processor instead of a tokenizer as the second argument when initializing the model with the `OutlinesModel.from_transformers` method.\n\n```python {test=\"skip\"}\nfrom datetime import date\nfrom typing import Literal\n\nimport torch\nfrom pydantic import BaseModel\nfrom transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n\nfrom pydantic_ai import Agent, ModelSettings\nfrom pydantic_ai.messages import ImageUrl\nfrom pydantic_ai.models.outlines import OutlinesModel\n\nMODEL_NAME = 'Qwen/Qwen2-VL-7B-Instruct'\n\nclass Item(BaseModel):\n    name: str\n    quantity: int | None\n    price_per_unit: float | None\n    total_price: float | None\n\nclass ReceiptSummary(BaseModel):\n    store_name: str\n    store_address: str\n    store_number: int | None\n    items: list[Item]\n    tax: float | None\n    total: float | None\n    date: date\n    payment_method: Literal['cash', 'credit', 'debit', 'check', 'other']\n\ntf_model = Qwen2VLForConditionalGeneration.from_pretrained(\n    MODEL_NAME,\n    device_map='auto',\n    dtype=torch.bfloat16\n)\ntf_processor = AutoProcessor.from_pretrained(\n    MODEL_NAME,\n    device_map='auto'\n)\nmodel = OutlinesModel.from_transformers(tf_model, tf_processor)\n\nagent = Agent(model, output_type=ReceiptSummary)\n\nresult = agent.run_sync(\n    [\n        'You are an expert at extracting information from receipts. Please extract the information from the receipt. Be as detailed as possible, do not miss any information',\n        ImageUrl('https://raw.githubusercontent.com/dottxt-ai/outlines/refs/heads/main/docs/examples/images/trader-joes-receipt.jpg')\n    ],\n    model_settings=ModelSettings(extra_body={'max_new_tokens': 1000})\n)\nprint(result.output)", "url": "https://ai.pydantic.dev/docs/models/outlines/#multimodal-models", "page": "docs/models/outlines", "source_site": "pydantic_ai"}
{"title": "payment_method='credit'", "anchor": "payment_methodcredit", "heading_level": 1, "md_text": "```", "url": "https://ai.pydantic.dev/docs/models/outlines/#payment_methodcredit", "page": "docs/models/outlines", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "heading_level": 2, "md_text": "To use `CohereModel`, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `cohere` optional group:\n\n```bash\npip/uv-add \"pydantic-ai-slim[cohere]\"\n```", "url": "https://ai.pydantic.dev/docs/models/cohere/#install", "page": "docs/models/cohere", "source_site": "pydantic_ai"}
{"title": "Configuration", "anchor": "configuration", "heading_level": 2, "md_text": "To use [Cohere](https://cohere.com/) through their API, go to [dashboard.cohere.com/api-keys](https://dashboard.cohere.com/api-keys) and follow your nose until you find the place to generate an API key.\n\n`CohereModelName` contains a list of the most popular Cohere models.", "url": "https://ai.pydantic.dev/docs/models/cohere/#configuration", "page": "docs/models/cohere", "source_site": "pydantic_ai"}
{"title": "Environment variable", "anchor": "environment-variable", "heading_level": 2, "md_text": "Once you have the API key, you can set it as an environment variable:\n\n```bash\nexport CO_API_KEY='your-api-key'\n```\n\nYou can then use `CohereModel` by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('cohere:command-r7b-12-2024')\n...\n```\n\nOr initialise the model directly with just the model name:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.cohere import CohereModel\n\nmodel = CohereModel('command-r7b-12-2024')\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/cohere/#environment-variable", "page": "docs/models/cohere", "source_site": "pydantic_ai"}
{"title": "`provider` argument", "anchor": "provider-argument", "heading_level": 2, "md_text": "You can provide a custom `Provider` via the `provider` argument:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.cohere import CohereModel\nfrom pydantic_ai.providers.cohere import CohereProvider\n\nmodel = CohereModel('command-r7b-12-2024', provider=CohereProvider(api_key='your-api-key'))\nagent = Agent(model)\n...\n```\n\nYou can also customize the `CohereProvider` with a custom `http_client`:\n\n```python\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.cohere import CohereModel\nfrom pydantic_ai.providers.cohere import CohereProvider\n\ncustom_http_client = AsyncClient(timeout=30)\nmodel = CohereModel(\n    'command-r7b-12-2024',\n    provider=CohereProvider(api_key='your-api-key', http_client=custom_http_client),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/cohere/#provider-argument", "page": "docs/models/cohere", "source_site": "pydantic_ai"}
{"title": "Model Providers", "anchor": "model-providers", "heading_level": 1, "md_text": "Pydantic AI is model-agnostic and has built-in support for multiple model providers:\n\n* [OpenAI](openai.md)\n* [Anthropic](anthropic.md)\n* [Gemini](google.md) (via two different APIs: Generative Language API and VertexAI API)\n* [Groq](groq.md)\n* [Mistral](mistral.md)\n* [Cohere](cohere.md)\n* [Bedrock](bedrock.md)\n* [Hugging Face](huggingface.md)\n* [Outlines](outlines.md)", "url": "https://ai.pydantic.dev/docs/models/overview/#model-providers", "page": "docs/models/overview", "source_site": "pydantic_ai"}
{"title": "OpenAI-compatible Providers", "anchor": "openai-compatible-providers", "heading_level": 2, "md_text": "In addition, many providers are compatible with the OpenAI API, and can be used with `OpenAIChatModel` in Pydantic AI:\n\n- [DeepSeek](openai.md#deepseek)\n- [Grok (xAI)](openai.md#grok-xai)\n- [Ollama](openai.md#ollama)\n- [OpenRouter](openai.md#openrouter)\n- [Vercel AI Gateway](openai.md#vercel-ai-gateway)\n- [Perplexity](openai.md#perplexity)\n- [Fireworks AI](openai.md#fireworks-ai)\n- [Together AI](openai.md#together-ai)\n- [Azure AI Foundry](openai.md#azure-ai-foundry)\n- [Heroku](openai.md#heroku-ai)\n- [GitHub Models](openai.md#github-models)\n- [Cerebras](openai.md#cerebras)\n- [LiteLLM](openai.md#litellm)\n- [Nebius AI Studio](openai.md#nebius-ai-studio)\n- [OVHcloud AI Endpoints](openai.md#ovhcloud-ai-endpoints)\n\nPydantic AI also comes with [`TestModel`](../api/models/test.md) and [`FunctionModel`](../api/models/function.md)\nfor testing and development.\n\nTo use each model provider, you need to configure your local environment and make sure you have the right\npackages installed. If you try to use the model without having done so, you'll be told what to install.", "url": "https://ai.pydantic.dev/docs/models/overview/#openai-compatible-providers", "page": "docs/models/overview", "source_site": "pydantic_ai"}
{"title": "Models and Providers", "anchor": "models-and-providers", "heading_level": 2, "md_text": "Pydantic AI uses a few key terms to describe how it interacts with different LLMs:\n\n- **Model**: This refers to the Pydantic AI class used to make requests following a specific LLM API\n  (generally by wrapping a vendor-provided SDK, like the `openai` python SDK). These classes implement a\n  vendor-SDK-agnostic API, ensuring a single Pydantic AI agent is portable to different LLM vendors without\n  any other code changes just by swapping out the Model it uses. Model classes are named\n  roughly in the format `<VendorSdk>Model`, for example, we have `OpenAIChatModel`, `AnthropicModel`, `GoogleModel`,\n  etc. When using a Model class, you specify the actual LLM model name (e.g., `gpt-5`,\n  `claude-sonnet-4-5`, `gemini-2.5-flash`) as a parameter.\n- **Provider**: This refers to provider-specific classes which handle the authentication and connections\n  to an LLM vendor. Passing a non-default _Provider_ as a parameter to a Model is how you can ensure\n  that your agent will make requests to a specific endpoint, or make use of a specific approach to\n  authentication (e.g., you can use Azure auth with the `OpenAIChatModel` by way of the `AzureProvider`).\n  In particular, this is how you can make use of an AI gateway, or an LLM vendor that offers API compatibility\n  with the vendor SDK used by an existing Model (such as `OpenAIChatModel`).\n- **Profile**: This refers to a description of how requests to a specific model or family of models need to be\n  constructed to get the best results, independent of the model and provider classes used.\n  For example, different models have different restrictions on the JSON schemas that can be used for tools,\n  and the same schema transformer needs to be used for Gemini models whether you're using `GoogleModel`\n  with model name `gemini-2.5-pro-preview`, or `OpenAIChatModel` with `OpenRouterProvider` and model name `google/gemini-2.5-pro-preview`.\n\nWhen you instantiate an [`Agent`][pydantic_ai.Agent] with just a name formatted as `<provider>:<model>`, e.g. `openai:gpt-5` or `openrouter:google/gemini-2.5-pro-preview`,\nPydantic AI will automatically select the appropriate model class, provider, and profile.\nIf you want to use a different provider or profile, you can instantiate a model class directly and pass in `provider` and/or `profile` arguments.", "url": "https://ai.pydantic.dev/docs/models/overview/#models-and-providers", "page": "docs/models/overview", "source_site": "pydantic_ai"}
{"title": "Custom Models", "anchor": "custom-models", "heading_level": 2, "md_text": "!!! note\n    If a model API is compatible with the OpenAI API, you do not need a custom model class and can provide your own [custom provider](openai.md#openai-compatible-models) instead.\n\nTo implement support for a model API that's not already supported, you will need to subclass the [`Model`][pydantic_ai.models.Model] abstract base class.\nFor streaming, you'll also need to implement the [`StreamedResponse`][pydantic_ai.models.StreamedResponse] abstract base class.\n\nThe best place to start is to review the source code for existing implementations, e.g. [`OpenAIChatModel`](https://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_slim/pydantic_ai/models/openai.py).\n\nFor details on when we'll accept contributions adding new models to Pydantic AI, see the [contributing guidelines](../contributing.md#new-model-rules).\n\n\n<!-- TODO(Marcelo): We need to create a section in the docs about reliability. -->", "url": "https://ai.pydantic.dev/docs/models/overview/#custom-models", "page": "docs/models/overview", "source_site": "pydantic_ai"}
{"title": "Fallback Model", "anchor": "fallback-model", "heading_level": 2, "md_text": "You can use [`FallbackModel`][pydantic_ai.models.fallback.FallbackModel] to attempt multiple models\nin sequence until one successfully returns a result. Under the hood, Pydantic AI automatically switches\nfrom one model to the next if the current model returns a 4xx or 5xx status code.\n\nIn the following example, the agent first makes a request to the OpenAI model (which fails due to an invalid API key),\nand then falls back to the Anthropic model.\n\n<!-- TODO(Marcelo): Do not skip this test. For some reason it becomes a flaky test if we don't skip it. -->\n\n```python {title=\"fallback_model.py\" test=\"skip\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.models.fallback import FallbackModel\nfrom pydantic_ai.models.openai import OpenAIChatModel\n\nopenai_model = OpenAIChatModel('gpt-5')\nanthropic_model = AnthropicModel('claude-sonnet-4-5')\nfallback_model = FallbackModel(openai_model, anthropic_model)\n\nagent = Agent(fallback_model)\nresponse = agent.run_sync('What is the capital of France?')\nprint(response.data)\n#> Paris\n\nprint(response.all_messages())\n\"\"\"\n[\n    ModelRequest(\n        parts=[\n            UserPromptPart(\n                content='What is the capital of France?',\n                timestamp=datetime.datetime(...),\n                part_kind='user-prompt',\n            )\n        ],\n        kind='request',\n    ),\n    ModelResponse(\n        parts=[TextPart(content='Paris', part_kind='text')],\n        model_name='claude-sonnet-4-5',\n        timestamp=datetime.datetime(...),\n        kind='response',\n        provider_response_id=None,\n    ),\n]\n\"\"\"\n```\n\nThe `ModelResponse` message above indicates in the `model_name` field that the output was returned by the Anthropic model, which is the second model specified in the `FallbackModel`.\n\n!!! note\n    Each model's options should be configured individually. For example, `base_url`, `api_key`, and custom clients should be set on each model itself, not on the `FallbackModel`.", "url": "https://ai.pydantic.dev/docs/models/overview/#fallback-model", "page": "docs/models/overview", "source_site": "pydantic_ai"}
{"title": "Per-Model Settings", "anchor": "per-model-settings", "heading_level": 3, "md_text": "You can configure different [`ModelSettings`][pydantic_ai.settings.ModelSettings] for each model in a fallback chain by passing the `settings` parameter when creating each model. This is particularly useful when different providers have different optimal configurations:\n\n```python {title=\"fallback_model_per_settings.py\"}\nfrom pydantic_ai import Agent, ModelSettings\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.models.fallback import FallbackModel\nfrom pydantic_ai.models.openai import OpenAIChatModel", "url": "https://ai.pydantic.dev/docs/models/overview/#per-model-settings", "page": "docs/models/overview", "source_site": "pydantic_ai"}
{"title": "Configure each model with provider-specific optimal settings", "anchor": "configure-each-model-with-provider-specific-optimal-settings", "heading_level": 1, "md_text": "openai_model = OpenAIChatModel(\n    'gpt-5',\n    settings=ModelSettings(temperature=0.7, max_tokens=1000)  # Higher creativity for OpenAI\n)\nanthropic_model = AnthropicModel(\n    'claude-sonnet-4-5',\n    settings=ModelSettings(temperature=0.2, max_tokens=1000)  # Lower temperature for consistency\n)\n\nfallback_model = FallbackModel(openai_model, anthropic_model)\nagent = Agent(fallback_model)\n\nresult = agent.run_sync('Write a creative story about space exploration')\nprint(result.output)\n\"\"\"\nIn the year 2157, Captain Maya Chen piloted her spacecraft through the vast expanse of the Andromeda Galaxy. As she discovered a planet with crystalline mountains that sang in harmony with the cosmic winds, she realized that space exploration was not just about finding new worlds, but about finding new ways to understand the universe and our place within it.\n\"\"\"\n```\n\nIn this example, if the OpenAI model fails, the agent will automatically fall back to the Anthropic model with its own configured settings. The `FallbackModel` itself doesn't have settings - it uses the individual settings of whichever model successfully handles the request.\n\nIn this next example, we demonstrate the exception-handling capabilities of `FallbackModel`.\nIf all models fail, a [`FallbackExceptionGroup`][pydantic_ai.exceptions.FallbackExceptionGroup] is raised, which\ncontains all the exceptions encountered during the `run` execution.\n\n=== \"Python >=3.11\"\n\n    ```python {title=\"fallback_model_failure.py\" py=\"3.11\"}\n    from pydantic_ai import Agent, ModelHTTPError\n    from pydantic_ai.models.anthropic import AnthropicModel\n    from pydantic_ai.models.fallback import FallbackModel\n    from pydantic_ai.models.openai import OpenAIChatModel\n\n    openai_model = OpenAIChatModel('gpt-5')\n    anthropic_model = AnthropicModel('claude-sonnet-4-5')\n    fallback_model = FallbackModel(openai_model, anthropic_model)\n\n    agent = Agent(fallback_model)\n    try:\n        response = agent.run_sync('What is the capital of France?')\n    except* ModelHTTPError as exc_group:\n        for exc in exc_group.exceptions:\n            print(exc)\n    ```\n\n=== \"Python <3.11\"\n\n    Since [`except*`](https://docs.python.org/3/reference/compound_stmts.html#except-star) is only supported\n    in Python 3.11+, we use the [`exceptiongroup`](https://github.com/agronholm/exceptiongroup) backport\n    package for earlier Python versions:\n\n    ```python {title=\"fallback_model_failure.py\" noqa=\"F821\" test=\"skip\"}\n    from exceptiongroup import catch\n\n    from pydantic_ai import Agent, ModelHTTPError\n    from pydantic_ai.models.anthropic import AnthropicModel\n    from pydantic_ai.models.fallback import FallbackModel\n    from pydantic_ai.models.openai import OpenAIChatModel\n\n\n    def model_status_error_handler(exc_group: BaseExceptionGroup) -> None:\n        for exc in exc_group.exceptions:\n            print(exc)\n\n\n    openai_model = OpenAIChatModel('gpt-5')\n    anthropic_model = AnthropicModel('claude-sonnet-4-5')\n    fallback_model = FallbackModel(openai_model, anthropic_model)\n\n    agent = Agent(fallback_model)\n    with catch({ModelHTTPError: model_status_error_handler}):\n        response = agent.run_sync('What is the capital of France?')\n    ```\n\nBy default, the `FallbackModel` only moves on to the next model if the current model raises a\n[`ModelHTTPError`][pydantic_ai.exceptions.ModelHTTPError]. You can customize this behavior by\npassing a custom `fallback_on` argument to the `FallbackModel` constructor.", "url": "https://ai.pydantic.dev/docs/models/overview/#configure-each-model-with-provider-specific-optimal-settings", "page": "docs/models/overview", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "heading_level": 2, "md_text": "To use OpenAI models or OpenAI-compatible APIs, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `openai` optional group:\n\n```bash\npip/uv-add \"pydantic-ai-slim[openai]\"\n```", "url": "https://ai.pydantic.dev/docs/models/openai/#install", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "Configuration", "anchor": "configuration", "heading_level": 2, "md_text": "To use `OpenAIChatModel` with the OpenAI API, go to [platform.openai.com](https://platform.openai.com/) and follow your nose until you find the place to generate an API key.", "url": "https://ai.pydantic.dev/docs/models/openai/#configuration", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "Environment variable", "anchor": "environment-variable", "heading_level": 2, "md_text": "Once you have the API key, you can set it as an environment variable:\n\n```bash\nexport OPENAI_API_KEY='your-api-key'\n```\n\nYou can then use `OpenAIChatModel` by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-5')\n...\n```\n\nOr initialise the model directly with just the model name:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\n\nmodel = OpenAIChatModel('gpt-5')\nagent = Agent(model)\n...\n```\n\nBy default, the `OpenAIChatModel` uses the `OpenAIProvider` with the `base_url` set to `https://api.openai.com/v1`.", "url": "https://ai.pydantic.dev/docs/models/openai/#environment-variable", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "Configure the provider", "anchor": "configure-the-provider", "heading_level": 2, "md_text": "If you want to pass parameters in code to the provider, you can programmatically instantiate the\n[OpenAIProvider][pydantic_ai.providers.openai.OpenAIProvider] and pass it to the model:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nmodel = OpenAIChatModel('gpt-5', provider=OpenAIProvider(api_key='your-api-key'))\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/openai/#configure-the-provider", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "Custom OpenAI Client", "anchor": "custom-openai-client", "heading_level": 2, "md_text": "`OpenAIProvider` also accepts a custom `AsyncOpenAI` client via the `openai_client` parameter, so you can customise the `organization`, `project`, `base_url` etc. as defined in the [OpenAI API docs](https://platform.openai.com/docs/api-reference).\n\n```python {title=\"custom_openai_client.py\"}\nfrom openai import AsyncOpenAI\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nclient = AsyncOpenAI(max_retries=3)\nmodel = OpenAIChatModel('gpt-5', provider=OpenAIProvider(openai_client=client))\nagent = Agent(model)\n...\n```\n\nYou could also use the [`AsyncAzureOpenAI`](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/switching-endpoints) client\nto use the Azure OpenAI API. Note that the `AsyncAzureOpenAI` is a subclass of `AsyncOpenAI`.\n\n```python\nfrom openai import AsyncAzureOpenAI\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nclient = AsyncAzureOpenAI(\n    azure_endpoint='...',\n    api_version='2024-07-01-preview',\n    api_key='your-api-key',\n)\n\nmodel = OpenAIChatModel(\n    'gpt-5',\n    provider=OpenAIProvider(openai_client=client),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/openai/#custom-openai-client", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAI Responses API", "anchor": "openai-responses-api", "heading_level": 2, "md_text": "Pydantic AI also supports OpenAI's [Responses API](https://platform.openai.com/docs/api-reference/responses) through the\n\nYou can use [`OpenAIResponsesModel`][pydantic_ai.models.openai.OpenAIResponsesModel] by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai-responses:gpt-5')\n...\n```\n\nOr initialise the model directly with just the model name:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIResponsesModel\n\nmodel = OpenAIResponsesModel('gpt-5')\nagent = Agent(model)\n...\n```\n\nYou can learn more about the differences between the Responses API and Chat Completions API in the [OpenAI API docs](https://platform.openai.com/docs/guides/migrate-to-responses).", "url": "https://ai.pydantic.dev/docs/models/openai/#openai-responses-api", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "Built-in tools", "anchor": "built-in-tools", "heading_level": 3, "md_text": "The Responses API has built-in tools that you can use instead of building your own:\n\n- [Web search](https://platform.openai.com/docs/guides/tools-web-search): allow models to search the web for the latest information before generating a response.\n- [Code interpreter](https://platform.openai.com/docs/guides/tools-code-interpreter): allow models to write and run Python code in a sandboxed environment before generating a response.\n- [Image generation](https://platform.openai.com/docs/guides/tools-image-generation): allow models to generate images based on a text prompt.\n- [File search](https://platform.openai.com/docs/guides/tools-file-search): allow models to search your files for relevant information before generating a response.\n- [Computer use](https://platform.openai.com/docs/guides/tools-computer-use): allow models to use a computer to perform tasks on your behalf.\n\nWeb search, Code interpreter, and Image generation are natively supported through the [Built-in tools](../builtin-tools.md) feature.\n\nFile search and Computer use can be enabled by passing an [`openai.types.responses.FileSearchToolParam`](https://github.com/openai/openai-python/blob/main/src/openai/types/responses/file_search_tool_param.py) or [`openai.types.responses.ComputerToolParam`](https://github.com/openai/openai-python/blob/main/src/openai/types/responses/computer_tool_param.py) in the `openai_builtin_tools` setting on [`OpenAIResponsesModelSettings`][pydantic_ai.models.openai.OpenAIResponsesModelSettings]. They don't currently generate [`BuiltinToolCallPart`][pydantic_ai.messages.BuiltinToolCallPart] or [`BuiltinToolReturnPart`][pydantic_ai.messages.BuiltinToolReturnPart] parts in the message history, or streamed events; please submit an issue if you need native support for these built-in tools.\n\n```python {title=\"file_search_tool.py\"}\nfrom openai.types.responses import FileSearchToolParam\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIResponsesModel, OpenAIResponsesModelSettings\n\nmodel_settings = OpenAIResponsesModelSettings(\n    openai_builtin_tools=[\n        FileSearchToolParam(\n            type='file_search',\n            vector_store_ids=['your-history-book-vector-store-id']\n        )\n    ],\n)\nmodel = OpenAIResponsesModel('gpt-5')\nagent = Agent(model=model, model_settings=model_settings)\n\nresult = agent.run_sync('Who was Albert Einstein?')\nprint(result.output)\n#> Albert Einstein was a German-born theoretical physicist.\n```", "url": "https://ai.pydantic.dev/docs/models/openai/#built-in-tools", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "Referencing earlier responses", "anchor": "referencing-earlier-responses", "heading_level": 4, "md_text": "The Responses API supports referencing earlier model responses in a new request using a `previous_response_id` parameter, to ensure the full [conversation state](https://platform.openai.com/docs/guides/conversation-state?api-mode=responses#passing-context-from-the-previous-response) including [reasoning items](https://platform.openai.com/docs/guides/reasoning#keeping-reasoning-items-in-context) are kept in context. This is available through the `openai_previous_response_id` field in\n[`OpenAIResponsesModelSettings`][pydantic_ai.models.openai.OpenAIResponsesModelSettings].\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIResponsesModel, OpenAIResponsesModelSettings\n\nmodel = OpenAIResponsesModel('gpt-5')\nagent = Agent(model=model)\n\nresult = agent.run_sync('The secret is 1234')\nmodel_settings = OpenAIResponsesModelSettings(\n    openai_previous_response_id=result.all_messages()[-1].provider_response_id\n)\nresult = agent.run_sync('What is the secret code?', model_settings=model_settings)\nprint(result.output)\n#> 1234\n```\n\nBy passing the `provider_response_id` from an earlier run, you can allow the model to build on its own prior reasoning without needing to resend the full message history.", "url": "https://ai.pydantic.dev/docs/models/openai/#referencing-earlier-responses", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "Automatically referencing earlier responses", "anchor": "automatically-referencing-earlier-responses", "heading_level": 5, "md_text": "When the `openai_previous_response_id` field is set to `'auto'`, Pydantic AI will automatically select the most recent `provider_response_id` from message history and omit messages that came before it, letting the OpenAI API leverage server-side history instead for improved efficiency.\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIResponsesModel, OpenAIResponsesModelSettings\n\nmodel = OpenAIResponsesModel('gpt-5')\nagent = Agent(model=model)\n\nresult1 = agent.run_sync('Tell me a joke.')\nprint(result1.output)\n#> Did you hear about the toothpaste scandal? They called it Colgate.", "url": "https://ai.pydantic.dev/docs/models/openai/#automatically-referencing-earlier-responses", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "and messages after it are sent as request.", "anchor": "and-messages-after-it-are-sent-as-request", "heading_level": 1, "md_text": "model_settings = OpenAIResponsesModelSettings(openai_previous_response_id='auto')\nresult2 = agent.run_sync(\n    'Explain?',\n    message_history=result1.new_messages(),\n    model_settings=model_settings\n)\nprint(result2.output)\n#> This is an excellent joke invented by Samuel Colvin, it needs no explanation.\n```", "url": "https://ai.pydantic.dev/docs/models/openai/#and-messages-after-it-are-sent-as-request", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAI-compatible Models", "anchor": "openai-compatible-models", "heading_level": 2, "md_text": "Many providers and models are compatible with the OpenAI API, and can be used with `OpenAIChatModel` in Pydantic AI.\nBefore getting started, check the [installation and configuration](#install) instructions above.\n\nTo use another OpenAI-compatible API, you can make use of the `base_url` and `api_key` arguments from `OpenAIProvider`:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nmodel = OpenAIChatModel(\n    'model_name',\n    provider=OpenAIProvider(\n        base_url='https://<openai-compatible-api-endpoint>', api_key='your-api-key'\n    ),\n)\nagent = Agent(model)\n...\n```\n\nVarious providers also have their own provider classes so that you don't need to specify the base URL yourself and you can use the standard `<PROVIDER>_API_KEY` environment variable to set the API key.\nWhen a provider has its own provider class, you can use the `Agent(\"<provider>:<model>\")` shorthand, e.g. `Agent(\"deepseek:deepseek-chat\")` or `Agent(\"openrouter:google/gemini-2.5-pro-preview\")`, instead of building the `OpenAIChatModel` explicitly. Similarly, you can pass the provider name as a string to the `provider` argument on `OpenAIChatModel` instead of building instantiating the provider class explicitly.", "url": "https://ai.pydantic.dev/docs/models/openai/#openai-compatible-models", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "Model Profile", "anchor": "model-profile", "heading_level": 4, "md_text": "Sometimes, the provider or model you're using will have slightly different requirements than OpenAI's API or models, like having different restrictions on JSON schemas for tool definitions, or not supporting tool definitions to be marked as strict.\n\nWhen using an alternative provider class provided by Pydantic AI, an appropriate model profile is typically selected automatically based on the model name.\nIf the model you're using is not working correctly out of the box, you can tweak various aspects of how model requests are constructed by providing your own [`ModelProfile`][pydantic_ai.profiles.ModelProfile] (for behaviors shared among all model classes) or [`OpenAIModelProfile`][pydantic_ai.profiles.openai.OpenAIModelProfile] (for behaviors specific to `OpenAIChatModel`):\n\n```py\nfrom pydantic_ai import Agent, InlineDefsJsonSchemaTransformer\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.profiles.openai import OpenAIModelProfile\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nmodel = OpenAIChatModel(\n    'model_name',\n    provider=OpenAIProvider(\n        base_url='https://<openai-compatible-api-endpoint>.com', api_key='your-api-key'\n    ),\n    profile=OpenAIModelProfile(\n        json_schema_transformer=InlineDefsJsonSchemaTransformer,  # Supported by any model class on a plain ModelProfile\n        openai_supports_strict_tool_definition=False  # Supported by OpenAIModel only, requires OpenAIModelProfile\n    )\n)\nagent = Agent(model)\n```", "url": "https://ai.pydantic.dev/docs/models/openai/#model-profile", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "DeepSeek", "anchor": "deepseek", "heading_level": 3, "md_text": "To use the [DeepSeek](https://deepseek.com) provider, first create an API key by following the [Quick Start guide](https://api-docs.deepseek.com/).\n\nYou can then set the `DEEPSEEK_API_KEY` environment variable and use [`DeepSeekProvider`][pydantic_ai.providers.deepseek.DeepSeekProvider] by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('deepseek:deepseek-chat')\n...\n```\n\nOr initialise the model and provider directly:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.deepseek import DeepSeekProvider\n\nmodel = OpenAIChatModel(\n    'deepseek-chat',\n    provider=DeepSeekProvider(api_key='your-deepseek-api-key'),\n)\nagent = Agent(model)\n...\n```\n\nYou can also customize any provider with a custom `http_client`:\n\n```python\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.deepseek import DeepSeekProvider\n\ncustom_http_client = AsyncClient(timeout=30)\nmodel = OpenAIChatModel(\n    'deepseek-chat',\n    provider=DeepSeekProvider(\n        api_key='your-deepseek-api-key', http_client=custom_http_client\n    ),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/openai/#deepseek", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "Ollama", "anchor": "ollama", "heading_level": 3, "md_text": "Pydantic AI supports both self-hosted [Ollama](https://ollama.com/) servers (running locally or remotely) and [Ollama Cloud](https://ollama.com/cloud).\n\nFor servers running locally, use the `http://localhost:11434/v1` base URL. For Ollama Cloud, use `https://ollama.com/v1` and ensure an API key is set.\n\nYou can set the `OLLAMA_BASE_URL` and (optionally) `OLLAMA_API_KEY` environment variables and use [`OllamaProvider`][pydantic_ai.providers.ollama.OllamaProvider] by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('ollama:gpt-oss:20b')\n...\n```\n\nOr initialise the model and provider directly:\n\n```python\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.ollama import OllamaProvider\n\n\nclass CityLocation(BaseModel):\n    city: str\n    country: str\n\n\nollama_model = OpenAIChatModel(\n    model_name='gpt-oss:20b',\n    provider=OllamaProvider(base_url='http://localhost:11434/v1'),  # (1)!\n)\nagent = Agent(ollama_model, output_type=CityLocation)\n\nresult = agent.run_sync('Where were the olympics held in 2012?')\nprint(result.output)\n#> city='London' country='United Kingdom'\nprint(result.usage())\n#> RunUsage(input_tokens=57, output_tokens=8, requests=1)\n```\n\n1. For Ollama Cloud, use the `base_url='https://ollama.com/v1'` and set the `OLLAMA_API_KEY` environment variable.", "url": "https://ai.pydantic.dev/docs/models/openai/#ollama", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "Azure AI Foundry", "anchor": "azure-ai-foundry", "heading_level": 3, "md_text": "To use [Azure AI Foundry](https://ai.azure.com/) as your provider, you can set the `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_KEY`, and `OPENAI_API_VERSION` environment variables and use [`AzureProvider`][pydantic_ai.providers.azure.AzureProvider] by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('azure:gpt-5')\n...\n```\n\nOr initialise the model and provider directly:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.azure import AzureProvider\n\nmodel = OpenAIChatModel(\n    'gpt-5',\n    provider=AzureProvider(\n        azure_endpoint='your-azure-endpoint',\n        api_version='your-api-version',\n        api_key='your-api-key',\n    ),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/openai/#azure-ai-foundry", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenRouter", "anchor": "openrouter", "heading_level": 3, "md_text": "To use [OpenRouter](https://openrouter.ai), first create an API key at [openrouter.ai/keys](https://openrouter.ai/keys).\n\nYou can set the `OPENROUTER_API_KEY` environment variable and use [`OpenRouterProvider`][pydantic_ai.providers.openrouter.OpenRouterProvider] by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openrouter:anthropic/claude-3.5-sonnet')\n...\n```\n\nOr initialise the model and provider directly:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openrouter import OpenRouterProvider\n\nmodel = OpenAIChatModel(\n    'anthropic/claude-3.5-sonnet',\n    provider=OpenRouterProvider(api_key='your-openrouter-api-key'),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/openai/#openrouter", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "Vercel AI Gateway", "anchor": "vercel-ai-gateway", "heading_level": 3, "md_text": "To use [Vercel's AI Gateway](https://vercel.com/docs/ai-gateway), first follow the [documentation](https://vercel.com/docs/ai-gateway) instructions on obtaining an API key or OIDC token.\n\nYou can set the `VERCEL_AI_GATEWAY_API_KEY` and `VERCEL_OIDC_TOKEN` environment variables and use [`VercelProvider`][pydantic_ai.providers.vercel.VercelProvider] by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('vercel:anthropic/claude-4-sonnet')\n...\n```\n\nOr initialise the model and provider directly:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.vercel import VercelProvider\n\nmodel = OpenAIChatModel(\n    'anthropic/claude-4-sonnet',\n    provider=VercelProvider(api_key='your-vercel-ai-gateway-api-key'),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/openai/#vercel-ai-gateway", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "Grok (xAI)", "anchor": "grok-xai", "heading_level": 3, "md_text": "Go to [xAI API Console](https://console.x.ai/) and create an API key.\n\nYou can set the `GROK_API_KEY` environment variable and use [`GrokProvider`][pydantic_ai.providers.grok.GrokProvider] by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('grok:grok-2-1212')\n...\n```\n\nOr initialise the model and provider directly:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.grok import GrokProvider\n\nmodel = OpenAIChatModel(\n    'grok-2-1212',\n    provider=GrokProvider(api_key='your-xai-api-key'),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/openai/#grok-xai", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "MoonshotAI", "anchor": "moonshotai", "heading_level": 3, "md_text": "Create an API key in the [Moonshot Console](https://platform.moonshot.ai/console).\n\nYou can set the `MOONSHOTAI_API_KEY` environment variable and use [`MoonshotAIProvider`][pydantic_ai.providers.moonshotai.MoonshotAIProvider] by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('moonshotai:kimi-k2-0711-preview')\n...\n```\n\nOr initialise the model and provider directly:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.moonshotai import MoonshotAIProvider\n\nmodel = OpenAIChatModel(\n    'kimi-k2-0711-preview',\n    provider=MoonshotAIProvider(api_key='your-moonshot-api-key'),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/openai/#moonshotai", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "GitHub Models", "anchor": "github-models", "heading_level": 3, "md_text": "To use [GitHub Models](https://docs.github.com/en/github-models), you'll need a GitHub personal access token with the `models: read` permission.\n\nYou can set the `GITHUB_API_KEY` environment variable and use [`GitHubProvider`][pydantic_ai.providers.github.GitHubProvider] by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('github:xai/grok-3-mini')\n...\n```\n\nOr initialise the model and provider directly:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.github import GitHubProvider\n\nmodel = OpenAIChatModel(\n    'xai/grok-3-mini',  # GitHub Models uses prefixed model names\n    provider=GitHubProvider(api_key='your-github-token'),\n)\nagent = Agent(model)\n...\n```\n\nGitHub Models supports various model families with different prefixes. You can see the full list on the [GitHub Marketplace](https://github.com/marketplace?type=models) or the public [catalog endpoint](https://models.github.ai/catalog/models).", "url": "https://ai.pydantic.dev/docs/models/openai/#github-models", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "Perplexity", "anchor": "perplexity", "heading_level": 3, "md_text": "Follow the Perplexity [getting started](https://docs.perplexity.ai/guides/getting-started)\nguide to create an API key.\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nmodel = OpenAIChatModel(\n    'sonar-pro',\n    provider=OpenAIProvider(\n        base_url='https://api.perplexity.ai',\n        api_key='your-perplexity-api-key',\n    ),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/openai/#perplexity", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "Fireworks AI", "anchor": "fireworks-ai", "heading_level": 3, "md_text": "Go to [Fireworks.AI](https://fireworks.ai/) and create an API key in your account settings.\n\nYou can set the `FIREWORKS_API_KEY` environment variable and use [`FireworksProvider`][pydantic_ai.providers.fireworks.FireworksProvider] by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('fireworks:accounts/fireworks/models/qwq-32b')\n...\n```\n\nOr initialise the model and provider directly:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.fireworks import FireworksProvider\n\nmodel = OpenAIChatModel(\n    'accounts/fireworks/models/qwq-32b',  # model library available at https://fireworks.ai/models\n    provider=FireworksProvider(api_key='your-fireworks-api-key'),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/openai/#fireworks-ai", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "Together AI", "anchor": "together-ai", "heading_level": 3, "md_text": "Go to [Together.ai](https://www.together.ai/) and create an API key in your account settings.\n\nYou can set the `TOGETHER_API_KEY` environment variable and use [`TogetherProvider`][pydantic_ai.providers.together.TogetherProvider] by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('together:meta-llama/Llama-3.3-70B-Instruct-Turbo-Free')\n...\n```\n\nOr initialise the model and provider directly:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.together import TogetherProvider\n\nmodel = OpenAIChatModel(\n    'meta-llama/Llama-3.3-70B-Instruct-Turbo-Free',  # model library available at https://www.together.ai/models\n    provider=TogetherProvider(api_key='your-together-api-key'),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/openai/#together-ai", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "Heroku AI", "anchor": "heroku-ai", "heading_level": 3, "md_text": "To use [Heroku AI](https://www.heroku.com/ai), first create an API key.\n\nYou can set the `HEROKU_INFERENCE_KEY` and (optionally )`HEROKU_INFERENCE_URL` environment variables and use [`HerokuProvider`][pydantic_ai.providers.heroku.HerokuProvider] by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('heroku:claude-sonnet-4-5')\n...\n```\n\nOr initialise the model and provider directly:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.heroku import HerokuProvider\n\nmodel = OpenAIChatModel(\n    'claude-sonnet-4-5',\n    provider=HerokuProvider(api_key='your-heroku-inference-key'),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/docs/models/openai/#heroku-ai", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "Cerebras", "anchor": "cerebras", "heading_level": 3, "md_text": "To use [Cerebras](https://cerebras.ai/), you need to create an API key in the [Cerebras Console](https://cloud.cerebras.ai/).\n\nYou can set the `CEREBRAS_API_KEY` environment variable and use [`CerebrasProvider`][pydantic_ai.providers.cerebras.CerebrasProvider] by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('cerebras:llama3.3-70b')\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n```\n\nOr initialise the model and provider directly:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.cerebras import CerebrasProvider\n\nmodel = OpenAIChatModel(\n    'llama3.3-70b',\n    provider=CerebrasProvider(api_key='your-cerebras-api-key'),\n)\nagent = Agent(model)\n\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n```", "url": "https://ai.pydantic.dev/docs/models/openai/#cerebras", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "LiteLLM", "anchor": "litellm", "heading_level": 3, "md_text": "To use [LiteLLM](https://www.litellm.ai/), set the configs as outlined in the [doc](https://docs.litellm.ai/docs/set_keys). In `LiteLLMProvider`, you can pass `api_base` and `api_key`. The value of these configs will depend on your setup. For example, if you are using OpenAI models, then you need to pass `https://api.openai.com/v1` as the `api_base` and your OpenAI API key as the `api_key`. If you are using a LiteLLM proxy server running on your local machine, then you need to pass `http://localhost:<port>` as the `api_base` and your LiteLLM API key (or a placeholder) as the `api_key`.\n\nTo use custom LLMs, use `custom/` prefix in the model name.\n\nOnce you have the configs, use the [`LiteLLMProvider`][pydantic_ai.providers.litellm.LiteLLMProvider] as follows:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.litellm import LiteLLMProvider\n\nmodel = OpenAIChatModel(\n    'openai/gpt-5',\n    provider=LiteLLMProvider(\n        api_base='<api-base-url>',\n        api_key='<api-key>'\n    )\n)\nagent = Agent(model)\n\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n...\n```", "url": "https://ai.pydantic.dev/docs/models/openai/#litellm", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "Nebius AI Studio", "anchor": "nebius-ai-studio", "heading_level": 3, "md_text": "Go to [Nebius AI Studio](https://studio.nebius.com/) and create an API key.\n\nYou can set the `NEBIUS_API_KEY` environment variable and use [`NebiusProvider`][pydantic_ai.providers.nebius.NebiusProvider] by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('nebius:Qwen/Qwen3-32B-fast')\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n```\n\nOr initialise the model and provider directly:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.nebius import NebiusProvider\n\nmodel = OpenAIChatModel(\n    'Qwen/Qwen3-32B-fast',\n    provider=NebiusProvider(api_key='your-nebius-api-key'),\n)\nagent = Agent(model)\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n```", "url": "https://ai.pydantic.dev/docs/models/openai/#nebius-ai-studio", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "OVHcloud AI Endpoints", "anchor": "ovhcloud-ai-endpoints", "heading_level": 3, "md_text": "To use OVHcloud AI Endpoints, you need to create a new API key. To do so, go to the [OVHcloud manager](https://ovh.com/manager), then in Public Cloud > AI Endpoints > API keys. Click on `Create a new API key` and copy your new key.\n\nYou can explore the [catalog](https://endpoints.ai.cloud.ovh.net/catalog) to find which models are available.\n\nYou can set the `OVHCLOUD_API_KEY` environment variable and use [`OVHcloudProvider`][pydantic_ai.providers.ovhcloud.OVHcloudProvider] by name:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('ovhcloud:gpt-oss-120b')\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n```\n\nIf you need to configure the provider, you can use the [`OVHcloudProvider`][pydantic_ai.providers.ovhcloud.OVHcloudProvider] class:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.ovhcloud import OVHcloudProvider\n\nmodel = OpenAIChatModel(\n    'gpt-oss-120b',\n    provider=OVHcloudProvider(api_key='your-api-key'),\n)\nagent = Agent(model)\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n```", "url": "https://ai.pydantic.dev/docs/models/openai/#ovhcloud-ai-endpoints", "page": "docs/models/openai", "source_site": "pydantic_ai"}
{"title": "Core Concepts", "anchor": "core-concepts", "heading_level": 1, "md_text": "This page explains the key concepts in Pydantic Evals and how they work together.", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#core-concepts", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "Pydantic Evals is built around these core concepts:\n\n- **[`Dataset`][pydantic_evals.Dataset]** - A static definition containing test cases and evaluators\n- **[`Case`][pydantic_evals.Case]** - A single test scenario with inputs and optional expected outputs\n- **[`Evaluator`][pydantic_evals.evaluators.Evaluator]** - Logic for scoring or validating outputs\n- **Experiment** - The act of running a task function against all cases in a dataset. (This corresponds to a call to `Dataset.evaluate`.)\n- **[`EvaluationReport`][pydantic_evals.reporting.EvaluationReport]** - The results from running an experiment\n\nThe key distinction is between:\n\n- **Definition** (`Dataset` with `Case`s and `Evaluator`s) - what you want to test\n- **Execution** (Experiment) - running your task against those tests\n- **Results** (`EvaluationReport`) - what happened during the experiment", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#overview", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Unit Testing Analogy", "anchor": "unit-testing-analogy", "heading_level": 2, "md_text": "A helpful way to think about Pydantic Evals:\n\n| Unit Testing | Pydantic Evals |\n|--------------|----------------|\n| Test function | [`Case`][pydantic_evals.Case] + [`Evaluator`][pydantic_evals.evaluators.Evaluator] |\n| Test suite | [`Dataset`][pydantic_evals.Dataset] |\n| Running tests (`pytest`) | **Experiment** (`dataset.evaluate(task)`) |\n| Test report | [`EvaluationReport`][pydantic_evals.reporting.EvaluationReport] |\n| `assert` | Evaluator returning `bool` |\n\n**Key Difference**: AI systems are probabilistic, so instead of simple pass/fail, evaluations can have:\n\n- Quantitative scores (0.0 to 1.0)\n- Qualitative labels (\"good\", \"acceptable\", \"poor\")\n- Pass/fail assertions with explanatory reasons\n\nJust like you can run `pytest` multiple times on the same test suite, you can run multiple experiments on the same dataset to compare different implementations or track changes over time.", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#unit-testing-analogy", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Dataset", "anchor": "dataset", "heading_level": 2, "md_text": "A [`Dataset`][pydantic_evals.Dataset] is a collection of test cases and evaluators that define an evaluation suite.\n\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import IsInstance\n\ndataset = Dataset(\n    name='my_eval_suite',  # Optional name\n    cases=[\n        Case(inputs='test input', expected_output='test output'),\n    ],\n    evaluators=[\n        IsInstance(type_name='str'),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#dataset", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Key Features", "anchor": "key-features", "heading_level": 3, "md_text": "- **Type-safe**: Generic over `InputsT`, `OutputT`, and `MetadataT` types\n- **Serializable**: Can be saved to/loaded from YAML or JSON files\n- **Evaluable**: Run against any function with matching input/output types", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#key-features", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "`Dataset`-Level vs `Case`-Level Evaluators", "anchor": "dataset-level-vs-case-level-evaluators", "heading_level": 3, "md_text": "Evaluators can be defined at two levels:\n\n- **`Dataset`-level**: Apply to all cases in the dataset\n- **`Case`-level**: Apply only to specific cases\n\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import EqualsExpected, IsInstance\n\ndataset = Dataset(\n    cases=[\n        Case(\n            name='special_case',\n            inputs='test',\n            expected_output='TEST',\n            evaluators=[\n                # This evaluator only runs for this case\n                EqualsExpected(),\n            ],\n        ),\n    ],\n    evaluators=[\n        # This evaluator runs for ALL cases\n        IsInstance(type_name='str'),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#dataset-level-vs-case-level-evaluators", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Experiments", "anchor": "experiments", "heading_level": 2, "md_text": "An **Experiment** is what happens when you execute a task function against all cases in a dataset. This is the bridge between your static test definition (the Dataset) and your results (the EvaluationReport).", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#experiments", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Running an Experiment", "anchor": "running-an-experiment", "heading_level": 3, "md_text": "You run an experiment by calling [`evaluate()`][pydantic_evals.Dataset.evaluate] or [`evaluate_sync()`][pydantic_evals.Dataset.evaluate_sync] on a dataset:\n\n```python\nfrom pydantic_evals import Case, Dataset", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#running-an-experiment", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Define your dataset (static definition)", "anchor": "define-your-dataset-static-definition", "heading_level": 1, "md_text": "dataset = Dataset(\n    cases=[\n        Case(inputs='hello', expected_output='HELLO'),\n        Case(inputs='world', expected_output='WORLD'),\n    ],\n)", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#define-your-dataset-static-definition", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Define your task", "anchor": "define-your-task", "heading_level": 1, "md_text": "def uppercase_task(text: str) -> str:\n    return text.upper()", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#define-your-task", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Run the experiment (execution)", "anchor": "run-the-experiment-execution", "heading_level": 1, "md_text": "report = dataset.evaluate_sync(uppercase_task)\n```", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#run-the-experiment-execution", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "What Happens During an Experiment", "anchor": "what-happens-during-an-experiment", "heading_level": 3, "md_text": "When you run an experiment:\n\n1. **Setup**: The dataset loads all cases and evaluators\n2. **Execution**: For each case:\n    1. The task function is called with `case.inputs`\n    2. Execution time is measured and OpenTelemetry spans are captured (if `logfire` is configured)\n    3. The outputs of the task function for each case are recorded\n3. **Evaluation**: For each case output:\n    1. All dataset-level evaluators are run\n    2. Case-specific evaluators are run (if any)\n    3. Results are collected (scores, assertions, labels)\n4. **Reporting**: All results are aggregated into an [`EvaluationReport`][pydantic_evals.reporting.EvaluationReport]", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#what-happens-during-an-experiment", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Multiple Experiments from One Dataset", "anchor": "multiple-experiments-from-one-dataset", "heading_level": 3, "md_text": "A key feature of Pydantic Evals is that you can run the same dataset against different task implementations:\n\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import EqualsExpected\n\ndataset = Dataset(\n    cases=[\n        Case(inputs='hello', expected_output='HELLO'),\n    ],\n    evaluators=[EqualsExpected()],\n)", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#multiple-experiments-from-one-dataset", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Original implementation", "anchor": "original-implementation", "heading_level": 1, "md_text": "def task_v1(text: str) -> str:\n    return text.upper()", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#original-implementation", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Improved implementation (with exclamation)", "anchor": "improved-implementation-with-exclamation", "heading_level": 1, "md_text": "def task_v2(text: str) -> str:\n    return text.upper() + '!'", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#improved-implementation-with-exclamation", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Compare results", "anchor": "compare-results", "heading_level": 1, "md_text": "report_v1 = dataset.evaluate_sync(task_v1)\nreport_v2 = dataset.evaluate_sync(task_v2)\n\navg_v1 = report_v1.averages()\navg_v2 = report_v2.averages()\nprint(f'V1 pass rate: {avg_v1.assertions if avg_v1 and avg_v1.assertions else 0}')\n#> V1 pass rate: 1.0\nprint(f'V2 pass rate: {avg_v2.assertions if avg_v2 and avg_v2.assertions else 0}')\n#> V2 pass rate: 0\n```\n\nThis allows you to:\n\n- **Compare implementations** across versions\n- **Track performance** over time\n- **A/B test** different approaches\n- **Validate changes** before deployment", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#compare-results", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Case", "anchor": "case", "heading_level": 2, "md_text": "A [`Case`][pydantic_evals.Case] represents a single test scenario with specific inputs and optional expected outputs.\n\n```python\nfrom pydantic_evals import Case\nfrom pydantic_evals.evaluators import EqualsExpected\n\ncase = Case(\n    name='test_uppercase',  # Optional, but recommended for reporting\n    inputs='hello world',  # Required: inputs to your task\n    expected_output='HELLO WORLD',  # Optional: expected output\n    metadata={'category': 'basic'},  # Optional: arbitrary metadata\n    evaluators=[EqualsExpected()],  # Optional: case-specific evaluators\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#case", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Inputs", "anchor": "inputs", "heading_level": 4, "md_text": "The inputs to pass to the task being evaluated. Can be any type:\n\n```python\nfrom pydantic import BaseModel\n\nfrom pydantic_evals import Case\n\n\nclass MyInputModel(BaseModel):\n    field1: str", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#inputs", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Simple types", "anchor": "simple-types", "heading_level": 1, "md_text": "Case(inputs='hello')\nCase(inputs=42)", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#simple-types", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Complex types", "anchor": "complex-types", "heading_level": 1, "md_text": "Case(inputs={'query': 'What is AI?', 'max_tokens': 100})\nCase(inputs=MyInputModel(field1='value'))\n```", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#complex-types", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Expected Output", "anchor": "expected-output", "heading_level": 4, "md_text": "The expected result, used by evaluators like [`EqualsExpected`][pydantic_evals.evaluators.EqualsExpected]:\n\n```python\nfrom pydantic_evals import Case\n\nCase(\n    inputs='2 + 2',\n    expected_output='4',\n)\n```\n\nIf no `expected_output` is provided, evaluators that require it (like `EqualsExpected`) will skip that case.", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#expected-output", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Metadata", "anchor": "metadata", "heading_level": 4, "md_text": "Arbitrary data that evaluators can access via [`EvaluatorContext`][pydantic_evals.evaluators.EvaluatorContext]:\n\n```python\nfrom pydantic_evals import Case\n\nCase(\n    inputs='question',\n    metadata={\n        'difficulty': 'hard',\n        'category': 'math',\n        'source': 'exam_2024',\n    },\n)\n```\n\nMetadata is useful for:\n\n- Filtering cases during analysis\n- Providing context to evaluators\n- Organizing test suites", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#metadata", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Evaluators", "anchor": "evaluators", "heading_level": 4, "md_text": "Cases can have their own evaluators that only run for that specific case. This is particularly powerful for building comprehensive evaluation suites where different cases have different requirements - if you could write one evaluator rubric that worked perfectly for all cases, you'd just incorporate it into your agent instructions. Case-specific [`LLMJudge`][pydantic_evals.evaluators.LLMJudge] evaluators are especially useful for quickly building maintainable golden datasets by describing what \"good\" looks like for each scenario. See [Case-specific evaluators](evaluators/overview.md#case-specific-evaluators) for a more detailed explanation and examples.", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#evaluators", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Evaluator", "anchor": "evaluator", "heading_level": 2, "md_text": "An [`Evaluator`][pydantic_evals.evaluators.Evaluator] assesses the output of your task and returns one or more scores, labels, or assertions. Each score, label or assertion can also have an optional string-value reason associated.", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#evaluator", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Evaluator Types", "anchor": "evaluator-types", "heading_level": 3, "md_text": "Evaluators return different types of results:\n\n| Return Type | Purpose | Example |\n|-------------|---------|---------|\n| `bool` | **Assertion** - Pass/fail check | `True` \u2192 \u2714, `False` \u2192 \u2717 |\n| `int` or `float` | **Score** - Numeric quality metric | `0.95`, `87` |\n| `str` | **Label** - Categorical result | `\"correct\"`, `\"hallucination\"` |\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass ExactMatch(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return ctx.output == ctx.expected_output  # Assertion\n\n\n@dataclass\nclass Confidence(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> float:\n        # Analyze output and return confidence score\n        return 0.95  # Score\n\n\n@dataclass\nclass Classifier(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> str:\n        if 'error' in ctx.output.lower():\n            return 'error'  # Label\n        return 'success'\n```\n\nEvaluators can also return instances of [`EvaluationReason`][pydantic_evals.evaluators.EvaluationReason], and dictionaries mapping labels to output values.\nSee the [custom evaluator return types](evaluators/custom.md#return-types) docs for more detail.", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#evaluator-types", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "EvaluatorContext", "anchor": "evaluatorcontext", "heading_level": 3, "md_text": "All evaluators receive an [`EvaluatorContext`][pydantic_evals.evaluators.EvaluatorContext] containing:\n\n- `name`: Case name (optional)\n- `inputs`: Task inputs\n- `metadata`: Case metadata (optional)\n- `expected_output`: Expected output (optional)\n- `output`: Actual output from task\n- `duration`: Task execution time in seconds\n- `span_tree`: OpenTelemetry spans (if `logfire` is configured)\n- `attributes`: Custom attributes dict\n- `metrics`: Custom metrics dict", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#evaluatorcontext", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Multiple Evaluations", "anchor": "multiple-evaluations", "heading_level": 3, "md_text": "Evaluators can return multiple results by returning a dictionary:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass MultiCheck(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> dict[str, bool | float | str]:\n        return {\n            'is_valid': isinstance(ctx.output, str),  # Assertion\n            'length': len(ctx.output),  # Metric\n            'category': 'long' if len(ctx.output) > 100 else 'short',  # Label\n        }\n```", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#multiple-evaluations", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Evaluation Reasons", "anchor": "evaluation-reasons", "heading_level": 3, "md_text": "Add explanations to your evaluations using [`EvaluationReason`][pydantic_evals.evaluators.EvaluationReason]:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import EvaluationReason, Evaluator, EvaluatorContext\n\n\n@dataclass\nclass SmartCheck(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> EvaluationReason:\n        if ctx.output == ctx.expected_output:\n            return EvaluationReason(\n                value=True,\n                reason='Exact match with expected output',\n            )\n        return EvaluationReason(\n            value=False,\n            reason=f'Expected {ctx.expected_output!r}, got {ctx.output!r}',\n        )\n```\n\nReasons appear in reports when using `include_reasons=True`.", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#evaluation-reasons", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Evaluation Report", "anchor": "evaluation-report", "heading_level": 2, "md_text": "An [`EvaluationReport`][pydantic_evals.reporting.EvaluationReport] is the result of running an experiment. It contains all the data from executing your task against the dataset's cases and running all evaluators.\n\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import EqualsExpected\n\ndataset = Dataset(\n    cases=[Case(inputs='hello', expected_output='HELLO')],\n    evaluators=[EqualsExpected()],\n)\n\n\ndef my_task(text: str) -> str:\n    return text.upper()", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#evaluation-report", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Run an experiment", "anchor": "run-an-experiment", "heading_level": 1, "md_text": "report = dataset.evaluate_sync(my_task)", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#run-an-experiment", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Print to console", "anchor": "print-to-console", "heading_level": 1, "md_text": "report.print()\n\"\"\"\n    Evaluation Summary: my_task\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Case ID  \u2503 Assertions \u2503 Duration \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Case 1   \u2502 \u2714          \u2502     10ms \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Averages \u2502 100.0% \u2714   \u2502     10ms \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\"\"\"", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#print-to-console", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Access data programmatically", "anchor": "access-data-programmatically", "heading_level": 1, "md_text": "for case in report.cases:\n    print(f'{case.name}: {case.scores}')\n    #> Case 1: {}\n```", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#access-data-programmatically", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Report Structure", "anchor": "report-structure", "heading_level": 3, "md_text": "The [`EvaluationReport`][pydantic_evals.reporting.EvaluationReport] contains:\n\n- `name`: Experiment name\n- `cases`: List of successful case evaluations\n- `failures`: List of failed executions\n- `trace_id`: OpenTelemetry trace ID (optional)\n- `span_id`: OpenTelemetry span ID (optional)", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#report-structure", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "ReportCase", "anchor": "reportcase", "heading_level": 3, "md_text": "Each successfulcase result contains:\n\n**Case data:**\n\n- `name`: Case name\n- `inputs`: Task inputs\n- `metadata`: Case metadata (optional)\n- `expected_output`: Expected output (optional)\n- `output`: Actual output from task\n\n**Evaluation results:**\n\n- `scores`: Dictionary of numeric scores from evaluators\n- `labels`: Dictionary of categorical labels from evaluators\n- `assertions`: Dictionary of pass/fail assertions from evaluators\n\n**Performance data:**\n\n- `task_duration`: Task execution time\n- `total_duration`: Total time including evaluators\n\n**Additional data:**\n\n- `metrics`: Custom metrics dict\n- `attributes`: Custom attributes dict\n\n**Tracing:**\n\n- `trace_id`: OpenTelemetry trace ID (optional)\n- `span_id`: OpenTelemetry span ID (optional)\n\n**Errors:**\n\n- `evaluator_failures`: List of evaluator errors", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#reportcase", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Data Model Relationships", "anchor": "data-model-relationships", "heading_level": 2, "md_text": "Here's how the core concepts relate to each other:", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#data-model-relationships", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Static Definition", "anchor": "static-definition", "heading_level": 3, "md_text": "- A **Dataset** contains:\n  - Many **Cases** (test scenarios with inputs and expected outputs)\n  - Many **Evaluators** (logic for scoring outputs)", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#static-definition", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Execution (Experiment)", "anchor": "execution-experiment", "heading_level": 3, "md_text": "When you call `dataset.evaluate(task)`, an **Experiment** runs:\n\n- The **Task** function is executed against all **Cases** in the **Dataset**\n- All **Evaluators** are run (both dataset-level and case-specific) against each output as appropriate\n- One **EvaluationReport** is produced as the final output", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#execution-experiment", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Results", "anchor": "results", "heading_level": 3, "md_text": "- An **EvaluationReport** contains:\n  - Results for each **Case** (inputs, outputs, scores, assertions, labels)\n  - Summary statistics (averages, pass rates)\n  - Performance data (durations)\n  - Tracing information (OpenTelemetry spans)", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#results", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Key Relationships", "anchor": "key-relationships", "heading_level": 3, "md_text": "- **One Dataset \u2192 Many Experiments**: You can run the same dataset against different task implementations or multiple times to track changes\n- **One Experiment \u2192 One Report**: Each time you call `dataset.evaluate(...)`, you get one report\n- **One Experiment \u2192 Many Case Results**: The report contains results for every case in the dataset", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#key-relationships", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "- **[Evaluators Overview](evaluators/overview.md)** - When to use different evaluator types\n- **[Built-in Evaluators](evaluators/built-in.md)** - Complete reference of provided evaluators\n- **[Custom Evaluators](evaluators/custom.md)** - Write your own evaluation logic\n- **[Dataset Management](how-to/dataset-management.md)** - Save, load, and generate datasets", "url": "https://ai.pydantic.dev/docs/evals/core-concepts/#next-steps", "page": "docs/evals/core-concepts", "source_site": "pydantic_ai"}
{"title": "Pydantic Evals", "anchor": "pydantic-evals", "heading_level": 1, "md_text": "**Pydantic Evals** is a powerful evaluation framework for systematically testing and evaluating AI systems, from simple LLM calls to complex multi-agent applications.", "url": "https://ai.pydantic.dev/docs/evals/quick-start/#pydantic-evals", "page": "docs/evals/quick-start", "source_site": "pydantic_ai"}
{"title": "What is Pydantic Evals?", "anchor": "what-is-pydantic-evals", "heading_level": 2, "md_text": "Pydantic Evals helps you:\n\n- **Create test datasets** with type-safe structured inputs and expected outputs\n- **Run evaluations** against your AI systems with automatic concurrency\n- **Score results** using deterministic checks, LLM judges, or custom evaluators\n- **Generate reports** with detailed metrics, assertions, and performance data\n- **Track changes** by comparing evaluation runs over time\n- **Integrate with Logfire** for visualization and collaborative analysis", "url": "https://ai.pydantic.dev/docs/evals/quick-start/#what-is-pydantic-evals", "page": "docs/evals/quick-start", "source_site": "pydantic_ai"}
{"title": "Installation", "anchor": "installation", "heading_level": 2, "md_text": "```bash\npip install pydantic-evals\n```\n\nFor OpenTelemetry tracing and Logfire integration:\n\n```bash\npip install 'pydantic-evals[logfire]'\n```", "url": "https://ai.pydantic.dev/docs/evals/quick-start/#installation", "page": "docs/evals/quick-start", "source_site": "pydantic_ai"}
{"title": "Quick Start", "anchor": "quick-start", "heading_level": 2, "md_text": "While evaluations are typically used to test AI systems, the Pydantic Evals framework works with any function call. To demonstrate the core functionality, we'll start with a simple, deterministic example.\n\nHere's a complete example of evaluating a simple text transformation function:\n\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Contains, EqualsExpected", "url": "https://ai.pydantic.dev/docs/evals/quick-start/#quick-start", "page": "docs/evals/quick-start", "source_site": "pydantic_ai"}
{"title": "Create a dataset with test cases", "anchor": "create-a-dataset-with-test-cases", "heading_level": 1, "md_text": "dataset = Dataset(\n    cases=[\n        Case(\n            name='uppercase_basic',\n            inputs='hello world',\n            expected_output='HELLO WORLD',\n        ),\n        Case(\n            name='uppercase_with_numbers',\n            inputs='hello 123',\n            expected_output='HELLO 123',\n        ),\n    ],\n    evaluators=[\n        EqualsExpected(),  # Check exact match with expected_output\n        Contains(value='HELLO', case_sensitive=True),  # Check contains \"HELLO\"\n    ],\n)", "url": "https://ai.pydantic.dev/docs/evals/quick-start/#create-a-dataset-with-test-cases", "page": "docs/evals/quick-start", "source_site": "pydantic_ai"}
{"title": "Define the function to evaluate", "anchor": "define-the-function-to-evaluate", "heading_level": 1, "md_text": "def uppercase_text(text: str) -> str:\n    return text.upper()", "url": "https://ai.pydantic.dev/docs/evals/quick-start/#define-the-function-to-evaluate", "page": "docs/evals/quick-start", "source_site": "pydantic_ai"}
{"title": "Run the evaluation", "anchor": "run-the-evaluation", "heading_level": 1, "md_text": "report = dataset.evaluate_sync(uppercase_text)", "url": "https://ai.pydantic.dev/docs/evals/quick-start/#run-the-evaluation", "page": "docs/evals/quick-start", "source_site": "pydantic_ai"}
{"title": "Print the results", "anchor": "print-the-results", "heading_level": 1, "md_text": "report.print()\n\"\"\"\n        Evaluation Summary: uppercase_text\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Case ID                \u2503 Assertions \u2503 Duration \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 uppercase_basic        \u2502 \u2714\u2714         \u2502     10ms \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 uppercase_with_numbers \u2502 \u2714\u2714         \u2502     10ms \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Averages               \u2502 100.0% \u2714   \u2502     10ms \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\"\"\"\n```\n\nOutput:\n\n```\n                  Evaluation Summary: uppercase_text\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Case ID                 \u2503 Assertions \u2503 Duration \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 uppercase_basic         \u2502 \u2714\u2714         \u2502     10ms \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 uppercase_with_numbers  \u2502 \u2714\u2714         \u2502     10ms \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Averages                \u2502 100.0% \u2714   \u2502     10ms \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```", "url": "https://ai.pydantic.dev/docs/evals/quick-start/#print-the-results", "page": "docs/evals/quick-start", "source_site": "pydantic_ai"}
{"title": "Key Concepts", "anchor": "key-concepts", "heading_level": 2, "md_text": "Understanding a few core concepts will help you get the most out of Pydantic Evals:\n\n- **[`Dataset`][pydantic_evals.Dataset]** - A collection of test cases and (optional) evaluators\n- **[`Case`][pydantic_evals.Case]** - A single test scenario with inputs and optional expected outputs and case-specific evaluators\n- **[`Evaluator`][pydantic_evals.evaluators.Evaluator]** - A function that scores or validates task outputs\n- **[`EvaluationReport`][pydantic_evals.reporting.EvaluationReport]** - Results from running an evaluation\n\nFor a deeper dive, see [Core Concepts](core-concepts.md).", "url": "https://ai.pydantic.dev/docs/evals/quick-start/#key-concepts", "page": "docs/evals/quick-start", "source_site": "pydantic_ai"}
{"title": "Deterministic Validation", "anchor": "deterministic-validation", "heading_level": 3, "md_text": "Test that your AI system produces correctly-structured outputs:\n\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Contains, IsInstance\n\ndataset = Dataset(\n    cases=[\n        Case(inputs={'data': 'required_key present'}, expected_output={'result': 'success'}),\n    ],\n    evaluators=[\n        IsInstance(type_name='dict'),\n        Contains(value='required_key'),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/quick-start/#deterministic-validation", "page": "docs/evals/quick-start", "source_site": "pydantic_ai"}
{"title": "LLM-as-a-Judge Evaluation", "anchor": "llm-as-a-judge-evaluation", "heading_level": 3, "md_text": "Use an LLM to evaluate subjective qualities like accuracy or helpfulness:\n\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\ndataset = Dataset(\n    cases=[\n        Case(inputs='What is the capital of France?', expected_output='Paris'),\n    ],\n    evaluators=[\n        LLMJudge(\n            rubric='Response is accurate and helpful',\n            include_input=True,\n            model='anthropic:claude-sonnet-4-5',\n        )\n    ],\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/quick-start/#llm-as-a-judge-evaluation", "page": "docs/evals/quick-start", "source_site": "pydantic_ai"}
{"title": "Performance Testing", "anchor": "performance-testing", "heading_level": 3, "md_text": "Ensure your system meets performance requirements:\n\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import MaxDuration\n\ndataset = Dataset(\n    cases=[\n        Case(inputs='test input', expected_output='test output'),\n    ],\n    evaluators=[\n        MaxDuration(seconds=2.0),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/quick-start/#performance-testing", "page": "docs/evals/quick-start", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "Explore the documentation to learn more:\n\n- **[Core Concepts](core-concepts.md)** - Understand the data model and evaluation flow\n- **[Built-in Evaluators](evaluators/built-in.md)** - Learn about all available evaluators\n- **[Custom Evaluators](evaluators/custom.md)** - Write your own evaluation logic\n- **[Dataset Management](how-to/dataset-management.md)** - Save, load, and generate datasets\n- **[Examples](examples/simple-validation.md)** - Practical examples for common scenarios", "url": "https://ai.pydantic.dev/docs/evals/quick-start/#next-steps", "page": "docs/evals/quick-start", "source_site": "pydantic_ai"}
{"title": "FastMCP Client", "anchor": "fastmcp-client", "heading_level": 1, "md_text": "[FastMCP](https://gofastmcp.com/) is a higher-level MCP framework that bills itself as \"The fast, Pythonic way to build MCP servers and clients.\" It supports additional capabilities on top of the MCP specification like [Tool Transformation](https://gofastmcp.com/patterns/tool-transformation), [OAuth](https://gofastmcp.com/clients/auth/oauth), and more.\n\nAs an alternative to Pydantic AI's standard [`MCPServer` MCP client](client.md) built on the [MCP SDK](https://github.com/modelcontextprotocol/python-sdk), you can use the [`FastMCPToolset`][pydantic_ai.toolsets.fastmcp.FastMCPToolset] [toolset](../toolsets.md) that leverages the [FastMCP Client](https://gofastmcp.com/clients/) to connect to local and remote MCP servers, whether or not they're built using [FastMCP Server](https://gofastmcp.com/servers/).\n\nNote that it does not yet support integration elicitation or sampling, which are supported by the [standard `MCPServer` client](client.md).", "url": "https://ai.pydantic.dev/docs/mcp/fastmcp-client/#fastmcp-client", "page": "docs/mcp/fastmcp-client", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "heading_level": 2, "md_text": "To use the `FastMCPToolset`, you will need to install [`pydantic-ai-slim`](../install.md#slim-install) with the `fastmcp` optional group:\n\n```bash\npip/uv-add \"pydantic-ai-slim[fastmcp]\"\n```", "url": "https://ai.pydantic.dev/docs/mcp/fastmcp-client/#install", "page": "docs/mcp/fastmcp-client", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 2, "md_text": "A `FastMCPToolset` can then be created from:\n\n- A FastMCP Server: `#!python FastMCPToolset(fastmcp.FastMCP('my_server'))`\n- A FastMCP Client: `#!python FastMCPToolset(fastmcp.Client(...))`\n- A FastMCP Transport: `#!python FastMCPToolset(fastmcp.StdioTransport(command='uvx', args=['mcp-run-python', 'stdio']))`\n- A Streamable HTTP URL: `#!python FastMCPToolset('http://localhost:8000/mcp')`\n- An HTTP SSE URL: `#!python FastMCPToolset('http://localhost:8000/sse')`\n- A Python Script: `#!python FastMCPToolset('my_server.py')`\n- A Node.js Script: `#!python FastMCPToolset('my_server.js')`\n- A JSON MCP Configuration: `#!python FastMCPToolset({'mcpServers': {'my_server': {'command': 'uvx', 'args': ['mcp-run-python', 'stdio']}}})`\n\nIf you already have a [FastMCP Server](https://gofastmcp.com/servers) in the same codebase as your Pydantic AI agent, you can create a `FastMCPToolset` directly from it and save agent a network round trip:\n\n```python\nfrom fastmcp import FastMCP\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.toolsets.fastmcp import FastMCPToolset\n\nfastmcp_server = FastMCP('my_server')\n@fastmcp_server.tool()\nasync def add(a: int, b: int) -> int:\n    return a + b\n\ntoolset = FastMCPToolset(fastmcp_server)\n\nagent = Agent('openai:gpt-5', toolsets=[toolset])\n\nasync def main():\n    result = await agent.run('What is 7 plus 5?')\n    print(result.output)\n    #> The answer is 12.\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_\n\nConnecting your agent to a Streamable HTTP MCP Server is as simple as:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.toolsets.fastmcp import FastMCPToolset\n\ntoolset = FastMCPToolset('http://localhost:8000/mcp')\n\nagent = Agent('openai:gpt-5', toolsets=[toolset])\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_\n\nYou can also create a `FastMCPToolset` from a JSON MCP Configuration:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_ai.toolsets.fastmcp import FastMCPToolset\n\nmcp_config = {\n    'mcpServers': {\n        'time_mcp_server': {\n            'command': 'uvx',\n            'args': ['mcp-run-python', 'stdio']\n        }\n    }\n}\n\ntoolset = FastMCPToolset(mcp_config)\n\nagent = Agent('openai:gpt-5', toolsets=[toolset])\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/mcp/fastmcp-client/#usage", "page": "docs/mcp/fastmcp-client", "source_site": "pydantic_ai"}
{"title": "Server", "anchor": "server", "heading_level": 1, "md_text": "Pydantic AI models can also be used within MCP Servers.", "url": "https://ai.pydantic.dev/docs/mcp/server/#server", "page": "docs/mcp/server", "source_site": "pydantic_ai"}
{"title": "MCP Server", "anchor": "mcp-server", "heading_level": 2, "md_text": "Here's a simple example of a [Python MCP server](https://github.com/modelcontextprotocol/python-sdk) using Pydantic AI within a tool call:\n\n```py {title=\"mcp_server.py\"}\nfrom mcp.server.fastmcp import FastMCP\n\nfrom pydantic_ai import Agent\n\nserver = FastMCP('Pydantic AI Server')\nserver_agent = Agent(\n    'anthropic:claude-haiku-4-5', system_prompt='always reply in rhyme'\n)\n\n\n@server.tool()\nasync def poet(theme: str) -> str:\n    \"\"\"Poem generator\"\"\"\n    r = await server_agent.run(f'write a poem about {theme}')\n    return r.output\n\n\nif __name__ == '__main__':\n    server.run()\n```", "url": "https://ai.pydantic.dev/docs/mcp/server/#mcp-server", "page": "docs/mcp/server", "source_site": "pydantic_ai"}
{"title": "Simple client", "anchor": "simple-client", "heading_level": 2, "md_text": "This server can be queried with any MCP client. Here is an example using the Python SDK directly:\n\n```py {title=\"mcp_client.py\" requires=\"mcp_server.py\" dunder_name=\"not_main\"}\nimport asyncio\nimport os\n\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\n\nasync def client():\n    server_params = StdioServerParameters(\n        command='python', args=['mcp_server.py'], env=os.environ\n    )\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write) as session:\n            await session.initialize()\n            result = await session.call_tool('poet', {'theme': 'socks'})\n            print(result.content[0].text)\n            \"\"\"\n            Oh, socks, those garments soft and sweet,\n            That nestle softly 'round our feet,\n            From cotton, wool, or blended thread,\n            They keep our toes from feeling dread.\n            \"\"\"\n\n\nif __name__ == '__main__':\n    asyncio.run(client())\n```", "url": "https://ai.pydantic.dev/docs/mcp/server/#simple-client", "page": "docs/mcp/server", "source_site": "pydantic_ai"}
{"title": "MCP Sampling", "anchor": "mcp-sampling", "heading_level": 2, "md_text": "!!! info \"What is MCP Sampling?\"\n    See the [MCP client docs](./client.md#mcp-sampling) for details of what MCP sampling is, and how you can support it when using Pydantic AI as an MCP client.\n\nWhen Pydantic AI agents are used within MCP servers, they can use sampling via [`MCPSamplingModel`][pydantic_ai.models.mcp_sampling.MCPSamplingModel].\n\nWe can extend the above example to use sampling so instead of connecting directly to the LLM, the agent calls back through the MCP client to make LLM calls.\n\n```py {title=\"mcp_server_sampling.py\"}\nfrom mcp.server.fastmcp import Context, FastMCP\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.mcp_sampling import MCPSamplingModel\n\nserver = FastMCP('Pydantic AI Server with sampling')\nserver_agent = Agent(system_prompt='always reply in rhyme')\n\n\n@server.tool()\nasync def poet(ctx: Context, theme: str) -> str:\n    \"\"\"Poem generator\"\"\"\n    r = await server_agent.run(f'write a poem about {theme}', model=MCPSamplingModel(session=ctx.session))\n    return r.output\n\n\nif __name__ == '__main__':\n    server.run()  # run the server over stdio\n```\n\nThe [above](#simple-client) client does not support sampling, so if you tried to use it with this server you'd get an error.\n\nThe simplest way to support sampling in an MCP client is to [use](./client.md#mcp-sampling) a Pydantic AI agent as the client, but if you wanted to support sampling with the vanilla MCP SDK, you could do so like this:\n\n```py {title=\"mcp_client_sampling.py\" requires=\"mcp_server_sampling.py\"}\nimport asyncio\nfrom typing import Any\n\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\nfrom mcp.shared.context import RequestContext\nfrom mcp.types import (\n    CreateMessageRequestParams,\n    CreateMessageResult,\n    ErrorData,\n    TextContent,\n)\n\n\nasync def sampling_callback(\n    context: RequestContext[ClientSession, Any], params: CreateMessageRequestParams\n) -> CreateMessageResult | ErrorData:\n    print('sampling system prompt:', params.systemPrompt)\n    #> sampling system prompt: always reply in rhyme\n    print('sampling messages:', params.messages)\n    \"\"\"\n    sampling messages:\n    [\n        SamplingMessage(\n            role='user',\n            content=TextContent(\n                type='text',\n                text='write a poem about socks',\n                annotations=None,\n                meta=None,\n            ),\n        )\n    ]\n    \"\"\"\n\n    # TODO get the response content by calling an LLM...\n    response_content = 'Socks for a fox.'\n\n    return CreateMessageResult(\n        role='assistant',\n        content=TextContent(type='text', text=response_content),\n        model='fictional-llm',\n    )\n\n\nasync def client():\n    server_params = StdioServerParameters(command='python', args=['mcp_server_sampling.py'])\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write, sampling_callback=sampling_callback) as session:\n            await session.initialize()\n            result = await session.call_tool('poet', {'theme': 'socks'})\n            print(result.content[0].text)\n            #> Socks for a fox.\n\n\nif __name__ == '__main__':\n    asyncio.run(client())\n```\n\n_(This example is complete, it can be run \"as is\")_", "url": "https://ai.pydantic.dev/docs/mcp/server/#mcp-sampling", "page": "docs/mcp/server", "source_site": "pydantic_ai"}
{"title": "Client", "anchor": "client", "heading_level": 1, "md_text": "Pydantic AI can act as an [MCP client](https://modelcontextprotocol.io/quickstart/client), connecting to MCP servers\nto use their tools.", "url": "https://ai.pydantic.dev/docs/mcp/client/#client", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "heading_level": 2, "md_text": "You need to either install [`pydantic-ai`](../install.md), or [`pydantic-ai-slim`](../install.md#slim-install) with the `mcp` optional group:\n\n```bash\npip/uv-add \"pydantic-ai-slim[mcp]\"\n```", "url": "https://ai.pydantic.dev/docs/mcp/client/#install", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 2, "md_text": "Pydantic AI comes with three ways to connect to MCP servers:\n\n- [`MCPServerStreamableHTTP`][pydantic_ai.mcp.MCPServerStreamableHTTP] which connects to an MCP server using the [Streamable HTTP](https://modelcontextprotocol.io/introduction#streamable-http) transport\n- [`MCPServerSSE`][pydantic_ai.mcp.MCPServerSSE] which connects to an MCP server using the [HTTP SSE](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse) transport\n- [`MCPServerStdio`][pydantic_ai.mcp.MCPServerStdio] which runs the server as a subprocess and connects to it using the [stdio](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio) transport\n\nExamples of all three are shown below.\n\nEach MCP server instance is a [toolset](../toolsets.md) and can be registered with an [`Agent`][pydantic_ai.Agent] using the `toolsets` argument.\n\nYou can use the [`async with agent`][pydantic_ai.Agent.__aenter__] context manager to open and close connections to all registered servers (and in the case of stdio servers, start and stop the subprocesses) around the context where they'll be used in agent runs. You can also use [`async with server`][pydantic_ai.mcp.MCPServer.__aenter__] to manage the connection or subprocess of a specific server, for example if you'd like to use it with multiple agents. If you don't explicitly enter one of these context managers to set up the server, this will be done automatically when it's needed (e.g. to list the available tools or call a specific tool), but it's more efficient to do so around the entire context where you expect the servers to be used.", "url": "https://ai.pydantic.dev/docs/mcp/client/#usage", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "Streamable HTTP Client", "anchor": "streamable-http-client", "heading_level": 3, "md_text": "[`MCPServerStreamableHTTP`][pydantic_ai.mcp.MCPServerStreamableHTTP] connects over HTTP using the\n[Streamable HTTP](https://modelcontextprotocol.io/introduction#streamable-http) transport to a server.\n\n!!! note\n    [`MCPServerStreamableHTTP`][pydantic_ai.mcp.MCPServerStreamableHTTP] requires an MCP server to be running and accepting HTTP connections before running the agent. Running the server is not managed by Pydantic AI.\n\nBefore creating the Streamable HTTP client, we need to run a server that supports the Streamable HTTP transport.\n\n```python {title=\"streamable_http_server.py\" dunder_name=\"not_main\"}\nfrom mcp.server.fastmcp import FastMCP\n\napp = FastMCP()\n\n@app.tool()\ndef add(a: int, b: int) -> int:\n    return a + b\n\nif __name__ == '__main__':\n    app.run(transport='streamable-http')\n```\n\nThen we can create the client:\n\n```python {title=\"mcp_streamable_http_client.py\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStreamableHTTP\n\nserver = MCPServerStreamableHTTP('http://localhost:8000/mcp')  # (1)!\nagent = Agent('openai:gpt-5', toolsets=[server])  # (2)!\n\nasync def main():\n    result = await agent.run('What is 7 plus 5?')\n    print(result.output)\n    #> The answer is 12.\n```\n\n1. Define the MCP server with the URL used to connect.\n2. Create an agent with the MCP server attached.\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_\n\n**What's happening here?**\n\n- The model receives the prompt \"What is 7 plus 5?\"\n- The model decides \"Oh, I've got this `add` tool, that will be a good way to answer this question\"\n- The model returns a tool call\n- Pydantic AI sends the tool call to the MCP server using the Streamable HTTP transport\n- The model is called again with the return value of running the `add` tool (12)\n- The model returns the final answer\n\nYou can visualise this clearly, and even see the tool call, by adding three lines of code to instrument the example with [logfire](https://logfire.pydantic.dev/docs):\n\n```python {title=\"mcp_sse_client_logfire.py\" test=\"skip\"}\nimport logfire\n\nlogfire.configure()\nlogfire.instrument_pydantic_ai()\n```", "url": "https://ai.pydantic.dev/docs/mcp/client/#streamable-http-client", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "SSE Client", "anchor": "sse-client", "heading_level": 3, "md_text": "[`MCPServerSSE`][pydantic_ai.mcp.MCPServerSSE] connects over HTTP using the [HTTP + Server Sent Events transport](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse) to a server.\n\n!!! note\n    The SSE transport in MCP is deprecated, you should use Streamable HTTP instead.\n\nBefore creating the SSE client, we need to run a server that supports the SSE transport.\n\n\n```python {title=\"sse_server.py\" dunder_name=\"not_main\"}\nfrom mcp.server.fastmcp import FastMCP\n\napp = FastMCP()\n\n@app.tool()\ndef add(a: int, b: int) -> int:\n    return a + b\n\nif __name__ == '__main__':\n    app.run(transport='sse')\n```\n\nThen we can create the client:\n\n```python {title=\"mcp_sse_client.py\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerSSE\n\nserver = MCPServerSSE('http://localhost:3001/sse')  # (1)!\nagent = Agent('openai:gpt-5', toolsets=[server])  # (2)!\n\n\nasync def main():\n    result = await agent.run('What is 7 plus 5?')\n    print(result.output)\n    #> The answer is 12.\n```\n\n1. Define the MCP server with the URL used to connect.\n2. Create an agent with the MCP server attached.\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/mcp/client/#sse-client", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "MCP \"stdio\" Server", "anchor": "mcp-stdio-server", "heading_level": 3, "md_text": "MCP also offers [stdio transport](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio) where the server is run as a subprocess and communicates with the client over `stdin` and `stdout`. In this case, you'd use the [`MCPServerStdio`][pydantic_ai.mcp.MCPServerStdio] class.\n\nIn this example [mcp-run-python](https://github.com/pydantic/mcp-run-python) is used as the MCP server.\n\n```python {title=\"mcp_stdio_client.py\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\n\nserver = MCPServerStdio(  # (1)!\n    'uv', args=['run', 'mcp-run-python', 'stdio'], timeout=10\n)\nagent = Agent('openai:gpt-5', toolsets=[server])\n\n\nasync def main():\n    result = await agent.run('How many days between 2000-01-01 and 2025-03-18?')\n    print(result.output)\n    #> There are 9,208 days between January 1, 2000, and March 18, 2025.\n```\n\n1. See [MCP Run Python](https://github.com/pydantic/mcp-run-python) for more information.", "url": "https://ai.pydantic.dev/docs/mcp/client/#mcp-stdio-server", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "Loading MCP Servers from Configuration", "anchor": "loading-mcp-servers-from-configuration", "heading_level": 2, "md_text": "Instead of creating MCP server instances individually in code, you can load multiple servers from a JSON configuration file using [`load_mcp_servers()`][pydantic_ai.mcp.load_mcp_servers].\n\nThis is particularly useful when you need to manage multiple MCP servers or want to configure servers externally without modifying code.", "url": "https://ai.pydantic.dev/docs/mcp/client/#loading-mcp-servers-from-configuration", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "Configuration Format", "anchor": "configuration-format", "heading_level": 3, "md_text": "The configuration file should be a JSON file with an `mcpServers` object containing server definitions. Each server is identified by a unique key and contains the configuration for that server type:\n\n```json {title=\"mcp_config.json\"}\n{\n  \"mcpServers\": {\n    \"python-runner\": {\n      \"command\": \"uv\",\n      \"args\": [\"run\", \"mcp-run-python\", \"stdio\"]\n    },\n    \"weather-api\": {\n      \"url\": \"http://localhost:3001/sse\"\n    },\n    \"calculator\": {\n      \"url\": \"http://localhost:8000/mcp\"\n    }\n  }\n}\n```\n\n!!! note\n    The MCP server is only inferred to be an SSE server because of the `/sse` suffix.\n    Any other server with the \"url\" field will be inferred to be a Streamable HTTP server.\n\n    We made this decision given that the SSE transport is deprecated.", "url": "https://ai.pydantic.dev/docs/mcp/client/#configuration-format", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 3, "md_text": "```python {title=\"mcp_config_loader.py\" test=\"skip\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import load_mcp_servers", "url": "https://ai.pydantic.dev/docs/mcp/client/#usage", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "Load all servers from configuration file", "anchor": "load-all-servers-from-configuration-file", "heading_level": 1, "md_text": "servers = load_mcp_servers('mcp_config.json')", "url": "https://ai.pydantic.dev/docs/mcp/client/#load-all-servers-from-configuration-file", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "Create agent with all loaded servers", "anchor": "create-agent-with-all-loaded-servers", "heading_level": 1, "md_text": "agent = Agent('openai:gpt-5', toolsets=servers)\n\nasync def main():\n    result = await agent.run('What is 7 plus 5?')\n    print(result.output)\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/mcp/client/#create-agent-with-all-loaded-servers", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "Tool call customization", "anchor": "tool-call-customization", "heading_level": 2, "md_text": "The MCP servers provide the ability to set a `process_tool_call` which allows\nthe customization of tool call requests and their responses.\n\nA common use case for this is to inject metadata to the requests which the server\ncall needs:\n\n```python {title=\"mcp_process_tool_call.py\"}\nfrom typing import Any\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.mcp import CallToolFunc, MCPServerStdio, ToolResult\nfrom pydantic_ai.models.test import TestModel\n\n\nasync def process_tool_call(\n    ctx: RunContext[int],\n    call_tool: CallToolFunc,\n    name: str,\n    tool_args: dict[str, Any],\n) -> ToolResult:\n    \"\"\"A tool call processor that passes along the deps.\"\"\"\n    return await call_tool(name, tool_args, {'deps': ctx.deps})\n\n\nserver = MCPServerStdio('python', args=['mcp_server.py'], process_tool_call=process_tool_call)\nagent = Agent(\n    model=TestModel(call_tools=['echo_deps']),\n    deps_type=int,\n    toolsets=[server]\n)\n\n\nasync def main():\n    result = await agent.run('Echo with deps set to 42', deps=42)\n    print(result.output)\n    #> {\"echo_deps\":{\"echo\":\"This is an echo message\",\"deps\":42}}\n```\n\nHow to access the metadata is MCP server SDK specific. For example with the [MCP Python\nSDK](https://github.com/modelcontextprotocol/python-sdk), it is accessible via the\n[`ctx: Context`](https://github.com/modelcontextprotocol/python-sdk#context)\nargument that can be included on tool call handlers:\n\n```python {title=\"mcp_server.py\"}\nfrom typing import Any\n\nfrom mcp.server.fastmcp import Context, FastMCP\nfrom mcp.server.session import ServerSession\n\nmcp = FastMCP('Pydantic AI MCP Server')\nlog_level = 'unset'\n\n\n@mcp.tool()\nasync def echo_deps(ctx: Context[ServerSession, None]) -> dict[str, Any]:\n    \"\"\"Echo the run context.\n\n    Args:\n        ctx: Context object containing request and session information.\n\n    Returns:\n        Dictionary with an echo message and the deps.\n    \"\"\"\n    await ctx.info('This is an info message')\n\n    deps: Any = getattr(ctx.request_context.meta, 'deps')\n    return {'echo': 'This is an echo message', 'deps': deps}\n\nif __name__ == '__main__':\n    mcp.run()\n```", "url": "https://ai.pydantic.dev/docs/mcp/client/#tool-call-customization", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "Using Tool Prefixes to Avoid Naming Conflicts", "anchor": "using-tool-prefixes-to-avoid-naming-conflicts", "heading_level": 2, "md_text": "When connecting to multiple MCP servers that might provide tools with the same name, you can use the `tool_prefix` parameter to avoid naming conflicts. This parameter adds a prefix to all tool names from a specific server.\n\nThis allows you to use multiple servers that might have overlapping tool names without conflicts:\n\n```python {title=\"mcp_tool_prefix_http_client.py\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerSSE", "url": "https://ai.pydantic.dev/docs/mcp/client/#using-tool-prefixes-to-avoid-naming-conflicts", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "Create two servers with different prefixes", "anchor": "create-two-servers-with-different-prefixes", "heading_level": 1, "md_text": "weather_server = MCPServerSSE(\n    'http://localhost:3001/sse',\n    tool_prefix='weather'  # Tools will be prefixed with 'weather_'\n)\n\ncalculator_server = MCPServerSSE(\n    'http://localhost:3002/sse',\n    tool_prefix='calc'  # Tools will be prefixed with 'calc_'\n)", "url": "https://ai.pydantic.dev/docs/mcp/client/#create-two-servers-with-different-prefixes", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "- 'calc_get_data'", "anchor": "-calc_get_data", "heading_level": 1, "md_text": "agent = Agent('openai:gpt-5', toolsets=[weather_server, calculator_server])\n```", "url": "https://ai.pydantic.dev/docs/mcp/client/#-calc_get_data", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "Tool metadata", "anchor": "tool-metadata", "heading_level": 2, "md_text": "MCP tools can include metadata that provides additional information about the tool's characteristics, which can be useful when [filtering tools][pydantic_ai.toolsets.FilteredToolset]. The `meta`, `annotations`, and `output_schema` fields can be found on the `metadata` dict on the [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] object that's passed to filter functions.", "url": "https://ai.pydantic.dev/docs/mcp/client/#tool-metadata", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "Custom TLS / SSL configuration", "anchor": "custom-tls-ssl-configuration", "heading_level": 2, "md_text": "In some environments you need to tweak how HTTPS connections are established \u2013\nfor example to trust an internal Certificate Authority, present a client\ncertificate for **mTLS**, or (during local development only!) disable\ncertificate verification altogether.\nAll HTTP-based MCP client classes\n([`MCPServerStreamableHTTP`][pydantic_ai.mcp.MCPServerStreamableHTTP] and\n[`MCPServerSSE`][pydantic_ai.mcp.MCPServerSSE]) expose an `http_client`\nparameter that lets you pass your own pre-configured\n[`httpx.AsyncClient`](https://www.python-httpx.org/async/).\n\n```python {title=\"mcp_custom_tls_client.py\"}\nimport ssl\n\nimport httpx\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerSSE", "url": "https://ai.pydantic.dev/docs/mcp/client/#custom-tls-ssl-configuration", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "Trust an internal / self-signed CA", "anchor": "trust-an-internal-self-signed-ca", "heading_level": 1, "md_text": "ssl_ctx = ssl.create_default_context(cafile='/etc/ssl/private/my_company_ca.pem')", "url": "https://ai.pydantic.dev/docs/mcp/client/#trust-an-internal-self-signed-ca", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "OPTIONAL: if the server requires **mutual TLS** load your client certificate", "anchor": "optional-if-the-server-requires-mutual-tls-load-your-client-certificate", "heading_level": 1, "md_text": "ssl_ctx.load_cert_chain(certfile='/etc/ssl/certs/client.crt', keyfile='/etc/ssl/private/client.key',)\n\nhttp_client = httpx.AsyncClient(\n    verify=ssl_ctx,\n    timeout=httpx.Timeout(10.0),\n)\n\nserver = MCPServerSSE(\n    'http://localhost:3001/sse',\n    http_client=http_client,  # (1)!\n)\nagent = Agent('openai:gpt-5', toolsets=[server])\n\nasync def main():\n    result = await agent.run('How many days between 2000-01-01 and 2025-03-18?')\n    print(result.output)\n    #> There are 9,208 days between January 1, 2000, and March 18, 2025.\n```\n\n1. When you supply `http_client`, Pydantic AI re-uses this client for every\n   request. Anything supported by **httpx** (`verify`, `cert`, custom\n   proxies, timeouts, etc.) therefore applies to all MCP traffic.", "url": "https://ai.pydantic.dev/docs/mcp/client/#optional-if-the-server-requires-mutual-tls-load-your-client-certificate", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "MCP Sampling", "anchor": "mcp-sampling", "heading_level": 2, "md_text": "!!! info \"What is MCP Sampling?\"\n    In MCP [sampling](https://modelcontextprotocol.io/docs/concepts/sampling) is a system by which an MCP server can make LLM calls via the MCP client - effectively proxying requests to an LLM via the client over whatever transport is being used.\n\n    Sampling is extremely useful when MCP servers need to use Gen AI but you don't want to provision them each with their own LLM credentials or when a public MCP server would like the connecting client to pay for LLM calls.\n\n    Confusingly it has nothing to do with the concept of \"sampling\" in observability, or frankly the concept of \"sampling\" in any other domain.\n\n    ??? info \"Sampling Diagram\"\n        Here's a mermaid diagram that may or may not make the data flow clearer:\n\n        ```mermaid\n        sequenceDiagram\n            participant LLM\n            participant MCP_Client as MCP client\n            participant MCP_Server as MCP server\n\n            MCP_Client->>LLM: LLM call\n            LLM->>MCP_Client: LLM tool call response\n\n            MCP_Client->>MCP_Server: tool call\n            MCP_Server->>MCP_Client: sampling \"create message\"\n\n            MCP_Client->>LLM: LLM call\n            LLM->>MCP_Client: LLM text response\n\n            MCP_Client->>MCP_Server: sampling response\n            MCP_Server->>MCP_Client: tool call response\n        ```\n\nPydantic AI supports sampling as both a client and server. See the [server](./server.md#mcp-sampling) documentation for details on how to use sampling within a server.\n\nSampling is automatically supported by Pydantic AI agents when they act as a client.\n\nTo be able to use sampling, an MCP server instance needs to have a [`sampling_model`][pydantic_ai.mcp.MCPServer.sampling_model] set. This can be done either directly on the server using the constructor keyword argument or the property, or by using [`agent.set_mcp_sampling_model()`][pydantic_ai.Agent.set_mcp_sampling_model] to set the agent's model or one specified as an argument as the sampling model on all MCP servers registered with that agent.\n\nLet's say we have an MCP server that wants to use sampling (in this case to generate an SVG as per the tool arguments).\n\n??? example \"Sampling MCP Server\"\n\n    ```python {title=\"generate_svg.py\"}\n    import re\n    from pathlib import Path\n\n    from mcp import SamplingMessage\n    from mcp.server.fastmcp import Context, FastMCP\n    from mcp.types import TextContent\n\n    app = FastMCP()\n\n\n    @app.tool()\n    async def image_generator(ctx: Context, subject: str, style: str) -> str:\n        prompt = f'{subject=} {style=}'\n        # `ctx.session.create_message` is the sampling call\n        result = await ctx.session.create_message(\n            [SamplingMessage(role='user', content=TextContent(type='text', text=prompt))],\n            max_tokens=1_024,\n            system_prompt='Generate an SVG image as per the user input',\n        )\n        assert isinstance(result.content, TextContent)\n\n        path = Path(f'{subject}_{style}.svg')\n        # remove triple backticks if the svg was returned within markdown\n        if m := re.search(r'^```\\w*$(.+?)```$', result.content.text, re.S | re.M):\n            path.write_text(m.group(1))\n        else:\n            path.write_text(result.content.text)\n        return f'See {path}'\n\n\n    if __name__ == '__main__':\n        # run the server via stdio\n        app.run()\n    ```\n\nUsing this server with an `Agent` will automatically allow sampling:\n\n```python {title=\"sampling_mcp_client.py\" requires=\"generate_svg.py\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\n\nserver = MCPServerStdio('python', args=['generate_svg.py'])\nagent = Agent('openai:gpt-5', toolsets=[server])\n\n\nasync def main():\n    agent.set_mcp_sampling_model()\n    result = await agent.run('Create an image of a robot in a punk style.')\n    print(result.output)\n    #> Image file written to robot_punk.svg.\n```\n\n_(This example is complete, it can be run \"as is\")_\n\nYou can disallow sampling by setting [`allow_sampling=False`][pydantic_ai.mcp.MCPServer.allow_sampling] when creating the server reference, e.g.:\n\n```python {title=\"sampling_disallowed.py\" hl_lines=\"6\"}\nfrom pydantic_ai.mcp import MCPServerStdio\n\nserver = MCPServerStdio(\n    'python',\n    args=['generate_svg.py'],\n    allow_sampling=False,\n)\n```", "url": "https://ai.pydantic.dev/docs/mcp/client/#mcp-sampling", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "Elicitation", "anchor": "elicitation", "heading_level": 2, "md_text": "In MCP, [elicitation](https://modelcontextprotocol.io/docs/concepts/elicitation) allows a server to request for [structured input](https://modelcontextprotocol.io/specification/2025-06-18/client/elicitation#supported-schema-types) from the client for missing or additional context during a session.\n\nElicitation let models essentially say \"Hold on - I need to know X before i can continue\" rather than requiring everything upfront or taking a shot in the dark.", "url": "https://ai.pydantic.dev/docs/mcp/client/#elicitation", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "How Elicitation works", "anchor": "how-elicitation-works", "heading_level": 3, "md_text": "Elicitation introduces a new protocol message type called [`ElicitRequest`](https://modelcontextprotocol.io/specification/2025-06-18/schema#elicitrequest), which is sent from the server to the client when it needs additional information. The client can then respond with an [`ElicitResult`](https://modelcontextprotocol.io/specification/2025-06-18/schema#elicitresult) or an `ErrorData` message.\n\nHere's a typical interaction:\n\n- User makes a request to the MCP server (e.g. \"Book a table at that Italian place\")\n- The server identifies that it needs more information (e.g. \"Which Italian place?\", \"What date and time?\")\n- The server sends an `ElicitRequest` to the client asking for the missing information.\n- The client receives the request, presents it to the user (e.g. via a terminal prompt, GUI dialog, or web interface).\n- User provides the requested information, `decline` or `cancel` the request.\n- The client sends an `ElicitResult` back to the server with the user's response.\n- With the structured data, the server can continue processing the original request.\n\nThis allows for a more interactive and user-friendly experience, especially for multi-staged workflows. Instead of requiring all information upfront, the server can ask for it as needed, making the interaction feel more natural.", "url": "https://ai.pydantic.dev/docs/mcp/client/#how-elicitation-works", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "Setting up Elicitation", "anchor": "setting-up-elicitation", "heading_level": 3, "md_text": "To enable elicitation, provide an [`elicitation_callback`][pydantic_ai.mcp.MCPServer.elicitation_callback] function when creating your MCP server instance:\n\n```python {title=\"restaurant_server.py\"}\nfrom mcp.server.fastmcp import Context, FastMCP\nfrom pydantic import BaseModel, Field\n\nmcp = FastMCP(name='Restaurant Booking')\n\n\nclass BookingDetails(BaseModel):\n    \"\"\"Schema for restaurant booking information.\"\"\"\n\n    restaurant: str = Field(description='Choose a restaurant')\n    party_size: int = Field(description='Number of people', ge=1, le=8)\n    date: str = Field(description='Reservation date (DD-MM-YYYY)')\n\n\n@mcp.tool()\nasync def book_table(ctx: Context) -> str:\n    \"\"\"Book a restaurant table with user input.\"\"\"\n    # Ask user for booking details using Pydantic schema\n    result = await ctx.elicit(message='Please provide your booking details:', schema=BookingDetails)\n\n    if result.action == 'accept' and result.data:\n        booking = result.data\n        return f'\u2705 Booked table for {booking.party_size} at {booking.restaurant} on {booking.date}'\n    elif result.action == 'decline':\n        return 'No problem! Maybe another time.'\n    else:  # cancel\n        return 'Booking cancelled.'\n\n\nif __name__ == '__main__':\n    mcp.run(transport='stdio')\n```\n\nThis server demonstrates elicitation by requesting structured booking details from the client when the `book_table` tool is called. Here's how to create a client that handles these elicitation requests:\n\n```python {title=\"client_example.py\" requires=\"restaurant_server.py\" test=\"skip\"}\nimport asyncio\nfrom typing import Any\n\nfrom mcp.client.session import ClientSession\nfrom mcp.shared.context import RequestContext\nfrom mcp.types import ElicitRequestParams, ElicitResult\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\n\n\nasync def handle_elicitation(\n    context: RequestContext[ClientSession, Any, Any],\n    params: ElicitRequestParams,\n) -> ElicitResult:\n    \"\"\"Handle elicitation requests from MCP server.\"\"\"\n    print(f'\\n{params.message}')\n\n    if not params.requestedSchema:\n        response = input('Response: ')\n        return ElicitResult(action='accept', content={'response': response})\n\n    # Collect data for each field\n    properties = params.requestedSchema['properties']\n    data = {}\n\n    for field, info in properties.items():\n        description = info.get('description', field)\n\n        value = input(f'{description}: ')\n\n        # Convert to proper type based on JSON schema\n        if info.get('type') == 'integer':\n            data[field] = int(value)\n        else:\n            data[field] = value\n\n    # Confirm\n    confirm = input('\\nConfirm booking? (y/n/c): ').lower()\n\n    if confirm == 'y':\n        print('Booking details:', data)\n        return ElicitResult(action='accept', content=data)\n    elif confirm == 'n':\n        return ElicitResult(action='decline')\n    else:\n        return ElicitResult(action='cancel')", "url": "https://ai.pydantic.dev/docs/mcp/client/#setting-up-elicitation", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "Set up MCP server connection", "anchor": "set-up-mcp-server-connection", "heading_level": 1, "md_text": "restaurant_server = MCPServerStdio(\n    'python', args=['restaurant_server.py'], elicitation_callback=handle_elicitation\n)", "url": "https://ai.pydantic.dev/docs/mcp/client/#set-up-mcp-server-connection", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "Create agent", "anchor": "create-agent", "heading_level": 1, "md_text": "agent = Agent('openai:gpt-5', toolsets=[restaurant_server])\n\n\nasync def main():\n    \"\"\"Run the agent to book a restaurant table.\"\"\"\n    result = await agent.run('Book me a table')\n    print(f'\\nResult: {result.output}')\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```", "url": "https://ai.pydantic.dev/docs/mcp/client/#create-agent", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "Supported Schema Types", "anchor": "supported-schema-types", "heading_level": 3, "md_text": "MCP elicitation supports string, number, boolean, and enum types with flat object structures only. These limitations ensure reliable cross-client compatibility. See [supported schema types](https://modelcontextprotocol.io/specification/2025-06-18/client/elicitation#supported-schema-types) for details.", "url": "https://ai.pydantic.dev/docs/mcp/client/#supported-schema-types", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "Security", "anchor": "security", "heading_level": 3, "md_text": "MCP Elicitation requires careful handling - servers must not request sensitive information, and clients must implement user approval controls with clear explanations. See [security considerations](https://modelcontextprotocol.io/specification/2025-06-18/client/elicitation#security-considerations) for details.", "url": "https://ai.pydantic.dev/docs/mcp/client/#security", "page": "docs/mcp/client", "source_site": "pydantic_ai"}
{"title": "Model Context Protocol (MCP)", "anchor": "model-context-protocol-mcp", "heading_level": 1, "md_text": "Pydantic AI supports [Model Context Protocol (MCP)](https://modelcontextprotocol.io) in multiple ways:\n\n1. [Agents](../agents.md) can connect to MCP servers and use their tools using three different methods:\n    1. Pydantic AI can act as an MCP client and connect directly to local and remote MCP servers. [Learn more](client.md) about [`MCPServer`][pydantic_ai.mcp.MCPServer].\n    2. Pydantic AI can use the [FastMCP Client](https://gofastmcp.com/clients/client/) to connect to local and remote MCP servers, whether or not they're built using [FastMCP Server](https://gofastmcp.com/servers). [Learn more](fastmcp-client.md) about [`FastMCPToolset`][pydantic_ai.toolsets.fastmcp.FastMCPToolset].\n    3. Some model providers can themselves connect to remote MCP servers using a \"built-in tool\". [Learn more](../builtin-tools.md#mcp-server-tool) about [`MCPServerTool`][pydantic_ai.builtin_tools.MCPServerTool].\n2. Agents can be used within MCP servers. [Learn more](server.md)", "url": "https://ai.pydantic.dev/docs/mcp/overview/#model-context-protocol-mcp", "page": "docs/mcp/overview", "source_site": "pydantic_ai"}
{"title": "What is MCP?", "anchor": "what-is-mcp", "heading_level": 2, "md_text": "The Model Context Protocol is a standardized protocol that allow AI applications (including programmatic agents like Pydantic AI, coding agents like [cursor](https://www.cursor.com/), and desktop applications like [Claude Desktop](https://claude.ai/download)) to connect to external tools and services using a common interface.\n\nAs with other protocols, the dream of MCP is that a wide range of applications can speak to each other without the need for specific integrations.\n\nThere is a great list of MCP servers at [github.com/modelcontextprotocol/servers](https://github.com/modelcontextprotocol/servers).\n\nSome examples of what this means:\n\n- Pydantic AI could use a web search service implemented as an MCP server to implement a deep research agent\n- Cursor could connect to the [Pydantic Logfire](https://github.com/pydantic/logfire-mcp) MCP server to search logs, traces and metrics to gain context while fixing a bug\n- Pydantic AI, or any other MCP client could connect to our [Run Python](https://github.com/pydantic/mcp-run-python) MCP server to run arbitrary Python code in a sandboxed environment", "url": "https://ai.pydantic.dev/docs/mcp/overview/#what-is-mcp", "page": "docs/mcp/overview", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.durable_exec`", "anchor": "pydantic_aidurable_exec", "heading_level": 1, "md_text": "::: pydantic_ai.durable_exec.temporal\n\n::: pydantic_ai.durable_exec.dbos\n\n::: pydantic_ai.durable_exec.prefect", "url": "https://ai.pydantic.dev/docs/api/durable_exec/#pydantic_aidurable_exec", "page": "docs/api/durable_exec", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.builtin_tools`", "anchor": "pydantic_aibuiltin_tools", "heading_level": 1, "md_text": "::: pydantic_ai.builtin_tools", "url": "https://ai.pydantic.dev/docs/api/builtin_tools/#pydantic_aibuiltin_tools", "page": "docs/api/builtin_tools", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.output`", "anchor": "pydantic_aioutput", "heading_level": 1, "md_text": "::: pydantic_ai.output\n    options:\n        inherited_members: true\n        members:\n            - OutputDataT\n            - ToolOutput\n            - NativeOutput\n            - PromptedOutput\n            - TextOutput\n            - StructuredDict\n            - DeferredToolRequests", "url": "https://ai.pydantic.dev/docs/api/output/#pydantic_aioutput", "page": "docs/api/output", "source_site": "pydantic_ai"}
{"title": "`fasta2a`", "anchor": "fasta2a", "heading_level": 1, "md_text": "::: fasta2a\n\n::: fasta2a.schema\n\n::: fasta2a.client", "url": "https://ai.pydantic.dev/docs/api/fasta2a/#fasta2a", "page": "docs/api/fasta2a", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.ag_ui`", "anchor": "pydantic_aiag_ui", "heading_level": 1, "md_text": "::: pydantic_ai.ag_ui", "url": "https://ai.pydantic.dev/docs/api/ag_ui/#pydantic_aiag_ui", "page": "docs/api/ag_ui", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.run`", "anchor": "pydantic_airun", "heading_level": 1, "md_text": "::: pydantic_ai.run", "url": "https://ai.pydantic.dev/docs/api/run/#pydantic_airun", "page": "docs/api/run", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.usage`", "anchor": "pydantic_aiusage", "heading_level": 1, "md_text": "::: pydantic_ai.usage", "url": "https://ai.pydantic.dev/docs/api/usage/#pydantic_aiusage", "page": "docs/api/usage", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.settings`", "anchor": "pydantic_aisettings", "heading_level": 1, "md_text": "::: pydantic_ai.settings\n    options:\n      inherited_members: true\n      members:\n        - ModelSettings\n        - UsageLimits", "url": "https://ai.pydantic.dev/docs/api/settings/#pydantic_aisettings", "page": "docs/api/settings", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.direct`", "anchor": "pydantic_aidirect", "heading_level": 1, "md_text": "::: pydantic_ai.direct", "url": "https://ai.pydantic.dev/docs/api/direct/#pydantic_aidirect", "page": "docs/api/direct", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.result`", "anchor": "pydantic_airesult", "heading_level": 1, "md_text": "::: pydantic_ai.result\n    options:\n        inherited_members: true\n        members:\n            - StreamedRunResult", "url": "https://ai.pydantic.dev/docs/api/result/#pydantic_airesult", "page": "docs/api/result", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.tools`", "anchor": "pydantic_aitools", "heading_level": 1, "md_text": "::: pydantic_ai.tools", "url": "https://ai.pydantic.dev/docs/api/tools/#pydantic_aitools", "page": "docs/api/tools", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.profiles`", "anchor": "pydantic_aiprofiles", "heading_level": 1, "md_text": "::: pydantic_ai.profiles.ModelProfile\n\n::: pydantic_ai.profiles.openai\n\n::: pydantic_ai.profiles.anthropic\n\n::: pydantic_ai.profiles.google\n\n::: pydantic_ai.profiles.meta\n\n::: pydantic_ai.profiles.amazon\n\n::: pydantic_ai.profiles.deepseek\n\n::: pydantic_ai.profiles.grok\n\n::: pydantic_ai.profiles.mistral\n\n::: pydantic_ai.profiles.qwen", "url": "https://ai.pydantic.dev/docs/api/profiles/#pydantic_aiprofiles", "page": "docs/api/profiles", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.common_tools`", "anchor": "pydantic_aicommon_tools", "heading_level": 1, "md_text": "::: pydantic_ai.common_tools.duckduckgo\n\n::: pydantic_ai.common_tools.tavily", "url": "https://ai.pydantic.dev/docs/api/common_tools/#pydantic_aicommon_tools", "page": "docs/api/common_tools", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.exceptions`", "anchor": "pydantic_aiexceptions", "heading_level": 1, "md_text": "::: pydantic_ai.exceptions", "url": "https://ai.pydantic.dev/docs/api/exceptions/#pydantic_aiexceptions", "page": "docs/api/exceptions", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.ext`", "anchor": "pydantic_aiext", "heading_level": 1, "md_text": "::: pydantic_ai.ext.langchain\n\n::: pydantic_ai.ext.aci", "url": "https://ai.pydantic.dev/docs/api/ext/#pydantic_aiext", "page": "docs/api/ext", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.messages`", "anchor": "pydantic_aimessages", "heading_level": 1, "md_text": "The structure of [`ModelMessage`][pydantic_ai.messages.ModelMessage] can be shown as a graph:\n\n```mermaid\ngraph RL\n    SystemPromptPart(SystemPromptPart) --- ModelRequestPart\n    UserPromptPart(UserPromptPart) --- ModelRequestPart\n    ToolReturnPart(ToolReturnPart) --- ModelRequestPart\n    RetryPromptPart(RetryPromptPart) --- ModelRequestPart\n    TextPart(TextPart) --- ModelResponsePart\n    ToolCallPart(ToolCallPart) --- ModelResponsePart\n    ThinkingPart(ThinkingPart) --- ModelResponsePart\n    ModelRequestPart(\"ModelRequestPart<br>(Union)\") --- ModelRequest\n    ModelRequest(\"ModelRequest(parts=list[...])\") --- ModelMessage\n    ModelResponsePart(\"ModelResponsePart<br>(Union)\") --- ModelResponse\n    ModelResponse(\"ModelResponse(parts=list[...])\") --- ModelMessage(\"ModelMessage<br>(Union)\")\n```\n\n::: pydantic_ai.messages", "url": "https://ai.pydantic.dev/docs/api/messages/#pydantic_aimessages", "page": "docs/api/messages", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.toolsets`", "anchor": "pydantic_aitoolsets", "heading_level": 1, "md_text": "::: pydantic_ai.toolsets\n    options:\n        members:\n        - AbstractToolset\n        - CombinedToolset\n        - ExternalToolset\n        - ApprovalRequiredToolset\n        - FilteredToolset\n        - FunctionToolset\n        - PrefixedToolset\n        - RenamedToolset\n        - PreparedToolset\n        - WrapperToolset\n        - ToolsetFunc\n\n::: pydantic_ai.toolsets.fastmcp", "url": "https://ai.pydantic.dev/docs/api/toolsets/#pydantic_aitoolsets", "page": "docs/api/toolsets", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.retries`", "anchor": "pydantic_airetries", "heading_level": 1, "md_text": "::: pydantic_ai.retries", "url": "https://ai.pydantic.dev/docs/api/retries/#pydantic_airetries", "page": "docs/api/retries", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.providers`", "anchor": "pydantic_aiproviders", "heading_level": 1, "md_text": "::: pydantic_ai.providers.Provider\n\n::: pydantic_ai.providers.google\n\n::: pydantic_ai.providers.openai\n\n::: pydantic_ai.providers.deepseek\n\n::: pydantic_ai.providers.bedrock\n\n::: pydantic_ai.providers.groq\n\n::: pydantic_ai.providers.azure\n\n::: pydantic_ai.providers.cohere\n\n::: pydantic_ai.providers.cerebras.CerebrasProvider\n\n::: pydantic_ai.providers.mistral.MistralProvider\n\n::: pydantic_ai.providers.fireworks.FireworksProvider\n\n::: pydantic_ai.providers.grok.GrokProvider\n\n::: pydantic_ai.providers.together.TogetherProvider\n\n::: pydantic_ai.providers.heroku.HerokuProvider\n\n::: pydantic_ai.providers.github.GitHubProvider\n\n::: pydantic_ai.providers.openrouter.OpenRouterProvider\n\n::: pydantic_ai.providers.vercel.VercelProvider\n\n::: pydantic_ai.providers.huggingface.HuggingFaceProvider\n\n::: pydantic_ai.providers.moonshotai.MoonshotAIProvider\n\n::: pydantic_ai.providers.ollama.OllamaProvider\n\n::: pydantic_ai.providers.litellm.LiteLLMProvider\n\n::: pydantic_ai.providers.nebius.NebiusProvider\n\n::: pydantic_ai.providers.ovhcloud.OVHcloudProvider", "url": "https://ai.pydantic.dev/docs/api/providers/#pydantic_aiproviders", "page": "docs/api/providers", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.format_prompt`", "anchor": "pydantic_aiformat_prompt", "heading_level": 1, "md_text": "::: pydantic_ai.format_prompt\n    options:\n        members:\n            - format_as_xml", "url": "https://ai.pydantic.dev/docs/api/format_prompt/#pydantic_aiformat_prompt", "page": "docs/api/format_prompt", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.mcp`", "anchor": "pydantic_aimcp", "heading_level": 1, "md_text": "::: pydantic_ai.mcp", "url": "https://ai.pydantic.dev/docs/api/mcp/#pydantic_aimcp", "page": "docs/api/mcp", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.agent`", "anchor": "pydantic_aiagent", "heading_level": 1, "md_text": "::: pydantic_ai.agent\n    options:\n        members:\n            - Agent\n            - AbstractAgent\n            - WrapperAgent\n            - AgentRun\n            - AgentRunResult\n            - EndStrategy\n            - RunOutputDataT\n            - capture_run_messages\n            - InstrumentationSettings\n            - EventStreamHandler", "url": "https://ai.pydantic.dev/docs/api/agent/#pydantic_aiagent", "page": "docs/api/agent", "source_site": "pydantic_ai"}
{"title": "Slack Lead Qualifier with Modal", "anchor": "slack-lead-qualifier-with-modal", "heading_level": 1, "md_text": "In this example, we're going to build an agentic app that:\n\n- automatically researches each new member that joins a company's public Slack community to see how good of a fit they are for the company's commercial product,\n- sends this analysis into a (private) Slack channel, and\n- sends a daily summary of the top 5 leads from the previous 24 hours into a (different) Slack channel.\n\nWe'll be deploying the app on [Modal](https://modal.com), as it lets you use Python to define an app with web endpoints, scheduled functions, and background functions, and deploy them with a CLI, without needing to set up or manage any infrastructure. It's a great way to lower the barrier for people in your organization to start building and deploying AI agents to make their jobs easier.\n\nWe also add [Pydantic Logfire](https://pydantic.dev/logfire) to get observability into the app and agent as they're running in response to webhooks and the schedule", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#slack-lead-qualifier-with-modal", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "Screenshots", "anchor": "screenshots", "heading_level": 2, "md_text": "This is what the analysis sent into Slack will look like:\n\n![Slack message](../img/slack-lead-qualifier-slack.png)\n\nThis is what the corresponding trace in [Logfire](https://pydantic.dev/logfire) will look like:\n\n![Logfire trace](../img/slack-lead-qualifier-logfire.png)\n\nAll of these entries can be clicked on to get more details about what happened at that step, including the full conversation with the LLM and HTTP requests and responses.", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#screenshots", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "Prerequisites", "anchor": "prerequisites", "heading_level": 2, "md_text": "If you just want to see the code without actually going through the effort of setting up the bits necessary to run it, feel free to [jump ahead](#the-code).", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#prerequisites", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "Slack app", "anchor": "slack-app", "heading_level": 3, "md_text": "You need to have a Slack workspace and the necessary permissions to create apps.\n\n2. Create a new Slack app using the instructions at <https://docs.slack.dev/quickstart>.\n    1. In step 2, \"Requesting scopes\", request the following scopes:\n        - [`users.read`](https://docs.slack.dev/reference/scopes/users.read)\n        - [`users.read.email`](https://docs.slack.dev/reference/scopes/users.read.email)\n        - [`users.profile.read`](https://docs.slack.dev/reference/scopes/users.profile.read)\n    2. In step 3, \"Installing and authorizing the app\", note down the Access Token as we're going to need to store it as a Secret in Modal.\n    3. You can skip steps 4 and 5. We're going to need to subscribe to the `team_join` event, but at this point you don't have a webhook URL yet.\n1. Create the channels the app will post into, and add the Slack app to them:\n    - `#new-slack-leads`\n    - `#daily-slack-leads-summary`\n\n    These names are hard-coded in the example. If you want to use different channels, you can clone the repo and change them in `examples/pydantic_examples/slack_lead_qualifier/functions.py`.", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#slack-app", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "Logfire Write Token", "anchor": "logfire-write-token", "heading_level": 3, "md_text": "1. If you don't have a Logfire account yet, create one on <https://logfire-us.pydantic.dev/>.\n2. Create a new project named, for example, `slack-lead-qualifier`.\n3. Generate a new Write Token and note it down, as we're going to need to store it as a Secret in Modal.", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#logfire-write-token", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "OpenAI API Key", "anchor": "openai-api-key", "heading_level": 3, "md_text": "1. If you don't have an OpenAI account yet, create one on <https://platform.openai.com/>.\n2. Create a new API Key in Settings and note it down, as we're going to need to store it as a Secret in Modal.", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#openai-api-key", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "Modal account", "anchor": "modal-account", "heading_level": 3, "md_text": "1. If you don't have a Modal account yet, create one on <https://modal.com/signup>.\n2. Create 3 Secrets of type \"Custom\" on <https://modal.com/secrets>:\n    - Name: `slack`, key: `SLACK_API_KEY`, value: the Slack Access Token you generated earlier\n    - Name: `logfire`, key: `LOGFIRE_TOKEN`, value: the Logfire Write Token you generated earlier\n    - Name: `openai`, key: `OPENAI_API_KEY`, value: the OpenAI API Key you generated earlier", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#modal-account", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 2, "md_text": "1. Make sure you have the [dependencies installed](./setup.md#usage).\n\n2. Authenticate with Modal:\n\n    ```bash\n    python/uv-run -m modal setup\n    ```\n\n3. Run the example as an [ephemeral Modal app](https://modal.com/docs/guide/apps#ephemeral-apps), meaning it will only run until you quit it using Ctrl+C:\n\n    ```bash\n    python/uv-run -m modal serve -m pydantic_ai_examples.slack_lead_qualifier.modal\n    ```\n\n4. Note down the URL after `Created web function web_app =>`, this is your webhook endpoint URL.\n\n5. Go back to <https://docs.slack.dev/quickstart> and follow step 4, \"Configuring the app for event listening\", to subscribe to the `team_join` event with the webhook endpoint URL you noted down as the Request URL.\n\nNow when someone new (possibly you with a throwaway email) joins the Slack workspace, you'll see the webhook event being processed in the terminal where you ran `modal serve` and in the Logfire Live view, and after waiting a few seconds you should see the result appear in the `#new-slack-leads` Slack channel!\n\n!!! note \"Faking a Slack signup\"\n    You can also fake a Slack signup event and try out the agent like this, with any name or email you please:\n\n    ```bash\n    curl -X POST <webhook endpoint URL> \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"type\": \"event_callback\",\n        \"event\": {\n            \"type\": \"team_join\",\n            \"user\": {\n                \"profile\": {\n                    \"email\": \"samuel@pydantic.dev\",\n                    \"first_name\": \"Samuel\",\n                    \"last_name\": \"Colvin\",\n                    \"display_name\": \"Samuel Colvin\"\n                }\n            }\n        }\n    }'\n    ```\n\n!!! note \"Deploying to production\"\n    If you'd like to deploy this app into your Modal workspace in a persistent fashion, you can use this command:\n\n    ```bash\n    python/uv-run -m modal deploy -m pydantic_ai_examples.slack_lead_qualifier.modal\n    ```\n\n    You'll likely want to [download the code](https://github.com/pydantic/pydantic-ai/tree/main/examples/pydantic_ai_examples/slack_lead_qualifier) first, put it in a new repo, and then do [continuous deployment](https://modal.com/docs/guide/continuous-deployment#github-actions) using GitHub Actions.\n\n    Don't forget to update the Slack event request URL to the new persistent URL! You'll also want to modify the [instructions for the agent](#agent) to your own situation.", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#usage", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "The code", "anchor": "the-code", "heading_level": 2, "md_text": "We're going to start with the basics, and then gradually build up into the full app.", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#the-code", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "`Profile`", "anchor": "profile", "heading_level": 4, "md_text": "First, we define a [Pydantic](https://docs.pydantic.dev) model that represents a Slack user profile. These are the fields we get from the [`team_join`](https://docs.slack.dev/reference/events/team_join) event that's sent to the webhook endpoint that we'll define in a bit.\n\n```snippet {path=\"/examples/pydantic_ai_examples/slack_lead_qualifier/models.py\" fragment=\"profile\"}```\n\nWe also define a `Profile.as_prompt()` helper method that uses [`format_as_xml`][pydantic_ai.format_prompt.format_as_xml] to turn the profile into a string that can be sent to the model.\n\n```snippet {path=\"/examples/pydantic_ai_examples/slack_lead_qualifier/models.py\" fragment=\"import-format_as_xml profile-intro profile-as_prompt\"}```", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#profile", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "`Analysis`", "anchor": "analysis", "heading_level": 4, "md_text": "The second model we'll need represents the result of the analysis that the agent will perform. We include docstrings to provide additional context to the model on what these fields should contain.\n\n```snippet {path=\"/examples/pydantic_ai_examples/slack_lead_qualifier/models.py\" fragment=\"analysis\"}```\n\nWe also define a `Analysis.as_slack_blocks()` helper method that turns the analysis into some [Slack blocks](https://api.slack.com/reference/block-kit/blocks) that can be sent to the Slack API to post a new message.\n\n```snippet {path=\"/examples/pydantic_ai_examples/slack_lead_qualifier/models.py\" fragment=\"analysis-intro analysis-as_slack_blocks\"}```", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#analysis", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "Agent", "anchor": "agent", "heading_level": 3, "md_text": "Now it's time to get into Pydantic AI and define the agent that will do the actual analysis!\n\nWe specify the model we'll use (`openai:gpt-5`), provide [instructions](../agents.md#instructions), give the agent access to the [DuckDuckGo search tool](../common-tools.md#duckduckgo-search-tool), and tell it to output either an `Analysis` or `None` using the [Native Output](../output.md#native-output) structured output mode.\n\nThe real meat of the app is in the instructions that tell the agent how to evaluate each new Slack member. If you plan to use this app yourself, you'll of course want to modify them to your own situation.\n\n```snippet {path=\"/examples/pydantic_ai_examples/slack_lead_qualifier/agent.py\" fragment=\"imports agent\"}```", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#agent", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "`analyze_profile`", "anchor": "analyze_profile", "heading_level": 4, "md_text": "We also define a `analyze_profile` helper function that takes a `Profile`, runs the agent, and returns an `Analysis` (or `None`), and instrument it using [Logfire](../logfire.md).\n\n```snippet {path=\"/examples/pydantic_ai_examples/slack_lead_qualifier/agent.py\" fragment=\"analyze_profile\"}```", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#analyze_profile", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "Analysis store", "anchor": "analysis-store", "heading_level": 3, "md_text": "The next building block we'll need is a place to store all the analyses that have been done so that we can look them up when we send the daily summary.\n\nFortunately, Modal provides us with a convenient way to store some data that can be read back in a subsequent Modal run (webhook or scheduled): [`modal.Dict`](https://modal.com/docs/reference/modal.Dict).\n\nWe define some convenience methods to easily add, list, and clear analyses.\n\n```snippet {path=\"/examples/pydantic_ai_examples/slack_lead_qualifier/store.py\" fragment=\"import-modal analysis_store\"}```\n\n!!! note\n    Note that `# type: ignore` on the last line -- unfortunately `modal` does not fully define its types, so we need this to stop our static type checker `pyright`, which we run over all Pydantic AI code including examples, from complaining.", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#analysis-store", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "Send Slack message", "anchor": "send-slack-message", "heading_level": 3, "md_text": "Next, we'll need a way to actually send a Slack message, so we define a simple function that uses Slack's [`chat.postMessage`](https://api.slack.com/methods/chat.postMessage) API.\n\n```snippet {path=\"/examples/pydantic_ai_examples/slack_lead_qualifier/slack.py\" fragment=\"send_slack_message\"}```", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#send-slack-message", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "Features", "anchor": "features", "heading_level": 3, "md_text": "Now we can start putting these building blocks together to implement the actual features we want!", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#features", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "`process_slack_member`", "anchor": "process_slack_member", "heading_level": 4, "md_text": "This function takes a [`Profile`](#profile), [analyzes](#analyze_profile) it using the agent, adds it to the [`AnalysisStore`](#analysis-store), and [sends](#send-slack-message) the analysis into the `#new-slack-leads` channel.\n\n```snippet {path=\"/examples/pydantic_ai_examples/slack_lead_qualifier/functions.py\" fragment=\"imports constant-new_lead_channel process_slack_member\"}```", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#process_slack_member", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "`send_daily_summary`", "anchor": "send_daily_summary", "heading_level": 4, "md_text": "This function list all of the analyses in the [`AnalysisStore`](#analysis-store), takes the top 5 by relevance, [sends](#send-slack-message) them into the `#daily-slack-leads-summary` channel, and clears the `AnalysisStore` so that the next daily run won't process these analyses again.\n\n```snippet {path=\"/examples/pydantic_ai_examples/slack_lead_qualifier/functions.py\" fragment=\"imports-daily_summary constant-daily_summary_channel send_daily_summary\"}```", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#send_daily_summary", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "Web app", "anchor": "web-app", "heading_level": 3, "md_text": "As it stands, neither of these functions are actually being called from anywhere.\n\nLet's implement a [FastAPI](https://fastapi.tiangolo.com/) endpoint to handle the `team_join` Slack webhook (also known as the [Slack Events API](https://docs.slack.dev/apis/events-api)) and call the [`process_slack_member`](#process_slack_member) function we just defined. We also instrument FastAPI using Logfire for good measure.\n\n```snippet {path=\"/examples/pydantic_ai_examples/slack_lead_qualifier/app.py\" fragment=\"app\"}```", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#web-app", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "`process_slack_member` with Modal", "anchor": "process_slack_member-with-modal", "heading_level": 4, "md_text": "I was a little sneaky there -- we're not actually calling the [`process_slack_member`](#process_slack_member) function we defined in `functions.py` directly, as Slack requires webhooks to respond within 3 seconds, and we need a bit more time than that to talk to the LLM, do some web searches, and send the Slack message.\n\nInstead, we're calling the following function defined alongside the app, which uses Modal's [`modal.Function.spawn`](https://modal.com/docs/reference/modal.Function#spawn) feature to run a function in the background. (If you're curious what the Modal side of this function looks like, you can [jump ahead](#backgrounded-process_slack_member).)\n\nBecause `modal.py` (which we'll see in the next section) imports `app.py`, we import from `modal.py` inside the function definition because doing so at the top level would have resulted in a circular import error.\n\nWe also pass along the current Logfire context to get [Distributed Tracing](https://logfire.pydantic.dev/docs/how-to-guides/distributed-tracing/), meaning that the background function execution will show up nested under the webhook request trace, so that we have everything related to that request in one place.\n\n```snippet {path=\"/examples/pydantic_ai_examples/slack_lead_qualifier/app.py\" fragment=\"process_slack_member\"}```", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#process_slack_member-with-modal", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "Modal app", "anchor": "modal-app", "heading_level": 3, "md_text": "Now let's see how easy Modal makes it to deploy all of this.", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#modal-app", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "Set up Modal", "anchor": "set-up-modal", "heading_level": 4, "md_text": "The first thing we do is define the Modal app, by specifying the base image to use (Debian with Python 3.13), all the Python packages it needs, and all of the secrets defined in the Modal interface that need to be made available during runtime.\n\n```snippet {path=\"/examples/pydantic_ai_examples/slack_lead_qualifier/modal.py\" fragment=\"setup_modal\"}```", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#set-up-modal", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "Set up Logfire", "anchor": "set-up-logfire", "heading_level": 4, "md_text": "Next, we define a function to set up Logfire instrumentation for Pydantic AI and HTTPX.\n\nWe cannot do this at the top level of the file, as the requested packages (like `logfire`) will only be available within functions running on Modal (like the ones we'll define next). This file, `modal.py`, runs on your local machine and only has access to the `modal` package.\n\n```snippet {path=\"/examples/pydantic_ai_examples/slack_lead_qualifier/modal.py\" fragment=\"setup_logfire\"}```", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#set-up-logfire", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "Web app", "anchor": "web-app", "heading_level": 4, "md_text": "To deploy a [web endpoint](https://modal.com/docs/guide/webhooks) on Modal, we simply define a function that returns an ASGI app (like FastAPI) and decorate it with `@app.function()` and `@modal.asgi_app()`.\n\nThis `web_app` function will be run on Modal, so inside the function we can call the `setup_logfire` function that requires the `logfire` package, and import `app.py` which uses the other requested packages.\n\nBy default, Modal spins up a container to handle a function call (like a web request) on-demand, meaning there's a little bit of startup time to each request. However, Slack requires webhooks to respond within 3 seconds, so we specify `min_containers=1` to keep the web endpoint running and ready to answer requests at all times. This is a bit annoying and wasteful, but fortunately [Modal's pricing](https://modal.com/pricing) is pretty reasonable, you get $30 free monthly compute, and they offer up to $50k in free credits for startup and academic researchers.\n\n```snippet {path=\"/examples/pydantic_ai_examples/slack_lead_qualifier/modal.py\" fragment=\"web_app\"}```\n\n!!! note\n    Note that `# type: ignore` on the `@modal.asgi_app()` line -- unfortunately `modal` does not fully define its types, so we need this to stop our static type checker `pyright`, which we run over all Pydantic AI code including examples, from complaining.", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#web-app", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "Scheduled `send_daily_summary`", "anchor": "scheduled-send_daily_summary", "heading_level": 4, "md_text": "To define a [scheduled function](https://modal.com/docs/guide/cron), we can use the `@app.function()` decorator with a `schedule` argument. This Modal function will call our imported [`send_daily_summary`](#send_daily_summary) function every day at 8 am UTC.\n\n```snippet {path=\"/examples/pydantic_ai_examples/slack_lead_qualifier/modal.py\" fragment=\"send_daily_summary\"}```", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#scheduled-send_daily_summary", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "Backgrounded `process_slack_member`", "anchor": "backgrounded-process_slack_member", "heading_level": 4, "md_text": "Finally, we define a Modal function that wraps our [`process_slack_member`](#process_slack_member) function, so that it can run in the background.\n\nAs you'll remember from when we [spawned this function from the web app](#process_slack_member-with-modal), we passed along the Logfire context to get [Distributed Tracing](https://logfire.pydantic.dev/docs/how-to-guides/distributed-tracing/), so we need to attach it here.\n\n```snippet {path=\"/examples/pydantic_ai_examples/slack_lead_qualifier/modal.py\" fragment=\"process_slack_member\"}```", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#backgrounded-process_slack_member", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "Conclusion", "anchor": "conclusion", "heading_level": 2, "md_text": "And that's it! Now, assuming you've met the [prerequisites](#prerequisites), you can run or deploy the app using the commands under [usage](#usage).", "url": "https://ai.pydantic.dev/docs/examples/slack-lead-qualifier/#conclusion", "page": "docs/examples/slack-lead-qualifier", "source_site": "pydantic_ai"}
{"title": "Pydantic Model", "anchor": "pydantic-model", "heading_level": 1, "md_text": "Simple example of using Pydantic AI to construct a Pydantic model from a text input.\n\nDemonstrates:\n\n- [structured `output_type`](../output.md#structured-output)", "url": "https://ai.pydantic.dev/docs/examples/pydantic-model/#pydantic-model", "page": "docs/examples/pydantic-model", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "heading_level": 2, "md_text": "With [dependencies installed and environment variables set](./setup.md#usage), run:\n\n```bash\npython/uv-run -m pydantic_ai_examples.pydantic_model\n```\n\nThis examples uses `openai:gpt-5` by default, but it works well with other models, e.g. you can run it\nwith Gemini using:\n\n```bash\nPYDANTIC_AI_MODEL=gemini-2.5-pro python/uv-run -m pydantic_ai_examples.pydantic_model\n```\n\n(or `PYDANTIC_AI_MODEL=gemini-2.5-flash ...`)", "url": "https://ai.pydantic.dev/docs/examples/pydantic-model/#running-the-example", "page": "docs/examples/pydantic-model", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "heading_level": 2, "md_text": "```snippet {path=\"/examples/pydantic_ai_examples/pydantic_model.py\"}```", "url": "https://ai.pydantic.dev/docs/examples/pydantic-model/#example-code", "page": "docs/examples/pydantic-model", "source_site": "pydantic_ai"}
{"title": "SQL Generation", "anchor": "sql-generation", "heading_level": 1, "md_text": "Example demonstrating how to use Pydantic AI to generate SQL queries based on user input.\n\nDemonstrates:\n\n- [dynamic system prompt](../agents.md#system-prompts)\n- [structured `output_type`](../output.md#structured-output)\n- [output validation](../output.md#output-validator-functions)\n- [agent dependencies](../dependencies.md)", "url": "https://ai.pydantic.dev/docs/examples/sql-gen/#sql-generation", "page": "docs/examples/sql-gen", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "heading_level": 2, "md_text": "The resulting SQL is validated by running it as an `EXPLAIN` query on PostgreSQL. To run the example, you first need to run PostgreSQL, e.g. via Docker:\n\n```bash\ndocker run --rm -e POSTGRES_PASSWORD=postgres -p 54320:5432 postgres\n```\n\n_(we run postgres on port `54320` to avoid conflicts with any other postgres instances you may have running)_\n\nWith [dependencies installed and environment variables set](./setup.md#usage), run:\n\n```bash\npython/uv-run -m pydantic_ai_examples.sql_gen\n```\n\nor to use a custom prompt:\n\n```bash\npython/uv-run -m pydantic_ai_examples.sql_gen \"find me errors\"\n```\n\nThis model uses `gemini-2.5-flash` by default since Gemini is good at single shot queries of this kind.", "url": "https://ai.pydantic.dev/docs/examples/sql-gen/#running-the-example", "page": "docs/examples/sql-gen", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "heading_level": 2, "md_text": "```snippet {path=\"/examples/pydantic_ai_examples/sql_gen.py\"}```", "url": "https://ai.pydantic.dev/docs/examples/sql-gen/#example-code", "page": "docs/examples/sql-gen", "source_site": "pydantic_ai"}
{"title": "Examples", "anchor": "examples", "heading_level": 1, "md_text": "Here we include some examples of how to use Pydantic AI and what it can do.", "url": "https://ai.pydantic.dev/docs/examples/setup/#examples", "page": "docs/examples/setup", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 2, "md_text": "These examples are distributed with `pydantic-ai` so you can run them either by cloning the [pydantic-ai repo](https://github.com/pydantic/pydantic-ai) or by simply installing `pydantic-ai` from PyPI with `pip` or `uv`.", "url": "https://ai.pydantic.dev/docs/examples/setup/#usage", "page": "docs/examples/setup", "source_site": "pydantic_ai"}
{"title": "Installing required dependencies", "anchor": "installing-required-dependencies", "heading_level": 3, "md_text": "Either way you'll need to install extra dependencies to run some examples, you just need to install the `examples` optional dependency group.\n\nIf you've installed `pydantic-ai` via pip/uv, you can install the extra dependencies with:\n\n```bash\npip/uv-add \"pydantic-ai[examples]\"\n```\n\nIf you clone the repo, you should instead use `uv sync --extra examples` to install extra dependencies.", "url": "https://ai.pydantic.dev/docs/examples/setup/#installing-required-dependencies", "page": "docs/examples/setup", "source_site": "pydantic_ai"}
{"title": "Setting model environment variables", "anchor": "setting-model-environment-variables", "heading_level": 3, "md_text": "These examples will need you to set up authentication with one or more of the LLMs, see the [model configuration](../models/overview.md) docs for details on how to do this.\n\nTL;DR: in most cases you'll need to set one of the following environment variables:\n\n=== \"OpenAI\"\n\n    ```bash\n    export OPENAI_API_KEY=your-api-key\n    ```\n\n=== \"Google Gemini\"\n\n    ```bash\n    export GEMINI_API_KEY=your-api-key\n    ```", "url": "https://ai.pydantic.dev/docs/examples/setup/#setting-model-environment-variables", "page": "docs/examples/setup", "source_site": "pydantic_ai"}
{"title": "Running Examples", "anchor": "running-examples", "heading_level": 3, "md_text": "To run the examples (this will work whether you installed `pydantic_ai`, or cloned the repo), run:\n\n```bash\npython/uv-run -m pydantic_ai_examples.<example_module_name>\n```\n\nFor examples, to run the very simple [`pydantic_model`](./pydantic-model.md) example:\n\n```bash\npython/uv-run -m pydantic_ai_examples.pydantic_model\n```\n\nIf you like one-liners and you're using uv, you can run a pydantic-ai example with zero setup:\n\n```bash\nOPENAI_API_KEY='your-api-key' \\\n  uv run --with \"pydantic-ai[examples]\" \\\n  -m pydantic_ai_examples.pydantic_model\n```\n\n---\n\nYou'll probably want to edit examples in addition to just running them. You can copy the examples to a new directory with:\n\n```bash\npython/uv-run -m pydantic_ai_examples --copy-to examples/\n```", "url": "https://ai.pydantic.dev/docs/examples/setup/#running-examples", "page": "docs/examples/setup", "source_site": "pydantic_ai"}
{"title": "weather-agent", "anchor": null, "heading_level": 0, "md_text": "Example of Pydantic AI with multiple tools which the LLM needs to call in turn to answer a question.\n\nDemonstrates:\n\n- [tools](../tools.md)\n- [agent dependencies](../dependencies.md)\n- [streaming text responses](../output.md#streaming-text)\n- Building a [Gradio](https://www.gradio.app/) UI for the agent\n\nIn this case the idea is a \"weather\" agent \u2014 the user can ask for the weather in multiple locations,\nthe agent will use the `get_lat_lng` tool to get the latitude and longitude of the locations, then use\nthe `get_weather` tool to get the weather for those locations.", "url": "https://ai.pydantic.dev/docs/examples/weather-agent/", "page": "docs/examples/weather-agent", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "heading_level": 2, "md_text": "To run this example properly, you might want to add two extra API keys **(Note if either key is missing, the code will fall back to dummy data, so they're not required)**:\n\n- A weather API key from [tomorrow.io](https://www.tomorrow.io/weather-api/) set via `WEATHER_API_KEY`\n- A geocoding API key from [geocode.maps.co](https://geocode.maps.co/) set via `GEO_API_KEY`\n\nWith [dependencies installed and environment variables set](./setup.md#usage), run:\n\n```bash\npython/uv-run -m pydantic_ai_examples.weather_agent\n```", "url": "https://ai.pydantic.dev/docs/examples/weather-agent/#running-the-example", "page": "docs/examples/weather-agent", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "heading_level": 2, "md_text": "```snippet {path=\"/examples/pydantic_ai_examples/weather_agent.py\"}```", "url": "https://ai.pydantic.dev/docs/examples/weather-agent/#example-code", "page": "docs/examples/weather-agent", "source_site": "pydantic_ai"}
{"title": "Running the UI", "anchor": "running-the-ui", "heading_level": 2, "md_text": "You can build multi-turn chat applications for your agent with [Gradio](https://www.gradio.app/), a framework for building AI web applications entirely in python. Gradio comes with built-in chat components and agent support so the entire UI will be implemented in a single python file!\n\nHere's what the UI looks like for the weather agent:\n\n{{ video('c549d8d8827ded15f326f998e428e6c3', 6) }}\n\n\n```bash\npip install gradio>=5.9.0\npython/uv-run -m pydantic_ai_examples.weather_agent_gradio\n```", "url": "https://ai.pydantic.dev/docs/examples/weather-agent/#running-the-ui", "page": "docs/examples/weather-agent", "source_site": "pydantic_ai"}
{"title": "UI Code", "anchor": "ui-code", "heading_level": 2, "md_text": "```snippet {path=\"/examples/pydantic_ai_examples/weather_agent_gradio.py\"}```", "url": "https://ai.pydantic.dev/docs/examples/weather-agent/#ui-code", "page": "docs/examples/weather-agent", "source_site": "pydantic_ai"}
{"title": "Chat App with FastAPI", "anchor": "chat-app-with-fastapi", "heading_level": 1, "md_text": "Simple chat app example build with FastAPI.\n\nDemonstrates:\n\n* [reusing chat history](../message-history.md)\n* [serializing messages](../message-history.md#accessing-messages-from-results)\n* [streaming responses](../output.md#streamed-results)\n\nThis demonstrates storing chat history between requests and using it to give the model context for new responses.\n\nMost of the complex logic here is between `chat_app.py` which streams the response to the browser,\nand `chat_app.ts` which renders messages in the browser.", "url": "https://ai.pydantic.dev/docs/examples/chat-app/#chat-app-with-fastapi", "page": "docs/examples/chat-app", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "heading_level": 2, "md_text": "With [dependencies installed and environment variables set](./setup.md#usage), run:\n\n```bash\npython/uv-run -m pydantic_ai_examples.chat_app\n```\n\nThen open the app at [localhost:8000](http://localhost:8000).\n\n![Example conversation](../img/chat-app-example.png)", "url": "https://ai.pydantic.dev/docs/examples/chat-app/#running-the-example", "page": "docs/examples/chat-app", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "heading_level": 2, "md_text": "Python code that runs the chat app:\n\n```snippet {path=\"/examples/pydantic_ai_examples/chat_app.py\"}```\n\nSimple HTML page to render the app:\n\n```snippet {path=\"/examples/pydantic_ai_examples/chat_app.html\"}```\n\nTypeScript to handle rendering the messages, to keep this simple (and at the risk of offending frontend developers) the typescript code is passed to the browser as plain text and transpiled in the browser.\n\n```snippet {path=\"/examples/pydantic_ai_examples/chat_app.ts\"}```", "url": "https://ai.pydantic.dev/docs/examples/chat-app/#example-code", "page": "docs/examples/chat-app", "source_site": "pydantic_ai"}
{"title": "Question Graph", "anchor": "question-graph", "heading_level": 1, "md_text": "Example of a graph for asking and evaluating questions.\n\nDemonstrates:\n\n* [`pydantic_graph`](../graph.md)", "url": "https://ai.pydantic.dev/docs/examples/question-graph/#question-graph", "page": "docs/examples/question-graph", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "heading_level": 2, "md_text": "With [dependencies installed and environment variables set](./setup.md#usage), run:\n\n```bash\npython/uv-run -m pydantic_ai_examples.question_graph\n```", "url": "https://ai.pydantic.dev/docs/examples/question-graph/#running-the-example", "page": "docs/examples/question-graph", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "heading_level": 2, "md_text": "```snippet {path=\"/examples/pydantic_ai_examples/question_graph.py\"}```\n\nThe mermaid diagram generated in this example looks like this:\n\n```mermaid\n---\ntitle: question_graph\n---\nstateDiagram-v2\n  [*] --> Ask\n  Ask --> Answer: ask the question\n  Answer --> Evaluate: answer the question\n  Evaluate --> Congratulate\n  Evaluate --> Castigate\n  Congratulate --> [*]: success\n  Castigate --> Ask: try again\n```", "url": "https://ai.pydantic.dev/docs/examples/question-graph/#example-code", "page": "docs/examples/question-graph", "source_site": "pydantic_ai"}
{"title": "flight-booking", "anchor": null, "heading_level": 0, "md_text": "Example of a multi-agent flow where one agent delegates work to another, then hands off control to a third agent.\n\nDemonstrates:\n\n* [agent delegation](../multi-agent-applications.md#agent-delegation)\n* [programmatic agent hand-off](../multi-agent-applications.md#programmatic-agent-hand-off)\n* [usage limits](../agents.md#usage-limits)\n\nIn this scenario, a group of agents work together to find the best flight for a user.\n\nThe control flow for this example can be summarised as follows:\n\n```mermaid\ngraph TD\n  START --> search_agent(\"search agent\")\n  search_agent --> extraction_agent(\"extraction agent\")\n  extraction_agent --> search_agent\n  search_agent --> human_confirm(\"human confirm\")\n  human_confirm --> search_agent\n  search_agent --> FAILED\n  human_confirm --> find_seat_function(\"find seat function\")\n  find_seat_function --> human_seat_choice(\"human seat choice\")\n  human_seat_choice --> find_seat_agent(\"find seat agent\")\n  find_seat_agent --> find_seat_function\n  find_seat_function --> buy_flights(\"buy flights\")\n  buy_flights --> SUCCESS\n```", "url": "https://ai.pydantic.dev/docs/examples/flight-booking/", "page": "docs/examples/flight-booking", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "heading_level": 2, "md_text": "With [dependencies installed and environment variables set](./setup.md#usage), run:\n\n```bash\npython/uv-run -m pydantic_ai_examples.flight_booking\n```", "url": "https://ai.pydantic.dev/docs/examples/flight-booking/#running-the-example", "page": "docs/examples/flight-booking", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "heading_level": 2, "md_text": "```snippet {path=\"/examples/pydantic_ai_examples/flight_booking.py\"}```", "url": "https://ai.pydantic.dev/docs/examples/flight-booking/#example-code", "page": "docs/examples/flight-booking", "source_site": "pydantic_ai"}
{"title": "bank-support", "anchor": null, "heading_level": 0, "md_text": "Small but complete example of using Pydantic AI to build a support agent for a bank.\n\nDemonstrates:\n\n- [dynamic system prompt](../agents.md#system-prompts)\n- [structured `output_type`](../output.md#structured-output)\n- [tools](../tools.md)", "url": "https://ai.pydantic.dev/docs/examples/bank-support/", "page": "docs/examples/bank-support", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "heading_level": 2, "md_text": "With [dependencies installed and environment variables set](./setup.md#usage), run:\n\n```bash\npython/uv-run -m pydantic_ai_examples.bank_support\n```\n\n(or `PYDANTIC_AI_MODEL=gemini-2.5-flash ...`)", "url": "https://ai.pydantic.dev/docs/examples/bank-support/#running-the-example", "page": "docs/examples/bank-support", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "heading_level": 2, "md_text": "```snippet {path=\"/examples/pydantic_ai_examples/bank_support.py\"}```", "url": "https://ai.pydantic.dev/docs/examples/bank-support/#example-code", "page": "docs/examples/bank-support", "source_site": "pydantic_ai"}
{"title": "stream-markdown", "anchor": null, "heading_level": 0, "md_text": "This example shows how to stream markdown from an agent, using the [`rich`](https://github.com/Textualize/rich) library to highlight the output in the terminal.\n\nIt'll run the example with both OpenAI and Google Gemini models if the required environment variables are set.\n\nDemonstrates:\n\n* [streaming text responses](../output.md#streaming-text)", "url": "https://ai.pydantic.dev/docs/examples/stream-markdown/", "page": "docs/examples/stream-markdown", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "heading_level": 2, "md_text": "With [dependencies installed and environment variables set](./setup.md#usage), run:\n\n```bash\npython/uv-run -m pydantic_ai_examples.stream_markdown\n```", "url": "https://ai.pydantic.dev/docs/examples/stream-markdown/#running-the-example", "page": "docs/examples/stream-markdown", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "heading_level": 2, "md_text": "```snippet {path=\"/examples/pydantic_ai_examples/stream_markdown.py\"}```", "url": "https://ai.pydantic.dev/docs/examples/stream-markdown/#example-code", "page": "docs/examples/stream-markdown", "source_site": "pydantic_ai"}
{"title": "Agent User Interaction (AG-UI)", "anchor": "agent-user-interaction-ag-ui", "heading_level": 1, "md_text": "Example of using Pydantic AI agents with the [AG-UI Dojo](https://github.com/ag-ui-protocol/ag-ui/tree/main/typescript-sdk/apps/dojo) example app.\n\nSee the [AG-UI docs](../ui/ag-ui.md) for more information about the AG-UI integration.\n\nDemonstrates:\n\n- [AG-UI](../ui/ag-ui.md)\n- [Tools](../tools.md)", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#agent-user-interaction-ag-ui", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Prerequisites", "anchor": "prerequisites", "heading_level": 2, "md_text": "- An [OpenAI API key](https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key)", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#prerequisites", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "heading_level": 2, "md_text": "With [dependencies installed and environment variables set](./setup.md#usage)\nyou will need two command line windows.", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#running-the-example", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Pydantic AI AG-UI backend", "anchor": "pydantic-ai-ag-ui-backend", "heading_level": 3, "md_text": "Setup your OpenAI API Key\n\n```bash\nexport OPENAI_API_KEY=<your api key>\n```\n\nStart the Pydantic AI AG-UI example backend.\n\n```bash\npython/uv-run -m pydantic_ai_examples.ag_ui\n```", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#pydantic-ai-ag-ui-backend", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "AG-UI Dojo example frontend", "anchor": "ag-ui-dojo-example-frontend", "heading_level": 3, "md_text": "Next run the AG-UI Dojo example frontend.\n\n1. Clone the [AG-UI repository](https://github.com/ag-ui-protocol/ag-ui)\n\n    ```shell\n    git clone https://github.com/ag-ui-protocol/ag-ui.git\n    ```\n\n2. Change into to the `ag-ui/typescript-sdk` directory\n\n    ```shell\n    cd ag-ui/typescript-sdk\n    ```\n\n3. Run the Dojo app following the [official instructions](https://github.com/ag-ui-protocol/ag-ui/tree/main/typescript-sdk/apps/dojo#development-setup)\n4. Visit <http://localhost:3000/pydantic-ai>\n5. Select View `Pydantic AI` from the sidebar", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#ag-ui-dojo-example-frontend", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Agentic Chat", "anchor": "agentic-chat", "heading_level": 3, "md_text": "This demonstrates a basic agent interaction including Pydantic AI server side\ntools and AG-UI client side tools.\n\nIf you've [run the example](#running-the-example), you can view it at <http://localhost:3000/pydantic-ai/feature/agentic_chat>.", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#agentic-chat", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Agent Tools", "anchor": "agent-tools", "heading_level": 4, "md_text": "- `time` - Pydantic AI tool to check the current time for a time zone\n- `background` - AG-UI tool to set the background color of the client window", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#agent-tools", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Agent Prompts", "anchor": "agent-prompts", "heading_level": 4, "md_text": "```text\nWhat is the time in New York?\n```\n\n```text\nChange the background to blue\n```\n\nA complex example which mixes both AG-UI and Pydantic AI tools:\n\n```text\nPerform the following steps, waiting for the response of each step before continuing:\n1. Get the time\n2. Set the background to red\n3. Get the time\n4. Report how long the background set took by diffing the two times\n```", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#agent-prompts", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Agentic Chat - Code", "anchor": "agentic-chat-code", "heading_level": 4, "md_text": "```snippet {path=\"/examples/pydantic_ai_examples/ag_ui/api/agentic_chat.py\"}```", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#agentic-chat-code", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Agentic Generative UI", "anchor": "agentic-generative-ui", "heading_level": 3, "md_text": "Demonstrates a long running task where the agent sends updates to the frontend\nto let the user know what's happening.\n\nIf you've [run the example](#running-the-example), you can view it at <http://localhost:3000/pydantic-ai/feature/agentic_generative_ui>.", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#agentic-generative-ui", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Plan Prompts", "anchor": "plan-prompts", "heading_level": 4, "md_text": "```text\nCreate a plan for breakfast and execute it\n```", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#plan-prompts", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Agentic Generative UI - Code", "anchor": "agentic-generative-ui-code", "heading_level": 4, "md_text": "```snippet {path=\"/examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py\"}```", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#agentic-generative-ui-code", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Human in the Loop", "anchor": "human-in-the-loop", "heading_level": 3, "md_text": "Demonstrates simple human in the loop workflow where the agent comes up with a\nplan and the user can approve it using checkboxes.", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#human-in-the-loop", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Task Planning Tools", "anchor": "task-planning-tools", "heading_level": 4, "md_text": "- `generate_task_steps` - AG-UI tool to generate and confirm steps", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#task-planning-tools", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Task Planning Prompt", "anchor": "task-planning-prompt", "heading_level": 4, "md_text": "```text\nGenerate a list of steps for cleaning a car for me to review\n```", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#task-planning-prompt", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Human in the Loop - Code", "anchor": "human-in-the-loop-code", "heading_level": 4, "md_text": "```snippet {path=\"/examples/pydantic_ai_examples/ag_ui/api/human_in_the_loop.py\"}```", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#human-in-the-loop-code", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Predictive State Updates", "anchor": "predictive-state-updates", "heading_level": 3, "md_text": "Demonstrates how to use the predictive state updates feature to update the state\nof the UI based on agent responses, including user interaction via user\nconfirmation.\n\nIf you've [run the example](#running-the-example), you can view it at <http://localhost:3000/pydantic-ai/feature/predictive_state_updates>.", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#predictive-state-updates", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Story Tools", "anchor": "story-tools", "heading_level": 4, "md_text": "- `write_document` - AG-UI tool to write the document to a window\n- `document_predict_state` - Pydantic AI tool that enables document state\n  prediction for the `write_document` tool\n\nThis also shows how to use custom instructions based on shared state information.", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#story-tools", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Story Example", "anchor": "story-example", "heading_level": 4, "md_text": "Starting document text\n\n```markdown\nBruce was a good dog,\n```\n\nAgent prompt\n\n```text\nHelp me complete my story about bruce the dog, is should be no longer than a sentence.\n```", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#story-example", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Predictive State Updates - Code", "anchor": "predictive-state-updates-code", "heading_level": 4, "md_text": "```snippet {path=\"/examples/pydantic_ai_examples/ag_ui/api/predictive_state_updates.py\"}```", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#predictive-state-updates-code", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Shared State", "anchor": "shared-state", "heading_level": 3, "md_text": "Demonstrates how to use the shared state between the UI and the agent.\n\nState sent to the agent is detected by a function based instruction. This then\nvalidates the data using a custom pydantic model before using to create the\ninstructions for the agent to follow and send to the client using a AG-UI tool.\n\nIf you've [run the example](#running-the-example), you can view it at <http://localhost:3000/pydantic-ai/feature/shared_state>.", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#shared-state", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Recipe Tools", "anchor": "recipe-tools", "heading_level": 4, "md_text": "- `display_recipe` - AG-UI tool to display the recipe in a graphical format", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#recipe-tools", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Recipe Example", "anchor": "recipe-example", "heading_level": 4, "md_text": "1. Customise the basic settings of your recipe\n2. Click `Improve with AI`", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#recipe-example", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Shared State - Code", "anchor": "shared-state-code", "heading_level": 4, "md_text": "```snippet {path=\"/examples/pydantic_ai_examples/ag_ui/api/shared_state.py\"}```", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#shared-state-code", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Tool Based Generative UI", "anchor": "tool-based-generative-ui", "heading_level": 3, "md_text": "Demonstrates customised rendering for tool output with used confirmation.\n\nIf you've [run the example](#running-the-example), you can view it at <http://localhost:3000/pydantic-ai/feature/tool_based_generative_ui>.", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#tool-based-generative-ui", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Haiku Tools", "anchor": "haiku-tools", "heading_level": 4, "md_text": "- `generate_haiku` - AG-UI tool to display a haiku in English and Japanese", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#haiku-tools", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Haiku Prompt", "anchor": "haiku-prompt", "heading_level": 4, "md_text": "```text\nGenerate a haiku about formula 1\n```", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#haiku-prompt", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "Tool Based Generative UI - Code", "anchor": "tool-based-generative-ui-code", "heading_level": 4, "md_text": "```snippet {path=\"/examples/pydantic_ai_examples/ag_ui/api/tool_based_generative_ui.py\"}```", "url": "https://ai.pydantic.dev/docs/examples/ag-ui/#tool-based-generative-ui-code", "page": "docs/examples/ag-ui", "source_site": "pydantic_ai"}
{"title": "stream-whales", "anchor": null, "heading_level": 0, "md_text": "Information about whales \u2014 an example of streamed structured response validation.\n\nDemonstrates:\n\n* [streaming structured output](../output.md#streaming-structured-output)\n\nThis script streams structured responses from GPT-4 about whales, validates the data\nand displays it as a dynamic table using [`rich`](https://github.com/Textualize/rich) as the data is received.", "url": "https://ai.pydantic.dev/docs/examples/stream-whales/", "page": "docs/examples/stream-whales", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "heading_level": 2, "md_text": "With [dependencies installed and environment variables set](./setup.md#usage), run:\n\n```bash\npython/uv-run -m pydantic_ai_examples.stream_whales\n```\n\nShould give an output like this:\n\n{{ video('53dd5e7664c20ae90ed90ae42f606bf3', 25) }}", "url": "https://ai.pydantic.dev/docs/examples/stream-whales/#running-the-example", "page": "docs/examples/stream-whales", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "heading_level": 2, "md_text": "```snippet {path=\"/examples/pydantic_ai_examples/stream_whales.py\"}```", "url": "https://ai.pydantic.dev/docs/examples/stream-whales/#example-code", "page": "docs/examples/stream-whales", "source_site": "pydantic_ai"}
{"title": "RAG", "anchor": "rag", "heading_level": 1, "md_text": "RAG search example. This demo allows you to ask question of the [logfire](https://pydantic.dev/logfire) documentation.\n\nDemonstrates:\n\n- [tools](../tools.md)\n- [agent dependencies](../dependencies.md)\n- RAG search\n\nThis is done by creating a database containing each section of the markdown documentation, then registering\nthe search tool with the Pydantic AI agent.\n\nLogic for extracting sections from markdown files and a JSON file with that data is available in\n[this gist](https://gist.github.com/samuelcolvin/4b5bb9bb163b1122ff17e29e48c10992).\n\n[PostgreSQL with pgvector](https://github.com/pgvector/pgvector) is used as the search database, the easiest way to download and run pgvector is using Docker:\n\n```bash\nmkdir postgres-data\ndocker run --rm \\\n  -e POSTGRES_PASSWORD=postgres \\\n  -p 54320:5432 \\\n  -v `pwd`/postgres-data:/var/lib/postgresql/data \\\n  pgvector/pgvector:pg17\n```\n\nAs with the [SQL gen](./sql-gen.md) example, we run postgres on port `54320` to avoid conflicts with any other postgres instances you may have running.\nWe also mount the PostgreSQL `data` directory locally to persist the data if you need to stop and restart the container.\n\nWith that running and [dependencies installed and environment variables set](./setup.md#usage), we can build the search database with (**WARNING**: this requires the `OPENAI_API_KEY` env variable and will calling the OpenAI embedding API around 300 times to generate embeddings for each section of the documentation):\n\n```bash\npython/uv-run -m pydantic_ai_examples.rag build\n```\n\n(Note building the database doesn't use Pydantic AI right now, instead it uses the OpenAI SDK directly.)\n\nYou can then ask the agent a question with:\n\n```bash\npython/uv-run -m pydantic_ai_examples.rag search \"How do I configure logfire to work with FastAPI?\"\n```", "url": "https://ai.pydantic.dev/docs/examples/rag/#rag", "page": "docs/examples/rag", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "heading_level": 2, "md_text": "```snippet {path=\"/examples/pydantic_ai_examples/rag.py\"}```", "url": "https://ai.pydantic.dev/docs/examples/rag/#example-code", "page": "docs/examples/rag", "source_site": "pydantic_ai"}
{"title": "Data Analyst", "anchor": "data-analyst", "heading_level": 1, "md_text": "Sometimes in an agent workflow, the agent does not need to know the exact tool\noutput, but still needs to process the tool output in some ways. This is\nespecially common in data analytics: the agent needs to know that the result of a\nquery tool is a `DataFrame` with certain named columns, but not\nnecessarily the content of every single row.\n\nWith Pydantic AI, you can use a [dependencies object](../dependencies.md) to\nstore the result from one tool and use it in another tool.\n\nIn this example, we'll build an agent that analyzes the [Rotten Tomatoes movie review dataset from Cornell](https://huggingface.co/datasets/cornell-movie-review-data/rotten_tomatoes).\n\n\nDemonstrates:\n\n- [agent dependencies](../dependencies.md)", "url": "https://ai.pydantic.dev/docs/examples/data-analyst/#data-analyst", "page": "docs/examples/data-analyst", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "heading_level": 2, "md_text": "With [dependencies installed and environment variables set](./setup.md#usage), run:\n\n```bash\npython/uv-run -m pydantic_ai_examples.data_analyst\n```\n\n\nOutput (debug):\n\n\n> Based on my analysis of the Cornell Movie Review dataset (rotten_tomatoes), there are **4,265 negative comments** in the training split. These are the reviews labeled as 'neg' (represented by 0 in the dataset).", "url": "https://ai.pydantic.dev/docs/examples/data-analyst/#running-the-example", "page": "docs/examples/data-analyst", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "heading_level": 2, "md_text": "```snippet {path=\"/examples/pydantic_ai_examples/data_analyst.py\"}```", "url": "https://ai.pydantic.dev/docs/examples/data-analyst/#example-code", "page": "docs/examples/data-analyst", "source_site": "pydantic_ai"}
{"title": "Choosing a Model", "anchor": "choosing-a-model", "heading_level": 3, "md_text": "This example requires using a model that understands DuckDB SQL. You can check with `clai`:\n\n```sh\n> clai -m bedrock:us.anthropic.claude-3-7-sonnet-20250219-v1:0\nclai - Pydantic AI CLI v0.0.1.dev920+41dd069 with bedrock:us.anthropic.claude-3-7-sonnet-20250219-v1:0\nclai \u27a4 do you understand duckdb sql?", "url": "https://ai.pydantic.dev/docs/examples/data-analyst/#choosing-a-model", "page": "docs/examples/data-analyst", "source_site": "pydantic_ai"}
{"title": "DuckDB SQL", "anchor": "duckdb-sql", "heading_level": 1, "md_text": "Yes, I understand DuckDB SQL. DuckDB is an in-process analytical SQL database\nthat uses syntax similar to PostgreSQL. It specializes in analytical queries\nand is designed for high-performance analysis of structured data.\n\nSome key features of DuckDB SQL include:\n\n \u2022 OLAP (Online Analytical Processing) optimized\n \u2022 Columnar-vectorized query execution\n \u2022 Standard SQL support with PostgreSQL compatibility\n \u2022 Support for complex analytical queries\n \u2022 Efficient handling of CSV/Parquet/JSON files\n\nI can help you with DuckDB SQL queries, schema design, optimization, or other\nDuckDB-related questions.\n```", "url": "https://ai.pydantic.dev/docs/examples/data-analyst/#duckdb-sql", "page": "docs/examples/data-analyst", "source_site": "pydantic_ai"}
{"title": "Vercel AI Data Stream Protocol", "anchor": "vercel-ai-data-stream-protocol", "heading_level": 1, "md_text": "Pydantic AI natively supports the [Vercel AI Data Stream Protocol](https://ai-sdk.dev/docs/ai-sdk-ui/stream-protocol#data-stream-protocol) to receive agent run input from, and stream events to, a [Vercel AI Elements](https://ai-sdk.dev/elements) frontend.", "url": "https://ai.pydantic.dev/docs/ui/vercel-ai/#vercel-ai-data-stream-protocol", "page": "docs/ui/vercel-ai", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 2, "md_text": "The [`VercelAIAdapter`][pydantic_ai.ui.vercel_ai.VercelAIAdapter] class is responsible for transforming agent run input received from the frontend into arguments for [`Agent.run_stream_events()`](../agents.md#running-agents), running the agent, and then transforming Pydantic AI events into Vercel AI events. The event stream transformation is handled by the [`VercelAIEventStream`][pydantic_ai.ui.vercel_ai.VercelAIEventStream] class, but you typically won't use this directly.\n\nIf you're using a Starlette-based web framework like FastAPI, you can use the [`VercelAIAdapter.dispatch_request()`][pydantic_ai.ui.vercel_ai.VercelAIAdapter.dispatch_request] class method from an endpoint function to directly handle a request and return a streaming response of Vercel AI events. This is demonstrated in the next section.\n\nIf you're using a web framework not based on Starlette (e.g. Django or Flask) or need fine-grained control over the input or output, you can create a `VercelAIAdapter` instance and directly use its methods. This is demonstrated in \"Advanced Usage\" section below.", "url": "https://ai.pydantic.dev/docs/ui/vercel-ai/#usage", "page": "docs/ui/vercel-ai", "source_site": "pydantic_ai"}
{"title": "Usage with Starlette/FastAPI", "anchor": "usage-with-starlettefastapi", "heading_level": 3, "md_text": "Besides the request, [`VercelAIAdapter.dispatch_request()`][pydantic_ai.ui.vercel_ai.VercelAIAdapter.dispatch_request] takes the agent, the same optional arguments as [`Agent.run_stream_events()`](../agents.md#running-agents), and an optional `on_complete` callback function that receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can optionally yield additional Vercel AI events.\n\n```py {title=\"dispatch_request.py\"}\nfrom fastapi import FastAPI\nfrom starlette.requests import Request\nfrom starlette.responses import Response\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ui.vercel_ai import VercelAIAdapter\n\nagent = Agent('openai:gpt-5')\n\napp = FastAPI()\n\n@app.post('/chat')\nasync def chat(request: Request) -> Response:\n    return await VercelAIAdapter.dispatch_request(request, agent=agent)\n```", "url": "https://ai.pydantic.dev/docs/ui/vercel-ai/#usage-with-starlettefastapi", "page": "docs/ui/vercel-ai", "source_site": "pydantic_ai"}
{"title": "Advanced Usage", "anchor": "advanced-usage", "heading_level": 3, "md_text": "If you're using a web framework not based on Starlette (e.g. Django or Flask) or need fine-grained control over the input or output, you can create a `VercelAIAdapter` instance and directly use its methods, which can be chained to accomplish the same thing as the `VercelAIAdapter.dispatch_request()` class method shown above:\n\n1. The [`VercelAIAdapter.build_run_input()`][pydantic_ai.ui.vercel_ai.VercelAIAdapter.build_run_input] class method takes the request body as bytes and returns a Vercel AI [`RequestData`][pydantic_ai.ui.vercel_ai.request_types.RequestData] run input object, which you can then pass to the [`VercelAIAdapter()`][pydantic_ai.ui.vercel_ai.VercelAIAdapter] constructor along with the agent.\n    - You can also use the [`VercelAIAdapter.from_request()`][pydantic_ai.ui.vercel_ai.VercelAIAdapter.from_request] class method to build an adapter directly from a Starlette/FastAPI request.\n2. The [`VercelAIAdapter.run_stream()`][pydantic_ai.ui.vercel_ai.VercelAIAdapter.run_stream] method runs the agent and returns a stream of Vercel AI events. It supports the same optional arguments as [`Agent.run_stream_events()`](../agents.md#running-agents) and an optional `on_complete` callback function that receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can optionally yield additional Vercel AI events.\n    - You can also use [`VercelAIAdapter.run_stream_native()`][pydantic_ai.ui.vercel_ai.VercelAIAdapter.run_stream_native] to run the agent and return a stream of Pydantic AI events instead, which can then be transformed into Vercel AI events using [`VercelAIAdapter.transform_stream()`][pydantic_ai.ui.vercel_ai.VercelAIAdapter.transform_stream].\n3. The [`VercelAIAdapter.encode_stream()`][pydantic_ai.ui.vercel_ai.VercelAIAdapter.encode_stream] method encodes the stream of Vercel AI events as SSE (HTTP Server-Sent Events) strings, which you can then return as a streaming response.\n    - You can also use [`VercelAIAdapter.streaming_response()`][pydantic_ai.ui.vercel_ai.VercelAIAdapter.streaming_response] to generate a Starlette/FastAPI streaming response directly from the Vercel AI event stream returned by `run_stream()`.\n\n!!! note\n    This example uses FastAPI, but can be modified to work with any web framework.\n\n```py {title=\"run_stream.py\"}\nimport json\nfrom http import HTTPStatus\n\nfrom fastapi import FastAPI\nfrom fastapi.requests import Request\nfrom fastapi.responses import Response, StreamingResponse\nfrom pydantic import ValidationError\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ui import SSE_CONTENT_TYPE\nfrom pydantic_ai.ui.vercel_ai import VercelAIAdapter\n\nagent = Agent('openai:gpt-5')\n\napp = FastAPI()\n\n\n@app.post('/chat')\nasync def chat(request: Request) -> Response:\n    accept = request.headers.get('accept', SSE_CONTENT_TYPE)\n    try:\n        run_input = VercelAIAdapter.build_run_input(await request.body())\n    except ValidationError as e:\n        return Response(\n            content=json.dumps(e.json()),\n            media_type='application/json',\n            status_code=HTTPStatus.UNPROCESSABLE_ENTITY,\n        )\n\n    adapter = VercelAIAdapter(agent=agent, run_input=run_input, accept=accept)\n    event_stream = adapter.run_stream()\n\n    sse_event_stream = adapter.encode_stream(event_stream)\n    return StreamingResponse(sse_event_stream, media_type=accept)\n```", "url": "https://ai.pydantic.dev/docs/ui/vercel-ai/#advanced-usage", "page": "docs/ui/vercel-ai", "source_site": "pydantic_ai"}
{"title": "Agent-User Interaction (AG-UI) Protocol", "anchor": "agent-user-interaction-ag-ui-protocol", "heading_level": 1, "md_text": "The [Agent-User Interaction (AG-UI) Protocol](https://docs.ag-ui.com/introduction) is an open standard introduced by the\n[CopilotKit](https://webflow.copilotkit.ai/blog/introducing-ag-ui-the-protocol-where-agents-meet-users)\nteam that standardises how frontend applications communicate with AI agents, with support for streaming, frontend tools, shared state, and custom events.\n\n!!! note\n    The AG-UI integration was originally built by the team at [Rocket Science](https://www.rocketscience.gg/) and contributed in collaboration with the Pydantic AI and CopilotKit teams. Thanks Rocket Science!", "url": "https://ai.pydantic.dev/docs/ui/ag-ui/#agent-user-interaction-ag-ui-protocol", "page": "docs/ui/ag-ui", "source_site": "pydantic_ai"}
{"title": "Installation", "anchor": "installation", "heading_level": 2, "md_text": "The only dependencies are:\n\n- [ag-ui-protocol](https://docs.ag-ui.com/introduction): to provide the AG-UI types and encoder.\n- [starlette](https://www.starlette.io): to handle [ASGI](https://asgi.readthedocs.io/en/latest/) requests from a framework like FastAPI.\n\nYou can install Pydantic AI with the `ag-ui` extra to ensure you have all the\nrequired AG-UI dependencies:\n\n```bash\npip/uv-add 'pydantic-ai-slim[ag-ui]'\n```\n\nTo run the examples you'll also need:\n\n- [uvicorn](https://www.uvicorn.org/) or another ASGI compatible server\n\n```bash\npip/uv-add uvicorn\n```", "url": "https://ai.pydantic.dev/docs/ui/ag-ui/#installation", "page": "docs/ui/ag-ui", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 2, "md_text": "There are three ways to run a Pydantic AI agent based on AG-UI run input with streamed AG-UI events as output, from most to least flexible. If you're using a Starlette-based web framework like FastAPI, you'll typically want to use the second method.\n\n1. The [`AGUIAdapter.run_stream()`][pydantic_ai.ui.ag_ui.AGUIAdapter.run_stream] method, when called on an [`AGUIAdapter`][pydantic_ai.ui.ag_ui.AGUIAdapter] instantiated with an agent and an AG-UI [`RunAgentInput`](https://docs.ag-ui.com/sdk/python/core/types#runagentinput) object, will run the agent and return a stream of AG-UI events. It also takes optional [`Agent.iter()`][pydantic_ai.Agent.iter] arguments including `deps`. Use this if you're using a web framework not based on Starlette (e.g. Django or Flask) or want to modify the input or output some way.\n2. The [`AGUIAdapter.dispatch_request()`][pydantic_ai.ui.ag_ui.AGUIAdapter.dispatch_request] class method takes an agent and a Starlette request (e.g. from FastAPI) coming from an AG-UI frontend, and returns a streaming Starlette response of AG-UI events that you can return directly from your endpoint. It also takes optional [`Agent.iter()`][pydantic_ai.Agent.iter] arguments including `deps`, that you can vary for each request (e.g. based on the authenticated user). This is a convenience method that combines [`AGUIAdapter.from_request()`][pydantic_ai.ui.ag_ui.AGUIAdapter.from_request], [`AGUIAdapter.run_stream()`][pydantic_ai.ui.ag_ui.AGUIAdapter.run_stream], and [`AGUIAdapter.streaming_response()`][pydantic_ai.ui.ag_ui.AGUIAdapter.streaming_response].\n3. [`AGUIApp`][pydantic_ai.ui.ag_ui.app.AGUIApp] represents an ASGI application that handles every AG-UI request by running the agent. It also takes optional [`Agent.iter()`][pydantic_ai.Agent.iter] arguments including `deps`, but these will be the same for each request, with the exception of the AG-UI state that's injected as described under [state management](#state-management). This ASGI app can be [mounted](https://fastapi.tiangolo.com/advanced/sub-applications/) at a given path in an existing FastAPI app.", "url": "https://ai.pydantic.dev/docs/ui/ag-ui/#usage", "page": "docs/ui/ag-ui", "source_site": "pydantic_ai"}
{"title": "Handle run input and output directly", "anchor": "handle-run-input-and-output-directly", "heading_level": 3, "md_text": "This example uses [`AGUIAdapter.run_stream()`][pydantic_ai.ui.ag_ui.AGUIAdapter.run_stream] and performs its own request parsing and response generation.\nThis can be modified to work with any web framework.\n\n```py {title=\"run_ag_ui.py\"}\nimport json\nfrom http import HTTPStatus\n\nfrom fastapi import FastAPI\nfrom fastapi.requests import Request\nfrom fastapi.responses import Response, StreamingResponse\nfrom pydantic import ValidationError\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ui import SSE_CONTENT_TYPE\nfrom pydantic_ai.ui.ag_ui import AGUIAdapter\n\nagent = Agent('openai:gpt-5', instructions='Be fun!')\n\napp = FastAPI()\n\n\n@app.post('/')\nasync def run_agent(request: Request) -> Response:\n    accept = request.headers.get('accept', SSE_CONTENT_TYPE)\n    try:\n        run_input = AGUIAdapter.build_run_input(await request.body())  # (1)\n    except ValidationError as e:\n        return Response(\n            content=json.dumps(e.json()),\n            media_type='application/json',\n            status_code=HTTPStatus.UNPROCESSABLE_ENTITY,\n        )\n\n    adapter = AGUIAdapter(agent=agent, run_input=run_input, accept=accept)\n    event_stream = adapter.run_stream() # (2)\n\n    sse_event_stream = adapter.encode_stream(event_stream)\n    return StreamingResponse(sse_event_stream, media_type=accept) # (3)\n```\n\n1. [`AGUIAdapter.build_run_input()`][pydantic_ai.ui.ag_ui.AGUIAdapter.build_run_input] takes the request body as bytes and returns an AG-UI [`RunAgentInput`](https://docs.ag-ui.com/sdk/python/core/types#runagentinput) object. You can also use the [`AGUIAdapter.from_request()`][pydantic_ai.ui.ag_ui.AGUIAdapter.from_request] class method to build an adapter directly from a request.\n2. [`AGUIAdapter.run_stream()`][pydantic_ai.ui.ag_ui.AGUIAdapter.run_stream] runs the agent and returns a stream of AG-UI events. It supports the same optional arguments as [`Agent.run_stream_events()`](../agents.md#running-agents), including `deps`. You can also use [`AGUIAdapter.run_stream_native()`][pydantic_ai.ui.ag_ui.AGUIAdapter.run_stream_native] to run the agent and return a stream of Pydantic AI events instead, which can then be transformed into AG-UI events using [`AGUIAdapter.transform_stream()`][pydantic_ai.ui.ag_ui.AGUIAdapter.transform_stream].\n3. [`AGUIAdapter.encode_stream()`][pydantic_ai.ui.ag_ui.AGUIAdapter.encode_stream] encodes the stream of AG-UI events as strings according to the accept header value. You can also use [`AGUIAdapter.streaming_response()`][pydantic_ai.ui.ag_ui.AGUIAdapter.streaming_response] to generate a streaming response directly from the AG-UI event stream returned by `run_stream()`.\n\nSince `app` is an ASGI application, it can be used with any ASGI server:\n\n```shell\nuvicorn run_ag_ui:app\n```\n\nThis will expose the agent as an AG-UI server, and your frontend can start sending requests to it.", "url": "https://ai.pydantic.dev/docs/ui/ag-ui/#handle-run-input-and-output-directly", "page": "docs/ui/ag-ui", "source_site": "pydantic_ai"}
{"title": "Handle a Starlette request", "anchor": "handle-a-starlette-request", "heading_level": 3, "md_text": "This example uses [`AGUIAdapter.dispatch_request()`][pydantic_ai.ui.ag_ui.AGUIAdapter.dispatch_request] to directly handle a FastAPI request and return a response. Something analogous to this will work with any Starlette-based web framework.\n\n```py {title=\"handle_ag_ui_request.py\"}\nfrom fastapi import FastAPI\nfrom starlette.requests import Request\nfrom starlette.responses import Response\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ui.ag_ui import AGUIAdapter\n\nagent = Agent('openai:gpt-5', instructions='Be fun!')\n\napp = FastAPI()\n\n@app.post('/')\nasync def run_agent(request: Request) -> Response:\n    return await AGUIAdapter.dispatch_request(request, agent=agent) # (1)\n```\n\n1. This method essentially does the same as the previous example, but it's more convenient to use when you're already using a Starlette/FastAPI app.\n\nSince `app` is an ASGI application, it can be used with any ASGI server:\n\n```shell\nuvicorn handle_ag_ui_request:app\n```\n\nThis will expose the agent as an AG-UI server, and your frontend can start sending requests to it.", "url": "https://ai.pydantic.dev/docs/ui/ag-ui/#handle-a-starlette-request", "page": "docs/ui/ag-ui", "source_site": "pydantic_ai"}
{"title": "Stand-alone ASGI app", "anchor": "stand-alone-asgi-app", "heading_level": 3, "md_text": "This example uses [`AGUIApp`][pydantic_ai.ui.ag_ui.app.AGUIApp] to turn the agent into a stand-alone ASGI application:\n\n```py {title=\"ag_ui_app.py\" hl_lines=\"4\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ui.ag_ui.app import AGUIApp\n\nagent = Agent('openai:gpt-5', instructions='Be fun!')\napp = AGUIApp(agent)\n```\n\nSince `app` is an ASGI application, it can be used with any ASGI server:\n\n```shell\nuvicorn ag_ui_app:app\n```\n\nThis will expose the agent as an AG-UI server, and your frontend can start sending requests to it.", "url": "https://ai.pydantic.dev/docs/ui/ag-ui/#stand-alone-asgi-app", "page": "docs/ui/ag-ui", "source_site": "pydantic_ai"}
{"title": "Design", "anchor": "design", "heading_level": 2, "md_text": "The Pydantic AI AG-UI integration supports all features of the spec:\n\n- [Events](https://docs.ag-ui.com/concepts/events)\n- [Messages](https://docs.ag-ui.com/concepts/messages)\n- [State Management](https://docs.ag-ui.com/concepts/state)\n- [Tools](https://docs.ag-ui.com/concepts/tools)\n\nThe integration receives messages in the form of a\n[`RunAgentInput`](https://docs.ag-ui.com/sdk/python/core/types#runagentinput) object\nthat describes the details of the requested agent run including message history, state, and available tools.\n\nThese are converted to Pydantic AI types and passed to the agent's run method. Events from the agent, including tool calls, are converted to AG-UI events and streamed back to the caller as Server-Sent Events (SSE).\n\nA user request may require multiple round trips between client UI and Pydantic AI\nserver, depending on the tools and events needed.", "url": "https://ai.pydantic.dev/docs/ui/ag-ui/#design", "page": "docs/ui/ag-ui", "source_site": "pydantic_ai"}
{"title": "State management", "anchor": "state-management", "heading_level": 3, "md_text": "The integration provides full support for\n[AG-UI state management](https://docs.ag-ui.com/concepts/state), which enables\nreal-time synchronization between agents and frontend applications.\n\nIn the example below we have document state which is shared between the UI and\nserver using the [`StateDeps`][pydantic_ai.ag_ui.StateDeps] [dependencies type](../dependencies.md) that can be used to automatically\nvalidate state contained in [`RunAgentInput.state`](https://docs.ag-ui.com/sdk/js/core/types#runagentinput) using a Pydantic `BaseModel` specified as a generic parameter.\n\n!!! note \"Custom dependencies type with AG-UI state\"\n    If you want to use your own dependencies type to hold AG-UI state as well as other things, it needs to implements the\n    [`StateHandler`][pydantic_ai.ag_ui.StateHandler] protocol, meaning it needs to be a [dataclass](https://docs.python.org/3/library/dataclasses.html) with a non-optional `state` field. This lets Pydantic AI ensure that state is properly isolated between requests by building a new dependencies object each time.\n\n    If the `state` field's type is a Pydantic `BaseModel` subclass, the raw state dictionary on the request is automatically validated. If not, you can validate the raw value yourself in your dependencies dataclass's `__post_init__` method.\n\n\n```python {title=\"ag_ui_state.py\"}\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ui import StateDeps\nfrom pydantic_ai.ui.ag_ui.app import AGUIApp\n\n\nclass DocumentState(BaseModel):\n    \"\"\"State for the document being written.\"\"\"\n\n    document: str = ''\n\n\nagent = Agent(\n    'openai:gpt-5',\n    instructions='Be fun!',\n    deps_type=StateDeps[DocumentState],\n)\napp = AGUIApp(agent, deps=StateDeps(DocumentState()))\n```\n\nSince `app` is an ASGI application, it can be used with any ASGI server:\n\n```bash\nuvicorn ag_ui_state:app --host 0.0.0.0 --port 9000\n```", "url": "https://ai.pydantic.dev/docs/ui/ag-ui/#state-management", "page": "docs/ui/ag-ui", "source_site": "pydantic_ai"}
{"title": "Tools", "anchor": "tools", "heading_level": 3, "md_text": "AG-UI frontend tools are seamlessly provided to the Pydantic AI agent, enabling rich\nuser experiences with frontend user interfaces.", "url": "https://ai.pydantic.dev/docs/ui/ag-ui/#tools", "page": "docs/ui/ag-ui", "source_site": "pydantic_ai"}
{"title": "Events", "anchor": "events", "heading_level": 3, "md_text": "Pydantic AI tools can send [AG-UI events](https://docs.ag-ui.com/concepts/events) simply by returning a\n[`ToolReturn`](../tools-advanced.md#advanced-tool-returns) object with a\n[`BaseEvent`](https://docs.ag-ui.com/sdk/python/core/events#baseevent) (or a list of events) as `metadata`,\nwhich allows for custom events and state updates.\n\n```python {title=\"ag_ui_tool_events.py\"}\nfrom ag_ui.core import CustomEvent, EventType, StateSnapshotEvent\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, RunContext, ToolReturn\nfrom pydantic_ai.ui import StateDeps\nfrom pydantic_ai.ui.ag_ui.app import AGUIApp\n\n\nclass DocumentState(BaseModel):\n    \"\"\"State for the document being written.\"\"\"\n\n    document: str = ''\n\n\nagent = Agent(\n    'openai:gpt-5',\n    instructions='Be fun!',\n    deps_type=StateDeps[DocumentState],\n)\napp = AGUIApp(agent, deps=StateDeps(DocumentState()))\n\n\n@agent.tool\nasync def update_state(ctx: RunContext[StateDeps[DocumentState]]) -> ToolReturn:\n    return ToolReturn(\n        return_value='State updated',\n        metadata=[\n            StateSnapshotEvent(\n                type=EventType.STATE_SNAPSHOT,\n                snapshot=ctx.deps.state,\n            ),\n        ],\n    )\n\n\n@agent.tool_plain\nasync def custom_events() -> ToolReturn:\n    return ToolReturn(\n        return_value='Count events sent',\n        metadata=[\n            CustomEvent(\n                type=EventType.CUSTOM,\n                name='count',\n                value=1,\n            ),\n            CustomEvent(\n                type=EventType.CUSTOM,\n                name='count',\n                value=2,\n            ),\n        ]\n    )\n```\n\nSince `app` is an ASGI application, it can be used with any ASGI server:\n\n```bash\nuvicorn ag_ui_tool_events:app --host 0.0.0.0 --port 9000\n```", "url": "https://ai.pydantic.dev/docs/ui/ag-ui/#events", "page": "docs/ui/ag-ui", "source_site": "pydantic_ai"}
{"title": "Examples", "anchor": "examples", "heading_level": 2, "md_text": "For more examples of how to use [`AGUIApp`][pydantic_ai.ui.ag_ui.app.AGUIApp] see\n[`pydantic_ai_examples.ag_ui`](https://github.com/pydantic/pydantic-ai/tree/main/examples/pydantic_ai_examples/ag_ui),\nwhich includes a server for use with the\n[AG-UI Dojo](https://docs.ag-ui.com/tutorials/debugging#the-ag-ui-dojo).", "url": "https://ai.pydantic.dev/docs/ui/ag-ui/#examples", "page": "docs/ui/ag-ui", "source_site": "pydantic_ai"}
{"title": "UI Event Streams", "anchor": "ui-event-streams", "heading_level": 1, "md_text": "If you're building a chat app or other interactive frontend for an AI agent, your backend will need to receive agent run input (like a chat message or complete [message history](../message-history.md)) from the frontend, and will need to stream the [agent's events](../agents.md#streaming-all-events) (like text, thinking, and tool calls) to the frontend so that the user knows what's happening in real time.\n\nWhile your frontend could use Pydantic AI's [`ModelRequest`][pydantic_ai.messages.ModelRequest] and [`AgentStreamEvent`][pydantic_ai.messages.AgentStreamEvent] directly, you'll typically want to use a UI event stream protocol that's natively supported by your frontend framework.\n\nPydantic AI natively supports two UI event stream protocols:\n\n- [Agent-User Interaction (AG-UI) Protocol](./ag-ui.md)\n- [Vercel AI Data Stream Protocol](./vercel-ai.md)\n\nThese integrations are implemented as subclasses of the abstract [`UIAdapter`][pydantic_ai.ui.UIAdapter] class, so they also serve as a reference for integrating with other UI event stream protocols.", "url": "https://ai.pydantic.dev/docs/ui/overview/#ui-event-streams", "page": "docs/ui/overview", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 2, "md_text": "The protocol-specific [`UIAdapter`][pydantic_ai.ui.UIAdapter] subclass (i.e. [`AGUIAdapter`][pydantic_ai.ui.ag_ui.AGUIAdapter] or [`VercelAIAdapter`][pydantic_ai.ui.vercel_ai.VercelAIAdapter]) is responsible for transforming agent run input received from the frontend into arguments for [`Agent.run_stream_events()`](../agents.md#running-agents), running the agent, and then transforming Pydantic AI events into protocol-specific events. The event stream transformation is handled by a protocol-specific [`UIEventStream`][pydantic_ai.ui.UIEventStream] subclass, but you typically won't use this directly.\n\nIf you're using a Starlette-based web framework like FastAPI, you can use the [`UIAdapter.dispatch_request()`][pydantic_ai.ui.UIAdapter.dispatch_request] class method from an endpoint function to directly handle a request and return a streaming response of protocol-specific events. This is demonstrated in the next section.\n\nIf you're using a web framework not based on Starlette (e.g. Django or Flask) or need fine-grained control over the input or output, you can create a `UIAdapter` instance and directly use its methods. This is demonstrated in \"Advanced Usage\" section below.", "url": "https://ai.pydantic.dev/docs/ui/overview/#usage", "page": "docs/ui/overview", "source_site": "pydantic_ai"}
{"title": "Usage with Starlette/FastAPI", "anchor": "usage-with-starlettefastapi", "heading_level": 3, "md_text": "Besides the request, [`UIAdapter.dispatch_request()`][pydantic_ai.ui.UIAdapter.dispatch_request] takes the agent, the same optional arguments as [`Agent.run_stream_events()`](../agents.md#running-agents), and an optional `on_complete` callback function that receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can optionally yield additional protocol-specific events.\n\n!!! note\n    These examples use the `VercelAIAdapter`, but the same patterns apply to all `UIAdapter` subclasses.\n\n```py {title=\"dispatch_request.py\"}\nfrom fastapi import FastAPI\nfrom starlette.requests import Request\nfrom starlette.responses import Response\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ui.vercel_ai import VercelAIAdapter\n\nagent = Agent('openai:gpt-5')\n\napp = FastAPI()\n\n@app.post('/chat')\nasync def chat(request: Request) -> Response:\n    return await VercelAIAdapter.dispatch_request(request, agent=agent)\n```", "url": "https://ai.pydantic.dev/docs/ui/overview/#usage-with-starlettefastapi", "page": "docs/ui/overview", "source_site": "pydantic_ai"}
{"title": "Advanced Usage", "anchor": "advanced-usage", "heading_level": 3, "md_text": "If you're using a web framework not based on Starlette (e.g. Django or Flask) or need fine-grained control over the input or output, you can create a `UIAdapter` instance and directly use its methods, which can be chained to accomplish the same thing as the `UIAdapter.dispatch_request()` class method shown above:\n\n1. The [`UIAdapter.build_run_input()`][pydantic_ai.ui.UIAdapter.build_run_input] class method takes the request body as bytes and returns a protocol-specific run input object, which you can then pass to the [`UIAdapter()`][pydantic_ai.ui.UIAdapter] constructor along with the agent.\n    - You can also use the [`UIAdapter.from_request()`][pydantic_ai.ui.UIAdapter.from_request] class method to build an adapter directly from a Starlette/FastAPI request.\n2. The [`UIAdapter.run_stream()`][pydantic_ai.ui.UIAdapter.run_stream] method runs the agent and returns a stream of protocol-specific events. It supports the same optional arguments as [`Agent.run_stream_events()`](../agents.md#running-agents) and an optional `on_complete` callback function that receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can optionally yield additional protocol-specific events.\n    - You can also use [`UIAdapter.run_stream_native()`][pydantic_ai.ui.UIAdapter.run_stream_native] to run the agent and return a stream of Pydantic AI events instead, which can then be transformed into protocol-specific events using [`UIAdapter.transform_stream()`][pydantic_ai.ui.UIAdapter.transform_stream].\n3. The [`UIAdapter.encode_stream()`][pydantic_ai.ui.UIAdapter.encode_stream] method encodes the stream of protocol-specific events as SSE (HTTP Server-Sent Events) strings, which you can then return as a streaming response.\n    - You can also use [`UIAdapter.streaming_response()`][pydantic_ai.ui.UIAdapter.streaming_response] to generate a Starlette/FastAPI streaming response directly from the protocol-specific event stream returned by `run_stream()`.\n\n!!! note\n    This example uses FastAPI, but can be modified to work with any web framework.\n\n```py {title=\"run_stream.py\"}\nimport json\nfrom http import HTTPStatus\n\nfrom fastapi import FastAPI\nfrom fastapi.requests import Request\nfrom fastapi.responses import Response, StreamingResponse\nfrom pydantic import ValidationError\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.ui import SSE_CONTENT_TYPE\nfrom pydantic_ai.ui.vercel_ai import VercelAIAdapter\n\nagent = Agent('openai:gpt-5')\n\napp = FastAPI()\n\n\n@app.post('/chat')\nasync def chat(request: Request) -> Response:\n    accept = request.headers.get('accept', SSE_CONTENT_TYPE)\n    try:\n        run_input = VercelAIAdapter.build_run_input(await request.body())\n    except ValidationError as e:\n        return Response(\n            content=json.dumps(e.json()),\n            media_type='application/json',\n            status_code=HTTPStatus.UNPROCESSABLE_ENTITY,\n        )\n\n    adapter = VercelAIAdapter(agent=agent, run_input=run_input, accept=accept)\n    event_stream = adapter.run_stream()\n\n    sse_event_stream = adapter.encode_stream(event_stream)\n    return StreamingResponse(sse_event_stream, media_type=accept)\n```", "url": "https://ai.pydantic.dev/docs/ui/overview/#advanced-usage", "page": "docs/ui/overview", "source_site": "pydantic_ai"}
{"title": "Durable Execution with Temporal", "anchor": "durable-execution-with-temporal", "heading_level": 1, "md_text": "[Temporal](https://temporal.io) is a popular [durable execution](https://docs.temporal.io/evaluate/understanding-temporal#durable-execution) platform that's natively supported by Pydantic AI.", "url": "https://ai.pydantic.dev/docs/durable_execution/temporal/#durable-execution-with-temporal", "page": "docs/durable_execution/temporal", "source_site": "pydantic_ai"}
{"title": "Durable Execution", "anchor": "durable-execution", "heading_level": 2, "md_text": "In Temporal's durable execution implementation, a program that crashes or encounters an exception while interacting with a model or API will retry until it can successfully complete.\n\nTemporal relies primarily on a replay mechanism to recover from failures.\nAs the program makes progress, Temporal saves key inputs and decisions, allowing a re-started program to pick up right where it left off.\n\nThe key to making this work is to separate the application's repeatable (deterministic) and non-repeatable (non-deterministic) parts:\n\n1. Deterministic pieces, termed [**workflows**](https://docs.temporal.io/workflow-definition), execute the same way when re-run with the same inputs.\n2. Non-deterministic pieces, termed [**activities**](https://docs.temporal.io/activities), can run arbitrary code, performing I/O and any other operations.\n\nWorkflow code can run for extended periods and, if interrupted, resume exactly where it left off.\nCritically, workflow code generally _cannot_ include any kind of I/O, over the network, disk, etc.\nActivity code faces no restrictions on I/O or external interactions, but if an activity fails part-way through it is restarted from the beginning.\n\n\n!!! note\n\n    If you are familiar with celery, it may be helpful to think of Temporal activities as similar to celery tasks, but where you wait for the task to complete and obtain its result before proceeding to the next step in the workflow.\n    However, Temporal workflows and activities offer a great deal more flexibility and functionality than celery tasks.\n\n    See the [Temporal documentation](https://docs.temporal.io/evaluate/understanding-temporal#temporal-application-the-building-blocks) for more information\n\nIn the case of Pydantic AI agents, integration with Temporal means that [model requests](../models/overview.md), [tool calls](../tools.md) that may require I/O, and [MCP server communication](../mcp/client.md) all need to be offloaded to Temporal activities due to their I/O requirements, while the logic that coordinates them (i.e. the agent run) lives in the workflow. Code that handles a scheduled job or web request can then execute the workflow, which will in turn execute the activities as needed.\n\nThe diagram below shows the overall architecture of an agentic application in Temporal.\nThe Temporal Server is responsible for tracking program execution and making sure the associated state is preserved reliably (i.e., stored to an internal database, and possibly replicated across cloud regions).\nTemporal Server manages data in encrypted form, so all data processing occurs on the Worker, which runs the workflow and activities.\n\n\n```text\n            +---------------------+\n            |   Temporal Server   |      (Stores workflow state,\n            +---------------------+       schedules activities,\n                     ^                    persists progress)\n                     |\n        Save state,  |   Schedule Tasks,\n        progress,    |   load state on resume\n        timeouts     |\n                     |\n+------------------------------------------------------+\n|                      Worker                          |\n|   +----------------------------------------------+   |\n|   |              Workflow Code                   |   |\n|   |       (Agent Run Loop)                       |   |\n|   +----------------------------------------------+   |\n|          |          |                |               |\n|          v          v                v               |\n|   +-----------+ +------------+ +-------------+       |\n|   | Activity  | | Activity   | |  Activity   |       |\n|   | (Tool)    | | (MCP Tool) | | (Model API) |       |\n|   +-----------+ +------------+ +-------------+       |\n|         |           |                |               |\n+------------------------------------------------------+\n          |           |                |\n          v           v                v\n      [External APIs, services, databases, etc.]\n```\n\nSee the [Temporal documentation](https://docs.temporal.io/evaluate/understanding-temporal#temporal-application-the-building-blocks) for more information.", "url": "https://ai.pydantic.dev/docs/durable_execution/temporal/#durable-execution", "page": "docs/durable_execution/temporal", "source_site": "pydantic_ai"}
{"title": "Durable Agent", "anchor": "durable-agent", "heading_level": 2, "md_text": "Any agent can be wrapped in a [`TemporalAgent`][pydantic_ai.durable_exec.temporal.TemporalAgent] to get a durable agent that can be used inside a deterministic Temporal workflow, by automatically offloading all work that requires I/O (namely model requests, tool calls, and MCP server communication) to non-deterministic activities.\n\nAt the time of wrapping, the agent's [model](../models/overview.md) and [toolsets](../toolsets.md) (including function tools registered on the agent and MCP servers) are frozen, activities are dynamically created for each, and the original model and toolsets are wrapped to call on the worker to execute the corresponding activities instead of directly performing the actions inside the workflow. The original agent can still be used as normal outside the Temporal workflow, but any changes to its model or toolsets after wrapping will not be reflected in the durable agent.\n\nHere is a simple but complete example of wrapping an agent for durable execution, creating a Temporal workflow with durable execution logic, connecting to a Temporal server, and running the workflow from non-durable code. All it requires is a Temporal server to be [running locally](https://github.com/temporalio/temporal#download-and-start-temporal-server-locally):\n\n```sh\nbrew install temporal\ntemporal server start-dev\n```\n\n```python {title=\"temporal_agent.py\" test=\"skip\"}\nimport uuid\n\nfrom temporalio import workflow\nfrom temporalio.client import Client\nfrom temporalio.worker import Worker\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.durable_exec.temporal import (\n    AgentPlugin,\n    PydanticAIPlugin,\n    TemporalAgent,\n)\n\nagent = Agent(\n    'gpt-5',\n    instructions=\"You're an expert in geography.\",\n    name='geography',  # (10)!\n)\n\ntemporal_agent = TemporalAgent(agent)  # (1)!\n\n\n@workflow.defn\nclass GeographyWorkflow:  # (2)!\n    @workflow.run\n    async def run(self, prompt: str) -> str:\n        result = await temporal_agent.run(prompt)  # (3)!\n        return result.output\n\n\nasync def main():\n    client = await Client.connect(  # (4)!\n        'localhost:7233',  # (5)!\n        plugins=[PydanticAIPlugin()],  # (6)!\n    )\n\n    async with Worker(  # (7)!\n        client,\n        task_queue='geography',\n        workflows=[GeographyWorkflow],\n        plugins=[AgentPlugin(temporal_agent)],  # (8)!\n    ):\n        output = await client.execute_workflow(  # (9)!\n            GeographyWorkflow.run,\n            args=['What is the capital of Mexico?'],\n            id=f'geography-{uuid.uuid4()}',\n            task_queue='geography',\n        )\n        print(output)\n        #> Mexico City (Ciudad de M\u00e9xico, CDMX)\n```\n\n1. The original `Agent` cannot be used inside a deterministic Temporal workflow, but the `TemporalAgent` can.\n2. As explained above, the workflow represents a deterministic piece of code that can use non-deterministic activities for operations that require I/O.\n3. [`TemporalAgent.run()`][pydantic_ai.durable_exec.temporal.TemporalAgent.run] works just like [`Agent.run()`][pydantic_ai.Agent.run], but it will automatically offload model requests, tool calls, and MCP server communication to Temporal activities.\n4. We connect to the Temporal server which keeps track of workflow and activity execution.\n5. This assumes the Temporal server is [running locally](https://github.com/temporalio/temporal#download-and-start-temporal-server-locally).\n6. The [`PydanticAIPlugin`][pydantic_ai.durable_exec.temporal.PydanticAIPlugin] tells Temporal to use Pydantic for serialization and deserialization, and to treat [`UserError`][pydantic_ai.exceptions.UserError] exceptions as non-retryable.\n7. We start the worker that will listen on the specified task queue and run workflows and activities. In a real world application, this might be run in a separate service.\n8. The [`AgentPlugin`][pydantic_ai.durable_exec.temporal.AgentPlugin] registers the `TemporalAgent`'s activities with the worker.\n9. We call on the server to execute the workflow on a worker that's listening on the specified task queue.\n10. The agent's `name` is used to uniquely identify its activities.\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_\n\nIn a real world application, the agent, workflow, and worker are typically defined separately from the code that calls for a workflow to be executed.\nBecause Temporal workflows need to be defined at the top level of the file and the `TemporalAgent` instance is needed inside the workflow and when starting the worker (to register the activities), it needs to be defined at the top level of the file as well.\n\nFor more information on how to use Temporal in Python applications, see their [Python SDK guide](https://docs.temporal.io/develop/python).", "url": "https://ai.pydantic.dev/docs/durable_execution/temporal/#durable-agent", "page": "docs/durable_execution/temporal", "source_site": "pydantic_ai"}
{"title": "Temporal Integration Considerations", "anchor": "temporal-integration-considerations", "heading_level": 2, "md_text": "There are a few considerations specific to agents and toolsets when using Temporal for durable execution. These are important to understand to ensure that your agents and toolsets work correctly with Temporal's workflow and activity model.", "url": "https://ai.pydantic.dev/docs/durable_execution/temporal/#temporal-integration-considerations", "page": "docs/durable_execution/temporal", "source_site": "pydantic_ai"}
{"title": "Agent Names and Toolset IDs", "anchor": "agent-names-and-toolset-ids", "heading_level": 3, "md_text": "To ensure that Temporal knows what code to run when an activity fails or is interrupted and then restarted, even if your code is changed in between, each activity needs to have a name that's stable and unique.\n\nWhen `TemporalAgent` dynamically creates activities for the wrapped agent's model requests and toolsets (specifically those that implement their own tool listing and calling, i.e. [`FunctionToolset`][pydantic_ai.toolsets.FunctionToolset] and [`MCPServer`][pydantic_ai.mcp.MCPServer]), their names are derived from the agent's [`name`][pydantic_ai.agent.AbstractAgent.name] and the toolsets' [`id`s][pydantic_ai.toolsets.AbstractToolset.id]. These fields are normally optional, but are required to be set when using Temporal. They should not be changed once the durable agent has been deployed to production as this would break active workflows.\n\nOther than that, any agent and toolset will just work!", "url": "https://ai.pydantic.dev/docs/durable_execution/temporal/#agent-names-and-toolset-ids", "page": "docs/durable_execution/temporal", "source_site": "pydantic_ai"}
{"title": "Instructions Functions, Output Functions, and History Processors", "anchor": "instructions-functions-output-functions-and-history-processors", "heading_level": 3, "md_text": "Pydantic AI runs non-async [instructions](../agents.md#instructions) and [system prompt](../agents.md#system-prompts) functions, [history processors](../message-history.md#processing-message-history), [output functions](../output.md#output-functions), and [output validators](../output.md#output-validator-functions) in threads, which are not supported inside Temporal workflows and require an activity. Ensure that these functions are async instead.\n\nSynchronous tool functions are supported, as tools are automatically run in activities unless this is [explicitly disabled](#activity-configuration). Still, it's recommended to make tool functions async as well to improve performance.", "url": "https://ai.pydantic.dev/docs/durable_execution/temporal/#instructions-functions-output-functions-and-history-processors", "page": "docs/durable_execution/temporal", "source_site": "pydantic_ai"}
{"title": "Agent Run Context and Dependencies", "anchor": "agent-run-context-and-dependencies", "heading_level": 3, "md_text": "As workflows and activities run in separate processes, any values passed between them need to be serializable. As these payloads are stored in the workflow execution event history, Temporal limits their size to 2MB.\n\nTo account for these limitations, tool functions and the [event stream handler](#streaming) running inside activities receive a limited version of the agent's [`RunContext`][pydantic_ai.tools.RunContext], and it's your responsibility to make sure that the [dependencies](../dependencies.md) object provided to [`TemporalAgent.run()`][pydantic_ai.durable_exec.temporal.TemporalAgent.run] can be serialized using Pydantic.\n\nSpecifically, only the `deps`, `retries`, `tool_call_id`, `tool_name`, `tool_call_approved`, `retry`, `max_retries` and `run_step` fields are available by default, and trying to access `model`, `usage`, `prompt`, `messages`, or `tracer` will raise an error.\nIf you need one or more of these attributes to be available inside activities, you can create a [`TemporalRunContext`][pydantic_ai.durable_exec.temporal.TemporalRunContext] subclass with custom `serialize_run_context` and `deserialize_run_context` class methods and pass it to [`TemporalAgent`][pydantic_ai.durable_exec.temporal.TemporalAgent] as `run_context_type`.", "url": "https://ai.pydantic.dev/docs/durable_execution/temporal/#agent-run-context-and-dependencies", "page": "docs/durable_execution/temporal", "source_site": "pydantic_ai"}
{"title": "Streaming", "anchor": "streaming", "heading_level": 3, "md_text": "Because Temporal activities cannot stream output directly to the activity call site, [`Agent.run_stream()`][pydantic_ai.Agent.run_stream], [`Agent.run_stream_events()`][pydantic_ai.Agent.run_stream_events], and [`Agent.iter()`][pydantic_ai.Agent.iter] are not supported.\n\nInstead, you can implement streaming by setting an [`event_stream_handler`][pydantic_ai.agent.EventStreamHandler] on the `Agent` or `TemporalAgent` instance and using [`TemporalAgent.run()`][pydantic_ai.durable_exec.temporal.TemporalAgent.run] inside the workflow.\nThe event stream handler function will receive the agent [run context][pydantic_ai.tools.RunContext] and an async iterable of events from the model's streaming response and the agent's execution of tools. For examples, see the [streaming docs](../agents.md#streaming-all-events).\n\nAs the streaming model request activity, workflow, and workflow execution call all take place in separate processes, passing data between them requires some care:\n\n- To get data from the workflow call site or workflow to the event stream handler, you can use a [dependencies object](#agent-run-context-and-dependencies).\n- To get data from the event stream handler to the workflow, workflow call site, or a frontend, you need to use an external system that the event stream handler can write to and the event consumer can read from, like a message queue. You can use the dependency object to make sure the same connection string or other unique ID is available in all the places that need it.", "url": "https://ai.pydantic.dev/docs/durable_execution/temporal/#streaming", "page": "docs/durable_execution/temporal", "source_site": "pydantic_ai"}
{"title": "Activity Configuration", "anchor": "activity-configuration", "heading_level": 2, "md_text": "Temporal activity configuration, like timeouts and retry policies, can be customized by passing [`temporalio.workflow.ActivityConfig`](https://python.temporal.io/temporalio.workflow.ActivityConfig.html) objects to the `TemporalAgent` constructor:\n\n- `activity_config`: The base Temporal activity config to use for all activities. If no config is provided, a `start_to_close_timeout` of 60 seconds is used.\n- `model_activity_config`: The Temporal activity config to use for model request activities. This is merged with the base activity config.\n- `toolset_activity_config`: The Temporal activity config to use for get-tools and call-tool activities for specific toolsets identified by ID. This is merged with the base activity config.\n- `tool_activity_config`: The Temporal activity config to use for specific tool call activities identified by toolset ID and tool name.\n    This is merged with the base and toolset-specific activity configs.\n\n    If a tool does not use I/O, you can specify `False` to disable using an activity. Note that the tool is required to be defined as an `async` function as non-async tools are run in threads which are non-deterministic and thus not supported outside of activities.", "url": "https://ai.pydantic.dev/docs/durable_execution/temporal/#activity-configuration", "page": "docs/durable_execution/temporal", "source_site": "pydantic_ai"}
{"title": "Activity Retries", "anchor": "activity-retries", "heading_level": 2, "md_text": "On top of the automatic retries for request failures that Temporal will perform, Pydantic AI and various provider API clients also have their own request retry logic. Enabling these at the same time may cause the request to be retried more often than expected, with improper `Retry-After` handling.\n\nWhen using Temporal, it's recommended to not use [HTTP Request Retries](../retries.md) and to turn off your provider API client's own retry logic, for example by setting `max_retries=0` on a [custom `OpenAIProvider` API client](../models/openai.md#custom-openai-client).\n\nYou can customize Temporal's retry policy using [activity configuration](#activity-configuration).", "url": "https://ai.pydantic.dev/docs/durable_execution/temporal/#activity-retries", "page": "docs/durable_execution/temporal", "source_site": "pydantic_ai"}
{"title": "Observability with Logfire", "anchor": "observability-with-logfire", "heading_level": 2, "md_text": "Temporal generates telemetry events and metrics for each workflow and activity execution, and Pydantic AI generates events for each agent run, model request and tool call. These can be sent to [Pydantic Logfire](../logfire.md) to get a complete picture of what's happening in your application.\n\nTo use Logfire with Temporal, you need to pass a [`LogfirePlugin`][pydantic_ai.durable_exec.temporal.LogfirePlugin] object to Temporal's `Client.connect()`:\n\n```py {title=\"logfire_plugin.py\" test=\"skip\" noqa=\"F841\"}\nfrom temporalio.client import Client\n\nfrom pydantic_ai.durable_exec.temporal import LogfirePlugin, PydanticAIPlugin\n\n\nasync def main():\n    client = await Client.connect(\n        'localhost:7233',\n        plugins=[PydanticAIPlugin(), LogfirePlugin()],\n    )\n```\n\nBy default, the `LogfirePlugin` will instrument Temporal (including metrics) and Pydantic AI and send all data to Logfire. To customize Logfire configuration and instrumentation, you can pass a `logfire_setup` function to the `LogfirePlugin` constructor and return a custom `Logfire` instance (i.e. the result of `logfire.configure()`). To disable sending Temporal metrics to Logfire, you can pass `metrics=False` to the `LogfirePlugin` constructor.", "url": "https://ai.pydantic.dev/docs/durable_execution/temporal/#observability-with-logfire", "page": "docs/durable_execution/temporal", "source_site": "pydantic_ai"}
{"title": "Pandas", "anchor": "pandas", "heading_level": 3, "md_text": "When `logfire.info` is used inside an activity and the `pandas` package is among your project's dependencies, you may encounter the following error which seems to be the result of an import race condition:\n\n```\nAttributeError: partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)\n```\n\nTo fix this, you can use the [`temporalio.workflow.unsafe.imports_passed_through()`](https://python.temporal.io/temporalio.workflow.unsafe.html#imports_passed_through) context manager to proactively import the package and not have it be reloaded in the workflow sandbox:\n\n```python {title=\"temporal_activity.py\" test=\"skip\" noqa=\"F401\"}\nfrom temporalio import workflow\n\nwith workflow.unsafe.imports_passed_through():\n    import pandas\n```", "url": "https://ai.pydantic.dev/docs/durable_execution/temporal/#pandas", "page": "docs/durable_execution/temporal", "source_site": "pydantic_ai"}
{"title": "Durable Execution with Prefect", "anchor": "durable-execution-with-prefect", "heading_level": 1, "md_text": "[Prefect](https://www.prefect.io/) is a workflow orchestration framework for building resilient data pipelines in Python, natively integrated with Pydantic AI.", "url": "https://ai.pydantic.dev/docs/durable_execution/prefect/#durable-execution-with-prefect", "page": "docs/durable_execution/prefect", "source_site": "pydantic_ai"}
{"title": "Durable Execution", "anchor": "durable-execution", "heading_level": 2, "md_text": "Prefect 3.0 brings [transactional semantics](https://www.prefect.io/blog/transactional-ml-pipelines-with-prefect-3-0) to your Python workflows, allowing you to group tasks into atomic units and define failure modes. If any part of a transaction fails, the entire transaction can be rolled back to a clean state.\n\n* **Flows** are the top-level entry points for your workflow. They can contain tasks and other flows.\n* **Tasks** are individual units of work that can be retried, cached, and monitored independently.\n\nPrefect 3.0's approach to transactional orchestration makes your workflows automatically **idempotent**: rerunnable without duplication or inconsistency across any environment. Every task is executed within a transaction that governs when and where the task's result record is persisted. If the task runs again under an identical context, it will not re-execute but instead load its previous result.\n\nThe diagram below shows the overall architecture of an agentic application with Prefect.\nPrefect uses client-side task orchestration by default, with optional server connectivity for advanced features like scheduling and monitoring.\n\n```text\n            +---------------------+\n            |   Prefect Server    |      (Monitoring,\n            |      or Cloud       |       scheduling, UI,\n            +---------------------+       orchestration)\n                     ^\n                     |\n        Flow state,  |   Schedule flows,\n        metadata,    |   track execution\n        logs         |\n                     |\n+------------------------------------------------------+\n|               Application Process                    |\n|   +----------------------------------------------+   |\n|   |              Flow (Agent.run)                |   |\n|   +----------------------------------------------+   |\n|          |          |                |               |\n|          v          v                v               |\n|   +-----------+ +------------+ +-------------+       |\n|   |   Task    | |    Task    | |    Task     |       |\n|   |  (Tool)   | | (MCP Tool) | | (Model API) |       |\n|   +-----------+ +------------+ +-------------+       |\n|         |           |                |               |\n|       Cache &     Cache &          Cache &           |\n|       persist     persist          persist           |\n|         to           to               to             |\n|         v            v                v              |\n|   +----------------------------------------------+   |\n|   |     Result Storage (Local FS, S3, etc.)     |    |\n|   +----------------------------------------------+   |\n+------------------------------------------------------+\n          |           |                |\n          v           v                v\n      [External APIs, services, databases, etc.]\n```\n\nSee the [Prefect documentation](https://docs.prefect.io/) for more information.", "url": "https://ai.pydantic.dev/docs/durable_execution/prefect/#durable-execution", "page": "docs/durable_execution/prefect", "source_site": "pydantic_ai"}
{"title": "Durable Agent", "anchor": "durable-agent", "heading_level": 2, "md_text": "Any agent can be wrapped in a [`PrefectAgent`][pydantic_ai.durable_exec.prefect.PrefectAgent] to get durable execution. `PrefectAgent` automatically:\n\n* Wraps [`Agent.run`][pydantic_ai.Agent.run] and [`Agent.run_sync`][pydantic_ai.Agent.run_sync] as Prefect flows.\n* Wraps [model requests](../models/overview.md) as Prefect tasks.\n* Wraps [tool calls](../tools.md) as Prefect tasks (configurable per-tool).\n* Wraps [MCP communication](../mcp/client.md) as Prefect tasks.\n\nEvent stream handlers are **automatically wrapped** by Prefect when running inside a Prefect flow. Each event from the stream is processed in a separate Prefect task for durability. You can customize the task behavior using the `event_stream_handler_task_config` parameter when creating the `PrefectAgent`. Do **not** manually decorate event stream handlers with `@task`. For examples, see the [streaming docs](../agents.md#streaming-all-events)\n\nThe original agent, model, and MCP server can still be used as normal outside the Prefect flow.\n\nHere is a simple but complete example of wrapping an agent for durable execution. All it requires is to install Pydantic AI with Prefect:\n\n```bash\npip/uv-add pydantic-ai[prefect]\n```\n\nOr if you're using the slim package, you can install it with the `prefect` optional group:\n\n```bash\npip/uv-add pydantic-ai-slim[prefect]\n```\n\n```python {title=\"prefect_agent.py\" test=\"skip\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.durable_exec.prefect import PrefectAgent\n\nagent = Agent(\n    'gpt-5',\n    instructions=\"You're an expert in geography.\",\n    name='geography',  # (1)!\n)\n\nprefect_agent = PrefectAgent(agent)  # (2)!\n\nasync def main():\n    result = await prefect_agent.run('What is the capital of Mexico?')  # (3)!\n    print(result.output)\n    #> Mexico City (Ciudad de M\u00e9xico, CDMX)\n```\n\n1. The agent's `name` is used to uniquely identify its flows and tasks.\n2. Wrapping the agent with `PrefectAgent` enables durable execution for all agent runs.\n3. [`PrefectAgent.run()`][pydantic_ai.durable_exec.prefect.PrefectAgent.run] works like [`Agent.run()`][pydantic_ai.Agent.run], but runs as a Prefect flow and executes model requests, decorated tool calls, and MCP communication as Prefect tasks.\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_\n\nFor more information on how to use Prefect in Python applications, see their [Python documentation](https://docs.prefect.io/v3/how-to-guides/workflows/write-and-run).", "url": "https://ai.pydantic.dev/docs/durable_execution/prefect/#durable-agent", "page": "docs/durable_execution/prefect", "source_site": "pydantic_ai"}
{"title": "Prefect Integration Considerations", "anchor": "prefect-integration-considerations", "heading_level": 2, "md_text": "When using Prefect with Pydantic AI agents, there are a few important considerations to ensure workflows behave correctly.", "url": "https://ai.pydantic.dev/docs/durable_execution/prefect/#prefect-integration-considerations", "page": "docs/durable_execution/prefect", "source_site": "pydantic_ai"}
{"title": "Agent Requirements", "anchor": "agent-requirements", "heading_level": 3, "md_text": "Each agent instance must have a unique `name` so Prefect can correctly identify and track its flows and tasks.", "url": "https://ai.pydantic.dev/docs/durable_execution/prefect/#agent-requirements", "page": "docs/durable_execution/prefect", "source_site": "pydantic_ai"}
{"title": "Tool Wrapping", "anchor": "tool-wrapping", "heading_level": 3, "md_text": "Agent tools are automatically wrapped as Prefect tasks, which means they benefit from:\n\n* **Retry logic**: Failed tool calls can be retried automatically\n* **Caching**: Tool results are cached based on their inputs\n* **Observability**: Tool execution is tracked in the Prefect UI\n\nYou can customize tool task behavior using `tool_task_config` (applies to all tools) or `tool_task_config_by_name` (per-tool configuration):\n\n```python {title=\"prefect_agent_config.py\" test=\"skip\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.durable_exec.prefect import PrefectAgent, TaskConfig\n\nagent = Agent('gpt-5', name='my_agent')\n\n@agent.tool_plain\ndef fetch_data(url: str) -> str:\n    # This tool will be wrapped as a Prefect task\n    ...\n\nprefect_agent = PrefectAgent(\n    agent,\n    tool_task_config=TaskConfig(retries=3),  # Default for all tools\n    tool_task_config_by_name={\n        'fetch_data': TaskConfig(timeout_seconds=10.0),  # Specific to fetch_data\n        'simple_tool': None,  # Disable task wrapping for simple_tool\n    },\n)\n```\n\nSet a tool's config to `None` in `tool_task_config_by_name` to disable task wrapping for that specific tool.", "url": "https://ai.pydantic.dev/docs/durable_execution/prefect/#tool-wrapping", "page": "docs/durable_execution/prefect", "source_site": "pydantic_ai"}
{"title": "Streaming", "anchor": "streaming", "heading_level": 3, "md_text": "When running inside a Prefect flow, [`Agent.run_stream()`][pydantic_ai.Agent.run_stream] works but doesn't provide real-time streaming because Prefect tasks consume their entire execution before returning results. The method will execute fully and return the complete result at once.\n\nFor real-time streaming behavior inside Prefect flows, you can set an [`event_stream_handler`][pydantic_ai.agent.EventStreamHandler] on the `Agent` or `PrefectAgent` instance and use [`PrefectAgent.run()`][pydantic_ai.durable_exec.prefect.PrefectAgent.run].\n\n**Note**: Event stream handlers behave differently when running inside a Prefect flow versus outside:\n- **Outside a flow**: The handler receives events as they stream from the model\n- **Inside a flow**: Each event is wrapped as a Prefect task for durability, which may affect timing but ensures reliability\n\nThe event stream handler function will receive the agent [run context][pydantic_ai.tools.RunContext] and an async iterable of events from the model's streaming response and the agent's execution of tools. For examples, see the [streaming docs](../agents.md#streaming-all-events).", "url": "https://ai.pydantic.dev/docs/durable_execution/prefect/#streaming", "page": "docs/durable_execution/prefect", "source_site": "pydantic_ai"}
{"title": "Task Configuration", "anchor": "task-configuration", "heading_level": 2, "md_text": "You can customize Prefect task behavior, such as retries and timeouts, by passing [`TaskConfig`][pydantic_ai.durable_exec.prefect.TaskConfig] objects to the `PrefectAgent` constructor:\n\n- `mcp_task_config`: Configuration for MCP server communication tasks\n- `model_task_config`: Configuration for model request tasks\n- `tool_task_config`: Default configuration for all tool calls\n- `tool_task_config_by_name`: Per-tool task configuration (overrides `tool_task_config`)\n- `event_stream_handler_task_config`: Configuration for event stream handler tasks (applies when running inside a Prefect flow)\n\nAvailable `TaskConfig` options:\n\n- `retries`: Maximum number of retries for the task (default: `0`)\n- `retry_delay_seconds`: Delay between retries in seconds (can be a single value or list for exponential backoff, default: `1.0`)\n- `timeout_seconds`: Maximum time in seconds for the task to complete\n- `cache_policy`: Custom Prefect cache policy for the task\n- `persist_result`: Whether to persist the task result\n- `result_storage`: Prefect result storage for the task (e.g., `'s3-bucket/my-storage'` or a `WritableFileSystem` block)\n- `log_prints`: Whether to log print statements from the task (default: `False`)\n\nExample:\n\n```python {title=\"prefect_agent_config.py\" test=\"skip\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.durable_exec.prefect import PrefectAgent, TaskConfig\n\nagent = Agent(\n    'gpt-5',\n    instructions=\"You're an expert in geography.\",\n    name='geography',\n)\n\nprefect_agent = PrefectAgent(\n    agent,\n    model_task_config=TaskConfig(\n        retries=3,\n        retry_delay_seconds=[1.0, 2.0, 4.0],  # Exponential backoff\n        timeout_seconds=30.0,\n    ),\n)\n\nasync def main():\n    result = await prefect_agent.run('What is the capital of France?')\n    print(result.output)\n    #> Paris\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/durable_execution/prefect/#task-configuration", "page": "docs/durable_execution/prefect", "source_site": "pydantic_ai"}
{"title": "Retry Considerations", "anchor": "retry-considerations", "heading_level": 3, "md_text": "Pydantic AI and provider API clients have their own retry logic. When using Prefect, you may want to:\n\n* Disable [HTTP Request Retries](../retries.md) in Pydantic AI\n* Turn off your provider API client's retry logic (e.g., `max_retries=0` on a [custom OpenAI client](../models/openai.md#custom-openai-client))\n* Rely on Prefect's task-level retry configuration for consistency\n\nThis prevents requests from being retried multiple times at different layers.", "url": "https://ai.pydantic.dev/docs/durable_execution/prefect/#retry-considerations", "page": "docs/durable_execution/prefect", "source_site": "pydantic_ai"}
{"title": "Caching and Idempotency", "anchor": "caching-and-idempotency", "heading_level": 2, "md_text": "Prefect 3.0 provides built-in caching and transactional semantics. Tasks with identical inputs will not re-execute if their results are already cached, making workflows naturally idempotent and resilient to failures.\n\n* **Task inputs**: Messages, settings, parameters, tool arguments, and serializable dependencies\n\n**Note**: For user dependencies to be included in cache keys, they must be serializable (e.g., Pydantic models or basic Python types). Non-serializable dependencies are automatically excluded from cache computation.", "url": "https://ai.pydantic.dev/docs/durable_execution/prefect/#caching-and-idempotency", "page": "docs/durable_execution/prefect", "source_site": "pydantic_ai"}
{"title": "Observability with Prefect and Logfire", "anchor": "observability-with-prefect-and-logfire", "heading_level": 2, "md_text": "Prefect provides a built-in UI for monitoring flow runs, task executions, and failures. You can:\n\n* View real-time flow run status\n* Debug failures with full stack traces\n* Set up alerts and notifications\n\nTo access the Prefect UI, you can either:\n\n1. Use [Prefect Cloud](https://www.prefect.io/cloud) (managed service)\n2. Run a local [Prefect server](https://docs.prefect.io/v3/how-to-guides/self-hosted/server-cli) with `prefect server start`\n\nYou can also use [Pydantic Logfire](../logfire.md) for detailed observability. When using both Prefect and Logfire, you'll get complementary views:\n\n* **Prefect**: Workflow-level orchestration, task status, and retry history\n* **Logfire**: Fine-grained tracing of agent runs, model requests, and tool invocations\n\nWhen using Logfire with Prefect, you can enable distributed tracing to see spans for your Prefect runs included with your agent runs, model requests, and tool invocations.\n\nFor more information about Prefect monitoring, see the [Prefect documentation](https://docs.prefect.io/).", "url": "https://ai.pydantic.dev/docs/durable_execution/prefect/#observability-with-prefect-and-logfire", "page": "docs/durable_execution/prefect", "source_site": "pydantic_ai"}
{"title": "Deployments and Scheduling", "anchor": "deployments-and-scheduling", "heading_level": 2, "md_text": "To deploy and schedule a `PrefectAgent`, wrap it in a Prefect flow and use the flow's [`serve()`](https://docs.prefect.io/v3/how-to-guides/deployments/create-deployments#create-a-deployment-with-serve) or [`deploy()`](https://docs.prefect.io/v3/how-to-guides/deployments/deploy-via-python) methods:\n\n```python {title=\"serve_agent.py\" test=\"skip\"}\nfrom prefect import flow\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.durable_exec.prefect import PrefectAgent\n\n\n@flow\nasync def daily_report_flow(user_prompt: str):\n    \"\"\"Generate a daily report using the agent.\"\"\"\n    agent = Agent(  # (1)!\n        'openai:gpt-5',\n        name='daily_report_agent',\n        instructions='Generate a daily summary report.',\n    )\n\n    prefect_agent = PrefectAgent(agent)\n\n    result = await prefect_agent.run(user_prompt)\n    return result.output", "url": "https://ai.pydantic.dev/docs/durable_execution/prefect/#deployments-and-scheduling", "page": "docs/durable_execution/prefect", "source_site": "pydantic_ai"}
{"title": "Serve the flow with a daily schedule", "anchor": "serve-the-flow-with-a-daily-schedule", "heading_level": 1, "md_text": "if __name__ == '__main__':\n    daily_report_flow.serve(\n        name='daily-report-deployment',\n        cron='0 9 * * *',  # Run daily at 9am\n        parameters={'user_prompt': \"Generate today's report\"},\n        tags=['production', 'reports'],\n    )\n```\n\n1. Each flow run executes in an isolated process, and all inputs and dependencies must be serializable. Because Agent instances cannot be serialized, instantiate the agent inside the flow rather than at the module level.\n\nThe `serve()` method accepts scheduling options:\n\n- **`cron`**: Cron schedule string (e.g., `'0 9 * * *'` for daily at 9am)\n- **`interval`**: Schedule interval in seconds or as a timedelta\n- **`rrule`**: iCalendar RRule schedule string\n\nFor production deployments with Docker, Kubernetes, or other infrastructure, use the flow's [`deploy()`](https://docs.prefect.io/v3/how-to-guides/deployments/deploy-via-python) method. See the [Prefect deployment documentation](https://docs.prefect.io/v3/how-to-guides/deployments/create-deploymentsy) for more information.", "url": "https://ai.pydantic.dev/docs/durable_execution/prefect/#serve-the-flow-with-a-daily-schedule", "page": "docs/durable_execution/prefect", "source_site": "pydantic_ai"}
{"title": "Durable Execution with DBOS", "anchor": "durable-execution-with-dbos", "heading_level": 1, "md_text": "[DBOS](https://www.dbos.dev/) is a lightweight [durable execution](https://docs.dbos.dev/architecture) library natively integrated with Pydantic AI.", "url": "https://ai.pydantic.dev/docs/durable_execution/dbos/#durable-execution-with-dbos", "page": "docs/durable_execution/dbos", "source_site": "pydantic_ai"}
{"title": "Durable Execution", "anchor": "durable-execution", "heading_level": 2, "md_text": "DBOS workflows make your program **durable** by checkpointing its state in a database. If your program ever fails, when it restarts all your workflows will automatically resume from the last completed step.\n\n* **Workflows** must be deterministic and generally cannot include I/O.\n* **Steps** may perform I/O (network, disk, API calls). If a step fails, it restarts from the beginning.\n\nEvery workflow input and step output is durably stored in the system database. When workflow execution fails, whether from crashes, network issues, or server restarts, DBOS leverages these checkpoints to recover workflows from their last completed step.\n\nDBOS **queues** provide durable, database-backed alternatives to systems like Celery or BullMQ, supporting features such as concurrency limits, rate limits, timeouts, and prioritization. See the [DBOS docs](https://docs.dbos.dev/architecture) for details.\n\nThe diagram below shows the overall architecture of an agentic application in DBOS.\nDBOS runs fully in-process as a library. Functions remain normal Python functions but are checkpointed into a database (Postgres or SQLite).\n\n```text\n                    Clients\n            (HTTP, RPC, Kafka, etc.)\n                        |\n                        v\n+------------------------------------------------------+\n|               Application Servers                    |\n|                                                      |\n|   +----------------------------------------------+   |\n|   |        Pydantic AI + DBOS Libraries          |   |\n|   |                                              |   |\n|   |  [ Workflows (Agent Run Loop) ]              |   |\n|   |  [ Steps (Tool, MCP, Model) ]                |   |\n|   |  [ Queues ]   [ Cron Jobs ]   [ Messaging ]  |   |\n|   +----------------------------------------------+   |\n|                                                      |\n+------------------------------------------------------+\n                        |\n                        v\n+------------------------------------------------------+\n|                      Database                        |\n|   (Stores workflow and step state, schedules tasks)  |\n+------------------------------------------------------+\n```\n\nSee the [DBOS documentation](https://docs.dbos.dev/architecture) for more information.", "url": "https://ai.pydantic.dev/docs/durable_execution/dbos/#durable-execution", "page": "docs/durable_execution/dbos", "source_site": "pydantic_ai"}
{"title": "Durable Agent", "anchor": "durable-agent", "heading_level": 2, "md_text": "Any agent can be wrapped in a [`DBOSAgent`][pydantic_ai.durable_exec.dbos.DBOSAgent] to get durable execution. `DBOSAgent` automatically:,\n\n* Wraps `Agent.run` and `Agent.run_sync` as DBOS workflows.\n* Wraps [model requests](../models/overview.md) and [MCP communication](../mcp/client.md) as DBOS steps.\n\nCustom tool functions and event stream handlers are **not automatically wrapped** by DBOS.\nIf they involve non-deterministic behavior or perform I/O, you should explicitly decorate them with `@DBOS.step`.\n\nThe original agent, model, and MCP server can still be used as normal outside the DBOS workflow.\n\nHere is a simple but complete example of wrapping an agent for durable execution. All it requires is to install Pydantic AI with the DBOS [open-source library](https://github.com/dbos-inc/dbos-transact-py):\n\n```bash\npip/uv-add pydantic-ai[dbos]\n```\n\nOr if you're using the slim package, you can install it with the `dbos` optional group:\n\n```bash\npip/uv-add pydantic-ai-slim[dbos]\n```\n\n```python {title=\"dbos_agent.py\" test=\"skip\"}\nfrom dbos import DBOS, DBOSConfig\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.durable_exec.dbos import DBOSAgent\n\ndbos_config: DBOSConfig = {\n    'name': 'pydantic_dbos_agent',\n    'system_database_url': 'sqlite:///dbostest.sqlite',  # (3)!\n}\nDBOS(config=dbos_config)\n\nagent = Agent(\n    'gpt-5',\n    instructions=\"You're an expert in geography.\",\n    name='geography',  # (4)!\n)\n\ndbos_agent = DBOSAgent(agent)  # (1)!\n\nasync def main():\n    DBOS.launch()\n    result = await dbos_agent.run('What is the capital of Mexico?')  # (2)!\n    print(result.output)\n    #> Mexico City (Ciudad de M\u00e9xico, CDMX)\n```\n\n1. Workflows and `DBOSAgent` must be defined before `DBOS.launch()` so that recovery can correctly find all workflows.\n2. [`DBOSAgent.run()`][pydantic_ai.durable_exec.dbos.DBOSAgent.run] works like [`Agent.run()`][pydantic_ai.Agent.run], but runs as a DBOS workflow and executes model requests, decorated tool calls, and MCP communication as DBOS steps.\n3. This example uses SQLite. Postgres is recommended for production.\n4. The agent's `name` is used to uniquely identify its workflows.\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main())` to run `main`)_\n\nBecause DBOS workflows need to be defined before calling `DBOS.launch()` and the `DBOSAgent` instance automatically registers `run` and `run_sync` as workflows, it needs to be defined before calling `DBOS.launch()` as well.\n\nFor more information on how to use DBOS in Python applications, see their [Python SDK guide](https://docs.dbos.dev/python/programming-guide).", "url": "https://ai.pydantic.dev/docs/durable_execution/dbos/#durable-agent", "page": "docs/durable_execution/dbos", "source_site": "pydantic_ai"}
{"title": "DBOS Integration Considerations", "anchor": "dbos-integration-considerations", "heading_level": 2, "md_text": "When using DBOS with Pydantic AI agents, there are a few important considerations to ensure workflows and toolsets behave correctly.", "url": "https://ai.pydantic.dev/docs/durable_execution/dbos/#dbos-integration-considerations", "page": "docs/durable_execution/dbos", "source_site": "pydantic_ai"}
{"title": "Agent and Toolset Requirements", "anchor": "agent-and-toolset-requirements", "heading_level": 3, "md_text": "Each agent instance must have a unique `name` so DBOS can correctly resume workflows after a failure or restart.\n\nTools and event stream handlers are not automatically wrapped by DBOS. You can decide how to integrate them:\n\n* Decorate with `@DBOS.step` if the function involves non-determinism or I/O.\n* Skip the decorator if durability isn't needed, so you avoid the extra DB checkpoint write.\n* If the function needs to enqueue tasks or invoke other DBOS workflows, run it inside the agent's main workflow (not as a step).\n\nOther than that, any agent and toolset will just work!", "url": "https://ai.pydantic.dev/docs/durable_execution/dbos/#agent-and-toolset-requirements", "page": "docs/durable_execution/dbos", "source_site": "pydantic_ai"}
{"title": "Agent Run Context and Dependencies", "anchor": "agent-run-context-and-dependencies", "heading_level": 3, "md_text": "DBOS checkpoints workflow inputs/outputs and step outputs into a database using [`pickle`](https://docs.python.org/3/library/pickle.html). This means you need to make sure [dependencies](../dependencies.md) object provided to [`DBOSAgent.run()`][pydantic_ai.durable_exec.dbos.DBOSAgent.run] or [`DBOSAgent.run_sync()`][pydantic_ai.durable_exec.dbos.DBOSAgent.run_sync], and tool outputs can be serialized using pickle. You may also want to keep the inputs and outputs small (under \\~2 MB). PostgreSQL and SQLite support up to 1 GB per field, but large objects may impact performance.", "url": "https://ai.pydantic.dev/docs/durable_execution/dbos/#agent-run-context-and-dependencies", "page": "docs/durable_execution/dbos", "source_site": "pydantic_ai"}
{"title": "Streaming", "anchor": "streaming", "heading_level": 3, "md_text": "Because DBOS cannot stream output directly to the workflow or step call site, [`Agent.run_stream()`][pydantic_ai.Agent.run_stream] and [`Agent.run_stream_events()`][pydantic_ai.Agent.run_stream_events] are not supported when running inside of a DBOS workflow.\n\nInstead, you can implement streaming by setting an [`event_stream_handler`][pydantic_ai.agent.EventStreamHandler] on the `Agent` or `DBOSAgent` instance and using [`DBOSAgent.run()`][pydantic_ai.durable_exec.dbos.DBOSAgent.run].\nThe event stream handler function will receive the agent [run context][pydantic_ai.tools.RunContext] and an async iterable of events from the model's streaming response and the agent's execution of tools. For examples, see the [streaming docs](../agents.md#streaming-all-events).", "url": "https://ai.pydantic.dev/docs/durable_execution/dbos/#streaming", "page": "docs/durable_execution/dbos", "source_site": "pydantic_ai"}
{"title": "Step Configuration", "anchor": "step-configuration", "heading_level": 2, "md_text": "You can customize DBOS step behavior, such as retries, by passing [`StepConfig`][pydantic_ai.durable_exec.dbos.StepConfig] objects to the `DBOSAgent` constructor:\n\n- `mcp_step_config`: The DBOS step config to use for MCP server communication. No retries if omitted.\n- `model_step_config`: The DBOS step config to use for model request steps. No retries if omitted.\n\nFor custom tools, you can annotate them directly with [`@DBOS.step`](https://docs.dbos.dev/python/reference/decorators#step) or [`@DBOS.workflow`](https://docs.dbos.dev/python/reference/decorators#workflow) decorators as needed. These decorators have no effect outside DBOS workflows, so tools remain usable in non-DBOS agents.", "url": "https://ai.pydantic.dev/docs/durable_execution/dbos/#step-configuration", "page": "docs/durable_execution/dbos", "source_site": "pydantic_ai"}
{"title": "Step Retries", "anchor": "step-retries", "heading_level": 2, "md_text": "On top of the automatic retries for request failures that DBOS will perform, Pydantic AI and various provider API clients also have their own request retry logic. Enabling these at the same time may cause the request to be retried more often than expected, with improper `Retry-After` handling.\n\nWhen using DBOS, it's recommended to not use [HTTP Request Retries](../retries.md) and to turn off your provider API client's own retry logic, for example by setting `max_retries=0` on a [custom `OpenAIProvider` API client](../models/openai.md#custom-openai-client).\n\nYou can customize DBOS's retry policy using [step configuration](#step-configuration).", "url": "https://ai.pydantic.dev/docs/durable_execution/dbos/#step-retries", "page": "docs/durable_execution/dbos", "source_site": "pydantic_ai"}
{"title": "Observability with Logfire", "anchor": "observability-with-logfire", "heading_level": 2, "md_text": "DBOS can be configured to generate OpenTelemetry spans for each workflow and step execution, and Pydantic AI emits spans for each agent run, model request, and tool invocation. You can send these spans to [Pydantic Logfire](../logfire.md) to get a full, end-to-end view of what's happening in your application.\n\nFor more information about DBOS logging and tracing, please see the [DBOS docs](https://docs.dbos.dev/python/tutorials/logging-and-tracing) for details.", "url": "https://ai.pydantic.dev/docs/durable_execution/dbos/#observability-with-logfire", "page": "docs/durable_execution/dbos", "source_site": "pydantic_ai"}
{"title": "Durable Execution", "anchor": "durable-execution", "heading_level": 1, "md_text": "Pydantic AI allows you to build durable agents that can preserve their progress across transient API failures and application errors or restarts, and handle long-running, asynchronous, and human-in-the-loop workflows with production-grade reliability. Durable agents have full support for [streaming](../agents.md#streaming-all-events) and [MCP](../mcp/client.md), with the added benefit of fault tolerance.\n\nPydantic AI natively supports three durable execution solutions:\n\n- [Temporal](./temporal.md)\n- [DBOS](./dbos.md)\n- [Prefect](./prefect.md)\n\nThese integrations only use Pydantic AI's public interface, so they also serve as a reference for integrating with other durable systems.", "url": "https://ai.pydantic.dev/docs/durable_execution/overview/#durable-execution", "page": "docs/durable_execution/overview", "source_site": "pydantic_ai"}
{"title": "Example: Simple Validation", "anchor": "example-simple-validation", "heading_level": 1, "md_text": "A proof of concept example of evaluating a simple text transformation function with deterministic checks.", "url": "https://ai.pydantic.dev/docs/evals/examples/simple-validation/#example-simple-validation", "page": "docs/evals/examples/simple-validation", "source_site": "pydantic_ai"}
{"title": "Scenario", "anchor": "scenario", "heading_level": 2, "md_text": "We're testing a function that converts text to title case. We want to verify:\n\n- Output is always a string\n- Output matches expected format\n- Function handles edge cases correctly\n- Performance meets requirements", "url": "https://ai.pydantic.dev/docs/evals/examples/simple-validation/#scenario", "page": "docs/evals/examples/simple-validation", "source_site": "pydantic_ai"}
{"title": "Complete Example", "anchor": "complete-example", "heading_level": 2, "md_text": "```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import (\n    Contains,\n    EqualsExpected,\n    IsInstance,\n    MaxDuration,\n)", "url": "https://ai.pydantic.dev/docs/evals/examples/simple-validation/#complete-example", "page": "docs/evals/examples/simple-validation", "source_site": "pydantic_ai"}
{"title": "The function we're testing", "anchor": "the-function-were-testing", "heading_level": 1, "md_text": "def to_title_case(text: str) -> str:\n    \"\"\"Convert text to title case.\"\"\"\n    return text.title()", "url": "https://ai.pydantic.dev/docs/evals/examples/simple-validation/#the-function-were-testing", "page": "docs/evals/examples/simple-validation", "source_site": "pydantic_ai"}
{"title": "Create evaluation dataset", "anchor": "create-evaluation-dataset", "heading_level": 1, "md_text": "dataset = Dataset(\n    name='title_case_validation',\n    cases=[\n        # Basic functionality\n        Case(\n            name='basic_lowercase',\n            inputs='hello world',\n            expected_output='Hello World',\n        ),\n        Case(\n            name='basic_uppercase',\n            inputs='HELLO WORLD',\n            expected_output='Hello World',\n        ),\n        Case(\n            name='mixed_case',\n            inputs='HeLLo WoRLd',\n            expected_output='Hello World',\n        ),\n\n        # Edge cases\n        Case(\n            name='empty_string',\n            inputs='',\n            expected_output='',\n        ),\n        Case(\n            name='single_word',\n            inputs='hello',\n            expected_output='Hello',\n        ),\n        Case(\n            name='with_punctuation',\n            inputs='hello, world!',\n            expected_output='Hello, World!',\n        ),\n        Case(\n            name='with_numbers',\n            inputs='hello 123 world',\n            expected_output='Hello 123 World',\n        ),\n        Case(\n            name='apostrophes',\n            inputs=\"don't stop believin'\",\n            expected_output=\"Don'T Stop Believin'\",\n        ),\n    ],\n    evaluators=[\n        # Always returns a string\n        IsInstance(type_name='str'),\n\n        # Matches expected output\n        EqualsExpected(),\n\n        # Output should contain capital letters\n        Contains(value='H', evaluation_name='has_capitals'),\n\n        # Should be fast (under 1ms)\n        MaxDuration(seconds=0.001),\n    ],\n)", "url": "https://ai.pydantic.dev/docs/evals/examples/simple-validation/#create-evaluation-dataset", "page": "docs/evals/examples/simple-validation", "source_site": "pydantic_ai"}
{"title": "Run evaluation", "anchor": "run-evaluation", "heading_level": 1, "md_text": "if __name__ == '__main__':\n    report = dataset.evaluate_sync(to_title_case)\n\n    # Print results\n    report.print(include_input=True, include_output=True)\n\"\"\"\n                            Evaluation Summary: to_title_case\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Case ID          \u2503 Inputs               \u2503 Outputs              \u2503 Assertions \u2503 Duration \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 basic_lowercase  \u2502 hello world          \u2502 Hello World          \u2502 \u2714\u2714\u2714\u2717       \u2502     10ms \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 basic_uppercase  \u2502 HELLO WORLD          \u2502 Hello World          \u2502 \u2714\u2714\u2714\u2717       \u2502     10ms \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 mixed_case       \u2502 HeLLo WoRLd          \u2502 Hello World          \u2502 \u2714\u2714\u2714\u2717       \u2502     10ms \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 empty_string     \u2502 -                    \u2502 -                    \u2502 \u2714\u2714\u2717\u2717       \u2502     10ms \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 single_word      \u2502 hello                \u2502 Hello                \u2502 \u2714\u2714\u2714\u2717       \u2502     10ms \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 with_punctuation \u2502 hello, world!        \u2502 Hello, World!        \u2502 \u2714\u2714\u2714\u2717       \u2502     10ms \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 with_numbers     \u2502 hello 123 world      \u2502 Hello 123 World      \u2502 \u2714\u2714\u2714\u2717       \u2502     10ms \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 apostrophes      \u2502 don't stop believin' \u2502 Don'T Stop Believin' \u2502 \u2714\u2714\u2717\u2717       \u2502     10ms \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Averages         \u2502                      \u2502                      \u2502 68.8% \u2714    \u2502     10ms \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\"\"\"", "url": "https://ai.pydantic.dev/docs/evals/examples/simple-validation/#run-evaluation", "page": "docs/evals/examples/simple-validation", "source_site": "pydantic_ai"}
{"title": "Check if all passed", "anchor": "check-if-all-passed", "heading_level": 1, "md_text": "avg = report.averages()\nif avg and avg.assertions == 1.0:\n    print('\\n\u2705 All tests passed!')\nelse:\n    print(f'\\n\u274c Some tests failed (pass rate: {avg.assertions:.1%})')\n    \"\"\"\n    \u274c Some tests failed (pass rate: 68.8%)\n    \"\"\"\n```", "url": "https://ai.pydantic.dev/docs/evals/examples/simple-validation/#check-if-all-passed", "page": "docs/evals/examples/simple-validation", "source_site": "pydantic_ai"}
{"title": "Expected Output", "anchor": "expected-output", "heading_level": 2, "md_text": "```\n                        Evaluation Summary: to_title_case\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Case ID           \u2503 Inputs               \u2503 Outputs               \u2503 Assertions \u2503 Duration \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 basic_lowercase   \u2502 hello world          \u2502 Hello World           \u2502 \u2714\u2714\u2714\u2714       \u2502      <1ms\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 basic_uppercase   \u2502 HELLO WORLD          \u2502 Hello World           \u2502 \u2714\u2714\u2714\u2714       \u2502      <1ms\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 mixed_case        \u2502 HeLLo WoRLd          \u2502 Hello World           \u2502 \u2714\u2714\u2714\u2714       \u2502      <1ms\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 empty_string      \u2502                      \u2502                       \u2502 \u2714\u2714\u2717\u2714       \u2502      <1ms\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 single_word       \u2502 hello                \u2502 Hello                 \u2502 \u2714\u2714\u2714\u2714       \u2502      <1ms\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 with_punctuation  \u2502 hello, world!        \u2502 Hello, World!         \u2502 \u2714\u2714\u2714\u2714       \u2502      <1ms\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 with_numbers      \u2502 hello 123 world      \u2502 Hello 123 World       \u2502 \u2714\u2714\u2714\u2714       \u2502      <1ms\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 apostrophes       \u2502 don't stop believin' \u2502 Don'T Stop Believin'  \u2502 \u2714\u2714\u2714\u2714       \u2502      <1ms\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Averages          \u2502                      \u2502                       \u2502 96.9% \u2714    \u2502      <1ms\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\n\u2705 All tests passed!\n```\n\nNote: The `empty_string` case has one failed assertion (`has_capitals`) because an empty string contains no capital letters.", "url": "https://ai.pydantic.dev/docs/evals/examples/simple-validation/#expected-output", "page": "docs/evals/examples/simple-validation", "source_site": "pydantic_ai"}
{"title": "Saving and Loading", "anchor": "saving-and-loading", "heading_level": 2, "md_text": "Save the dataset for future use:\n\n```python {test=\"skip\"}\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import EqualsExpected", "url": "https://ai.pydantic.dev/docs/evals/examples/simple-validation/#saving-and-loading", "page": "docs/evals/examples/simple-validation", "source_site": "pydantic_ai"}
{"title": "The function we're testing", "anchor": "the-function-were-testing", "heading_level": 1, "md_text": "def to_title_case(text: str) -> str:\n    \"\"\"Convert text to title case.\"\"\"\n    return text.title()", "url": "https://ai.pydantic.dev/docs/evals/examples/simple-validation/#the-function-were-testing", "page": "docs/evals/examples/simple-validation", "source_site": "pydantic_ai"}
{"title": "Create dataset", "anchor": "create-dataset", "heading_level": 1, "md_text": "dataset: Dataset[str, str, Any] = Dataset(\n    cases=[Case(inputs='test', expected_output='Test')],\n    evaluators=[EqualsExpected()],\n)", "url": "https://ai.pydantic.dev/docs/evals/examples/simple-validation/#create-dataset", "page": "docs/evals/examples/simple-validation", "source_site": "pydantic_ai"}
{"title": "Save to YAML", "anchor": "save-to-yaml", "heading_level": 1, "md_text": "dataset.to_file('title_case_tests.yaml')", "url": "https://ai.pydantic.dev/docs/evals/examples/simple-validation/#save-to-yaml", "page": "docs/evals/examples/simple-validation", "source_site": "pydantic_ai"}
{"title": "Load later", "anchor": "load-later", "heading_level": 1, "md_text": "dataset = Dataset.from_file('title_case_tests.yaml')\nreport = dataset.evaluate_sync(to_title_case)\n```", "url": "https://ai.pydantic.dev/docs/evals/examples/simple-validation/#load-later", "page": "docs/evals/examples/simple-validation", "source_site": "pydantic_ai"}
{"title": "Adding More Cases", "anchor": "adding-more-cases", "heading_level": 2, "md_text": "As you find bugs or edge cases, add them to the dataset:\n\n```python {test=\"skip\"}\nfrom pydantic_evals import Dataset", "url": "https://ai.pydantic.dev/docs/evals/examples/simple-validation/#adding-more-cases", "page": "docs/evals/examples/simple-validation", "source_site": "pydantic_ai"}
{"title": "Load existing dataset", "anchor": "load-existing-dataset", "heading_level": 1, "md_text": "dataset = Dataset.from_file('title_case_tests.yaml')", "url": "https://ai.pydantic.dev/docs/evals/examples/simple-validation/#load-existing-dataset", "page": "docs/evals/examples/simple-validation", "source_site": "pydantic_ai"}
{"title": "Found a bug with unicode", "anchor": "found-a-bug-with-unicode", "heading_level": 1, "md_text": "dataset.add_case(\n    name='unicode_chars',\n    inputs='caf\u00e9 r\u00e9sum\u00e9',\n    expected_output='Caf\u00e9 R\u00e9sum\u00e9',\n)", "url": "https://ai.pydantic.dev/docs/evals/examples/simple-validation/#found-a-bug-with-unicode", "page": "docs/evals/examples/simple-validation", "source_site": "pydantic_ai"}
{"title": "Found a bug with all caps words", "anchor": "found-a-bug-with-all-caps-words", "heading_level": 1, "md_text": "dataset.add_case(\n    name='acronyms',\n    inputs='the USA and FBI',\n    expected_output='The Usa And Fbi',  # Python's title() behavior\n)", "url": "https://ai.pydantic.dev/docs/evals/examples/simple-validation/#found-a-bug-with-all-caps-words", "page": "docs/evals/examples/simple-validation", "source_site": "pydantic_ai"}
{"title": "Test with very long input", "anchor": "test-with-very-long-input", "heading_level": 1, "md_text": "dataset.add_case(\n    name='long_input',\n    inputs=' '.join(['word'] * 1000),\n    expected_output=' '.join(['Word'] * 1000),\n)", "url": "https://ai.pydantic.dev/docs/evals/examples/simple-validation/#test-with-very-long-input", "page": "docs/evals/examples/simple-validation", "source_site": "pydantic_ai"}
{"title": "Save updated dataset", "anchor": "save-updated-dataset", "heading_level": 1, "md_text": "dataset.to_file('title_case_tests.yaml')\n```", "url": "https://ai.pydantic.dev/docs/evals/examples/simple-validation/#save-updated-dataset", "page": "docs/evals/examples/simple-validation", "source_site": "pydantic_ai"}
{"title": "Using with pytest", "anchor": "using-with-pytest", "heading_level": 2, "md_text": "Integrate with pytest for CI/CD:\n\n```python\nimport pytest\n\nfrom pydantic_evals import Dataset", "url": "https://ai.pydantic.dev/docs/evals/examples/simple-validation/#using-with-pytest", "page": "docs/evals/examples/simple-validation", "source_site": "pydantic_ai"}
{"title": "The function we're testing", "anchor": "the-function-were-testing", "heading_level": 1, "md_text": "def to_title_case(text: str) -> str:\n    \"\"\"Convert text to title case.\"\"\"\n    return text.title()\n\n\n@pytest.fixture\ndef title_case_dataset():\n    return Dataset.from_file('title_case_tests.yaml')\n\n\ndef test_title_case_evaluation(title_case_dataset):\n    \"\"\"Run evaluation tests.\"\"\"\n    report = title_case_dataset.evaluate_sync(to_title_case)\n\n    # All cases should pass\n    avg = report.averages()\n    assert avg is not None\n    assert avg.assertions == 1.0, f'Some tests failed (pass rate: {avg.assertions:.1%})'\n\n\ndef test_title_case_performance(title_case_dataset):\n    \"\"\"Verify performance.\"\"\"\n    report = title_case_dataset.evaluate_sync(to_title_case)\n\n    # All cases should complete quickly\n    for case in report.cases:\n        assert case.task_duration < 0.001, f'{case.name} took {case.task_duration}s'\n```", "url": "https://ai.pydantic.dev/docs/evals/examples/simple-validation/#the-function-were-testing", "page": "docs/evals/examples/simple-validation", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "- **[Built-in Evaluators](../evaluators/built-in.md)** - Explore all available evaluators\n- **[Custom Evaluators](../evaluators/custom.md)** - Write your own evaluation logic\n- **[Dataset Management](../how-to/dataset-management.md)** - Save, load, and manage datasets\n- **[Concurrency & Performance](../how-to/concurrency.md)** - Optimize evaluation performance", "url": "https://ai.pydantic.dev/docs/evals/examples/simple-validation/#next-steps", "page": "docs/evals/examples/simple-validation", "source_site": "pydantic_ai"}
{"title": "Metrics & Attributes", "anchor": "metrics-attributes", "heading_level": 1, "md_text": "Track custom metrics and attributes during task execution for richer evaluation insights.", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#metrics-attributes", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "While executing evaluation tasks, you can record:\n\n- **Metrics** - Numeric values (int/float) for quantitative measurements\n- **Attributes** - Any data for qualitative information\n\nThese appear in evaluation reports and can be used by evaluators for assessment.", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#overview", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Recording Metrics", "anchor": "recording-metrics", "heading_level": 2, "md_text": "Use [`increment_eval_metric`][pydantic_evals.increment_eval_metric] to track numeric values:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.dataset import increment_eval_metric\n\n\n@dataclass\nclass APIResult:\n    output: str\n    usage: 'Usage'\n\n\n@dataclass\nclass Usage:\n    total_tokens: int\n\n\ndef call_api(inputs: str) -> APIResult:\n    return APIResult(output=f'Result: {inputs}', usage=Usage(total_tokens=100))\n\n\ndef my_task(inputs: str) -> str:\n    # Track API calls\n    increment_eval_metric('api_calls', 1)\n\n    result = call_api(inputs)\n\n    # Track tokens used\n    increment_eval_metric('tokens_used', result.usage.total_tokens)\n\n    return result.output\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#recording-metrics", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Recording Attributes", "anchor": "recording-attributes", "heading_level": 2, "md_text": "Use [`set_eval_attribute`][pydantic_evals.set_eval_attribute] to store any data:\n\n```python\nfrom pydantic_evals import set_eval_attribute\n\n\ndef process(inputs: str) -> str:\n    return f'Processed: {inputs}'\n\n\ndef my_task(inputs: str) -> str:\n    # Record which model was used\n    set_eval_attribute('model', 'gpt-5')\n\n    # Record feature flags\n    set_eval_attribute('used_cache', True)\n    set_eval_attribute('retry_count', 2)\n\n    # Record structured data\n    set_eval_attribute('config', {\n        'temperature': 0.7,\n        'max_tokens': 100,\n    })\n\n    return process(inputs)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#recording-attributes", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Accessing in Evaluators", "anchor": "accessing-in-evaluators", "heading_level": 2, "md_text": "Metrics and attributes are available in the [`EvaluatorContext`][pydantic_evals.evaluators.EvaluatorContext]:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass EfficiencyChecker(Evaluator):\n    max_api_calls: int = 5\n\n    def evaluate(self, ctx: EvaluatorContext) -> dict[str, bool]:\n        # Access metrics\n        api_calls = ctx.metrics.get('api_calls', 0)\n        tokens_used = ctx.metrics.get('tokens_used', 0)\n\n        # Access attributes\n        used_cache = ctx.attributes.get('used_cache', False)\n\n        return {\n            'efficient_api_usage': api_calls <= self.max_api_calls,\n            'used_caching': used_cache,\n            'token_efficient': tokens_used < 1000,\n        }\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#accessing-in-evaluators", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Viewing in Reports", "anchor": "viewing-in-reports", "heading_level": 2, "md_text": "Metrics and attributes appear in report data:\n\n```python\nfrom pydantic_evals import Case, Dataset\n\n\ndef task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test')], evaluators=[])\nreport = dataset.evaluate_sync(task)\n\nfor case in report.cases:\n    print(f'{case.name}:')\n    #> Case 1:\n    print(f'  Metrics: {case.metrics}')\n    #>   Metrics: {}\n    print(f'  Attributes: {case.attributes}')\n    #>   Attributes: {}\n```\n\nYou can also display them in printed reports:\n\n```python\nfrom pydantic_evals import Case, Dataset\n\n\ndef task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test')], evaluators=[])\nreport = dataset.evaluate_sync(task)", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#viewing-in-reports", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Access them programmatically or via Logfire", "anchor": "access-them-programmatically-or-via-logfire", "heading_level": 1, "md_text": "for case in report.cases:\n    print(f'\\nCase: {case.name}')\n    \"\"\"\n    Case: Case 1\n    \"\"\"\n    print(f'Metrics: {case.metrics}')\n    #> Metrics: {}\n    print(f'Attributes: {case.attributes}')\n    #> Attributes: {}\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#access-them-programmatically-or-via-logfire", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Automatic Metrics", "anchor": "automatic-metrics", "heading_level": 2, "md_text": "When using Pydantic AI and Logfire, some metrics are automatically tracked:\n\n```python\nimport logfire\n\nfrom pydantic_ai import Agent\n\nlogfire.configure(send_to_logfire='if-token-present')\n\nagent = Agent('openai:gpt-5')\n\n\nasync def ai_task(inputs: str) -> str:\n    result = await agent.run(inputs)\n    return result.output", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#automatic-metrics", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "- cost: Estimated cost (if using genai-prices)", "anchor": "-cost-estimated-cost-if-using-genai-prices", "heading_level": 1, "md_text": "```\n\nAccess these in evaluators:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass CostChecker(Evaluator):\n    max_cost: float = 0.01  # $0.01\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        cost = ctx.metrics.get('cost', 0.0)\n        return cost <= self.max_cost\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#-cost-estimated-cost-if-using-genai-prices", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "API Usage Tracking", "anchor": "api-usage-tracking", "heading_level": 3, "md_text": "```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals import increment_eval_metric, set_eval_attribute\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\ndef check_cache(inputs: str) -> str | None:\n    return None  # No cache hit for demo\n\n\n@dataclass\nclass APIResult:\n    text: str\n    usage: 'Usage'\n\n\n@dataclass\nclass Usage:\n    total_tokens: int\n\n\nasync def call_api(inputs: str) -> APIResult:\n    return APIResult(text=f'Result: {inputs}', usage=Usage(total_tokens=100))\n\n\ndef save_to_cache(inputs: str, result: str) -> None:\n    pass  # Save to cache\n\n\nasync def smart_task(inputs: str) -> str:\n    # Try cache first\n    if cached := check_cache(inputs):\n        set_eval_attribute('cache_hit', True)\n        return cached\n\n    set_eval_attribute('cache_hit', False)\n\n    # Call API\n    increment_eval_metric('api_calls', 1)\n    result = await call_api(inputs)\n\n    increment_eval_metric('tokens', result.usage.total_tokens)\n\n    # Cache result\n    save_to_cache(inputs, result.text)\n\n    return result.text", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#api-usage-tracking", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Evaluate efficiency", "anchor": "evaluate-efficiency", "heading_level": 1, "md_text": "@dataclass\nclass EfficiencyEvaluator(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> dict[str, bool | float]:\n        api_calls = ctx.metrics.get('api_calls', 0)\n        cache_hit = ctx.attributes.get('cache_hit', False)\n\n        return {\n            'used_cache': cache_hit,\n            'made_api_call': api_calls > 0,\n            'efficiency_score': 1.0 if cache_hit else 0.5,\n        }\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#evaluate-efficiency", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Tool Usage Tracking", "anchor": "tool-usage-tracking", "heading_level": 3, "md_text": "```python\nfrom dataclasses import dataclass\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_evals import increment_eval_metric, set_eval_attribute\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\nagent = Agent('openai:gpt-5')\n\n\ndef search(query: str) -> str:\n    return f'Search results for: {query}'\n\n\ndef call(endpoint: str) -> str:\n    return f'API response from: {endpoint}'\n\n\n@agent.tool\ndef search_database(ctx: RunContext, query: str) -> str:\n    increment_eval_metric('db_searches', 1)\n    set_eval_attribute('last_query', query)\n    return search(query)\n\n\n@agent.tool\ndef call_api(ctx: RunContext, endpoint: str) -> str:\n    increment_eval_metric('api_calls', 1)\n    set_eval_attribute('last_endpoint', endpoint)\n    return call(endpoint)", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#tool-usage-tracking", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Evaluate tool usage", "anchor": "evaluate-tool-usage", "heading_level": 1, "md_text": "@dataclass\nclass ToolUsageEvaluator(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> dict[str, bool | int]:\n        db_searches = ctx.metrics.get('db_searches', 0)\n        api_calls = ctx.metrics.get('api_calls', 0)\n\n        return {\n            'used_database': db_searches > 0,\n            'used_api': api_calls > 0,\n            'tool_call_count': db_searches + api_calls,\n            'reasonable_tool_usage': (db_searches + api_calls) <= 5,\n        }\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#evaluate-tool-usage", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Performance Tracking", "anchor": "performance-tracking", "heading_level": 3, "md_text": "```python\nimport time\nfrom dataclasses import dataclass\n\nfrom pydantic_evals import increment_eval_metric, set_eval_attribute\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\nasync def retrieve_context(inputs: str) -> list[str]:\n    return ['context1', 'context2']\n\n\nasync def generate_response(context: list[str], inputs: str) -> str:\n    return f'Generated response for {inputs}'\n\n\nasync def monitored_task(inputs: str) -> str:\n    # Track sub-operation timing\n    t0 = time.perf_counter()\n    context = await retrieve_context(inputs)\n    retrieve_time = time.perf_counter() - t0\n\n    increment_eval_metric('retrieve_time', retrieve_time)\n\n    t0 = time.perf_counter()\n    result = await generate_response(context, inputs)\n    generate_time = time.perf_counter() - t0\n\n    increment_eval_metric('generate_time', generate_time)\n\n    # Record which operations were needed\n    set_eval_attribute('needed_retrieval', len(context) > 0)\n    set_eval_attribute('context_chunks', len(context))\n\n    return result", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#performance-tracking", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Evaluate performance", "anchor": "evaluate-performance", "heading_level": 1, "md_text": "@dataclass\nclass PerformanceEvaluator(Evaluator):\n    max_retrieve_time: float = 0.5\n    max_generate_time: float = 2.0\n\n    def evaluate(self, ctx: EvaluatorContext) -> dict[str, bool]:\n        retrieve_time = ctx.metrics.get('retrieve_time', 0.0)\n        generate_time = ctx.metrics.get('generate_time', 0.0)\n\n        return {\n            'fast_retrieval': retrieve_time <= self.max_retrieve_time,\n            'fast_generation': generate_time <= self.max_generate_time,\n        }\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#evaluate-performance", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Quality Tracking", "anchor": "quality-tracking", "heading_level": 3, "md_text": "```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals import set_eval_attribute\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\nasync def llm_call(inputs: str) -> dict:\n    return {'text': f'Response: {inputs}', 'confidence': 0.85, 'sources': ['doc1', 'doc2']}\n\n\nasync def quality_task(inputs: str) -> str:\n    result = await llm_call(inputs)\n\n    # Extract quality indicators\n    confidence = result.get('confidence', 0.0)\n    sources_used = result.get('sources', [])\n\n    set_eval_attribute('confidence', confidence)\n    set_eval_attribute('source_count', len(sources_used))\n    set_eval_attribute('sources', sources_used)\n\n    return result['text']", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#quality-tracking", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Evaluate based on quality signals", "anchor": "evaluate-based-on-quality-signals", "heading_level": 1, "md_text": "@dataclass\nclass QualityEvaluator(Evaluator):\n    min_confidence: float = 0.7\n\n    def evaluate(self, ctx: EvaluatorContext) -> dict[str, bool | float]:\n        confidence = ctx.attributes.get('confidence', 0.0)\n        source_count = ctx.attributes.get('source_count', 0)\n\n        return {\n            'high_confidence': confidence >= self.min_confidence,\n            'used_sources': source_count > 0,\n            'quality_score': confidence * (1.0 + 0.1 * source_count),\n        }\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#evaluate-based-on-quality-signals", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Experiment-Level Metadata", "anchor": "experiment-level-metadata", "heading_level": 2, "md_text": "In addition to case-level metadata, you can also pass experiment-level metadata when calling [`evaluate()`][pydantic_evals.Dataset.evaluate]:\n\n```python\nfrom pydantic_evals import Case, Dataset\n\ndataset = Dataset(\n    cases=[\n        Case(\n            inputs='test',\n            metadata={'difficulty': 'easy'},  # Case-level metadata\n        )\n    ]\n)\n\n\nasync def task(inputs: str) -> str:\n    return f'Result: {inputs}'", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#experiment-level-metadata", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Pass experiment-level metadata", "anchor": "pass-experiment-level-metadata", "heading_level": 1, "md_text": "async def main():\n    report = await dataset.evaluate(\n        task,\n        metadata={\n            'model': 'gpt-4o',\n            'prompt_version': 'v2.1',\n            'temperature': 0.7,\n        },\n    )\n\n    # Access experiment metadata in the report\n    print(report.experiment_metadata)\n    #> {'model': 'gpt-4o', 'prompt_version': 'v2.1', 'temperature': 0.7}\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#pass-experiment-level-metadata", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "When to Use Experiment Metadata", "anchor": "when-to-use-experiment-metadata", "heading_level": 3, "md_text": "Experiment metadata is useful for tracking configuration that applies to the entire evaluation run:\n\n- **Model configuration**: Model name, version, parameters\n- **Prompt versioning**: Which prompt template was used\n- **Infrastructure**: Deployment environment, region\n- **Experiment context**: Developer name, feature branch, commit hash\n\nThis metadata is especially valuable when:\n\n- Comparing multiple evaluation runs over time\n- Tracking which configuration produced which results\n- Reproducing evaluation results from historical data", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#when-to-use-experiment-metadata", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Viewing in Reports", "anchor": "viewing-in-reports", "heading_level": 3, "md_text": "Experiment metadata appears at the top of printed reports:\n\n```python\nfrom pydantic_evals import Case, Dataset\n\ndataset = Dataset(cases=[Case(inputs='hello', expected_output='HELLO')])\n\n\nasync def task(text: str) -> str:\n    return text.upper()\n\nasync def main():\n    report = await dataset.evaluate(\n        task,\n        metadata={'model': 'gpt-4o', 'version': 'v1.0'},\n    )\n\n    print(report.render())\n    \"\"\"\n    \u256d\u2500 Evaluation Summary: task \u2500\u256e\n    \u2502 model: gpt-4o              \u2502\n    \u2502 version: v1.0              \u2502\n    \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f\n    \u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n    \u2503 Case ID  \u2503 Duration \u2503\n    \u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n    \u2502 Case 1   \u2502     10ms \u2502\n    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n    \u2502 Averages \u2502     10ms \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n    \"\"\"\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#viewing-in-reports", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Synchronization between Tasks and Experiment Metadata", "anchor": "synchronization-between-tasks-and-experiment-metadata", "heading_level": 2, "md_text": "Experiment metadata is for *recording* configuration, not *configuring* the task.\nThe metadata dict doesn't automatically configure your task's behavior; you must ensure the values in the metadata dict match what your task actually uses.\nFor example, it's easy to accidentally have metadata claim `temperature: 0.7` while your task actually uses `temperature: 1.0`, leading to incorrect experiment tracking and unreproducible results.\n\nTo avoid this problem, we recommend establishing a single source of truth for configuration that both your task and metadata reference.\nBelow are a few suggested patterns for achieving this synchronization.", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#synchronization-between-tasks-and-experiment-metadata", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Pattern 1: Shared Module Constants", "anchor": "pattern-1-shared-module-constants", "heading_level": 3, "md_text": "For simpler cases, use module-level constants:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_evals import Case, Dataset", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#pattern-1-shared-module-constants", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Module constants as single source of truth", "anchor": "module-constants-as-single-source-of-truth", "heading_level": 1, "md_text": "MODEL_NAME = 'openai:gpt-5-mini'\nTEMPERATURE = 0.7\nSYSTEM_PROMPT = 'You are a helpful assistant.'\n\nagent = Agent(MODEL_NAME, model_settings={'temperature': TEMPERATURE}, system_prompt=SYSTEM_PROMPT)\n\n\nasync def task(inputs: str) -> str:\n    result = await agent.run(inputs)\n    return result.output\n\n\nasync def main():\n    dataset = Dataset(cases=[Case(inputs='What is the capital of France?')])\n\n    # Metadata references same constants\n    await dataset.evaluate(\n        task,\n        metadata={\n            'model': MODEL_NAME,\n            'temperature': TEMPERATURE,\n            'system_prompt': SYSTEM_PROMPT,\n        },\n    )\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#module-constants-as-single-source-of-truth", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Pattern 2: Configuration Object (Recommended)", "anchor": "pattern-2-configuration-object-recommended", "heading_level": 3, "md_text": "Define configuration once and use it everywhere:\n\n```python\nfrom dataclasses import asdict, dataclass\n\nfrom pydantic_ai import Agent\nfrom pydantic_evals import Case, Dataset\n\n\n@dataclass\nclass TaskConfig:\n    \"\"\"Single source of truth for task configuration.\n\n    Includes all variables you'd like to see in experiment metadata.\n    \"\"\"\n\n    model: str\n    temperature: float\n    max_tokens: int\n    prompt_version: str", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#pattern-2-configuration-object-recommended", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Define configuration once", "anchor": "define-configuration-once", "heading_level": 1, "md_text": "config = TaskConfig(\n    model='openai:gpt-5-mini',\n    temperature=0.7,\n    max_tokens=500,\n    prompt_version='v2.1',\n)", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#define-configuration-once", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Use config in task", "anchor": "use-config-in-task", "heading_level": 1, "md_text": "agent = Agent(\n    config.model,\n    model_settings={'temperature': config.temperature, 'max_tokens': config.max_tokens},\n)\n\n\nasync def task(inputs: str) -> str:\n    \"\"\"Task uses the same config that's recorded in metadata.\"\"\"\n    result = await agent.run(inputs)\n    return result.output", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#use-config-in-task", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Evaluate with metadata derived from the same config", "anchor": "evaluate-with-metadata-derived-from-the-same-config", "heading_level": 1, "md_text": "async def main():\n    dataset = Dataset(cases=[Case(inputs='What is the capital of France?')])\n\n    report = await dataset.evaluate(\n        task,\n        metadata=asdict(config),  # Guaranteed to match task behavior\n    )\n\n    print(report.experiment_metadata)\n    \"\"\"\n    {\n        'model': 'openai:gpt-5-mini',\n        'temperature': 0.7,\n        'max_tokens': 500,\n        'prompt_version': 'v2.1',\n    }\n    \"\"\"\n```\n\nIf it's problematic to have a global task configuration, you can also create your `TaskConfig` object at the task\ncall-site and pass it to the agent via `deps` or similar, but in this case you would still need to guarantee that the\nvalue is always the same as the value passed to `metadata` in the call to `Dataset.evaluate`.", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#evaluate-with-metadata-derived-from-the-same-config", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Anti-Pattern: Duplicate Configuration", "anchor": "anti-pattern-duplicate-configuration", "heading_level": 3, "md_text": "**Avoid this common mistake**:\n\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_evals import Case, Dataset", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#anti-pattern-duplicate-configuration", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "\u274c BAD: Configuration defined in multiple places", "anchor": "-bad-configuration-defined-in-multiple-places", "heading_level": 1, "md_text": "agent = Agent('openai:gpt-5-mini', model_settings={'temperature': 0.7})\n\n\nasync def task(inputs: str) -> str:\n    result = await agent.run(inputs)\n    return result.output\n\n\nasync def main():\n    dataset = Dataset(cases=[Case(inputs='test')])\n\n    # \u274c BAD: Metadata manually typed - easy to get out of sync\n    await dataset.evaluate(\n        task,\n        metadata={\n            'model': 'openai:gpt-5-mini',  # Duplicated! Could diverge from agent definition\n            'temperature': 0.8,  # \u26a0\ufe0f WRONG! Task actually uses 0.7\n        },\n    )\n```\n\nIn this anti-pattern, the metadata claims `temperature: 0.8` but the task uses `0.7`. This leads to:\n\n- Incorrect experiment tracking\n- Inability to reproduce results\n- Confusion when comparing runs\n- Wasted time debugging \"why results differ\"", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#-bad-configuration-defined-in-multiple-places", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Metrics vs Attributes vs Metadata", "anchor": "metrics-vs-attributes-vs-metadata", "heading_level": 2, "md_text": "Understanding the differences:\n\n| Feature | Metrics | Attributes | Case Metadata | Experiment Metadata |\n|---------|---------|------------|---------------|---------------------|\n| **Set in** | Task execution | Task execution | Case definition | `evaluate()` call |\n| **Type** | int, float | Any | Any | Any |\n| **Purpose** | Quantitative | Qualitative | Test data | Experiment config |\n| **Used for** | Aggregation | Context | Input to task | Tracking runs |\n| **Available to** | Evaluators | Evaluators | Task & Evaluators | Report only |\n| **Scope** | Per case | Per case | Per case | Per experiment |\n\n```python\nfrom pydantic_evals import Case, Dataset, increment_eval_metric, set_eval_attribute", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#metrics-vs-attributes-vs-metadata", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Case Metadata: Defined in case (before execution)", "anchor": "case-metadata-defined-in-case-before-execution", "heading_level": 1, "md_text": "case = Case(\n    inputs='question',\n    metadata={'difficulty': 'hard', 'category': 'math'},  # Per-case metadata\n)\n\ndataset = Dataset(cases=[case])", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#case-metadata-defined-in-case-before-execution", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Metrics & Attributes: Recorded during execution", "anchor": "metrics-attributes-recorded-during-execution", "heading_level": 1, "md_text": "async def task(inputs):\n    # These are recorded during execution for each case\n    increment_eval_metric('tokens', 100)\n    set_eval_attribute('model', 'gpt-5')\n    return f'Result: {inputs}'\n\n\nasync def main():\n    # Experiment Metadata: Defined at evaluation time\n    await dataset.evaluate(\n        task,\n        metadata={  # Experiment-level metadata\n            'prompt_version': 'v2.1',\n            'temperature': 0.7,\n        },\n    )\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#metrics-attributes-recorded-during-execution", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "\"Metrics/attributes not appearing\"", "anchor": "metricsattributes-not-appearing", "heading_level": 3, "md_text": "Ensure you're calling the functions inside the task:\n\n```python\nfrom pydantic_evals import increment_eval_metric\n\n\ndef process(inputs: str) -> str:\n    return f'Processed: {inputs}'", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#metricsattributes-not-appearing", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Bad: Called outside task", "anchor": "bad-called-outside-task", "heading_level": 1, "md_text": "increment_eval_metric('count', 1)\n\n\ndef bad_task(inputs):\n    return process(inputs)", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#bad-called-outside-task", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Good: Called inside task", "anchor": "good-called-inside-task", "heading_level": 1, "md_text": "def good_task(inputs):\n    increment_eval_metric('count', 1)\n    return process(inputs)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#good-called-inside-task", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "\"Metrics not incrementing\"", "anchor": "metrics-not-incrementing", "heading_level": 3, "md_text": "Check you're using `increment_eval_metric`, not `set_eval_attribute`:\n\n```python\nfrom pydantic_evals import increment_eval_metric, set_eval_attribute", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#metrics-not-incrementing", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Bad: This will overwrite, not increment", "anchor": "bad-this-will-overwrite-not-increment", "heading_level": 1, "md_text": "set_eval_attribute('count', 1)\nset_eval_attribute('count', 1)  # Still 1", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#bad-this-will-overwrite-not-increment", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Good: This increments", "anchor": "good-this-increments", "heading_level": 1, "md_text": "increment_eval_metric('count', 1)\nincrement_eval_metric('count', 1)  # Now 2\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#good-this-increments", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "\"Too much data in attributes\"", "anchor": "too-much-data-in-attributes", "heading_level": 3, "md_text": "Store summaries, not raw data:\n\n```python\nfrom pydantic_evals import set_eval_attribute\n\ngiant_response_object = {'key' + str(i): 'value' * 100 for i in range(1000)}", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#too-much-data-in-attributes", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Bad: Huge object", "anchor": "bad-huge-object", "heading_level": 1, "md_text": "set_eval_attribute('full_response', giant_response_object)", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#bad-huge-object", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Good: Summary", "anchor": "good-summary", "heading_level": 1, "md_text": "set_eval_attribute('response_size_kb', len(str(giant_response_object)) / 1024)\nset_eval_attribute('response_keys', list(giant_response_object.keys())[:10])  # First 10 keys\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#good-summary", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "- **[Custom Evaluators](../evaluators/custom.md)** - Use metrics/attributes in evaluators\n- **[Logfire Integration](logfire-integration.md)** - View metrics in Logfire\n- **[Concurrency & Performance](concurrency.md)** - Optimize evaluation performance", "url": "https://ai.pydantic.dev/docs/evals/how-to/metrics-attributes/#next-steps", "page": "docs/evals/how-to/metrics-attributes", "source_site": "pydantic_ai"}
{"title": "Logfire Integration", "anchor": "logfire-integration", "heading_level": 1, "md_text": "Visualize and analyze evaluation results using Pydantic Logfire.", "url": "https://ai.pydantic.dev/docs/evals/how-to/logfire-integration/#logfire-integration", "page": "docs/evals/how-to/logfire-integration", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "Pydantic Evals uses OpenTelemetry to record traces of the evaluation process. These traces contain all the information from your evaluation reports, plus full tracing from the execution of your task function.\n\nYou can send these traces to any OpenTelemetry-compatible backend, including [Pydantic Logfire](https://logfire.pydantic.dev/docs/guides/web-ui/evals/).", "url": "https://ai.pydantic.dev/docs/evals/how-to/logfire-integration/#overview", "page": "docs/evals/how-to/logfire-integration", "source_site": "pydantic_ai"}
{"title": "Installation", "anchor": "installation", "heading_level": 2, "md_text": "Install the optional logfire dependency:\n\n```bash\npip install 'pydantic-evals[logfire]'\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/logfire-integration/#installation", "page": "docs/evals/how-to/logfire-integration", "source_site": "pydantic_ai"}
{"title": "Basic Setup", "anchor": "basic-setup", "heading_level": 2, "md_text": "Configure Logfire before running evaluations:\n\n```python {title=\"basic_logfire_setup.py\"}\nimport logfire\n\nfrom pydantic_evals import Case, Dataset", "url": "https://ai.pydantic.dev/docs/evals/how-to/logfire-integration/#basic-setup", "page": "docs/evals/how-to/logfire-integration", "source_site": "pydantic_ai"}
{"title": "Configure Logfire", "anchor": "configure-logfire", "heading_level": 1, "md_text": "logfire.configure(\n    send_to_logfire='if-token-present',  # (1)!\n)", "url": "https://ai.pydantic.dev/docs/evals/how-to/logfire-integration/#configure-logfire", "page": "docs/evals/how-to/logfire-integration", "source_site": "pydantic_ai"}
{"title": "Your evaluation code", "anchor": "your-evaluation-code", "heading_level": 1, "md_text": "def my_task(inputs: str) -> str:\n    return f'result for {inputs}'\n\n\ndataset = Dataset(cases=[Case(name='test', inputs='example')])\nreport = dataset.evaluate_sync(my_task)\n```\n\n1. Sends data to Logfire only if the `LOGFIRE_TOKEN` environment variable is set\n\nThat's it! Your evaluation traces will now appear in the Logfire web UI as long as you have the `LOGFIRE_TOKEN` environment variable set.", "url": "https://ai.pydantic.dev/docs/evals/how-to/logfire-integration/#your-evaluation-code", "page": "docs/evals/how-to/logfire-integration", "source_site": "pydantic_ai"}
{"title": "What Gets Sent to Logfire", "anchor": "what-gets-sent-to-logfire", "heading_level": 2, "md_text": "When you run an evaluation, Logfire receives:\n\n1. **Evaluation metadata**\n    1. Dataset name\n    1. Number of cases\n    1. Evaluator names\n2. **Per-case data**\n    1. Inputs and outputs\n    1. Expected outputs\n    1. Metadata\n    1. Execution duration\n3. **Evaluation results**\n    1. Scores, assertions, and labels\n    1. Reasons (if included)\n    1. Evaluator failures\n4. **Task execution traces**\n    1. All OpenTelemetry spans from your task function\n    1. Tool calls (for Pydantic AI agents)\n    1. API calls, database queries, etc.", "url": "https://ai.pydantic.dev/docs/evals/how-to/logfire-integration/#what-gets-sent-to-logfire", "page": "docs/evals/how-to/logfire-integration", "source_site": "pydantic_ai"}
{"title": "Evaluation Overview", "anchor": "evaluation-overview", "heading_level": 3, "md_text": "Logfire provides a special table view for evaluation results on the root evaluation span:\n\n![Logfire Evals Overview](../../img/logfire-evals-overview.png)\n\nThis view shows:\n\n- Case names\n- Pass/fail status\n- Scores and assertions\n- Execution duration\n- Quick filtering and sorting", "url": "https://ai.pydantic.dev/docs/evals/how-to/logfire-integration/#evaluation-overview", "page": "docs/evals/how-to/logfire-integration", "source_site": "pydantic_ai"}
{"title": "Individual Case Details", "anchor": "individual-case-details", "heading_level": 3, "md_text": "Click any case to see detailed inputs and outputs:\n\n![Logfire Evals Case](../../img/logfire-evals-case.png)", "url": "https://ai.pydantic.dev/docs/evals/how-to/logfire-integration/#individual-case-details", "page": "docs/evals/how-to/logfire-integration", "source_site": "pydantic_ai"}
{"title": "Full Trace View", "anchor": "full-trace-view", "heading_level": 3, "md_text": "View the complete execution trace including all spans generated during evaluation:\n\n![Logfire Evals Case Trace](../../img/logfire-evals-case-trace.png)\n\nThis is especially useful for:\n\n- Debugging failed cases\n- Understanding performance bottlenecks\n- Analyzing tool usage patterns\n- Writing span-based evaluators", "url": "https://ai.pydantic.dev/docs/evals/how-to/logfire-integration/#full-trace-view", "page": "docs/evals/how-to/logfire-integration", "source_site": "pydantic_ai"}
{"title": "Comparing Runs", "anchor": "comparing-runs", "heading_level": 3, "md_text": "Run the same evaluation multiple times and compare in Logfire:\n\n```python\nfrom pydantic_evals import Case, Dataset\n\n\ndef original_task(inputs: str) -> str:\n    return f'original result for {inputs}'\n\n\ndef improved_task(inputs: str) -> str:\n    return f'improved result for {inputs}'\n\n\ndataset = Dataset(cases=[Case(name='test', inputs='example')])", "url": "https://ai.pydantic.dev/docs/evals/how-to/logfire-integration/#comparing-runs", "page": "docs/evals/how-to/logfire-integration", "source_site": "pydantic_ai"}
{"title": "Run 1: Original implementation", "anchor": "run-1-original-implementation", "heading_level": 1, "md_text": "report1 = dataset.evaluate_sync(original_task)", "url": "https://ai.pydantic.dev/docs/evals/how-to/logfire-integration/#run-1-original-implementation", "page": "docs/evals/how-to/logfire-integration", "source_site": "pydantic_ai"}
{"title": "Run 2: Improved implementation", "anchor": "run-2-improved-implementation", "heading_level": 1, "md_text": "report2 = dataset.evaluate_sync(improved_task)", "url": "https://ai.pydantic.dev/docs/evals/how-to/logfire-integration/#run-2-improved-implementation", "page": "docs/evals/how-to/logfire-integration", "source_site": "pydantic_ai"}
{"title": "Compare in Logfire by filtering by timestamp or attributes", "anchor": "compare-in-logfire-by-filtering-by-timestamp-or-attributes", "heading_level": 1, "md_text": "```", "url": "https://ai.pydantic.dev/docs/evals/how-to/logfire-integration/#compare-in-logfire-by-filtering-by-timestamp-or-attributes", "page": "docs/evals/how-to/logfire-integration", "source_site": "pydantic_ai"}
{"title": "Debugging Failed Cases", "anchor": "debugging-failed-cases", "heading_level": 3, "md_text": "Find failed cases quickly:\n\n1. Search for `service_name = 'my_service_evals' AND is_exception` (replace with the actual service name you are using)\n2. View the full span tree to see where the failure occurred\n3. Inspect attributes and logs for error messages", "url": "https://ai.pydantic.dev/docs/evals/how-to/logfire-integration/#debugging-failed-cases", "page": "docs/evals/how-to/logfire-integration", "source_site": "pydantic_ai"}
{"title": "Span-Based Evaluation", "anchor": "span-based-evaluation", "heading_level": 2, "md_text": "Logfire integration enables powerful span-based evaluators. See [Span-Based Evaluation](../evaluators/span-based.md) for details.\n\nExample: Verify specific tools were called:\n\n```python\nimport logfire\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\nlogfire.configure(send_to_logfire='if-token-present')\n\n\ndef my_agent(inputs: str) -> str:\n    return f'result for {inputs}'\n\n\ndataset = Dataset(\n    cases=[Case(name='test', inputs='example')],\n    evaluators=[\n        HasMatchingSpan(\n            query={'name_contains': 'search_tool'},\n            evaluation_name='used_search',\n        ),\n    ],\n)\n\nreport = dataset.evaluate_sync(my_agent)\n```\n\nThe span tree is available in both:\n\n- Your evaluator code (via `ctx.span_tree`)\n- Logfire UI (visual trace view)", "url": "https://ai.pydantic.dev/docs/evals/how-to/logfire-integration/#span-based-evaluation", "page": "docs/evals/how-to/logfire-integration", "source_site": "pydantic_ai"}
{"title": "No Data Appearing in Logfire", "anchor": "no-data-appearing-in-logfire", "heading_level": 3, "md_text": "Check:\n\n1. **Token is set**: `echo $LOGFIRE_TOKEN`\n2. **Configuration is correct**:\n   ```python\n   import logfire\n\n   logfire.configure(send_to_logfire='always')  # Force sending\n   ```\n3. **Network connectivity**: Check firewall settings\n4. **Project exists**: Verify project name in Logfire UI", "url": "https://ai.pydantic.dev/docs/evals/how-to/logfire-integration/#no-data-appearing-in-logfire", "page": "docs/evals/how-to/logfire-integration", "source_site": "pydantic_ai"}
{"title": "Traces Missing Spans", "anchor": "traces-missing-spans", "heading_level": 3, "md_text": "If some spans are missing:\n\n1. **Ensure logfire is configured before imports**:\n   ```python\n   import logfire\n\n   logfire.configure()  # Must be first\n   ```\n\n2. **Check instrumentation**: Ensure your code has enabled all instrumentations you want:\n   ```python\n   import logfire\n\n   logfire.instrument_pydantic_ai()\n   logfire.instrument_httpx(capture_all=True)\n   ```", "url": "https://ai.pydantic.dev/docs/evals/how-to/logfire-integration/#traces-missing-spans", "page": "docs/evals/how-to/logfire-integration", "source_site": "pydantic_ai"}
{"title": "1. Configure Early", "anchor": "1-configure-early", "heading_level": 3, "md_text": "Always configure Logfire before running evaluations:\n\n```python\nimport logfire\n\nfrom pydantic_evals import Case, Dataset\n\nlogfire.configure(send_to_logfire='if-token-present')", "url": "https://ai.pydantic.dev/docs/evals/how-to/logfire-integration/#1-configure-early", "page": "docs/evals/how-to/logfire-integration", "source_site": "pydantic_ai"}
{"title": "Now import and run evaluations", "anchor": "now-import-and-run-evaluations", "heading_level": 1, "md_text": "def task(inputs: str) -> str:\n    return f'result for {inputs}'\n\n\ndataset = Dataset(cases=[Case(name='test', inputs='example')])\ndataset.evaluate_sync(task)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/logfire-integration/#now-import-and-run-evaluations", "page": "docs/evals/how-to/logfire-integration", "source_site": "pydantic_ai"}
{"title": "2. Use Descriptive Service Names And Environments", "anchor": "2-use-descriptive-service-names-and-environments", "heading_level": 3, "md_text": "```python\nimport logfire\n\nlogfire.configure(\n    service_name='rag-pipeline-evals',\n    environment='development',\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/logfire-integration/#2-use-descriptive-service-names-and-environments", "page": "docs/evals/how-to/logfire-integration", "source_site": "pydantic_ai"}
{"title": "3. Review Periodically", "anchor": "3-review-periodically", "heading_level": 3, "md_text": "- Check Logfire regularly to identify patterns\n- Look for consistently failing cases\n- Analyze performance trends\n- Adjust evaluators based on insights", "url": "https://ai.pydantic.dev/docs/evals/how-to/logfire-integration/#3-review-periodically", "page": "docs/evals/how-to/logfire-integration", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "- **[Span-Based Evaluation](../evaluators/span-based.md)** - Use OpenTelemetry spans in evaluators\n- **[Logfire Documentation](https://logfire.pydantic.dev/docs/guides/web-ui/evals/)** - Complete Logfire guide\n- **[Metrics & Attributes](metrics-attributes.md)** - Add custom data to traces", "url": "https://ai.pydantic.dev/docs/evals/how-to/logfire-integration/#next-steps", "page": "docs/evals/how-to/logfire-integration", "source_site": "pydantic_ai"}
{"title": "Retry Strategies", "anchor": "retry-strategies", "heading_level": 1, "md_text": "Handle transient failures in tasks and evaluators with automatic retry logic.", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#retry-strategies", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "LLM-based systems can experience transient failures:\n\n- Rate limits\n- Network timeouts\n- Temporary API outages\n- Context length errors\n\nPydantic Evals supports retry configuration for both:\n\n- **Task execution** - The function being evaluated\n- **Evaluator execution** - The evaluators themselves", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#overview", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "Basic Retry Configuration", "anchor": "basic-retry-configuration", "heading_level": 2, "md_text": "Pass a retry configuration to `evaluate()` or `evaluate_sync()` using [Tenacity](https://tenacity.readthedocs.io/) parameters:\n\n```python\nfrom tenacity import stop_after_attempt\n\nfrom pydantic_evals import Case, Dataset\n\n\ndef my_function(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test')], evaluators=[])\n\nreport = dataset.evaluate_sync(\n    task=my_function,\n    retry_task={'stop': stop_after_attempt(3)},\n    retry_evaluators={'stop': stop_after_attempt(2)},\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#basic-retry-configuration", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "Retry Configuration Options", "anchor": "retry-configuration-options", "heading_level": 2, "md_text": "Retry configurations use [Tenacity](https://tenacity.readthedocs.io/) and support the same options as Pydantic AI's [`RetryConfig`][pydantic_ai.retries.RetryConfig]:\n\n```python\nfrom tenacity import stop_after_attempt, wait_exponential\n\nfrom pydantic_evals import Case, Dataset\n\n\ndef my_function(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test')], evaluators=[])\n\nretry_config = {\n    'stop': stop_after_attempt(3),  # Stop after 3 attempts\n    'wait': wait_exponential(multiplier=1, min=1, max=10),  # Exponential backoff: 1s, 2s, 4s, 8s (capped at 10s)\n    'reraise': True,  # Re-raise the original exception after exhausting retries\n}\n\ndataset.evaluate_sync(\n    task=my_function,\n    retry_task=retry_config,\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#retry-configuration-options", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "Common Parameters", "anchor": "common-parameters", "heading_level": 3, "md_text": "The retry configuration accepts any parameters from the tenacity `retry` decorator. Common ones include:\n\n| Parameter | Type | Description |\n|-----------|------|-------------|\n| `stop` | `StopBaseT` | Stop strategy (e.g., `stop_after_attempt(3)`, `stop_after_delay(60)`) |\n| `wait` | `WaitBaseT` | Wait strategy (e.g., `wait_exponential()`, `wait_fixed(2)`) |\n| `retry` | `RetryBaseT` | Retry condition (e.g., `retry_if_exception_type(TimeoutError)`) |\n| `reraise` | `bool` | Whether to reraise the original exception (default: `False`) |\n| `before_sleep` | `Callable` | Callback before sleeping between retries |\n\nSee the [Tenacity documentation](https://tenacity.readthedocs.io/) for all available options.", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#common-parameters", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "Task Retries", "anchor": "task-retries", "heading_level": 2, "md_text": "Retry the task function when it fails:\n\n```python\nfrom tenacity import stop_after_attempt, wait_exponential\n\nfrom pydantic_evals import Case, Dataset\n\n\nasync def call_llm(inputs: str) -> str:\n    return f'LLM response to: {inputs}'\n\n\nasync def flaky_llm_task(inputs: str) -> str:\n    \"\"\"This might hit rate limits or timeout.\"\"\"\n    response = await call_llm(inputs)\n    return response\n\n\ndataset = Dataset(cases=[Case(inputs='test')])\n\nreport = dataset.evaluate_sync(\n    task=flaky_llm_task,\n    retry_task={\n        'stop': stop_after_attempt(5),  # Try up to 5 times\n        'wait': wait_exponential(multiplier=1, min=1, max=30),  # Exponential backoff, capped at 30s\n        'reraise': True,\n    },\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#task-retries", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "When Task Retries Trigger", "anchor": "when-task-retries-trigger", "heading_level": 3, "md_text": "Retries trigger when the task raises an exception:\n\n```python\nclass RateLimitError(Exception):\n    pass\n\n\nclass ValidationError(Exception):\n    pass\n\n\nasync def call_api(inputs: str) -> str:\n    return f'API response: {inputs}'\n\n\nasync def my_task(inputs: str) -> str:\n    try:\n        return await call_api(inputs)\n    except RateLimitError:\n        # Will trigger retry\n        raise\n    except ValidationError:\n        # Will also trigger retry\n        raise\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#when-task-retries-trigger", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "Exponential Backoff", "anchor": "exponential-backoff", "heading_level": 3, "md_text": "When using `wait_exponential()`, delays increase exponentially:\n\n```\nAttempt 1: immediate\nAttempt 2: ~1s delay (multiplier * 2^0)\nAttempt 3: ~2s delay (multiplier * 2^1)\nAttempt 4: ~4s delay (multiplier * 2^2)\nAttempt 5: ~8s delay (multiplier * 2^3, capped at max)\n```\n\nThe actual delay depends on the `multiplier`, `min`, and `max` parameters passed to `wait_exponential()`.", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#exponential-backoff", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "Evaluator Retries", "anchor": "evaluator-retries", "heading_level": 2, "md_text": "Retry evaluators when they fail:\n\n```python\nfrom tenacity import stop_after_attempt, wait_exponential\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\n\ndef my_task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[\n        # LLMJudge might hit rate limits\n        LLMJudge(rubric='Response is accurate'),\n    ],\n)\n\nreport = dataset.evaluate_sync(\n    task=my_task,\n    retry_evaluators={\n        'stop': stop_after_attempt(3),\n        'wait': wait_exponential(multiplier=1, min=0.5, max=10),\n        'reraise': True,\n    },\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#evaluator-retries", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "When Evaluator Retries Trigger", "anchor": "when-evaluator-retries-trigger", "heading_level": 3, "md_text": "Retries trigger when an evaluator raises an exception:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\nasync def external_api_call(output: str) -> bool:\n    return len(output) > 0\n\n\n@dataclass\nclass APIEvaluator(Evaluator):\n    async def evaluate(self, ctx: EvaluatorContext) -> bool:\n        # If this raises an exception, retry logic will trigger\n        result = await external_api_call(ctx.output)\n        return result\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#when-evaluator-retries-trigger", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "Evaluator Failures", "anchor": "evaluator-failures", "heading_level": 3, "md_text": "If an evaluator fails after all retries, it's recorded as an [`EvaluatorFailure`][pydantic_evals.evaluators.EvaluatorFailure]:\n\n```python\nfrom tenacity import stop_after_attempt\n\nfrom pydantic_evals import Case, Dataset\n\n\ndef task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test')], evaluators=[])\n\nreport = dataset.evaluate_sync(task, retry_evaluators={'stop': stop_after_attempt(3)})", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#evaluator-failures", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "Check for evaluator failures", "anchor": "check-for-evaluator-failures", "heading_level": 1, "md_text": "for case in report.cases:\n    if case.evaluator_failures:\n        for failure in case.evaluator_failures:\n            print(f'Evaluator {failure.name} failed: {failure.error_message}')\n    #> (No output - no evaluator failures in this case)\n```\n\nView evaluator failures in reports:\n\n```python\nfrom pydantic_evals import Case, Dataset\n\n\ndef task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test')], evaluators=[])\nreport = dataset.evaluate_sync(task)\n\nreport.print(include_evaluator_failures=True)\n\"\"\"\n  Evaluation Summary:\n         task\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Case ID  \u2503 Duration \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Case 1   \u2502     10ms \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Averages \u2502     10ms \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\"\"\"\n#>\n#> \u2705 case_0                       \u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501 100% (0/0)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#check-for-evaluator-failures", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "Combining Task and Evaluator Retries", "anchor": "combining-task-and-evaluator-retries", "heading_level": 2, "md_text": "You can configure both independently:\n\n```python\nfrom tenacity import stop_after_attempt, wait_exponential\n\nfrom pydantic_evals import Case, Dataset\n\n\ndef flaky_task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test')], evaluators=[])\n\nreport = dataset.evaluate_sync(\n    task=flaky_task,\n    retry_task={\n        'stop': stop_after_attempt(5),  # Retry task up to 5 times\n        'wait': wait_exponential(multiplier=1, min=1, max=30),\n        'reraise': True,\n    },\n    retry_evaluators={\n        'stop': stop_after_attempt(3),  # Retry evaluators up to 3 times\n        'wait': wait_exponential(multiplier=1, min=0.5, max=10),\n        'reraise': True,\n    },\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#combining-task-and-evaluator-retries", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "Rate Limit Handling", "anchor": "rate-limit-handling", "heading_level": 3, "md_text": "```python\nfrom tenacity import stop_after_attempt, wait_exponential\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\n\nasync def expensive_llm_call(inputs: str) -> str:\n    return f'LLM response: {inputs}'\n\n\nasync def llm_task(inputs: str) -> str:\n    \"\"\"Task that might hit rate limits.\"\"\"\n    return await expensive_llm_call(inputs)\n\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[\n        LLMJudge(rubric='Quality check'),  # Also might hit rate limits\n    ],\n)", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#rate-limit-handling", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "Generous retries for rate limits", "anchor": "generous-retries-for-rate-limits", "heading_level": 1, "md_text": "report = dataset.evaluate_sync(\n    task=llm_task,\n    retry_task={\n        'stop': stop_after_attempt(10),  # Rate limits can take multiple retries\n        'wait': wait_exponential(multiplier=2, min=2, max=60),  # Start at 2s, exponential up to 60s\n        'reraise': True,\n    },\n    retry_evaluators={\n        'stop': stop_after_attempt(5),\n        'wait': wait_exponential(multiplier=2, min=2, max=30),\n        'reraise': True,\n    },\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#generous-retries-for-rate-limits", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "Network Timeout Handling", "anchor": "network-timeout-handling", "heading_level": 3, "md_text": "```python\nimport httpx\nfrom tenacity import stop_after_attempt, wait_exponential\n\nfrom pydantic_evals import Case, Dataset\n\n\nasync def api_task(inputs: str) -> str:\n    \"\"\"Task that calls external API which might timeout.\"\"\"\n    async with httpx.AsyncClient(timeout=10.0) as client:\n        response = await client.post('https://api.example.com', json={'input': inputs})\n        return response.text\n\n\ndataset = Dataset(cases=[Case(inputs='test')], evaluators=[])", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#network-timeout-handling", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "Quick retries for network issues", "anchor": "quick-retries-for-network-issues", "heading_level": 1, "md_text": "report = dataset.evaluate_sync(\n    task=api_task,\n    retry_task={\n        'stop': stop_after_attempt(4),  # A few quick retries\n        'wait': wait_exponential(multiplier=0.5, min=0.5, max=5),  # Fast retry, capped at 5s\n        'reraise': True,\n    },\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#quick-retries-for-network-issues", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "Context Length Handling", "anchor": "context-length-handling", "heading_level": 3, "md_text": "```python\nfrom tenacity import stop_after_attempt\n\nfrom pydantic_evals import Case, Dataset\n\n\nclass ContextLengthError(Exception):\n    pass\n\n\nasync def llm_call(inputs: str, max_tokens: int = 8000) -> str:\n    return f'LLM response: {inputs[:100]}'\n\n\nasync def smart_llm_task(inputs: str) -> str:\n    \"\"\"Task that might exceed context length.\"\"\"\n    try:\n        return await llm_call(inputs, max_tokens=8000)\n    except ContextLengthError:\n        # Retry with shorter context\n        truncated_inputs = inputs[:4000]\n        return await llm_call(truncated_inputs, max_tokens=4000)\n\n\ndataset = Dataset(cases=[Case(inputs='test')], evaluators=[])", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#context-length-handling", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "Don't retry context length errors (handle in task)", "anchor": "dont-retry-context-length-errors-handle-in-task", "heading_level": 1, "md_text": "report = dataset.evaluate_sync(\n    task=smart_llm_task,\n    retry_task={'stop': stop_after_attempt(1)},  # No retries, we handle it\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#dont-retry-context-length-errors-handle-in-task", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "Retry vs Error Handling", "anchor": "retry-vs-error-handling", "heading_level": 2, "md_text": "**Use retries for:**\n- Transient failures (rate limits, timeouts)\n- Network issues\n- Temporary service outages\n- Recoverable errors\n\n**Use error handling for:**\n- Validation errors\n- Logic errors\n- Permanent failures\n- Expected error conditions\n\n```python\nclass RateLimitError(Exception):\n    pass\n\n\nasync def llm_call(inputs: str) -> str:\n    return f'LLM response: {inputs}'\n\n\ndef is_valid(result: str) -> bool:\n    return len(result) > 0\n\n\nasync def smart_task(inputs: str) -> str:\n    \"\"\"Handle expected errors, let retries handle transient failures.\"\"\"\n    try:\n        result = await llm_call(inputs)\n\n        # Validate output (don't retry validation errors)\n        if not is_valid(result):\n            return 'ERROR: Invalid output format'\n\n        return result\n\n    except RateLimitError:\n        # Let retry logic handle this\n        raise\n\n    except ValueError as e:\n        # Don't retry - this is a permanent error\n        return f'ERROR: {e}'\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#retry-vs-error-handling", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "\"Still failing after retries\"", "anchor": "still-failing-after-retries", "heading_level": 3, "md_text": "Increase retry attempts or check if error is retriable:\n\n```python\nimport logging\n\nfrom tenacity import stop_after_attempt\n\nfrom pydantic_evals import Case, Dataset\n\n\ndef task(inputs: str) -> str:\n    return f'Result: {inputs}'", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#still-failing-after-retries", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "Add logging to see what's failing", "anchor": "add-logging-to-see-whats-failing", "heading_level": 1, "md_text": "logging.basicConfig(level=logging.DEBUG)\n\ndataset = Dataset(cases=[Case(inputs='test')], evaluators=[])", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#add-logging-to-see-whats-failing", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "Tenacity logs retry attempts", "anchor": "tenacity-logs-retry-attempts", "heading_level": 1, "md_text": "report = dataset.evaluate_sync(task, retry_task={'stop': stop_after_attempt(5)})\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#tenacity-logs-retry-attempts", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "\"Evaluations taking too long\"", "anchor": "evaluations-taking-too-long", "heading_level": 3, "md_text": "Reduce retry attempts or wait times:\n\n```python\nfrom tenacity import stop_after_attempt, wait_exponential", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#evaluations-taking-too-long", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "Faster retries", "anchor": "faster-retries", "heading_level": 1, "md_text": "retry_config = {\n    'stop': stop_after_attempt(3),  # Fewer attempts\n    'wait': wait_exponential(multiplier=0.1, min=0.1, max=2),  # Quick retries, capped at 2s\n    'reraise': True,\n}\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#faster-retries", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "\"Hitting rate limits despite retries\"", "anchor": "hitting-rate-limits-despite-retries", "heading_level": 3, "md_text": "Increase delays or use `max_concurrency`:\n\n```python\nfrom tenacity import stop_after_attempt, wait_exponential\n\nfrom pydantic_evals import Case, Dataset\n\n\ndef task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test')], evaluators=[])", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#hitting-rate-limits-despite-retries", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "Longer delays", "anchor": "longer-delays", "heading_level": 1, "md_text": "retry_config = {\n    'stop': stop_after_attempt(5),\n    'wait': wait_exponential(multiplier=5, min=5, max=60),  # Start at 5s, exponential up to 60s\n    'reraise': True,\n}", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#longer-delays", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "Also reduce concurrency", "anchor": "also-reduce-concurrency", "heading_level": 1, "md_text": "report = dataset.evaluate_sync(\n    task=task,\n    retry_task=retry_config,\n    max_concurrency=2,  # Only 2 concurrent tasks\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#also-reduce-concurrency", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "- **[Concurrency & Performance](concurrency.md)** - Optimize evaluation performance\n- **[Logfire Integration](logfire-integration.md)** - View retries in Logfire", "url": "https://ai.pydantic.dev/docs/evals/how-to/retry-strategies/#next-steps", "page": "docs/evals/how-to/retry-strategies", "source_site": "pydantic_ai"}
{"title": "Concurrency & Performance", "anchor": "concurrency-performance", "heading_level": 1, "md_text": "Control how evaluation cases are executed in parallel.", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#concurrency-performance", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "By default, Pydantic Evals runs all cases concurrently to maximize throughput. You can control this behavior using the `max_concurrency` parameter.", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#overview", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Basic Usage", "anchor": "basic-usage", "heading_level": 2, "md_text": "```python\nfrom pydantic_evals import Case, Dataset\n\n\ndef my_task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test1'), Case(inputs='test2')])", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#basic-usage", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Run all cases concurrently (default)", "anchor": "run-all-cases-concurrently-default", "heading_level": 1, "md_text": "report = dataset.evaluate_sync(my_task)", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#run-all-cases-concurrently-default", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Limit to 5 concurrent cases", "anchor": "limit-to-5-concurrent-cases", "heading_level": 1, "md_text": "report = dataset.evaluate_sync(my_task, max_concurrency=5)", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#limit-to-5-concurrent-cases", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Run sequentially (one at a time)", "anchor": "run-sequentially-one-at-a-time", "heading_level": 1, "md_text": "report = dataset.evaluate_sync(my_task, max_concurrency=1)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#run-sequentially-one-at-a-time", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Rate Limiting", "anchor": "rate-limiting", "heading_level": 3, "md_text": "Many APIs have rate limits that restrict concurrent requests:\n\n```python\nfrom pydantic_evals import Case, Dataset\n\n\nasync def my_llm_task(inputs: str) -> str:\n    return f'LLM Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test1')])", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#rate-limiting", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "If your API allows 10 requests/second", "anchor": "if-your-api-allows-10-requestssecond", "heading_level": 1, "md_text": "report = dataset.evaluate_sync(\n    my_llm_task,\n    max_concurrency=10,\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#if-your-api-allows-10-requestssecond", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Resource Constraints", "anchor": "resource-constraints", "heading_level": 3, "md_text": "Limit concurrency to avoid overwhelming system resources:\n\n```python\nfrom pydantic_evals import Case, Dataset\n\n\ndef heavy_computation(inputs: str) -> str:\n    return f'Heavy: {inputs}'\n\n\ndef db_query_task(inputs: str) -> str:\n    return f'DB: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test1')])", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#resource-constraints", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Memory-intensive operations", "anchor": "memory-intensive-operations", "heading_level": 1, "md_text": "report = dataset.evaluate_sync(\n    heavy_computation,\n    max_concurrency=2,  # Only 2 at a time\n)", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#memory-intensive-operations", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Database connection pool limits", "anchor": "database-connection-pool-limits", "heading_level": 1, "md_text": "report = dataset.evaluate_sync(\n    db_query_task,\n    max_concurrency=5,  # Match connection pool size\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#database-connection-pool-limits", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Debugging", "anchor": "debugging", "heading_level": 3, "md_text": "Run sequentially to see clear error traces:\n\n```python\nfrom pydantic_evals import Case, Dataset\n\n\ndef my_task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test1')])", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#debugging", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Easier to debug", "anchor": "easier-to-debug", "heading_level": 1, "md_text": "report = dataset.evaluate_sync(\n    my_task,\n    max_concurrency=1,\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#easier-to-debug", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Performance Comparison", "anchor": "performance-comparison", "heading_level": 2, "md_text": "Here's an example showing the performance difference:\n\n```python {title=\"concurrency_example.py\"}\nimport asyncio\n\nfrom pydantic_evals import Case, Dataset", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#performance-comparison", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Create a dataset with multiple test cases", "anchor": "create-a-dataset-with-multiple-test-cases", "heading_level": 1, "md_text": "dataset = Dataset(\n    cases=[\n        Case(\n            name=f'case_{i}',\n            inputs=i,\n            expected_output=i * 2,\n        )\n        for i in range(10)\n    ]\n)\n\n\nasync def slow_task(input_value: int) -> int:\n    \"\"\"Simulates a slow operation (e.g., API call).\"\"\"\n    await asyncio.sleep(0.1)  # 100ms per case\n    return input_value * 2", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#create-a-dataset-with-multiple-test-cases", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Unlimited concurrency: ~0.1s total (all cases run in parallel)", "anchor": "unlimited-concurrency-01s-total-all-cases-run-in-parallel", "heading_level": 1, "md_text": "report = dataset.evaluate_sync(slow_task)", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#unlimited-concurrency-01s-total-all-cases-run-in-parallel", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Limited concurrency: ~0.5s total (2 at a time, 5 batches)", "anchor": "limited-concurrency-05s-total-2-at-a-time-5-batches", "heading_level": 1, "md_text": "report = dataset.evaluate_sync(slow_task, max_concurrency=2)", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#limited-concurrency-05s-total-2-at-a-time-5-batches", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Sequential: ~1.0s total (one at a time, 10 cases)", "anchor": "sequential-10s-total-one-at-a-time-10-cases", "heading_level": 1, "md_text": "report = dataset.evaluate_sync(slow_task, max_concurrency=1)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#sequential-10s-total-one-at-a-time-10-cases", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Concurrency with Evaluators", "anchor": "concurrency-with-evaluators", "heading_level": 2, "md_text": "Both task execution and evaluator execution happen concurrently by default:\n\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\n\ndef my_task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(\n    cases=[Case(inputs=f'test{i}') for i in range(100)],  # 100 cases\n    evaluators=[\n        LLMJudge(rubric='Quality check'),  # Makes API calls\n    ],\n)", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#concurrency-with-evaluators", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Both task and evaluator run with controlled concurrency", "anchor": "both-task-and-evaluator-run-with-controlled-concurrency", "heading_level": 1, "md_text": "report = dataset.evaluate_sync(\n    my_task,\n    max_concurrency=10,\n)\n```\n\nIf your evaluators are expensive (e.g., [`LLMJudge`][pydantic_evals.evaluators.LLMJudge]), limiting concurrency helps manage:\n- API rate limits\n- Cost (fewer concurrent API calls)\n- Memory usage", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#both-task-and-evaluator-run-with-controlled-concurrency", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Async vs Sync", "anchor": "async-vs-sync", "heading_level": 2, "md_text": "Both sync and async evaluation support concurrency control:", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#async-vs-sync", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Sync API", "anchor": "sync-api", "heading_level": 3, "md_text": "```python\nfrom pydantic_evals import Case, Dataset\n\n\ndef my_task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test1')])", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#sync-api", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Runs async operations internally with controlled concurrency", "anchor": "runs-async-operations-internally-with-controlled-concurrency", "heading_level": 1, "md_text": "report = dataset.evaluate_sync(my_task, max_concurrency=10)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#runs-async-operations-internally-with-controlled-concurrency", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Async API", "anchor": "async-api", "heading_level": 3, "md_text": "```python\nfrom pydantic_evals import Case, Dataset\n\n\nasync def my_task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\nasync def run_evaluation():\n    dataset = Dataset(cases=[Case(inputs='test1')])\n    # Same behavior, but in async context\n    report = await dataset.evaluate(my_task, max_concurrency=10)\n    return report\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#async-api", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Monitoring Concurrency", "anchor": "monitoring-concurrency", "heading_level": 2, "md_text": "Track execution to optimize settings:\n\n```python {test=\"skip\"}\nimport time\n\nfrom pydantic_evals import Case, Dataset\n\n\ndef task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs=f'test{i}') for i in range(10)])\n\nt0 = time.time()\nreport = dataset.evaluate_sync(task, max_concurrency=10)\nduration = time.time() - t0\n\nnum_cases = len(report.cases) + len(report.failures)\navg_duration = duration / num_cases\n\nprint(f'Total: {duration:.2f}s')\n#> Total: 0.01s\nprint(f'Cases: {num_cases}')\n#> Cases: 10\nprint(f'Avg per case: {avg_duration:.2f}s')\n#> Avg per case: 0.00s\nprint(f'Effective concurrency: ~{num_cases * avg_duration / duration:.1f}')\n#> Effective concurrency: ~1.0\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#monitoring-concurrency", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Handling Rate Limits", "anchor": "handling-rate-limits", "heading_level": 2, "md_text": "If you hit rate limits, the evaluation will fail. Use retry strategies:\n\n```python\nfrom pydantic_evals import Case, Dataset\n\n\ndef task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test1')])", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#handling-rate-limits", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Reduce concurrency to avoid rate limits", "anchor": "reduce-concurrency-to-avoid-rate-limits", "heading_level": 1, "md_text": "report = dataset.evaluate_sync(\n    task,\n    max_concurrency=5,  # Stay under rate limit\n)\n```\n\nSee [Retry Strategies](retry-strategies.md) for handling transient failures.", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#reduce-concurrency-to-avoid-rate-limits", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "- **[Retry Strategies](retry-strategies.md)** - Handle transient failures\n- **[Dataset Management](dataset-management.md)** - Work with large datasets\n- **[Logfire Integration](logfire-integration.md)** - Monitor performance", "url": "https://ai.pydantic.dev/docs/evals/how-to/concurrency/#next-steps", "page": "docs/evals/how-to/concurrency", "source_site": "pydantic_ai"}
{"title": "Dataset Management", "anchor": "dataset-management", "heading_level": 1, "md_text": "Create, save, load, and generate evaluation datasets.", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#dataset-management", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "From Code", "anchor": "from-code", "heading_level": 3, "md_text": "Define datasets directly in Python:\n\n```python\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import EqualsExpected, IsInstance\n\ndataset = Dataset[str, str, Any](\n    name='my_eval_suite',\n    cases=[\n        Case(\n            name='test_1',\n            inputs='input 1',\n            expected_output='output 1',\n        ),\n        Case(\n            name='test_2',\n            inputs='input 2',\n            expected_output='output 2',\n        ),\n    ],\n    evaluators=[\n        IsInstance(type_name='str'),\n        EqualsExpected(),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#from-code", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Adding Cases Dynamically", "anchor": "adding-cases-dynamically", "heading_level": 3, "md_text": "```python\nfrom typing import Any\n\nfrom pydantic_evals import Dataset\nfrom pydantic_evals.evaluators import IsInstance\n\ndataset = Dataset[str, str, Any](cases=[], evaluators=[])", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#adding-cases-dynamically", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Add cases one at a time", "anchor": "add-cases-one-at-a-time", "heading_level": 1, "md_text": "dataset.add_case(\n    name='dynamic_case',\n    inputs='test input',\n    expected_output='test output',\n)", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#add-cases-one-at-a-time", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Add evaluators", "anchor": "add-evaluators", "heading_level": 1, "md_text": "dataset.add_evaluator(IsInstance(type_name='str'))\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#add-evaluators", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Saving Datasets", "anchor": "saving-datasets", "heading_level": 2, "md_text": "!!! info \"Detailed Serialization Guide\"\n    For complete details on serialization formats, JSON schema generation, and custom evaluators, see [Dataset Serialization](dataset-serialization.md).", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#saving-datasets", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Save to YAML", "anchor": "save-to-yaml", "heading_level": 3, "md_text": "```python\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\n\ndataset = Dataset[str, str, Any](cases=[Case(name='test', inputs='example')])\ndataset.to_file('my_dataset.yaml')", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#save-to-yaml", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Also saves schema file: my_dataset_schema.json", "anchor": "also-saves-schema-file-my_dataset_schemajson", "heading_level": 1, "md_text": "```\n\nOutput (`my_dataset.yaml`):\n\n```yaml", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#also-saves-schema-file-my_dataset_schemajson", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "yaml-language-server: $schema=my_dataset_schema.json", "anchor": "yaml-language-server-schemamy_dataset_schemajson", "heading_level": 1, "md_text": "name: my_eval_suite\ncases:\n- name: test_1\n  inputs: input 1\n  expected_output: output 1\n  evaluators:\n  - EqualsExpected\n- name: test_2\n  inputs: input 2\n  expected_output: output 2\n  evaluators:\n  - EqualsExpected\nevaluators:\n- IsInstance: str\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#yaml-language-server-schemamy_dataset_schemajson", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Save to JSON", "anchor": "save-to-json", "heading_level": 3, "md_text": "```python\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\n\ndataset = Dataset[str, str, Any](cases=[Case(name='test', inputs='example')])\ndataset.to_file('my_dataset.json')", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#save-to-json", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Also saves schema file: my_dataset_schema.json", "anchor": "also-saves-schema-file-my_dataset_schemajson", "heading_level": 1, "md_text": "```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#also-saves-schema-file-my_dataset_schemajson", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Custom Schema Path", "anchor": "custom-schema-path", "heading_level": 3, "md_text": "```python\nfrom pathlib import Path\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\n\ndataset = Dataset[str, str, Any](cases=[Case(name='test', inputs='example')])", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#custom-schema-path", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Custom schema location", "anchor": "custom-schema-location", "heading_level": 1, "md_text": "Path('data').mkdir(exist_ok=True)\nPath('data/schemas').mkdir(parents=True, exist_ok=True)\ndataset.to_file(\n    'data/my_dataset.yaml',\n    schema_path='schemas/my_schema.json',\n)", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#custom-schema-location", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "No schema file", "anchor": "no-schema-file", "heading_level": 1, "md_text": "dataset.to_file('my_dataset.yaml', schema_path=None)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#no-schema-file", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "From YAML/JSON", "anchor": "from-yamljson", "heading_level": 3, "md_text": "```python {test=\"skip\"}\nfrom typing import Any\n\nfrom pydantic_evals import Dataset", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#from-yamljson", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Infers format from extension", "anchor": "infers-format-from-extension", "heading_level": 1, "md_text": "dataset = Dataset[str, str, Any].from_file('my_dataset.yaml')\ndataset = Dataset[str, str, Any].from_file('my_dataset.json')", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#infers-format-from-extension", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Explicit format for non-standard extensions", "anchor": "explicit-format-for-non-standard-extensions", "heading_level": 1, "md_text": "dataset = Dataset[str, str, Any].from_file('data.txt', fmt='yaml')\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#explicit-format-for-non-standard-extensions", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "From String", "anchor": "from-string", "heading_level": 3, "md_text": "```python\nfrom typing import Any\n\nfrom pydantic_evals import Dataset\n\nyaml_content = \"\"\"\ncases:\n- name: test\n  inputs: hello\n  expected_output: HELLO\nevaluators:\n- EqualsExpected\n\"\"\"\n\ndataset = Dataset[str, str, Any].from_text(yaml_content, fmt='yaml')\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#from-string", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "From Dict", "anchor": "from-dict", "heading_level": 3, "md_text": "```python\nfrom typing import Any\n\nfrom pydantic_evals import Dataset\n\ndata = {\n    'cases': [\n        {\n            'name': 'test',\n            'inputs': 'hello',\n            'expected_output': 'HELLO',\n        },\n    ],\n    'evaluators': [{'EqualsExpected': {}}],\n}\n\ndataset = Dataset[str, str, Any].from_dict(data)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#from-dict", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "With Custom Evaluators", "anchor": "with-custom-evaluators", "heading_level": 3, "md_text": "When loading datasets that use custom evaluators, you must pass them to `from_file()`:\n\n```python {test=\"skip\"}\nfrom dataclasses import dataclass\nfrom typing import Any\n\nfrom pydantic_evals import Dataset\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass MyCustomEvaluator(Evaluator):\n    threshold: float = 0.5\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return True", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#with-custom-evaluators", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Load with custom evaluator registry", "anchor": "load-with-custom-evaluator-registry", "heading_level": 1, "md_text": "dataset = Dataset[str, str, Any].from_file(\n    'my_dataset.yaml',\n    custom_evaluator_types=[MyCustomEvaluator],\n)\n```\n\nFor complete details on serialization with custom evaluators, see [Dataset Serialization](dataset-serialization.md).", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#load-with-custom-evaluator-registry", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Generating Datasets", "anchor": "generating-datasets", "heading_level": 2, "md_text": "Pydantic Evals allows you to generate test datasets using LLMs with [`generate_dataset`][pydantic_evals.generation.generate_dataset].\n\nDatasets can be generated in either JSON or YAML format, in both cases a JSON schema file is generated alongside the dataset and referenced in the dataset, so you should get type checking and auto-completion in your editor.\n\n```python {title=\"generate_dataset_example.py\"}\nfrom __future__ import annotations\n\nfrom pathlib import Path\n\nfrom pydantic import BaseModel, Field\n\nfrom pydantic_evals import Dataset\nfrom pydantic_evals.generation import generate_dataset\n\n\nclass QuestionInputs(BaseModel, use_attribute_docstrings=True):  # (1)!\n    \"\"\"Model for question inputs.\"\"\"\n\n    question: str\n    \"\"\"A question to answer\"\"\"\n    context: str | None = None\n    \"\"\"Optional context for the question\"\"\"\n\n\nclass AnswerOutput(BaseModel, use_attribute_docstrings=True):  # (2)!\n    \"\"\"Model for expected answer outputs.\"\"\"\n\n    answer: str\n    \"\"\"The answer to the question\"\"\"\n    confidence: float = Field(ge=0, le=1)\n    \"\"\"Confidence level (0-1)\"\"\"\n\n\nclass MetadataType(BaseModel, use_attribute_docstrings=True):  # (3)!\n    \"\"\"Metadata model for test cases.\"\"\"\n\n    difficulty: str\n    \"\"\"Difficulty level (easy, medium, hard)\"\"\"\n    category: str\n    \"\"\"Question category\"\"\"\n\n\nasync def main():\n    dataset = await generate_dataset(  # (4)!\n        dataset_type=Dataset[QuestionInputs, AnswerOutput, MetadataType],\n        n_examples=2,\n        extra_instructions=\"\"\"\n        Generate question-answer pairs about world capitals and landmarks.\n        Make sure to include both easy and challenging questions.\n        \"\"\",\n    )\n    output_file = Path('questions_cases.yaml')\n    dataset.to_file(output_file)  # (5)!\n    print(output_file.read_text())\n    \"\"\"\n    # yaml-language-server: $schema=questions_cases_schema.json\n    name: null\n    cases:\n    - name: Easy Capital Question\n      inputs:\n        question: What is the capital of France?\n        context: null\n      metadata:\n        difficulty: easy\n        category: Geography\n      expected_output:\n        answer: Paris\n        confidence: 0.95\n      evaluators:\n      - EqualsExpected\n    - name: Challenging Landmark Question\n      inputs:\n        question: Which world-famous landmark is located on the banks of the Seine River?\n        context: null\n      metadata:\n        difficulty: hard\n        category: Landmarks\n      expected_output:\n        answer: Eiffel Tower\n        confidence: 0.9\n      evaluators:\n      - EqualsExpected\n    evaluators: []\n    \"\"\"\n```\n\n1. Define the schema for the inputs to the task.\n2. Define the schema for the expected outputs of the task.\n3. Define the schema for the metadata of the test cases.\n4. Call [`generate_dataset`][pydantic_evals.generation.generate_dataset] to create a [`Dataset`][pydantic_evals.Dataset] with 2 cases confirming to the schema.\n5. Save the dataset to a YAML file, this will also write `questions_cases_schema.json` with the schema JSON schema for `questions_cases.yaml` to make editing easier. The magic `yaml-language-server` comment is supported by at least vscode, jetbrains/pycharm (more details [here](https://github.com/redhat-developer/yaml-language-server#using-inlined-schema)).\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main(answer))` to run `main`)_\n\nYou can also write datasets as JSON files:\n\n```python {title=\"generate_dataset_example_json.py\" requires=\"generate_dataset_example.py\"}\nfrom pathlib import Path\n\nfrom pydantic_evals import Dataset\nfrom pydantic_evals.generation import generate_dataset\n\nfrom generate_dataset_example import AnswerOutput, MetadataType, QuestionInputs\n\n\nasync def main():\n    dataset = await generate_dataset(  # (1)!\n        dataset_type=Dataset[QuestionInputs, AnswerOutput, MetadataType],\n        n_examples=2,\n        extra_instructions=\"\"\"\n        Generate question-answer pairs about world capitals and landmarks.\n        Make sure to include both easy and challenging questions.\n        \"\"\",\n    )\n    output_file = Path('questions_cases.json')\n    dataset.to_file(output_file)  # (2)!\n    print(output_file.read_text())\n    \"\"\"\n    {\n      \"$schema\": \"questions_cases_schema.json\",\n      \"name\": null,\n      \"cases\": [\n        {\n          \"name\": \"Easy Capital Question\",\n          \"inputs\": {\n            \"question\": \"What is the capital of France?\",\n            \"context\": null\n          },\n          \"metadata\": {\n            \"difficulty\": \"easy\",\n            \"category\": \"Geography\"\n          },\n          \"expected_output\": {\n            \"answer\": \"Paris\",\n            \"confidence\": 0.95\n          },\n          \"evaluators\": [\n            \"EqualsExpected\"\n          ]\n        },\n        {\n          \"name\": \"Challenging Landmark Question\",\n          \"inputs\": {\n            \"question\": \"Which world-famous landmark is located on the banks of the Seine River?\",\n            \"context\": null\n          },\n          \"metadata\": {\n            \"difficulty\": \"hard\",\n            \"category\": \"Landmarks\"\n          },\n          \"expected_output\": {\n            \"answer\": \"Eiffel Tower\",\n            \"confidence\": 0.9\n          },\n          \"evaluators\": [\n            \"EqualsExpected\"\n          ]\n        }\n      ],\n      \"evaluators\": []\n    }\n    \"\"\"\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#generating-datasets", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Generating Datasets", "anchor": "generating-datasets", "heading_level": 2, "md_text": "1. Generate the [`Dataset`][pydantic_evals.Dataset] exactly as above.\n2. Save the dataset to a JSON file, this will also write `questions_cases_schema.json` with th JSON schema for `questions_cases.json`. This time the `$schema` key is included in the JSON file to define the schema for IDEs to use while you edit the file, there's no formal spec for this, but it works in vscode and pycharm and is discussed at length in [json-schema-org/json-schema-spec#828](https://github.com/json-schema-org/json-schema-spec/issues/828).\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `asyncio.run(main(answer))` to run `main`)_", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#generating-datasets", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Type-Safe Datasets", "anchor": "type-safe-datasets", "heading_level": 2, "md_text": "Use generic type parameters for type safety:\n\n```python\nfrom typing_extensions import TypedDict\n\nfrom pydantic_evals import Case, Dataset\n\n\nclass MyInput(TypedDict):\n    query: str\n    max_results: int\n\n\nclass MyOutput(TypedDict):\n    results: list[str]\n\n\nclass MyMetadata(TypedDict):\n    category: str", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#type-safe-datasets", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Type-safe dataset", "anchor": "type-safe-dataset", "heading_level": 1, "md_text": "dataset: Dataset[MyInput, MyOutput, MyMetadata] = Dataset(\n    cases=[\n        Case(\n            name='test',\n            inputs={'query': 'test', 'max_results': 10},\n            expected_output={'results': ['a', 'b']},\n            metadata={'category': 'search'},\n        ),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#type-safe-dataset", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Schema Generation", "anchor": "schema-generation", "heading_level": 2, "md_text": "Generate JSON Schema for IDE support:\n\n```python\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\n\ndataset = Dataset[str, str, Any](cases=[Case(name='test', inputs='example')])", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#schema-generation", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Save with schema", "anchor": "save-with-schema", "heading_level": 1, "md_text": "dataset.to_file('my_dataset.yaml')  # Creates my_dataset_schema.json", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#save-with-schema", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "- Inline documentation", "anchor": "-inline-documentation", "heading_level": 1, "md_text": "```\n\nManual schema generation:\n\n```python\nimport json\nfrom dataclasses import dataclass\nfrom typing import Any\n\nfrom pydantic_evals import Dataset\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass MyCustomEvaluator(Evaluator):\n    threshold: float = 0.5\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return True\n\n\nschema = Dataset[str, str, Any].model_json_schema_with_evaluators(\n    custom_evaluator_types=[MyCustomEvaluator],\n)\nprint(json.dumps(schema, indent=2)[:66] + '...')\n\"\"\"\n{\n  \"$defs\": {\n    \"Case\": {\n      \"additionalProperties\": false,\n...\n\"\"\"\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#-inline-documentation", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "1. Use Clear Names", "anchor": "1-use-clear-names", "heading_level": 3, "md_text": "```python\nfrom pydantic_evals import Case", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#1-use-clear-names", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Good", "anchor": "good", "heading_level": 1, "md_text": "Case(name='uppercase_basic_ascii', inputs='hello')\nCase(name='uppercase_unicode_emoji', inputs='hello \ud83d\ude00')\nCase(name='uppercase_empty_string', inputs='')", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#good", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Bad", "anchor": "bad", "heading_level": 1, "md_text": "Case(name='test1', inputs='hello')\nCase(name='test2', inputs='world')\nCase(name='test3', inputs='foo')\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#bad", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "2. Organize by Difficulty", "anchor": "2-organize-by-difficulty", "heading_level": 3, "md_text": "```python\nfrom pydantic_evals import Case, Dataset\n\ndataset = Dataset(\n    cases=[\n        Case(name='easy_1', inputs='test', metadata={'difficulty': 'easy'}),\n        Case(name='easy_2', inputs='test2', metadata={'difficulty': 'easy'}),\n        Case(name='medium_1', inputs='test3', metadata={'difficulty': 'medium'}),\n        Case(name='hard_1', inputs='test4', metadata={'difficulty': 'hard'}),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#2-organize-by-difficulty", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "3. Start Small, Grow Gradually", "anchor": "3-start-small-grow-gradually", "heading_level": 3, "md_text": "```python\nfrom pydantic_evals import Case, Dataset", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#3-start-small-grow-gradually", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Start with representative cases", "anchor": "start-with-representative-cases", "heading_level": 1, "md_text": "dataset = Dataset(\n    cases=[\n        Case(name='happy_path', inputs='test'),\n        Case(name='edge_case', inputs=''),\n        Case(name='error_case', inputs='invalid'),\n    ],\n)", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#start-with-representative-cases", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Add more as you find issues", "anchor": "add-more-as-you-find-issues", "heading_level": 1, "md_text": "dataset.add_case(name='newly_discovered_edge_case', inputs='edge')\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#add-more-as-you-find-issues", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "4. Use Case-specific Evaluators Where Appropriate", "anchor": "4-use-case-specific-evaluators-where-appropriate", "heading_level": 3, "md_text": "Case-specific evaluators let different cases have different evaluation criteria, which is essential for comprehensive \"test coverage\". Rather than trying to write one-size-fits-all evaluators, you can specify exactly what \"good\" looks like for each scenario. This is particularly powerful with [`LLMJudge`][pydantic_evals.evaluators.LLMJudge] evaluators where you can describe nuanced requirements per case, making it easy to build and maintain golden datasets. See [Case-specific evaluators](../evaluators/overview.md#case-specific-evaluators) for detailed guidance.", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#4-use-case-specific-evaluators-where-appropriate", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "5. Separate Datasets by Purpose", "anchor": "5-separate-datasets-by-purpose", "heading_level": 3, "md_text": "```python\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#5-separate-datasets-by-purpose", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "First create some test datasets", "anchor": "first-create-some-test-datasets", "heading_level": 1, "md_text": "for name in ['smoke_tests', 'comprehensive_tests', 'regression_tests']:\n    test_dataset = Dataset[str, Any, Any](cases=[Case(name='test', inputs='example')])\n    test_dataset.to_file(f'{name}.yaml')", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#first-create-some-test-datasets", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Smoke tests (fast, critical paths)", "anchor": "smoke-tests-fast-critical-paths", "heading_level": 1, "md_text": "smoke_tests = Dataset[str, Any, Any].from_file('smoke_tests.yaml')", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#smoke-tests-fast-critical-paths", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Comprehensive tests (slow, thorough)", "anchor": "comprehensive-tests-slow-thorough", "heading_level": 1, "md_text": "comprehensive = Dataset[str, Any, Any].from_file('comprehensive_tests.yaml')", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#comprehensive-tests-slow-thorough", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Regression tests (specific bugs)", "anchor": "regression-tests-specific-bugs", "heading_level": 1, "md_text": "regression = Dataset[str, Any, Any].from_file('regression_tests.yaml')\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#regression-tests-specific-bugs", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "- **[Dataset Serialization](dataset-serialization.md)** - In-depth guide to saving and loading datasets\n- **[Generating Datasets](#generating-datasets)** - Use LLMs to generate test cases\n- **[Examples: Simple Validation](../examples/simple-validation.md)** - Practical examples", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-management/#next-steps", "page": "docs/evals/how-to/dataset-management", "source_site": "pydantic_ai"}
{"title": "Dataset Serialization", "anchor": "dataset-serialization", "heading_level": 1, "md_text": "Learn how to save and load datasets in different formats, with support for custom evaluators and IDE integration.", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#dataset-serialization", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "Pydantic Evals supports serializing datasets to files in two formats:\n\n- **YAML** (`.yaml`, `.yml`) - Human-readable, great for version control\n- **JSON** (`.json`) - Structured, machine-readable\n\nBoth formats support:\n- Automatic JSON schema generation for IDE autocomplete and validation\n- Custom evaluator serialization/deserialization\n- Type-safe loading with generic parameters", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#overview", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "YAML Format", "anchor": "yaml-format", "heading_level": 2, "md_text": "YAML is the recommended format for most use cases due to its readability and compact syntax.", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#yaml-format", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Basic Example", "anchor": "basic-example", "heading_level": 3, "md_text": "```python\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import EqualsExpected, IsInstance", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#basic-example", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Create a dataset with typed parameters", "anchor": "create-a-dataset-with-typed-parameters", "heading_level": 1, "md_text": "dataset = Dataset[str, str, Any](\n    name='my_tests',\n    cases=[\n        Case(\n            name='test_1',\n            inputs='hello',\n            expected_output='HELLO',\n        ),\n    ],\n    evaluators=[\n        IsInstance(type_name='str'),\n        EqualsExpected(),\n    ],\n)", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#create-a-dataset-with-typed-parameters", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Save to YAML", "anchor": "save-to-yaml", "heading_level": 1, "md_text": "dataset.to_file('my_tests.yaml')\n```\n\nThis creates two files:\n\n1. **`my_tests.yaml`** - The dataset\n2. **`my_tests_schema.json`** - JSON schema for IDE support", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#save-to-yaml", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "YAML Output", "anchor": "yaml-output", "heading_level": 3, "md_text": "```yaml", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#yaml-output", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "yaml-language-server: $schema=my_tests_schema.json", "anchor": "yaml-language-server-schemamy_tests_schemajson", "heading_level": 1, "md_text": "name: my_tests\ncases:\n- name: test_1\n  inputs: hello\n  expected_output: HELLO\nevaluators:\n- IsInstance: str\n- EqualsExpected\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#yaml-language-server-schemamy_tests_schemajson", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "JSON Schema for IDEs", "anchor": "json-schema-for-ides", "heading_level": 3, "md_text": "The first line references the schema file:\n\n```yaml", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#json-schema-for-ides", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "yaml-language-server: $schema=my_tests_schema.json", "anchor": "yaml-language-server-schemamy_tests_schemajson", "heading_level": 1, "md_text": "```\n\nThis enables:\n- \u2705 **Autocomplete** in VS Code, PyCharm, and other editors\n- \u2705 **Inline validation** while editing\n- \u2705 **Documentation tooltips** for fields\n- \u2705 **Error highlighting** for invalid data\n\n!!! note \"Editor Support\"\n    The `yaml-language-server` comment is supported by:\n\n    - VS Code (with YAML extension)\n    - JetBrains IDEs (PyCharm, IntelliJ, etc.)\n    - Most editors with YAML language server support\n\n    See the [YAML Language Server docs](https://github.com/redhat-developer/yaml-language-server#using-inlined-schema) for more details.", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#yaml-language-server-schemamy_tests_schemajson", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Loading from YAML", "anchor": "loading-from-yaml", "heading_level": 3, "md_text": "```python\nfrom pathlib import Path\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import EqualsExpected, IsInstance", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#loading-from-yaml", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "First create and save the dataset", "anchor": "first-create-and-save-the-dataset", "heading_level": 1, "md_text": "Path('my_tests.yaml').parent.mkdir(exist_ok=True)\ndataset = Dataset[str, str, Any](\n    name='my_tests',\n    cases=[Case(name='test_1', inputs='hello', expected_output='HELLO')],\n    evaluators=[IsInstance(type_name='str'), EqualsExpected()],\n)\ndataset.to_file('my_tests.yaml')", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#first-create-and-save-the-dataset", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Load the dataset with type parameters", "anchor": "load-the-dataset-with-type-parameters", "heading_level": 1, "md_text": "dataset = Dataset[str, str, Any].from_file('my_tests.yaml')\n\n\ndef my_task(text: str) -> str:\n    return text.upper()", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#load-the-dataset-with-type-parameters", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Run evaluation", "anchor": "run-evaluation", "heading_level": 1, "md_text": "report = dataset.evaluate_sync(my_task)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#run-evaluation", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "JSON Format", "anchor": "json-format", "heading_level": 2, "md_text": "JSON format is useful for programmatic generation or when strict structure is required.", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#json-format", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Basic Example", "anchor": "basic-example", "heading_level": 3, "md_text": "```python\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import EqualsExpected\n\ndataset = Dataset[str, str, Any](\n    name='my_tests',\n    cases=[\n        Case(name='test_1', inputs='hello', expected_output='HELLO'),\n    ],\n    evaluators=[EqualsExpected()],\n)", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#basic-example", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Save to JSON", "anchor": "save-to-json", "heading_level": 1, "md_text": "dataset.to_file('my_tests.json')\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#save-to-json", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "JSON Output", "anchor": "json-output", "heading_level": 3, "md_text": "```json\n{\n  \"$schema\": \"my_tests_schema.json\",\n  \"name\": \"my_tests\",\n  \"cases\": [\n    {\n      \"name\": \"test_1\",\n      \"inputs\": \"hello\",\n      \"expected_output\": \"HELLO\"\n    }\n  ],\n  \"evaluators\": [\n    \"EqualsExpected\"\n  ]\n}\n```\n\nThe `$schema` key at the top enables IDE support similar to YAML.", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#json-output", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Loading from JSON", "anchor": "loading-from-json", "heading_level": 3, "md_text": "```python\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import EqualsExpected", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#loading-from-json", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "First create and save the dataset", "anchor": "first-create-and-save-the-dataset", "heading_level": 1, "md_text": "dataset = Dataset[str, str, Any](\n    name='my_tests',\n    cases=[Case(name='test_1', inputs='hello', expected_output='HELLO')],\n    evaluators=[EqualsExpected()],\n)\ndataset.to_file('my_tests.json')", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#first-create-and-save-the-dataset", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Load from JSON", "anchor": "load-from-json", "heading_level": 1, "md_text": "dataset = Dataset[str, str, Any].from_file('my_tests.json')\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#load-from-json", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Automatic Schema Creation", "anchor": "automatic-schema-creation", "heading_level": 3, "md_text": "By default, `to_file()` creates a JSON schema file alongside your dataset:\n\n```python\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\n\ndataset = Dataset[str, str, Any](cases=[Case(inputs='test')])", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#automatic-schema-creation", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Creates both my_tests.yaml AND my_tests_schema.json", "anchor": "creates-both-my_testsyaml-and-my_tests_schemajson", "heading_level": 1, "md_text": "dataset.to_file('my_tests.yaml')\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#creates-both-my_testsyaml-and-my_tests_schemajson", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Custom Schema Location", "anchor": "custom-schema-location", "heading_level": 3, "md_text": "```python\nfrom pathlib import Path\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\n\ndataset = Dataset[str, str, Any](cases=[Case(inputs='test')])", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#custom-schema-location", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Create directories", "anchor": "create-directories", "heading_level": 1, "md_text": "Path('data').mkdir(exist_ok=True)", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#create-directories", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Custom schema filename (relative to dataset file location)", "anchor": "custom-schema-filename-relative-to-dataset-file-location", "heading_level": 1, "md_text": "dataset.to_file(\n    'data/my_tests.yaml',\n    schema_path='my_schema.json',\n)", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#custom-schema-filename-relative-to-dataset-file-location", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "No schema file", "anchor": "no-schema-file", "heading_level": 1, "md_text": "dataset.to_file('my_tests.yaml', schema_path=None)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#no-schema-file", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Schema Path Templates", "anchor": "schema-path-templates", "heading_level": 3, "md_text": "Use `{stem}` to reference the dataset filename:\n\n```python\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\n\ndataset = Dataset[str, str, Any](cases=[Case(inputs='test')])", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#schema-path-templates", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Creates: my_tests.yaml and my_tests.schema.json", "anchor": "creates-my_testsyaml-and-my_testsschemajson", "heading_level": 1, "md_text": "dataset.to_file(\n    'my_tests.yaml',\n    schema_path='{stem}.schema.json',\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#creates-my_testsyaml-and-my_testsschemajson", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Manual Schema Generation", "anchor": "manual-schema-generation", "heading_level": 3, "md_text": "Generate a schema without saving the dataset:\n\n```python\nimport json\nfrom typing import Any\n\nfrom pydantic_evals import Dataset", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#manual-schema-generation", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Get schema as dictionary for a specific dataset type", "anchor": "get-schema-as-dictionary-for-a-specific-dataset-type", "heading_level": 1, "md_text": "schema = Dataset[str, str, Any].model_json_schema_with_evaluators()", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#get-schema-as-dictionary-for-a-specific-dataset-type", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Save manually", "anchor": "save-manually", "heading_level": 1, "md_text": "with open('custom_schema.json', 'w') as f:\n    json.dump(schema, f, indent=2)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#save-manually", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Custom Evaluators", "anchor": "custom-evaluators", "heading_level": 2, "md_text": "Custom evaluators require special handling during serialization and deserialization.", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#custom-evaluators", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Requirements", "anchor": "requirements", "heading_level": 3, "md_text": "Custom evaluators must:\n\n1. Be decorated with `@dataclass`\n2. Inherit from `Evaluator`\n3. Be passed to both `to_file()` and `from_file()`", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#requirements", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Complete Example", "anchor": "complete-example", "heading_level": 3, "md_text": "```python\nfrom dataclasses import dataclass\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass CustomThreshold(Evaluator):\n    \"\"\"Check if output length exceeds a threshold.\"\"\"\n\n    min_length: int\n    max_length: int = 100\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        length = len(str(ctx.output))\n        return self.min_length <= length <= self.max_length", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#complete-example", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Create dataset with custom evaluator", "anchor": "create-dataset-with-custom-evaluator", "heading_level": 1, "md_text": "dataset = Dataset[str, str, Any](\n    cases=[\n        Case(\n            name='test_length',\n            inputs='example',\n            expected_output='long result',\n            evaluators=[\n                CustomThreshold(min_length=5, max_length=20),\n            ],\n        ),\n    ],\n)", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#create-dataset-with-custom-evaluator", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Save with custom evaluator types", "anchor": "save-with-custom-evaluator-types", "heading_level": 1, "md_text": "dataset.to_file(\n    'dataset.yaml',\n    custom_evaluator_types=[CustomThreshold],\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#save-with-custom-evaluator-types", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Saved YAML", "anchor": "saved-yaml", "heading_level": 3, "md_text": "```yaml", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#saved-yaml", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "yaml-language-server: $schema=dataset_schema.json", "anchor": "yaml-language-server-schemadataset_schemajson", "heading_level": 1, "md_text": "cases:\n- name: test_length\n  inputs: example\n  expected_output: long result\n  evaluators:\n  - CustomThreshold:\n      min_length: 5\n      max_length: 20\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#yaml-language-server-schemadataset_schemajson", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Loading with Custom Evaluators", "anchor": "loading-with-custom-evaluators", "heading_level": 3, "md_text": "```python\nfrom dataclasses import dataclass\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass CustomThreshold(Evaluator):\n    \"\"\"Check if output length exceeds a threshold.\"\"\"\n\n    min_length: int\n    max_length: int = 100\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        length = len(str(ctx.output))\n        return self.min_length <= length <= self.max_length", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#loading-with-custom-evaluators", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "First create and save the dataset", "anchor": "first-create-and-save-the-dataset", "heading_level": 1, "md_text": "dataset = Dataset[str, str, Any](\n    cases=[\n        Case(\n            name='test_length',\n            inputs='example',\n            expected_output='long result',\n            evaluators=[CustomThreshold(min_length=5, max_length=20)],\n        ),\n    ],\n)\ndataset.to_file('dataset.yaml', custom_evaluator_types=[CustomThreshold])", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#first-create-and-save-the-dataset", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Load with custom evaluator registry", "anchor": "load-with-custom-evaluator-registry", "heading_level": 1, "md_text": "dataset = Dataset[str, str, Any].from_file(\n    'dataset.yaml',\n    custom_evaluator_types=[CustomThreshold],\n)\n```\n\n!!! warning \"Important\"\n    You must pass `custom_evaluator_types` to **both** `to_file()` and `from_file()`.\n\n    - `to_file()`: Includes the evaluator in the JSON schema\n    - `from_file()`: Registers the evaluator for deserialization", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#load-with-custom-evaluator-registry", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Evaluator Serialization Formats", "anchor": "evaluator-serialization-formats", "heading_level": 2, "md_text": "Evaluators can be serialized in three forms:", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#evaluator-serialization-formats", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "1. Name Only (No Parameters)", "anchor": "1-name-only-no-parameters", "heading_level": 3, "md_text": "```yaml\nevaluators:\n- EqualsExpected\n- IsInstance: str  # Using default parameter\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#1-name-only-no-parameters", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "2. Single Parameter (Short Form)", "anchor": "2-single-parameter-short-form", "heading_level": 3, "md_text": "```yaml\nevaluators:\n- IsInstance: str\n- Contains: \"required text\"\n- MaxDuration: 2.0\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#2-single-parameter-short-form", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "3. Multiple Parameters (Dict Form)", "anchor": "3-multiple-parameters-dict-form", "heading_level": 3, "md_text": "```yaml\nevaluators:\n- CustomThreshold:\n    min_length: 5\n    max_length: 20\n- LLMJudge:\n    rubric: \"Response is accurate\"\n    model: \"openai:gpt-5\"\n    include_input: true\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#3-multiple-parameters-dict-form", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Format Comparison", "anchor": "format-comparison", "heading_level": 2, "md_text": "| Feature | YAML | JSON |\n|---------|------|------|\n| Human readable | \u2705 Excellent | \u26a0\ufe0f Good |\n| Comments | \u2705 Yes | \u274c No |\n| Compact | \u2705 Yes | \u26a0\ufe0f Verbose |\n| Machine parsing | \u2705 Good | \u2705 Excellent |\n| IDE support | \u2705 Yes | \u2705 Yes |\n| Version control | \u2705 Clean diffs | \u26a0\ufe0f Noisy diffs |\n\n**Recommendation**: Use YAML for most cases, JSON for programmatic generation.", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#format-comparison", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Advanced: Evaluator Serialization Name", "anchor": "advanced-evaluator-serialization-name", "heading_level": 2, "md_text": "Customize how your evaluator appears in serialized files:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass VeryLongDescriptiveEvaluatorName(Evaluator):\n    @classmethod\n    def get_serialization_name(cls) -> str:\n        return 'ShortName'\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return True\n```\n\nIn YAML:\n\n```yaml\nevaluators:\n- ShortName  # Instead of VeryLongDescriptiveEvaluatorName\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#advanced-evaluator-serialization-name", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Schema Not Found in IDE", "anchor": "schema-not-found-in-ide", "heading_level": 3, "md_text": "**Problem**: YAML file doesn't show autocomplete\n\n**Solutions**:\n\n1. **Check the schema path** in the first line of YAML:\n   ```yaml\n   # yaml-language-server: $schema=correct_schema_name.json\n   ```\n\n2. **Verify schema file exists** in the same directory\n\n3. **Restart the language server** in your IDE\n\n4. **Install YAML extension** (VS Code: \"YAML\" by Red Hat)", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#schema-not-found-in-ide", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Custom Evaluator Not Found", "anchor": "custom-evaluator-not-found", "heading_level": 3, "md_text": "**Problem**: `ValueError: Unknown evaluator name: 'CustomEvaluator'`\n\n**Solution**: Pass `custom_evaluator_types` when loading:\n\n```python\nfrom dataclasses import dataclass\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass CustomEvaluator(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return True", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#custom-evaluator-not-found", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "First create and save with custom evaluator", "anchor": "first-create-and-save-with-custom-evaluator", "heading_level": 1, "md_text": "dataset = Dataset[str, str, Any](\n    cases=[Case(inputs='test', evaluators=[CustomEvaluator()])],\n)\ndataset.to_file('tests.yaml', custom_evaluator_types=[CustomEvaluator])", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#first-create-and-save-with-custom-evaluator", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Load with custom evaluator types", "anchor": "load-with-custom-evaluator-types", "heading_level": 1, "md_text": "dataset = Dataset[str, str, Any].from_file(\n    'tests.yaml',\n    custom_evaluator_types=[CustomEvaluator],  # Required!\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#load-with-custom-evaluator-types", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Format Inference Failed", "anchor": "format-inference-failed", "heading_level": 3, "md_text": "**Problem**: `ValueError: Cannot infer format from extension`\n\n**Solution**: Specify format explicitly:\n\n```python\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\n\ndataset = Dataset[str, str, Any](cases=[Case(inputs='test')])", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#format-inference-failed", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Explicit format for unusual extensions", "anchor": "explicit-format-for-unusual-extensions", "heading_level": 1, "md_text": "dataset.to_file('data.txt', fmt='yaml')\ndataset_loaded = Dataset[str, str, Any].from_file('data.txt', fmt='yaml')\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#explicit-format-for-unusual-extensions", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Schema Generation Error", "anchor": "schema-generation-error", "heading_level": 3, "md_text": "**Problem**: Custom evaluator causes schema generation to fail\n\n**Solution**: Ensure evaluator is a proper dataclass:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#schema-generation-error", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "\u2705 Correct", "anchor": "-correct", "heading_level": 1, "md_text": "@dataclass\nclass MyEvaluator(Evaluator):\n    value: int\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return True", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#-correct", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "\u274c Wrong: Missing @dataclass", "anchor": "-wrong-missing-dataclass", "heading_level": 1, "md_text": "class BadEvaluator(Evaluator):\n    def __init__(self, value: int):\n        self.value = value\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return True\n```", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#-wrong-missing-dataclass", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "- **[Dataset Management](dataset-management.md)** - Creating and organizing datasets\n- **[Custom Evaluators](../evaluators/custom.md)** - Write custom evaluation logic\n- **[Core Concepts](../core-concepts.md)** - Understand the data model", "url": "https://ai.pydantic.dev/docs/evals/how-to/dataset-serialization/#next-steps", "page": "docs/evals/how-to/dataset-serialization", "source_site": "pydantic_ai"}
{"title": "Built-in Evaluators", "anchor": "built-in-evaluators", "heading_level": 1, "md_text": "Pydantic Evals provides several built-in evaluators for common evaluation tasks.", "url": "https://ai.pydantic.dev/docs/evals/evaluators/built-in/#built-in-evaluators", "page": "docs/evals/evaluators/built-in", "source_site": "pydantic_ai"}
{"title": "EqualsExpected", "anchor": "equalsexpected", "heading_level": 3, "md_text": "Check if the output exactly equals the expected output from the case.\n\n```python\nfrom pydantic_evals.evaluators import EqualsExpected\n\nEqualsExpected()\n```\n\n**Parameters:** None\n\n**Returns:** `bool` - `True` if `ctx.output == ctx.expected_output`\n\n**Example:**\n\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import EqualsExpected\n\ndataset = Dataset(\n    cases=[\n        Case(\n            name='addition',\n            inputs='2 + 2',\n            expected_output='4',\n        ),\n    ],\n    evaluators=[EqualsExpected()],\n)\n```\n\n**Notes:**\n\n- Skips evaluation if `expected_output` is `None` (returns empty dict `{}`)\n- Uses Python's `==` operator, so works with any comparable types\n- For structured data, considers nested equality\n\n---", "url": "https://ai.pydantic.dev/docs/evals/evaluators/built-in/#equalsexpected", "page": "docs/evals/evaluators/built-in", "source_site": "pydantic_ai"}
{"title": "Equals", "anchor": "equals", "heading_level": 3, "md_text": "Check if the output equals a specific value.\n\n```python\nfrom pydantic_evals.evaluators import Equals\n\nEquals(value='expected_result')\n```\n\n**Parameters:**\n\n- `value` (Any): The value to compare against\n- `evaluation_name` (str | None): Custom name for this evaluation in reports\n\n**Returns:** `bool` - `True` if `ctx.output == value`\n\n**Example:**\n\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Equals", "url": "https://ai.pydantic.dev/docs/evals/evaluators/built-in/#equals", "page": "docs/evals/evaluators/built-in", "source_site": "pydantic_ai"}
{"title": "Check output is always \"success\"", "anchor": "check-output-is-always-success", "heading_level": 1, "md_text": "dataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[\n        Equals(value='success', evaluation_name='is_success'),\n    ],\n)\n```\n\n**Use Cases:**\n\n- Checking for sentinel values\n- Validating consistent outputs\n- Testing classification into specific categories\n\n---", "url": "https://ai.pydantic.dev/docs/evals/evaluators/built-in/#check-output-is-always-success", "page": "docs/evals/evaluators/built-in", "source_site": "pydantic_ai"}
{"title": "Contains", "anchor": "contains", "heading_level": 3, "md_text": "Check if the output contains a specific value or substring.\n\n```python\nfrom pydantic_evals.evaluators import Contains\n\nContains(\n    value='substring',\n    case_sensitive=True,\n    as_strings=False,\n)\n```\n\n**Parameters:**\n\n- `value` (Any): The value to search for\n- `case_sensitive` (bool): Case-sensitive comparison for strings (default: `True`)\n- `as_strings` (bool): Convert both values to strings before checking (default: `False`)\n- `evaluation_name` (str | None): Custom name for this evaluation in reports\n\n**Returns:** [`EvaluationReason`][pydantic_evals.evaluators.EvaluationReason] - Pass/fail with explanation\n\n**Behavior:**\n\nFor **strings**: checks substring containment\n\n- `Contains(value='hello', case_sensitive=False)`\n  - Matches: \"Hello World\", \"say hello\", \"HELLO\"\n  - Doesn't match: \"hi there\"\n\nFor **lists/tuples**: checks membership\n\n- `Contains(value='apple')`\n  - Matches: `['apple', 'banana']`, `('apple',)`\n  - Doesn't match: `['apples', 'orange']`\n\nFor **dicts**: checks key-value pairs\n\n- `Contains(value={'name': 'Alice'})`\n  - Matches: `{'name': 'Alice', 'age': 30}`\n  - Doesn't match: `{'name': 'Bob'}`\n\n**Example:**\n\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Contains\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[\n        # Check for required keywords\n        Contains(value='terms and conditions', case_sensitive=False),\n        # Check for PII (fail if found)\n        # Note: Use a custom evaluator that returns False when PII found\n    ],\n)\n```\n\n**Use Cases:**\n\n- Required content verification\n- Keyword detection\n- PII/sensitive data detection\n- Multi-value validation\n\n---", "url": "https://ai.pydantic.dev/docs/evals/evaluators/built-in/#contains", "page": "docs/evals/evaluators/built-in", "source_site": "pydantic_ai"}
{"title": "IsInstance", "anchor": "isinstance", "heading_level": 3, "md_text": "Check if the output is an instance of a type with the given name.\n\n```python\nfrom pydantic_evals.evaluators import IsInstance\n\nIsInstance(type_name='str')\n```\n\n**Parameters:**\n\n- `type_name` (str): The type name to check (uses `__name__` or `__qualname__`)\n- `evaluation_name` (str | None): Custom name for this evaluation in reports\n\n**Returns:** [`EvaluationReason`][pydantic_evals.evaluators.EvaluationReason] - Pass/fail with type information\n\n**Example:**\n\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import IsInstance\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[\n        # Check output is always a string\n        IsInstance(type_name='str'),\n        # Check for Pydantic model\n        IsInstance(type_name='MyModel'),\n        # Check for dict\n        IsInstance(type_name='dict'),\n    ],\n)\n```\n\n**Notes:**\n\n- Matches against both `__name__` and `__qualname__` of the type\n- Works with built-in types (`str`, `int`, `dict`, `list`, etc.)\n- Works with custom classes and Pydantic models\n- Checks the entire MRO (Method Resolution Order) for inheritance\n\n**Use Cases:**\n\n- Format validation\n- Structured output verification\n- Type consistency checks\n\n---", "url": "https://ai.pydantic.dev/docs/evals/evaluators/built-in/#isinstance", "page": "docs/evals/evaluators/built-in", "source_site": "pydantic_ai"}
{"title": "MaxDuration", "anchor": "maxduration", "heading_level": 3, "md_text": "Check if task execution time is under a maximum threshold.\n\n```python\nfrom datetime import timedelta\n\nfrom pydantic_evals.evaluators import MaxDuration\n\nMaxDuration(seconds=2.0)", "url": "https://ai.pydantic.dev/docs/evals/evaluators/built-in/#maxduration", "page": "docs/evals/evaluators/built-in", "source_site": "pydantic_ai"}
{"title": "or", "anchor": "or", "heading_level": 1, "md_text": "MaxDuration(seconds=timedelta(seconds=2))\n```\n\n**Parameters:**\n\n- `seconds` (float | timedelta): Maximum allowed duration\n\n**Returns:** `bool` - `True` if `ctx.duration <= seconds`\n\n**Example:**\n\n```python\nfrom datetime import timedelta\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import MaxDuration\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[\n        # SLA: must respond in under 2 seconds\n        MaxDuration(seconds=2.0),\n        # Or using timedelta\n        MaxDuration(seconds=timedelta(milliseconds=500)),\n    ],\n)\n```\n\n**Use Cases:**\n\n- SLA compliance\n- Performance regression testing\n- Latency requirements\n- Timeout validation\n\n**See Also:** [Concurrency & Performance](../how-to/concurrency.md)\n\n---", "url": "https://ai.pydantic.dev/docs/evals/evaluators/built-in/#or", "page": "docs/evals/evaluators/built-in", "source_site": "pydantic_ai"}
{"title": "LLMJudge", "anchor": "llmjudge", "heading_level": 3, "md_text": "Use an LLM to evaluate subjective qualities based on a rubric.\n\n```python\nfrom pydantic_evals.evaluators import LLMJudge\n\nLLMJudge(\n    rubric='Response is accurate and helpful',\n    model='openai:gpt-5',\n    include_input=False,\n    include_expected_output=False,\n    model_settings=None,\n    score=False,\n    assertion={'include_reason': True},\n)\n```\n\n**Parameters:**\n\n- `rubric` (str): The evaluation criteria (required)\n- `model` (Model | KnownModelName | None): Model to use (default: `'openai:gpt-4o'`)\n- `include_input` (bool): Include task inputs in the prompt (default: `False`)\n- `include_expected_output` (bool): Include expected output in the prompt (default: `False`)\n- `model_settings` (ModelSettings | None): Custom model settings\n- `score` (OutputConfig | False): Configure score output (default: `False`)\n- `assertion` (OutputConfig | False): Configure assertion output (default: includes reason)\n\n**Returns:** Depends on `score` and `assertion` parameters (see below)\n\n**Output Modes:**\n\nBy default, returns a **boolean assertion** with reason:\n\n- `LLMJudge(rubric='Response is polite')`\n  - Returns: `{'LLMJudge_pass': EvaluationReason(value=True, reason='...')}`\n\nReturn a **score** (0.0 to 1.0) instead:\n\n- `LLMJudge(rubric='Response quality', score={'include_reason': True}, assertion=False)`\n  - Returns: `{'LLMJudge_score': EvaluationReason(value=0.85, reason='...')}`\n\nReturn **both** score and assertion:\n\n- `LLMJudge(rubric='Response quality', score={'include_reason': True}, assertion={'include_reason': True})`\n  - Returns: `{'LLMJudge_score': EvaluationReason(value=0.85, reason='...'), 'LLMJudge_pass': EvaluationReason(value=True, reason='...')}`\n\n**Customize evaluation names:**\n\n- `LLMJudge(rubric='Response is factually accurate', assertion={'evaluation_name': 'accuracy', 'include_reason': True})`\n  - Returns: `{'accuracy': EvaluationReason(value=True, reason='...')}`\n\n**Example:**\n\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\ndataset = Dataset(\n    cases=[Case(inputs='test', expected_output='result')],\n    evaluators=[\n        # Basic accuracy check\n        LLMJudge(\n            rubric='Response is factually accurate',\n            include_input=True,\n        ),\n        # Quality score with different model\n        LLMJudge(\n            rubric='Overall response quality',\n            model='anthropic:claude-sonnet-4-5',\n            score={'evaluation_name': 'quality', 'include_reason': False},\n            assertion=False,\n        ),\n        # Check against expected output\n        LLMJudge(\n            rubric='Response matches the expected answer semantically',\n            include_input=True,\n            include_expected_output=True,\n        ),\n    ],\n)\n```\n\n**See Also:** [LLM Judge Deep Dive](llm-judge.md)\n\n---", "url": "https://ai.pydantic.dev/docs/evals/evaluators/built-in/#llmjudge", "page": "docs/evals/evaluators/built-in", "source_site": "pydantic_ai"}
{"title": "HasMatchingSpan", "anchor": "hasmatchingspan", "heading_level": 3, "md_text": "Check if OpenTelemetry spans match a query (requires Logfire configuration).\n\n```python\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\nHasMatchingSpan(\n    query={'name_contains': 'tool_call'},\n    evaluation_name='called_tool',\n)\n```\n\n**Parameters:**\n\n- `query` ([`SpanQuery`][pydantic_evals.otel.SpanQuery]): Query to match against spans\n- `evaluation_name` (str | None): Custom name for this evaluation in reports\n\n**Returns:** `bool` - `True` if any span matches the query\n\n**Example:**\n\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[\n        # Check that a specific tool was called\n        HasMatchingSpan(\n            query={'name_contains': 'search_database'},\n            evaluation_name='used_database',\n        ),\n        # Check for errors\n        HasMatchingSpan(\n            query={'has_attributes': {'error': True}},\n            evaluation_name='had_errors',\n        ),\n        # Check duration constraints\n        HasMatchingSpan(\n            query={\n                'name_equals': 'llm_call',\n                'max_duration': 2.0,  # seconds\n            },\n            evaluation_name='llm_fast_enough',\n        ),\n    ],\n)\n```\n\n**See Also:** [Span-Based Evaluation](span-based.md)\n\n---", "url": "https://ai.pydantic.dev/docs/evals/evaluators/built-in/#hasmatchingspan", "page": "docs/evals/evaluators/built-in", "source_site": "pydantic_ai"}
{"title": "Quick Reference Table", "anchor": "quick-reference-table", "heading_level": 2, "md_text": "| Evaluator | Purpose | Return Type | Cost | Speed |\n|-----------|---------|-------------|------|-------|\n| [`EqualsExpected`][pydantic_evals.evaluators.EqualsExpected] | Exact match with expected | `bool` | Free | Instant |\n| [`Equals`][pydantic_evals.evaluators.Equals] | Equals specific value | `bool` | Free | Instant |\n| [`Contains`][pydantic_evals.evaluators.Contains] | Contains value/substring | `bool` + reason | Free | Instant |\n| [`IsInstance`][pydantic_evals.evaluators.IsInstance] | Type validation | `bool` + reason | Free | Instant |\n| [`MaxDuration`][pydantic_evals.evaluators.MaxDuration] | Performance threshold | `bool` | Free | Instant |\n| [`LLMJudge`][pydantic_evals.evaluators.LLMJudge] | Subjective quality | `bool` and/or `float` | $$ | Slow |\n| [`HasMatchingSpan`][pydantic_evals.evaluators.HasMatchingSpan] | Behavioral check | `bool` | Free | Fast |", "url": "https://ai.pydantic.dev/docs/evals/evaluators/built-in/#quick-reference-table", "page": "docs/evals/evaluators/built-in", "source_site": "pydantic_ai"}
{"title": "Combining Evaluators", "anchor": "combining-evaluators", "heading_level": 2, "md_text": "Best practice is to combine fast deterministic checks with slower LLM evaluations:\n\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import (\n    Contains,\n    IsInstance,\n    LLMJudge,\n    MaxDuration,\n)\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[\n        # Fast checks first (fail fast)\n        IsInstance(type_name='str'),\n        Contains(value='required_field'),\n        MaxDuration(seconds=2.0),\n        # Expensive LLM checks last\n        LLMJudge(rubric='Response is helpful and accurate'),\n    ],\n)\n```\n\nThis approach:\n\n1. Catches format/structure issues immediately\n2. Validates required content quickly\n3. Only runs expensive LLM evaluation if basic checks pass\n4. Provides comprehensive quality assessment", "url": "https://ai.pydantic.dev/docs/evals/evaluators/built-in/#combining-evaluators", "page": "docs/evals/evaluators/built-in", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "- **[LLM Judge](llm-judge.md)** - Deep dive on LLM-as-a-Judge evaluation\n- **[Custom Evaluators](custom.md)** - Write your own evaluation logic\n- **[Span-Based Evaluation](span-based.md)** - Using OpenTelemetry spans for behavioral checks", "url": "https://ai.pydantic.dev/docs/evals/evaluators/built-in/#next-steps", "page": "docs/evals/evaluators/built-in", "source_site": "pydantic_ai"}
{"title": "Custom Evaluators", "anchor": "custom-evaluators", "heading_level": 1, "md_text": "Write custom evaluators for domain-specific logic, external integrations, or specialized metrics.", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#custom-evaluators", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "Basic Custom Evaluator", "anchor": "basic-custom-evaluator", "heading_level": 2, "md_text": "All evaluators inherit from [`Evaluator`][pydantic_evals.evaluators.Evaluator] and must implement `evaluate`:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass ExactMatch(Evaluator):\n    \"\"\"Check if output exactly matches expected output.\"\"\"\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return ctx.output == ctx.expected_output\n```\n\n**Key Points:**\n\n- Use `@dataclass` decorator (required)\n- Inherit from `Evaluator`\n- Implement `evaluate(self, ctx: EvaluatorContext) -> EvaluatorOutput`\n- Return `bool`, `int`, `float`, `str`, [`EvaluationReason`][pydantic_evals.evaluators.EvaluationReason], or `dict` of these", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#basic-custom-evaluator", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "EvaluatorContext", "anchor": "evaluatorcontext", "heading_level": 2, "md_text": "The context provides all information about the case execution:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass MyEvaluator(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        # Access case data\n        ctx.name              # Case name\n        ctx.inputs            # Task inputs\n        ctx.metadata          # Case metadata\n        ctx.expected_output   # Expected output (may be None)\n        ctx.output            # Actual output\n\n        # Performance data\n        ctx.duration          # Task execution time (seconds)\n\n        # Custom metrics/attributes (see metrics guide)\n        ctx.metrics           # dict[str, int | float]\n        ctx.attributes        # dict[str, Any]\n\n        # OpenTelemetry spans (if logfire configured)\n        ctx.span_tree         # SpanTree for behavioral checks\n\n        return True\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#evaluatorcontext", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "Evaluator Parameters", "anchor": "evaluator-parameters", "heading_level": 2, "md_text": "Add configurable parameters as dataclass fields:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass ContainsKeyword(Evaluator):\n    keyword: str\n    case_sensitive: bool = True\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        output = ctx.output\n        keyword = self.keyword\n\n        if not self.case_sensitive:\n            output = output.lower()\n            keyword = keyword.lower()\n\n        return keyword in output", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#evaluator-parameters", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 1, "md_text": "dataset = Dataset(\n    cases=[Case(name='test', inputs='This is important')],\n    evaluators=[\n        ContainsKeyword(keyword='important', case_sensitive=False),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#usage", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "Boolean Assertions", "anchor": "boolean-assertions", "heading_level": 3, "md_text": "Simple pass/fail checks:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass IsValidJSON(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        try:\n            import json\n            json.loads(ctx.output)\n            return True\n        except Exception:\n            return False\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#boolean-assertions", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "Numeric Scores", "anchor": "numeric-scores", "heading_level": 3, "md_text": "Quality metrics:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass LengthScore(Evaluator):\n    \"\"\"Score based on output length (0.0 = too short, 1.0 = ideal).\"\"\"\n\n    ideal_length: int = 100\n    tolerance: int = 20\n\n    def evaluate(self, ctx: EvaluatorContext) -> float:\n        length = len(ctx.output)\n        diff = abs(length - self.ideal_length)\n\n        if diff <= self.tolerance:\n            return 1.0\n        else:\n            # Decay score as we move away from ideal\n            score = max(0.0, 1.0 - (diff - self.tolerance) / self.ideal_length)\n            return score\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#numeric-scores", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "String Labels", "anchor": "string-labels", "heading_level": 3, "md_text": "Categorical classifications:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass SentimentClassifier(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> str:\n        output_lower = ctx.output.lower()\n\n        if any(word in output_lower for word in ['error', 'failed', 'wrong']):\n            return 'negative'\n        elif any(word in output_lower for word in ['success', 'correct', 'great']):\n            return 'positive'\n        else:\n            return 'neutral'\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#string-labels", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "With Reasons", "anchor": "with-reasons", "heading_level": 3, "md_text": "Add explanations to any result:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import EvaluationReason, Evaluator, EvaluatorContext\n\n\n@dataclass\nclass SmartCheck(Evaluator):\n    threshold: float = 0.8\n\n    def evaluate(self, ctx: EvaluatorContext) -> EvaluationReason:\n        score = self._calculate_score(ctx.output)\n\n        if score >= self.threshold:\n            return EvaluationReason(\n                value=True,\n                reason=f'Score {score:.2f} exceeds threshold {self.threshold}',\n            )\n        else:\n            return EvaluationReason(\n                value=False,\n                reason=f'Score {score:.2f} below threshold {self.threshold}',\n            )\n\n    def _calculate_score(self, output: str) -> float:\n        # Your scoring logic\n        return 0.75\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#with-reasons", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "Multiple Results", "anchor": "multiple-results", "heading_level": 3, "md_text": "You can return multiple evaluations from one evaluator by returning a dictionary of key-value pairs.\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import (\n    EvaluationReason,\n    Evaluator,\n    EvaluatorContext,\n    EvaluatorOutput,\n)\n\n\n@dataclass\nclass ComprehensiveCheck(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> EvaluatorOutput:\n        format_valid = self._check_format(ctx.output)\n\n        return {\n            'valid_format': EvaluationReason(\n                value=format_valid,\n                reason='Valid JSON format' if format_valid else 'Invalid JSON format',\n            ),\n            'quality_score': self._score_quality(ctx.output),  # float\n            'category': self._classify(ctx.output),  # str\n        }\n\n    def _check_format(self, output: str) -> bool:\n        return output.startswith('{') and output.endswith('}')\n\n    def _score_quality(self, output: str) -> float:\n        return len(output) / 100.0\n\n    def _classify(self, output: str) -> str:\n        return 'short' if len(output) < 50 else 'long'\n```\n\nEach key in the returned dictionary becomes a separate result in the report. Values can be:\n\n- Primitives (`bool`, `int`, `float`, `str`)\n- [`EvaluationReason`][pydantic_evals.evaluators.EvaluationReason] (value with explanation)\n- Nested dicts of these types\n\nThe [`EvaluatorOutput`][pydantic_evals.evaluators.evaluator.EvaluatorOutput] type represents all legal values\nthat can be returned by an evaluator, and can be used as the return type annotation for your custom `evaluate` method.", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#multiple-results", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "Conditional Results", "anchor": "conditional-results", "heading_level": 3, "md_text": "Evaluators can dynamically choose whether to produce results for a given case by returning an empty dict when not applicable:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import (\n    EvaluationReason,\n    Evaluator,\n    EvaluatorContext,\n    EvaluatorOutput,\n)\n\n\n@dataclass\nclass SQLValidator(Evaluator):\n    \"\"\"Only evaluates SQL queries, skips other outputs.\"\"\"\n\n    def evaluate(self, ctx: EvaluatorContext) -> EvaluatorOutput:\n        # Check if this case is relevant for SQL validation\n        if not isinstance(ctx.output, str) or not ctx.output.strip().upper().startswith(\n            ('SELECT', 'INSERT', 'UPDATE', 'DELETE')\n        ):\n            # Return empty dict - this evaluator doesn't apply to this case\n            return {}\n\n        # This is a SQL query, perform validation\n        try:\n            # In real implementation, use sqlparse or similar\n            is_valid = self._validate_sql(ctx.output)\n            return {\n                'sql_valid': is_valid,\n                'sql_complexity': self._measure_complexity(ctx.output),\n            }\n        except Exception as e:\n            return {'sql_valid': EvaluationReason(False, reason=f'Exception: {e}')}\n\n    def _validate_sql(self, query: str) -> bool:\n        # Simplified validation\n        return 'FROM' in query.upper() or 'INTO' in query.upper()\n\n    def _measure_complexity(self, query: str) -> str:\n        joins = query.upper().count('JOIN')\n        if joins == 0:\n            return 'simple'\n        elif joins <= 2:\n            return 'moderate'\n        else:\n            return 'complex'\n```\n\nThis pattern is useful when:\n\n- An evaluator only applies to certain types of outputs (e.g., code validation only for code outputs)\n- Validation depends on metadata tags (e.g., only evaluate cases marked with `language='python'`)\n- You want to run expensive checks conditionally based on other evaluator results\n\n**Key Points:**\n\n- Returning `{}` means \"this evaluator doesn't apply here\" - the case won't show results from this evaluator\n- Returning `{'key': value}` means \"this evaluator applies and here are the results\"\n- This is more practical than using case-level evaluators when it applies to a large fraction of cases, or when the\n  condition is based on the output itself\n- The evaluator still runs for every case, but can short-circuit when not relevant", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#conditional-results", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "Async Evaluators", "anchor": "async-evaluators", "heading_level": 2, "md_text": "Use `async def` for I/O-bound operations:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass APIValidator(Evaluator):\n    api_url: str\n\n    async def evaluate(self, ctx: EvaluatorContext) -> bool:\n        import httpx\n\n        async with httpx.AsyncClient() as client:\n            response = await client.post(\n                self.api_url,\n                json={'output': ctx.output},\n            )\n            return response.json()['valid']\n```\n\nPydantic Evals handles both sync and async evaluators automatically.", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#async-evaluators", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "Using Metadata", "anchor": "using-metadata", "heading_level": 2, "md_text": "Access case metadata for context-aware evaluation:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass DifficultyAwareScore(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> float:\n        # Base score\n        base_score = self._score_output(ctx.output)\n\n        # Adjust based on difficulty from metadata\n        if ctx.metadata and 'difficulty' in ctx.metadata:\n            difficulty = ctx.metadata['difficulty']\n\n            if difficulty == 'easy':\n                # Penalize mistakes more on easy questions\n                return base_score\n            elif difficulty == 'hard':\n                # Be more lenient on hard questions\n                return min(1.0, base_score * 1.2)\n\n        return base_score\n\n    def _score_output(self, output: str) -> float:\n        # Your scoring logic\n        return 0.8\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#using-metadata", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "Using Metrics", "anchor": "using-metrics", "heading_level": 2, "md_text": "Access custom metrics set during task execution:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals import increment_eval_metric, set_eval_attribute\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#using-metrics", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "In your task", "anchor": "in-your-task", "heading_level": 1, "md_text": "def my_task(inputs: str) -> str:\n    result = f'processed: {inputs}'\n\n    # Record metrics\n    increment_eval_metric('api_calls', 3)\n    set_eval_attribute('used_cache', True)\n\n    return result", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#in-your-task", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "In your evaluator", "anchor": "in-your-evaluator", "heading_level": 1, "md_text": "@dataclass\nclass EfficiencyCheck(Evaluator):\n    max_api_calls: int = 5\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        api_calls = ctx.metrics.get('api_calls', 0)\n        return api_calls <= self.max_api_calls\n```\n\nSee [Metrics & Attributes Guide](../how-to/metrics-attributes.md) for more.", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#in-your-evaluator", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "Generic Type Parameters", "anchor": "generic-type-parameters", "heading_level": 2, "md_text": "Make evaluators type-safe with generics:\n\n```python\nfrom dataclasses import dataclass\nfrom typing import TypeVar\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\nInputsT = TypeVar('InputsT')\nOutputT = TypeVar('OutputT')\n\n\n@dataclass\nclass TypedEvaluator(Evaluator[InputsT, OutputT, dict]):\n    def evaluate(self, ctx: EvaluatorContext[InputsT, OutputT, dict]) -> bool:\n        # ctx.inputs and ctx.output are now properly typed\n        return True\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#generic-type-parameters", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "Custom Evaluation Names", "anchor": "custom-evaluation-names", "heading_level": 2, "md_text": "Control how evaluations appear in reports:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass CustomNameEvaluator(Evaluator):\n    check_type: str\n\n    def get_default_evaluation_name(self) -> str:\n        # Use check_type as the name instead of class name\n        return f'{self.check_type}_check'\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return True", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#custom-evaluation-names", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "In reports, appears as \"format_check\" instead of \"CustomNameEvaluator\"", "anchor": "in-reports-appears-as-format_check-instead-of-customnameevaluator", "heading_level": 1, "md_text": "evaluator = CustomNameEvaluator(check_type='format')\n```\n\nOr use the `evaluation_name` field (if using the built-in pattern):\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass MyEvaluator(Evaluator):\n    evaluation_name: str | None = None\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return True", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#in-reports-appears-as-format_check-instead-of-customnameevaluator", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 1, "md_text": "MyEvaluator(evaluation_name='my_custom_name')\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#usage", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "SQL Validation", "anchor": "sql-validation", "heading_level": 3, "md_text": "```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import EvaluationReason, Evaluator, EvaluatorContext\n\n\n@dataclass\nclass ValidSQL(Evaluator):\n    dialect: str = 'postgresql'\n\n    def evaluate(self, ctx: EvaluatorContext) -> EvaluationReason:\n        try:\n            import sqlparse\n            parsed = sqlparse.parse(ctx.output)\n\n            if not parsed:\n                return EvaluationReason(\n                    value=False,\n                    reason='Could not parse SQL',\n                )\n\n            # Check for dangerous operations\n            sql_upper = ctx.output.upper()\n            if 'DROP' in sql_upper or 'DELETE' in sql_upper:\n                return EvaluationReason(\n                    value=False,\n                    reason='Contains dangerous operations (DROP/DELETE)',\n                )\n\n            return EvaluationReason(\n                value=True,\n                reason='Valid SQL syntax',\n            )\n        except Exception as e:\n            return EvaluationReason(\n                value=False,\n                reason=f'SQL parsing error: {e}',\n            )\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#sql-validation", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "Code Execution", "anchor": "code-execution", "heading_level": 3, "md_text": "```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import EvaluationReason, Evaluator, EvaluatorContext\n\n\n@dataclass\nclass ExecutablePython(Evaluator):\n    timeout_seconds: float = 5.0\n\n    async def evaluate(self, ctx: EvaluatorContext) -> EvaluationReason:\n        import asyncio\n        import os\n        import tempfile\n\n        # Write code to temp file\n        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n            f.write(ctx.output)\n            temp_path = f.name\n\n        try:\n            # Execute with timeout\n            process = await asyncio.create_subprocess_exec(\n                'python', temp_path,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE,\n            )\n\n            try:\n                stdout, stderr = await asyncio.wait_for(\n                    process.communicate(),\n                    timeout=self.timeout_seconds,\n                )\n            except asyncio.TimeoutError:\n                process.kill()\n                return EvaluationReason(\n                    value=False,\n                    reason=f'Execution timeout after {self.timeout_seconds}s',\n                )\n\n            if process.returncode == 0:\n                return EvaluationReason(\n                    value=True,\n                    reason='Code executed successfully',\n                )\n            else:\n                return EvaluationReason(\n                    value=False,\n                    reason=f'Execution failed: {stderr.decode()}',\n                )\n        finally:\n            os.unlink(temp_path)\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#code-execution", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "External API Validation", "anchor": "external-api-validation", "heading_level": 3, "md_text": "```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass APIResponseValid(Evaluator):\n    api_endpoint: str\n    api_key: str\n\n    async def evaluate(self, ctx: EvaluatorContext) -> dict[str, bool | float]:\n        import httpx\n\n        try:\n            async with httpx.AsyncClient() as client:\n                response = await client.post(\n                    self.api_endpoint,\n                    headers={'Authorization': f'Bearer {self.api_key}'},\n                    json={'data': ctx.output},\n                    timeout=10.0,\n                )\n\n                result = response.json()\n\n                return {\n                    'api_reachable': True,\n                    'validation_passed': result.get('valid', False),\n                    'confidence_score': result.get('confidence', 0.0),\n                }\n        except Exception:\n            return {\n                'api_reachable': False,\n                'validation_passed': False,\n                'confidence_score': 0.0,\n            }\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#external-api-validation", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "Testing Evaluators", "anchor": "testing-evaluators", "heading_level": 2, "md_text": "Test evaluators like any other Python code:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass ExactMatch(Evaluator):\n    \"\"\"Check if output exactly matches expected output.\"\"\"\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return ctx.output == ctx.expected_output\n\n\ndef test_exact_match():\n    evaluator = ExactMatch()\n\n    # Test match\n    ctx = EvaluatorContext(\n        name='test',\n        inputs='input',\n        metadata=None,\n        expected_output='expected',\n        output='expected',\n        duration=0.1,\n        _span_tree=None,\n        attributes={},\n        metrics={},\n    )\n    assert evaluator.evaluate(ctx) is True\n\n    # Test mismatch\n    ctx.output = 'different'\n    assert evaluator.evaluate(ctx) is False\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#testing-evaluators", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "1. Keep Evaluators Focused", "anchor": "1-keep-evaluators-focused", "heading_level": 3, "md_text": "Each evaluator should check one thing:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\ndef check_format(output: str) -> bool:\n    return output.startswith('{')\n\n\ndef check_content(output: str) -> bool:\n    return len(output) > 10\n\n\ndef check_length(output: str) -> bool:\n    return len(output) < 1000\n\n\ndef check_spelling(output: str) -> bool:\n    return True  # Placeholder", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#1-keep-evaluators-focused", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "Bad: Doing too much", "anchor": "bad-doing-too-much", "heading_level": 1, "md_text": "@dataclass\nclass EverythingChecker(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> dict:\n        return {\n            'format_valid': check_format(ctx.output),\n            'content_good': check_content(ctx.output),\n            'length_ok': check_length(ctx.output),\n            'spelling_correct': check_spelling(ctx.output),\n        }", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#bad-doing-too-much", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "Good: Separate evaluators", "anchor": "good-separate-evaluators", "heading_level": 1, "md_text": "@dataclass\nclass FormatValidator(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return check_format(ctx.output)\n\n\n@dataclass\nclass ContentChecker(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return check_content(ctx.output)\n\n\n@dataclass\nclass LengthChecker(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return check_length(ctx.output)\n\n\n@dataclass\nclass SpellingChecker(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return check_spelling(ctx.output)\n```\n\nSome exceptions to this:\n\n* When there is a significant amount of shared computation or network request latency, it may be better to have a single evaluator calculate all dependent outputs together.\n* If multiple checks are tightly coupled or very closely related to each other, it may make sense to include all their logic in one evaluator.", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#good-separate-evaluators", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "2. Handle Missing Data Gracefully", "anchor": "2-handle-missing-data-gracefully", "heading_level": 3, "md_text": "```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import EvaluationReason, Evaluator, EvaluatorContext\n\n\n@dataclass\nclass SafeEvaluator(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> EvaluationReason:\n        if ctx.expected_output is None:\n            return EvaluationReason(\n                value=True,\n                reason='Skipped: no expected output provided',\n            )\n\n        # Your evaluation logic\n        ...\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#2-handle-missing-data-gracefully", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "3. Provide Helpful Reasons", "anchor": "3-provide-helpful-reasons", "heading_level": 3, "md_text": "```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import EvaluationReason, Evaluator, EvaluatorContext\n\n\n@dataclass\nclass HelpfulEvaluator(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> EvaluationReason:\n        # Bad\n        return EvaluationReason(value=False, reason='Failed')\n\n        # Good\n        return EvaluationReason(\n            value=False,\n            reason=f'Expected {ctx.expected_output!r}, got {ctx.output!r}',\n        )\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#3-provide-helpful-reasons", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "4. Use Timeouts for External Calls", "anchor": "4-use-timeouts-for-external-calls", "heading_level": 3, "md_text": "```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass APIEvaluator(Evaluator):\n    timeout: float = 10.0\n\n    async def _call_api(self, output: str) -> bool:\n        # Placeholder for API call\n        return True\n\n    async def evaluate(self, ctx: EvaluatorContext) -> bool:\n        import asyncio\n\n        try:\n            return await asyncio.wait_for(\n                self._call_api(ctx.output),\n                timeout=self.timeout,\n            )\n        except asyncio.TimeoutError:\n            return False\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#4-use-timeouts-for-external-calls", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "- **[Span-Based Evaluation](span-based.md)** - Using OpenTelemetry spans\n- **[Examples](../examples/simple-validation.md)** - Practical examples", "url": "https://ai.pydantic.dev/docs/evals/evaluators/custom/#next-steps", "page": "docs/evals/evaluators/custom", "source_site": "pydantic_ai"}
{"title": "Span-Based Evaluation", "anchor": "span-based-evaluation", "heading_level": 1, "md_text": "Evaluate AI system behavior by analyzing OpenTelemetry spans captured during execution.\n\n!!! note \"Requires Logfire\"\n    Span-based evaluation requires `logfire` to be installed and configured:\n    ```bash\n    pip install 'pydantic-evals[logfire]'\n    ```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#span-based-evaluation", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "Span-based evaluation enables you to evaluate **how** your AI system executes, not just **what** it produces. This is essential for complex agents where ensuring the desired behavior depends on the execution path taken, not just the final output.", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#overview", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Why Span-Based Evaluation?", "anchor": "why-span-based-evaluation", "heading_level": 3, "md_text": "Traditional evaluators assess task inputs and outputs. For simple tasks, this may be sufficient\u2014if the output is correct, the task succeeded. But for complex multi-step agents, the _process_ matters as much as the result:\n\n- **A correct answer reached incorrectly** - An agent might produce the right output by accident (e.g., guessing, using cached data when it should have searched, calling the wrong tools but getting lucky)\n- **Verification of required behaviors** - You need to ensure specific tools were called, certain code paths executed, or particular patterns followed\n- **Performance and efficiency** - The agent should reach the answer efficiently, without unnecessary tool calls, infinite loops, or excessive retries\n- **Safety and compliance** - Critical to verify that dangerous operations weren't attempted, sensitive data wasn't accessed inappropriately, or guardrails weren't bypassed", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#why-span-based-evaluation", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Real-World Scenarios", "anchor": "real-world-scenarios", "heading_level": 3, "md_text": "Span-based evaluation is particularly valuable for:\n\n- **RAG systems** - Verify documents were retrieved and reranked before generation, not just that the answer included citations\n- **Multi-agent coordination** - Ensure the orchestrator delegated to the right specialist agents in the correct order\n- **Tool-calling agents** - Confirm specific tools were used (or avoided), and in the expected sequence\n- **Debugging and regression testing** - Catch behavioral regressions where outputs remain correct but the internal logic deteriorates\n- **Production alignment** - Ensure your evaluation assertions operate on the same telemetry data captured in production, so eval insights directly translate to production monitoring", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#real-world-scenarios", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "How It Works", "anchor": "how-it-works", "heading_level": 3, "md_text": "When you configure logfire (`logfire.configure()`), Pydantic Evals captures all OpenTelemetry spans generated during task execution. You can then write evaluators that assert conditions on:\n\n- **Which tools were called** - `HasMatchingSpan(query={'name_contains': 'search_tool'})`\n- **Code paths executed** - Verify specific functions ran or particular branches taken\n- **Timing characteristics** - Check that operations complete within SLA bounds\n- **Error conditions** - Detect retries, fallbacks, or specific failure modes\n- **Execution structure** - Verify parent-child relationships, delegation patterns, or execution order\n\nThis creates a fundamentally different evaluation paradigm: you're testing behavioral contracts, not just input-output relationships.", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#how-it-works", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Basic Usage", "anchor": "basic-usage", "heading_level": 2, "md_text": "```python\nimport logfire\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import HasMatchingSpan", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#basic-usage", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Configure logfire to capture spans", "anchor": "configure-logfire-to-capture-spans", "heading_level": 1, "md_text": "logfire.configure(send_to_logfire='if-token-present')\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[\n        # Check that database was queried\n        HasMatchingSpan(\n            query={'name_contains': 'database_query'},\n            evaluation_name='used_database',\n        ),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#configure-logfire-to-capture-spans", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "HasMatchingSpan Evaluator", "anchor": "hasmatchingspan-evaluator", "heading_level": 2, "md_text": "The [`HasMatchingSpan`][pydantic_evals.evaluators.HasMatchingSpan] evaluator checks if any span matches a query:\n\n```python\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\nHasMatchingSpan(\n    query={'name_contains': 'test'},\n    evaluation_name='span_check',\n)\n```\n\n**Returns:** `bool` - `True` if any span matches the query", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#hasmatchingspan-evaluator", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "SpanQuery Reference", "anchor": "spanquery-reference", "heading_level": 2, "md_text": "A [`SpanQuery`][pydantic_evals.otel.SpanQuery] is a dictionary with query conditions:", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#spanquery-reference", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Name Conditions", "anchor": "name-conditions", "heading_level": 3, "md_text": "Match spans by name:\n\n```python", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#name-conditions", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Exact name match", "anchor": "exact-name-match", "heading_level": 1, "md_text": "{'name_equals': 'search_database'}", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#exact-name-match", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Contains substring", "anchor": "contains-substring", "heading_level": 1, "md_text": "{'name_contains': 'tool_call'}", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#contains-substring", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Regex pattern", "anchor": "regex-pattern", "heading_level": 1, "md_text": "{'name_matches_regex': r'llm_call_\\d+'}\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#regex-pattern", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Attribute Conditions", "anchor": "attribute-conditions", "heading_level": 3, "md_text": "Match spans with specific attributes:\n\n```python", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#attribute-conditions", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Has specific attribute values", "anchor": "has-specific-attribute-values", "heading_level": 1, "md_text": "{'has_attributes': {'operation': 'search', 'status': 'success'}}", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#has-specific-attribute-values", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Has attribute keys (any value)", "anchor": "has-attribute-keys-any-value", "heading_level": 1, "md_text": "{'has_attribute_keys': ['user_id', 'request_id']}\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#has-attribute-keys-any-value", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Duration Conditions", "anchor": "duration-conditions", "heading_level": 3, "md_text": "Match based on execution time:\n\n```python\nfrom datetime import timedelta", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#duration-conditions", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Minimum duration", "anchor": "minimum-duration", "heading_level": 1, "md_text": "{'min_duration': 1.0}  # seconds\n{'min_duration': timedelta(seconds=1)}", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#minimum-duration", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Maximum duration", "anchor": "maximum-duration", "heading_level": 1, "md_text": "{'max_duration': 5.0}  # seconds\n{'max_duration': timedelta(seconds=5)}", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#maximum-duration", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Range", "anchor": "range", "heading_level": 1, "md_text": "{'min_duration': 0.5, 'max_duration': 2.0}\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#range", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Logical Operators", "anchor": "logical-operators", "heading_level": 3, "md_text": "Combine conditions:\n\n```python", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#logical-operators", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "NOT", "anchor": "not", "heading_level": 1, "md_text": "{'not_': {'name_contains': 'error'}}", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#not", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "AND (all must match)", "anchor": "and-all-must-match", "heading_level": 1, "md_text": "{'and_': [\n    {'name_contains': 'tool'},\n    {'max_duration': 1.0},\n]}", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#and-all-must-match", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "OR (any must match)", "anchor": "or-any-must-match", "heading_level": 1, "md_text": "{'or_': [\n    {'name_equals': 'search'},\n    {'name_equals': 'query'},\n]}\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#or-any-must-match", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Child/Descendant Conditions", "anchor": "childdescendant-conditions", "heading_level": 3, "md_text": "Query relationships between spans:\n\n```python", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#childdescendant-conditions", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Count direct children", "anchor": "count-direct-children", "heading_level": 1, "md_text": "{'min_child_count': 1}\n{'max_child_count': 5}", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#count-direct-children", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Some child matches query", "anchor": "some-child-matches-query", "heading_level": 1, "md_text": "{'some_child_has': {'name_contains': 'retry'}}", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#some-child-matches-query", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "All children match query", "anchor": "all-children-match-query", "heading_level": 1, "md_text": "{'all_children_have': {'max_duration': 0.5}}", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#all-children-match-query", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "No children match query", "anchor": "no-children-match-query", "heading_level": 1, "md_text": "{'no_child_has': {'has_attributes': {'error': True}}}", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#no-children-match-query", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Descendant queries (recursive)", "anchor": "descendant-queries-recursive", "heading_level": 1, "md_text": "{'min_descendant_count': 5}\n{'some_descendant_has': {'name_contains': 'api_call'}}\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#descendant-queries-recursive", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Ancestor/Depth Conditions", "anchor": "ancestordepth-conditions", "heading_level": 3, "md_text": "Query span hierarchy:\n\n```python", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#ancestordepth-conditions", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Depth (root spans have depth 0)", "anchor": "depth-root-spans-have-depth-0", "heading_level": 1, "md_text": "{'min_depth': 1}  # Not a root span\n{'max_depth': 2}  # At most 2 levels deep", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#depth-root-spans-have-depth-0", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Ancestor queries", "anchor": "ancestor-queries", "heading_level": 1, "md_text": "{'some_ancestor_has': {'name_equals': 'agent_run'}}\n{'all_ancestors_have': {'max_duration': 10.0}}\n{'no_ancestor_has': {'has_attributes': {'error': True}}}\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#ancestor-queries", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Stop Recursing", "anchor": "stop-recursing", "heading_level": 3, "md_text": "Control recursive queries:\n\n```python\n{\n    'some_descendant_has': {'name_contains': 'expensive'},\n    'stop_recursing_when': {'name_equals': 'boundary'},\n}", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#stop-recursing", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Only search descendants until hitting a span named 'boundary'", "anchor": "only-search-descendants-until-hitting-a-span-named-boundary", "heading_level": 1, "md_text": "```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#only-search-descendants-until-hitting-a-span-named-boundary", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Verify Tool Usage", "anchor": "verify-tool-usage", "heading_level": 3, "md_text": "Check that specific tools were called:\n\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[\n        # Must call search tool\n        HasMatchingSpan(\n            query={'name_contains': 'search_tool'},\n            evaluation_name='used_search',\n        ),\n\n        # Must NOT call dangerous tool\n        HasMatchingSpan(\n            query={'not_': {'name_contains': 'delete_database'}},\n            evaluation_name='safe_execution',\n        ),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#verify-tool-usage", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Check Multiple Tools", "anchor": "check-multiple-tools", "heading_level": 3, "md_text": "Verify a sequence of operations:\n\n```python\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\nevaluators = [\n    HasMatchingSpan(\n        query={'name_contains': 'retrieve_context'},\n        evaluation_name='retrieved_context',\n    ),\n    HasMatchingSpan(\n        query={'name_contains': 'generate_response'},\n        evaluation_name='generated_response',\n    ),\n    HasMatchingSpan(\n        query={'and_': [\n            {'name_contains': 'cite'},\n            {'has_attribute_keys': ['source_id']},\n        ]},\n        evaluation_name='added_citations',\n    ),\n]\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#check-multiple-tools", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Performance Assertions", "anchor": "performance-assertions", "heading_level": 3, "md_text": "Ensure operations meet latency requirements:\n\n```python\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\nevaluators = [\n    # Database queries should be fast\n    HasMatchingSpan(\n        query={'and_': [\n            {'name_contains': 'database'},\n            {'max_duration': 0.1},  # 100ms max\n        ]},\n        evaluation_name='fast_db_queries',\n    ),\n\n    # Overall should complete quickly\n    HasMatchingSpan(\n        query={'and_': [\n            {'name_equals': 'task_execution'},\n            {'max_duration': 2.0},\n        ]},\n        evaluation_name='within_sla',\n    ),\n]\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#performance-assertions", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Error Detection", "anchor": "error-detection", "heading_level": 3, "md_text": "Check for error conditions:\n\n```python\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\nevaluators = [\n    # No errors occurred\n    HasMatchingSpan(\n        query={'not_': {'has_attributes': {'error': True}}},\n        evaluation_name='no_errors',\n    ),\n\n    # Retries happened\n    HasMatchingSpan(\n        query={'name_contains': 'retry'},\n        evaluation_name='had_retries',\n    ),\n\n    # Fallback was used\n    HasMatchingSpan(\n        query={'name_contains': 'fallback_model'},\n        evaluation_name='used_fallback',\n    ),\n]\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#error-detection", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Complex Behavioral Checks", "anchor": "complex-behavioral-checks", "heading_level": 3, "md_text": "Verify sophisticated behavior patterns:\n\n```python\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\nevaluators = [\n    # Agent delegated to sub-agent\n    HasMatchingSpan(\n        query={'and_': [\n            {'name_contains': 'agent'},\n            {'some_child_has': {'name_contains': 'delegate'}},\n        ]},\n        evaluation_name='used_delegation',\n    ),\n\n    # Made multiple LLM calls with retries\n    HasMatchingSpan(\n        query={'and_': [\n            {'name_contains': 'llm_call'},\n            {'some_descendant_has': {'name_contains': 'retry'}},\n            {'min_descendant_count': 3},\n        ]},\n        evaluation_name='retry_pattern',\n    ),\n]\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#complex-behavioral-checks", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Custom Evaluators with SpanTree", "anchor": "custom-evaluators-with-spantree", "heading_level": 2, "md_text": "For more complex span analysis, write custom evaluators:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass CustomSpanCheck(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> dict[str, bool | int]:\n        span_tree = ctx.span_tree\n\n        # Find specific spans\n        llm_spans = span_tree.find(lambda node: 'llm' in node.name)\n        tool_spans = span_tree.find(lambda node: 'tool' in node.name)\n\n        # Calculate metrics\n        total_llm_time = sum(\n            span.duration.total_seconds() for span in llm_spans\n        )\n\n        return {\n            'used_llm': len(llm_spans) > 0,\n            'used_tools': len(tool_spans) > 0,\n            'tool_count': len(tool_spans),\n            'llm_fast': total_llm_time < 2.0,\n        }\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#custom-evaluators-with-spantree", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "SpanTree API", "anchor": "spantree-api", "heading_level": 3, "md_text": "The [`SpanTree`][pydantic_evals.otel.SpanTree] provides methods for span analysis:\n\n```python\nfrom pydantic_evals.otel import SpanTree", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#spantree-api", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Example API (requires span_tree from context)", "anchor": "example-api-requires-span_tree-from-context", "heading_level": 1, "md_text": "def example_api(span_tree: SpanTree) -> None:\n    span_tree.find(lambda n: True)  # Find all matching nodes\n    span_tree.any({'name_contains': 'test'})  # Check if any span matches\n    span_tree.all({'name_contains': 'test'})  # Check if all spans match\n    span_tree.count({'name_contains': 'test'})  # Count matching spans\n\n    # Iteration\n    for node in span_tree:\n        print(node.name, node.duration, node.attributes)\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#example-api-requires-span_tree-from-context", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "SpanNode Properties", "anchor": "spannode-properties", "heading_level": 3, "md_text": "Each [`SpanNode`][pydantic_evals.otel.SpanNode] has:\n\n```python\nfrom pydantic_evals.otel import SpanNode", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#spannode-properties", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Example properties (requires node from context)", "anchor": "example-properties-requires-node-from-context", "heading_level": 1, "md_text": "def example_properties(node: SpanNode) -> None:\n    _ = node.name  # Span name\n    _ = node.duration  # timedelta\n    _ = node.attributes  # dict[str, AttributeValue]\n    _ = node.start_timestamp  # datetime\n    _ = node.end_timestamp  # datetime\n    _ = node.children  # list[SpanNode]\n    _ = node.descendants  # list[SpanNode] (recursive)\n    _ = node.ancestors  # list[SpanNode]\n    _ = node.parent  # SpanNode | None\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#example-properties-requires-node-from-context", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "View Spans in Logfire", "anchor": "view-spans-in-logfire", "heading_level": 3, "md_text": "If you're sending data to Logfire, you can view all spans in the web UI to understand the trace structure.", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#view-spans-in-logfire", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Print Span Tree", "anchor": "print-span-tree", "heading_level": 3, "md_text": "```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass DebugSpans(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        for node in ctx.span_tree:\n            print(f\"{'  ' * len(node.ancestors)}{node.name} ({node.duration})\")\n        return True\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#print-span-tree", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Query Testing", "anchor": "query-testing", "heading_level": 3, "md_text": "Test queries incrementally:\n\n```python\nfrom pydantic_evals.evaluators import HasMatchingSpan", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#query-testing", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Start simple", "anchor": "start-simple", "heading_level": 1, "md_text": "query = {'name_contains': 'tool'}", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#start-simple", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Add conditions gradually", "anchor": "add-conditions-gradually", "heading_level": 1, "md_text": "query = {'and_': [\n    {'name_contains': 'tool'},\n    {'max_duration': 1.0},\n]}", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#add-conditions-gradually", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Test in evaluator", "anchor": "test-in-evaluator", "heading_level": 1, "md_text": "HasMatchingSpan(query=query, evaluation_name='test')\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#test-in-evaluator", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "RAG System Verification", "anchor": "rag-system-verification", "heading_level": 3, "md_text": "Verify retrieval-augmented generation workflow:\n\n```python\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\nevaluators = [\n    # Retrieved documents\n    HasMatchingSpan(\n        query={'name_contains': 'vector_search'},\n        evaluation_name='retrieved_docs',\n    ),\n\n    # Reranked results\n    HasMatchingSpan(\n        query={'name_contains': 'rerank'},\n        evaluation_name='reranked_results',\n    ),\n\n    # Generated with context\n    HasMatchingSpan(\n        query={'and_': [\n            {'name_contains': 'generate'},\n            {'has_attribute_keys': ['context_ids']},\n        ]},\n        evaluation_name='used_context',\n    ),\n]\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#rag-system-verification", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Multi-Agent Systems", "anchor": "multi-agent-systems", "heading_level": 3, "md_text": "Verify agent coordination:\n\n```python\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\nevaluators = [\n    # Master agent ran\n    HasMatchingSpan(\n        query={'name_equals': 'master_agent'},\n        evaluation_name='master_ran',\n    ),\n\n    # Delegated to specialist\n    HasMatchingSpan(\n        query={'and_': [\n            {'name_contains': 'specialist_agent'},\n            {'some_ancestor_has': {'name_equals': 'master_agent'}},\n        ]},\n        evaluation_name='delegated_correctly',\n    ),\n\n    # No circular delegation\n    HasMatchingSpan(\n        query={'not_': {'and_': [\n            {'name_contains': 'agent'},\n            {'some_descendant_has': {'name_contains': 'agent'}},\n            {'some_ancestor_has': {'name_contains': 'agent'}},\n        ]}},\n        evaluation_name='no_circular_delegation',\n    ),\n]\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#multi-agent-systems", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Tool Usage Patterns", "anchor": "tool-usage-patterns", "heading_level": 3, "md_text": "Verify intelligent tool selection:\n\n```python\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\nevaluators = [\n    # Used search before answering\n    HasMatchingSpan(\n        query={'and_': [\n            {'name_contains': 'search'},\n            {'some_ancestor_has': {'name_contains': 'answer'}},\n        ]},\n        evaluation_name='searched_before_answering',\n    ),\n\n    # Limited tool calls (no loops)\n    HasMatchingSpan(\n        query={'and_': [\n            {'name_contains': 'tool'},\n            {'max_child_count': 5},\n        ]},\n        evaluation_name='reasonable_tool_usage',\n    ),\n]\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#tool-usage-patterns", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Best Practices", "anchor": "best-practices", "heading_level": 2, "md_text": "1. **Start Simple**: Begin with basic name queries, add complexity as needed\n2. **Use Descriptive Names**: Name your spans well in your application code\n3. **Test Queries**: Verify queries work before running full evaluations\n4. **Combine with Other Evaluators**: Use span checks alongside output validation\n5. **Document Expectations**: Comment why specific spans should/shouldn't exist", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#best-practices", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "- **[Logfire Integration](../how-to/logfire-integration.md)** - Set up Logfire for span capture\n- **[Custom Evaluators](custom.md)** - Write advanced span analysis\n- **[Built-in Evaluators](built-in.md)** - Other evaluator types", "url": "https://ai.pydantic.dev/docs/evals/evaluators/span-based/#next-steps", "page": "docs/evals/evaluators/span-based", "source_site": "pydantic_ai"}
{"title": "LLM Judge Deep Dive", "anchor": "llm-judge-deep-dive", "heading_level": 1, "md_text": "The [`LLMJudge`][pydantic_evals.evaluators.LLMJudge] evaluator uses an LLM to assess subjective qualities of outputs based on a rubric.", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#llm-judge-deep-dive", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "When to Use LLM-as-a-Judge", "anchor": "when-to-use-llm-as-a-judge", "heading_level": 2, "md_text": "LLM judges are ideal for evaluating qualities that require understanding and judgment:\n\n**Good Use Cases:**\n\n- Factual accuracy\n- Helpfulness and relevance\n- Tone and style compliance\n- Completeness of responses\n- Following complex instructions\n- RAG groundedness (does the answer use provided context?)\n- Citation accuracy\n\n**Poor Use Cases:**\n\n- Format validation (use [`IsInstance`][pydantic_evals.evaluators.IsInstance] instead)\n- Exact matching (use [`EqualsExpected`][pydantic_evals.evaluators.EqualsExpected])\n- Performance checks (use [`MaxDuration`][pydantic_evals.evaluators.MaxDuration])\n- Deterministic logic (write a custom evaluator)", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#when-to-use-llm-as-a-judge", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Basic Usage", "anchor": "basic-usage", "heading_level": 2, "md_text": "```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[\n        LLMJudge(rubric='Response is factually accurate'),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#basic-usage", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Rubric", "anchor": "rubric", "heading_level": 3, "md_text": "The `rubric` is your evaluation criteria. Be specific and clear:\n\n**Bad rubrics (vague):**\n```python\nfrom pydantic_evals.evaluators import LLMJudge\n\nLLMJudge(rubric='Good response')  # Too vague\nLLMJudge(rubric='Check quality')  # What aspect of quality?\n```\n\n**Good rubrics (specific):**\n```python\nfrom pydantic_evals.evaluators import LLMJudge\n\nLLMJudge(rubric='Response directly answers the user question without hallucination')\nLLMJudge(rubric='Response uses formal, professional language appropriate for business communication')\nLLMJudge(rubric='All factual claims in the response are supported by the provided context')\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#rubric", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Including Context", "anchor": "including-context", "heading_level": 3, "md_text": "Control what information the judge sees:\n\n```python\nfrom pydantic_evals.evaluators import LLMJudge", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#including-context", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Output only (default)", "anchor": "output-only-default", "heading_level": 1, "md_text": "LLMJudge(rubric='Response is polite')", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#output-only-default", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Output + Input", "anchor": "output-input", "heading_level": 1, "md_text": "LLMJudge(\n    rubric='Response accurately answers the input question',\n    include_input=True,\n)", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#output-input", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Output + Input + Expected Output", "anchor": "output-input-expected-output", "heading_level": 1, "md_text": "LLMJudge(\n    rubric='Response is semantically equivalent to the expected output',\n    include_input=True,\n    include_expected_output=True,\n)\n```\n\n**Example:**\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\ndataset = Dataset(\n    cases=[\n        Case(\n            inputs='What is 2+2?',\n            expected_output='4',\n        ),\n    ],\n    evaluators=[\n        # This judge sees: output + inputs + expected_output\n        LLMJudge(\n            rubric='Response provides the same answer as expected, possibly with explanation',\n            include_input=True,\n            include_expected_output=True,\n        ),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#output-input-expected-output", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Model Selection", "anchor": "model-selection", "heading_level": 3, "md_text": "Choose the judge model based on cost/quality tradeoffs:\n\n```python\nfrom pydantic_evals.evaluators import LLMJudge", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#model-selection", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Default: GPT-4o (good balance)", "anchor": "default-gpt-4o-good-balance", "heading_level": 1, "md_text": "LLMJudge(rubric='...')", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#default-gpt-4o-good-balance", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Anthropic Claude (alternative default)", "anchor": "anthropic-claude-alternative-default", "heading_level": 1, "md_text": "LLMJudge(\n    rubric='...',\n    model='anthropic:claude-sonnet-4-5',\n)", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#anthropic-claude-alternative-default", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Cheaper option for simple checks", "anchor": "cheaper-option-for-simple-checks", "heading_level": 1, "md_text": "LLMJudge(\n    rubric='Response contains profanity',\n    model='openai:gpt-5-mini',\n)", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#cheaper-option-for-simple-checks", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Premium option for nuanced evaluation", "anchor": "premium-option-for-nuanced-evaluation", "heading_level": 1, "md_text": "LLMJudge(\n    rubric='Response demonstrates deep understanding of quantum mechanics',\n    model='anthropic:claude-opus-4-20250514',\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#premium-option-for-nuanced-evaluation", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Model Settings", "anchor": "model-settings", "heading_level": 3, "md_text": "Customize model behavior:\n\n```python\nfrom pydantic_ai.settings import ModelSettings\nfrom pydantic_evals.evaluators import LLMJudge\n\nLLMJudge(\n    rubric='...',\n    model_settings=ModelSettings(\n        temperature=0.0,  # Deterministic evaluation\n        max_tokens=100,  # Shorter responses\n    ),\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#model-settings", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Assertion Only (Default)", "anchor": "assertion-only-default", "heading_level": 3, "md_text": "Returns pass/fail with reason:\n\n```python\nfrom pydantic_evals.evaluators import LLMJudge\n\nLLMJudge(rubric='Response is accurate')", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#assertion-only-default", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Returns: {'LLMJudge_pass': EvaluationReason(value=True, reason='...')}", "anchor": "returns-llmjudge_pass-evaluationreasonvaluetrue-reason", "heading_level": 1, "md_text": "```\n\nIn reports:\n```\n\u2503 Assertions \u2503\n\u2503 \u2714          \u2503\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#returns-llmjudge_pass-evaluationreasonvaluetrue-reason", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Score Only", "anchor": "score-only", "heading_level": 3, "md_text": "Returns a numeric score (0.0 to 1.0):\n\n```python\nfrom pydantic_evals.evaluators import LLMJudge\n\nLLMJudge(\n    rubric='Response quality',\n    score={'include_reason': True},\n    assertion=False,\n)", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#score-only", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Returns: {'LLMJudge_score': EvaluationReason(value=0.85, reason='...')}", "anchor": "returns-llmjudge_score-evaluationreasonvalue085-reason", "heading_level": 1, "md_text": "```\n\nIn reports:\n```\n\u2503 Scores             \u2503\n\u2503 LLMJudge_score: 0.85 \u2503\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#returns-llmjudge_score-evaluationreasonvalue085-reason", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Both Score and Assertion", "anchor": "both-score-and-assertion", "heading_level": 3, "md_text": "```python\nfrom pydantic_evals.evaluators import LLMJudge\n\nLLMJudge(\n    rubric='Response quality',\n    score={'include_reason': True},\n    assertion={'include_reason': True},\n)", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#both-score-and-assertion", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "}", "anchor": "", "heading_level": 1, "md_text": "```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Custom Names", "anchor": "custom-names", "heading_level": 3, "md_text": "```python\nfrom pydantic_evals.evaluators import LLMJudge\n\nLLMJudge(\n    rubric='Response is factually accurate',\n    assertion={\n        'evaluation_name': 'accuracy',\n        'include_reason': True,\n    },\n)", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#custom-names", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Returns: {'accuracy': EvaluationReason(value=True, reason='...')}", "anchor": "returns-accuracy-evaluationreasonvaluetrue-reason", "heading_level": 1, "md_text": "```\n\nIn reports:\n```\n\u2503 Assertions \u2503\n\u2503 accuracy: \u2714 \u2503\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#returns-accuracy-evaluationreasonvaluetrue-reason", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "RAG Evaluation", "anchor": "rag-evaluation", "heading_level": 3, "md_text": "Evaluate whether a RAG system uses provided context:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\n\n@dataclass\nclass RAGInput:\n    question: str\n    context: str\n\n\ndataset = Dataset(\n    cases=[\n        Case(\n            inputs=RAGInput(\n                question='What is the capital of France?',\n                context='France is a country in Europe. Its capital is Paris.',\n            ),\n        ),\n    ],\n    evaluators=[\n        LLMJudge(\n            rubric='Response answers the question using only information from the provided context',\n            include_input=True,\n            assertion={'evaluation_name': 'grounded', 'include_reason': True},\n        ),\n        LLMJudge(\n            rubric='Response cites specific quotes or facts from the context',\n            include_input=True,\n            assertion={'evaluation_name': 'uses_citations', 'include_reason': True},\n        ),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#rag-evaluation", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Recipe Generation with Case-Specific Rubrics", "anchor": "recipe-generation-with-case-specific-rubrics", "heading_level": 3, "md_text": "This example shows how to use both dataset-level and case-specific evaluators:\n\n```python {title=\"recipe_evaluation.py\" test=\"skip\"}\nfrom __future__ import annotations\n\nfrom typing import Any\n\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, format_as_xml\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import IsInstance, LLMJudge\n\n\nclass CustomerOrder(BaseModel):\n    dish_name: str\n    dietary_restriction: str | None = None\n\n\nclass Recipe(BaseModel):\n    ingredients: list[str]\n    steps: list[str]\n\n\nrecipe_agent = Agent(\n    'openai:gpt-5-mini',\n    output_type=Recipe,\n    system_prompt=(\n        'Generate a recipe to cook the dish that meets the dietary restrictions.'\n    ),\n)\n\n\nasync def transform_recipe(customer_order: CustomerOrder) -> Recipe:\n    r = await recipe_agent.run(format_as_xml(customer_order))\n    return r.output\n\n\nrecipe_dataset = Dataset[CustomerOrder, Recipe, Any](\n    cases=[\n        Case(\n            name='vegetarian_recipe',\n            inputs=CustomerOrder(\n                dish_name='Spaghetti Bolognese', dietary_restriction='vegetarian'\n            ),\n            expected_output=None,\n            metadata={'focus': 'vegetarian'},\n            evaluators=(  # (1)!\n                LLMJudge(\n                    rubric='Recipe should not contain meat or animal products',\n                ),\n            ),\n        ),\n        Case(\n            name='gluten_free_recipe',\n            inputs=CustomerOrder(\n                dish_name='Chocolate Cake', dietary_restriction='gluten-free'\n            ),\n            expected_output=None,\n            metadata={'focus': 'gluten-free'},\n            evaluators=(  # (2)!\n                LLMJudge(\n                    rubric='Recipe should not contain gluten or wheat products',\n                ),\n            ),\n        ),\n    ],\n    evaluators=[  # (3)!\n        IsInstance(type_name='Recipe'),\n        LLMJudge(\n            rubric='Recipe should have clear steps and relevant ingredients',\n            include_input=True,\n            model='anthropic:claude-sonnet-4-5',\n        ),\n    ],\n)\n\n\nreport = recipe_dataset.evaluate_sync(transform_recipe)\nprint(report)\n\"\"\"\n     Evaluation Summary: transform_recipe\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Case ID            \u2503 Assertions \u2503 Duration \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 vegetarian_recipe  \u2502 \u2714\u2714\u2714        \u2502    38.1s \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 gluten_free_recipe \u2502 \u2714\u2714\u2714        \u2502    22.4s \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Averages           \u2502 100.0% \u2714   \u2502    30.3s \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\"\"\"\n```\n\n1. Case-specific evaluator - only runs for the vegetarian recipe case\n2. Case-specific evaluator - only runs for the gluten-free recipe case\n3. Dataset-level evaluators - run for all cases", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#recipe-generation-with-case-specific-rubrics", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Multi-Aspect Evaluation", "anchor": "multi-aspect-evaluation", "heading_level": 3, "md_text": "Use multiple judges for different quality dimensions:\n\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[\n        # Accuracy\n        LLMJudge(\n            rubric='Response is factually accurate',\n            include_input=True,\n            assertion={'evaluation_name': 'accurate'},\n        ),\n\n        # Helpfulness\n        LLMJudge(\n            rubric='Response is helpful and actionable',\n            include_input=True,\n            score={'evaluation_name': 'helpfulness'},\n            assertion=False,\n        ),\n\n        # Tone\n        LLMJudge(\n            rubric='Response uses professional, respectful language',\n            assertion={'evaluation_name': 'professional_tone'},\n        ),\n\n        # Safety\n        LLMJudge(\n            rubric='Response contains no harmful, biased, or inappropriate content',\n            assertion={'evaluation_name': 'safe'},\n        ),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#multi-aspect-evaluation", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Comparative Evaluation", "anchor": "comparative-evaluation", "heading_level": 3, "md_text": "Compare output against expected output:\n\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\ndataset = Dataset(\n    cases=[\n        Case(\n            name='translation',\n            inputs='Hello world',\n            expected_output='Bonjour le monde',\n        ),\n    ],\n    evaluators=[\n        LLMJudge(\n            rubric='Response is semantically equivalent to the expected output',\n            include_input=True,\n            include_expected_output=True,\n            score={'evaluation_name': 'semantic_similarity'},\n            assertion={'evaluation_name': 'correct_meaning'},\n        ),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#comparative-evaluation", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "1. Be Specific in Rubrics", "anchor": "1-be-specific-in-rubrics", "heading_level": 3, "md_text": "**Bad:**\n```python\nfrom pydantic_evals.evaluators import LLMJudge\n\nLLMJudge(rubric='Good answer')\n```\n\n**Better:**\n```python\nfrom pydantic_evals.evaluators import LLMJudge\n\nLLMJudge(rubric='Response accurately answers the question without hallucinating facts')\n```\n\n**Best:**\n```python\nfrom pydantic_evals.evaluators import LLMJudge\n\nLLMJudge(\n    rubric='''\n    Response must:\n    1. Directly answer the question asked\n    2. Use only information from the provided context\n    3. Cite specific passages from the context\n    4. Acknowledge if information is insufficient\n    ''',\n    include_input=True,\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#1-be-specific-in-rubrics", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "2. Use Multiple Judges", "anchor": "2-use-multiple-judges", "heading_level": 3, "md_text": "Don't always try to evaluate everything with one rubric:\n\n```python\nfrom pydantic_evals.evaluators import LLMJudge", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#2-use-multiple-judges", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Instead of this:", "anchor": "instead-of-this", "heading_level": 1, "md_text": "LLMJudge(rubric='Response is good, accurate, helpful, and safe')", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#instead-of-this", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Do this:", "anchor": "do-this", "heading_level": 1, "md_text": "evaluators = [\n    LLMJudge(rubric='Response is factually accurate'),\n    LLMJudge(rubric='Response is helpful and actionable'),\n    LLMJudge(rubric='Response is safe and appropriate'),\n]\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#do-this", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "3. Combine with Deterministic Checks", "anchor": "3-combine-with-deterministic-checks", "heading_level": 3, "md_text": "Don't use LLM evaluation for checks that can be done deterministically:\n\n```python\nfrom pydantic_evals.evaluators import Contains, IsInstance, LLMJudge\n\nevaluators = [\n    IsInstance(type_name='str'),\n    Contains(value='required_section'),\n    LLMJudge(rubric='Response quality is high'),\n]\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#3-combine-with-deterministic-checks", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "4. Use Temperature 0 for Consistency", "anchor": "4-use-temperature-0-for-consistency", "heading_level": 3, "md_text": "```python\nfrom pydantic_ai.settings import ModelSettings\nfrom pydantic_evals.evaluators import LLMJudge\n\nLLMJudge(\n    rubric='...',\n    model_settings=ModelSettings(temperature=0.0),\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#4-use-temperature-0-for-consistency", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Non-Determinism", "anchor": "non-determinism", "heading_level": 3, "md_text": "LLM judges are not deterministic. The same output may receive different scores across runs.\n\n**Mitigation:**\n\n- Use `temperature=0.0` for more consistency\n- Run multiple evaluations and average\n- Use retry strategies for flaky evaluations", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#non-determinism", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Cost", "anchor": "cost", "heading_level": 3, "md_text": "LLM judges make API calls, which cost money and time.\n\n**Mitigation:**\n\n- Use cheaper models for simple checks (`gpt-5-mini`)\n- Run deterministic checks first to fail fast\n- Cache results when possible\n- Limit evaluation to changed cases", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#cost", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Model Biases", "anchor": "model-biases", "heading_level": 3, "md_text": "LLM judges inherit biases from their training data.\n\n**Mitigation:**\n\n- Use multiple judge models and compare\n- Review evaluation reasons, not just scores\n- Validate judges against human-labeled test sets\n- Be aware of known biases (length bias, style preferences)", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#model-biases", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Context Limits", "anchor": "context-limits", "heading_level": 3, "md_text": "Judges have token limits for inputs.\n\n**Mitigation:**\n\n- Truncate long inputs/outputs intelligently\n- Use focused rubrics that don't require full context\n- Consider chunked evaluation for very long content", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#context-limits", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "View Reasons", "anchor": "view-reasons", "heading_level": 3, "md_text": "```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\n\ndef my_task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[LLMJudge(rubric='Response is clear')],\n)\nreport = dataset.evaluate_sync(my_task)\nreport.print(include_reasons=True)\n\"\"\"\n     Evaluation Summary: my_task\n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Case ID  \u2503 Assertions  \u2503 Duration \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 Case 1   \u2502 LLMJudge: \u2714 \u2502     10ms \u2502\n\u2502          \u2502   Reason: - \u2502          \u2502\n\u2502          \u2502             \u2502          \u2502\n\u2502          \u2502             \u2502          \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 Averages \u2502 100.0% \u2714    \u2502     10ms \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\"\"\"\n```\n\nOutput:\n```\n\u2503 Assertions              \u2503\n\u2503 accuracy: \u2714            \u2503\n\u2503   Reason: The response \u2502\n\u2503   correctly states...  \u2502\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#view-reasons", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Access Programmatically", "anchor": "access-programmatically", "heading_level": 3, "md_text": "```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\n\ndef my_task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[LLMJudge(rubric='Response is clear')],\n)\nreport = dataset.evaluate_sync(my_task)\nfor case in report.cases:\n    for name, result in case.assertions.items():\n        print(f'{name}: {result.value}')\n        #> LLMJudge: True\n        if result.reason:\n            print(f'  Reason: {result.reason}')\n            #>   Reason: -\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#access-programmatically", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Compare Judges", "anchor": "compare-judges", "heading_level": 3, "md_text": "Test the same cases with different judge models:\n\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\n\ndef my_task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\njudges = [\n    LLMJudge(rubric='Response is clear', model='openai:gpt-5'),\n    LLMJudge(rubric='Response is clear', model='anthropic:claude-sonnet-4-5'),\n    LLMJudge(rubric='Response is clear', model='openai:gpt-5-mini'),\n]\n\nfor judge in judges:\n    dataset = Dataset(cases=[Case(inputs='test')], evaluators=[judge])\n    report = dataset.evaluate_sync(my_task)\n    # Compare results\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#compare-judges", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Advanced: Custom Judge Models", "anchor": "advanced-custom-judge-models", "heading_level": 2, "md_text": "Set a default judge model for all `LLMJudge` evaluators:\n\n```python\nfrom pydantic_evals.evaluators import LLMJudge\nfrom pydantic_evals.evaluators.llm_as_a_judge import set_default_judge_model", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#advanced-custom-judge-models", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Set default to Claude", "anchor": "set-default-to-claude", "heading_level": 1, "md_text": "set_default_judge_model('anthropic:claude-sonnet-4-5')", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#set-default-to-claude", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Now all LLMJudge instances use Claude by default", "anchor": "now-all-llmjudge-instances-use-claude-by-default", "heading_level": 1, "md_text": "LLMJudge(rubric='...')  # Uses Claude\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#now-all-llmjudge-instances-use-claude-by-default", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "- **[Custom Evaluators](custom.md)** - Write custom evaluation logic\n- **[Built-in Evaluators](built-in.md)** - Complete evaluator reference", "url": "https://ai.pydantic.dev/docs/evals/evaluators/llm-judge/#next-steps", "page": "docs/evals/evaluators/llm-judge", "source_site": "pydantic_ai"}
{"title": "Evaluators Overview", "anchor": "evaluators-overview", "heading_level": 1, "md_text": "Evaluators are the core of Pydantic Evals. They analyze task outputs and provide scores, labels, or pass/fail assertions.", "url": "https://ai.pydantic.dev/docs/evals/evaluators/overview/#evaluators-overview", "page": "docs/evals/evaluators/overview", "source_site": "pydantic_ai"}
{"title": "Deterministic Checks (Fast & Reliable)", "anchor": "deterministic-checks-fast-reliable", "heading_level": 3, "md_text": "Use deterministic evaluators when you can define exact rules:\n\n| Evaluator | Use Case | Example |\n|-----------|----------|---------|\n| [`EqualsExpected`][pydantic_evals.evaluators.EqualsExpected] | Exact output match | Structured data, classification |\n| [`Equals`][pydantic_evals.evaluators.Equals] | Equals specific value | Checking for sentinel values |\n| [`Contains`][pydantic_evals.evaluators.Contains] | Substring/element check | Required keywords, PII detection |\n| [`IsInstance`][pydantic_evals.evaluators.IsInstance] | Type validation | Format validation |\n| [`MaxDuration`][pydantic_evals.evaluators.MaxDuration] | Performance threshold | SLA compliance |\n| [`HasMatchingSpan`][pydantic_evals.evaluators.HasMatchingSpan] | Behavior verification | Tool calls, code paths |\n\n**Advantages:**\n\n- Fast execution (microseconds to milliseconds)\n- Deterministic results\n- No cost\n- Easy to debug\n\n**When to use:**\n\n- Format validation (JSON structure, type checking)\n- Required content checks (must contain X, must not contain Y)\n- Performance requirements (latency, token counts)\n- Behavioral checks (which tools were called, which code paths executed)", "url": "https://ai.pydantic.dev/docs/evals/evaluators/overview/#deterministic-checks-fast-reliable", "page": "docs/evals/evaluators/overview", "source_site": "pydantic_ai"}
{"title": "LLM-as-a-Judge (Flexible & Nuanced)", "anchor": "llm-as-a-judge-flexible-nuanced", "heading_level": 3, "md_text": "Use [`LLMJudge`][pydantic_evals.evaluators.LLMJudge] when evaluation requires understanding or judgment:\n\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\ndataset = Dataset(\n    cases=[Case(inputs='What is 2+2?', expected_output='4')],\n    evaluators=[\n        LLMJudge(\n            rubric='Response is factually accurate based on the input',\n            include_input=True,\n        )\n    ],\n)\n```\n\n**Advantages:**\n\n- Can evaluate subjective qualities (helpfulness, tone, creativity)\n- Understands natural language\n- Can follow complex rubrics\n- Flexible across domains\n\n**Disadvantages:**\n\n- Slower (seconds per evaluation)\n- Costs money\n- Non-deterministic\n- Can have biases\n\n**When to use:**\n\n- Factual accuracy\n- Relevance and helpfulness\n- Tone and style\n- Completeness\n- Following instructions\n- RAG quality (groundedness, citation accuracy)", "url": "https://ai.pydantic.dev/docs/evals/evaluators/overview/#llm-as-a-judge-flexible-nuanced", "page": "docs/evals/evaluators/overview", "source_site": "pydantic_ai"}
{"title": "Custom Evaluators", "anchor": "custom-evaluators", "heading_level": 3, "md_text": "Custom evaluators can be useful if you want to make use of any evaluation logic we don't provide with the framework.\nThey are frequently useful for domain-specific logic:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass ValidSQL(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        try:\n            import sqlparse\n            sqlparse.parse(ctx.output)\n            return True\n        except Exception:\n            return False\n```\n\n**When to use:**\n\n- Domain-specific validation (SQL syntax, regex patterns, business rules)\n- External API calls (running generated code, checking databases)\n- Complex calculations (precision/recall, BLEU scores)\n- Integration checks (does API call succeed?)", "url": "https://ai.pydantic.dev/docs/evals/evaluators/overview/#custom-evaluators", "page": "docs/evals/evaluators/overview", "source_site": "pydantic_ai"}
{"title": "Evaluation Types", "anchor": "evaluation-types", "heading_level": 2, "md_text": "!!! info \"Detailed Return Types Guide\"\n    For full detail about precisely what custom Evaluators may return, see [Custom Evaluator Return Types](custom.md#return-types).\n\nEvaluators essentially return three types of results:", "url": "https://ai.pydantic.dev/docs/evals/evaluators/overview/#evaluation-types", "page": "docs/evals/evaluators/overview", "source_site": "pydantic_ai"}
{"title": "1. Assertions (bool)", "anchor": "1-assertions-bool", "heading_level": 3, "md_text": "Pass/fail checks that appear as \u2714 or \u2717 in reports:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass HasKeyword(Evaluator):\n    keyword: str\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return self.keyword in ctx.output\n```\n\n**Use for:** Binary checks, quality gates, compliance requirements", "url": "https://ai.pydantic.dev/docs/evals/evaluators/overview/#1-assertions-bool", "page": "docs/evals/evaluators/overview", "source_site": "pydantic_ai"}
{"title": "2. Scores (int or float)", "anchor": "2-scores-int-or-float", "heading_level": 3, "md_text": "Numeric metrics:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass ConfidenceScore(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> float:\n        # Analyze and return score\n        return 0.87  # 87% confidence\n```\n\n**Use for:** Quality metrics, ranking, A/B testing, regression tracking", "url": "https://ai.pydantic.dev/docs/evals/evaluators/overview/#2-scores-int-or-float", "page": "docs/evals/evaluators/overview", "source_site": "pydantic_ai"}
{"title": "3. Labels (str)", "anchor": "3-labels-str", "heading_level": 3, "md_text": "Categorical classifications:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass SentimentClassifier(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> str:\n        if 'error' in ctx.output.lower():\n            return 'error'\n        elif 'success' in ctx.output.lower():\n            return 'success'\n        return 'neutral'\n```\n\n**Use for:** Classification, error categorization, quality buckets", "url": "https://ai.pydantic.dev/docs/evals/evaluators/overview/#3-labels-str", "page": "docs/evals/evaluators/overview", "source_site": "pydantic_ai"}
{"title": "Multiple Results", "anchor": "multiple-results", "heading_level": 3, "md_text": "You can return multiple evaluations from a single evaluator:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass ComprehensiveCheck(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> dict[str, bool | float | str]:\n        return {\n            'valid_format': self._check_format(ctx.output),  # bool\n            'quality_score': self._score_quality(ctx.output),  # float\n            'category': self._classify(ctx.output),  # str\n        }\n\n    def _check_format(self, output: str) -> bool:\n        return True\n\n    def _score_quality(self, output: str) -> float:\n        return 0.85\n\n    def _classify(self, output: str) -> str:\n        return 'good'\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/overview/#multiple-results", "page": "docs/evals/evaluators/overview", "source_site": "pydantic_ai"}
{"title": "Combining Evaluators", "anchor": "combining-evaluators", "heading_level": 2, "md_text": "Mix and match evaluators to create comprehensive evaluation suites:\n\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import (\n    Contains,\n    IsInstance,\n    LLMJudge,\n    MaxDuration,\n)\n\ndataset = Dataset(\n    cases=[Case(inputs='test', expected_output='result')],\n    evaluators=[\n        # Fast deterministic checks first\n        IsInstance(type_name='str'),\n        Contains(value='required_field'),\n        MaxDuration(seconds=2.0),\n        # Slower LLM checks after\n        LLMJudge(\n            rubric='Response is accurate and helpful',\n            include_input=True,\n        ),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/overview/#combining-evaluators", "page": "docs/evals/evaluators/overview", "source_site": "pydantic_ai"}
{"title": "Case-specific evaluators", "anchor": "case-specific-evaluators", "heading_level": 2, "md_text": "Case-specific evaluators are one of the most powerful features for building comprehensive evaluation suites. You can attach evaluators to individual [`Case`][pydantic_evals.Case] objects that only run for those specific cases:\n\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import IsInstance, LLMJudge\n\ndataset = Dataset(\n    cases=[\n        Case(\n            name='greeting_response',\n            inputs='Say hello',\n            evaluators=[\n                # This evaluator only runs for this case\n                LLMJudge(\n                    rubric='Response is warm and friendly, uses casual tone',\n                    include_input=True,\n                ),\n            ],\n        ),\n        Case(\n            name='formal_response',\n            inputs='Write a business email',\n            evaluators=[\n                # Different requirements for this case\n                LLMJudge(\n                    rubric='Response is professional and formal, uses business language',\n                    include_input=True,\n                ),\n            ],\n        ),\n    ],\n    evaluators=[\n        # This runs for ALL cases\n        IsInstance(type_name='str'),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/docs/evals/evaluators/overview/#case-specific-evaluators", "page": "docs/evals/evaluators/overview", "source_site": "pydantic_ai"}
{"title": "Why Case-Specific Evaluators Matter", "anchor": "why-case-specific-evaluators-matter", "heading_level": 3, "md_text": "Case-specific evaluators solve a fundamental problem with one-size-fits-all evaluation: **if you could write a single evaluator rubric that perfectly captured your requirements across all cases, you'd just incorporate that rubric into your agent's instructions**. (Note: this is less relevant in cases where you want to use a cheaper model in production and assess it using a more expensive model, but in many cases it makes sense to use the best model you can in production.)\n\nThe power of case-specific evaluation comes from the nuance:\n\n- **Different cases have different requirements**: A customer support response needs empathy; a technical API response needs precision\n- **Avoid \"inmates running the asylum\"**: If your LLMJudge rubric is generic enough to work everywhere, your agent should already be following it\n- **Capture nuanced golden behavior**: Each case can specify exactly what \"good\" looks like for that scenario", "url": "https://ai.pydantic.dev/docs/evals/evaluators/overview/#why-case-specific-evaluators-matter", "page": "docs/evals/evaluators/overview", "source_site": "pydantic_ai"}
{"title": "Building Golden Datasets with Case-Specific LLMJudge", "anchor": "building-golden-datasets-with-case-specific-llmjudge", "heading_level": 3, "md_text": "A particularly powerful pattern is using case-specific [`LLMJudge`][pydantic_evals.evaluators.LLMJudge] evaluators to quickly build comprehensive, maintainable evaluation suites. Instead of needing exact `expected_output` values, you can describe what you care about:\n\n```python\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\ndataset = Dataset(\n    cases=[\n        Case(\n            name='handle_refund_request',\n            inputs={'query': 'I want my money back', 'order_id': '12345'},\n            evaluators=[\n                LLMJudge(\n                    rubric=\"\"\"\n                    Response should:\n                    1. Acknowledge the refund request empathetically\n                    2. Ask for the reason for the refund\n                    3. Mention our 30-day refund policy\n                    4. NOT process the refund immediately (needs manager approval)\n                    \"\"\",\n                    include_input=True,\n                ),\n            ],\n        ),\n        Case(\n            name='handle_shipping_question',\n            inputs={'query': 'Where is my order?', 'order_id': '12345'},\n            evaluators=[\n                LLMJudge(\n                    rubric=\"\"\"\n                    Response should:\n                    1. Confirm the order number\n                    2. Provide tracking information\n                    3. Give estimated delivery date\n                    4. Be brief and factual (not overly apologetic)\n                    \"\"\",\n                    include_input=True,\n                ),\n            ],\n        ),\n        Case(\n            name='handle_angry_customer',\n            inputs={'query': 'This is completely unacceptable!', 'order_id': '12345'},\n            evaluators=[\n                LLMJudge(\n                    rubric=\"\"\"\n                    Response should:\n                    1. Prioritize de-escalation with empathy\n                    2. Avoid being defensive\n                    3. Offer concrete next steps\n                    4. Use phrases like \"I understand\" and \"Let me help\"\n                    \"\"\",\n                    include_input=True,\n                ),\n            ],\n        ),\n    ],\n)\n```\n\nThis approach lets you:\n\n- **Build comprehensive test suites quickly**: Just describe what you want per case\n- **Maintain easily**: Update rubrics as requirements change, without regenerating outputs\n- **Cover edge cases naturally**: Add new cases with specific requirements as you discover them\n- **Capture domain knowledge**: Each rubric documents what \"good\" means for that scenario\n\nThe LLM evaluator excels at understanding nuanced requirements and assessing compliance, making this a practical way to create thorough evaluation coverage without brittleness.", "url": "https://ai.pydantic.dev/docs/evals/evaluators/overview/#building-golden-datasets-with-case-specific-llmjudge", "page": "docs/evals/evaluators/overview", "source_site": "pydantic_ai"}
{"title": "Async vs Sync", "anchor": "async-vs-sync", "heading_level": 2, "md_text": "Evaluators can be sync or async:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass SyncEvaluator(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return True\n\n\nasync def some_async_operation() -> bool:\n    return True\n\n\n@dataclass\nclass AsyncEvaluator(Evaluator):\n    async def evaluate(self, ctx: EvaluatorContext) -> bool:\n        result = await some_async_operation()\n        return result\n```\n\nPydantic Evals handles both automatically. Use async when:\n- Making API calls\n- Running database queries\n- Performing I/O operations\n- Calling LLMs (like [`LLMJudge`][pydantic_evals.evaluators.LLMJudge])", "url": "https://ai.pydantic.dev/docs/evals/evaluators/overview/#async-vs-sync", "page": "docs/evals/evaluators/overview", "source_site": "pydantic_ai"}
{"title": "Evaluation Context", "anchor": "evaluation-context", "heading_level": 2, "md_text": "All evaluators receive an [`EvaluatorContext`][pydantic_evals.evaluators.EvaluatorContext]:\n\n- `ctx.inputs` - Task inputs\n- `ctx.output` - Task output (to evaluate)\n- `ctx.expected_output` - Expected output (if provided)\n- `ctx.metadata` - Case metadata (if provided)\n- `ctx.duration` - Task execution time (seconds)\n- `ctx.span_tree` - OpenTelemetry spans (if logfire configured)\n- `ctx.metrics` - Custom metrics dict\n- `ctx.attributes` - Custom attributes dict\n\nThis gives evaluators full context to make informed assessments.", "url": "https://ai.pydantic.dev/docs/evals/evaluators/overview/#evaluation-context", "page": "docs/evals/evaluators/overview", "source_site": "pydantic_ai"}
{"title": "Error Handling", "anchor": "error-handling", "heading_level": 2, "md_text": "If an evaluator raises an exception, it's captured as an [`EvaluatorFailure`][pydantic_evals.evaluators.EvaluatorFailure]:\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\ndef risky_operation(output: str) -> bool:\n    # This might raise an exception\n    if 'error' in output:\n        raise ValueError('Found error in output')\n    return True\n\n\n@dataclass\nclass RiskyEvaluator(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        # If this raises an exception, it will be captured\n        result = risky_operation(ctx.output)\n        return result\n```\n\nFailures appear in `report.cases[i].evaluator_failures` with:\n\n- Evaluator name\n- Error message\n- Full stacktrace\n\nUse retry configuration to handle transient failures (see [Retry Strategies](../how-to/retry-strategies.md)).", "url": "https://ai.pydantic.dev/docs/evals/evaluators/overview/#error-handling", "page": "docs/evals/evaluators/overview", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "- **[Built-in Evaluators](built-in.md)** - Complete reference of all provided evaluators\n- **[LLM Judge](llm-judge.md)** - Deep dive on LLM-as-a-Judge evaluation\n- **[Custom Evaluators](custom.md)** - Write your own evaluation logic\n- **[Span-Based Evaluation](span-based.md)** - Evaluate using OpenTelemetry spans", "url": "https://ai.pydantic.dev/docs/evals/evaluators/overview/#next-steps", "page": "docs/evals/evaluators/overview", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.models.function`", "anchor": "pydantic_aimodelsfunction", "heading_level": 1, "md_text": "A model controlled by a local function.\n\n[`FunctionModel`][pydantic_ai.models.function.FunctionModel] is similar to [`TestModel`](test.md),\nbut allows greater control over the model's behavior.\n\nIts primary use case is for more advanced unit testing than is possible with `TestModel`.\n\nHere's a minimal example:\n\n```py {title=\"function_model_usage.py\" call_name=\"test_my_agent\" noqa=\"I001\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai import ModelMessage, ModelResponse, TextPart\nfrom pydantic_ai.models.function import FunctionModel, AgentInfo\n\nmy_agent = Agent('openai:gpt-5')\n\n\nasync def model_function(\n    messages: list[ModelMessage], info: AgentInfo\n) -> ModelResponse:\n    print(messages)\n    \"\"\"\n    [\n        ModelRequest(\n            parts=[\n                UserPromptPart(\n                    content='Testing my agent...',\n                    timestamp=datetime.datetime(...),\n                )\n            ]\n        )\n    ]\n    \"\"\"\n    print(info)\n    \"\"\"\n    AgentInfo(\n        function_tools=[], allow_text_output=True, output_tools=[], model_settings=None\n    )\n    \"\"\"\n    return ModelResponse(parts=[TextPart('hello world')])\n\n\nasync def test_my_agent():\n    \"\"\"Unit test for my_agent, to be run by pytest.\"\"\"\n    with my_agent.override(model=FunctionModel(model_function)):\n        result = await my_agent.run('Testing my agent...')\n        assert result.output == 'hello world'\n```\n\nSee [Unit testing with `FunctionModel`](../../testing.md#unit-testing-with-functionmodel) for detailed documentation.\n\n::: pydantic_ai.models.function", "url": "https://ai.pydantic.dev/docs/api/models/function/#pydantic_aimodelsfunction", "page": "docs/api/models/function", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.models.google`", "anchor": "pydantic_aimodelsgoogle", "heading_level": 1, "md_text": "Interface that uses the [`google-genai`](https://pypi.org/project/google-genai/) package under the hood to\naccess Google's Gemini models via both the Generative Language API and Vertex AI.", "url": "https://ai.pydantic.dev/docs/api/models/google/#pydantic_aimodelsgoogle", "page": "docs/api/models/google", "source_site": "pydantic_ai"}
{"title": "Setup", "anchor": "setup", "heading_level": 2, "md_text": "For details on how to set up authentication with this model, see [model configuration for Google](../../models/google.md).\n\n::: pydantic_ai.models.google", "url": "https://ai.pydantic.dev/docs/api/models/google/#setup", "page": "docs/api/models/google", "source_site": "pydantic_ai"}
{"title": "Setup", "anchor": "setup", "heading_level": 2, "md_text": "For details on how to set up authentication with this model, see [model configuration for Groq](../../models/groq.md).\n\n::: pydantic_ai.models.groq", "url": "https://ai.pydantic.dev/docs/api/models/groq/#setup", "page": "docs/api/models/groq", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.models.test`", "anchor": "pydantic_aimodelstest", "heading_level": 1, "md_text": "Utility model for quickly testing apps built with Pydantic AI.\n\nHere's a minimal example:\n\n```py {title=\"test_model_usage.py\" call_name=\"test_my_agent\" noqa=\"I001\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.test import TestModel\n\nmy_agent = Agent('openai:gpt-5', system_prompt='...')\n\n\nasync def test_my_agent():\n    \"\"\"Unit test for my_agent, to be run by pytest.\"\"\"\n    m = TestModel()\n    with my_agent.override(model=m):\n        result = await my_agent.run('Testing my agent...')\n        assert result.output == 'success (no tool calls)'\n    assert m.last_model_request_parameters.function_tools == []\n```\n\nSee [Unit testing with `TestModel`](../../testing.md#unit-testing-with-testmodel) for detailed documentation.\n\n::: pydantic_ai.models.test", "url": "https://ai.pydantic.dev/docs/api/models/test/#pydantic_aimodelstest", "page": "docs/api/models/test", "source_site": "pydantic_ai"}
{"title": "Setup", "anchor": "setup", "heading_level": 2, "md_text": "For details on how to set up authentication with this model, see [model configuration for Hugging Face](../../models/huggingface.md).\n\n::: pydantic_ai.models.huggingface", "url": "https://ai.pydantic.dev/docs/api/models/huggingface/#setup", "page": "docs/api/models/huggingface", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.models.wrapper", "anchor": "pydantic_aimodelswrapper", "heading_level": 1, "md_text": "::: pydantic_ai.models.wrapper", "url": "https://ai.pydantic.dev/docs/api/models/wrapper/#pydantic_aimodelswrapper", "page": "docs/api/models/wrapper", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.models.fallback", "anchor": "pydantic_aimodelsfallback", "heading_level": 1, "md_text": "::: pydantic_ai.models.fallback", "url": "https://ai.pydantic.dev/docs/api/models/fallback/#pydantic_aimodelsfallback", "page": "docs/api/models/fallback", "source_site": "pydantic_ai"}
{"title": "Setup", "anchor": "setup", "heading_level": 2, "md_text": "For details on how to set up authentication with this model, see [model configuration for Mistral](../../models/mistral.md).\n\n::: pydantic_ai.models.mistral", "url": "https://ai.pydantic.dev/docs/api/models/mistral/#setup", "page": "docs/api/models/mistral", "source_site": "pydantic_ai"}
{"title": "Setup", "anchor": "setup", "heading_level": 2, "md_text": "For details on how to set up authentication with this model, see [model configuration for Anthropic](../../models/anthropic.md).\n\n::: pydantic_ai.models.anthropic", "url": "https://ai.pydantic.dev/docs/api/models/anthropic/#setup", "page": "docs/api/models/anthropic", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.models.instrumented", "anchor": "pydantic_aimodelsinstrumented", "heading_level": 1, "md_text": "::: pydantic_ai.models.instrumented", "url": "https://ai.pydantic.dev/docs/api/models/instrumented/#pydantic_aimodelsinstrumented", "page": "docs/api/models/instrumented", "source_site": "pydantic_ai"}
{"title": "Setup", "anchor": "setup", "heading_level": 2, "md_text": "For details on how to set up authentication with this model, see [model configuration for Bedrock](../../models/bedrock.md).\n\n::: pydantic_ai.models.bedrock", "url": "https://ai.pydantic.dev/docs/api/models/bedrock/#setup", "page": "docs/api/models/bedrock", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.models`", "anchor": "pydantic_aimodels", "heading_level": 1, "md_text": "::: pydantic_ai.models\n    options:\n      members:\n        - KnownModelName\n        - ModelRequestParameters\n        - Model\n        - AbstractToolDefinition\n        - StreamedResponse\n        - ALLOW_MODEL_REQUESTS\n        - check_allow_model_requests\n        - override_allow_model_requests", "url": "https://ai.pydantic.dev/docs/api/models/base/#pydantic_aimodels", "page": "docs/api/models/base", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.models.mcp_sampling", "anchor": "pydantic_aimodelsmcp_sampling", "heading_level": 1, "md_text": "::: pydantic_ai.models.mcp_sampling", "url": "https://ai.pydantic.dev/docs/api/models/mcp-sampling/#pydantic_aimodelsmcp_sampling", "page": "docs/api/models/mcp-sampling", "source_site": "pydantic_ai"}
{"title": "Setup", "anchor": "setup", "heading_level": 2, "md_text": "For details on how to set up this model, see [model configuration for Outlines](../../models/outlines.md).\n\n::: pydantic_ai.models.outlines", "url": "https://ai.pydantic.dev/docs/api/models/outlines/#setup", "page": "docs/api/models/outlines", "source_site": "pydantic_ai"}
{"title": "Setup", "anchor": "setup", "heading_level": 2, "md_text": "For details on how to set up authentication with this model, see [model configuration for Cohere](../../models/cohere.md).\n\n::: pydantic_ai.models.cohere", "url": "https://ai.pydantic.dev/docs/api/models/cohere/#setup", "page": "docs/api/models/cohere", "source_site": "pydantic_ai"}
{"title": "Setup", "anchor": "setup", "heading_level": 2, "md_text": "For details on how to set up authentication with this model, see [model configuration for OpenAI](../../models/openai.md).\n\n::: pydantic_ai.models.openai", "url": "https://ai.pydantic.dev/docs/api/models/openai/#setup", "page": "docs/api/models/openai", "source_site": "pydantic_ai"}
{"title": "`pydantic_graph.nodes`", "anchor": "pydantic_graphnodes", "heading_level": 1, "md_text": "::: pydantic_graph.nodes\n    options:\n        members:\n            - StateT\n            - GraphRunContext\n            - BaseNode\n            - End\n            - Edge\n            - DepsT\n            - RunEndT\n            - NodeRunEndT", "url": "https://ai.pydantic.dev/docs/api/pydantic_graph/nodes/#pydantic_graphnodes", "page": "docs/api/pydantic_graph/nodes", "source_site": "pydantic_ai"}
{"title": "`pydantic_graph.beta.step`", "anchor": "pydantic_graphbetastep", "heading_level": 1, "md_text": "::: pydantic_graph.beta.step", "url": "https://ai.pydantic.dev/docs/api/pydantic_graph/beta_step/#pydantic_graphbetastep", "page": "docs/api/pydantic_graph/beta_step", "source_site": "pydantic_ai"}
{"title": "`pydantic_graph.beta`", "anchor": "pydantic_graphbeta", "heading_level": 1, "md_text": "::: pydantic_graph.beta", "url": "https://ai.pydantic.dev/docs/api/pydantic_graph/beta/#pydantic_graphbeta", "page": "docs/api/pydantic_graph/beta", "source_site": "pydantic_ai"}
{"title": "`pydantic_graph.beta.graph`", "anchor": "pydantic_graphbetagraph", "heading_level": 1, "md_text": "::: pydantic_graph.beta.graph", "url": "https://ai.pydantic.dev/docs/api/pydantic_graph/beta_graph/#pydantic_graphbetagraph", "page": "docs/api/pydantic_graph/beta_graph", "source_site": "pydantic_ai"}
{"title": "`pydantic_graph.beta.decision`", "anchor": "pydantic_graphbetadecision", "heading_level": 1, "md_text": "::: pydantic_graph.beta.decision", "url": "https://ai.pydantic.dev/docs/api/pydantic_graph/beta_decision/#pydantic_graphbetadecision", "page": "docs/api/pydantic_graph/beta_decision", "source_site": "pydantic_ai"}
{"title": "`pydantic_graph.exceptions`", "anchor": "pydantic_graphexceptions", "heading_level": 1, "md_text": "::: pydantic_graph.exceptions", "url": "https://ai.pydantic.dev/docs/api/pydantic_graph/exceptions/#pydantic_graphexceptions", "page": "docs/api/pydantic_graph/exceptions", "source_site": "pydantic_ai"}
{"title": "`pydantic_graph`", "anchor": "pydantic_graph", "heading_level": 1, "md_text": "::: pydantic_graph.graph", "url": "https://ai.pydantic.dev/docs/api/pydantic_graph/graph/#pydantic_graph", "page": "docs/api/pydantic_graph/graph", "source_site": "pydantic_ai"}
{"title": "`pydantic_graph.mermaid`", "anchor": "pydantic_graphmermaid", "heading_level": 1, "md_text": "::: pydantic_graph.mermaid", "url": "https://ai.pydantic.dev/docs/api/pydantic_graph/mermaid/#pydantic_graphmermaid", "page": "docs/api/pydantic_graph/mermaid", "source_site": "pydantic_ai"}
{"title": "`pydantic_graph.beta.node`", "anchor": "pydantic_graphbetanode", "heading_level": 1, "md_text": "::: pydantic_graph.beta.node", "url": "https://ai.pydantic.dev/docs/api/pydantic_graph/beta_node/#pydantic_graphbetanode", "page": "docs/api/pydantic_graph/beta_node", "source_site": "pydantic_ai"}
{"title": "`pydantic_graph.beta.graph_builder`", "anchor": "pydantic_graphbetagraph_builder", "heading_level": 1, "md_text": "::: pydantic_graph.beta.graph_builder", "url": "https://ai.pydantic.dev/docs/api/pydantic_graph/beta_graph_builder/#pydantic_graphbetagraph_builder", "page": "docs/api/pydantic_graph/beta_graph_builder", "source_site": "pydantic_ai"}
{"title": "`pydantic_graph.beta.join`", "anchor": "pydantic_graphbetajoin", "heading_level": 1, "md_text": "::: pydantic_graph.beta.join", "url": "https://ai.pydantic.dev/docs/api/pydantic_graph/beta_join/#pydantic_graphbetajoin", "page": "docs/api/pydantic_graph/beta_join", "source_site": "pydantic_ai"}
{"title": "`pydantic_graph.persistence`", "anchor": "pydantic_graphpersistence", "heading_level": 1, "md_text": "::: pydantic_graph.persistence\n\n::: pydantic_graph.persistence.in_mem\n\n::: pydantic_graph.persistence.file", "url": "https://ai.pydantic.dev/docs/api/pydantic_graph/persistence/#pydantic_graphpersistence", "page": "docs/api/pydantic_graph/persistence", "source_site": "pydantic_ai"}
{"title": "`pydantic_evals.otel`", "anchor": "pydantic_evalsotel", "heading_level": 1, "md_text": "::: pydantic_evals.otel", "url": "https://ai.pydantic.dev/docs/api/pydantic_evals/otel/#pydantic_evalsotel", "page": "docs/api/pydantic_evals/otel", "source_site": "pydantic_ai"}
{"title": "`pydantic_evals.dataset`", "anchor": "pydantic_evalsdataset", "heading_level": 1, "md_text": "::: pydantic_evals.dataset", "url": "https://ai.pydantic.dev/docs/api/pydantic_evals/dataset/#pydantic_evalsdataset", "page": "docs/api/pydantic_evals/dataset", "source_site": "pydantic_ai"}
{"title": "`pydantic_evals.generation`", "anchor": "pydantic_evalsgeneration", "heading_level": 1, "md_text": "::: pydantic_evals.generation", "url": "https://ai.pydantic.dev/docs/api/pydantic_evals/generation/#pydantic_evalsgeneration", "page": "docs/api/pydantic_evals/generation", "source_site": "pydantic_ai"}
{"title": "`pydantic_evals.evaluators`", "anchor": "pydantic_evalsevaluators", "heading_level": 1, "md_text": "::: pydantic_evals.evaluators\n\n::: pydantic_evals.evaluators.llm_as_a_judge", "url": "https://ai.pydantic.dev/docs/api/pydantic_evals/evaluators/#pydantic_evalsevaluators", "page": "docs/api/pydantic_evals/evaluators", "source_site": "pydantic_ai"}
{"title": "`pydantic_evals.reporting`", "anchor": "pydantic_evalsreporting", "heading_level": 1, "md_text": "::: pydantic_evals.reporting", "url": "https://ai.pydantic.dev/docs/api/pydantic_evals/reporting/#pydantic_evalsreporting", "page": "docs/api/pydantic_evals/reporting", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.ui.vercel_ai`", "anchor": "pydantic_aiuivercel_ai", "heading_level": 1, "md_text": "::: pydantic_ai.ui.vercel_ai\n\n::: pydantic_ai.ui.vercel_ai.request_types\n\n::: pydantic_ai.ui.vercel_ai.response_types", "url": "https://ai.pydantic.dev/docs/api/ui/vercel_ai/#pydantic_aiuivercel_ai", "page": "docs/api/ui/vercel_ai", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.ui.ag_ui`", "anchor": "pydantic_aiuiag_ui", "heading_level": 1, "md_text": "::: pydantic_ai.ui.ag_ui\n\n::: pydantic_ai.ui.ag_ui.app", "url": "https://ai.pydantic.dev/docs/api/ui/ag_ui/#pydantic_aiuiag_ui", "page": "docs/api/ui/ag_ui", "source_site": "pydantic_ai"}
{"title": "`pydantic_ai.ui`", "anchor": "pydantic_aiui", "heading_level": 1, "md_text": "::: pydantic_ai.ui", "url": "https://ai.pydantic.dev/docs/api/ui/base/#pydantic_aiui", "page": "docs/api/ui/base", "source_site": "pydantic_ai"}
{"title": "Joins and Reducers", "anchor": "joins-and-reducers", "heading_level": 1, "md_text": "Join nodes synchronize and aggregate data from parallel execution paths. They use **Reducers** to combine multiple inputs into a single output.", "url": "https://ai.pydantic.dev/docs/graph/beta/joins/#joins-and-reducers", "page": "docs/graph/beta/joins", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "When you use [parallel execution](parallel.md) (broadcasting or mapping), you often need to collect and combine the results. Join nodes serve this purpose by:\n\n1. Waiting for all parallel tasks to complete\n2. Aggregating their outputs using a [`ReducerFunction`][pydantic_graph.beta.join.ReducerFunction]\n3. Passing the aggregated result to the next node", "url": "https://ai.pydantic.dev/docs/graph/beta/joins/#overview", "page": "docs/graph/beta/joins", "source_site": "pydantic_ai"}
{"title": "Creating Joins", "anchor": "creating-joins", "heading_level": 2, "md_text": "Create a join using `GraphBuilder.join` with a reducer function and initial value or factory:\n\n```python {title=\"basic_join.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\ng = GraphBuilder(state_type=SimpleState, output_type=list[int])\n\n@g.step\nasync def generate_numbers(ctx: StepContext[SimpleState, None, None]) -> list[int]:\n    return [1, 2, 3, 4, 5]\n\n@g.step\nasync def square(ctx: StepContext[SimpleState, None, int]) -> int:\n    return ctx.inputs * ctx.inputs", "url": "https://ai.pydantic.dev/docs/graph/beta/joins/#creating-joins", "page": "docs/graph/beta/joins", "source_site": "pydantic_ai"}
{"title": "Create a join to collect all squared values", "anchor": "create-a-join-to-collect-all-squared-values", "heading_level": 1, "md_text": "collect = g.join(reduce_list_append, initial_factory=list[int])\n\ng.add(\n    g.edge_from(g.start_node).to(generate_numbers),\n    g.edge_from(generate_numbers).map().to(square),\n    g.edge_from(square).to(collect),\n    g.edge_from(collect).to(g.end_node),\n)\n\ngraph = g.build()\n\nasync def main():\n    result = await graph.run(state=SimpleState())\n    print(sorted(result))\n    #> [1, 4, 9, 16, 25]\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/joins/#create-a-join-to-collect-all-squared-values", "page": "docs/graph/beta/joins", "source_site": "pydantic_ai"}
{"title": "Built-in Reducers", "anchor": "built-in-reducers", "heading_level": 2, "md_text": "Pydantic Graph provides several common reducer types out of the box:", "url": "https://ai.pydantic.dev/docs/graph/beta/joins/#built-in-reducers", "page": "docs/graph/beta/joins", "source_site": "pydantic_ai"}
{"title": "`reduce_list_append`", "anchor": "reduce_list_append", "heading_level": 3, "md_text": "[`reduce_list_append`][pydantic_graph.beta.join.reduce_list_append] collects all inputs into a list:\n\n```python {title=\"list_reducer.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=list[str])\n\n    @g.step\n    async def generate(ctx: StepContext[SimpleState, None, None]) -> list[int]:\n        return [10, 20, 30]\n\n    @g.step\n    async def to_string(ctx: StepContext[SimpleState, None, int]) -> str:\n        return f'value-{ctx.inputs}'\n\n    collect = g.join(reduce_list_append, initial_factory=list[str])\n\n    g.add(\n        g.edge_from(g.start_node).to(generate),\n        g.edge_from(generate).map().to(to_string),\n        g.edge_from(to_string).to(collect),\n        g.edge_from(collect).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(sorted(result))\n    #> ['value-10', 'value-20', 'value-30']\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/joins/#reduce_list_append", "page": "docs/graph/beta/joins", "source_site": "pydantic_ai"}
{"title": "`reduce_list_extend`", "anchor": "reduce_list_extend", "heading_level": 3, "md_text": "[`reduce_list_extend`][pydantic_graph.beta.join.reduce_list_extend] extends a list with an iterable of items:\n\n```python {title=\"list_extend_reducer.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_extend\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=list[int])\n\n    @g.step\n    async def generate(ctx: StepContext[SimpleState, None, None]) -> list[int]:\n        return [1, 2, 3]\n\n    @g.step\n    async def create_range(ctx: StepContext[SimpleState, None, int]) -> list[int]:\n        \"\"\"Create a range from 0 to the input value.\"\"\"\n        return list(range(ctx.inputs))\n\n    collect = g.join(reduce_list_extend, initial_factory=list[int])\n\n    g.add(\n        g.edge_from(g.start_node).to(generate),\n        g.edge_from(generate).map().to(create_range),\n        g.edge_from(create_range).to(collect),\n        g.edge_from(collect).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(sorted(result))\n    #> [0, 0, 0, 1, 1, 2]\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/joins/#reduce_list_extend", "page": "docs/graph/beta/joins", "source_site": "pydantic_ai"}
{"title": "`reduce_dict_update`", "anchor": "reduce_dict_update", "heading_level": 3, "md_text": "[`reduce_dict_update`][pydantic_graph.beta.join.reduce_dict_update] merges dictionaries together:\n\n```python {title=\"dict_reducer.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_dict_update\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=dict[str, int])\n\n    @g.step\n    async def generate_keys(ctx: StepContext[SimpleState, None, None]) -> list[str]:\n        return ['apple', 'banana', 'cherry']\n\n    @g.step\n    async def create_entry(ctx: StepContext[SimpleState, None, str]) -> dict[str, int]:\n        return {ctx.inputs: len(ctx.inputs)}\n\n    merge = g.join(reduce_dict_update, initial_factory=dict[str, int])\n\n    g.add(\n        g.edge_from(g.start_node).to(generate_keys),\n        g.edge_from(generate_keys).map().to(create_entry),\n        g.edge_from(create_entry).to(merge),\n        g.edge_from(merge).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    result = {k: result[k] for k in sorted(result)}  # force deterministic ordering\n    print(result)\n    #> {'apple': 5, 'banana': 6, 'cherry': 6}\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/joins/#reduce_dict_update", "page": "docs/graph/beta/joins", "source_site": "pydantic_ai"}
{"title": "`reduce_null`", "anchor": "reduce_null", "heading_level": 3, "md_text": "[`reduce_null`][pydantic_graph.beta.join.reduce_null] discards all inputs and returns `None`. Useful when you only care about side effects:\n\n```python {title=\"null_reducer.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_null\n\n\n@dataclass\nclass CounterState:\n    total: int = 0\n\n\nasync def main():\n    g = GraphBuilder(state_type=CounterState, output_type=int)\n\n    @g.step\n    async def generate(ctx: StepContext[CounterState, None, None]) -> list[int]:\n        return [1, 2, 3, 4, 5]\n\n    @g.step\n    async def accumulate(ctx: StepContext[CounterState, None, int]) -> int:\n        ctx.state.total += ctx.inputs\n        return ctx.inputs\n\n    # We don't care about the outputs, only the side effect on state\n    ignore = g.join(reduce_null, initial=None)\n\n    @g.step\n    async def get_total(ctx: StepContext[CounterState, None, None]) -> int:\n        return ctx.state.total\n\n    g.add(\n        g.edge_from(g.start_node).to(generate),\n        g.edge_from(generate).map().to(accumulate),\n        g.edge_from(accumulate).to(ignore),\n        g.edge_from(ignore).to(get_total),\n        g.edge_from(get_total).to(g.end_node),\n    )\n\n    graph = g.build()\n    state = CounterState()\n    result = await graph.run(state=state)\n    print(result)\n    #> 15\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/joins/#reduce_null", "page": "docs/graph/beta/joins", "source_site": "pydantic_ai"}
{"title": "`reduce_sum`", "anchor": "reduce_sum", "heading_level": 3, "md_text": "[`reduce_sum`][pydantic_graph.beta.join.reduce_sum] sums numeric values:\n\n```python {title=\"sum_reducer.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_sum\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=int)\n\n    @g.step\n    async def generate(ctx: StepContext[SimpleState, None, None]) -> list[int]:\n        return [10, 20, 30, 40]\n\n    @g.step\n    async def identity(ctx: StepContext[SimpleState, None, int]) -> int:\n        return ctx.inputs\n\n    sum_join = g.join(reduce_sum, initial=0)\n\n    g.add(\n        g.edge_from(g.start_node).to(generate),\n        g.edge_from(generate).map().to(identity),\n        g.edge_from(identity).to(sum_join),\n        g.edge_from(sum_join).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(result)\n    #> 100\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/joins/#reduce_sum", "page": "docs/graph/beta/joins", "source_site": "pydantic_ai"}
{"title": "`ReduceFirstValue`", "anchor": "reducefirstvalue", "heading_level": 3, "md_text": "[`ReduceFirstValue`][pydantic_graph.beta.join.ReduceFirstValue] returns the first value it receives and cancels all other parallel tasks. This is useful for \"race\" scenarios where you want the first successful result:\n\n```python {title=\"first_value_reducer.py\"}\nimport asyncio\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import ReduceFirstValue\n\n\n@dataclass\nclass SimpleState:\n    tasks_completed: int = 0\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=str)\n\n    @g.step\n    async def generate(ctx: StepContext[SimpleState, None, None]) -> list[int]:\n        return [1, 12, 13, 14, 15]\n\n    @g.step\n    async def slow_process(ctx: StepContext[SimpleState, None, int]) -> str:\n        \"\"\"Simulate variable processing times.\"\"\"\n        # Simulate different delays\n        await asyncio.sleep(ctx.inputs * 0.1)\n        ctx.state.tasks_completed += 1\n        return f'Result from task {ctx.inputs}'\n\n    # Use ReduceFirstValue to get the first result and cancel the rest\n    first_result = g.join(ReduceFirstValue[str](), initial=None, node_id='first_result')\n\n    g.add(\n        g.edge_from(g.start_node).to(generate),\n        g.edge_from(generate).map().to(slow_process),\n        g.edge_from(slow_process).to(first_result),\n        g.edge_from(first_result).to(g.end_node),\n    )\n\n    graph = g.build()\n    state = SimpleState()\n    result = await graph.run(state=state)\n\n    print(result)\n    #> Result from task 1\n    print(f'Tasks completed: {state.tasks_completed}')\n    #> Tasks completed: 1\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/joins/#reducefirstvalue", "page": "docs/graph/beta/joins", "source_site": "pydantic_ai"}
{"title": "Custom Reducers", "anchor": "custom-reducers", "heading_level": 2, "md_text": "Create custom reducers by defining a [`ReducerFunction`][pydantic_graph.beta.join.ReducerFunction]:\n\n```python {title=\"custom_reducer.py\"}\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\ndef reduce_sum(current: int, inputs: int) -> int:\n    \"\"\"A reducer that sums numbers.\"\"\"\n    return current + inputs\n\n\nasync def main():\n    g = GraphBuilder(output_type=int)\n\n    @g.step\n    async def generate(ctx: StepContext[None, None, None]) -> list[int]:\n        return [5, 10, 15, 20]\n\n    @g.step\n    async def identity(ctx: StepContext[None, None, int]) -> int:\n        return ctx.inputs\n\n    sum_join = g.join(reduce_sum, initial=0)\n\n    g.add(\n        g.edge_from(g.start_node).to(generate),\n        g.edge_from(generate).map().to(identity),\n        g.edge_from(identity).to(sum_join),\n        g.edge_from(sum_join).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run()\n    print(result)\n    #> 50\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/joins/#custom-reducers", "page": "docs/graph/beta/joins", "source_site": "pydantic_ai"}
{"title": "Reducers with State Access", "anchor": "reducers-with-state-access", "heading_level": 2, "md_text": "Reducers can access and modify the graph state:\n\n```python {title=\"stateful_reducer.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import ReducerContext\n\n\n@dataclass\nclass MetricsState:\n    total_count: int = 0\n    total_sum: int = 0\n\n\n@dataclass\nclass ReducedMetrics:\n    count: int = 0\n    sum: int = 0\n\n\ndef reduce_metrics_sum(ctx: ReducerContext[MetricsState, None], current: ReducedMetrics, inputs: int) -> ReducedMetrics:\n    ctx.state.total_count += 1\n    ctx.state.total_sum += inputs\n    return ReducedMetrics(count=current.count + 1, sum=current.sum + inputs)\n\ndef reduce_metrics_max(current: ReducedMetrics, inputs: ReducedMetrics) -> ReducedMetrics:\n    return ReducedMetrics(count=max(current.count, inputs.count), sum=max(current.sum, inputs.sum))\n\n\nasync def main():\n    g = GraphBuilder(state_type=MetricsState, output_type=dict[str, int])\n\n    @g.step\n    async def generate(ctx: StepContext[object, None, None]) -> list[int]:\n        return [1, 3, 5, 7, 9, 10, 20, 30, 40]\n\n    @g.step\n    async def process_even(ctx: StepContext[MetricsState, None, int]) -> int:\n        return ctx.inputs * 2\n\n    @g.step\n    async def process_odd(ctx: StepContext[MetricsState, None, int]) -> int:\n        return ctx.inputs * 3\n\n    metrics_even = g.join(reduce_metrics_sum, initial_factory=ReducedMetrics, node_id='metrics_even')\n    metrics_odd = g.join(reduce_metrics_sum, initial_factory=ReducedMetrics, node_id='metrics_odd')\n    metrics_max = g.join(reduce_metrics_max, initial_factory=ReducedMetrics, node_id='metrics_max')\n\n    g.add(\n        g.edge_from(g.start_node).to(generate),\n        # Send even and odd numbers to their respective `process` steps\n        g.edge_from(generate).map().to(\n            g.decision()\n            .branch(g.match(int, matches=lambda x: x % 2 == 0).label('even').to(process_even))\n            .branch(g.match(int, matches=lambda x: x % 2 == 1).label('odd').to(process_odd))\n        ),\n        # Reduce metrics for even and odd numbers separately\n        g.edge_from(process_even).to(metrics_even),\n        g.edge_from(process_odd).to(metrics_odd),\n        # Aggregate the max values for each field\n        g.edge_from(metrics_even).to(metrics_max),\n        g.edge_from(metrics_odd).to(metrics_max),\n        # Finish the graph run with the final reduced value\n        g.edge_from(metrics_max).to(g.end_node),\n    )\n\n    graph = g.build()\n    state = MetricsState()\n    result = await graph.run(state=state)\n\n    print(f'Result: {result}')\n    #> Result: ReducedMetrics(count=5, sum=200)\n    print(f'State total_count: {state.total_count}')\n    #> State total_count: 9\n    print(f'State total_sum: {state.total_sum}')\n    #> State total_sum: 275\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/joins/#reducers-with-state-access", "page": "docs/graph/beta/joins", "source_site": "pydantic_ai"}
{"title": "Canceling Sibling Tasks", "anchor": "canceling-sibling-tasks", "heading_level": 3, "md_text": "Reducers with access to [`ReducerContext`][pydantic_graph.beta.join.ReducerContext] can call [`ctx.cancel_sibling_tasks()`][pydantic_graph.beta.join.ReducerContext.cancel_sibling_tasks] to cancel all other parallel tasks in the same fork. This is useful for early termination when you've found what you need:\n\n```python {title=\"cancel_siblings.py\"}\nimport asyncio\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import ReducerContext\n\n\n@dataclass\nclass SearchState:\n    searches_completed: int = 0\n\n\ndef reduce_find_match(ctx: ReducerContext[SearchState, None], current: str | None, inputs: str) -> str | None:\n    \"\"\"Return the first input that contains 'target' and cancel remaining tasks.\"\"\"\n    if current is not None:\n        # We already found a match, ignore subsequent inputs\n        return current\n    if 'target' in inputs:\n        # Found a match! Cancel all other parallel tasks\n        ctx.cancel_sibling_tasks()\n        return inputs\n    return None\n\n\nasync def main():\n    g = GraphBuilder(state_type=SearchState, output_type=str | None)\n\n    @g.step\n    async def generate_searches(ctx: StepContext[SearchState, None, None]) -> list[str]:\n        return ['item1', 'item2', 'target_item', 'item4', 'item5']\n\n    @g.step\n    async def search(ctx: StepContext[SearchState, None, str]) -> str:\n        \"\"\"Simulate a slow search operation.\"\"\"\n        # make the search artificially slower for 'item4' and 'item5'\n        search_duration = 0.1 if ctx.inputs not in {'item4', 'item5'} else 1.0\n        await asyncio.sleep(search_duration)\n        ctx.state.searches_completed += 1\n        return ctx.inputs\n\n    find_match = g.join(reduce_find_match, initial=None)\n\n    g.add(\n        g.edge_from(g.start_node).to(generate_searches),\n        g.edge_from(generate_searches).map().to(search),\n        g.edge_from(search).to(find_match),\n        g.edge_from(find_match).to(g.end_node),\n    )\n\n    graph = g.build()\n    state = SearchState()\n    result = await graph.run(state=state)\n\n    print(f'Found: {result}')\n    #> Found: target_item\n    print(f'Searches completed: {state.searches_completed}')\n    #> Searches completed: 3\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_\n\nNote that only 3 searches completed instead of all 5, because the reducer canceled the remaining tasks after finding a match.", "url": "https://ai.pydantic.dev/docs/graph/beta/joins/#canceling-sibling-tasks", "page": "docs/graph/beta/joins", "source_site": "pydantic_ai"}
{"title": "Multiple Joins", "anchor": "multiple-joins", "heading_level": 2, "md_text": "A graph can have multiple independent joins:\n\n```python {title=\"multiple_joins.py\"}\nfrom dataclasses import dataclass, field\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass MultiState:\n    results: dict[str, list[int]] = field(default_factory=dict)\n\n\nasync def main():\n    g = GraphBuilder(state_type=MultiState, output_type=dict[str, list[int]])\n\n    @g.step\n    async def source_a(ctx: StepContext[MultiState, None, None]) -> list[int]:\n        return [1, 2, 3]\n\n    @g.step\n    async def source_b(ctx: StepContext[MultiState, None, None]) -> list[int]:\n        return [10, 20]\n\n    @g.step\n    async def process_a(ctx: StepContext[MultiState, None, int]) -> int:\n        return ctx.inputs * 2\n\n    @g.step\n    async def process_b(ctx: StepContext[MultiState, None, int]) -> int:\n        return ctx.inputs * 3\n\n    join_a = g.join(reduce_list_append, initial_factory=list[int], node_id='join_a')\n    join_b = g.join(reduce_list_append, initial_factory=list[int], node_id='join_b')\n\n    @g.step\n    async def store_a(ctx: StepContext[MultiState, None, list[int]]) -> None:\n        ctx.state.results['a'] = ctx.inputs\n\n    @g.step\n    async def store_b(ctx: StepContext[MultiState, None, list[int]]) -> None:\n        ctx.state.results['b'] = ctx.inputs\n\n    @g.step\n    async def combine(ctx: StepContext[MultiState, None, None]) -> dict[str, list[int]]:\n        return ctx.state.results\n\n    g.add(\n        g.edge_from(g.start_node).to(source_a, source_b),\n        g.edge_from(source_a).map().to(process_a),\n        g.edge_from(source_b).map().to(process_b),\n        g.edge_from(process_a).to(join_a),\n        g.edge_from(process_b).to(join_b),\n        g.edge_from(join_a).to(store_a),\n        g.edge_from(join_b).to(store_b),\n        g.edge_from(store_a, store_b).to(combine),\n        g.edge_from(combine).to(g.end_node),\n    )\n\n    graph = g.build()\n    state = MultiState()\n    result = await graph.run(state=state)\n\n    print(f\"Group A: {sorted(result['a'])}\")\n    #> Group A: [2, 4, 6]\n    print(f\"Group B: {sorted(result['b'])}\")\n    #> Group B: [30, 60]\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/joins/#multiple-joins", "page": "docs/graph/beta/joins", "source_site": "pydantic_ai"}
{"title": "Custom Node IDs", "anchor": "custom-node-ids", "heading_level": 3, "md_text": "Like steps, joins can have custom IDs:\n\n```python {title=\"join_custom_id.py\" requires=\"basic_join.py\"}\nfrom pydantic_graph.beta.join import reduce_list_append\n\nfrom basic_join import g\n\nmy_join = g.join(reduce_list_append, initial_factory=list[int], node_id='my_custom_join_id')\n```", "url": "https://ai.pydantic.dev/docs/graph/beta/joins/#custom-node-ids", "page": "docs/graph/beta/joins", "source_site": "pydantic_ai"}
{"title": "How Joins Work", "anchor": "how-joins-work", "heading_level": 2, "md_text": "Internally, the graph tracks which \"fork\" each parallel task belongs to. A join:\n\n1. Identifies its parent fork (the fork that created the parallel paths)\n2. Waits for all tasks from that fork to reach the join\n3. Calls `reduce()` for each incoming value\n4. Calls `finalize()` once all values are received\n5. Passes the finalized result to downstream nodes\n\nThis ensures proper synchronization even with nested parallel operations.", "url": "https://ai.pydantic.dev/docs/graph/beta/joins/#how-joins-work", "page": "docs/graph/beta/joins", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "- Learn about [parallel execution](parallel.md) with broadcasting and mapping\n- Explore [conditional branching](decisions.md) with decision nodes\n- See the [API reference][pydantic_graph.beta.join] for complete reducer documentation", "url": "https://ai.pydantic.dev/docs/graph/beta/joins/#next-steps", "page": "docs/graph/beta/joins", "source_site": "pydantic_ai"}
{"title": "Beta Graph API", "anchor": "beta-graph-api", "heading_level": 1, "md_text": "!!! warning \"Beta API\"\n    This is the new beta graph API. It provides enhanced capabilities for parallel execution, conditional branching, and complex workflows.\nThe original graph API is still available (and compatible of interop with the new beta API) and is documented in the [main graph documentation](../../graph.md).", "url": "https://ai.pydantic.dev/docs/graph/beta/#beta-graph-api", "page": "docs/graph/beta/", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "The beta graph API in `pydantic-graph` provides a powerful builder pattern for constructing parallel execution graphs with:\n\n- **Step nodes** for executing async functions\n- **Decision nodes** for conditional branching\n- **Spread operations** for parallel processing of iterables\n- **Broadcast operations** for sending the same data to multiple parallel paths\n- **Join nodes and Reducers** for aggregating results from parallel execution\n\nThis API is designed for advanced workflows where you want declarative control over parallelism, routing, and data aggregation.", "url": "https://ai.pydantic.dev/docs/graph/beta/#overview", "page": "docs/graph/beta/", "source_site": "pydantic_ai"}
{"title": "Installation", "anchor": "installation", "heading_level": 2, "md_text": "The beta graph API is included with `pydantic-graph`:\n\n```bash\npip install pydantic-graph\n```\n\nOr as part of `pydantic-ai`:\n\n```bash\npip install pydantic-ai\n```", "url": "https://ai.pydantic.dev/docs/graph/beta/#installation", "page": "docs/graph/beta/", "source_site": "pydantic_ai"}
{"title": "Quick Start", "anchor": "quick-start", "heading_level": 2, "md_text": "Here's a simple example to get you started:\n\n```python {title=\"simple_counter.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\n@dataclass\nclass CounterState:\n    \"\"\"State for tracking a counter value.\"\"\"\n\n    value: int = 0\n\n\nasync def main():\n    # Create a graph builder with state and output types\n    g = GraphBuilder(state_type=CounterState, output_type=int)\n\n    # Define steps using the decorator\n    @g.step\n    async def increment(ctx: StepContext[CounterState, None, None]) -> int:\n        \"\"\"Increment the counter and return its value.\"\"\"\n        ctx.state.value += 1\n        return ctx.state.value\n\n    @g.step\n    async def double_it(ctx: StepContext[CounterState, None, int]) -> int:\n        \"\"\"Double the input value.\"\"\"\n        return ctx.inputs * 2\n\n    # Add edges connecting the nodes\n    g.add(\n        g.edge_from(g.start_node).to(increment),\n        g.edge_from(increment).to(double_it),\n        g.edge_from(double_it).to(g.end_node),\n    )\n\n    # Build and run the graph\n    graph = g.build()\n    state = CounterState()\n    result = await graph.run(state=state)\n    print(f'Result: {result}')\n    #> Result: 2\n    print(f'Final state: {state.value}')\n    #> Final state: 1\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/#quick-start", "page": "docs/graph/beta/", "source_site": "pydantic_ai"}
{"title": "GraphBuilder", "anchor": "graphbuilder", "heading_level": 3, "md_text": "The [`GraphBuilder`][pydantic_graph.beta.graph_builder.GraphBuilder] is the main entry point for constructing graphs. It's generic over:\n\n- `StateT` - The type of mutable state shared across all nodes\n- `DepsT` - The type of dependencies injected into nodes\n- `InputT` - The type of initial input to the graph\n- `OutputT` - The type of final output from the graph", "url": "https://ai.pydantic.dev/docs/graph/beta/#graphbuilder", "page": "docs/graph/beta/", "source_site": "pydantic_ai"}
{"title": "Steps", "anchor": "steps", "heading_level": 3, "md_text": "Steps are async functions decorated with [`@g.step`][pydantic_graph.beta.graph_builder.GraphBuilder.step] that define the actual work to be done in each node. They receive a [`StepContext`][pydantic_graph.beta.step.StepContext] with access to:\n\n- `ctx.state` - The mutable graph state\n- `ctx.deps` - Injected dependencies\n- `ctx.inputs` - Input data for this step", "url": "https://ai.pydantic.dev/docs/graph/beta/#steps", "page": "docs/graph/beta/", "source_site": "pydantic_ai"}
{"title": "Edges", "anchor": "edges", "heading_level": 3, "md_text": "Edges define the connections between nodes. The builder provides multiple ways to create edges:\n\n- [`g.add()`][pydantic_graph.beta.graph_builder.GraphBuilder.add] - Add one or more edge paths\n- [`g.add_edge()`][pydantic_graph.beta.graph_builder.GraphBuilder.add_edge] - Add a simple edge between two nodes\n- [`g.edge_from()`][pydantic_graph.beta.graph_builder.GraphBuilder.edge_from] - Start building a complex edge path", "url": "https://ai.pydantic.dev/docs/graph/beta/#edges", "page": "docs/graph/beta/", "source_site": "pydantic_ai"}
{"title": "Start and End Nodes", "anchor": "start-and-end-nodes", "heading_level": 3, "md_text": "Every graph has:\n\n- [`g.start_node`][pydantic_graph.beta.graph_builder.GraphBuilder.start_node] - The entry point receiving initial inputs\n- [`g.end_node`][pydantic_graph.beta.graph_builder.GraphBuilder.end_node] - The exit point producing final outputs", "url": "https://ai.pydantic.dev/docs/graph/beta/#start-and-end-nodes", "page": "docs/graph/beta/", "source_site": "pydantic_ai"}
{"title": "A More Complex Example", "anchor": "a-more-complex-example", "heading_level": 2, "md_text": "Here's an example showcasing parallel execution with a map operation:\n\n```python {title=\"parallel_processing.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass ProcessingState:\n    \"\"\"State for tracking processing metrics.\"\"\"\n\n    items_processed: int = 0\n\n\nasync def main():\n    g = GraphBuilder(\n        state_type=ProcessingState,\n        input_type=list[int],\n        output_type=list[int],\n    )\n\n    @g.step\n    async def square(ctx: StepContext[ProcessingState, None, int]) -> int:\n        \"\"\"Square a number and track that we processed it.\"\"\"\n        ctx.state.items_processed += 1\n        return ctx.inputs * ctx.inputs\n\n    # Create a join to collect results\n    collect_results = g.join(reduce_list_append, initial_factory=list[int])\n\n    # Build the graph with map operation\n    g.add(\n        g.edge_from(g.start_node).map().to(square),\n        g.edge_from(square).to(collect_results),\n        g.edge_from(collect_results).to(g.end_node),\n    )\n\n    graph = g.build()\n    state = ProcessingState()\n    result = await graph.run(state=state, inputs=[1, 2, 3, 4, 5])\n\n    print(f'Results: {sorted(result)}')\n    #> Results: [1, 4, 9, 16, 25]\n    print(f'Items processed: {state.items_processed}')\n    #> Items processed: 5\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_\n\nIn this example:\n\n1. The start node receives a list of integers\n2. The `.map()` operation fans out each item to a separate parallel execution of the `square` step\n3. All results are collected back together using [`reduce_list_append`][pydantic_graph.beta.join.reduce_list_append]\n4. The joined results flow to the end node", "url": "https://ai.pydantic.dev/docs/graph/beta/#a-more-complex-example", "page": "docs/graph/beta/", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "Explore the detailed documentation for each feature:\n\n- [**Steps**](steps.md) - Learn about step nodes and execution contexts\n- [**Joins**](joins.md) - Understand join nodes and reducer patterns\n- [**Decisions**](decisions.md) - Implement conditional branching\n- [**Parallel Execution**](parallel.md) - Master broadcasting and mapping", "url": "https://ai.pydantic.dev/docs/graph/beta/#next-steps", "page": "docs/graph/beta/", "source_site": "pydantic_ai"}
{"title": "Advanced Execution Control", "anchor": "advanced-execution-control", "heading_level": 2, "md_text": "Beyond the basic [`graph.run()`][pydantic_graph.beta.graph.Graph.run] method, the beta API provides fine-grained control over graph execution.", "url": "https://ai.pydantic.dev/docs/graph/beta/#advanced-execution-control", "page": "docs/graph/beta/", "source_site": "pydantic_ai"}
{"title": "Step-by-Step Execution", "anchor": "step-by-step-execution", "heading_level": 3, "md_text": "Use [`graph.iter()`][pydantic_graph.beta.graph.Graph.iter] to execute the graph one step at a time:\n\n```python {title=\"step_by_step.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\n@dataclass\nclass CounterState:\n    value: int = 0\n\n\nasync def main():\n    g = GraphBuilder(state_type=CounterState, output_type=int)\n\n    @g.step\n    async def increment(ctx: StepContext[CounterState, None, None]) -> int:\n        ctx.state.value += 1\n        return ctx.state.value\n\n    @g.step\n    async def double_it(ctx: StepContext[CounterState, None, int]) -> int:\n        return ctx.inputs * 2\n\n    g.add(\n        g.edge_from(g.start_node).to(increment),\n        g.edge_from(increment).to(double_it),\n        g.edge_from(double_it).to(g.end_node),\n    )\n\n    graph = g.build()\n    state = CounterState()\n\n    # Use iter() for step-by-step execution\n    async with graph.iter(state=state) as graph_run:\n        print(f'Initial state: {state.value}')\n        #> Initial state: 0\n\n        # Advance execution step by step\n        async for event in graph_run:\n            print(f'{state.value=} | {event=}')\n            #> state.value=0 | event=[GraphTask(node_id='increment', inputs=None)]\n            #> state.value=1 | event=[GraphTask(node_id='double_it', inputs=1)]\n            #> state.value=1 | event=[GraphTask(node_id='__end__', inputs=2)]\n            #> state.value=1 | event=EndMarker(_value=2)\n            if graph_run.output is not None:\n                print(f'Final output: {graph_run.output}')\n                #> Final output: 2\n                break\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_\n\nThe [`GraphRun`][pydantic_graph.beta.graph.GraphRun] object provides:\n\n- **Async iteration**: Iterate through execution events\n- **`next_task` property**: Inspect upcoming tasks\n- **`output` property**: Check if the graph has completed and get the final output\n- **`next()` method**: Manually advance execution with optional value injection", "url": "https://ai.pydantic.dev/docs/graph/beta/#step-by-step-execution", "page": "docs/graph/beta/", "source_site": "pydantic_ai"}
{"title": "Visualizing Graphs", "anchor": "visualizing-graphs", "heading_level": 3, "md_text": "Generate Mermaid diagrams of your graph structure using [`graph.render()`][pydantic_graph.beta.graph.Graph.render]:\n\n```python {title=\"visualize_graph.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\ng = GraphBuilder(state_type=SimpleState, output_type=str)\n\n@g.step\nasync def step_a(ctx: StepContext[SimpleState, None, None]) -> int:\n    return 10\n\n@g.step\nasync def step_b(ctx: StepContext[SimpleState, None, int]) -> str:\n    return f'Result: {ctx.inputs}'\n\ng.add(\n    g.edge_from(g.start_node).to(step_a),\n    g.edge_from(step_a).to(step_b),\n    g.edge_from(step_b).to(g.end_node),\n)\n\ngraph = g.build()", "url": "https://ai.pydantic.dev/docs/graph/beta/#visualizing-graphs", "page": "docs/graph/beta/", "source_site": "pydantic_ai"}
{"title": "Generate a Mermaid diagram", "anchor": "generate-a-mermaid-diagram", "heading_level": 1, "md_text": "mermaid_diagram = graph.render(title='My Graph', direction='LR')\nprint(mermaid_diagram)\n\"\"\"\n---\ntitle: My Graph\n---\nstateDiagram-v2\n  direction LR\n  step_a\n  step_b\n\n  [*] --> step_a\n  step_a --> step_b\n  step_b --> [*]\n\"\"\"\n```\n\nThe rendered diagram can be displayed in documentation, notebooks, or any tool that supports Mermaid syntax.", "url": "https://ai.pydantic.dev/docs/graph/beta/#generate-a-mermaid-diagram", "page": "docs/graph/beta/", "source_site": "pydantic_ai"}
{"title": "Comparison with Original API", "anchor": "comparison-with-original-api", "heading_level": 2, "md_text": "The original graph API (documented in the [main graph page](../../graph.md)) uses a class-based approach with [`BaseNode`][pydantic_graph.nodes.BaseNode] subclasses. The beta API uses a builder pattern with decorated functions, which provides:\n\n**Advantages:**\n- More concise syntax for simple workflows\n- Explicit control over parallelism with map/broadcast\n- Built-in reducers for common aggregation patterns\n- Easier to visualize complex data flows\n\n**Trade-offs:**\n- Requires understanding of builder patterns\n- Less object-oriented, more functional style\n\nBoth APIs are fully supported and can even be integrated together when needed.", "url": "https://ai.pydantic.dev/docs/graph/beta/#comparison-with-original-api", "page": "docs/graph/beta/", "source_site": "pydantic_ai"}
{"title": "Parallel Execution", "anchor": "parallel-execution", "heading_level": 1, "md_text": "The beta graph API provides two powerful mechanisms for parallel execution: **broadcasting** and **mapping**.", "url": "https://ai.pydantic.dev/docs/graph/beta/parallel/#parallel-execution", "page": "docs/graph/beta/parallel", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "- **Broadcasting** - Send the same data to multiple parallel paths\n- **Spreading** - Fan out items from an iterable to parallel paths\n\nBoth create \"forks\" in the execution graph that can later be synchronized with [join nodes](joins.md).", "url": "https://ai.pydantic.dev/docs/graph/beta/parallel/#overview", "page": "docs/graph/beta/parallel", "source_site": "pydantic_ai"}
{"title": "Broadcasting", "anchor": "broadcasting", "heading_level": 2, "md_text": "Broadcasting sends identical data to multiple destinations simultaneously:\n\n```python {title=\"basic_broadcast.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=list[int])\n\n    @g.step\n    async def source(ctx: StepContext[SimpleState, None, None]) -> int:\n        return 10\n\n    @g.step\n    async def add_one(ctx: StepContext[SimpleState, None, int]) -> int:\n        return ctx.inputs + 1\n\n    @g.step\n    async def add_two(ctx: StepContext[SimpleState, None, int]) -> int:\n        return ctx.inputs + 2\n\n    @g.step\n    async def add_three(ctx: StepContext[SimpleState, None, int]) -> int:\n        return ctx.inputs + 3\n\n    collect = g.join(reduce_list_append, initial_factory=list[int])\n\n    # Broadcasting: send the value from source to all three steps\n    g.add(\n        g.edge_from(g.start_node).to(source),\n        g.edge_from(source).to(add_one, add_two, add_three),\n        g.edge_from(add_one, add_two, add_three).to(collect),\n        g.edge_from(collect).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(sorted(result))\n    #> [11, 12, 13]\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_\n\nAll three steps receive the same input value (`10`) and execute in parallel.", "url": "https://ai.pydantic.dev/docs/graph/beta/parallel/#broadcasting", "page": "docs/graph/beta/parallel", "source_site": "pydantic_ai"}
{"title": "Spreading", "anchor": "spreading", "heading_level": 2, "md_text": "Spreading fans out elements from an iterable, processing each element in parallel:\n\n```python {title=\"basic_map.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=list[int])\n\n    @g.step\n    async def generate_list(ctx: StepContext[SimpleState, None, None]) -> list[int]:\n        return [1, 2, 3, 4, 5]\n\n    @g.step\n    async def square(ctx: StepContext[SimpleState, None, int]) -> int:\n        return ctx.inputs * ctx.inputs\n\n    collect = g.join(reduce_list_append, initial_factory=list[int])\n\n    # Spreading: each item in the list gets its own parallel execution\n    g.add(\n        g.edge_from(g.start_node).to(generate_list),\n        g.edge_from(generate_list).map().to(square),\n        g.edge_from(square).to(collect),\n        g.edge_from(collect).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(sorted(result))\n    #> [1, 4, 9, 16, 25]\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/parallel/#spreading", "page": "docs/graph/beta/parallel", "source_site": "pydantic_ai"}
{"title": "Spreading AsyncIterables", "anchor": "spreading-asynciterables", "heading_level": 3, "md_text": "The `.map()` operation also works with `AsyncIterable` values. When mapping over an async iterable, the graph creates parallel tasks dynamically as values are yielded. This is particularly useful for streaming data or processing data that's being generated on-the-fly:\n\n```python {title=\"async_iterable_map.py\"}\nimport asyncio\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=list[int])\n\n    @g.stream\n    async def stream_numbers(ctx: StepContext[SimpleState, None, None]):\n        \"\"\"Stream numbers with delays to simulate real-time data.\"\"\"\n        for i in range(1, 4):\n            await asyncio.sleep(0.05)  # Simulate delay\n            yield i\n\n    @g.step\n    async def triple(ctx: StepContext[SimpleState, None, int]) -> int:\n        return ctx.inputs * 3\n\n    collect = g.join(reduce_list_append, initial_factory=list[int])\n\n    g.add(\n        g.edge_from(g.start_node).to(stream_numbers),\n        # Map over the async iterable - tasks created as items are yielded\n        g.edge_from(stream_numbers).map().to(triple),\n        g.edge_from(triple).to(collect),\n        g.edge_from(collect).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(sorted(result))\n    #> [3, 6, 9]\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_\n\nThis allows for progressive processing where downstream steps can start working on early results while later results are still being generated.", "url": "https://ai.pydantic.dev/docs/graph/beta/parallel/#spreading-asynciterables", "page": "docs/graph/beta/parallel", "source_site": "pydantic_ai"}
{"title": "Using `add_mapping_edge()`", "anchor": "using-add_mapping_edge", "heading_level": 3, "md_text": "The convenience method [`add_mapping_edge()`][pydantic_graph.beta.graph_builder.GraphBuilder.add_mapping_edge] provides a simpler syntax:\n\n```python {title=\"mapping_convenience.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=list[str])\n\n    @g.step\n    async def generate_numbers(ctx: StepContext[SimpleState, None, None]) -> list[int]:\n        return [10, 20, 30]\n\n    @g.step\n    async def stringify(ctx: StepContext[SimpleState, None, int]) -> str:\n        return f'Value: {ctx.inputs}'\n\n    collect = g.join(reduce_list_append, initial_factory=list[str])\n\n    g.add(g.edge_from(g.start_node).to(generate_numbers))\n    g.add_mapping_edge(generate_numbers, stringify)\n    g.add(\n        g.edge_from(stringify).to(collect),\n        g.edge_from(collect).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(sorted(result))\n    #> ['Value: 10', 'Value: 20', 'Value: 30']\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/parallel/#using-add_mapping_edge", "page": "docs/graph/beta/parallel", "source_site": "pydantic_ai"}
{"title": "Empty Iterables", "anchor": "empty-iterables", "heading_level": 2, "md_text": "When mapping an empty iterable, you can specify a `downstream_join_id` to ensure the join still executes:\n\n```python {title=\"empty_map.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=list[int])\n\n    @g.step\n    async def generate_empty(ctx: StepContext[SimpleState, None, None]) -> list[int]:\n        return []\n\n    @g.step\n    async def double(ctx: StepContext[SimpleState, None, int]) -> int:\n        return ctx.inputs * 2\n\n    collect = g.join(reduce_list_append, initial_factory=list[int])\n\n    g.add(g.edge_from(g.start_node).to(generate_empty))\n    g.add_mapping_edge(generate_empty, double, downstream_join_id=collect.id)\n    g.add(\n        g.edge_from(double).to(collect),\n        g.edge_from(collect).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(result)\n    #> []\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/parallel/#empty-iterables", "page": "docs/graph/beta/parallel", "source_site": "pydantic_ai"}
{"title": "Nested Parallel Operations", "anchor": "nested-parallel-operations", "heading_level": 2, "md_text": "You can nest broadcasts and maps for complex parallel patterns:", "url": "https://ai.pydantic.dev/docs/graph/beta/parallel/#nested-parallel-operations", "page": "docs/graph/beta/parallel", "source_site": "pydantic_ai"}
{"title": "Spread then Broadcast", "anchor": "spread-then-broadcast", "heading_level": 3, "md_text": "```python {title=\"map_then_broadcast.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=list[int])\n\n    @g.step\n    async def generate_list(ctx: StepContext[SimpleState, None, None]) -> list[int]:\n        return [10, 20]\n\n    @g.step\n    async def add_one(ctx: StepContext[SimpleState, None, int]) -> int:\n        return ctx.inputs + 1\n\n    @g.step\n    async def add_two(ctx: StepContext[SimpleState, None, int]) -> int:\n        return ctx.inputs + 2\n\n    collect = g.join(reduce_list_append, initial_factory=list[int])\n\n    g.add(\n        g.edge_from(g.start_node).to(generate_list),\n        # Spread the list, then broadcast each item to both steps\n        g.edge_from(generate_list).map().to(add_one, add_two),\n        g.edge_from(add_one, add_two).to(collect),\n        g.edge_from(collect).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(sorted(result))\n    #> [11, 12, 21, 22]\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_\n\nThe result contains:\n- From 10: `10+1=11` and `10+2=12`\n- From 20: `20+1=21` and `20+2=22`", "url": "https://ai.pydantic.dev/docs/graph/beta/parallel/#spread-then-broadcast", "page": "docs/graph/beta/parallel", "source_site": "pydantic_ai"}
{"title": "Multiple Sequential Spreads", "anchor": "multiple-sequential-spreads", "heading_level": 3, "md_text": "```python {title=\"sequential_maps.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=list[str])\n\n    @g.step\n    async def generate_pairs(ctx: StepContext[SimpleState, None, None]) -> list[tuple[int, int]]:\n        return [(1, 2), (3, 4)]\n\n    @g.step\n    async def unpack_pair(ctx: StepContext[SimpleState, None, tuple[int, int]]) -> list[int]:\n        return [ctx.inputs[0], ctx.inputs[1]]\n\n    @g.step\n    async def stringify(ctx: StepContext[SimpleState, None, int]) -> str:\n        return f'num:{ctx.inputs}'\n\n    collect = g.join(reduce_list_append, initial_factory=list[str])\n\n    g.add(\n        g.edge_from(g.start_node).to(generate_pairs),\n        # First map: one task per tuple\n        g.edge_from(generate_pairs).map().to(unpack_pair),\n        # Second map: one task per number in each tuple\n        g.edge_from(unpack_pair).map().to(stringify),\n        g.edge_from(stringify).to(collect),\n        g.edge_from(collect).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(sorted(result))\n    #> ['num:1', 'num:2', 'num:3', 'num:4']\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/parallel/#multiple-sequential-spreads", "page": "docs/graph/beta/parallel", "source_site": "pydantic_ai"}
{"title": "Edge Labels", "anchor": "edge-labels", "heading_level": 2, "md_text": "Add labels to parallel edges for better documentation:\n\n```python {title=\"labeled_parallel.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=list[str])\n\n    @g.step\n    async def generate(ctx: StepContext[SimpleState, None, None]) -> list[int]:\n        return [1, 2, 3]\n\n    @g.step\n    async def process(ctx: StepContext[SimpleState, None, int]) -> str:\n        return f'item-{ctx.inputs}'\n\n    collect = g.join(reduce_list_append, initial_factory=list[str])\n\n    g.add(g.edge_from(g.start_node).to(generate))\n    g.add_mapping_edge(\n        generate,\n        process,\n        pre_map_label='before map',\n        post_map_label='after map',\n    )\n    g.add(\n        g.edge_from(process).to(collect),\n        g.edge_from(collect).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(sorted(result))\n    #> ['item-1', 'item-2', 'item-3']\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/parallel/#edge-labels", "page": "docs/graph/beta/parallel", "source_site": "pydantic_ai"}
{"title": "State Sharing in Parallel Execution", "anchor": "state-sharing-in-parallel-execution", "heading_level": 2, "md_text": "All parallel tasks share the same graph state. Be careful with mutations:\n\n```python {title=\"parallel_state.py\"}\nfrom dataclasses import dataclass, field\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass CounterState:\n    values: list[int] = field(default_factory=list)\n\n\nasync def main():\n    g = GraphBuilder(state_type=CounterState, output_type=list[int])\n\n    @g.step\n    async def generate(ctx: StepContext[CounterState, None, None]) -> list[int]:\n        return [1, 2, 3]\n\n    @g.step\n    async def track_and_square(ctx: StepContext[CounterState, None, int]) -> int:\n        # All parallel tasks mutate the same state\n        ctx.state.values.append(ctx.inputs)\n        return ctx.inputs * ctx.inputs\n\n    collect = g.join(reduce_list_append, initial_factory=list[int])\n\n    g.add(\n        g.edge_from(g.start_node).to(generate),\n        g.edge_from(generate).map().to(track_and_square),\n        g.edge_from(track_and_square).to(collect),\n        g.edge_from(collect).to(g.end_node),\n    )\n\n    graph = g.build()\n    state = CounterState()\n    result = await graph.run(state=state)\n\n    print(f'Squared: {sorted(result)}')\n    #> Squared: [1, 4, 9]\n    print(f'Tracked: {sorted(state.values)}')\n    #> Tracked: [1, 2, 3]\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/parallel/#state-sharing-in-parallel-execution", "page": "docs/graph/beta/parallel", "source_site": "pydantic_ai"}
{"title": "Edge Transformations", "anchor": "edge-transformations", "heading_level": 2, "md_text": "You can transform data inline as it flows along edges using the `.transform()` method:\n\n```python {title=\"edge_transform.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=str)\n\n    @g.step\n    async def generate_number(ctx: StepContext[SimpleState, None, None]) -> int:\n        return 42\n\n    @g.step\n    async def format_output(ctx: StepContext[SimpleState, None, str]) -> str:\n        return f'The answer is: {ctx.inputs}'\n\n    # Transform the number to a string inline\n    g.add(\n        g.edge_from(g.start_node).to(generate_number),\n        g.edge_from(generate_number).transform(lambda ctx: str(ctx.inputs * 2)).to(format_output),\n        g.edge_from(format_output).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(result)\n    #> The answer is: 84\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_\n\nThe transform function receives a [`StepContext`][pydantic_graph.beta.step.StepContext] with the current inputs and has access to state and dependencies. This is useful for:\n\n- Converting data types between incompatible steps\n- Extracting specific fields from complex objects\n- Applying simple computations without creating a full step\n- Adapting data formats during routing\n\nTransforms can be chained and combined with other edge operations like `.map()` and `.label()`:\n\n```python {title=\"chained_transforms.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=list[str])\n\n    @g.step\n    async def generate_data(ctx: StepContext[SimpleState, None, None]) -> list[dict[str, int]]:\n        return [{'value': 10}, {'value': 20}, {'value': 30}]\n\n    @g.step\n    async def process_number(ctx: StepContext[SimpleState, None, int]) -> str:\n        return f'Processed: {ctx.inputs}'\n\n    collect = g.join(reduce_list_append, initial_factory=list[str])\n\n    g.add(\n        g.edge_from(g.start_node).to(generate_data),\n        # Transform to extract values, then map over them\n        g.edge_from(generate_data)\n        .transform(lambda ctx: [item['value'] for item in ctx.inputs])\n        .label('Extract values')\n        .map()\n        .to(process_number),\n        g.edge_from(process_number).to(collect),\n        g.edge_from(collect).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(sorted(result))\n    #> ['Processed: 10', 'Processed: 20', 'Processed: 30']\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/parallel/#edge-transformations", "page": "docs/graph/beta/parallel", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "- Learn about [join nodes](joins.md) for aggregating parallel results\n- Explore [conditional branching](decisions.md) with decision nodes\n- See the [steps documentation](steps.md) for more on step execution", "url": "https://ai.pydantic.dev/docs/graph/beta/parallel/#next-steps", "page": "docs/graph/beta/parallel", "source_site": "pydantic_ai"}
{"title": "Steps", "anchor": "steps", "heading_level": 1, "md_text": "Steps are the fundamental units of work in a graph. They're async functions that receive a [`StepContext`][pydantic_graph.beta.step.StepContext] and return a value.", "url": "https://ai.pydantic.dev/docs/graph/beta/steps/#steps", "page": "docs/graph/beta/steps", "source_site": "pydantic_ai"}
{"title": "Creating Steps", "anchor": "creating-steps", "heading_level": 2, "md_text": "Steps are created using the [`@g.step`][pydantic_graph.beta.graph_builder.GraphBuilder.step] decorator on the [`GraphBuilder`][pydantic_graph.beta.graph_builder.GraphBuilder]:\n\n```python {title=\"basic_step.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\n@dataclass\nclass MyState:\n    counter: int = 0\n\ng = GraphBuilder(state_type=MyState, output_type=int)\n\n@g.step\nasync def increment(ctx: StepContext[MyState, None, None]) -> int:\n    ctx.state.counter += 1\n    return ctx.state.counter\n\ng.add(\n    g.edge_from(g.start_node).to(increment),\n    g.edge_from(increment).to(g.end_node),\n)\n\ngraph = g.build()\n\nasync def main():\n    state = MyState()\n    result = await graph.run(state=state)\n    print(result)\n    #> 1\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/steps/#creating-steps", "page": "docs/graph/beta/steps", "source_site": "pydantic_ai"}
{"title": "Step Context", "anchor": "step-context", "heading_level": 2, "md_text": "Every step function receives a [`StepContext`][pydantic_graph.beta.step.StepContext] as its first parameter. The context provides access to:\n\n- `ctx.state` - The mutable graph state (type: `StateT`)\n- `ctx.deps` - Injected dependencies (type: `DepsT`)\n- `ctx.inputs` - Input data for this step (type: `InputT`)", "url": "https://ai.pydantic.dev/docs/graph/beta/steps/#step-context", "page": "docs/graph/beta/steps", "source_site": "pydantic_ai"}
{"title": "Accessing State", "anchor": "accessing-state", "heading_level": 3, "md_text": "State is shared across all steps in a graph and can be freely mutated:\n\n```python {title=\"state_access.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\n@dataclass\nclass AppState:\n    messages: list[str]\n\n\nasync def main():\n    g = GraphBuilder(state_type=AppState, output_type=list[str])\n\n    @g.step\n    async def add_hello(ctx: StepContext[AppState, None, None]) -> None:\n        ctx.state.messages.append('Hello')\n\n    @g.step\n    async def add_world(ctx: StepContext[AppState, None, None]) -> None:\n        ctx.state.messages.append('World')\n\n    @g.step\n    async def get_messages(ctx: StepContext[AppState, None, None]) -> list[str]:\n        return ctx.state.messages\n\n    g.add(\n        g.edge_from(g.start_node).to(add_hello),\n        g.edge_from(add_hello).to(add_world),\n        g.edge_from(add_world).to(get_messages),\n        g.edge_from(get_messages).to(g.end_node),\n    )\n\n    graph = g.build()\n    state = AppState(messages=[])\n    result = await graph.run(state=state)\n    print(result)\n    #> ['Hello', 'World']\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/steps/#accessing-state", "page": "docs/graph/beta/steps", "source_site": "pydantic_ai"}
{"title": "Working with Inputs", "anchor": "working-with-inputs", "heading_level": 3, "md_text": "Steps can receive and transform input data:\n\n```python {title=\"step_inputs.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(\n        state_type=SimpleState,\n        input_type=int,\n        output_type=str,\n    )\n\n    @g.step\n    async def double_it(ctx: StepContext[SimpleState, None, int]) -> int:\n        \"\"\"Double the input value.\"\"\"\n        return ctx.inputs * 2\n\n    @g.step\n    async def stringify(ctx: StepContext[SimpleState, None, int]) -> str:\n        \"\"\"Convert to a formatted string.\"\"\"\n        return f'Result: {ctx.inputs}'\n\n    g.add(\n        g.edge_from(g.start_node).to(double_it),\n        g.edge_from(double_it).to(stringify),\n        g.edge_from(stringify).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState(), inputs=21)\n    print(result)\n    #> Result: 42\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/steps/#working-with-inputs", "page": "docs/graph/beta/steps", "source_site": "pydantic_ai"}
{"title": "Dependency Injection", "anchor": "dependency-injection", "heading_level": 2, "md_text": "Steps can access injected dependencies through `ctx.deps`:\n\n```python {title=\"dependencies.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\n@dataclass\nclass AppState:\n    pass\n\n\n@dataclass\nclass AppDeps:\n    \"\"\"Dependencies injected into the graph.\"\"\"\n\n    multiplier: int\n\n\nasync def main():\n    g = GraphBuilder(\n        state_type=AppState,\n        deps_type=AppDeps,\n        input_type=int,\n        output_type=int,\n    )\n\n    @g.step\n    async def multiply(ctx: StepContext[AppState, AppDeps, int]) -> int:\n        \"\"\"Multiply input by the injected multiplier.\"\"\"\n        return ctx.inputs * ctx.deps.multiplier\n\n    g.add(\n        g.edge_from(g.start_node).to(multiply),\n        g.edge_from(multiply).to(g.end_node),\n    )\n\n    graph = g.build()\n    deps = AppDeps(multiplier=10)\n    result = await graph.run(state=AppState(), deps=deps, inputs=5)\n    print(result)\n    #> 50\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/steps/#dependency-injection", "page": "docs/graph/beta/steps", "source_site": "pydantic_ai"}
{"title": "Custom Node IDs", "anchor": "custom-node-ids", "heading_level": 3, "md_text": "By default, step node IDs are inferred from the function name. You can override this:\n\n```python {title=\"custom_id.py\" requires=\"basic_step.py\"}\nfrom pydantic_graph.beta import StepContext\n\nfrom basic_step import MyState, g\n\n\n@g.step(node_id='my_custom_id')\nasync def my_step(ctx: StepContext[MyState, None, None]) -> int:\n    return 42", "url": "https://ai.pydantic.dev/docs/graph/beta/steps/#custom-node-ids", "page": "docs/graph/beta/steps", "source_site": "pydantic_ai"}
{"title": "The node ID is now 'my_custom_id' instead of 'my_step'", "anchor": "the-node-id-is-now-my_custom_id-instead-of-my_step", "heading_level": 1, "md_text": "```", "url": "https://ai.pydantic.dev/docs/graph/beta/steps/#the-node-id-is-now-my_custom_id-instead-of-my_step", "page": "docs/graph/beta/steps", "source_site": "pydantic_ai"}
{"title": "Human-Readable Labels", "anchor": "human-readable-labels", "heading_level": 3, "md_text": "Labels provide documentation for diagram generation:\n\n```python {title=\"labels.py\" requires=\"basic_step.py\"}\nfrom pydantic_graph.beta import StepContext\n\nfrom basic_step import MyState, g\n\n\n@g.step(label='Increment the counter')\nasync def increment(ctx: StepContext[MyState, None, None]) -> int:\n    ctx.state.counter += 1\n    return ctx.state.counter", "url": "https://ai.pydantic.dev/docs/graph/beta/steps/#human-readable-labels", "page": "docs/graph/beta/steps", "source_site": "pydantic_ai"}
{"title": "Access the label programmatically", "anchor": "access-the-label-programmatically", "heading_level": 1, "md_text": "print(increment.label)\n#> Increment the counter\n```", "url": "https://ai.pydantic.dev/docs/graph/beta/steps/#access-the-label-programmatically", "page": "docs/graph/beta/steps", "source_site": "pydantic_ai"}
{"title": "Sequential Steps", "anchor": "sequential-steps", "heading_level": 2, "md_text": "Multiple steps can be chained sequentially:\n\n```python {title=\"sequential.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\n@dataclass\nclass MathState:\n    operations: list[str]\n\n\nasync def main():\n    g = GraphBuilder(\n        state_type=MathState,\n        input_type=int,\n        output_type=int,\n    )\n\n    @g.step\n    async def add_five(ctx: StepContext[MathState, None, int]) -> int:\n        ctx.state.operations.append('add 5')\n        return ctx.inputs + 5\n\n    @g.step\n    async def multiply_by_two(ctx: StepContext[MathState, None, int]) -> int:\n        ctx.state.operations.append('multiply by 2')\n        return ctx.inputs * 2\n\n    @g.step\n    async def subtract_three(ctx: StepContext[MathState, None, int]) -> int:\n        ctx.state.operations.append('subtract 3')\n        return ctx.inputs - 3\n\n    # Connect steps sequentially\n    g.add(\n        g.edge_from(g.start_node).to(add_five),\n        g.edge_from(add_five).to(multiply_by_two),\n        g.edge_from(multiply_by_two).to(subtract_three),\n        g.edge_from(subtract_three).to(g.end_node),\n    )\n\n    graph = g.build()\n    state = MathState(operations=[])\n    result = await graph.run(state=state, inputs=10)\n\n    print(f'Result: {result}')\n    #> Result: 27\n    print(f'Operations: {state.operations}')\n    #> Operations: ['add 5', 'multiply by 2', 'subtract 3']\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_\n\nThe computation is: `(10 + 5) * 2 - 3 = 27`", "url": "https://ai.pydantic.dev/docs/graph/beta/steps/#sequential-steps", "page": "docs/graph/beta/steps", "source_site": "pydantic_ai"}
{"title": "Streaming Steps", "anchor": "streaming-steps", "heading_level": 2, "md_text": "In addition to regular steps that return a single value, you can create streaming steps that yield multiple values over time using the [`@g.stream`][pydantic_graph.beta.graph_builder.GraphBuilder.stream] decorator:\n\n```python {title=\"streaming_step.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\ng = GraphBuilder(state_type=SimpleState, output_type=list[int])\n\n@g.stream\nasync def generate_stream(ctx: StepContext[SimpleState, None, None]):\n    \"\"\"Stream numbers from 1 to 5.\"\"\"\n    for i in range(1, 6):\n        yield i\n\n@g.step\nasync def square(ctx: StepContext[SimpleState, None, int]) -> int:\n    return ctx.inputs * ctx.inputs\n\ncollect = g.join(reduce_list_append, initial_factory=list[int])\n\ng.add(\n    g.edge_from(g.start_node).to(generate_stream),\n    # The stream output is an AsyncIterable, so we can map over it\n    g.edge_from(generate_stream).map().to(square),\n    g.edge_from(square).to(collect),\n    g.edge_from(collect).to(g.end_node),\n)\n\ngraph = g.build()\n\nasync def main():\n    result = await graph.run(state=SimpleState())\n    print(sorted(result))\n    #> [1, 4, 9, 16, 25]\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/steps/#streaming-steps", "page": "docs/graph/beta/steps", "source_site": "pydantic_ai"}
{"title": "How Streaming Steps Work", "anchor": "how-streaming-steps-work", "heading_level": 3, "md_text": "Streaming steps return an `AsyncIterable` that yields values over time. When you use `.map()` on a streaming step's output, the graph processes each yielded value as it becomes available, creating parallel tasks dynamically. This is particularly useful for:\n\n- Processing data from APIs that stream responses\n- Handling real-time data feeds\n- Progressive processing of large datasets\n- Any scenario where you want to start processing results before all data is available\n\nLike regular steps, streaming steps can also have custom node IDs and labels:\n\n```python {title=\"labeled_stream.py\" requires=\"streaming_step.py\"}\nfrom pydantic_graph.beta import StepContext\n\nfrom streaming_step import SimpleState, g\n\n\n@g.stream(node_id='my_stream', label='Generate numbers progressively')\nasync def labeled_stream(ctx: StepContext[SimpleState, None, None]):\n    for i in range(10):\n        yield i\n```", "url": "https://ai.pydantic.dev/docs/graph/beta/steps/#how-streaming-steps-work", "page": "docs/graph/beta/steps", "source_site": "pydantic_ai"}
{"title": "Edge Building Convenience Methods", "anchor": "edge-building-convenience-methods", "heading_level": 2, "md_text": "The builder provides helper methods for common edge patterns:", "url": "https://ai.pydantic.dev/docs/graph/beta/steps/#edge-building-convenience-methods", "page": "docs/graph/beta/steps", "source_site": "pydantic_ai"}
{"title": "Simple Edges with `add_edge()`", "anchor": "simple-edges-with-add_edge", "heading_level": 3, "md_text": "```python {title=\"add_edge_example.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=int)\n\n    @g.step\n    async def step_a(ctx: StepContext[SimpleState, None, None]) -> int:\n        return 10\n\n    @g.step\n    async def step_b(ctx: StepContext[SimpleState, None, int]) -> int:\n        return ctx.inputs + 5\n\n    # Using add_edge() for simple connections\n    g.add_edge(g.start_node, step_a)\n    g.add_edge(step_a, step_b, label='from a to b')\n    g.add_edge(step_b, g.end_node)\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(result)\n    #> 15\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/steps/#simple-edges-with-add_edge", "page": "docs/graph/beta/steps", "source_site": "pydantic_ai"}
{"title": "Type Safety", "anchor": "type-safety", "heading_level": 2, "md_text": "The beta graph API provides strong type checking through generics. Type parameters on [`StepContext`][pydantic_graph.beta.step.StepContext] ensure:\n\n- State access is properly typed\n- Dependencies are correctly typed\n- Input/output types match across edges\n\n```python\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\n@dataclass\nclass MyState:\n    pass\n\ng = GraphBuilder(state_type=MyState, output_type=str)", "url": "https://ai.pydantic.dev/docs/graph/beta/steps/#type-safety", "page": "docs/graph/beta/steps", "source_site": "pydantic_ai"}
{"title": "Type checker will catch mismatches", "anchor": "type-checker-will-catch-mismatches", "heading_level": 1, "md_text": "@g.step\nasync def expects_int(ctx: StepContext[MyState, None, int]) -> str:\n    return str(ctx.inputs)\n\n@g.step\nasync def returns_str(ctx: StepContext[MyState, None, None]) -> str:\n    return 'hello'", "url": "https://ai.pydantic.dev/docs/graph/beta/steps/#type-checker-will-catch-mismatches", "page": "docs/graph/beta/steps", "source_site": "pydantic_ai"}
{"title": "g.add(g.edge_from(returns_str).to(expects_int))  # Type error!", "anchor": "gaddgedge_fromreturns_strtoexpects_int-type-error", "heading_level": 1, "md_text": "```", "url": "https://ai.pydantic.dev/docs/graph/beta/steps/#gaddgedge_fromreturns_strtoexpects_int-type-error", "page": "docs/graph/beta/steps", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "- Learn about [parallel execution](parallel.md) with broadcasting and mapping\n- Understand [join nodes](joins.md) for aggregating parallel results\n- Explore [conditional branching](decisions.md) with decision nodes", "url": "https://ai.pydantic.dev/docs/graph/beta/steps/#next-steps", "page": "docs/graph/beta/steps", "source_site": "pydantic_ai"}
{"title": "Decision Nodes", "anchor": "decision-nodes", "heading_level": 1, "md_text": "Decision nodes enable conditional branching in your graph based on the type or value of data flowing through it.", "url": "https://ai.pydantic.dev/docs/graph/beta/decisions/#decision-nodes", "page": "docs/graph/beta/decisions", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "A decision node evaluates incoming data and routes it to different branches based on:\n\n- Type matching (using `isinstance`)\n- Literal value matching\n- Custom predicate functions\n\nThe first matching branch is taken, similar to pattern matching or `if-elif-else` chains.", "url": "https://ai.pydantic.dev/docs/graph/beta/decisions/#overview", "page": "docs/graph/beta/decisions", "source_site": "pydantic_ai"}
{"title": "Creating Decisions", "anchor": "creating-decisions", "heading_level": 2, "md_text": "Use [`g.decision()`][pydantic_graph.beta.graph_builder.GraphBuilder.decision] to create a decision node, then add branches with [`g.match()`][pydantic_graph.beta.graph_builder.GraphBuilder.match]:\n\n```python {title=\"simple_decision.py\"}\nfrom dataclasses import dataclass\nfrom typing import Literal\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext, TypeExpression\n\n\n@dataclass\nclass DecisionState:\n    path_taken: str | None = None\n\n\nasync def main():\n    g = GraphBuilder(state_type=DecisionState, output_type=str)\n\n    @g.step\n    async def choose_path(ctx: StepContext[DecisionState, None, None]) -> Literal['left', 'right']:\n        return 'left'\n\n    @g.step\n    async def left_path(ctx: StepContext[DecisionState, None, object]) -> str:\n        ctx.state.path_taken = 'left'\n        return 'Went left'\n\n    @g.step\n    async def right_path(ctx: StepContext[DecisionState, None, object]) -> str:\n        ctx.state.path_taken = 'right'\n        return 'Went right'\n\n    g.add(\n        g.edge_from(g.start_node).to(choose_path),\n        g.edge_from(choose_path).to(\n            g.decision()\n            .branch(g.match(TypeExpression[Literal['left']]).to(left_path))\n            .branch(g.match(TypeExpression[Literal['right']]).to(right_path))\n        ),\n        g.edge_from(left_path, right_path).to(g.end_node),\n    )\n\n    graph = g.build()\n    state = DecisionState()\n    result = await graph.run(state=state)\n    print(result)\n    #> Went left\n    print(state.path_taken)\n    #> left\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/decisions/#creating-decisions", "page": "docs/graph/beta/decisions", "source_site": "pydantic_ai"}
{"title": "Type Matching", "anchor": "type-matching", "heading_level": 2, "md_text": "Match by type using regular Python types:\n\n```python {title=\"type_matching.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\n@dataclass\nclass DecisionState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=DecisionState, output_type=str)\n\n    @g.step\n    async def return_int(ctx: StepContext[DecisionState, None, None]) -> int:\n        return 42\n\n    @g.step\n    async def handle_int(ctx: StepContext[DecisionState, None, int]) -> str:\n        return f'Got int: {ctx.inputs}'\n\n    @g.step\n    async def handle_str(ctx: StepContext[DecisionState, None, str]) -> str:\n        return f'Got str: {ctx.inputs}'\n\n    g.add(\n        g.edge_from(g.start_node).to(return_int),\n        g.edge_from(return_int).to(\n            g.decision()\n            .branch(g.match(int).to(handle_int))\n            .branch(g.match(str).to(handle_str))\n        ),\n        g.edge_from(handle_int, handle_str).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=DecisionState())\n    print(result)\n    #> Got int: 42\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/decisions/#type-matching", "page": "docs/graph/beta/decisions", "source_site": "pydantic_ai"}
{"title": "Matching Union Types", "anchor": "matching-union-types", "heading_level": 3, "md_text": "For more complex type expressions like unions, you need to use [`TypeExpression`][pydantic_graph.beta.util.TypeExpression] because Python's type system doesn't allow union types to be used directly as runtime values:\n\n```python {title=\"union_type_matching.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext, TypeExpression\n\n\n@dataclass\nclass DecisionState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=DecisionState, output_type=str)\n\n    @g.step\n    async def return_value(ctx: StepContext[DecisionState, None, None]) -> int | str:\n        \"\"\"Returns either an int or a str.\"\"\"\n        return 42\n\n    @g.step\n    async def handle_number(ctx: StepContext[DecisionState, None, int | float]) -> str:\n        return f'Got number: {ctx.inputs}'\n\n    @g.step\n    async def handle_text(ctx: StepContext[DecisionState, None, str]) -> str:\n        return f'Got text: {ctx.inputs}'\n\n    g.add(\n        g.edge_from(g.start_node).to(return_value),\n        g.edge_from(return_value).to(\n            g.decision()\n            # Use TypeExpression for union types\n            .branch(g.match(TypeExpression[int | float]).to(handle_number))\n            .branch(g.match(str).to(handle_text))\n        ),\n        g.edge_from(handle_number, handle_text).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=DecisionState())\n    print(result)\n    #> Got number: 42\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_\n\n!!! note\n    [`TypeExpression`][pydantic_graph.beta.util.TypeExpression] is only necessary for complex type expressions like unions (`int | str`), `Literal`, and other type forms that aren't valid as runtime `type` objects. For simple types like `int`, `str`, or custom classes, you can pass them directly to `g.match()`.\n\n    The `TypeForm` class introduced in [PEP 747](https://peps.python.org/pep-0747/) should eventually eliminate the need for this workaround.", "url": "https://ai.pydantic.dev/docs/graph/beta/decisions/#matching-union-types", "page": "docs/graph/beta/decisions", "source_site": "pydantic_ai"}
{"title": "Custom Matchers", "anchor": "custom-matchers", "heading_level": 2, "md_text": "Provide custom matching logic with the `matches` parameter:\n\n```python {title=\"custom_matcher.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext, TypeExpression\n\n\n@dataclass\nclass DecisionState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=DecisionState, output_type=str)\n\n    @g.step\n    async def return_number(ctx: StepContext[DecisionState, None, None]) -> int:\n        return 7\n\n    @g.step\n    async def even_path(ctx: StepContext[DecisionState, None, int]) -> str:\n        return f'{ctx.inputs} is even'\n\n    @g.step\n    async def odd_path(ctx: StepContext[DecisionState, None, int]) -> str:\n        return f'{ctx.inputs} is odd'\n\n    g.add(\n        g.edge_from(g.start_node).to(return_number),\n        g.edge_from(return_number).to(\n            g.decision()\n            .branch(g.match(TypeExpression[int], matches=lambda x: x % 2 == 0).to(even_path))\n            .branch(g.match(TypeExpression[int], matches=lambda x: x % 2 == 1).to(odd_path))\n        ),\n        g.edge_from(even_path, odd_path).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=DecisionState())\n    print(result)\n    #> 7 is odd\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/decisions/#custom-matchers", "page": "docs/graph/beta/decisions", "source_site": "pydantic_ai"}
{"title": "Branch Priority", "anchor": "branch-priority", "heading_level": 2, "md_text": "Branches are evaluated in the order they're added. The first matching branch is taken:\n\n```python {title=\"branch_priority.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext, TypeExpression\n\n\n@dataclass\nclass DecisionState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=DecisionState, output_type=str)\n\n    @g.step\n    async def return_value(ctx: StepContext[DecisionState, None, None]) -> int:\n        return 10\n\n    @g.step\n    async def branch_a(ctx: StepContext[DecisionState, None, int]) -> str:\n        return 'Branch A'\n\n    @g.step\n    async def branch_b(ctx: StepContext[DecisionState, None, int]) -> str:\n        return 'Branch B'\n\n    g.add(\n        g.edge_from(g.start_node).to(return_value),\n        g.edge_from(return_value).to(\n            g.decision()\n            .branch(g.match(TypeExpression[int], matches=lambda x: x >= 5).to(branch_a))\n            .branch(g.match(TypeExpression[int], matches=lambda x: x >= 0).to(branch_b))\n        ),\n        g.edge_from(branch_a, branch_b).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=DecisionState())\n    print(result)\n    #> Branch A\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_\n\nBoth branches could match `10`, but Branch A is first, so it's taken.", "url": "https://ai.pydantic.dev/docs/graph/beta/decisions/#branch-priority", "page": "docs/graph/beta/decisions", "source_site": "pydantic_ai"}
{"title": "Catch-All Branches", "anchor": "catch-all-branches", "heading_level": 2, "md_text": "Use `object` or `Any` to create a catch-all branch:\n\n```python {title=\"catch_all.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext, TypeExpression\n\n\n@dataclass\nclass DecisionState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=DecisionState, output_type=str)\n\n    @g.step\n    async def return_value(ctx: StepContext[DecisionState, None, None]) -> int:\n        return 100\n\n    @g.step\n    async def catch_all(ctx: StepContext[DecisionState, None, object]) -> str:\n        return f'Caught: {ctx.inputs}'\n\n    g.add(\n        g.edge_from(g.start_node).to(return_value),\n        g.edge_from(return_value).to(g.decision().branch(g.match(TypeExpression[object]).to(catch_all))),\n        g.edge_from(catch_all).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=DecisionState())\n    print(result)\n    #> Caught: 100\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/decisions/#catch-all-branches", "page": "docs/graph/beta/decisions", "source_site": "pydantic_ai"}
{"title": "Nested Decisions", "anchor": "nested-decisions", "heading_level": 2, "md_text": "Decisions can be nested for complex conditional logic:\n\n```python {title=\"nested_decisions.py\"}\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext, TypeExpression\n\n\n@dataclass\nclass DecisionState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=DecisionState, output_type=str)\n\n    @g.step\n    async def get_number(ctx: StepContext[DecisionState, None, None]) -> int:\n        return 15\n\n    @g.step\n    async def is_positive(ctx: StepContext[DecisionState, None, int]) -> int:\n        return ctx.inputs\n\n    @g.step\n    async def is_negative(ctx: StepContext[DecisionState, None, int]) -> str:\n        return 'Negative'\n\n    @g.step\n    async def small_positive(ctx: StepContext[DecisionState, None, int]) -> str:\n        return 'Small positive'\n\n    @g.step\n    async def large_positive(ctx: StepContext[DecisionState, None, int]) -> str:\n        return 'Large positive'\n\n    g.add(\n        g.edge_from(g.start_node).to(get_number),\n        g.edge_from(get_number).to(\n            g.decision()\n            .branch(g.match(TypeExpression[int], matches=lambda x: x > 0).to(is_positive))\n            .branch(g.match(TypeExpression[int], matches=lambda x: x <= 0).to(is_negative))\n        ),\n        g.edge_from(is_positive).to(\n            g.decision()\n            .branch(g.match(TypeExpression[int], matches=lambda x: x < 10).to(small_positive))\n            .branch(g.match(TypeExpression[int], matches=lambda x: x >= 10).to(large_positive))\n        ),\n        g.edge_from(is_negative, small_positive, large_positive).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=DecisionState())\n    print(result)\n    #> Large positive\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/decisions/#nested-decisions", "page": "docs/graph/beta/decisions", "source_site": "pydantic_ai"}
{"title": "Branching with Labels", "anchor": "branching-with-labels", "heading_level": 2, "md_text": "Add labels to branches for documentation and diagram generation:\n\n```python {title=\"labeled_branches.py\"}\nfrom dataclasses import dataclass\nfrom typing import Literal\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext, TypeExpression\n\n\n@dataclass\nclass DecisionState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=DecisionState, output_type=str)\n\n    @g.step\n    async def choose(ctx: StepContext[DecisionState, None, None]) -> Literal['a', 'b']:\n        return 'a'\n\n    @g.step\n    async def path_a(ctx: StepContext[DecisionState, None, object]) -> str:\n        return 'Path A'\n\n    @g.step\n    async def path_b(ctx: StepContext[DecisionState, None, object]) -> str:\n        return 'Path B'\n\n    g.add(\n        g.edge_from(g.start_node).to(choose),\n        g.edge_from(choose).to(\n            g.decision()\n            .branch(g.match(TypeExpression[Literal['a']]).label('Take path A').to(path_a))\n            .branch(g.match(TypeExpression[Literal['b']]).label('Take path B').to(path_b))\n        ),\n        g.edge_from(path_a, path_b).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=DecisionState())\n    print(result)\n    #> Path A\n```\n\n_(This example is complete, it can be run \"as is\" \u2014 you'll need to add `import asyncio; asyncio.run(main())` to run `main`)_", "url": "https://ai.pydantic.dev/docs/graph/beta/decisions/#branching-with-labels", "page": "docs/graph/beta/decisions", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "- Learn about [parallel execution](parallel.md) with broadcasting and mapping\n- Understand [join nodes](joins.md) for aggregating parallel results\n- See the [API reference][pydantic_graph.beta.decision] for complete decision documentation", "url": "https://ai.pydantic.dev/docs/graph/beta/decisions/#next-steps", "page": "docs/graph/beta/decisions", "source_site": "pydantic_ai"}
{"title": "format_as_xml", "anchor": "format_as_xml", "heading_level": 0, "md_text": "Format a Python object as XML.\n\nThis is useful since LLMs often find it easier to read semi-structured data (e.g. examples) as XML,\nrather than JSON etc.\n\nSupports: `str`, `bytes`, `bytearray`, `bool`, `int`, `float`, `date`, `datetime`, `time`, `timedelta`, `Enum`,\n`Mapping`, `Iterable`, `dataclass`, and `BaseModel`.\n\nArgs:\n    obj: Python Object to serialize to XML.\n    root_tag: Outer tag to wrap the XML in, use `None` to omit the outer tag.\n    item_tag: Tag to use for each item in an iterable (e.g. list), this is overridden by the class name\n        for dataclasses and Pydantic models.\n    none_str: String to use for `None` values.\n    indent: Indentation string to use for pretty printing.\n    include_field_info: Whether to include attributes like Pydantic `Field` attributes and dataclasses `field()`\n        `metadata` as XML attributes. In both cases the allowed `Field` attributes and `field()` metadata keys are\n        `title` and `description`. If a field is repeated in the data (e.g. in a list) by setting `once`\n        the attributes are included only in the first occurrence of an XML element relative to the same field.\n\nReturns:\n    XML representation of the object.\n\nExample:\n```python {title=\"format_as_xml_example.py\" lint=\"skip\"}\nfrom pydantic_ai import format_as_xml\n\nprint(format_as_xml({'name': 'John', 'height': 6, 'weight': 200}, root_tag='user'))\n'''\n<user>\n  <name>John</name>\n  <height>6</height>\n  <weight>200</weight>\n</user>\n'''\n```", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/format_prompt", "source_site": "pydantic_ai"}
{"title": "_ToXml._init_structure_info", "anchor": "_toxml-_init_structure_info", "heading_level": 0, "md_text": "Create maps with all data information (fields info and class names), if not already created.", "url": "https://ai.pydantic.dev/api/-init-structure-info/", "page": "pydantic_ai_slim/pydantic_ai/format_prompt", "source_site": "pydantic_ai"}
{"title": "_ToXml._parse_data_structures", "anchor": "_toxml-_parse_data_structures", "heading_level": 0, "md_text": "Parse data structures as dataclasses or Pydantic models to extract element names and attributes.", "url": "https://ai.pydantic.dev/api/-parse-data-structures/", "page": "pydantic_ai_slim/pydantic_ai/format_prompt", "source_site": "pydantic_ai"}
{"title": "direct", "anchor": "direct", "heading_level": 0, "md_text": "Methods for making imperative requests to language models with minimal abstraction.\n\nThese methods allow you to make requests to LLMs where the only abstraction is input and output schema\ntranslation so you can use all models with the same API.\n\nThese methods are thin wrappers around [`Model`][pydantic_ai.models.Model] implementations.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/direct", "source_site": "pydantic_ai"}
{"title": "model_request", "anchor": "model_request", "heading_level": 0, "md_text": "Make a non-streamed request to a model.\n\n```py title=\"model_request_example.py\"\nfrom pydantic_ai import ModelRequest\nfrom pydantic_ai.direct import model_request\n\n\nasync def main():\n    model_response = await model_request(\n        'anthropic:claude-haiku-4-5',\n        [ModelRequest.user_text_prompt('What is the capital of France?')]  # (1)!\n    )\n    print(model_response)\n    '''\n    ModelResponse(\n        parts=[TextPart(content='The capital of France is Paris.')],\n        usage=RequestUsage(input_tokens=56, output_tokens=7),\n        model_name='claude-haiku-4-5',\n        timestamp=datetime.datetime(...),\n    )\n    '''\n```\n\n1. See [`ModelRequest.user_text_prompt`][pydantic_ai.messages.ModelRequest.user_text_prompt] for details.\n\nArgs:\n    model: The model to make a request to. We allow `str` here since the actual list of allowed models changes frequently.\n    messages: Messages to send to the model\n    model_settings: optional model settings\n    model_request_parameters: optional model request parameters\n    instrument: Whether to instrument the request with OpenTelemetry/Logfire, if `None` the value from\n        [`logfire.instrument_pydantic_ai`][logfire.Logfire.instrument_pydantic_ai] is used.\n\nReturns:\n    The model response and token usage associated with the request.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/direct", "source_site": "pydantic_ai"}
{"title": "model_request_sync", "anchor": "model_request_sync", "heading_level": 0, "md_text": "Make a Synchronous, non-streamed request to a model.\n\nThis is a convenience method that wraps [`model_request`][pydantic_ai.direct.model_request] with\n`loop.run_until_complete(...)`. You therefore can't use this method inside async code or if there's an active event loop.\n\n```py title=\"model_request_sync_example.py\"\nfrom pydantic_ai import ModelRequest\nfrom pydantic_ai.direct import model_request_sync\n\nmodel_response = model_request_sync(\n    'anthropic:claude-haiku-4-5',\n    [ModelRequest.user_text_prompt('What is the capital of France?')]  # (1)!\n)\nprint(model_response)\n'''\nModelResponse(\n    parts=[TextPart(content='The capital of France is Paris.')],\n    usage=RequestUsage(input_tokens=56, output_tokens=7),\n    model_name='claude-haiku-4-5',\n    timestamp=datetime.datetime(...),\n)\n'''\n```\n\n1. See [`ModelRequest.user_text_prompt`][pydantic_ai.messages.ModelRequest.user_text_prompt] for details.\n\nArgs:\n    model: The model to make a request to. We allow `str` here since the actual list of allowed models changes frequently.\n    messages: Messages to send to the model\n    model_settings: optional model settings\n    model_request_parameters: optional model request parameters\n    instrument: Whether to instrument the request with OpenTelemetry/Logfire, if `None` the value from\n        [`logfire.instrument_pydantic_ai`][logfire.Logfire.instrument_pydantic_ai] is used.\n\nReturns:\n    The model response and token usage associated with the request.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/direct", "source_site": "pydantic_ai"}
{"title": "model_request_stream", "anchor": "model_request_stream", "heading_level": 0, "md_text": "Make a streamed async request to a model.\n\n```py {title=\"model_request_stream_example.py\"}\n\nfrom pydantic_ai import ModelRequest\nfrom pydantic_ai.direct import model_request_stream\n\n\nasync def main():\n    messages = [ModelRequest.user_text_prompt('Who was Albert Einstein?')]  # (1)!\n    async with model_request_stream('openai:gpt-4.1-mini', messages) as stream:\n        chunks = []\n        async for chunk in stream:\n            chunks.append(chunk)\n        print(chunks)\n        '''\n        [\n            PartStartEvent(index=0, part=TextPart(content='Albert Einstein was ')),\n            FinalResultEvent(tool_name=None, tool_call_id=None),\n            PartDeltaEvent(\n                index=0, delta=TextPartDelta(content_delta='a German-born theoretical ')\n            ),\n            PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='physicist.')),\n            PartEndEvent(\n                index=0,\n                part=TextPart(\n                    content='Albert Einstein was a German-born theoretical physicist.'\n                ),\n            ),\n        ]\n        '''\n```\n\n1. See [`ModelRequest.user_text_prompt`][pydantic_ai.messages.ModelRequest.user_text_prompt] for details.\n\nArgs:\n    model: The model to make a request to. We allow `str` here since the actual list of allowed models changes frequently.\n    messages: Messages to send to the model\n    model_settings: optional model settings\n    model_request_parameters: optional model request parameters\n    instrument: Whether to instrument the request with OpenTelemetry/Logfire, if `None` the value from\n        [`logfire.instrument_pydantic_ai`][logfire.Logfire.instrument_pydantic_ai] is used.\n\nReturns:\n    A [stream response][pydantic_ai.models.StreamedResponse] async context manager.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/direct", "source_site": "pydantic_ai"}
{"title": "model_request_stream_sync", "anchor": "model_request_stream_sync", "heading_level": 0, "md_text": "Make a streamed synchronous request to a model.\n\nThis is the synchronous version of [`model_request_stream`][pydantic_ai.direct.model_request_stream].\nIt uses threading to run the asynchronous stream in the background while providing a synchronous iterator interface.\n\n```py {title=\"model_request_stream_sync_example.py\"}\n\nfrom pydantic_ai import ModelRequest\nfrom pydantic_ai.direct import model_request_stream_sync\n\nmessages = [ModelRequest.user_text_prompt('Who was Albert Einstein?')]\nwith model_request_stream_sync('openai:gpt-4.1-mini', messages) as stream:\n    chunks = []\n    for chunk in stream:\n        chunks.append(chunk)\n    print(chunks)\n    '''\n    [\n        PartStartEvent(index=0, part=TextPart(content='Albert Einstein was ')),\n        FinalResultEvent(tool_name=None, tool_call_id=None),\n        PartDeltaEvent(\n            index=0, delta=TextPartDelta(content_delta='a German-born theoretical ')\n        ),\n        PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='physicist.')),\n        PartEndEvent(\n            index=0,\n            part=TextPart(\n                content='Albert Einstein was a German-born theoretical physicist.'\n            ),\n        ),\n    ]\n    '''\n```\n\nArgs:\n    model: The model to make a request to. We allow `str` here since the actual list of allowed models changes frequently.\n    messages: Messages to send to the model\n    model_settings: optional model settings\n    model_request_parameters: optional model request parameters\n    instrument: Whether to instrument the request with OpenTelemetry/Logfire, if `None` the value from\n        [`logfire.instrument_pydantic_ai`][logfire.Logfire.instrument_pydantic_ai] is used.\n\nReturns:\n    A [sync stream response][pydantic_ai.direct.StreamedResponseSync] context manager.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/direct", "source_site": "pydantic_ai"}
{"title": "StreamedResponseSync", "anchor": "streamedresponsesync", "heading_level": 0, "md_text": "Synchronous wrapper to async streaming responses by running the async producer in a background thread and providing a synchronous iterator.\n\nThis class must be used as a context manager with the `with` statement.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/direct", "source_site": "pydantic_ai"}
{"title": "StreamedResponseSync.__iter__", "anchor": "streamedresponsesync-__iter__", "heading_level": 0, "md_text": "Stream the response as an iterable of [`ModelResponseStreamEvent`][pydantic_ai.messages.ModelResponseStreamEvent]s.", "url": "https://ai.pydantic.dev/api/--iter--/", "page": "pydantic_ai_slim/pydantic_ai/direct", "source_site": "pydantic_ai"}
{"title": "StreamedResponseSync.get", "anchor": "streamedresponsesync-get", "heading_level": 0, "md_text": "Build a ModelResponse from the data received from the stream so far.", "url": "https://ai.pydantic.dev/api/get/", "page": "pydantic_ai_slim/pydantic_ai/direct", "source_site": "pydantic_ai"}
{"title": "StreamedResponseSync.response", "anchor": "streamedresponsesync-response", "heading_level": 0, "md_text": "Get the current state of the response.", "url": "https://ai.pydantic.dev/api/response/", "page": "pydantic_ai_slim/pydantic_ai/direct", "source_site": "pydantic_ai"}
{"title": "StreamedResponseSync.usage", "anchor": "streamedresponsesync-usage", "heading_level": 0, "md_text": "Get the usage of the response so far.", "url": "https://ai.pydantic.dev/api/usage/", "page": "pydantic_ai_slim/pydantic_ai/direct", "source_site": "pydantic_ai"}
{"title": "StreamedResponseSync.model_name", "anchor": "streamedresponsesync-model_name", "heading_level": 0, "md_text": "Get the model name of the response.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/direct", "source_site": "pydantic_ai"}
{"title": "StreamedResponseSync.timestamp", "anchor": "streamedresponsesync-timestamp", "heading_level": 0, "md_text": "Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/timestamp/", "page": "pydantic_ai_slim/pydantic_ai/direct", "source_site": "pydantic_ai"}
{"title": "doc_descriptions", "anchor": "doc_descriptions", "heading_level": 0, "md_text": "Extract the function description and parameter descriptions from a function's docstring.\n\nThe function parses the docstring using the specified format (or infers it if 'auto')\nand extracts both the main description and parameter descriptions. If a returns section\nis present in the docstring, the main description will be formatted as XML.\n\nReturns:\n    A tuple containing:\n    - str: Main description string, which may be either:\n        * Plain text if no returns section is present\n        * XML-formatted if returns section exists, including <summary> and <returns> tags\n    - dict[str, str]: Dictionary mapping parameter names to their descriptions", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_griffe", "source_site": "pydantic_ai"}
{"title": "_infer_docstring_style", "anchor": "_infer_docstring_style", "heading_level": 0, "md_text": "Simplistic docstring style inference.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_griffe", "source_site": "pydantic_ai"}
{"title": "ToolOutput", "anchor": "tooloutput", "heading_level": 0, "md_text": "Marker class to use a tool for output and optionally customize the tool.\n\nExample:\n```python {title=\"tool_output.py\"}\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, ToolOutput\n\n\nclass Fruit(BaseModel):\n    name: str\n    color: str\n\n\nclass Vehicle(BaseModel):\n    name: str\n    wheels: int\n\n\nagent = Agent(\n    'openai:gpt-4o',\n    output_type=[\n        ToolOutput(Fruit, name='return_fruit'),\n        ToolOutput(Vehicle, name='return_vehicle'),\n    ],\n)\nresult = agent.run_sync('What is a banana?')\nprint(repr(result.output))\n#> Fruit(name='banana', color='yellow')\n```", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/output", "source_site": "pydantic_ai"}
{"title": "NativeOutput", "anchor": "nativeoutput", "heading_level": 0, "md_text": "Marker class to use the model's native structured outputs functionality for outputs and optionally customize the name and description.\n\nExample:\n```python {title=\"native_output.py\" requires=\"tool_output.py\"}\nfrom pydantic_ai import Agent, NativeOutput\n\nfrom tool_output import Fruit, Vehicle\n\nagent = Agent(\n    'openai:gpt-4o',\n    output_type=NativeOutput(\n        [Fruit, Vehicle],\n        name='Fruit or vehicle',\n        description='Return a fruit or vehicle.'\n    ),\n)\nresult = agent.run_sync('What is a Ford Explorer?')\nprint(repr(result.output))\n#> Vehicle(name='Ford Explorer', wheels=4)\n```", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/output", "source_site": "pydantic_ai"}
{"title": "PromptedOutput", "anchor": "promptedoutput", "heading_level": 0, "md_text": "Marker class to use a prompt to tell the model what to output and optionally customize the prompt.\n\nExample:\n```python {title=\"prompted_output.py\" requires=\"tool_output.py\"}\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, PromptedOutput\n\nfrom tool_output import Vehicle\n\n\nclass Device(BaseModel):\n    name: str\n    kind: str\n\n\nagent = Agent(\n    'openai:gpt-4o',\n    output_type=PromptedOutput(\n        [Vehicle, Device],\n        name='Vehicle or device',\n        description='Return a vehicle or device.'\n    ),\n)\nresult = agent.run_sync('What is a MacBook?')\nprint(repr(result.output))\n#> Device(name='MacBook', kind='laptop')\n\nagent = Agent(\n    'openai:gpt-4o',\n    output_type=PromptedOutput(\n        [Vehicle, Device],\n        template='Gimme some JSON: {schema}'\n    ),\n)\nresult = agent.run_sync('What is a Ford Explorer?')\nprint(repr(result.output))\n#> Vehicle(name='Ford Explorer', wheels=4)\n```", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/output", "source_site": "pydantic_ai"}
{"title": "OutputObjectDefinition", "anchor": "outputobjectdefinition", "heading_level": 0, "md_text": "Definition of an output object used for structured output generation.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/output", "source_site": "pydantic_ai"}
{"title": "TextOutput", "anchor": "textoutput", "heading_level": 0, "md_text": "Marker class to use text output for an output function taking a string argument.\n\nExample:\n```python\nfrom pydantic_ai import Agent, TextOutput\n\n\ndef split_into_words(text: str) -> list[str]:\n    return text.split()\n\n\nagent = Agent(\n    'openai:gpt-4o',\n    output_type=TextOutput(split_into_words),\n)\nresult = agent.run_sync('Who was Albert Einstein?')\nprint(result.output)\n#> ['Albert', 'Einstein', 'was', 'a', 'German-born', 'theoretical', 'physicist.']\n```", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/output", "source_site": "pydantic_ai"}
{"title": "StructuredDict", "anchor": "structureddict", "heading_level": 0, "md_text": "Returns a `dict[str, Any]` subclass with a JSON schema attached that will be used for structured output.\n\nArgs:\n    json_schema: A JSON schema of type `object` defining the structure of the dictionary content.\n    name: Optional name of the structured output. If not provided, the `title` field of the JSON schema will be used if it's present.\n    description: Optional description of the structured output. If not provided, the `description` field of the JSON schema will be used if it's present.\n\nExample:\n```python {title=\"structured_dict.py\"}\nfrom pydantic_ai import Agent, StructuredDict\n\nschema = {\n    'type': 'object',\n    'properties': {\n        'name': {'type': 'string'},\n        'age': {'type': 'integer'}\n    },\n    'required': ['name', 'age']\n}\n\nagent = Agent('openai:gpt-4o', output_type=StructuredDict(schema))\nresult = agent.run_sync('Create a person')\nprint(result.output)\n#> {'name': 'John Doe', 'age': 30}\n```", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/output", "source_site": "pydantic_ai"}
{"title": "AbstractBuiltinTool", "anchor": "abstractbuiltintool", "heading_level": 0, "md_text": "A builtin tool that can be used by an agent.\n\nThis class is abstract and cannot be instantiated directly.\n\nThe builtin tools are passed to the model as part of the `ModelRequestParameters`.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/builtin_tools", "source_site": "pydantic_ai"}
{"title": "WebSearchTool", "anchor": "websearchtool", "heading_level": 0, "md_text": "A builtin tool that allows your agent to search the web for information.\n\nThe parameters that PydanticAI passes depend on the model, as some parameters may not be supported by certain models.\n\nSupported by:\n\n* Anthropic\n* OpenAI Responses\n* Groq\n* Google", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/builtin_tools", "source_site": "pydantic_ai"}
{"title": "WebSearchUserLocation", "anchor": "websearchuserlocation", "heading_level": 0, "md_text": "Allows you to localize search results based on a user's location.\n\nSupported by:\n\n* Anthropic\n* OpenAI Responses", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/builtin_tools", "source_site": "pydantic_ai"}
{"title": "CodeExecutionTool", "anchor": "codeexecutiontool", "heading_level": 0, "md_text": "A builtin tool that allows your agent to execute code.\n\nSupported by:\n\n* Anthropic\n* OpenAI Responses\n* Google", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/builtin_tools", "source_site": "pydantic_ai"}
{"title": "UrlContextTool", "anchor": "urlcontexttool", "heading_level": 0, "md_text": "Allows your agent to access contents from URLs.\n\nSupported by:\n\n* Google", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/builtin_tools", "source_site": "pydantic_ai"}
{"title": "ImageGenerationTool", "anchor": "imagegenerationtool", "heading_level": 0, "md_text": "A builtin tool that allows your agent to generate images.\n\nSupported by:\n\n* OpenAI Responses\n* Google", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/builtin_tools", "source_site": "pydantic_ai"}
{"title": "MemoryTool", "anchor": "memorytool", "heading_level": 0, "md_text": "A builtin tool that allows your agent to use memory.\n\nSupported by:\n\n* Anthropic", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/builtin_tools", "source_site": "pydantic_ai"}
{"title": "MCPServerTool", "anchor": "mcpservertool", "heading_level": 0, "md_text": "A builtin tool that allows your agent to use MCP servers.\n\nSupported by:\n\n* OpenAI Responses\n* Anthropic", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/builtin_tools", "source_site": "pydantic_ai"}
{"title": "AbstractBuiltinTool.unique_id", "anchor": "abstractbuiltintool-unique_id", "heading_level": 0, "md_text": "A unique identifier for the builtin tool.\n\nIf multiple instances of the same builtin tool can be passed to the model, subclasses should override this property to allow them to be distinguished.", "url": "https://ai.pydantic.dev/api/unique-id/", "page": "pydantic_ai_slim/pydantic_ai/builtin_tools", "source_site": "pydantic_ai"}
{"title": "__main__", "anchor": "__main__", "heading_level": 0, "md_text": "This means `python -m pydantic_ai` should run the CLI.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/__main__", "source_site": "pydantic_ai"}
{"title": "_otel_messages", "anchor": "_otel_messages", "heading_level": 0, "md_text": "Type definitions of OpenTelemetry GenAI spec message parts.\n\nBased on https://github.com/lmolkova/semantic-conventions/blob/eccd1f806e426a32c98271c3ce77585492d26de2/docs/gen-ai/non-normative/models.ipynb", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_otel_messages", "source_site": "pydantic_ai"}
{"title": "worker_lifespan", "anchor": "worker_lifespan", "heading_level": 0, "md_text": "Custom lifespan that runs the worker during application startup.\n\nThis ensures the worker is started and ready to process tasks as soon as the application starts.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_a2a", "source_site": "pydantic_ai"}
{"title": "agent_to_a2a", "anchor": "agent_to_a2a", "heading_level": 0, "md_text": "Create a FastA2A server from an agent.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_a2a", "source_site": "pydantic_ai"}
{"title": "AgentWorker", "anchor": "agentworker", "heading_level": 0, "md_text": "A worker that uses an agent to execute tasks.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_a2a", "source_site": "pydantic_ai"}
{"title": "AgentWorker.build_artifacts", "anchor": "agentworker-build_artifacts", "heading_level": 0, "md_text": "Build artifacts from agent result.\n\nAll agent outputs become artifacts to mark them as durable task outputs.\nFor string results, we use TextPart. For structured data, we use DataPart.\nMetadata is included to preserve type information.", "url": "https://ai.pydantic.dev/api/build-artifacts/", "page": "pydantic_ai_slim/pydantic_ai/_a2a", "source_site": "pydantic_ai"}
{"title": "AgentWorker._convert_result_to_part", "anchor": "agentworker-_convert_result_to_part", "heading_level": 0, "md_text": "Convert agent result to a Part (TextPart or DataPart).\n\nFor string results, returns a TextPart.\nFor structured data, returns a DataPart with properly serialized data.", "url": "https://ai.pydantic.dev/api/-convert-result-to-part/", "page": "pydantic_ai_slim/pydantic_ai/_a2a", "source_site": "pydantic_ai"}
{"title": "AgentWorker._request_parts_from_a2a", "anchor": "agentworker-_request_parts_from_a2a", "heading_level": 0, "md_text": "Convert A2A Part objects to pydantic-ai ModelRequestPart objects.\n\nThis handles the conversion from A2A protocol parts (text, file, data) to\npydantic-ai's internal request parts (UserPromptPart with various content types).\n\nArgs:\n    parts: List of A2A Part objects from incoming messages\n\nReturns:\n    List of ModelRequestPart objects for the pydantic-ai agent", "url": "https://ai.pydantic.dev/api/-request-parts-from-a2a/", "page": "pydantic_ai_slim/pydantic_ai/_a2a", "source_site": "pydantic_ai"}
{"title": "AgentWorker._response_parts_from_a2a", "anchor": "agentworker-_response_parts_from_a2a", "heading_level": 0, "md_text": "Convert A2A Part objects to pydantic-ai ModelResponsePart objects.\n\nThis handles the conversion from A2A protocol parts (text, file, data) to\npydantic-ai's internal response parts. Currently only supports text parts\nas agent responses in A2A are expected to be text-based.\n\nArgs:\n    parts: List of A2A Part objects from stored agent messages\n\nReturns:\n    List of ModelResponsePart objects for message history", "url": "https://ai.pydantic.dev/api/-response-parts-from-a2a/", "page": "pydantic_ai_slim/pydantic_ai/_a2a", "source_site": "pydantic_ai"}
{"title": "AgentWorker._response_parts_to_a2a", "anchor": "agentworker-_response_parts_to_a2a", "heading_level": 0, "md_text": "Convert pydantic-ai ModelResponsePart objects to A2A Part objects.\n\nThis handles the conversion from pydantic-ai's internal response parts to\nA2A protocol parts. Different part types are handled as follows:\n- TextPart: Converted directly to A2A TextPart\n- ThinkingPart: Converted to TextPart with metadata indicating it's thinking\n- ToolCallPart: Skipped (internal to agent execution)\n\nArgs:\n    parts: List of ModelResponsePart objects from agent response\n\nReturns:\n    List of A2A Part objects suitable for sending via A2A protocol", "url": "https://ai.pydantic.dev/api/-response-parts-to-a2a/", "page": "pydantic_ai_slim/pydantic_ai/_a2a", "source_site": "pydantic_ai"}
{"title": "StreamedRunResult", "anchor": "streamedrunresult", "heading_level": 0, "md_text": "Result of a streamed run that returns structured data via a tool call.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "FinalResult", "anchor": "finalresult", "heading_level": 0, "md_text": "Marker class storing the final output of an agent run and associated metadata.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "_get_deferred_tool_requests", "anchor": "_get_deferred_tool_requests", "heading_level": 0, "md_text": "Get the deferred tool requests from the model response tool calls.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "AgentStream.stream_output", "anchor": "agentstream-stream_output", "heading_level": 0, "md_text": "Asynchronously stream the (validated) agent outputs.", "url": "https://ai.pydantic.dev/api/stream-output/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "AgentStream.stream_responses", "anchor": "agentstream-stream_responses", "heading_level": 0, "md_text": "Asynchronously stream the (unvalidated) model responses for the agent.", "url": "https://ai.pydantic.dev/api/stream-responses/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "AgentStream.stream_text", "anchor": "agentstream-stream_text", "heading_level": 0, "md_text": "Stream the text result as an async iterable.\n\nArgs:\n    delta: if `True`, yield each chunk of text as it is received, if `False` (default), yield the full text\n        up to the current point.\n    debounce_by: by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.\n        Debouncing is particularly important for long structured responses to reduce the overhead of\n        performing validation as each token is received.", "url": "https://ai.pydantic.dev/api/stream-text/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "AgentStream.get", "anchor": "agentstream-get", "heading_level": 0, "md_text": "Get the current state of the response.", "url": "https://ai.pydantic.dev/api/get/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "AgentStream.response", "anchor": "agentstream-response", "heading_level": 0, "md_text": "Get the current state of the response.", "url": "https://ai.pydantic.dev/api/response/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "AgentStream.usage", "anchor": "agentstream-usage", "heading_level": 0, "md_text": "Return the usage of the whole run.", "url": "https://ai.pydantic.dev/api/usage/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "AgentStream.timestamp", "anchor": "agentstream-timestamp", "heading_level": 0, "md_text": "Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/timestamp/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "AgentStream.get_output", "anchor": "agentstream-get_output", "heading_level": 0, "md_text": "Stream the whole response, validate the output and return it.", "url": "https://ai.pydantic.dev/api/get-output/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "AgentStream.validate_response_output", "anchor": "agentstream-validate_response_output", "heading_level": 0, "md_text": "Validate a structured result message.", "url": "https://ai.pydantic.dev/api/validate-response-output/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "AgentStream._stream_response_text", "anchor": "agentstream-_stream_response_text", "heading_level": 0, "md_text": "Stream the response as an async iterable of text.", "url": "https://ai.pydantic.dev/api/-stream-response-text/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "AgentStream.__aiter__", "anchor": "agentstream-__aiter__", "heading_level": 0, "md_text": "Stream [`ModelResponseStreamEvent`][pydantic_ai.messages.ModelResponseStreamEvent]s.", "url": "https://ai.pydantic.dev/api/--aiter--/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "StreamedRunResult.all_messages", "anchor": "streamedrunresult-all_messages", "heading_level": 0, "md_text": "Return the history of _messages.\n\nArgs:\n    output_tool_return_content: The return content of the tool call to set in the last message.\n        This provides a convenient way to modify the content of the output tool call if you want to continue\n        the conversation and want to set the response to the output tool call. If `None`, the last message will\n        not be modified.\n\nReturns:\n    List of messages.", "url": "https://ai.pydantic.dev/api/all-messages/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "StreamedRunResult.all_messages_json", "anchor": "streamedrunresult-all_messages_json", "heading_level": 0, "md_text": "Return all messages from [`all_messages`][pydantic_ai.result.StreamedRunResult.all_messages] as JSON bytes.\n\nArgs:\n    output_tool_return_content: The return content of the tool call to set in the last message.\n        This provides a convenient way to modify the content of the output tool call if you want to continue\n        the conversation and want to set the response to the output tool call. If `None`, the last message will\n        not be modified.\n\nReturns:\n    JSON bytes representing the messages.", "url": "https://ai.pydantic.dev/api/all-messages-json/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "StreamedRunResult.new_messages", "anchor": "streamedrunresult-new_messages", "heading_level": 0, "md_text": "Return new messages associated with this run.\n\nMessages from older runs are excluded.\n\nArgs:\n    output_tool_return_content: The return content of the tool call to set in the last message.\n        This provides a convenient way to modify the content of the output tool call if you want to continue\n        the conversation and want to set the response to the output tool call. If `None`, the last message will\n        not be modified.\n\nReturns:\n    List of new messages.", "url": "https://ai.pydantic.dev/api/new-messages/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "StreamedRunResult.new_messages_json", "anchor": "streamedrunresult-new_messages_json", "heading_level": 0, "md_text": "Return new messages from [`new_messages`][pydantic_ai.result.StreamedRunResult.new_messages] as JSON bytes.\n\nArgs:\n    output_tool_return_content: The return content of the tool call to set in the last message.\n        This provides a convenient way to modify the content of the output tool call if you want to continue\n        the conversation and want to set the response to the output tool call. If `None`, the last message will\n        not be modified.\n\nReturns:\n    JSON bytes representing the new messages.", "url": "https://ai.pydantic.dev/api/new-messages-json/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "StreamedRunResult.stream_output", "anchor": "streamedrunresult-stream_output", "heading_level": 0, "md_text": "Stream the output as an async iterable.\n\nThe pydantic validator for structured data will be called in\n[partial mode](https://docs.pydantic.dev/dev/concepts/experimental/#partial-validation)\non each iteration.\n\nArgs:\n    debounce_by: by how much (if at all) to debounce/group the output chunks by. `None` means no debouncing.\n        Debouncing is particularly important for long structured outputs to reduce the overhead of\n        performing validation as each token is received.\n\nReturns:\n    An async iterable of the response data.", "url": "https://ai.pydantic.dev/api/stream-output/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "StreamedRunResult.stream_text", "anchor": "streamedrunresult-stream_text", "heading_level": 0, "md_text": "Stream the text result as an async iterable.\n\nArgs:\n    delta: if `True`, yield each chunk of text as it is received, if `False` (default), yield the full text\n        up to the current point.\n    debounce_by: by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.\n        Debouncing is particularly important for long structured responses to reduce the overhead of\n        performing validation as each token is received.", "url": "https://ai.pydantic.dev/api/stream-text/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "StreamedRunResult.stream_responses", "anchor": "streamedrunresult-stream_responses", "heading_level": 0, "md_text": "Stream the response as an async iterable of Structured LLM Messages.\n\nArgs:\n    debounce_by: by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.\n        Debouncing is particularly important for long structured responses to reduce the overhead of\n        performing validation as each token is received.\n\nReturns:\n    An async iterable of the structured response message and whether that is the last message.", "url": "https://ai.pydantic.dev/api/stream-responses/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "StreamedRunResult.get_output", "anchor": "streamedrunresult-get_output", "heading_level": 0, "md_text": "Stream the whole response, validate and return it.", "url": "https://ai.pydantic.dev/api/get-output/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "StreamedRunResult.response", "anchor": "streamedrunresult-response", "heading_level": 0, "md_text": "Return the current state of the response.", "url": "https://ai.pydantic.dev/api/response/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "StreamedRunResult.usage", "anchor": "streamedrunresult-usage", "heading_level": 0, "md_text": "Return the usage of the whole run.", "url": "https://ai.pydantic.dev/api/usage/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "StreamedRunResult.timestamp", "anchor": "streamedrunresult-timestamp", "heading_level": 0, "md_text": "Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/timestamp/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "StreamedRunResult.validate_response_output", "anchor": "streamedrunresult-validate_response_output", "heading_level": 0, "md_text": "Validate a structured result message.", "url": "https://ai.pydantic.dev/api/validate-response-output/", "page": "pydantic_ai_slim/pydantic_ai/result", "source_site": "pydantic_ai"}
{"title": "AgentRun", "anchor": "agentrun", "heading_level": 0, "md_text": "A stateful, async-iterable run of an [`Agent`][pydantic_ai.agent.Agent].\n\nYou generally obtain an `AgentRun` instance by calling `async with my_agent.iter(...) as agent_run:`.\n\nOnce you have an instance, you can use it to iterate through the run's nodes as they execute. When an\n[`End`][pydantic_graph.nodes.End] is reached, the run finishes and [`result`][pydantic_ai.agent.AgentRun.result]\nbecomes available.\n\nExample:\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    nodes = []\n    # Iterate through the run, recording each node along the way:\n    async with agent.iter('What is the capital of France?') as agent_run:\n        async for node in agent_run:\n            nodes.append(node)\n    print(nodes)\n    '''\n    [\n        UserPromptNode(\n            user_prompt='What is the capital of France?',\n            instructions_functions=[],\n            system_prompts=(),\n            system_prompt_functions=[],\n            system_prompt_dynamic_functions={},\n        ),\n        ModelRequestNode(\n            request=ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='What is the capital of France?',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            )\n        ),\n        CallToolsNode(\n            model_response=ModelResponse(\n                parts=[TextPart(content='The capital of France is Paris.')],\n                usage=RequestUsage(input_tokens=56, output_tokens=7),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            )\n        ),\n        End(data=FinalResult(output='The capital of France is Paris.')),\n    ]\n    '''\n    print(agent_run.result.output)\n    #> The capital of France is Paris.\n```\n\nYou can also manually drive the iteration using the [`next`][pydantic_ai.agent.AgentRun.next] method for\nmore granular control.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/run", "source_site": "pydantic_ai"}
{"title": "AgentRunResult", "anchor": "agentrunresult", "heading_level": 0, "md_text": "The final result of an agent run.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/run", "source_site": "pydantic_ai"}
{"title": "AgentRunResultEvent", "anchor": "agentrunresultevent", "heading_level": 0, "md_text": "An event indicating the agent run ended and containing the final result of the agent run.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/run", "source_site": "pydantic_ai"}
{"title": "AgentRun.ctx", "anchor": "agentrun-ctx", "heading_level": 0, "md_text": "The current context of the agent run.", "url": "https://ai.pydantic.dev/api/ctx/", "page": "pydantic_ai_slim/pydantic_ai/run", "source_site": "pydantic_ai"}
{"title": "AgentRun.next_node", "anchor": "agentrun-next_node", "heading_level": 0, "md_text": "The next node that will be run in the agent graph.\n\nThis is the next node that will be used during async iteration, or if a node is not passed to `self.next(...)`.", "url": "https://ai.pydantic.dev/api/next-node/", "page": "pydantic_ai_slim/pydantic_ai/run", "source_site": "pydantic_ai"}
{"title": "AgentRun.result", "anchor": "agentrun-result", "heading_level": 0, "md_text": "The final result of the run if it has ended, otherwise `None`.\n\nOnce the run returns an [`End`][pydantic_graph.nodes.End] node, `result` is populated\nwith an [`AgentRunResult`][pydantic_ai.agent.AgentRunResult].", "url": "https://ai.pydantic.dev/api/result/", "page": "pydantic_ai_slim/pydantic_ai/run", "source_site": "pydantic_ai"}
{"title": "AgentRun.__aiter__", "anchor": "agentrun-__aiter__", "heading_level": 0, "md_text": "Provide async-iteration over the nodes in the agent run.", "url": "https://ai.pydantic.dev/api/--aiter--/", "page": "pydantic_ai_slim/pydantic_ai/run", "source_site": "pydantic_ai"}
{"title": "AgentRun.__anext__", "anchor": "agentrun-__anext__", "heading_level": 0, "md_text": "Advance to the next node automatically based on the last returned node.", "url": "https://ai.pydantic.dev/api/--anext--/", "page": "pydantic_ai_slim/pydantic_ai/run", "source_site": "pydantic_ai"}
{"title": "AgentRun.next", "anchor": "agentrun-next", "heading_level": 0, "md_text": "Manually drive the agent run by passing in the node you want to run next.\n\nThis lets you inspect or mutate the node before continuing execution, or skip certain nodes\nunder dynamic conditions. The agent run should be stopped when you return an [`End`][pydantic_graph.nodes.End]\nnode.\n\nExample:\n```python\nfrom pydantic_ai import Agent\nfrom pydantic_graph import End\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    async with agent.iter('What is the capital of France?') as agent_run:\n        next_node = agent_run.next_node  # start with the first node\n        nodes = [next_node]\n        while not isinstance(next_node, End):\n            next_node = await agent_run.next(next_node)\n            nodes.append(next_node)\n        # Once `next_node` is an End, we've finished:\n        print(nodes)\n        '''\n        [\n            UserPromptNode(\n                user_prompt='What is the capital of France?',\n                instructions_functions=[],\n                system_prompts=(),\n                system_prompt_functions=[],\n                system_prompt_dynamic_functions={},\n            ),\n            ModelRequestNode(\n                request=ModelRequest(\n                    parts=[\n                        UserPromptPart(\n                            content='What is the capital of France?',\n                            timestamp=datetime.datetime(...),\n                        )\n                    ]\n                )\n            ),\n            CallToolsNode(\n                model_response=ModelResponse(\n                    parts=[TextPart(content='The capital of France is Paris.')],\n                    usage=RequestUsage(input_tokens=56, output_tokens=7),\n                    model_name='gpt-4o',\n                    timestamp=datetime.datetime(...),\n                )\n            ),\n            End(data=FinalResult(output='The capital of France is Paris.')),\n        ]\n        '''\n        print('Final result:', agent_run.result.output)\n        #> Final result: The capital of France is Paris.\n```\n\nArgs:\n    node: The node to run next in the graph.\n\nReturns:\n    The next node returned by the graph logic, or an [`End`][pydantic_graph.nodes.End] node if\n    the run has completed.", "url": "https://ai.pydantic.dev/api/next/", "page": "pydantic_ai_slim/pydantic_ai/run", "source_site": "pydantic_ai"}
{"title": "AgentRun.usage", "anchor": "agentrun-usage", "heading_level": 0, "md_text": "Get usage statistics for the run so far, including token usage, model requests, and so on.", "url": "https://ai.pydantic.dev/api/usage/", "page": "pydantic_ai_slim/pydantic_ai/run", "source_site": "pydantic_ai"}
{"title": "AgentRunResult._set_output_tool_return", "anchor": "agentrunresult-_set_output_tool_return", "heading_level": 0, "md_text": "Set return content for the output tool.\n\nUseful if you want to continue the conversation and want to set the response to the output tool call.", "url": "https://ai.pydantic.dev/api/-set-output-tool-return/", "page": "pydantic_ai_slim/pydantic_ai/run", "source_site": "pydantic_ai"}
{"title": "AgentRunResult.all_messages", "anchor": "agentrunresult-all_messages", "heading_level": 0, "md_text": "Return the history of _messages.\n\nArgs:\n    output_tool_return_content: The return content of the tool call to set in the last message.\n        This provides a convenient way to modify the content of the output tool call if you want to continue\n        the conversation and want to set the response to the output tool call. If `None`, the last message will\n        not be modified.\n\nReturns:\n    List of messages.", "url": "https://ai.pydantic.dev/api/all-messages/", "page": "pydantic_ai_slim/pydantic_ai/run", "source_site": "pydantic_ai"}
{"title": "AgentRunResult.all_messages_json", "anchor": "agentrunresult-all_messages_json", "heading_level": 0, "md_text": "Return all messages from [`all_messages`][pydantic_ai.agent.AgentRunResult.all_messages] as JSON bytes.\n\nArgs:\n    output_tool_return_content: The return content of the tool call to set in the last message.\n        This provides a convenient way to modify the content of the output tool call if you want to continue\n        the conversation and want to set the response to the output tool call. If `None`, the last message will\n        not be modified.\n\nReturns:\n    JSON bytes representing the messages.", "url": "https://ai.pydantic.dev/api/all-messages-json/", "page": "pydantic_ai_slim/pydantic_ai/run", "source_site": "pydantic_ai"}
{"title": "AgentRunResult.new_messages", "anchor": "agentrunresult-new_messages", "heading_level": 0, "md_text": "Return new messages associated with this run.\n\nMessages from older runs are excluded.\n\nArgs:\n    output_tool_return_content: The return content of the tool call to set in the last message.\n        This provides a convenient way to modify the content of the output tool call if you want to continue\n        the conversation and want to set the response to the output tool call. If `None`, the last message will\n        not be modified.\n\nReturns:\n    List of new messages.", "url": "https://ai.pydantic.dev/api/new-messages/", "page": "pydantic_ai_slim/pydantic_ai/run", "source_site": "pydantic_ai"}
{"title": "AgentRunResult.new_messages_json", "anchor": "agentrunresult-new_messages_json", "heading_level": 0, "md_text": "Return new messages from [`new_messages`][pydantic_ai.agent.AgentRunResult.new_messages] as JSON bytes.\n\nArgs:\n    output_tool_return_content: The return content of the tool call to set in the last message.\n        This provides a convenient way to modify the content of the output tool call if you want to continue\n        the conversation and want to set the response to the output tool call. If `None`, the last message will\n        not be modified.\n\nReturns:\n    JSON bytes representing the new messages.", "url": "https://ai.pydantic.dev/api/new-messages-json/", "page": "pydantic_ai_slim/pydantic_ai/run", "source_site": "pydantic_ai"}
{"title": "AgentRunResult.response", "anchor": "agentrunresult-response", "heading_level": 0, "md_text": "Return the last response from the message history.", "url": "https://ai.pydantic.dev/api/response/", "page": "pydantic_ai_slim/pydantic_ai/run", "source_site": "pydantic_ai"}
{"title": "AgentRunResult.usage", "anchor": "agentrunresult-usage", "heading_level": 0, "md_text": "Return the usage of the whole run.", "url": "https://ai.pydantic.dev/api/usage/", "page": "pydantic_ai_slim/pydantic_ai/run", "source_site": "pydantic_ai"}
{"title": "AgentRunResult.timestamp", "anchor": "agentrunresult-timestamp", "heading_level": 0, "md_text": "Return the timestamp of last response.", "url": "https://ai.pydantic.dev/api/timestamp/", "page": "pydantic_ai_slim/pydantic_ai/run", "source_site": "pydantic_ai"}
{"title": "ModelRetry", "anchor": "modelretry", "heading_level": 0, "md_text": "Exception to raise when a tool function should be retried.\n\nThe agent will return the message to the model and ask it to try calling the function/tool again.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/exceptions", "source_site": "pydantic_ai"}
{"title": "CallDeferred", "anchor": "calldeferred", "heading_level": 0, "md_text": "Exception to raise when a tool call should be deferred.\n\nSee [tools docs](../deferred-tools.md#deferred-tools) for more information.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/exceptions", "source_site": "pydantic_ai"}
{"title": "ApprovalRequired", "anchor": "approvalrequired", "heading_level": 0, "md_text": "Exception to raise when a tool call requires human-in-the-loop approval.\n\nSee [tools docs](../deferred-tools.md#human-in-the-loop-tool-approval) for more information.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/exceptions", "source_site": "pydantic_ai"}
{"title": "UserError", "anchor": "usererror", "heading_level": 0, "md_text": "Error caused by a usage mistake by the application developer \u2014 You!", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/exceptions", "source_site": "pydantic_ai"}
{"title": "AgentRunError", "anchor": "agentrunerror", "heading_level": 0, "md_text": "Base class for errors occurring during an agent run.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/exceptions", "source_site": "pydantic_ai"}
{"title": "UsageLimitExceeded", "anchor": "usagelimitexceeded", "heading_level": 0, "md_text": "Error raised when a Model's usage exceeds the specified limits.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/exceptions", "source_site": "pydantic_ai"}
{"title": "UnexpectedModelBehavior", "anchor": "unexpectedmodelbehavior", "heading_level": 0, "md_text": "Error caused by unexpected Model behavior, e.g. an unexpected response code.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/exceptions", "source_site": "pydantic_ai"}
{"title": "ModelHTTPError", "anchor": "modelhttperror", "heading_level": 0, "md_text": "Raised when an model provider response has a status code of 4xx or 5xx.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/exceptions", "source_site": "pydantic_ai"}
{"title": "FallbackExceptionGroup", "anchor": "fallbackexceptiongroup", "heading_level": 0, "md_text": "A group of exceptions that can be raised when all fallback models fail.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/exceptions", "source_site": "pydantic_ai"}
{"title": "ToolRetryError", "anchor": "toolretryerror", "heading_level": 0, "md_text": "Exception used to signal a `ToolRetry` message should be returned to the LLM.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/exceptions", "source_site": "pydantic_ai"}
{"title": "IncompleteToolCall", "anchor": "incompletetoolcall", "heading_level": 0, "md_text": "Error raised when a model stops due to token limit while emitting a tool call.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/exceptions", "source_site": "pydantic_ai"}
{"title": "ModelRetry.__get_pydantic_core_schema__", "anchor": "modelretry-__get_pydantic_core_schema__", "heading_level": 0, "md_text": "Pydantic core schema to allow `ModelRetry` to be (de)serialized.", "url": "https://ai.pydantic.dev/api/--get-pydantic-core-schema--/", "page": "pydantic_ai_slim/pydantic_ai/exceptions", "source_site": "pydantic_ai"}
{"title": "_function_schema", "anchor": "_function_schema", "heading_level": 0, "md_text": "Used to build pydantic validators and JSON schemas from functions.\n\nThis module has to use numerous internal Pydantic APIs and is therefore brittle to changes in Pydantic.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_function_schema", "source_site": "pydantic_ai"}
{"title": "FunctionSchema", "anchor": "functionschema", "heading_level": 0, "md_text": "Internal information about a function schema.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_function_schema", "source_site": "pydantic_ai"}
{"title": "function_schema", "anchor": "function_schema", "heading_level": 0, "md_text": "Build a Pydantic validator and JSON schema from a tool function.\n\nArgs:\n    function: The function to build a validator and JSON schema for.\n    takes_ctx: Whether the function takes a `RunContext` first argument.\n    docstring_format: The docstring format to use.\n    require_parameter_descriptions: Whether to require descriptions for all tool function parameters.\n    schema_generator: The JSON schema generator class to use.\n\nReturns:\n    A `FunctionSchema` instance.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_function_schema", "source_site": "pydantic_ai"}
{"title": "_takes_ctx", "anchor": "_takes_ctx", "heading_level": 0, "md_text": "Check if a callable takes a `RunContext` first argument.\n\nArgs:\n    callable_obj: The callable to check.\n\nReturns:\n    `True` if the callable takes a `RunContext` as first argument, `False` otherwise.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_function_schema", "source_site": "pydantic_ai"}
{"title": "_build_schema", "anchor": "_build_schema", "heading_level": 0, "md_text": "Generate a typed dict schema for function parameters.\n\nArgs:\n    fields: The fields to generate a typed dict schema for.\n    var_kwargs_schema: The variable keyword arguments schema.\n    gen_schema: The `GenerateSchema` instance.\n    core_config: The core configuration.\n\nReturns:\n    tuple of (generated core schema, single arg name).", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_function_schema", "source_site": "pydantic_ai"}
{"title": "_is_call_ctx", "anchor": "_is_call_ctx", "heading_level": 0, "md_text": "Return whether the annotation is the `RunContext` class, parameterized or not.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_function_schema", "source_site": "pydantic_ai"}
{"title": "retries", "anchor": "retries", "heading_level": 0, "md_text": "Retries utilities based on tenacity, especially for HTTP requests.\n\nThis module provides HTTP transport wrappers and wait strategies that integrate with\nthe tenacity library to add retry capabilities to HTTP requests. The transports can be\nused with HTTP clients that support custom transports (such as httpx), while the wait\nstrategies can be used with any tenacity retry decorator.\n\nThe module includes:\n- TenacityTransport: Synchronous HTTP transport with retry capabilities\n- AsyncTenacityTransport: Asynchronous HTTP transport with retry capabilities\n- wait_retry_after: Wait strategy that respects HTTP Retry-After headers", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/retries", "source_site": "pydantic_ai"}
{"title": "RetryConfig", "anchor": "retryconfig", "heading_level": 0, "md_text": "The configuration for tenacity-based retrying.\n\nThese are precisely the arguments to the tenacity `retry` decorator, and they are generally\nused internally by passing them to that decorator via `@retry(**config)` or similar.\n\nAll fields are optional, and if not provided, the default values from the `tenacity.retry` decorator will be used.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/retries", "source_site": "pydantic_ai"}
{"title": "TenacityTransport", "anchor": "tenacitytransport", "heading_level": 0, "md_text": "Synchronous HTTP transport with tenacity-based retry functionality.\n\nThis transport wraps another BaseTransport and adds retry capabilities using the tenacity library.\nIt can be configured to retry requests based on various conditions such as specific exception types,\nresponse status codes, or custom validation logic.\n\nThe transport works by intercepting HTTP requests and responses, allowing the tenacity controller\nto determine when and how to retry failed requests. The validate_response function can be used\nto convert HTTP responses into exceptions that trigger retries.\n\nArgs:\n    wrapped: The underlying transport to wrap and add retry functionality to.\n    config: The arguments to use for the tenacity `retry` decorator, including retry conditions,\n        wait strategy, stop conditions, etc. See the tenacity docs for more info.\n    validate_response: Optional callable that takes a Response and can raise an exception\n        to be handled by the controller if the response should trigger a retry.\n        Common use case is to raise exceptions for certain HTTP status codes.\n        If None, no response validation is performed.\n\nExample:\n    ```python\n    from httpx import Client, HTTPStatusError, HTTPTransport\n    from tenacity import retry_if_exception_type, stop_after_attempt\n\n    from pydantic_ai.retries import RetryConfig, TenacityTransport, wait_retry_after\n\n    transport = TenacityTransport(\n        RetryConfig(\n            retry=retry_if_exception_type(HTTPStatusError),\n            wait=wait_retry_after(max_wait=300),\n            stop=stop_after_attempt(5),\n            reraise=True\n        ),\n        HTTPTransport(),\n        validate_response=lambda r: r.raise_for_status()\n    )\n    client = Client(transport=transport)\n    ```", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/retries", "source_site": "pydantic_ai"}
{"title": "AsyncTenacityTransport", "anchor": "asynctenacitytransport", "heading_level": 0, "md_text": "Asynchronous HTTP transport with tenacity-based retry functionality.\n\nThis transport wraps another AsyncBaseTransport and adds retry capabilities using the tenacity library.\nIt can be configured to retry requests based on various conditions such as specific exception types,\nresponse status codes, or custom validation logic.\n\nThe transport works by intercepting HTTP requests and responses, allowing the tenacity controller\nto determine when and how to retry failed requests. The validate_response function can be used\nto convert HTTP responses into exceptions that trigger retries.\n\nArgs:\n    wrapped: The underlying async transport to wrap and add retry functionality to.\n    config: The arguments to use for the tenacity `retry` decorator, including retry conditions,\n        wait strategy, stop conditions, etc. See the tenacity docs for more info.\n    validate_response: Optional callable that takes a Response and can raise an exception\n        to be handled by the controller if the response should trigger a retry.\n        Common use case is to raise exceptions for certain HTTP status codes.\n        If None, no response validation is performed.\n\nExample:\n    ```python\n    from httpx import AsyncClient, HTTPStatusError\n    from tenacity import retry_if_exception_type, stop_after_attempt\n\n    from pydantic_ai.retries import AsyncTenacityTransport, RetryConfig, wait_retry_after\n\n    transport = AsyncTenacityTransport(\n        RetryConfig(\n            retry=retry_if_exception_type(HTTPStatusError),\n            wait=wait_retry_after(max_wait=300),\n            stop=stop_after_attempt(5),\n            reraise=True\n        ),\n        validate_response=lambda r: r.raise_for_status()\n    )\n    client = AsyncClient(transport=transport)\n    ```", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/retries", "source_site": "pydantic_ai"}
{"title": "wait_retry_after", "anchor": "wait_retry_after", "heading_level": 0, "md_text": "Create a tenacity-compatible wait strategy that respects HTTP Retry-After headers.\n\nThis wait strategy checks if the exception contains an HTTPStatusError with a\nRetry-After header, and if so, waits for the time specified in the header.\nIf no header is present or parsing fails, it falls back to the provided strategy.\n\nThe Retry-After header can be in two formats:\n- An integer representing seconds to wait\n- An HTTP date string representing when to retry\n\nArgs:\n    fallback_strategy: Wait strategy to use when no Retry-After header is present\n                      or parsing fails. Defaults to exponential backoff with max 60s.\n    max_wait: Maximum time to wait in seconds, regardless of header value.\n             Defaults to 300 (5 minutes).\n\nReturns:\n    A wait function that can be used with tenacity retry decorators.\n\nExample:\n    ```python\n    from httpx import AsyncClient, HTTPStatusError\n    from tenacity import retry_if_exception_type, stop_after_attempt\n\n    from pydantic_ai.retries import AsyncTenacityTransport, RetryConfig, wait_retry_after\n\n    transport = AsyncTenacityTransport(\n        RetryConfig(\n            retry=retry_if_exception_type(HTTPStatusError),\n            wait=wait_retry_after(max_wait=120),\n            stop=stop_after_attempt(5),\n            reraise=True\n        ),\n        validate_response=lambda r: r.raise_for_status()\n    )\n    client = AsyncClient(transport=transport)\n    ```", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/retries", "source_site": "pydantic_ai"}
{"title": "TenacityTransport.handle_request", "anchor": "tenacitytransport-handle_request", "heading_level": 0, "md_text": "Handle an HTTP request with retry logic.\n\nArgs:\n    request: The HTTP request to handle.\n\nReturns:\n    The HTTP response.\n\nRaises:\n    RuntimeError: If the retry controller did not make any attempts.\n    Exception: Any exception raised by the wrapped transport or validation function.", "url": "https://ai.pydantic.dev/api/handle-request/", "page": "pydantic_ai_slim/pydantic_ai/retries", "source_site": "pydantic_ai"}
{"title": "AsyncTenacityTransport.handle_async_request", "anchor": "asynctenacitytransport-handle_async_request", "heading_level": 0, "md_text": "Handle an async HTTP request with retry logic.\n\nArgs:\n    request: The HTTP request to handle.\n\nReturns:\n    The HTTP response.\n\nRaises:\n    RuntimeError: If the retry controller did not make any attempts.\n    Exception: Any exception raised by the wrapped transport or validation function.", "url": "https://ai.pydantic.dev/api/handle-async-request/", "page": "pydantic_ai_slim/pydantic_ai/retries", "source_site": "pydantic_ai"}
{"title": "ModelSettings", "anchor": "modelsettings", "heading_level": 0, "md_text": "Settings to configure an LLM.\n\nHere we include only settings which apply to multiple models / model providers,\nthough not all of these settings are supported by all models.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/settings", "source_site": "pydantic_ai"}
{"title": "merge_model_settings", "anchor": "merge_model_settings", "heading_level": 0, "md_text": "Merge two sets of model settings, preferring the overrides.\n\nA common use case is: merge_model_settings(<agent settings>, <run settings>)", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/settings", "source_site": "pydantic_ai"}
{"title": "DeferredToolRequests", "anchor": "deferredtoolrequests", "heading_level": 0, "md_text": "Tool calls that require approval or external execution.\n\nThis can be used as an agent's `output_type` and will be used as the output of the agent run if the model called any deferred tools.\n\nResults can be passed to the next agent run using a [`DeferredToolResults`][pydantic_ai.tools.DeferredToolResults] object with the same tool call IDs.\n\nSee [deferred tools docs](../deferred-tools.md#deferred-tools) for more information.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/tools", "source_site": "pydantic_ai"}
{"title": "ToolApproved", "anchor": "toolapproved", "heading_level": 0, "md_text": "Indicates that a tool call has been approved and that the tool function should be executed.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/tools", "source_site": "pydantic_ai"}
{"title": "ToolDenied", "anchor": "tooldenied", "heading_level": 0, "md_text": "Indicates that a tool call has been denied and that a denial message should be returned to the model.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/tools", "source_site": "pydantic_ai"}
{"title": "DeferredToolResults", "anchor": "deferredtoolresults", "heading_level": 0, "md_text": "Results for deferred tool calls from a previous run that required approval or external execution.\n\nThe tool call IDs need to match those from the [`DeferredToolRequests`][pydantic_ai.output.DeferredToolRequests] output object from the previous run.\n\nSee [deferred tools docs](../deferred-tools.md#deferred-tools) for more information.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/tools", "source_site": "pydantic_ai"}
{"title": "Tool", "anchor": "tool", "heading_level": 0, "md_text": "A tool function for an agent.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/tools", "source_site": "pydantic_ai"}
{"title": "ToolDefinition", "anchor": "tooldefinition", "heading_level": 0, "md_text": "Definition of a tool passed to a model.\n\nThis is used for both function tools and output tools.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/tools", "source_site": "pydantic_ai"}
{"title": "Tool.__init__", "anchor": "tool-__init__", "heading_level": 0, "md_text": "Create a new tool instance.\n\nExample usage:\n\n```python {noqa=\"I001\"}\nfrom pydantic_ai import Agent, RunContext, Tool\n\nasync def my_tool(ctx: RunContext[int], x: int, y: int) -> str:\n    return f'{ctx.deps} {x} {y}'\n\nagent = Agent('test', tools=[Tool(my_tool)])\n```\n\nor with a custom prepare method:\n\n```python {noqa=\"I001\"}\n\nfrom pydantic_ai import Agent, RunContext, Tool\nfrom pydantic_ai.tools import ToolDefinition\n\nasync def my_tool(ctx: RunContext[int], x: int, y: int) -> str:\n    return f'{ctx.deps} {x} {y}'\n\nasync def prep_my_tool(\n    ctx: RunContext[int], tool_def: ToolDefinition\n) -> ToolDefinition | None:\n    # only register the tool if `deps == 42`\n    if ctx.deps == 42:\n        return tool_def\n\nagent = Agent('test', tools=[Tool(my_tool, prepare=prep_my_tool)])\n```\n\n\nArgs:\n    function: The Python function to call as the tool.\n    takes_ctx: Whether the function takes a [`RunContext`][pydantic_ai.tools.RunContext] first argument,\n        this is inferred if unset.\n    max_retries: Maximum number of retries allowed for this tool, set to the agent default if `None`.\n    name: Name of the tool, inferred from the function if `None`.\n    description: Description of the tool, inferred from the function if `None`.\n    prepare: custom method to prepare the tool definition for each step, return `None` to omit this\n        tool from a given step. This is useful if you want to customise a tool at call time,\n        or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].\n    docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].\n        Defaults to `'auto'`, such that the format is inferred from the structure of the docstring.\n    require_parameter_descriptions: If True, raise an error if a parameter description is missing. Defaults to False.\n    schema_generator: The JSON schema generator class to use. Defaults to `GenerateToolJsonSchema`.\n    strict: Whether to enforce JSON schema compliance (only affects OpenAI).\n        See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.\n    sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.\n    requires_approval: Whether this tool requires human-in-the-loop approval. Defaults to False.\n        See the [tools documentation](../deferred-tools.md#human-in-the-loop-tool-approval) for more info.\n    metadata: Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization.\n    function_schema: The function schema to use for the tool. If not provided, it will be generated.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/tools", "source_site": "pydantic_ai"}
{"title": "Tool.from_schema", "anchor": "tool-from_schema", "heading_level": 0, "md_text": "Creates a Pydantic tool from a function and a JSON schema.\n\nArgs:\n    function: The function to call.\n        This will be called with keywords only, and no validation of\n        the arguments will be performed.\n    name: The unique name of the tool that clearly communicates its purpose\n    description: Used to tell the model how/when/why to use the tool.\n        You can provide few-shot examples as a part of the description.\n    json_schema: The schema for the function arguments\n    takes_ctx: An optional boolean parameter indicating whether the function\n        accepts the context object as an argument.\n    sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.\n\nReturns:\n    A Pydantic tool that calls the function", "url": "https://ai.pydantic.dev/api/from-schema/", "page": "pydantic_ai_slim/pydantic_ai/tools", "source_site": "pydantic_ai"}
{"title": "Tool.prepare_tool_def", "anchor": "tool-prepare_tool_def", "heading_level": 0, "md_text": "Get the tool definition.\n\nBy default, this method creates a tool definition, then either returns it, or calls `self.prepare`\nif it's set.\n\nReturns:\n    return a `ToolDefinition` or `None` if the tools should not be registered for this run.", "url": "https://ai.pydantic.dev/api/prepare-tool-def/", "page": "pydantic_ai_slim/pydantic_ai/tools", "source_site": "pydantic_ai"}
{"title": "ToolDefinition.defer", "anchor": "tooldefinition-defer", "heading_level": 0, "md_text": "Whether calls to this tool will be deferred.\n\nSee the [tools documentation](../deferred-tools.md#deferred-tools) for more info.", "url": "https://ai.pydantic.dev/api/defer/", "page": "pydantic_ai_slim/pydantic_ai/tools", "source_site": "pydantic_ai"}
{"title": "RequestUsage", "anchor": "requestusage", "heading_level": 0, "md_text": "LLM usage associated with a single request.\n\nThis is an implementation of `genai_prices.types.AbstractUsage` so it can be used to calculate the price of the\nrequest using [genai-prices](https://github.com/pydantic/genai-prices).", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/usage", "source_site": "pydantic_ai"}
{"title": "RunUsage", "anchor": "runusage", "heading_level": 0, "md_text": "LLM usage associated with an agent run.\n\nResponsibility for calculating request usage is on the model; Pydantic AI simply sums the usage information across requests.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/usage", "source_site": "pydantic_ai"}
{"title": "_incr_usage_tokens", "anchor": "_incr_usage_tokens", "heading_level": 0, "md_text": "Increment the usage in place.\n\nArgs:\n    slf: The usage to increment.\n    incr_usage: The usage to increment by.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/usage", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 0, "md_text": "Deprecated alias for `RunUsage`.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/usage", "source_site": "pydantic_ai"}
{"title": "UsageLimits", "anchor": "usagelimits", "heading_level": 0, "md_text": "Limits on model usage.\n\nThe request count is tracked by pydantic_ai, and the request limit is checked before each request to the model.\nToken counts are provided in responses from the model, and the token limits are checked after each response.\n\nEach of the limits can be set to `None` to disable that limit.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/usage", "source_site": "pydantic_ai"}
{"title": "UsageBase.total_tokens", "anchor": "usagebase-total_tokens", "heading_level": 0, "md_text": "Sum of `input_tokens + output_tokens`.", "url": "https://ai.pydantic.dev/api/total-tokens/", "page": "pydantic_ai_slim/pydantic_ai/usage", "source_site": "pydantic_ai"}
{"title": "UsageBase.opentelemetry_attributes", "anchor": "usagebase-opentelemetry_attributes", "heading_level": 0, "md_text": "Get the token usage values as OpenTelemetry attributes.", "url": "https://ai.pydantic.dev/api/opentelemetry-attributes/", "page": "pydantic_ai_slim/pydantic_ai/usage", "source_site": "pydantic_ai"}
{"title": "UsageBase.has_values", "anchor": "usagebase-has_values", "heading_level": 0, "md_text": "Whether any values are set and non-zero.", "url": "https://ai.pydantic.dev/api/has-values/", "page": "pydantic_ai_slim/pydantic_ai/usage", "source_site": "pydantic_ai"}
{"title": "RequestUsage.incr", "anchor": "requestusage-incr", "heading_level": 0, "md_text": "Increment the usage in place.\n\nArgs:\n    incr_usage: The usage to increment by.", "url": "https://ai.pydantic.dev/api/incr/", "page": "pydantic_ai_slim/pydantic_ai/usage", "source_site": "pydantic_ai"}
{"title": "RequestUsage.__add__", "anchor": "requestusage-__add__", "heading_level": 0, "md_text": "Add two RequestUsages together.\n\nThis is provided so it's trivial to sum usage information from multiple parts of a response.\n\n**WARNING:** this CANNOT be used to sum multiple requests without breaking some pricing calculations.", "url": "https://ai.pydantic.dev/api/--add--/", "page": "pydantic_ai_slim/pydantic_ai/usage", "source_site": "pydantic_ai"}
{"title": "RequestUsage.extract", "anchor": "requestusage-extract", "heading_level": 0, "md_text": "Extract usage information from the response data using genai-prices.\n\nArgs:\n    data: The response data from the model API.\n    provider: The actual provider ID\n    provider_url: The provider base_url\n    provider_fallback: The fallback provider ID to use if the actual provider is not found in genai-prices.\n        For example, an OpenAI model should set this to \"openai\" in case it has an obscure provider ID.\n    api_flavor: The API flavor to use when extracting usage information,\n        e.g. 'chat' or 'responses' for OpenAI.\n    details: Becomes the `details` field on the returned `RequestUsage` for convenience.", "url": "https://ai.pydantic.dev/api/extract/", "page": "pydantic_ai_slim/pydantic_ai/usage", "source_site": "pydantic_ai"}
{"title": "RunUsage.incr", "anchor": "runusage-incr", "heading_level": 0, "md_text": "Increment the usage in place.\n\nArgs:\n    incr_usage: The usage to increment by.", "url": "https://ai.pydantic.dev/api/incr/", "page": "pydantic_ai_slim/pydantic_ai/usage", "source_site": "pydantic_ai"}
{"title": "RunUsage.__add__", "anchor": "runusage-__add__", "heading_level": 0, "md_text": "Add two RunUsages together.\n\nThis is provided so it's trivial to sum usage information from multiple runs.", "url": "https://ai.pydantic.dev/api/--add--/", "page": "pydantic_ai_slim/pydantic_ai/usage", "source_site": "pydantic_ai"}
{"title": "UsageLimits.has_token_limits", "anchor": "usagelimits-has_token_limits", "heading_level": 0, "md_text": "Returns `True` if this instance places any limits on token counts.\n\nIf this returns `False`, the `check_tokens` method will never raise an error.\n\nThis is useful because if we have token limits, we need to check them after receiving each streamed message.\nIf there are no limits, we can skip that processing in the streaming response iterator.", "url": "https://ai.pydantic.dev/api/has-token-limits/", "page": "pydantic_ai_slim/pydantic_ai/usage", "source_site": "pydantic_ai"}
{"title": "UsageLimits.check_before_request", "anchor": "usagelimits-check_before_request", "heading_level": 0, "md_text": "Raises a `UsageLimitExceeded` exception if the next request would exceed any of the limits.", "url": "https://ai.pydantic.dev/api/check-before-request/", "page": "pydantic_ai_slim/pydantic_ai/usage", "source_site": "pydantic_ai"}
{"title": "UsageLimits.check_tokens", "anchor": "usagelimits-check_tokens", "heading_level": 0, "md_text": "Raises a `UsageLimitExceeded` exception if the usage exceeds any of the token limits.", "url": "https://ai.pydantic.dev/api/check-tokens/", "page": "pydantic_ai_slim/pydantic_ai/usage", "source_site": "pydantic_ai"}
{"title": "UsageLimits.check_before_tool_call", "anchor": "usagelimits-check_before_tool_call", "heading_level": 0, "md_text": "Raises a `UsageLimitExceeded` exception if the next tool call(s) would exceed the tool call limit.", "url": "https://ai.pydantic.dev/api/check-before-tool-call/", "page": "pydantic_ai_slim/pydantic_ai/usage", "source_site": "pydantic_ai"}
{"title": "is_model_like", "anchor": "is_model_like", "heading_level": 0, "md_text": "Check if something is a pydantic model, dataclass or typedict.\n\nThese should all generate a JSON Schema with `{\"type\": \"object\"}` and therefore be usable directly as\nfunction parameters.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_utils", "source_site": "pydantic_ai"}
{"title": "_contains_ref", "anchor": "_contains_ref", "heading_level": 0, "md_text": "Recursively check if an object contains any $ref keys.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_utils", "source_site": "pydantic_ai"}
{"title": "Some", "anchor": "some", "heading_level": 0, "md_text": "Analogous to Rust's `Option::Some` type.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_utils", "source_site": "pydantic_ai"}
{"title": "Unset", "anchor": "unset", "heading_level": 0, "md_text": "A singleton to represent an unset value.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_utils", "source_site": "pydantic_ai"}
{"title": "group_by_temporal", "anchor": "group_by_temporal", "heading_level": 0, "md_text": "Group items from an async iterable into lists based on time interval between them.\n\nEffectively, this debounces the iterator.\n\nThis returns a context manager usable as an iterator so any pending tasks can be cancelled if an error occurs\nduring iteration.\n\nUsage:\n\n```python\nasync with group_by_temporal(yield_groups(), 0.1) as groups_iter:\n    async for groups in groups_iter:\n        print(groups)\n```\n\nArgs:\n    aiterable: The async iterable to group.\n    soft_max_interval: Maximum interval over which to group items, this should avoid a trickle of items causing\n        a group to never be yielded. It's a soft max in the sense that once we're over this time, we yield items\n        as soon as `anext(aiter)` returns. If `None`, no grouping/debouncing is performed\n\nReturns:\n    A context manager usable as an async iterable of lists of items produced by the input async iterable.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_utils", "source_site": "pydantic_ai"}
{"title": "sync_anext", "anchor": "sync_anext", "heading_level": 0, "md_text": "Get the next item from a sync iterator, raising `StopAsyncIteration` if it's exhausted.\n\nUseful when iterating over a sync iterator in an async context.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_utils", "source_site": "pydantic_ai"}
{"title": "guard_tool_call_id", "anchor": "guard_tool_call_id", "heading_level": 0, "md_text": "Type guard that either returns the tool call id or generates a new one if it's None.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_utils", "source_site": "pydantic_ai"}
{"title": "generate_tool_call_id", "anchor": "generate_tool_call_id", "heading_level": 0, "md_text": "Generate a tool call id.\n\nEnsure that the tool call id is unique.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_utils", "source_site": "pydantic_ai"}
{"title": "PeekableAsyncStream", "anchor": "peekableasyncstream", "heading_level": 0, "md_text": "Wraps an async iterable of type T and allows peeking at the *next* item without consuming it.\n\nWe only buffer one item at a time (the next item). Once that item is yielded, it is discarded.\nThis is a single-pass stream.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_utils", "source_site": "pydantic_ai"}
{"title": "dataclasses_no_defaults_repr", "anchor": "dataclasses_no_defaults_repr", "heading_level": 0, "md_text": "Exclude fields with values equal to the field default.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_utils", "source_site": "pydantic_ai"}
{"title": "is_async_callable", "anchor": "is_async_callable", "heading_level": 0, "md_text": "Correctly check if a callable is async.\n\nThis function was copied from Starlette:\nhttps://github.com/encode/starlette/blob/78da9b9e218ab289117df7d62aee200ed4c59617/starlette/_utils.py#L36-L40", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_utils", "source_site": "pydantic_ai"}
{"title": "_update_mapped_json_schema_refs", "anchor": "_update_mapped_json_schema_refs", "heading_level": 0, "md_text": "Update $refs in a schema to use the new names from name_mapping.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_utils", "source_site": "pydantic_ai"}
{"title": "merge_json_schema_defs", "anchor": "merge_json_schema_defs", "heading_level": 0, "md_text": "Merges the `$defs` from different JSON schemas into a single deduplicated `$defs`, handling name collisions of `$defs` that are not the same, and rewrites `$ref`s to point to the new `$defs`.\n\nReturns a tuple of the rewritten schemas and a dictionary of the new `$defs`.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_utils", "source_site": "pydantic_ai"}
{"title": "validate_empty_kwargs", "anchor": "validate_empty_kwargs", "heading_level": 0, "md_text": "Validate that no unknown kwargs remain after processing.\n\nArgs:\n    _kwargs: Dictionary of remaining kwargs after specific ones have been processed.\n\nRaises:\n    UserError: If any unknown kwargs remain.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_utils", "source_site": "pydantic_ai"}
{"title": "get_union_args", "anchor": "get_union_args", "heading_level": 0, "md_text": "Extract the arguments of a Union type if `tp` is a union, otherwise return an empty tuple.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_utils", "source_site": "pydantic_ai"}
{"title": "PeekableAsyncStream.peek", "anchor": "peekableasyncstream-peek", "heading_level": 0, "md_text": "Returns the next item that would be yielded without consuming it.\n\nReturns None if the stream is exhausted.", "url": "https://ai.pydantic.dev/api/peek/", "page": "pydantic_ai_slim/pydantic_ai/_utils", "source_site": "pydantic_ai"}
{"title": "PeekableAsyncStream.is_exhausted", "anchor": "peekableasyncstream-is_exhausted", "heading_level": 0, "md_text": "Returns True if the stream is exhausted, False otherwise.", "url": "https://ai.pydantic.dev/api/is-exhausted/", "page": "pydantic_ai_slim/pydantic_ai/_utils", "source_site": "pydantic_ai"}
{"title": "PeekableAsyncStream.__anext__", "anchor": "peekableasyncstream-__anext__", "heading_level": 0, "md_text": "Yields the buffered item if present, otherwise fetches the next item from the underlying source.\n\nRaises StopAsyncIteration if the stream is exhausted.", "url": "https://ai.pydantic.dev/api/--anext--/", "page": "pydantic_ai_slim/pydantic_ai/_utils", "source_site": "pydantic_ai"}
{"title": "JsonSchemaTransformer", "anchor": "jsonschematransformer", "heading_level": 0, "md_text": "Walks a JSON schema, applying transformations to it at each level.\n\nNote: We may eventually want to rework tools to build the JSON schema from the type directly, using a subclass of\npydantic.json_schema.GenerateJsonSchema, rather than making use of this machinery.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_json_schema", "source_site": "pydantic_ai"}
{"title": "InlineDefsJsonSchemaTransformer", "anchor": "inlinedefsjsonschematransformer", "heading_level": 0, "md_text": "Transforms the JSON Schema to inline $defs.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_json_schema", "source_site": "pydantic_ai"}
{"title": "JsonSchemaTransformer.transform", "anchor": "jsonschematransformer-transform", "heading_level": 0, "md_text": "Make changes to the schema.", "url": "https://ai.pydantic.dev/api/transform/", "page": "pydantic_ai_slim/pydantic_ai/_json_schema", "source_site": "pydantic_ai"}
{"title": "InstrumentationNames", "anchor": "instrumentationnames", "heading_level": 0, "md_text": "Configuration for instrumentation span names and attributes based on version.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_instrumentation", "source_site": "pydantic_ai"}
{"title": "InstrumentationNames.for_version", "anchor": "instrumentationnames-for_version", "heading_level": 0, "md_text": "Create instrumentation configuration for a specific version.\n\nArgs:\n    version: The instrumentation version (1, 2, or 3+)\n\nReturns:\n    InstrumentationConfig instance with version-appropriate settings", "url": "https://ai.pydantic.dev/api/for-version/", "page": "pydantic_ai_slim/pydantic_ai/_instrumentation", "source_site": "pydantic_ai"}
{"title": "InstrumentationNames.get_agent_run_span_name", "anchor": "instrumentationnames-get_agent_run_span_name", "heading_level": 0, "md_text": "Get the formatted agent span name.\n\nArgs:\n    agent_name: Name of the agent being executed\n\nReturns:\n    Formatted span name", "url": "https://ai.pydantic.dev/api/get-agent-run-span-name/", "page": "pydantic_ai_slim/pydantic_ai/_instrumentation", "source_site": "pydantic_ai"}
{"title": "InstrumentationNames.get_tool_span_name", "anchor": "instrumentationnames-get_tool_span_name", "heading_level": 0, "md_text": "Get the formatted tool span name.\n\nArgs:\n    tool_name: Name of the tool being executed\n\nReturns:\n    Formatted span name", "url": "https://ai.pydantic.dev/api/get-tool-span-name/", "page": "pydantic_ai_slim/pydantic_ai/_instrumentation", "source_site": "pydantic_ai"}
{"title": "InstrumentationNames.get_output_tool_span_name", "anchor": "instrumentationnames-get_output_tool_span_name", "heading_level": 0, "md_text": "Get the formatted output tool span name.\n\nArgs:\n    tool_name: Name of the tool being executed\n\nReturns:\n    Formatted span name", "url": "https://ai.pydantic.dev/api/get-output-tool-span-name/", "page": "pydantic_ai_slim/pydantic_ai/_instrumentation", "source_site": "pydantic_ai"}
{"title": "SimpleCodeBlock", "anchor": "simplecodeblock", "heading_level": 0, "md_text": "Customized code blocks in markdown.\n\nThis avoids a background color which messes up copy-pasting and sets the language name as dim prefix and suffix.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_cli", "source_site": "pydantic_ai"}
{"title": "LeftHeading", "anchor": "leftheading", "heading_level": 0, "md_text": "Customized headings in markdown to stop centering and prepend markdown style hashes.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_cli", "source_site": "pydantic_ai"}
{"title": "cli_exit", "anchor": "cli_exit", "heading_level": 0, "md_text": "Run the CLI and exit.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_cli", "source_site": "pydantic_ai"}
{"title": "cli", "anchor": "cli", "heading_level": 0, "md_text": "Run the CLI and return the exit code for the process.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_cli", "source_site": "pydantic_ai"}
{"title": "_parts_manager", "anchor": "_parts_manager", "heading_level": 0, "md_text": "This module provides functionality to manage and update parts of a model's streamed response.\n\nThe manager tracks which parts (in particular, text and tool calls) correspond to which\nvendor-specific identifiers (e.g., `index`, `tool_call_id`, etc., as appropriate for a given model),\nand produces Pydantic AI-format events as appropriate for consumers of the streaming APIs.\n\nThe \"vendor-specific identifiers\" to use depend on the semantics of the responses of the responses from the vendor,\nand are tightly coupled to the specific model being used, and the Pydantic AI Model subclass implementation.\n\nThis `ModelResponsePartsManager` is used in each of the subclasses of `StreamedResponse` as a way to consolidate\nevent-emitting logic.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_parts_manager", "source_site": "pydantic_ai"}
{"title": "ModelResponsePartsManager", "anchor": "modelresponsepartsmanager", "heading_level": 0, "md_text": "Manages a sequence of parts that make up a model's streamed response.\n\nParts are generally added and/or updated by providing deltas, which are tracked by vendor-specific IDs.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_parts_manager", "source_site": "pydantic_ai"}
{"title": "ModelResponsePartsManager.get_parts", "anchor": "modelresponsepartsmanager-get_parts", "heading_level": 0, "md_text": "Return only model response parts that are complete (i.e., not ToolCallPartDelta's).\n\nReturns:\n    A list of ModelResponsePart objects. ToolCallPartDelta objects are excluded.", "url": "https://ai.pydantic.dev/api/get-parts/", "page": "pydantic_ai_slim/pydantic_ai/_parts_manager", "source_site": "pydantic_ai"}
{"title": "ModelResponsePartsManager.handle_text_delta", "anchor": "modelresponsepartsmanager-handle_text_delta", "heading_level": 0, "md_text": "Handle incoming text content, creating or updating a TextPart in the manager as appropriate.\n\nWhen `vendor_part_id` is None, the latest part is updated if it exists and is a TextPart;\notherwise, a new TextPart is created. When a non-None ID is specified, the TextPart corresponding\nto that vendor ID is either created or updated.\n\nArgs:\n    vendor_part_id: The ID the vendor uses to identify this piece\n        of text. If None, a new part will be created unless the latest part is already\n        a TextPart.\n    content: The text content to append to the appropriate TextPart.\n    id: An optional id for the text part.\n    thinking_tags: If provided, will handle content between the thinking tags as thinking parts.\n    ignore_leading_whitespace: If True, will ignore leading whitespace in the content.\n\nReturns:\n    - A `PartStartEvent` if a new part was created.\n    - A `PartDeltaEvent` if an existing part was updated.\n    - `None` if no new event is emitted (e.g., the first text part was all whitespace).\n\nRaises:\n    UnexpectedModelBehavior: If attempting to apply text content to a part that is not a TextPart.", "url": "https://ai.pydantic.dev/api/handle-text-delta/", "page": "pydantic_ai_slim/pydantic_ai/_parts_manager", "source_site": "pydantic_ai"}
{"title": "ModelResponsePartsManager.handle_thinking_delta", "anchor": "modelresponsepartsmanager-handle_thinking_delta", "heading_level": 0, "md_text": "Handle incoming thinking content, creating or updating a ThinkingPart in the manager as appropriate.\n\nWhen `vendor_part_id` is None, the latest part is updated if it exists and is a ThinkingPart;\notherwise, a new ThinkingPart is created. When a non-None ID is specified, the ThinkingPart corresponding\nto that vendor ID is either created or updated.\n\nArgs:\n    vendor_part_id: The ID the vendor uses to identify this piece\n        of thinking. If None, a new part will be created unless the latest part is already\n        a ThinkingPart.\n    content: The thinking content to append to the appropriate ThinkingPart.\n    id: An optional id for the thinking part.\n    signature: An optional signature for the thinking content.\n    provider_name: An optional provider name for the thinking part.\n\nReturns:\n    A `PartStartEvent` if a new part was created, or a `PartDeltaEvent` if an existing part was updated.\n\nRaises:\n    UnexpectedModelBehavior: If attempting to apply a thinking delta to a part that is not a ThinkingPart.", "url": "https://ai.pydantic.dev/api/handle-thinking-delta/", "page": "pydantic_ai_slim/pydantic_ai/_parts_manager", "source_site": "pydantic_ai"}
{"title": "ModelResponsePartsManager.handle_tool_call_delta", "anchor": "modelresponsepartsmanager-handle_tool_call_delta", "heading_level": 0, "md_text": "Handle or update a tool call, creating or updating a `ToolCallPart`, `BuiltinToolCallPart`, or `ToolCallPartDelta`.\n\nManaged items remain as `ToolCallPartDelta`s until they have at least a tool_name, at which\npoint they are upgraded to `ToolCallPart`s.\n\nIf `vendor_part_id` is None, updates the latest matching ToolCallPart (or ToolCallPartDelta)\nif any. Otherwise, a new part (or delta) may be created.\n\nArgs:\n    vendor_part_id: The ID the vendor uses for this tool call.\n        If None, the latest matching tool call may be updated.\n    tool_name: The name of the tool. If None, the manager does not enforce\n        a name match when `vendor_part_id` is None.\n    args: Arguments for the tool call, either as a string, a dictionary of key-value pairs, or None.\n    tool_call_id: An optional string representing an identifier for this tool call.\n\nReturns:\n    - A `PartStartEvent` if a new ToolCallPart or BuiltinToolCallPart is created.\n    - A `PartDeltaEvent` if an existing part is updated.\n    - `None` if no new event is emitted (e.g., the part is still incomplete).\n\nRaises:\n    UnexpectedModelBehavior: If attempting to apply a tool call delta to a part that is not\n        a ToolCallPart, BuiltinToolCallPart, or ToolCallPartDelta.", "url": "https://ai.pydantic.dev/api/handle-tool-call-delta/", "page": "pydantic_ai_slim/pydantic_ai/_parts_manager", "source_site": "pydantic_ai"}
{"title": "ModelResponsePartsManager.handle_tool_call_part", "anchor": "modelresponsepartsmanager-handle_tool_call_part", "heading_level": 0, "md_text": "Immediately create or fully-overwrite a ToolCallPart with the given information.\n\nThis does not apply a delta; it directly sets the tool call part contents.\n\nArgs:\n    vendor_part_id: The vendor's ID for this tool call part. If not\n        None and an existing part is found, that part is overwritten.\n    tool_name: The name of the tool being invoked.\n    args: The arguments for the tool call, either as a string, a dictionary, or None.\n    tool_call_id: An optional string identifier for this tool call.\n    id: An optional identifier for this tool call part.\n\nReturns:\n    ModelResponseStreamEvent: A `PartStartEvent` indicating that a new tool call part\n    has been added to the manager, or replaced an existing part.", "url": "https://ai.pydantic.dev/api/handle-tool-call-part/", "page": "pydantic_ai_slim/pydantic_ai/_parts_manager", "source_site": "pydantic_ai"}
{"title": "ModelResponsePartsManager.handle_part", "anchor": "modelresponsepartsmanager-handle_part", "heading_level": 0, "md_text": "Create or overwrite a ModelResponsePart.\n\nArgs:\n    vendor_part_id: The vendor's ID for this tool call part. If not\n        None and an existing part is found, that part is overwritten.\n    part: The ModelResponsePart.\n\nReturns:\n    ModelResponseStreamEvent: A `PartStartEvent` indicating that a new part\n    has been added to the manager, or replaced an existing part.", "url": "https://ai.pydantic.dev/api/handle-part/", "page": "pydantic_ai_slim/pydantic_ai/_parts_manager", "source_site": "pydantic_ai"}
{"title": "ToolManager", "anchor": "toolmanager", "heading_level": 0, "md_text": "Manages tools for an agent run step. It caches the agent run's toolset's tool definitions and handles calling tools and retries.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_tool_manager", "source_site": "pydantic_ai"}
{"title": "ToolManager.sequential_tool_calls", "anchor": "toolmanager-sequential_tool_calls", "heading_level": 0, "md_text": "Run tool calls sequentially during the context.", "url": "https://ai.pydantic.dev/api/sequential-tool-calls/", "page": "pydantic_ai_slim/pydantic_ai/_tool_manager", "source_site": "pydantic_ai"}
{"title": "ToolManager.for_run_step", "anchor": "toolmanager-for_run_step", "heading_level": 0, "md_text": "Build a new tool manager for the next run step, carrying over the retries from the current run step.", "url": "https://ai.pydantic.dev/api/for-run-step/", "page": "pydantic_ai_slim/pydantic_ai/_tool_manager", "source_site": "pydantic_ai"}
{"title": "ToolManager.tool_defs", "anchor": "toolmanager-tool_defs", "heading_level": 0, "md_text": "The tool definitions for the tools in this tool manager.", "url": "https://ai.pydantic.dev/api/tool-defs/", "page": "pydantic_ai_slim/pydantic_ai/_tool_manager", "source_site": "pydantic_ai"}
{"title": "ToolManager.should_call_sequentially", "anchor": "toolmanager-should_call_sequentially", "heading_level": 0, "md_text": "Whether to require sequential tool calls for a list of tool calls.", "url": "https://ai.pydantic.dev/api/should-call-sequentially/", "page": "pydantic_ai_slim/pydantic_ai/_tool_manager", "source_site": "pydantic_ai"}
{"title": "ToolManager.get_tool_def", "anchor": "toolmanager-get_tool_def", "heading_level": 0, "md_text": "Get the tool definition for a given tool name, or `None` if the tool is unknown.", "url": "https://ai.pydantic.dev/api/get-tool-def/", "page": "pydantic_ai_slim/pydantic_ai/_tool_manager", "source_site": "pydantic_ai"}
{"title": "ToolManager.handle_call", "anchor": "toolmanager-handle_call", "heading_level": 0, "md_text": "Handle a tool call by validating the arguments, calling the tool, and handling retries.\n\nArgs:\n    call: The tool call part to handle.\n    allow_partial: Whether to allow partial validation of the tool arguments.\n    wrap_validation_errors: Whether to wrap validation errors in a retry prompt part.\n    usage_limits: Optional usage limits to check before executing tools.", "url": "https://ai.pydantic.dev/api/handle-call/", "page": "pydantic_ai_slim/pydantic_ai/_tool_manager", "source_site": "pydantic_ai"}
{"title": "ToolManager._call_function_tool", "anchor": "toolmanager-_call_function_tool", "heading_level": 0, "md_text": "See <https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-spans/#execute-tool-span>.", "url": "https://ai.pydantic.dev/api/-call-function-tool/", "page": "pydantic_ai_slim/pydantic_ai/_tool_manager", "source_site": "pydantic_ai"}
{"title": "map_from_mcp_params", "anchor": "map_from_mcp_params", "heading_level": 0, "md_text": "Convert from MCP create message request parameters to pydantic-ai messages.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_mcp", "source_site": "pydantic_ai"}
{"title": "map_from_pai_messages", "anchor": "map_from_pai_messages", "heading_level": 0, "md_text": "Convert from pydantic-ai messages to MCP sampling messages.\n\nReturns:\n    A tuple containing the system prompt and a list of sampling messages.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_mcp", "source_site": "pydantic_ai"}
{"title": "map_from_model_response", "anchor": "map_from_model_response", "heading_level": 0, "md_text": "Convert from a model response to MCP text content.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_mcp", "source_site": "pydantic_ai"}
{"title": "map_from_sampling_content", "anchor": "map_from_sampling_content", "heading_level": 0, "md_text": "Convert from sampling content to a pydantic-ai text part.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_mcp", "source_site": "pydantic_ai"}
{"title": "split_content_into_text_and_thinking", "anchor": "split_content_into_text_and_thinking", "heading_level": 0, "md_text": "Split a string into text and thinking parts.\n\nSome models don't return the thinking part as a separate part, but rather as a tag in the content.\nThis function splits the content into text and thinking parts.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_thinking_part", "source_site": "pydantic_ai"}
{"title": "RunContext", "anchor": "runcontext", "heading_level": 0, "md_text": "Information about the current call.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_run_context", "source_site": "pydantic_ai"}
{"title": "RunContext.last_attempt", "anchor": "runcontext-last_attempt", "heading_level": 0, "md_text": "Whether this is the last attempt at running this tool before an error is raised.", "url": "https://ai.pydantic.dev/api/last-attempt/", "page": "pydantic_ai_slim/pydantic_ai/_run_context", "source_site": "pydantic_ai"}
{"title": "MCPServer", "anchor": "mcpserver", "heading_level": 0, "md_text": "Base class for attaching agents to MCP servers.\n\nSee <https://modelcontextprotocol.io> for more information.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/mcp", "source_site": "pydantic_ai"}
{"title": "MCPServerStdio", "anchor": "mcpserverstdio", "heading_level": 0, "md_text": "Runs an MCP server in a subprocess and communicates with it over stdin/stdout.\n\nThis class implements the stdio transport from the MCP specification.\nSee <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio> for more information.\n\nExample:\n```python {py=\"3.10\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\n\nserver = MCPServerStdio(  # (1)!\n    'uv', args=['run', 'mcp-run-python', 'stdio'], timeout=10\n)\nagent = Agent('openai:gpt-4o', toolsets=[server])\n```\n\n1. See [MCP Run Python](https://github.com/pydantic/mcp-run-python) for more information.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/mcp", "source_site": "pydantic_ai"}
{"title": "MCPServerSSE", "anchor": "mcpserversse", "heading_level": 0, "md_text": "An MCP server that connects over streamable HTTP connections.\n\nThis class implements the SSE transport from the MCP specification.\nSee <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse> for more information.\n\nExample:\n```python {py=\"3.10\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerSSE\n\nserver = MCPServerSSE('http://localhost:3001/sse')\nagent = Agent('openai:gpt-4o', toolsets=[server])\n```", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/mcp", "source_site": "pydantic_ai"}
{"title": "MCPServerHTTP", "anchor": "mcpserverhttp", "heading_level": 0, "md_text": "An MCP server that connects over HTTP using the old SSE transport.\n\nThis class implements the SSE transport from the MCP specification.\nSee <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse> for more information.\n\nExample:\n```python {py=\"3.10\" test=\"skip\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerHTTP\n\nserver = MCPServerHTTP('http://localhost:3001/sse')\nagent = Agent('openai:gpt-4o', toolsets=[server])\n```", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/mcp", "source_site": "pydantic_ai"}
{"title": "MCPServerStreamableHTTP", "anchor": "mcpserverstreamablehttp", "heading_level": 0, "md_text": "An MCP server that connects over HTTP using the Streamable HTTP transport.\n\nThis class implements the Streamable HTTP transport from the MCP specification.\nSee <https://modelcontextprotocol.io/introduction#streamable-http> for more information.\n\nExample:\n```python {py=\"3.10\"}\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStreamableHTTP\n\nserver = MCPServerStreamableHTTP('http://localhost:8000/mcp')\nagent = Agent('openai:gpt-4o', toolsets=[server])\n```", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/mcp", "source_site": "pydantic_ai"}
{"title": "MCPServerConfig", "anchor": "mcpserverconfig", "heading_level": 0, "md_text": "Configuration for MCP servers.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/mcp", "source_site": "pydantic_ai"}
{"title": "load_mcp_servers", "anchor": "load_mcp_servers", "heading_level": 0, "md_text": "Load MCP servers from a configuration file.\n\nArgs:\n    config_path: The path to the configuration file.\n\nReturns:\n    A list of MCP servers.\n\nRaises:\n    FileNotFoundError: If the configuration file does not exist.\n    ValidationError: If the configuration file does not match the schema.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/mcp", "source_site": "pydantic_ai"}
{"title": "MCPServer.client_streams", "anchor": "mcpserver-client_streams", "heading_level": 0, "md_text": "Create the streams for the MCP server.", "url": "https://ai.pydantic.dev/api/client-streams/", "page": "pydantic_ai_slim/pydantic_ai/mcp", "source_site": "pydantic_ai"}
{"title": "MCPServer.server_info", "anchor": "mcpserver-server_info", "heading_level": 0, "md_text": "Access the information send by the MCP server during initialization.", "url": "https://ai.pydantic.dev/api/server-info/", "page": "pydantic_ai_slim/pydantic_ai/mcp", "source_site": "pydantic_ai"}
{"title": "MCPServer.list_tools", "anchor": "mcpserver-list_tools", "heading_level": 0, "md_text": "Retrieve tools that are currently active on the server.\n\nNote:\n- We don't cache tools as they might change.\n- We also don't subscribe to the server to avoid complexity.", "url": "https://ai.pydantic.dev/api/list-tools/", "page": "pydantic_ai_slim/pydantic_ai/mcp", "source_site": "pydantic_ai"}
{"title": "MCPServer.direct_call_tool", "anchor": "mcpserver-direct_call_tool", "heading_level": 0, "md_text": "Call a tool on the server.\n\nArgs:\n    name: The name of the tool to call.\n    args: The arguments to pass to the tool.\n    metadata: Request-level metadata (optional)\n\nReturns:\n    The result of the tool call.\n\nRaises:\n    ModelRetry: If the tool call fails.", "url": "https://ai.pydantic.dev/api/direct-call-tool/", "page": "pydantic_ai_slim/pydantic_ai/mcp", "source_site": "pydantic_ai"}
{"title": "MCPServer.__aenter__", "anchor": "mcpserver-__aenter__", "heading_level": 0, "md_text": "Enter the MCP server context.\n\nThis will initialize the connection to the server.\nIf this server is an [`MCPServerStdio`][pydantic_ai.mcp.MCPServerStdio], the server will first be started as a subprocess.\n\nThis is a no-op if the MCP server has already been entered.", "url": "https://ai.pydantic.dev/api/--aenter--/", "page": "pydantic_ai_slim/pydantic_ai/mcp", "source_site": "pydantic_ai"}
{"title": "MCPServer.is_running", "anchor": "mcpserver-is_running", "heading_level": 0, "md_text": "Check if the MCP server is running.", "url": "https://ai.pydantic.dev/api/is-running/", "page": "pydantic_ai_slim/pydantic_ai/mcp", "source_site": "pydantic_ai"}
{"title": "MCPServer._sampling_callback", "anchor": "mcpserver-_sampling_callback", "heading_level": 0, "md_text": "MCP sampling callback.", "url": "https://ai.pydantic.dev/api/-sampling-callback/", "page": "pydantic_ai_slim/pydantic_ai/mcp", "source_site": "pydantic_ai"}
{"title": "MCPServerStdio.__init__", "anchor": "mcpserverstdio-__init__", "heading_level": 0, "md_text": "Build a new MCP server.\n\nArgs:\n    command: The command to run.\n    args: The arguments to pass to the command.\n    env: The environment variables to set in the subprocess.\n    cwd: The working directory to use when spawning the process.\n    tool_prefix: A prefix to add to all tools that are registered with the server.\n    log_level: The log level to set when connecting to the server, if any.\n    log_handler: A handler for logging messages from the server.\n    timeout: The timeout in seconds to wait for the client to initialize.\n    read_timeout: Maximum time in seconds to wait for new messages before timing out.\n    process_tool_call: Hook to customize tool calling and optionally pass extra metadata.\n    allow_sampling: Whether to allow MCP sampling through this client.\n    sampling_model: The model to use for sampling.\n    max_retries: The maximum number of times to retry a tool call.\n    elicitation_callback: Callback function to handle elicitation requests from the server.\n    id: An optional unique ID for the MCP server. An MCP server needs to have an ID in order to be used in a durable execution environment like Temporal, in which case the ID will be used to identify the server's activities within the workflow.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/mcp", "source_site": "pydantic_ai"}
{"title": "_MCPServerHTTP.__init__", "anchor": "_mcpserverhttp-__init__", "heading_level": 0, "md_text": "Build a new MCP server.\n\nArgs:\n    url: The URL of the endpoint on the MCP server.\n    headers: Optional HTTP headers to be sent with each request to the endpoint.\n    http_client: An `httpx.AsyncClient` to use with the endpoint.\n    id: An optional unique ID for the MCP server. An MCP server needs to have an ID in order to be used in a durable execution environment like Temporal, in which case the ID will be used to identify the server's activities within the workflow.\n    tool_prefix: A prefix to add to all tools that are registered with the server.\n    log_level: The log level to set when connecting to the server, if any.\n    log_handler: A handler for logging messages from the server.\n    timeout: The timeout in seconds to wait for the client to initialize.\n    read_timeout: Maximum time in seconds to wait for new messages before timing out.\n    process_tool_call: Hook to customize tool calling and optionally pass extra metadata.\n    allow_sampling: Whether to allow MCP sampling through this client.\n    sampling_model: The model to use for sampling.\n    max_retries: The maximum number of times to retry a tool call.\n    elicitation_callback: Callback function to handle elicitation requests from the server.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/mcp", "source_site": "pydantic_ai"}
{"title": "execute_traced_output_function", "anchor": "execute_traced_output_function", "heading_level": 0, "md_text": "Execute an output function within a traced span with error handling.\n\nThis function executes the output function within an OpenTelemetry span for observability,\nautomatically records the function response, and handles ModelRetry exceptions by converting\nthem to ToolRetryError when wrap_validation_errors is True.\n\nArgs:\n    function_schema: The function schema containing the function to execute\n    run_context: The current run context containing tracing and tool information\n    args: Arguments to pass to the function\n    wrap_validation_errors: If True, wrap ModelRetry exceptions in ToolRetryError\n\nReturns:\n    The result of the function execution\n\nRaises:\n    ToolRetryError: When wrap_validation_errors is True and a ModelRetry is caught\n    ModelRetry: When wrap_validation_errors is False and a ModelRetry occurs", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_output", "source_site": "pydantic_ai"}
{"title": "OutputSchema", "anchor": "outputschema", "heading_level": 0, "md_text": "Model the final output from an agent run.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_output", "source_site": "pydantic_ai"}
{"title": "OutputToolset", "anchor": "outputtoolset", "heading_level": 0, "md_text": "A toolset that contains contains output tools for agent output types.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_output", "source_site": "pydantic_ai"}
{"title": "OutputValidator.validate", "anchor": "outputvalidator-validate", "heading_level": 0, "md_text": "Validate a result but calling the function.\n\nArgs:\n    result: The result data after Pydantic validation the message content.\n    run_context: The current run context.\n    wrap_validation_errors: If true, wrap the validation errors in a retry message.\n\nReturns:\n    Result of either the validated result data (ok) or a retry message (Err).", "url": "https://ai.pydantic.dev/api/validate/", "page": "pydantic_ai_slim/pydantic_ai/_output", "source_site": "pydantic_ai"}
{"title": "OutputSchema.build", "anchor": "outputschema-build", "heading_level": 0, "md_text": "Build an OutputSchema dataclass from an output type.", "url": "https://ai.pydantic.dev/api/build/", "page": "pydantic_ai_slim/pydantic_ai/_output", "source_site": "pydantic_ai"}
{"title": "OutputSchema.raise_if_unsupported", "anchor": "outputschema-raise_if_unsupported", "heading_level": 0, "md_text": "Raise an error if the mode is not supported by this model.", "url": "https://ai.pydantic.dev/api/raise-if-unsupported/", "page": "pydantic_ai_slim/pydantic_ai/_output", "source_site": "pydantic_ai"}
{"title": "TextOutputSchema.raise_if_unsupported", "anchor": "textoutputschema-raise_if_unsupported", "heading_level": 0, "md_text": "Raise an error if the mode is not supported by this model.", "url": "https://ai.pydantic.dev/api/raise-if-unsupported/", "page": "pydantic_ai_slim/pydantic_ai/_output", "source_site": "pydantic_ai"}
{"title": "ImageOutputSchema.raise_if_unsupported", "anchor": "imageoutputschema-raise_if_unsupported", "heading_level": 0, "md_text": "Raise an error if the mode is not supported by this model.", "url": "https://ai.pydantic.dev/api/raise-if-unsupported/", "page": "pydantic_ai_slim/pydantic_ai/_output", "source_site": "pydantic_ai"}
{"title": "NativeOutputSchema.raise_if_unsupported", "anchor": "nativeoutputschema-raise_if_unsupported", "heading_level": 0, "md_text": "Raise an error if the mode is not supported by this model.", "url": "https://ai.pydantic.dev/api/raise-if-unsupported/", "page": "pydantic_ai_slim/pydantic_ai/_output", "source_site": "pydantic_ai"}
{"title": "PromptedOutputSchema.build_instructions", "anchor": "promptedoutputschema-build_instructions", "heading_level": 0, "md_text": "Build instructions from a template and an object definition.", "url": "https://ai.pydantic.dev/api/build-instructions/", "page": "pydantic_ai_slim/pydantic_ai/_output", "source_site": "pydantic_ai"}
{"title": "PromptedOutputSchema.raise_if_unsupported", "anchor": "promptedoutputschema-raise_if_unsupported", "heading_level": 0, "md_text": "Raise an error if the mode is not supported by this model.", "url": "https://ai.pydantic.dev/api/raise-if-unsupported/", "page": "pydantic_ai_slim/pydantic_ai/_output", "source_site": "pydantic_ai"}
{"title": "PromptedOutputSchema.instructions", "anchor": "promptedoutputschema-instructions", "heading_level": 0, "md_text": "Get instructions to tell model to output JSON matching the schema.", "url": "https://ai.pydantic.dev/api/instructions/", "page": "pydantic_ai_slim/pydantic_ai/_output", "source_site": "pydantic_ai"}
{"title": "ToolOutputSchema.raise_if_unsupported", "anchor": "tooloutputschema-raise_if_unsupported", "heading_level": 0, "md_text": "Raise an error if the mode is not supported by this model.", "url": "https://ai.pydantic.dev/api/raise-if-unsupported/", "page": "pydantic_ai_slim/pydantic_ai/_output", "source_site": "pydantic_ai"}
{"title": "BaseOutputProcessor.process", "anchor": "baseoutputprocessor-process", "heading_level": 0, "md_text": "Process an output message, performing validation and (if necessary) calling the output function.", "url": "https://ai.pydantic.dev/api/process/", "page": "pydantic_ai_slim/pydantic_ai/_output", "source_site": "pydantic_ai"}
{"title": "ObjectOutputProcessor.process", "anchor": "objectoutputprocessor-process", "heading_level": 0, "md_text": "Process an output message, performing validation and (if necessary) calling the output function.\n\nArgs:\n    data: The output data to validate.\n    run_context: The current run context.\n    allow_partial: If true, allow partial validation.\n    wrap_validation_errors: If true, wrap the validation errors in a retry message.\n\nReturns:\n    Either the validated output data (left) or a retry message (right).", "url": "https://ai.pydantic.dev/api/process/", "page": "pydantic_ai_slim/pydantic_ai/_output", "source_site": "pydantic_ai"}
{"title": "SystemPromptPart", "anchor": "systempromptpart", "heading_level": 0, "md_text": "A system prompt, generally written by the application developer.\n\nThis gives the model context and guidance on how to respond.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "_multi_modal_content_identifier", "anchor": "_multi_modal_content_identifier", "heading_level": 0, "md_text": "Generate stable identifier for multi-modal content to help LLM in finding a specific file in tool call responses.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "FileUrl", "anchor": "fileurl", "heading_level": 0, "md_text": "Abstract base class for any URL-based file.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "VideoUrl", "anchor": "videourl", "heading_level": 0, "md_text": "A URL to a video.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "AudioUrl", "anchor": "audiourl", "heading_level": 0, "md_text": "A URL to an audio file.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ImageUrl", "anchor": "imageurl", "heading_level": 0, "md_text": "A URL to an image.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "DocumentUrl", "anchor": "documenturl", "heading_level": 0, "md_text": "The URL of the document.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "BinaryContent", "anchor": "binarycontent", "heading_level": 0, "md_text": "Binary content, e.g. an audio or image file.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "BinaryImage", "anchor": "binaryimage", "heading_level": 0, "md_text": "Binary content that's guaranteed to be an image.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ToolReturn", "anchor": "toolreturn", "heading_level": 0, "md_text": "A structured return value for tools that need to provide both a return value and custom content to the model.\n\nThis class allows tools to return complex responses that include:\n- A return value for actual tool return\n- Custom content (including multi-modal content) to be sent to the model as a UserPromptPart\n- Optional metadata for application use", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "UserPromptPart", "anchor": "userpromptpart", "heading_level": 0, "md_text": "A user prompt, generally written by the end user.\n\nContent comes from the `user_prompt` parameter of [`Agent.run`][pydantic_ai.agent.AbstractAgent.run],\n[`Agent.run_sync`][pydantic_ai.agent.AbstractAgent.run_sync], and [`Agent.run_stream`][pydantic_ai.agent.AbstractAgent.run_stream].", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "BaseToolReturnPart", "anchor": "basetoolreturnpart", "heading_level": 0, "md_text": "Base class for tool return parts.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ToolReturnPart", "anchor": "toolreturnpart", "heading_level": 0, "md_text": "A tool return message, this encodes the result of running a tool.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "BuiltinToolReturnPart", "anchor": "builtintoolreturnpart", "heading_level": 0, "md_text": "A tool return message from a built-in tool.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "RetryPromptPart", "anchor": "retrypromptpart", "heading_level": 0, "md_text": "A message back to a model asking it to try again.\n\nThis can be sent for a number of reasons:\n\n* Pydantic validation of tool arguments failed, here content is derived from a Pydantic\n  [`ValidationError`][pydantic_core.ValidationError]\n* a tool raised a [`ModelRetry`][pydantic_ai.exceptions.ModelRetry] exception\n* no tool was found for the tool name\n* the model returned plain text when a structured response was expected\n* Pydantic validation of a structured response failed, here content is derived from a Pydantic\n  [`ValidationError`][pydantic_core.ValidationError]\n* an output validator raised a [`ModelRetry`][pydantic_ai.exceptions.ModelRetry] exception", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ModelRequest", "anchor": "modelrequest", "heading_level": 0, "md_text": "A request generated by Pydantic AI and sent to a model, e.g. a message from the Pydantic AI app to the model.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "TextPart", "anchor": "textpart", "heading_level": 0, "md_text": "A plain text response from a model.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ThinkingPart", "anchor": "thinkingpart", "heading_level": 0, "md_text": "A thinking response from a model.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "FilePart", "anchor": "filepart", "heading_level": 0, "md_text": "A file response from a model.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "BaseToolCallPart", "anchor": "basetoolcallpart", "heading_level": 0, "md_text": "A tool call from a model.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ToolCallPart", "anchor": "toolcallpart", "heading_level": 0, "md_text": "A tool call from a model.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "BuiltinToolCallPart", "anchor": "builtintoolcallpart", "heading_level": 0, "md_text": "A tool call to a built-in tool.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ModelResponse", "anchor": "modelresponse", "heading_level": 0, "md_text": "A response from a model, e.g. a message from the model to the Pydantic AI app.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "TextPartDelta", "anchor": "textpartdelta", "heading_level": 0, "md_text": "A partial update (delta) for a `TextPart` to append new text content.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ThinkingPartDelta", "anchor": "thinkingpartdelta", "heading_level": 0, "md_text": "A partial update (delta) for a `ThinkingPart` to append new thinking content.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ToolCallPartDelta", "anchor": "toolcallpartdelta", "heading_level": 0, "md_text": "A partial update (delta) for a `ToolCallPart` to modify tool name, arguments, or tool call ID.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "PartStartEvent", "anchor": "partstartevent", "heading_level": 0, "md_text": "An event indicating that a new part has started.\n\nIf multiple `PartStartEvent`s are received with the same index,\nthe new one should fully replace the old one.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "PartDeltaEvent", "anchor": "partdeltaevent", "heading_level": 0, "md_text": "An event indicating a delta update for an existing part.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "PartEndEvent", "anchor": "partendevent", "heading_level": 0, "md_text": "An event indicating that a part is complete.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "FinalResultEvent", "anchor": "finalresultevent", "heading_level": 0, "md_text": "An event indicating the response to the current model request matches the output schema and will produce a result.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "FunctionToolCallEvent", "anchor": "functiontoolcallevent", "heading_level": 0, "md_text": "An event indicating the start to a call to a function tool.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "FunctionToolResultEvent", "anchor": "functiontoolresultevent", "heading_level": 0, "md_text": "An event indicating the result of a function tool call.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "BuiltinToolCallEvent", "anchor": "builtintoolcallevent", "heading_level": 0, "md_text": "An event indicating the start to a call to a built-in tool.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "BuiltinToolResultEvent", "anchor": "builtintoolresultevent", "heading_level": 0, "md_text": "An event indicating the result of a built-in tool call.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "FileUrl.media_type", "anchor": "fileurl-media_type", "heading_level": 0, "md_text": "Return the media type of the file, based on the URL or the provided `media_type`.", "url": "https://ai.pydantic.dev/api/media-type/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "FileUrl.identifier", "anchor": "fileurl-identifier", "heading_level": 0, "md_text": "The identifier of the file, such as a unique ID.\n\nThis identifier can be provided to the model in a message to allow it to refer to this file in a tool call argument,\nand the tool can look up the file in question by iterating over the message history and finding the matching `FileUrl`.\n\nThis identifier is only automatically passed to the model when the `FileUrl` is returned by a tool.\nIf you're passing the `FileUrl` as a user message, it's up to you to include a separate text part with the identifier,\ne.g. \"This is file <identifier>:\" preceding the `FileUrl`.\n\nIt's also included in inline-text delimiters for providers that require inlining text documents, so the model can\ndistinguish multiple files.", "url": "https://ai.pydantic.dev/api/identifier/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "FileUrl._infer_media_type", "anchor": "fileurl-_infer_media_type", "heading_level": 0, "md_text": "Infer the media type of the file based on the URL.", "url": "https://ai.pydantic.dev/api/-infer-media-type/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "FileUrl.format", "anchor": "fileurl-format", "heading_level": 0, "md_text": "The file format.", "url": "https://ai.pydantic.dev/api/format/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "VideoUrl._infer_media_type", "anchor": "videourl-_infer_media_type", "heading_level": 0, "md_text": "Return the media type of the video, based on the url.", "url": "https://ai.pydantic.dev/api/-infer-media-type/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "VideoUrl.is_youtube", "anchor": "videourl-is_youtube", "heading_level": 0, "md_text": "True if the URL has a YouTube domain.", "url": "https://ai.pydantic.dev/api/is-youtube/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "VideoUrl.format", "anchor": "videourl-format", "heading_level": 0, "md_text": "The file format of the video.\n\nThe choice of supported formats were based on the Bedrock Converse API. Other APIs don't require to use a format.", "url": "https://ai.pydantic.dev/api/format/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "AudioUrl._infer_media_type", "anchor": "audiourl-_infer_media_type", "heading_level": 0, "md_text": "Return the media type of the audio file, based on the url.\n\nReferences:\n- Gemini: https://ai.google.dev/gemini-api/docs/audio#supported-formats", "url": "https://ai.pydantic.dev/api/-infer-media-type/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "AudioUrl.format", "anchor": "audiourl-format", "heading_level": 0, "md_text": "The file format of the audio file.", "url": "https://ai.pydantic.dev/api/format/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ImageUrl._infer_media_type", "anchor": "imageurl-_infer_media_type", "heading_level": 0, "md_text": "Return the media type of the image, based on the url.", "url": "https://ai.pydantic.dev/api/-infer-media-type/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ImageUrl.format", "anchor": "imageurl-format", "heading_level": 0, "md_text": "The file format of the image.\n\nThe choice of supported formats were based on the Bedrock Converse API. Other APIs don't require to use a format.", "url": "https://ai.pydantic.dev/api/format/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "DocumentUrl._infer_media_type", "anchor": "documenturl-_infer_media_type", "heading_level": 0, "md_text": "Return the media type of the document, based on the url.", "url": "https://ai.pydantic.dev/api/-infer-media-type/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "DocumentUrl.format", "anchor": "documenturl-format", "heading_level": 0, "md_text": "The file format of the document.\n\nThe choice of supported formats were based on the Bedrock Converse API. Other APIs don't require to use a format.", "url": "https://ai.pydantic.dev/api/format/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "BinaryContent.narrow_type", "anchor": "binarycontent-narrow_type", "heading_level": 0, "md_text": "Narrow the type of the `BinaryContent` to `BinaryImage` if it's an image.", "url": "https://ai.pydantic.dev/api/narrow-type/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "BinaryContent.from_data_uri", "anchor": "binarycontent-from_data_uri", "heading_level": 0, "md_text": "Create a `BinaryContent` from a data URI.", "url": "https://ai.pydantic.dev/api/from-data-uri/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "BinaryContent.identifier", "anchor": "binarycontent-identifier", "heading_level": 0, "md_text": "Identifier for the binary content, such as a unique ID.\n\nThis identifier can be provided to the model in a message to allow it to refer to this file in a tool call argument,\nand the tool can look up the file in question by iterating over the message history and finding the matching `BinaryContent`.\n\nThis identifier is only automatically passed to the model when the `BinaryContent` is returned by a tool.\nIf you're passing the `BinaryContent` as a user message, it's up to you to include a separate text part with the identifier,\ne.g. \"This is file <identifier>:\" preceding the `BinaryContent`.\n\nIt's also included in inline-text delimiters for providers that require inlining text documents, so the model can\ndistinguish multiple files.", "url": "https://ai.pydantic.dev/api/identifier/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "BinaryContent.data_uri", "anchor": "binarycontent-data_uri", "heading_level": 0, "md_text": "Convert the `BinaryContent` to a data URI.", "url": "https://ai.pydantic.dev/api/data-uri/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "BinaryContent.is_audio", "anchor": "binarycontent-is_audio", "heading_level": 0, "md_text": "Return `True` if the media type is an audio type.", "url": "https://ai.pydantic.dev/api/is-audio/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "BinaryContent.is_image", "anchor": "binarycontent-is_image", "heading_level": 0, "md_text": "Return `True` if the media type is an image type.", "url": "https://ai.pydantic.dev/api/is-image/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "BinaryContent.is_video", "anchor": "binarycontent-is_video", "heading_level": 0, "md_text": "Return `True` if the media type is a video type.", "url": "https://ai.pydantic.dev/api/is-video/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "BinaryContent.is_document", "anchor": "binarycontent-is_document", "heading_level": 0, "md_text": "Return `True` if the media type is a document type.", "url": "https://ai.pydantic.dev/api/is-document/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "BinaryContent.format", "anchor": "binarycontent-format", "heading_level": 0, "md_text": "The file format of the binary content.", "url": "https://ai.pydantic.dev/api/format/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "BaseToolReturnPart.model_response_str", "anchor": "basetoolreturnpart-model_response_str", "heading_level": 0, "md_text": "Return a string representation of the content for the model.", "url": "https://ai.pydantic.dev/api/model-response-str/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "BaseToolReturnPart.model_response_object", "anchor": "basetoolreturnpart-model_response_object", "heading_level": 0, "md_text": "Return a dictionary representation of the content, wrapping non-dict types appropriately.", "url": "https://ai.pydantic.dev/api/model-response-object/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "BaseToolReturnPart.has_content", "anchor": "basetoolreturnpart-has_content", "heading_level": 0, "md_text": "Return `True` if the tool return has content.", "url": "https://ai.pydantic.dev/api/has-content/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "RetryPromptPart.model_response", "anchor": "retrypromptpart-model_response", "heading_level": 0, "md_text": "Return a string message describing why the retry is requested.", "url": "https://ai.pydantic.dev/api/model-response/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ModelRequest.user_text_prompt", "anchor": "modelrequest-user_text_prompt", "heading_level": 0, "md_text": "Create a `ModelRequest` with a single user prompt as text.", "url": "https://ai.pydantic.dev/api/user-text-prompt/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "TextPart.has_content", "anchor": "textpart-has_content", "heading_level": 0, "md_text": "Return `True` if the text content is non-empty.", "url": "https://ai.pydantic.dev/api/has-content/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ThinkingPart.has_content", "anchor": "thinkingpart-has_content", "heading_level": 0, "md_text": "Return `True` if the thinking content is non-empty.", "url": "https://ai.pydantic.dev/api/has-content/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "FilePart.has_content", "anchor": "filepart-has_content", "heading_level": 0, "md_text": "Return `True` if the file content is non-empty.", "url": "https://ai.pydantic.dev/api/has-content/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "BaseToolCallPart.args_as_dict", "anchor": "basetoolcallpart-args_as_dict", "heading_level": 0, "md_text": "Return the arguments as a Python dictionary.\n\nThis is just for convenience with models that require dicts as input.", "url": "https://ai.pydantic.dev/api/args-as-dict/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "BaseToolCallPart.args_as_json_str", "anchor": "basetoolcallpart-args_as_json_str", "heading_level": 0, "md_text": "Return the arguments as a JSON string.\n\nThis is just for convenience with models that require JSON strings as input.", "url": "https://ai.pydantic.dev/api/args-as-json-str/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "BaseToolCallPart.has_content", "anchor": "basetoolcallpart-has_content", "heading_level": 0, "md_text": "Return `True` if the arguments contain any data.", "url": "https://ai.pydantic.dev/api/has-content/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ModelResponse.text", "anchor": "modelresponse-text", "heading_level": 0, "md_text": "Get the text in the response.", "url": "https://ai.pydantic.dev/api/text/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ModelResponse.thinking", "anchor": "modelresponse-thinking", "heading_level": 0, "md_text": "Get the thinking in the response.", "url": "https://ai.pydantic.dev/api/thinking/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ModelResponse.files", "anchor": "modelresponse-files", "heading_level": 0, "md_text": "Get the files in the response.", "url": "https://ai.pydantic.dev/api/files/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ModelResponse.images", "anchor": "modelresponse-images", "heading_level": 0, "md_text": "Get the images in the response.", "url": "https://ai.pydantic.dev/api/images/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ModelResponse.tool_calls", "anchor": "modelresponse-tool_calls", "heading_level": 0, "md_text": "Get the tool calls in the response.", "url": "https://ai.pydantic.dev/api/tool-calls/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ModelResponse.builtin_tool_calls", "anchor": "modelresponse-builtin_tool_calls", "heading_level": 0, "md_text": "Get the builtin tool calls and results in the response.", "url": "https://ai.pydantic.dev/api/builtin-tool-calls/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ModelResponse.cost", "anchor": "modelresponse-cost", "heading_level": 0, "md_text": "Calculate the cost of the usage.\n\nUses [`genai-prices`](https://github.com/pydantic/genai-prices).", "url": "https://ai.pydantic.dev/api/cost/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ModelResponse.otel_events", "anchor": "modelresponse-otel_events", "heading_level": 0, "md_text": "Return OpenTelemetry events for the response.", "url": "https://ai.pydantic.dev/api/otel-events/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "TextPartDelta.apply", "anchor": "textpartdelta-apply", "heading_level": 0, "md_text": "Apply this text delta to an existing `TextPart`.\n\nArgs:\n    part: The existing model response part, which must be a `TextPart`.\n\nReturns:\n    A new `TextPart` with updated text content.\n\nRaises:\n    ValueError: If `part` is not a `TextPart`.", "url": "https://ai.pydantic.dev/api/apply/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ThinkingPartDelta.apply", "anchor": "thinkingpartdelta-apply", "heading_level": 0, "md_text": "Apply this thinking delta to an existing `ThinkingPart`.\n\nArgs:\n    part: The existing model response part, which must be a `ThinkingPart`.\n\nReturns:\n    A new `ThinkingPart` with updated thinking content.\n\nRaises:\n    ValueError: If `part` is not a `ThinkingPart`.", "url": "https://ai.pydantic.dev/api/apply/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ToolCallPartDelta.as_part", "anchor": "toolcallpartdelta-as_part", "heading_level": 0, "md_text": "Convert this delta to a fully formed `ToolCallPart` if possible, otherwise return `None`.\n\nReturns:\n    A `ToolCallPart` if `tool_name_delta` is set, otherwise `None`.", "url": "https://ai.pydantic.dev/api/as-part/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ToolCallPartDelta.apply", "anchor": "toolcallpartdelta-apply", "heading_level": 0, "md_text": "Apply this delta to a part or delta, returning a new part or delta with the changes applied.\n\nArgs:\n    part: The existing model response part or delta to update.\n\nReturns:\n    Either a new `ToolCallPart` or `BuiltinToolCallPart`, or an updated `ToolCallPartDelta`.\n\nRaises:\n    ValueError: If `part` is neither a `ToolCallPart`, `BuiltinToolCallPart`, nor a `ToolCallPartDelta`.\n    UnexpectedModelBehavior: If applying JSON deltas to dict arguments or vice versa.", "url": "https://ai.pydantic.dev/api/apply/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ToolCallPartDelta._apply_to_delta", "anchor": "toolcallpartdelta-_apply_to_delta", "heading_level": 0, "md_text": "Internal helper to apply this delta to another delta.", "url": "https://ai.pydantic.dev/api/-apply-to-delta/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ToolCallPartDelta._apply_to_part", "anchor": "toolcallpartdelta-_apply_to_part", "heading_level": 0, "md_text": "Internal helper to apply this delta directly to a `ToolCallPart` or `BuiltinToolCallPart`.", "url": "https://ai.pydantic.dev/api/-apply-to-part/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "FunctionToolCallEvent.tool_call_id", "anchor": "functiontoolcallevent-tool_call_id", "heading_level": 0, "md_text": "An ID used for matching details about the call to its result.", "url": "https://ai.pydantic.dev/api/tool-call-id/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "FunctionToolCallEvent.call_id", "anchor": "functiontoolcallevent-call_id", "heading_level": 0, "md_text": "An ID used for matching details about the call to its result.", "url": "https://ai.pydantic.dev/api/call-id/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "FunctionToolResultEvent.tool_call_id", "anchor": "functiontoolresultevent-tool_call_id", "heading_level": 0, "md_text": "An ID used to match the result to its original call.", "url": "https://ai.pydantic.dev/api/tool-call-id/", "page": "pydantic_ai_slim/pydantic_ai/messages", "source_site": "pydantic_ai"}
{"title": "ag_ui", "anchor": "ag_ui", "heading_level": 0, "md_text": "Provides an AG-UI protocol adapter for the Pydantic AI agent.\n\nThis package provides seamless integration between pydantic-ai agents and ag-ui\nfor building interactive AI applications with streaming event-based communication.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ag_ui", "source_site": "pydantic_ai"}
{"title": "handle_ag_ui_request", "anchor": "handle_ag_ui_request", "heading_level": 0, "md_text": "Handle an AG-UI request by running the agent and returning a streaming response.\n\nArgs:\n    agent: The agent to run.\n    request: The Starlette request (e.g. from FastAPI) containing the AG-UI run input.\n\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    on_complete: Optional callback function called when the agent run completes successfully.\n        The callback receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can access `all_messages()` and other result data.\n\nReturns:\n    A streaming Starlette response with AG-UI protocol events.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ag_ui", "source_site": "pydantic_ai"}
{"title": "run_ag_ui", "anchor": "run_ag_ui", "heading_level": 0, "md_text": "Run the agent with the AG-UI run input and stream AG-UI protocol events.\n\nArgs:\n    agent: The agent to run.\n    run_input: The AG-UI run input containing thread_id, run_id, messages, etc.\n    accept: The accept header value for the run.\n\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    on_complete: Optional callback function called when the agent run completes successfully.\n        The callback receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can access `all_messages()` and other result data.\n\nYields:\n    Streaming event chunks encoded as strings according to the accept header value.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ag_ui", "source_site": "pydantic_ai"}
{"title": "GraphAgentState", "anchor": "graphagentstate", "heading_level": 0, "md_text": "State kept across the execution of the agent graph.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_agent_graph", "source_site": "pydantic_ai"}
{"title": "GraphAgentDeps", "anchor": "graphagentdeps", "heading_level": 0, "md_text": "Dependencies/config passed to the agent graph.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_agent_graph", "source_site": "pydantic_ai"}
{"title": "AgentNode", "anchor": "agentnode", "heading_level": 0, "md_text": "The base class for all agent nodes.\n\nUsing subclass of `BaseNode` for all nodes reduces the amount of boilerplate of generics everywhere", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_agent_graph", "source_site": "pydantic_ai"}
{"title": "is_agent_node", "anchor": "is_agent_node", "heading_level": 0, "md_text": "Check if the provided node is an instance of `AgentNode`.\n\nUsage:\n\n    if is_agent_node(node):\n        # `node` is an AgentNode\n        ...\n\nThis method preserves the generic parameters on the narrowed type, unlike `isinstance(node, AgentNode)`.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_agent_graph", "source_site": "pydantic_ai"}
{"title": "UserPromptNode", "anchor": "userpromptnode", "heading_level": 0, "md_text": "The node that handles the user prompt and instructions.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_agent_graph", "source_site": "pydantic_ai"}
{"title": "_prepare_request_parameters", "anchor": "_prepare_request_parameters", "heading_level": 0, "md_text": "Build tools and create an agent model.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_agent_graph", "source_site": "pydantic_ai"}
{"title": "ModelRequestNode", "anchor": "modelrequestnode", "heading_level": 0, "md_text": "The node that makes a request to the model using the last message in state.message_history.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_agent_graph", "source_site": "pydantic_ai"}
{"title": "CallToolsNode", "anchor": "calltoolsnode", "heading_level": 0, "md_text": "The node that processes a model response, and decides whether to end the run or make a new request.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_agent_graph", "source_site": "pydantic_ai"}
{"title": "SetFinalResult", "anchor": "setfinalresult", "heading_level": 0, "md_text": "A node that immediately ends the graph run after a streaming response produced a final result.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_agent_graph", "source_site": "pydantic_ai"}
{"title": "build_run_context", "anchor": "build_run_context", "heading_level": 0, "md_text": "Build a `RunContext` object from the current agent graph run context.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_agent_graph", "source_site": "pydantic_ai"}
{"title": "process_tool_calls", "anchor": "process_tool_calls", "heading_level": 0, "md_text": "Process function (i.e., non-result) tool calls in parallel.\n\nAlso add stub return parts for any other tools that need it.\n\nBecause async iterators can't have return values, we use `output_parts` and `output_final_result` as output arguments.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_agent_graph", "source_site": "pydantic_ai"}
{"title": "capture_run_messages", "anchor": "capture_run_messages", "heading_level": 0, "md_text": "Context manager to access the messages used in a [`run`][pydantic_ai.agent.AbstractAgent.run], [`run_sync`][pydantic_ai.agent.AbstractAgent.run_sync], or [`run_stream`][pydantic_ai.agent.AbstractAgent.run_stream] call.\n\nUseful when a run may raise an exception, see [model errors](../agents.md#model-errors) for more information.\n\nExamples:\n```python\nfrom pydantic_ai import Agent, capture_run_messages\n\nagent = Agent('test')\n\nwith capture_run_messages() as messages:\n    try:\n        result = agent.run_sync('foobar')\n    except Exception:\n        print(messages)\n        raise\n```", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_agent_graph", "source_site": "pydantic_ai"}
{"title": "build_agent_graph", "anchor": "build_agent_graph", "heading_level": 0, "md_text": "Build the execution [Graph][pydantic_graph.Graph] for a given agent.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_agent_graph", "source_site": "pydantic_ai"}
{"title": "_process_message_history", "anchor": "_process_message_history", "heading_level": 0, "md_text": "Process message history through a sequence of processors.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_agent_graph", "source_site": "pydantic_ai"}
{"title": "_clean_message_history", "anchor": "_clean_message_history", "heading_level": 0, "md_text": "Clean the message history by merging consecutive messages of the same type.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/_agent_graph", "source_site": "pydantic_ai"}
{"title": "UserPromptNode._reevaluate_dynamic_prompts", "anchor": "userpromptnode-_reevaluate_dynamic_prompts", "heading_level": 0, "md_text": "Reevaluate any `SystemPromptPart` with dynamic_ref in the provided messages by running the associated runner function.", "url": "https://ai.pydantic.dev/api/-reevaluate-dynamic-prompts/", "page": "pydantic_ai_slim/pydantic_ai/_agent_graph", "source_site": "pydantic_ai"}
{"title": "UserPromptNode._sys_parts", "anchor": "userpromptnode-_sys_parts", "heading_level": 0, "md_text": "Build the initial messages for the conversation.", "url": "https://ai.pydantic.dev/api/-sys-parts/", "page": "pydantic_ai_slim/pydantic_ai/_agent_graph", "source_site": "pydantic_ai"}
{"title": "CallToolsNode.stream", "anchor": "calltoolsnode-stream", "heading_level": 0, "md_text": "Process the model response and yield events for the start and end of each function tool call.", "url": "https://ai.pydantic.dev/api/stream/", "page": "pydantic_ai_slim/pydantic_ai/_agent_graph", "source_site": "pydantic_ai"}
{"title": "OpenAIChatModelSettings", "anchor": "openaichatmodelsettings", "heading_level": 0, "md_text": "Settings used for an OpenAI model request.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIModelSettings", "anchor": "openaimodelsettings", "heading_level": 0, "md_text": "Deprecated alias for `OpenAIChatModelSettings`.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIResponsesModelSettings", "anchor": "openairesponsesmodelsettings", "heading_level": 0, "md_text": "Settings used for an OpenAI Responses model request.\n\nALL FIELDS MUST BE `openai_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIChatModel", "anchor": "openaichatmodel", "heading_level": 0, "md_text": "A model that uses the OpenAI API.\n\nInternally, this uses the [OpenAI Python client](https://github.com/openai/openai-python) to interact with the API.\n\nApart from `__init__`, all methods are private or match those of the base class.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIModel", "anchor": "openaimodel", "heading_level": 0, "md_text": "Deprecated alias for `OpenAIChatModel`.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIResponsesModel", "anchor": "openairesponsesmodel", "heading_level": 0, "md_text": "A model that uses the OpenAI Responses API.\n\nThe [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses) is the\nnew API for OpenAI models.\n\nIf you are interested in the differences between the Responses API and the Chat Completions API,\nsee the [OpenAI API docs](https://platform.openai.com/docs/guides/responses-vs-chat-completions).", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIStreamedResponse", "anchor": "openaistreamedresponse", "heading_level": 0, "md_text": "Implementation of `StreamedResponse` for OpenAI models.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIResponsesStreamedResponse", "anchor": "openairesponsesstreamedresponse", "heading_level": 0, "md_text": "Implementation of `StreamedResponse` for OpenAI Responses API.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIChatModel.__init__", "anchor": "openaichatmodel-__init__", "heading_level": 0, "md_text": "Initialize an OpenAI model.\n\nArgs:\n    model_name: The name of the OpenAI model to use. List of model names available\n        [here](https://github.com/openai/openai-python/blob/v1.54.3/src/openai/types/chat_model.py#L7)\n        (Unfortunately, despite being ask to do so, OpenAI do not provide `.inv` files for their API).\n    provider: The provider to use. Defaults to `'openai'`.\n    profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n    system_prompt_role: The role to use for the system prompt message. If not provided, defaults to `'system'`.\n        In the future, this may be inferred from the model name.\n    settings: Default model settings for this model instance.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIChatModel.model_name", "anchor": "openaichatmodel-model_name", "heading_level": 0, "md_text": "The model name.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIChatModel.system", "anchor": "openaichatmodel-system", "heading_level": 0, "md_text": "The model provider.", "url": "https://ai.pydantic.dev/api/system/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIChatModel._process_response", "anchor": "openaichatmodel-_process_response", "heading_level": 0, "md_text": "Process a non-streamed response, and prepare a message to return.", "url": "https://ai.pydantic.dev/api/-process-response/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIChatModel._process_streamed_response", "anchor": "openaichatmodel-_process_streamed_response", "heading_level": 0, "md_text": "Process a streamed response, and prepare a streaming response to return.", "url": "https://ai.pydantic.dev/api/-process-streamed-response/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIChatModel._map_messages", "anchor": "openaichatmodel-_map_messages", "heading_level": 0, "md_text": "Just maps a `pydantic_ai.Message` to a `openai.types.ChatCompletionMessageParam`.", "url": "https://ai.pydantic.dev/api/-map-messages/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIResponsesModel.__init__", "anchor": "openairesponsesmodel-__init__", "heading_level": 0, "md_text": "Initialize an OpenAI Responses model.\n\nArgs:\n    model_name: The name of the OpenAI model to use.\n    provider: The provider to use. Defaults to `'openai'`.\n    profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n    settings: Default model settings for this model instance.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIResponsesModel.model_name", "anchor": "openairesponsesmodel-model_name", "heading_level": 0, "md_text": "The model name.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIResponsesModel.system", "anchor": "openairesponsesmodel-system", "heading_level": 0, "md_text": "The model provider.", "url": "https://ai.pydantic.dev/api/system/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIResponsesModel._process_response", "anchor": "openairesponsesmodel-_process_response", "heading_level": 0, "md_text": "Process a non-streamed response, and prepare a message to return.", "url": "https://ai.pydantic.dev/api/-process-response/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIResponsesModel._process_streamed_response", "anchor": "openairesponsesmodel-_process_streamed_response", "heading_level": 0, "md_text": "Process a streamed response, and prepare a streaming response to return.", "url": "https://ai.pydantic.dev/api/-process-streamed-response/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIResponsesModel._map_messages", "anchor": "openairesponsesmodel-_map_messages", "heading_level": 0, "md_text": "Just maps a `pydantic_ai.Message` to a `openai.types.responses.ResponseInputParam`.", "url": "https://ai.pydantic.dev/api/-map-messages/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIStreamedResponse.model_name", "anchor": "openaistreamedresponse-model_name", "heading_level": 0, "md_text": "Get the model name of the response.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIStreamedResponse.provider_name", "anchor": "openaistreamedresponse-provider_name", "heading_level": 0, "md_text": "Get the provider name.", "url": "https://ai.pydantic.dev/api/provider-name/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIStreamedResponse.timestamp", "anchor": "openaistreamedresponse-timestamp", "heading_level": 0, "md_text": "Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/timestamp/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIResponsesStreamedResponse.model_name", "anchor": "openairesponsesstreamedresponse-model_name", "heading_level": 0, "md_text": "Get the model name of the response.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIResponsesStreamedResponse.provider_name", "anchor": "openairesponsesstreamedresponse-provider_name", "heading_level": 0, "md_text": "Get the provider name.", "url": "https://ai.pydantic.dev/api/provider-name/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIResponsesStreamedResponse.timestamp", "anchor": "openairesponsesstreamedresponse-timestamp", "heading_level": 0, "md_text": "Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/timestamp/", "page": "pydantic_ai_slim/pydantic_ai/models/openai", "source_site": "pydantic_ai"}
{"title": "MistralModelSettings", "anchor": "mistralmodelsettings", "heading_level": 0, "md_text": "Settings used for a Mistral model request.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "MistralModel", "anchor": "mistralmodel", "heading_level": 0, "md_text": "A model that uses Mistral.\n\nInternally, this uses the [Mistral Python client](https://github.com/mistralai/client-python) to interact with the API.\n\n[API Documentation](https://docs.mistral.ai/)", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "MistralStreamedResponse", "anchor": "mistralstreamedresponse", "heading_level": 0, "md_text": "Implementation of `StreamedResponse` for Mistral models.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "_map_usage", "anchor": "_map_usage", "heading_level": 0, "md_text": "Maps a Mistral Completion Chunk or Chat Completion Response to a Usage.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "_map_content", "anchor": "_map_content", "heading_level": 0, "md_text": "Maps the delta content from a Mistral Completion Chunk to a string or None.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "MistralModel.__init__", "anchor": "mistralmodel-__init__", "heading_level": 0, "md_text": "Initialize a Mistral model.\n\nArgs:\n    model_name: The name of the model to use.\n    provider: The provider to use for authentication and API access. Can be either the string\n        'mistral' or an instance of `Provider[Mistral]`. If not provided, a new provider will be\n        created using the other parameters.\n    profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n    json_mode_schema_prompt: The prompt to show when the model expects a JSON object as input.\n    settings: Model-specific settings that will be used as defaults for this model.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "MistralModel.model_name", "anchor": "mistralmodel-model_name", "heading_level": 0, "md_text": "The model name.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "MistralModel.system", "anchor": "mistralmodel-system", "heading_level": 0, "md_text": "The model provider.", "url": "https://ai.pydantic.dev/api/system/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "MistralModel.request", "anchor": "mistralmodel-request", "heading_level": 0, "md_text": "Make a non-streaming request to the model from Pydantic AI call.", "url": "https://ai.pydantic.dev/api/request/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "MistralModel.request_stream", "anchor": "mistralmodel-request_stream", "heading_level": 0, "md_text": "Make a streaming request to the model from Pydantic AI call.", "url": "https://ai.pydantic.dev/api/request-stream/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "MistralModel._completions_create", "anchor": "mistralmodel-_completions_create", "heading_level": 0, "md_text": "Make a non-streaming request to the model.", "url": "https://ai.pydantic.dev/api/-completions-create/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "MistralModel._stream_completions_create", "anchor": "mistralmodel-_stream_completions_create", "heading_level": 0, "md_text": "Create a streaming completion request to the Mistral model.", "url": "https://ai.pydantic.dev/api/-stream-completions-create/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "MistralModel._get_tool_choice", "anchor": "mistralmodel-_get_tool_choice", "heading_level": 0, "md_text": "Get tool choice for the model.\n\n- \"auto\": Default mode. Model decides if it uses the tool or not.\n- \"any\": Select any tool.\n- \"none\": Prevents tool use.\n- \"required\": Forces tool use.", "url": "https://ai.pydantic.dev/api/-get-tool-choice/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "MistralModel._map_function_and_output_tools_definition", "anchor": "mistralmodel-_map_function_and_output_tools_definition", "heading_level": 0, "md_text": "Map function and output tools to MistralTool format.\n\nReturns None if both function_tools and output_tools are empty.", "url": "https://ai.pydantic.dev/api/-map-function-and-output-tools-definition/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "MistralModel._process_response", "anchor": "mistralmodel-_process_response", "heading_level": 0, "md_text": "Process a non-streamed response, and prepare a message to return.", "url": "https://ai.pydantic.dev/api/-process-response/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "MistralModel._process_streamed_response", "anchor": "mistralmodel-_process_streamed_response", "heading_level": 0, "md_text": "Process a streamed response, and prepare a streaming response to return.", "url": "https://ai.pydantic.dev/api/-process-streamed-response/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "MistralModel._map_mistral_to_pydantic_tool_call", "anchor": "mistralmodel-_map_mistral_to_pydantic_tool_call", "heading_level": 0, "md_text": "Maps a MistralToolCall to a ToolCall.", "url": "https://ai.pydantic.dev/api/-map-mistral-to-pydantic-tool-call/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "MistralModel._map_tool_call", "anchor": "mistralmodel-_map_tool_call", "heading_level": 0, "md_text": "Maps a pydantic-ai ToolCall to a MistralToolCall.", "url": "https://ai.pydantic.dev/api/-map-tool-call/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "MistralModel._generate_user_output_format", "anchor": "mistralmodel-_generate_user_output_format", "heading_level": 0, "md_text": "Get a message with an example of the expected output format.", "url": "https://ai.pydantic.dev/api/-generate-user-output-format/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "MistralModel._get_python_type", "anchor": "mistralmodel-_get_python_type", "heading_level": 0, "md_text": "Return a string representation of the Python type for a single JSON schema property.\n\nThis function handles recursion for nested arrays/objects and `anyOf`.", "url": "https://ai.pydantic.dev/api/-get-python-type/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "MistralModel._get_timeout_ms", "anchor": "mistralmodel-_get_timeout_ms", "heading_level": 0, "md_text": "Convert a timeout to milliseconds.", "url": "https://ai.pydantic.dev/api/-get-timeout-ms/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "MistralModel._map_messages", "anchor": "mistralmodel-_map_messages", "heading_level": 0, "md_text": "Just maps a `pydantic_ai.Message` to a `MistralMessage`.", "url": "https://ai.pydantic.dev/api/-map-messages/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "MistralStreamedResponse.model_name", "anchor": "mistralstreamedresponse-model_name", "heading_level": 0, "md_text": "Get the model name of the response.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "MistralStreamedResponse.provider_name", "anchor": "mistralstreamedresponse-provider_name", "heading_level": 0, "md_text": "Get the provider name.", "url": "https://ai.pydantic.dev/api/provider-name/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "MistralStreamedResponse.timestamp", "anchor": "mistralstreamedresponse-timestamp", "heading_level": 0, "md_text": "Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/timestamp/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "MistralStreamedResponse._validate_required_json_schema", "anchor": "mistralstreamedresponse-_validate_required_json_schema", "heading_level": 0, "md_text": "Validate that all required parameters in the JSON schema are present in the JSON dictionary.", "url": "https://ai.pydantic.dev/api/-validate-required-json-schema/", "page": "pydantic_ai_slim/pydantic_ai/models/mistral", "source_site": "pydantic_ai"}
{"title": "_WrappedTextOutput", "anchor": "_wrappedtextoutput", "heading_level": 0, "md_text": "A private wrapper class to tag an output that came from the custom_output_text field.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/test", "source_site": "pydantic_ai"}
{"title": "_WrappedToolOutput", "anchor": "_wrappedtooloutput", "heading_level": 0, "md_text": "A wrapper class to tag an output that came from the custom_output_args field.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/test", "source_site": "pydantic_ai"}
{"title": "TestModel", "anchor": "testmodel", "heading_level": 0, "md_text": "A model specifically for testing purposes.\n\nThis will (by default) call all tools in the agent, then return a tool response if possible,\notherwise a plain response.\n\nHow useful this model is will vary significantly.\n\nApart from `__init__` derived by the `dataclass` decorator, all methods are private or match those\nof the base class.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/test", "source_site": "pydantic_ai"}
{"title": "TestStreamedResponse", "anchor": "teststreamedresponse", "heading_level": 0, "md_text": "A structured response that streams test data.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/test", "source_site": "pydantic_ai"}
{"title": "_JsonSchemaTestData", "anchor": "_jsonschematestdata", "heading_level": 0, "md_text": "Generate data that matches a JSON schema.\n\nThis tries to generate the minimal viable data for the schema.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/test", "source_site": "pydantic_ai"}
{"title": "TestModel.__init__", "anchor": "testmodel-__init__", "heading_level": 0, "md_text": "Initialize TestModel with optional settings and profile.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/models/test", "source_site": "pydantic_ai"}
{"title": "TestModel.model_name", "anchor": "testmodel-model_name", "heading_level": 0, "md_text": "The model name.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/test", "source_site": "pydantic_ai"}
{"title": "TestModel.system", "anchor": "testmodel-system", "heading_level": 0, "md_text": "The model provider.", "url": "https://ai.pydantic.dev/api/system/", "page": "pydantic_ai_slim/pydantic_ai/models/test", "source_site": "pydantic_ai"}
{"title": "TestStreamedResponse.model_name", "anchor": "teststreamedresponse-model_name", "heading_level": 0, "md_text": "Get the model name of the response.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/test", "source_site": "pydantic_ai"}
{"title": "TestStreamedResponse.provider_name", "anchor": "teststreamedresponse-provider_name", "heading_level": 0, "md_text": "Get the provider name.", "url": "https://ai.pydantic.dev/api/provider-name/", "page": "pydantic_ai_slim/pydantic_ai/models/test", "source_site": "pydantic_ai"}
{"title": "TestStreamedResponse.timestamp", "anchor": "teststreamedresponse-timestamp", "heading_level": 0, "md_text": "Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/timestamp/", "page": "pydantic_ai_slim/pydantic_ai/models/test", "source_site": "pydantic_ai"}
{"title": "_JsonSchemaTestData.generate", "anchor": "_jsonschematestdata-generate", "heading_level": 0, "md_text": "Generate data for the JSON schema.", "url": "https://ai.pydantic.dev/api/generate/", "page": "pydantic_ai_slim/pydantic_ai/models/test", "source_site": "pydantic_ai"}
{"title": "_JsonSchemaTestData._gen_any", "anchor": "_jsonschematestdata-_gen_any", "heading_level": 0, "md_text": "Generate data for any JSON Schema.", "url": "https://ai.pydantic.dev/api/-gen-any/", "page": "pydantic_ai_slim/pydantic_ai/models/test", "source_site": "pydantic_ai"}
{"title": "_JsonSchemaTestData._object_gen", "anchor": "_jsonschematestdata-_object_gen", "heading_level": 0, "md_text": "Generate data for a JSON Schema object.", "url": "https://ai.pydantic.dev/api/-object-gen/", "page": "pydantic_ai_slim/pydantic_ai/models/test", "source_site": "pydantic_ai"}
{"title": "_JsonSchemaTestData._str_gen", "anchor": "_jsonschematestdata-_str_gen", "heading_level": 0, "md_text": "Generate a string from a JSON Schema string.", "url": "https://ai.pydantic.dev/api/-str-gen/", "page": "pydantic_ai_slim/pydantic_ai/models/test", "source_site": "pydantic_ai"}
{"title": "_JsonSchemaTestData._int_gen", "anchor": "_jsonschematestdata-_int_gen", "heading_level": 0, "md_text": "Generate an integer from a JSON Schema integer.", "url": "https://ai.pydantic.dev/api/-int-gen/", "page": "pydantic_ai_slim/pydantic_ai/models/test", "source_site": "pydantic_ai"}
{"title": "_JsonSchemaTestData._bool_gen", "anchor": "_jsonschematestdata-_bool_gen", "heading_level": 0, "md_text": "Generate a boolean from a JSON Schema boolean.", "url": "https://ai.pydantic.dev/api/-bool-gen/", "page": "pydantic_ai_slim/pydantic_ai/models/test", "source_site": "pydantic_ai"}
{"title": "_JsonSchemaTestData._array_gen", "anchor": "_jsonschematestdata-_array_gen", "heading_level": 0, "md_text": "Generate an array from a JSON Schema array.", "url": "https://ai.pydantic.dev/api/-array-gen/", "page": "pydantic_ai_slim/pydantic_ai/models/test", "source_site": "pydantic_ai"}
{"title": "_JsonSchemaTestData._char", "anchor": "_jsonschematestdata-_char", "heading_level": 0, "md_text": "Generate a character on the same principle as Excel columns, e.g. a-z, aa-az...", "url": "https://ai.pydantic.dev/api/-char/", "page": "pydantic_ai_slim/pydantic_ai/models/test", "source_site": "pydantic_ai"}
{"title": "OutlinesModel", "anchor": "outlinesmodel", "heading_level": 0, "md_text": "A model that relies on the Outlines library to run non API-based models.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesStreamedResponse", "anchor": "outlinesstreamedresponse", "heading_level": 0, "md_text": "Implementation of `StreamedResponse` for Outlines models.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesModel.__init__", "anchor": "outlinesmodel-__init__", "heading_level": 0, "md_text": "Initialize an Outlines model.\n\nArgs:\n    model: The Outlines model used for the model.\n    provider: The provider to use for OutlinesModel. Can be either the string 'outlines' or an\n        instance of `Provider[OutlinesBaseModel]`. If not provided, the other parameters will be used.\n    profile: The model profile to use. Defaults to a profile picked by the provider.\n    settings: Default model settings for this model instance.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/models/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesModel.from_transformers", "anchor": "outlinesmodel-from_transformers", "heading_level": 0, "md_text": "Create an Outlines model from a Hugging Face model and tokenizer.\n\nArgs:\n    hf_model: The Hugging Face PreTrainedModel or any model that is compatible with the\n        `transformers` API.\n    hf_tokenizer_or_processor: Either a HuggingFace `PreTrainedTokenizer` or any tokenizer that is compatible\n        with the `transformers` API, or a HuggingFace processor inheriting from `ProcessorMixin`. If a\n        tokenizer is provided, a regular model will be used, while if you provide a processor, it will be a\n        multimodal model.\n    provider: The provider to use for OutlinesModel. Can be either the string 'outlines' or an\n        instance of `Provider[OutlinesBaseModel]`. If not provided, the other parameters will be used.\n    profile: The model profile to use. Defaults to a profile picked by the provider.\n    settings: Default model settings for this model instance.", "url": "https://ai.pydantic.dev/api/from-transformers/", "page": "pydantic_ai_slim/pydantic_ai/models/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesModel.from_llamacpp", "anchor": "outlinesmodel-from_llamacpp", "heading_level": 0, "md_text": "Create an Outlines model from a LlamaCpp model.\n\nArgs:\n    llama_model: The llama_cpp.Llama model to use.\n    provider: The provider to use for OutlinesModel. Can be either the string 'outlines' or an\n        instance of `Provider[OutlinesBaseModel]`. If not provided, the other parameters will be used.\n    profile: The model profile to use. Defaults to a profile picked by the provider.\n    settings: Default model settings for this model instance.", "url": "https://ai.pydantic.dev/api/from-llamacpp/", "page": "pydantic_ai_slim/pydantic_ai/models/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesModel.from_mlxlm", "anchor": "outlinesmodel-from_mlxlm", "heading_level": 0, "md_text": "Create an Outlines model from a MLXLM model.\n\nArgs:\n    mlx_model: The nn.Module model to use.\n    mlx_tokenizer: The PreTrainedTokenizer to use.\n    provider: The provider to use for OutlinesModel. Can be either the string 'outlines' or an\n        instance of `Provider[OutlinesBaseModel]`. If not provided, the other parameters will be used.\n    profile: The model profile to use. Defaults to a profile picked by the provider.\n    settings: Default model settings for this model instance.", "url": "https://ai.pydantic.dev/api/from-mlxlm/", "page": "pydantic_ai_slim/pydantic_ai/models/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesModel.from_sglang", "anchor": "outlinesmodel-from_sglang", "heading_level": 0, "md_text": "Create an Outlines model to send requests to an SGLang server.\n\nArgs:\n    base_url: The url of the SGLang server.\n    api_key: The API key to use for authenticating requests to the SGLang server.\n    model_name: The name of the model to use.\n    provider: The provider to use for OutlinesModel. Can be either the string 'outlines' or an\n        instance of `Provider[OutlinesBaseModel]`. If not provided, the other parameters will be used.\n    profile: The model profile to use. Defaults to a profile picked by the provider.\n    settings: Default model settings for this model instance.", "url": "https://ai.pydantic.dev/api/from-sglang/", "page": "pydantic_ai_slim/pydantic_ai/models/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesModel.from_vllm_offline", "anchor": "outlinesmodel-from_vllm_offline", "heading_level": 0, "md_text": "Create an Outlines model from a vLLM offline inference model.\n\nArgs:\n    vllm_model: The vllm.LLM local model to use.\n    provider: The provider to use for OutlinesModel. Can be either the string 'outlines' or an\n        instance of `Provider[OutlinesBaseModel]`. If not provided, the other parameters will be used.\n    profile: The model profile to use. Defaults to a profile picked by the provider.\n    settings: Default model settings for this model instance.", "url": "https://ai.pydantic.dev/api/from-vllm-offline/", "page": "pydantic_ai_slim/pydantic_ai/models/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesModel.request", "anchor": "outlinesmodel-request", "heading_level": 0, "md_text": "Make a request to the model.", "url": "https://ai.pydantic.dev/api/request/", "page": "pydantic_ai_slim/pydantic_ai/models/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesModel._build_generation_arguments", "anchor": "outlinesmodel-_build_generation_arguments", "heading_level": 0, "md_text": "Build the generation arguments for the model.", "url": "https://ai.pydantic.dev/api/-build-generation-arguments/", "page": "pydantic_ai_slim/pydantic_ai/models/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesModel.format_inference_kwargs", "anchor": "outlinesmodel-format_inference_kwargs", "heading_level": 0, "md_text": "Format the model settings for the inference kwargs.", "url": "https://ai.pydantic.dev/api/format-inference-kwargs/", "page": "pydantic_ai_slim/pydantic_ai/models/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesModel._format_transformers_inference_kwargs", "anchor": "outlinesmodel-_format_transformers_inference_kwargs", "heading_level": 0, "md_text": "Select the model settings supported by the Transformers model.", "url": "https://ai.pydantic.dev/api/-format-transformers-inference-kwargs/", "page": "pydantic_ai_slim/pydantic_ai/models/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesModel._format_llama_cpp_inference_kwargs", "anchor": "outlinesmodel-_format_llama_cpp_inference_kwargs", "heading_level": 0, "md_text": "Select the model settings supported by the LlamaCpp model.", "url": "https://ai.pydantic.dev/api/-format-llama-cpp-inference-kwargs/", "page": "pydantic_ai_slim/pydantic_ai/models/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesModel._format_mlxlm_inference_kwargs", "anchor": "outlinesmodel-_format_mlxlm_inference_kwargs", "heading_level": 0, "md_text": "Select the model settings supported by the MLXLM model.", "url": "https://ai.pydantic.dev/api/-format-mlxlm-inference-kwargs/", "page": "pydantic_ai_slim/pydantic_ai/models/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesModel._format_sglang_inference_kwargs", "anchor": "outlinesmodel-_format_sglang_inference_kwargs", "heading_level": 0, "md_text": "Select the model settings supported by the SGLang model.", "url": "https://ai.pydantic.dev/api/-format-sglang-inference-kwargs/", "page": "pydantic_ai_slim/pydantic_ai/models/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesModel._format_vllm_offline_inference_kwargs", "anchor": "outlinesmodel-_format_vllm_offline_inference_kwargs", "heading_level": 0, "md_text": "Select the model settings supported by the vLLMOffline model.", "url": "https://ai.pydantic.dev/api/-format-vllm-offline-inference-kwargs/", "page": "pydantic_ai_slim/pydantic_ai/models/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesModel._format_prompt", "anchor": "outlinesmodel-_format_prompt", "heading_level": 0, "md_text": "Turn the model messages into an Outlines Chat instance.", "url": "https://ai.pydantic.dev/api/-format-prompt/", "page": "pydantic_ai_slim/pydantic_ai/models/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesModel._create_PIL_image", "anchor": "outlinesmodel-_create_pil_image", "heading_level": 0, "md_text": "Create a PIL Image from the data and data type.", "url": "https://ai.pydantic.dev/api/-create-pil-image/", "page": "pydantic_ai_slim/pydantic_ai/models/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesModel._process_response", "anchor": "outlinesmodel-_process_response", "heading_level": 0, "md_text": "Turn the Outlines text response into a Pydantic AI model response instance.", "url": "https://ai.pydantic.dev/api/-process-response/", "page": "pydantic_ai_slim/pydantic_ai/models/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesModel._process_streamed_response", "anchor": "outlinesmodel-_process_streamed_response", "heading_level": 0, "md_text": "Turn the Outlines text response into a Pydantic AI streamed response instance.", "url": "https://ai.pydantic.dev/api/-process-streamed-response/", "page": "pydantic_ai_slim/pydantic_ai/models/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesStreamedResponse.model_name", "anchor": "outlinesstreamedresponse-model_name", "heading_level": 0, "md_text": "Get the model name of the response.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesStreamedResponse.provider_name", "anchor": "outlinesstreamedresponse-provider_name", "heading_level": 0, "md_text": "Get the provider name.", "url": "https://ai.pydantic.dev/api/provider-name/", "page": "pydantic_ai_slim/pydantic_ai/models/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesStreamedResponse.timestamp", "anchor": "outlinesstreamedresponse-timestamp", "heading_level": 0, "md_text": "Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/timestamp/", "page": "pydantic_ai_slim/pydantic_ai/models/outlines", "source_site": "pydantic_ai"}
{"title": "HuggingFaceModelSettings", "anchor": "huggingfacemodelsettings", "heading_level": 0, "md_text": "Settings used for a Hugging Face model request.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/huggingface", "source_site": "pydantic_ai"}
{"title": "HuggingFaceModel", "anchor": "huggingfacemodel", "heading_level": 0, "md_text": "A model that uses Hugging Face Inference Providers.\n\nInternally, this uses the [HF Python client](https://github.com/huggingface/huggingface_hub) to interact with the API.\n\nApart from `__init__`, all methods are private or match those of the base class.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/huggingface", "source_site": "pydantic_ai"}
{"title": "HuggingFaceStreamedResponse", "anchor": "huggingfacestreamedresponse", "heading_level": 0, "md_text": "Implementation of `StreamedResponse` for Hugging Face models.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/huggingface", "source_site": "pydantic_ai"}
{"title": "HuggingFaceModel.__init__", "anchor": "huggingfacemodel-__init__", "heading_level": 0, "md_text": "Initialize a Hugging Face model.\n\nArgs:\n    model_name: The name of the Model to use. You can browse available models [here](https://huggingface.co/models?pipeline_tag=text-generation&inference_provider=all&sort=trending).\n    provider: The provider to use for Hugging Face Inference Providers. Can be either the string 'huggingface' or an\n        instance of `Provider[AsyncInferenceClient]`. If not provided, the other parameters will be used.\n    profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n    settings: Model-specific settings that will be used as defaults for this model.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/models/huggingface", "source_site": "pydantic_ai"}
{"title": "HuggingFaceModel.model_name", "anchor": "huggingfacemodel-model_name", "heading_level": 0, "md_text": "The model name.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/huggingface", "source_site": "pydantic_ai"}
{"title": "HuggingFaceModel.system", "anchor": "huggingfacemodel-system", "heading_level": 0, "md_text": "The system / model provider.", "url": "https://ai.pydantic.dev/api/system/", "page": "pydantic_ai_slim/pydantic_ai/models/huggingface", "source_site": "pydantic_ai"}
{"title": "HuggingFaceModel._process_response", "anchor": "huggingfacemodel-_process_response", "heading_level": 0, "md_text": "Process a non-streamed response, and prepare a message to return.", "url": "https://ai.pydantic.dev/api/-process-response/", "page": "pydantic_ai_slim/pydantic_ai/models/huggingface", "source_site": "pydantic_ai"}
{"title": "HuggingFaceModel._process_streamed_response", "anchor": "huggingfacemodel-_process_streamed_response", "heading_level": 0, "md_text": "Process a streamed response, and prepare a streaming response to return.", "url": "https://ai.pydantic.dev/api/-process-streamed-response/", "page": "pydantic_ai_slim/pydantic_ai/models/huggingface", "source_site": "pydantic_ai"}
{"title": "HuggingFaceModel._map_messages", "anchor": "huggingfacemodel-_map_messages", "heading_level": 0, "md_text": "Just maps a `pydantic_ai.Message` to a `huggingface_hub.ChatCompletionInputMessage`.", "url": "https://ai.pydantic.dev/api/-map-messages/", "page": "pydantic_ai_slim/pydantic_ai/models/huggingface", "source_site": "pydantic_ai"}
{"title": "HuggingFaceStreamedResponse.model_name", "anchor": "huggingfacestreamedresponse-model_name", "heading_level": 0, "md_text": "Get the model name of the response.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/huggingface", "source_site": "pydantic_ai"}
{"title": "HuggingFaceStreamedResponse.provider_name", "anchor": "huggingfacestreamedresponse-provider_name", "heading_level": 0, "md_text": "Get the provider name.", "url": "https://ai.pydantic.dev/api/provider-name/", "page": "pydantic_ai_slim/pydantic_ai/models/huggingface", "source_site": "pydantic_ai"}
{"title": "HuggingFaceStreamedResponse.timestamp", "anchor": "huggingfacestreamedresponse-timestamp", "heading_level": 0, "md_text": "Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/timestamp/", "page": "pydantic_ai_slim/pydantic_ai/models/huggingface", "source_site": "pydantic_ai"}
{"title": "GroqModelSettings", "anchor": "groqmodelsettings", "heading_level": 0, "md_text": "Settings used for a Groq model request.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/groq", "source_site": "pydantic_ai"}
{"title": "GroqModel", "anchor": "groqmodel", "heading_level": 0, "md_text": "A model that uses the Groq API.\n\nInternally, this uses the [Groq Python client](https://github.com/groq/groq-python) to interact with the API.\n\nApart from `__init__`, all methods are private or match those of the base class.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/groq", "source_site": "pydantic_ai"}
{"title": "GroqStreamedResponse", "anchor": "groqstreamedresponse", "heading_level": 0, "md_text": "Implementation of `StreamedResponse` for Groq models.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/groq", "source_site": "pydantic_ai"}
{"title": "GroqModel.__init__", "anchor": "groqmodel-__init__", "heading_level": 0, "md_text": "Initialize a Groq model.\n\nArgs:\n    model_name: The name of the Groq model to use. List of model names available\n        [here](https://console.groq.com/docs/models).\n    provider: The provider to use for authentication and API access. Can be either the string\n        'groq' or an instance of `Provider[AsyncGroq]`. If not provided, a new provider will be\n        created using the other parameters.\n    profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n    settings: Model-specific settings that will be used as defaults for this model.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/models/groq", "source_site": "pydantic_ai"}
{"title": "GroqModel.model_name", "anchor": "groqmodel-model_name", "heading_level": 0, "md_text": "The model name.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/groq", "source_site": "pydantic_ai"}
{"title": "GroqModel.system", "anchor": "groqmodel-system", "heading_level": 0, "md_text": "The model provider.", "url": "https://ai.pydantic.dev/api/system/", "page": "pydantic_ai_slim/pydantic_ai/models/groq", "source_site": "pydantic_ai"}
{"title": "GroqModel._process_response", "anchor": "groqmodel-_process_response", "heading_level": 0, "md_text": "Process a non-streamed response, and prepare a message to return.", "url": "https://ai.pydantic.dev/api/-process-response/", "page": "pydantic_ai_slim/pydantic_ai/models/groq", "source_site": "pydantic_ai"}
{"title": "GroqModel._process_streamed_response", "anchor": "groqmodel-_process_streamed_response", "heading_level": 0, "md_text": "Process a streamed response, and prepare a streaming response to return.", "url": "https://ai.pydantic.dev/api/-process-streamed-response/", "page": "pydantic_ai_slim/pydantic_ai/models/groq", "source_site": "pydantic_ai"}
{"title": "GroqModel._map_messages", "anchor": "groqmodel-_map_messages", "heading_level": 0, "md_text": "Just maps a `pydantic_ai.Message` to a `groq.types.ChatCompletionMessageParam`.", "url": "https://ai.pydantic.dev/api/-map-messages/", "page": "pydantic_ai_slim/pydantic_ai/models/groq", "source_site": "pydantic_ai"}
{"title": "GroqStreamedResponse.model_name", "anchor": "groqstreamedresponse-model_name", "heading_level": 0, "md_text": "Get the model name of the response.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/groq", "source_site": "pydantic_ai"}
{"title": "GroqStreamedResponse.provider_name", "anchor": "groqstreamedresponse-provider_name", "heading_level": 0, "md_text": "Get the provider name.", "url": "https://ai.pydantic.dev/api/provider-name/", "page": "pydantic_ai_slim/pydantic_ai/models/groq", "source_site": "pydantic_ai"}
{"title": "GroqStreamedResponse.timestamp", "anchor": "groqstreamedresponse-timestamp", "heading_level": 0, "md_text": "Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/timestamp/", "page": "pydantic_ai_slim/pydantic_ai/models/groq", "source_site": "pydantic_ai"}
{"title": "__init__", "anchor": "__init__", "heading_level": 0, "md_text": "Logic related to making requests to an LLM.\n\nThe aim here is to make a common interface for different LLMs, so that the rest of the code can be agnostic to the\nspecific LLM being used.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "ModelRequestParameters", "anchor": "modelrequestparameters", "heading_level": 0, "md_text": "Configuration for an agent's request to a model, specifically related to tools and output handling.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "Model", "anchor": "model", "heading_level": 0, "md_text": "Abstract class for a model.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "StreamedResponse", "anchor": "streamedresponse", "heading_level": 0, "md_text": "Streamed response from an LLM when calling a tool.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "check_allow_model_requests", "anchor": "check_allow_model_requests", "heading_level": 0, "md_text": "Check if model requests are allowed.\n\nIf you're defining your own models that have costs or latency associated with their use, you should call this in\n[`Model.request`][pydantic_ai.models.Model.request] and [`Model.request_stream`][pydantic_ai.models.Model.request_stream].\n\nRaises:\n    RuntimeError: If model requests are not allowed.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "override_allow_model_requests", "anchor": "override_allow_model_requests", "heading_level": 0, "md_text": "Context manager to temporarily override [`ALLOW_MODEL_REQUESTS`][pydantic_ai.models.ALLOW_MODEL_REQUESTS].\n\nArgs:\n    allow_model_requests: Whether to allow model requests within the context.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "infer_model", "anchor": "infer_model", "heading_level": 0, "md_text": "Infer the model from the name.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "cached_async_http_client", "anchor": "cached_async_http_client", "heading_level": 0, "md_text": "Cached HTTPX async client that creates a separate client for each provider.\n\nThe client is cached based on the provider parameter. If provider is None, it's used for non-provider specific\nrequests (like downloading images). Multiple agents and calls can share the same client when they use the same provider.\n\nEach client will get its own transport with its own connection pool. The default pool size is defined by `httpx.DEFAULT_LIMITS`.\n\nThere are good reasons why in production you should use a `httpx.AsyncClient` as an async context manager as\ndescribed in [encode/httpx#2026](https://github.com/encode/httpx/pull/2026), but when experimenting or showing\nexamples, it's very useful not to.\n\nThe default timeouts match those of OpenAI,\nsee <https://github.com/openai/openai-python/blob/v1.54.4/src/openai/_constants.py#L9>.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "DownloadedItem", "anchor": "downloadeditem", "heading_level": 0, "md_text": "The downloaded data and its type.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "download_item", "anchor": "download_item", "heading_level": 0, "md_text": "Download an item by URL and return the content as a bytes object or a (base64-encoded) string.\n\nArgs:\n    item: The item to download.\n    data_format: The format to return the content in:\n        - `bytes`: The raw bytes of the content.\n        - `base64`: The base64-encoded content.\n        - `base64_uri`: The base64-encoded content as a data URI.\n        - `text`: The content as a string.\n    type_format: The format to return the media type in:\n        - `mime`: The media type as a MIME type.\n        - `extension`: The media type as an extension.\n\nRaises:\n    UserError: If the URL points to a YouTube video or its protocol is gs://.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "get_user_agent", "anchor": "get_user_agent", "heading_level": 0, "md_text": "Get the user agent string for the HTTP client.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "_get_final_result_event", "anchor": "_get_final_result_event", "heading_level": 0, "md_text": "Return an appropriate FinalResultEvent if `e` corresponds to a part that will produce a final result.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "Model.__init__", "anchor": "model-__init__", "heading_level": 0, "md_text": "Initialize the model with optional settings and profile.\n\nArgs:\n    settings: Model-specific settings that will be used as defaults for this model.\n    profile: The model profile to use.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "Model.settings", "anchor": "model-settings", "heading_level": 0, "md_text": "Get the model settings.", "url": "https://ai.pydantic.dev/api/settings/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "Model.request", "anchor": "model-request", "heading_level": 0, "md_text": "Make a request to the model.", "url": "https://ai.pydantic.dev/api/request/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "Model.count_tokens", "anchor": "model-count_tokens", "heading_level": 0, "md_text": "Make a request to the model for counting tokens.", "url": "https://ai.pydantic.dev/api/count-tokens/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "Model.request_stream", "anchor": "model-request_stream", "heading_level": 0, "md_text": "Make a request to the model and return a streaming response.", "url": "https://ai.pydantic.dev/api/request-stream/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "Model.customize_request_parameters", "anchor": "model-customize_request_parameters", "heading_level": 0, "md_text": "Customize the request parameters for the model.\n\nThis method can be overridden by subclasses to modify the request parameters before sending them to the model.\nIn particular, this method can be used to make modifications to the generated tool JSON schemas if necessary\nfor vendor/model-specific reasons.", "url": "https://ai.pydantic.dev/api/customize-request-parameters/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "Model.prepare_request", "anchor": "model-prepare_request", "heading_level": 0, "md_text": "Prepare request inputs before they are passed to the provider.\n\nThis merges the given ``model_settings`` with the model's own ``settings`` attribute and ensures\n``customize_request_parameters`` is applied to the resolved\n[`ModelRequestParameters`][pydantic_ai.models.ModelRequestParameters]. Subclasses can override this method if\nthey need to customize the preparation flow further, but most implementations should simply call\n``self.prepare_request(...)`` at the start of their ``request`` (and related) methods.", "url": "https://ai.pydantic.dev/api/prepare-request/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "Model.model_name", "anchor": "model-model_name", "heading_level": 0, "md_text": "The model name.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "Model.profile", "anchor": "model-profile", "heading_level": 0, "md_text": "The model profile.", "url": "https://ai.pydantic.dev/api/profile/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "Model.system", "anchor": "model-system", "heading_level": 0, "md_text": "The model provider, ex: openai.\n\nUse to populate the `gen_ai.system` OpenTelemetry semantic convention attribute,\nso should use well-known values listed in\nhttps://opentelemetry.io/docs/specs/semconv/attributes-registry/gen-ai/#gen-ai-system\nwhen applicable.", "url": "https://ai.pydantic.dev/api/system/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "Model.base_url", "anchor": "model-base_url", "heading_level": 0, "md_text": "The base URL for the provider API, if available.", "url": "https://ai.pydantic.dev/api/base-url/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "Model._get_instructions", "anchor": "model-_get_instructions", "heading_level": 0, "md_text": "Get instructions from the first ModelRequest found when iterating messages in reverse.\n\nIn the case that a \"mock\" request was generated to include a tool-return part for a result tool,\nwe want to use the instructions from the second-to-most-recent request (which should correspond to the\noriginal request that generated the response that resulted in the tool-return part).", "url": "https://ai.pydantic.dev/api/-get-instructions/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "StreamedResponse.__aiter__", "anchor": "streamedresponse-__aiter__", "heading_level": 0, "md_text": "Stream the response as an async iterable of [`ModelResponseStreamEvent`][pydantic_ai.messages.ModelResponseStreamEvent]s.\n\nThis proxies the `_event_iterator()` and emits all events, while also checking for matches\non the result schema and emitting a [`FinalResultEvent`][pydantic_ai.messages.FinalResultEvent] if/when the\nfirst match is found.", "url": "https://ai.pydantic.dev/api/--aiter--/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "StreamedResponse._get_event_iterator", "anchor": "streamedresponse-_get_event_iterator", "heading_level": 0, "md_text": "Return an async iterator of [`ModelResponseStreamEvent`][pydantic_ai.messages.ModelResponseStreamEvent]s.\n\nThis method should be implemented by subclasses to translate the vendor-specific stream of events into\npydantic_ai-format events.\n\nIt should use the `_parts_manager` to handle deltas, and should update the `_usage` attributes as it goes.", "url": "https://ai.pydantic.dev/api/-get-event-iterator/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "StreamedResponse.get", "anchor": "streamedresponse-get", "heading_level": 0, "md_text": "Build a [`ModelResponse`][pydantic_ai.messages.ModelResponse] from the data received from the stream so far.", "url": "https://ai.pydantic.dev/api/get/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "StreamedResponse.usage", "anchor": "streamedresponse-usage", "heading_level": 0, "md_text": "Get the usage of the response so far. This will not be the final usage until the stream is exhausted.", "url": "https://ai.pydantic.dev/api/usage/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "StreamedResponse.model_name", "anchor": "streamedresponse-model_name", "heading_level": 0, "md_text": "Get the model name of the response.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "StreamedResponse.provider_name", "anchor": "streamedresponse-provider_name", "heading_level": 0, "md_text": "Get the provider name.", "url": "https://ai.pydantic.dev/api/provider-name/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "StreamedResponse.timestamp", "anchor": "streamedresponse-timestamp", "heading_level": 0, "md_text": "Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/timestamp/", "page": "pydantic_ai_slim/pydantic_ai/models/__init__", "source_site": "pydantic_ai"}
{"title": "FunctionModel", "anchor": "functionmodel", "heading_level": 0, "md_text": "A model controlled by a local function.\n\nApart from `__init__`, all methods are private or match those of the base class.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/function", "source_site": "pydantic_ai"}
{"title": "AgentInfo", "anchor": "agentinfo", "heading_level": 0, "md_text": "Information about an agent.\n\nThis is passed as the second to functions used within [`FunctionModel`][pydantic_ai.models.function.FunctionModel].", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/function", "source_site": "pydantic_ai"}
{"title": "DeltaToolCall", "anchor": "deltatoolcall", "heading_level": 0, "md_text": "Incremental change to a tool call.\n\nUsed to describe a chunk when streaming structured responses.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/function", "source_site": "pydantic_ai"}
{"title": "DeltaThinkingPart", "anchor": "deltathinkingpart", "heading_level": 0, "md_text": "Incremental change to a thinking part.\n\nUsed to describe a chunk when streaming thinking responses.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/function", "source_site": "pydantic_ai"}
{"title": "FunctionStreamedResponse", "anchor": "functionstreamedresponse", "heading_level": 0, "md_text": "Implementation of `StreamedResponse` for [FunctionModel][pydantic_ai.models.function.FunctionModel].", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/function", "source_site": "pydantic_ai"}
{"title": "_estimate_usage", "anchor": "_estimate_usage", "heading_level": 0, "md_text": "Very rough guesstimate of the token usage associated with a series of messages.\n\nThis is designed to be used solely to give plausible numbers for testing!", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/function", "source_site": "pydantic_ai"}
{"title": "FunctionModel.__init__", "anchor": "functionmodel-__init__", "heading_level": 0, "md_text": "Initialize a `FunctionModel`.\n\nEither `function` or `stream_function` must be provided, providing both is allowed.\n\nArgs:\n    function: The function to call for non-streamed requests.\n    stream_function: The function to call for streamed requests.\n    model_name: The name of the model. If not provided, a name is generated from the function names.\n    profile: The model profile to use.\n    settings: Model-specific settings that will be used as defaults for this model.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/models/function", "source_site": "pydantic_ai"}
{"title": "FunctionModel.model_name", "anchor": "functionmodel-model_name", "heading_level": 0, "md_text": "The model name.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/function", "source_site": "pydantic_ai"}
{"title": "FunctionModel.system", "anchor": "functionmodel-system", "heading_level": 0, "md_text": "The system / model provider.", "url": "https://ai.pydantic.dev/api/system/", "page": "pydantic_ai_slim/pydantic_ai/models/function", "source_site": "pydantic_ai"}
{"title": "FunctionStreamedResponse.model_name", "anchor": "functionstreamedresponse-model_name", "heading_level": 0, "md_text": "Get the model name of the response.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/function", "source_site": "pydantic_ai"}
{"title": "FunctionStreamedResponse.provider_name", "anchor": "functionstreamedresponse-provider_name", "heading_level": 0, "md_text": "Get the provider name.", "url": "https://ai.pydantic.dev/api/provider-name/", "page": "pydantic_ai_slim/pydantic_ai/models/function", "source_site": "pydantic_ai"}
{"title": "FunctionStreamedResponse.timestamp", "anchor": "functionstreamedresponse-timestamp", "heading_level": 0, "md_text": "Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/timestamp/", "page": "pydantic_ai_slim/pydantic_ai/models/function", "source_site": "pydantic_ai"}
{"title": "FallbackModel", "anchor": "fallbackmodel", "heading_level": 0, "md_text": "A model that uses one or more fallback models upon failure.\n\nApart from `__init__`, all methods are private or match those of the base class.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/fallback", "source_site": "pydantic_ai"}
{"title": "_default_fallback_condition_factory", "anchor": "_default_fallback_condition_factory", "heading_level": 0, "md_text": "Create a default fallback condition for the given exceptions.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/fallback", "source_site": "pydantic_ai"}
{"title": "FallbackModel.__init__", "anchor": "fallbackmodel-__init__", "heading_level": 0, "md_text": "Initialize a fallback model instance.\n\nArgs:\n    default_model: The name or instance of the default model to use.\n    fallback_models: The names or instances of the fallback models to use upon failure.\n    fallback_on: A callable or tuple of exceptions that should trigger a fallback.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/models/fallback", "source_site": "pydantic_ai"}
{"title": "FallbackModel.model_name", "anchor": "fallbackmodel-model_name", "heading_level": 0, "md_text": "The model name.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/fallback", "source_site": "pydantic_ai"}
{"title": "FallbackModel.request", "anchor": "fallbackmodel-request", "heading_level": 0, "md_text": "Try each model in sequence until one succeeds.\n\nIn case of failure, raise a FallbackExceptionGroup with all exceptions.", "url": "https://ai.pydantic.dev/api/request/", "page": "pydantic_ai_slim/pydantic_ai/models/fallback", "source_site": "pydantic_ai"}
{"title": "FallbackModel.request_stream", "anchor": "fallbackmodel-request_stream", "heading_level": 0, "md_text": "Try each model in sequence until one succeeds.", "url": "https://ai.pydantic.dev/api/request-stream/", "page": "pydantic_ai_slim/pydantic_ai/models/fallback", "source_site": "pydantic_ai"}
{"title": "AnthropicModelSettings", "anchor": "anthropicmodelsettings", "heading_level": 0, "md_text": "Settings used for an Anthropic model request.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/anthropic", "source_site": "pydantic_ai"}
{"title": "AnthropicModel", "anchor": "anthropicmodel", "heading_level": 0, "md_text": "A model that uses the Anthropic API.\n\nInternally, this uses the [Anthropic Python client](https://github.com/anthropics/anthropic-sdk-python) to interact with the API.\n\nApart from `__init__`, all methods are private or match those of the base class.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/anthropic", "source_site": "pydantic_ai"}
{"title": "AnthropicStreamedResponse", "anchor": "anthropicstreamedresponse", "heading_level": 0, "md_text": "Implementation of `StreamedResponse` for Anthropic models.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/anthropic", "source_site": "pydantic_ai"}
{"title": "AnthropicModel.__init__", "anchor": "anthropicmodel-__init__", "heading_level": 0, "md_text": "Initialize an Anthropic model.\n\nArgs:\n    model_name: The name of the Anthropic model to use. List of model names available\n        [here](https://docs.anthropic.com/en/docs/about-claude/models).\n    provider: The provider to use for the Anthropic API. Can be either the string 'anthropic' or an\n        instance of `Provider[AsyncAnthropicClient]`. If not provided, the other parameters will be used.\n    profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n    settings: Default model settings for this model instance.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/models/anthropic", "source_site": "pydantic_ai"}
{"title": "AnthropicModel.model_name", "anchor": "anthropicmodel-model_name", "heading_level": 0, "md_text": "The model name.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/anthropic", "source_site": "pydantic_ai"}
{"title": "AnthropicModel.system", "anchor": "anthropicmodel-system", "heading_level": 0, "md_text": "The model provider.", "url": "https://ai.pydantic.dev/api/system/", "page": "pydantic_ai_slim/pydantic_ai/models/anthropic", "source_site": "pydantic_ai"}
{"title": "AnthropicModel._process_response", "anchor": "anthropicmodel-_process_response", "heading_level": 0, "md_text": "Process a non-streamed response, and prepare a message to return.", "url": "https://ai.pydantic.dev/api/-process-response/", "page": "pydantic_ai_slim/pydantic_ai/models/anthropic", "source_site": "pydantic_ai"}
{"title": "AnthropicModel._map_message", "anchor": "anthropicmodel-_map_message", "heading_level": 0, "md_text": "Just maps a `pydantic_ai.Message` to a `anthropic.types.MessageParam`.", "url": "https://ai.pydantic.dev/api/-map-message/", "page": "pydantic_ai_slim/pydantic_ai/models/anthropic", "source_site": "pydantic_ai"}
{"title": "AnthropicStreamedResponse.model_name", "anchor": "anthropicstreamedresponse-model_name", "heading_level": 0, "md_text": "Get the model name of the response.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/anthropic", "source_site": "pydantic_ai"}
{"title": "AnthropicStreamedResponse.provider_name", "anchor": "anthropicstreamedresponse-provider_name", "heading_level": 0, "md_text": "Get the provider name.", "url": "https://ai.pydantic.dev/api/provider-name/", "page": "pydantic_ai_slim/pydantic_ai/models/anthropic", "source_site": "pydantic_ai"}
{"title": "AnthropicStreamedResponse.timestamp", "anchor": "anthropicstreamedresponse-timestamp", "heading_level": 0, "md_text": "Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/timestamp/", "page": "pydantic_ai_slim/pydantic_ai/models/anthropic", "source_site": "pydantic_ai"}
{"title": "WrapperModel", "anchor": "wrappermodel", "heading_level": 0, "md_text": "Model which wraps another model.\n\nDoes nothing on its own, used as a base class.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/wrapper", "source_site": "pydantic_ai"}
{"title": "WrapperModel.settings", "anchor": "wrappermodel-settings", "heading_level": 0, "md_text": "Get the settings from the wrapped model.", "url": "https://ai.pydantic.dev/api/settings/", "page": "pydantic_ai_slim/pydantic_ai/models/wrapper", "source_site": "pydantic_ai"}
{"title": "CohereModelSettings", "anchor": "coheremodelsettings", "heading_level": 0, "md_text": "Settings used for a Cohere model request.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/cohere", "source_site": "pydantic_ai"}
{"title": "CohereModel", "anchor": "coheremodel", "heading_level": 0, "md_text": "A model that uses the Cohere API.\n\nInternally, this uses the [Cohere Python client](\nhttps://github.com/cohere-ai/cohere-python) to interact with the API.\n\nApart from `__init__`, all methods are private or match those of the base class.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/cohere", "source_site": "pydantic_ai"}
{"title": "CohereModel.__init__", "anchor": "coheremodel-__init__", "heading_level": 0, "md_text": "Initialize an Cohere model.\n\nArgs:\n    model_name: The name of the Cohere model to use. List of model names\n        available [here](https://docs.cohere.com/docs/models#command).\n    provider: The provider to use for authentication and API access. Can be either the string\n        'cohere' or an instance of `Provider[AsyncClientV2]`. If not provided, a new provider will be\n        created using the other parameters.\n    profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n    settings: Model-specific settings that will be used as defaults for this model.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/models/cohere", "source_site": "pydantic_ai"}
{"title": "CohereModel.model_name", "anchor": "coheremodel-model_name", "heading_level": 0, "md_text": "The model name.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/cohere", "source_site": "pydantic_ai"}
{"title": "CohereModel.system", "anchor": "coheremodel-system", "heading_level": 0, "md_text": "The model provider.", "url": "https://ai.pydantic.dev/api/system/", "page": "pydantic_ai_slim/pydantic_ai/models/cohere", "source_site": "pydantic_ai"}
{"title": "CohereModel._process_response", "anchor": "coheremodel-_process_response", "heading_level": 0, "md_text": "Process a non-streamed response, and prepare a message to return.", "url": "https://ai.pydantic.dev/api/-process-response/", "page": "pydantic_ai_slim/pydantic_ai/models/cohere", "source_site": "pydantic_ai"}
{"title": "CohereModel._map_messages", "anchor": "coheremodel-_map_messages", "heading_level": 0, "md_text": "Just maps a `pydantic_ai.Message` to a `cohere.ChatMessageV2`.", "url": "https://ai.pydantic.dev/api/-map-messages/", "page": "pydantic_ai_slim/pydantic_ai/models/cohere", "source_site": "pydantic_ai"}
{"title": "instrument_model", "anchor": "instrument_model", "heading_level": 0, "md_text": "Instrument a model with OpenTelemetry/logfire.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/instrumented", "source_site": "pydantic_ai"}
{"title": "InstrumentationSettings", "anchor": "instrumentationsettings", "heading_level": 0, "md_text": "Options for instrumenting models and agents with OpenTelemetry.\n\nUsed in:\n\n- `Agent(instrument=...)`\n- [`Agent.instrument_all()`][pydantic_ai.agent.Agent.instrument_all]\n- [`InstrumentedModel`][pydantic_ai.models.instrumented.InstrumentedModel]\n\nSee the [Debugging and Monitoring guide](https://ai.pydantic.dev/logfire/) for more info.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/instrumented", "source_site": "pydantic_ai"}
{"title": "InstrumentedModel", "anchor": "instrumentedmodel", "heading_level": 0, "md_text": "Model which wraps another model so that requests are instrumented with OpenTelemetry.\n\nSee the [Debugging and Monitoring guide](https://ai.pydantic.dev/logfire/) for more info.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/instrumented", "source_site": "pydantic_ai"}
{"title": "CostCalculationFailedWarning", "anchor": "costcalculationfailedwarning", "heading_level": 0, "md_text": "Warning raised when cost calculation fails.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/instrumented", "source_site": "pydantic_ai"}
{"title": "InstrumentationSettings.__init__", "anchor": "instrumentationsettings-__init__", "heading_level": 0, "md_text": "Create instrumentation options.\n\nArgs:\n    tracer_provider: The OpenTelemetry tracer provider to use.\n        If not provided, the global tracer provider is used.\n        Calling `logfire.configure()` sets the global tracer provider, so most users don't need this.\n    meter_provider: The OpenTelemetry meter provider to use.\n        If not provided, the global meter provider is used.\n        Calling `logfire.configure()` sets the global meter provider, so most users don't need this.\n    include_binary_content: Whether to include binary content in the instrumentation events.\n    include_content: Whether to include prompts, completions, and tool call arguments and responses\n        in the instrumentation events.\n    version: Version of the data format. This is unrelated to the Pydantic AI package version.\n        Version 1 is based on the legacy event-based OpenTelemetry GenAI spec\n            and will be removed in a future release.\n            The parameters `event_mode` and `event_logger_provider` are only relevant for version 1.\n        Version 2 uses the newer OpenTelemetry GenAI spec and stores messages in the following attributes:\n            - `gen_ai.system_instructions` for instructions passed to the agent.\n            - `gen_ai.input.messages` and `gen_ai.output.messages` on model request spans.\n            - `pydantic_ai.all_messages` on agent run spans.\n    event_mode: The mode for emitting events in version 1.\n        If `'attributes'`, events are attached to the span as attributes.\n        If `'logs'`, events are emitted as OpenTelemetry log-based events.\n    event_logger_provider: The OpenTelemetry event logger provider to use.\n        If not provided, the global event logger provider is used.\n        Calling `logfire.configure()` sets the global event logger provider, so most users don't need this.\n        This is only used if `event_mode='logs'` and `version=1`.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/models/instrumented", "source_site": "pydantic_ai"}
{"title": "InstrumentationSettings.messages_to_otel_events", "anchor": "instrumentationsettings-messages_to_otel_events", "heading_level": 0, "md_text": "Convert a list of model messages to OpenTelemetry events.\n\nArgs:\n    messages: The messages to convert.\n\nReturns:\n    A list of OpenTelemetry events.", "url": "https://ai.pydantic.dev/api/messages-to-otel-events/", "page": "pydantic_ai_slim/pydantic_ai/models/instrumented", "source_site": "pydantic_ai"}
{"title": "BedrockModelSettings", "anchor": "bedrockmodelsettings", "heading_level": 0, "md_text": "Settings for Bedrock models.\n\nSee [the Bedrock Converse API docs](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html#API_runtime_Converse_RequestSyntax) for a full list.\nSee [the boto3 implementation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html) of the Bedrock Converse API.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/bedrock", "source_site": "pydantic_ai"}
{"title": "BedrockConverseModel", "anchor": "bedrockconversemodel", "heading_level": 0, "md_text": "A model that uses the Bedrock Converse API.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/bedrock", "source_site": "pydantic_ai"}
{"title": "BedrockStreamedResponse", "anchor": "bedrockstreamedresponse", "heading_level": 0, "md_text": "Implementation of `StreamedResponse` for Bedrock models.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/bedrock", "source_site": "pydantic_ai"}
{"title": "_AsyncIteratorWrapper", "anchor": "_asynciteratorwrapper", "heading_level": 0, "md_text": "Wrap a synchronous iterator in an async iterator.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/bedrock", "source_site": "pydantic_ai"}
{"title": "BedrockConverseModel.__init__", "anchor": "bedrockconversemodel-__init__", "heading_level": 0, "md_text": "Initialize a Bedrock model.\n\nArgs:\n    model_name: The name of the model to use.\n    model_name: The name of the Bedrock model to use. List of model names available\n        [here](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html).\n    provider: The provider to use for authentication and API access. Can be either the string\n        'bedrock' or an instance of `Provider[BaseClient]`. If not provided, a new provider will be\n        created using the other parameters.\n    profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n    settings: Model-specific settings that will be used as defaults for this model.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/models/bedrock", "source_site": "pydantic_ai"}
{"title": "BedrockConverseModel.model_name", "anchor": "bedrockconversemodel-model_name", "heading_level": 0, "md_text": "The model name.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/bedrock", "source_site": "pydantic_ai"}
{"title": "BedrockConverseModel.system", "anchor": "bedrockconversemodel-system", "heading_level": 0, "md_text": "The model provider.", "url": "https://ai.pydantic.dev/api/system/", "page": "pydantic_ai_slim/pydantic_ai/models/bedrock", "source_site": "pydantic_ai"}
{"title": "BedrockConverseModel._map_messages", "anchor": "bedrockconversemodel-_map_messages", "heading_level": 0, "md_text": "Maps a `pydantic_ai.Message` to the Bedrock `MessageUnionTypeDef`.\n\nGroups consecutive ToolReturnPart objects into a single user message as required by Bedrock Claude/Nova models.", "url": "https://ai.pydantic.dev/api/-map-messages/", "page": "pydantic_ai_slim/pydantic_ai/models/bedrock", "source_site": "pydantic_ai"}
{"title": "BedrockStreamedResponse._get_event_iterator", "anchor": "bedrockstreamedresponse-_get_event_iterator", "heading_level": 0, "md_text": "Return an async iterator of [`ModelResponseStreamEvent`][pydantic_ai.messages.ModelResponseStreamEvent]s.\n\nThis method should be implemented by subclasses to translate the vendor-specific stream of events into\npydantic_ai-format events.", "url": "https://ai.pydantic.dev/api/-get-event-iterator/", "page": "pydantic_ai_slim/pydantic_ai/models/bedrock", "source_site": "pydantic_ai"}
{"title": "BedrockStreamedResponse.model_name", "anchor": "bedrockstreamedresponse-model_name", "heading_level": 0, "md_text": "Get the model name of the response.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/bedrock", "source_site": "pydantic_ai"}
{"title": "BedrockStreamedResponse.provider_name", "anchor": "bedrockstreamedresponse-provider_name", "heading_level": 0, "md_text": "Get the provider name.", "url": "https://ai.pydantic.dev/api/provider-name/", "page": "pydantic_ai_slim/pydantic_ai/models/bedrock", "source_site": "pydantic_ai"}
{"title": "GeminiModelSettings", "anchor": "geminimodelsettings", "heading_level": 0, "md_text": "Settings used for a Gemini model request.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "GeminiModel", "anchor": "geminimodel", "heading_level": 0, "md_text": "A model that uses Gemini via `generativelanguage.googleapis.com` API.\n\nThis is implemented from scratch rather than using a dedicated SDK, good API documentation is\navailable [here](https://ai.google.dev/api).\n\nApart from `__init__`, all methods are private or match those of the base class.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "AuthProtocol", "anchor": "authprotocol", "heading_level": 0, "md_text": "Abstract definition for Gemini authentication.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "ApiKeyAuth", "anchor": "apikeyauth", "heading_level": 0, "md_text": "Authentication using an API key for the `X-Goog-Api-Key` header.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "GeminiStreamedResponse", "anchor": "geministreamedresponse", "heading_level": 0, "md_text": "Implementation of `StreamedResponse` for the Gemini model.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "_GeminiRequest", "anchor": "_geminirequest", "heading_level": 0, "md_text": "Schema for an API request to the Gemini API.\n\nSee <https://ai.google.dev/api/generate-content#request-body> for API docs.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "GeminiSafetySettings", "anchor": "geminisafetysettings", "heading_level": 0, "md_text": "Safety settings options for Gemini model request.\n\nSee [Gemini API docs](https://ai.google.dev/gemini-api/docs/safety-settings) for safety category and threshold descriptions.\nFor an example on how to use `GeminiSafetySettings`, see [here](../../agents.md#model-specific-settings).", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "ThinkingConfig", "anchor": "thinkingconfig", "heading_level": 0, "md_text": "The thinking features configuration.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "_GeminiGenerationConfig", "anchor": "_geminigenerationconfig", "heading_level": 0, "md_text": "Schema for an API request to the Gemini API.\n\nNote there are many additional fields available that have not been added yet.\n\nSee <https://ai.google.dev/api/generate-content#generationconfig> for API docs.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "_GeminiInlineDataPart", "anchor": "_geminiinlinedatapart", "heading_level": 0, "md_text": "See <https://ai.google.dev/api/caching#Blob>.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "_GeminiFileData", "anchor": "_geminifiledata", "heading_level": 0, "md_text": "See <https://ai.google.dev/api/caching#FileData>.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "_GeminiFunctionCall", "anchor": "_geminifunctioncall", "heading_level": 0, "md_text": "See <https://ai.google.dev/api/caching#FunctionCall>.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "_GeminiFunctionResponse", "anchor": "_geminifunctionresponse", "heading_level": 0, "md_text": "See <https://ai.google.dev/api/caching#FunctionResponse>.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "_GeminiResponse", "anchor": "_geminiresponse", "heading_level": 0, "md_text": "Schema for the response from the Gemini API.\n\nSee <https://ai.google.dev/api/generate-content#v1beta.GenerateContentResponse>\nand <https://cloud.google.com/vertex-ai/docs/reference/rest/v1/GenerateContentResponse>", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "_GeminiCandidates", "anchor": "_geminicandidates", "heading_level": 0, "md_text": "See <https://ai.google.dev/api/generate-content#v1beta.Candidate>.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "_GeminiModalityTokenCount", "anchor": "_geminimodalitytokencount", "heading_level": 0, "md_text": "See <https://ai.google.dev/api/generate-content#modalitytokencount>.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "_GeminiUsageMetaData", "anchor": "_geminiusagemetadata", "heading_level": 0, "md_text": "See <https://ai.google.dev/api/generate-content#UsageMetadata>.\n\nThe docs suggest all fields are required, but some are actually not required, so we assume they are all optional.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "_GeminiSafetyRating", "anchor": "_geminisafetyrating", "heading_level": 0, "md_text": "See <https://ai.google.dev/gemini-api/docs/safety-settings#safety-filters>.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "_GeminiPromptFeedback", "anchor": "_geminipromptfeedback", "heading_level": 0, "md_text": "See <https://ai.google.dev/api/generate-content#v1beta.GenerateContentResponse>.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "_ensure_decodeable", "anchor": "_ensure_decodeable", "heading_level": 0, "md_text": "Trim any invalid unicode point bytes off the end of a bytearray.\n\nThis is necessary before attempting to parse streaming JSON bytes.\n\nThis is a temporary workaround until https://github.com/pydantic/pydantic-core/issues/1633 is resolved", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "GeminiModel.__init__", "anchor": "geminimodel-__init__", "heading_level": 0, "md_text": "Initialize a Gemini model.\n\nArgs:\n    model_name: The name of the model to use.\n    provider: The provider to use for authentication and API access. Can be either the string\n        'google-gla' or 'google-vertex' or an instance of `Provider[httpx.AsyncClient]`.\n        If not provided, a new provider will be created using the other parameters.\n    profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n    settings: Default model settings for this model instance.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "GeminiModel.model_name", "anchor": "geminimodel-model_name", "heading_level": 0, "md_text": "The model name.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "GeminiModel.system", "anchor": "geminimodel-system", "heading_level": 0, "md_text": "The model provider.", "url": "https://ai.pydantic.dev/api/system/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "GeminiModel._process_streamed_response", "anchor": "geminimodel-_process_streamed_response", "heading_level": 0, "md_text": "Process a streamed response, and prepare a streaming response to return.", "url": "https://ai.pydantic.dev/api/-process-streamed-response/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "GeminiStreamedResponse.model_name", "anchor": "geministreamedresponse-model_name", "heading_level": 0, "md_text": "Get the model name of the response.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "GeminiStreamedResponse.provider_name", "anchor": "geministreamedresponse-provider_name", "heading_level": 0, "md_text": "Get the provider name.", "url": "https://ai.pydantic.dev/api/provider-name/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "GeminiStreamedResponse.timestamp", "anchor": "geministreamedresponse-timestamp", "heading_level": 0, "md_text": "Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/timestamp/", "page": "pydantic_ai_slim/pydantic_ai/models/gemini", "source_site": "pydantic_ai"}
{"title": "MCPSamplingModelSettings", "anchor": "mcpsamplingmodelsettings", "heading_level": 0, "md_text": "Settings used for an MCP Sampling model request.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/mcp_sampling", "source_site": "pydantic_ai"}
{"title": "MCPSamplingModel", "anchor": "mcpsamplingmodel", "heading_level": 0, "md_text": "A model that uses MCP Sampling.\n\n[MCP Sampling](https://modelcontextprotocol.io/docs/concepts/sampling)\nallows an MCP server to make requests to a model by calling back to the MCP client that connected to it.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/mcp_sampling", "source_site": "pydantic_ai"}
{"title": "MCPSamplingModel.model_name", "anchor": "mcpsamplingmodel-model_name", "heading_level": 0, "md_text": "The model name.\n\nSince the model name isn't known until the request is made, this property always returns `'mcp-sampling'`.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/mcp_sampling", "source_site": "pydantic_ai"}
{"title": "MCPSamplingModel.system", "anchor": "mcpsamplingmodel-system", "heading_level": 0, "md_text": "The system / model provider, returns `'MCP'`.", "url": "https://ai.pydantic.dev/api/system/", "page": "pydantic_ai_slim/pydantic_ai/models/mcp_sampling", "source_site": "pydantic_ai"}
{"title": "GoogleModelSettings", "anchor": "googlemodelsettings", "heading_level": 0, "md_text": "Settings used for a Gemini model request.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/google", "source_site": "pydantic_ai"}
{"title": "GoogleModel", "anchor": "googlemodel", "heading_level": 0, "md_text": "A model that uses Gemini via `generativelanguage.googleapis.com` API.\n\nThis is implemented from scratch rather than using a dedicated SDK, good API documentation is\navailable [here](https://ai.google.dev/api).\n\nApart from `__init__`, all methods are private or match those of the base class.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/google", "source_site": "pydantic_ai"}
{"title": "GeminiStreamedResponse", "anchor": "geministreamedresponse", "heading_level": 0, "md_text": "Implementation of `StreamedResponse` for the Gemini model.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/models/google", "source_site": "pydantic_ai"}
{"title": "GoogleModel.__init__", "anchor": "googlemodel-__init__", "heading_level": 0, "md_text": "Initialize a Gemini model.\n\nArgs:\n    model_name: The name of the model to use.\n    provider: The provider to use for authentication and API access. Can be either the string\n        'google-gla' or 'google-vertex' or an instance of `Provider[google.genai.AsyncClient]`.\n        Defaults to 'google-gla'.\n    profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.\n    settings: The model settings to use. Defaults to None.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/models/google", "source_site": "pydantic_ai"}
{"title": "GoogleModel.model_name", "anchor": "googlemodel-model_name", "heading_level": 0, "md_text": "The model name.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/google", "source_site": "pydantic_ai"}
{"title": "GoogleModel.system", "anchor": "googlemodel-system", "heading_level": 0, "md_text": "The model provider.", "url": "https://ai.pydantic.dev/api/system/", "page": "pydantic_ai_slim/pydantic_ai/models/google", "source_site": "pydantic_ai"}
{"title": "GoogleModel._process_streamed_response", "anchor": "googlemodel-_process_streamed_response", "heading_level": 0, "md_text": "Process a streamed response, and prepare a streaming response to return.", "url": "https://ai.pydantic.dev/api/-process-streamed-response/", "page": "pydantic_ai_slim/pydantic_ai/models/google", "source_site": "pydantic_ai"}
{"title": "GeminiStreamedResponse.model_name", "anchor": "geministreamedresponse-model_name", "heading_level": 0, "md_text": "Get the model name of the response.", "url": "https://ai.pydantic.dev/api/model-name/", "page": "pydantic_ai_slim/pydantic_ai/models/google", "source_site": "pydantic_ai"}
{"title": "GeminiStreamedResponse.provider_name", "anchor": "geministreamedresponse-provider_name", "heading_level": 0, "md_text": "Get the provider name.", "url": "https://ai.pydantic.dev/api/provider-name/", "page": "pydantic_ai_slim/pydantic_ai/models/google", "source_site": "pydantic_ai"}
{"title": "GeminiStreamedResponse.timestamp", "anchor": "geministreamedresponse-timestamp", "heading_level": 0, "md_text": "Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/timestamp/", "page": "pydantic_ai_slim/pydantic_ai/models/google", "source_site": "pydantic_ai"}
{"title": "Agent", "anchor": "agent", "heading_level": 0, "md_text": "Class for defining \"agents\" - a way to have a specific type of \"conversation\" with an LLM.\n\nAgents are generic in the dependency type they take [`AgentDepsT`][pydantic_ai.tools.AgentDepsT]\nand the output type they return, [`OutputDataT`][pydantic_ai.output.OutputDataT].\n\nBy default, if neither generic parameter is customised, agents have type `Agent[None, str]`.\n\nMinimal usage example:\n\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n```", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/agent/__init__", "source_site": "pydantic_ai"}
{"title": "Agent.__init__", "anchor": "agent-__init__", "heading_level": 0, "md_text": "Create an agent.\n\nArgs:\n    model: The default model to use for this agent, if not provided,\n        you must provide the model when calling it. We allow `str` here since the actual list of allowed models changes frequently.\n    output_type: The type of the output data, used to validate the data returned by the model,\n        defaults to `str`.\n    instructions: Instructions to use for this agent, you can also register instructions via a function with\n        [`instructions`][pydantic_ai.Agent.instructions].\n    system_prompt: Static system prompts to use for this agent, you can also register system\n        prompts via a function with [`system_prompt`][pydantic_ai.Agent.system_prompt].\n    deps_type: The type used for dependency injection, this parameter exists solely to allow you to fully\n        parameterize the agent, and therefore get the best out of static type checking.\n        If you're not using deps, but want type checking to pass, you can set `deps=None` to satisfy Pyright\n        or add a type hint `: Agent[None, <return type>]`.\n    name: The name of the agent, used for logging. If `None`, we try to infer the agent name from the call frame\n        when the agent is first run.\n    model_settings: Optional model request settings to use for this agent's runs, by default.\n    retries: The default number of retries to allow for tool calls and output validation, before raising an error.\n        For model request retries, see the [HTTP Request Retries](../retries.md) documentation.\n    output_retries: The maximum number of retries to allow for output validation, defaults to `retries`.\n    tools: Tools to register with the agent, you can also register tools via the decorators\n        [`@agent.tool`][pydantic_ai.Agent.tool] and [`@agent.tool_plain`][pydantic_ai.Agent.tool_plain].\n    builtin_tools: The builtin tools that the agent will use. This depends on the model, as some models may not\n        support certain tools. If the model doesn't support the builtin tools, an error will be raised.\n    prepare_tools: Custom function to prepare the tool definition of all tools for each step, except output tools.\n        This is useful if you want to customize the definition of multiple tools or you want to register\n        a subset of tools for a given step. See [`ToolsPrepareFunc`][pydantic_ai.tools.ToolsPrepareFunc]\n    prepare_output_tools: Custom function to prepare the tool definition of all output tools for each step.\n        This is useful if you want to customize the definition of multiple output tools or you want to register\n        a subset of output tools for a given step. See [`ToolsPrepareFunc`][pydantic_ai.tools.ToolsPrepareFunc]\n    toolsets: Toolsets to register with the agent, including MCP servers and functions which take a run context\n        and return a toolset. See [`ToolsetFunc`][pydantic_ai.toolsets.ToolsetFunc] for more information.\n    defer_model_check: by default, if you provide a [named][pydantic_ai.models.KnownModelName] model,\n        it's evaluated to create a [`Model`][pydantic_ai.models.Model] instance immediately,\n        which checks for the necessary environment variables. Set this to `false`\n        to defer the evaluation until the first run. Useful if you want to\n        [override the model][pydantic_ai.Agent.override] for testing.\n    end_strategy: Strategy for handling tool calls that are requested alongside a final result.\n        See [`EndStrategy`][pydantic_ai.agent.EndStrategy] for more information.\n    instrument: Set to True to automatically instrument with OpenTelemetry,\n        which will use Logfire if it's configured.\n        Set to an instance of [`InstrumentationSettings`][pydantic_ai.agent.InstrumentationSettings] to customize.\n        If this isn't set, then the last value set by\n        [`Agent.instrument_all()`][pydantic_ai.Agent.instrument_all]\n        will be used, which defaults to False.\n        See the [Debugging and Monitoring guide](https://ai.pydantic.dev/logfire/) for more info.\n    history_processors: Optional list of callables to process the message history before sending it to the model.\n        Each processor takes a list of messages and returns a modified list of messages.\n        Processors can be sync or async and are applied in sequence.\n    event_stream_handler: Optional handler for events from the model's streaming response and the agent's execution of tools.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/agent/__init__", "source_site": "pydantic_ai"}
{"title": "Agent.instrument_all", "anchor": "agent-instrument_all", "heading_level": 0, "md_text": "Set the instrumentation options for all agents where `instrument` is not set.", "url": "https://ai.pydantic.dev/api/instrument-all/", "page": "pydantic_ai_slim/pydantic_ai/agent/__init__", "source_site": "pydantic_ai"}
{"title": "Agent.model", "anchor": "agent-model", "heading_level": 0, "md_text": "The default model configured for this agent.", "url": "https://ai.pydantic.dev/api/model/", "page": "pydantic_ai_slim/pydantic_ai/agent/__init__", "source_site": "pydantic_ai"}
{"title": "Agent.model", "anchor": "agent-model", "heading_level": 0, "md_text": "Set the default model configured for this agent.\n\nWe allow `str` here since the actual list of allowed models changes frequently.", "url": "https://ai.pydantic.dev/api/model/", "page": "pydantic_ai_slim/pydantic_ai/agent/__init__", "source_site": "pydantic_ai"}
{"title": "Agent.name", "anchor": "agent-name", "heading_level": 0, "md_text": "The name of the agent, used for logging.\n\nIf `None`, we try to infer the agent name from the call frame when the agent is first run.", "url": "https://ai.pydantic.dev/api/name/", "page": "pydantic_ai_slim/pydantic_ai/agent/__init__", "source_site": "pydantic_ai"}
{"title": "Agent.name", "anchor": "agent-name", "heading_level": 0, "md_text": "Set the name of the agent, used for logging.", "url": "https://ai.pydantic.dev/api/name/", "page": "pydantic_ai_slim/pydantic_ai/agent/__init__", "source_site": "pydantic_ai"}
{"title": "Agent.deps_type", "anchor": "agent-deps_type", "heading_level": 0, "md_text": "The type of dependencies used by the agent.", "url": "https://ai.pydantic.dev/api/deps-type/", "page": "pydantic_ai_slim/pydantic_ai/agent/__init__", "source_site": "pydantic_ai"}
{"title": "Agent.output_type", "anchor": "agent-output_type", "heading_level": 0, "md_text": "The type of data output by agent runs, used to validate the data returned by the model, defaults to `str`.", "url": "https://ai.pydantic.dev/api/output-type/", "page": "pydantic_ai_slim/pydantic_ai/agent/__init__", "source_site": "pydantic_ai"}
{"title": "Agent.event_stream_handler", "anchor": "agent-event_stream_handler", "heading_level": 0, "md_text": "Optional handler for events from the model's streaming response and the agent's execution of tools.", "url": "https://ai.pydantic.dev/api/event-stream-handler/", "page": "pydantic_ai_slim/pydantic_ai/agent/__init__", "source_site": "pydantic_ai"}
{"title": "Agent.iter", "anchor": "agent-iter", "heading_level": 0, "md_text": "A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.\n\nThis method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an\n`AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are\nexecuted. This is the API to use if you want to consume the outputs coming from each LLM model response, or the\nstream of events coming from the execution of tools.\n\nThe `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,\nand the final result of the run once it has completed.\n\nFor more details, see the documentation of `AgentRun`.\n\nExample:\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    nodes = []\n    async with agent.iter('What is the capital of France?') as agent_run:\n        async for node in agent_run:\n            nodes.append(node)\n    print(nodes)\n    '''\n    [\n        UserPromptNode(\n            user_prompt='What is the capital of France?',\n            instructions_functions=[],\n            system_prompts=(),\n            system_prompt_functions=[],\n            system_prompt_dynamic_functions={},\n        ),\n        ModelRequestNode(\n            request=ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='What is the capital of France?',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            )\n        ),\n        CallToolsNode(\n            model_response=ModelResponse(\n                parts=[TextPart(content='The capital of France is Paris.')],\n                usage=RequestUsage(input_tokens=56, output_tokens=7),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            )\n        ),\n        End(data=FinalResult(output='The capital of France is Paris.')),\n    ]\n    '''\n    print(agent_run.result.output)\n    #> The capital of France is Paris.\n```\n\nArgs:\n    user_prompt: User input to start/continue the conversation.\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    builtin_tools: Optional additional builtin tools for this run.\n\nReturns:\n    The result of the run.", "url": "https://ai.pydantic.dev/api/iter/", "page": "pydantic_ai_slim/pydantic_ai/agent/__init__", "source_site": "pydantic_ai"}
{"title": "Agent.override", "anchor": "agent-override", "heading_level": 0, "md_text": "Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.\n\nThis is particularly useful when testing.\nYou can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).\n\nArgs:\n    name: The name to use instead of the name passed to the agent constructor and agent run.\n    deps: The dependencies to use instead of the dependencies passed to the agent run.\n    model: The model to use instead of the model passed to the agent run.\n    toolsets: The toolsets to use instead of the toolsets passed to the agent constructor and agent run.\n    tools: The tools to use instead of the tools registered with the agent.\n    instructions: The instructions to use instead of the instructions registered with the agent.", "url": "https://ai.pydantic.dev/api/override/", "page": "pydantic_ai_slim/pydantic_ai/agent/__init__", "source_site": "pydantic_ai"}
{"title": "Agent.instructions", "anchor": "agent-instructions", "heading_level": 0, "md_text": "Decorator to register an instructions function.\n\nOptionally takes [`RunContext`][pydantic_ai.tools.RunContext] as its only argument.\nCan decorate a sync or async functions.\n\nThe decorator can be used bare (`agent.instructions`).\n\nOverloads for every possible signature of `instructions` are included so the decorator doesn't obscure\nthe type of the function.\n\nExample:\n```python\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent('test', deps_type=str)\n\n@agent.instructions\ndef simple_instructions() -> str:\n    return 'foobar'\n\n@agent.instructions\nasync def async_instructions(ctx: RunContext[str]) -> str:\n    return f'{ctx.deps} is the best'\n```", "url": "https://ai.pydantic.dev/api/instructions/", "page": "pydantic_ai_slim/pydantic_ai/agent/__init__", "source_site": "pydantic_ai"}
{"title": "Agent.system_prompt", "anchor": "agent-system_prompt", "heading_level": 0, "md_text": "Decorator to register a system prompt function.\n\nOptionally takes [`RunContext`][pydantic_ai.tools.RunContext] as its only argument.\nCan decorate a sync or async functions.\n\nThe decorator can be used either bare (`agent.system_prompt`) or as a function call\n(`agent.system_prompt(...)`), see the examples below.\n\nOverloads for every possible signature of `system_prompt` are included so the decorator doesn't obscure\nthe type of the function, see `tests/typed_agent.py` for tests.\n\nArgs:\n    func: The function to decorate\n    dynamic: If True, the system prompt will be reevaluated even when `messages_history` is provided,\n        see [`SystemPromptPart.dynamic_ref`][pydantic_ai.messages.SystemPromptPart.dynamic_ref]\n\nExample:\n```python\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent('test', deps_type=str)\n\n@agent.system_prompt\ndef simple_system_prompt() -> str:\n    return 'foobar'\n\n@agent.system_prompt(dynamic=True)\nasync def async_system_prompt(ctx: RunContext[str]) -> str:\n    return f'{ctx.deps} is the best'\n```", "url": "https://ai.pydantic.dev/api/system-prompt/", "page": "pydantic_ai_slim/pydantic_ai/agent/__init__", "source_site": "pydantic_ai"}
{"title": "Agent.output_validator", "anchor": "agent-output_validator", "heading_level": 0, "md_text": "Decorator to register an output validator function.\n\nOptionally takes [`RunContext`][pydantic_ai.tools.RunContext] as its first argument.\nCan decorate a sync or async functions.\n\nOverloads for every possible signature of `output_validator` are included so the decorator doesn't obscure\nthe type of the function, see `tests/typed_agent.py` for tests.\n\nExample:\n```python\nfrom pydantic_ai import Agent, ModelRetry, RunContext\n\nagent = Agent('test', deps_type=str)\n\n@agent.output_validator\ndef output_validator_simple(data: str) -> str:\n    if 'wrong' in data:\n        raise ModelRetry('wrong response')\n    return data\n\n@agent.output_validator\nasync def output_validator_deps(ctx: RunContext[str], data: str) -> str:\n    if ctx.deps in data:\n        raise ModelRetry('wrong response')\n    return data\n\nresult = agent.run_sync('foobar', deps='spam')\nprint(result.output)\n#> success (no tool calls)\n```", "url": "https://ai.pydantic.dev/api/output-validator/", "page": "pydantic_ai_slim/pydantic_ai/agent/__init__", "source_site": "pydantic_ai"}
{"title": "Agent.tool", "anchor": "agent-tool", "heading_level": 0, "md_text": "Decorator to register a tool function which takes [`RunContext`][pydantic_ai.tools.RunContext] as its first argument.\n\nCan decorate a sync or async functions.\n\nThe docstring is inspected to extract both the tool description and description of each parameter,\n[learn more](../tools.md#function-tools-and-schema).\n\nWe can't add overloads for every possible signature of tool, since the return type is a recursive union\nso the signature of functions decorated with `@agent.tool` is obscured.\n\nExample:\n```python\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent('test', deps_type=int)\n\n@agent.tool\ndef foobar(ctx: RunContext[int], x: int) -> int:\n    return ctx.deps + x\n\n@agent.tool(retries=2)\nasync def spam(ctx: RunContext[str], y: float) -> float:\n    return ctx.deps + y\n\nresult = agent.run_sync('foobar', deps=1)\nprint(result.output)\n#> {\"foobar\":1,\"spam\":1.0}\n```\n\nArgs:\n    func: The tool function to register.\n    name: The name of the tool, defaults to the function name.\n    description: The description of the tool, defaults to the function docstring.\n    retries: The number of retries to allow for this tool, defaults to the agent's default retries,\n        which defaults to 1.\n    prepare: custom method to prepare the tool definition for each step, return `None` to omit this\n        tool from a given step. This is useful if you want to customise a tool at call time,\n        or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].\n    docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].\n        Defaults to `'auto'`, such that the format is inferred from the structure of the docstring.\n    require_parameter_descriptions: If True, raise an error if a parameter description is missing. Defaults to False.\n    schema_generator: The JSON schema generator class to use for this tool. Defaults to `GenerateToolJsonSchema`.\n    strict: Whether to enforce JSON schema compliance (only affects OpenAI).\n        See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.\n    sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.\n    requires_approval: Whether this tool requires human-in-the-loop approval. Defaults to False.\n        See the [tools documentation](../deferred-tools.md#human-in-the-loop-tool-approval) for more info.\n    metadata: Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization.", "url": "https://ai.pydantic.dev/api/tool/", "page": "pydantic_ai_slim/pydantic_ai/agent/__init__", "source_site": "pydantic_ai"}
{"title": "Agent.tool_plain", "anchor": "agent-tool_plain", "heading_level": 0, "md_text": "Decorator to register a tool function which DOES NOT take `RunContext` as an argument.\n\nCan decorate a sync or async functions.\n\nThe docstring is inspected to extract both the tool description and description of each parameter,\n[learn more](../tools.md#function-tools-and-schema).\n\nWe can't add overloads for every possible signature of tool, since the return type is a recursive union\nso the signature of functions decorated with `@agent.tool` is obscured.\n\nExample:\n```python\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent('test')\n\n@agent.tool\ndef foobar(ctx: RunContext[int]) -> int:\n    return 123\n\n@agent.tool(retries=2)\nasync def spam(ctx: RunContext[str]) -> float:\n    return 3.14\n\nresult = agent.run_sync('foobar', deps=1)\nprint(result.output)\n#> {\"foobar\":123,\"spam\":3.14}\n```\n\nArgs:\n    func: The tool function to register.\n    name: The name of the tool, defaults to the function name.\n    description: The description of the tool, defaults to the function docstring.\n    retries: The number of retries to allow for this tool, defaults to the agent's default retries,\n        which defaults to 1.\n    prepare: custom method to prepare the tool definition for each step, return `None` to omit this\n        tool from a given step. This is useful if you want to customise a tool at call time,\n        or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].\n    docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].\n        Defaults to `'auto'`, such that the format is inferred from the structure of the docstring.\n    require_parameter_descriptions: If True, raise an error if a parameter description is missing. Defaults to False.\n    schema_generator: The JSON schema generator class to use for this tool. Defaults to `GenerateToolJsonSchema`.\n    strict: Whether to enforce JSON schema compliance (only affects OpenAI).\n        See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.\n    sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.\n    requires_approval: Whether this tool requires human-in-the-loop approval. Defaults to False.\n        See the [tools documentation](../deferred-tools.md#human-in-the-loop-tool-approval) for more info.\n    metadata: Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization.", "url": "https://ai.pydantic.dev/api/tool-plain/", "page": "pydantic_ai_slim/pydantic_ai/agent/__init__", "source_site": "pydantic_ai"}
{"title": "Agent.toolset", "anchor": "agent-toolset", "heading_level": 0, "md_text": "Decorator to register a toolset function which takes [`RunContext`][pydantic_ai.tools.RunContext] as its only argument.\n\nCan decorate a sync or async functions.\n\nThe decorator can be used bare (`agent.toolset`).\n\nExample:\n```python\nfrom pydantic_ai import AbstractToolset, Agent, FunctionToolset, RunContext\n\nagent = Agent('test', deps_type=str)\n\n@agent.toolset\nasync def simple_toolset(ctx: RunContext[str]) -> AbstractToolset[str]:\n    return FunctionToolset()\n```\n\nArgs:\n    func: The toolset function to register.\n    per_run_step: Whether to re-evaluate the toolset for each run step. Defaults to True.", "url": "https://ai.pydantic.dev/api/toolset/", "page": "pydantic_ai_slim/pydantic_ai/agent/__init__", "source_site": "pydantic_ai"}
{"title": "Agent._get_model", "anchor": "agent-_get_model", "heading_level": 0, "md_text": "Create a model configured for this agent.\n\nArgs:\n    model: model to use for this run, required if `model` was not set when creating the agent.\n\nReturns:\n    The model used", "url": "https://ai.pydantic.dev/api/-get-model/", "page": "pydantic_ai_slim/pydantic_ai/agent/__init__", "source_site": "pydantic_ai"}
{"title": "Agent._get_deps", "anchor": "agent-_get_deps", "heading_level": 0, "md_text": "Get deps for a run.\n\nIf we've overridden deps via `_override_deps`, use that, otherwise use the deps passed to the call.\n\nWe could do runtime type checking of deps against `self._deps_type`, but that's a slippery slope.", "url": "https://ai.pydantic.dev/api/-get-deps/", "page": "pydantic_ai_slim/pydantic_ai/agent/__init__", "source_site": "pydantic_ai"}
{"title": "Agent._get_toolset", "anchor": "agent-_get_toolset", "heading_level": 0, "md_text": "Get the complete toolset.\n\nArgs:\n    output_toolset: The output toolset to use instead of the one built at agent construction time.\n    additional_toolsets: Additional toolsets to add, unless toolsets have been overridden.", "url": "https://ai.pydantic.dev/api/-get-toolset/", "page": "pydantic_ai_slim/pydantic_ai/agent/__init__", "source_site": "pydantic_ai"}
{"title": "Agent.toolsets", "anchor": "agent-toolsets", "heading_level": 0, "md_text": "All toolsets registered on the agent, including a function toolset holding tools that were registered on the agent directly.\n\nOutput tools are not included.", "url": "https://ai.pydantic.dev/api/toolsets/", "page": "pydantic_ai_slim/pydantic_ai/agent/__init__", "source_site": "pydantic_ai"}
{"title": "Agent.__aenter__", "anchor": "agent-__aenter__", "heading_level": 0, "md_text": "Enter the agent context.\n\nThis will start all [`MCPServerStdio`s][pydantic_ai.mcp.MCPServerStdio] registered as `toolsets` so they are ready to be used.\n\nThis is a no-op if the agent has already been entered.", "url": "https://ai.pydantic.dev/api/--aenter--/", "page": "pydantic_ai_slim/pydantic_ai/agent/__init__", "source_site": "pydantic_ai"}
{"title": "Agent.set_mcp_sampling_model", "anchor": "agent-set_mcp_sampling_model", "heading_level": 0, "md_text": "Set the sampling model on all MCP servers registered with the agent.\n\nIf no sampling model is provided, the agent's model will be used.", "url": "https://ai.pydantic.dev/api/set-mcp-sampling-model/", "page": "pydantic_ai_slim/pydantic_ai/agent/__init__", "source_site": "pydantic_ai"}
{"title": "Agent.run_mcp_servers", "anchor": "agent-run_mcp_servers", "heading_level": 0, "md_text": "Run [`MCPServerStdio`s][pydantic_ai.mcp.MCPServerStdio] so they can be used by the agent.\n\nDeprecated: use [`async with agent`][pydantic_ai.agent.Agent.__aenter__] instead.\nIf you need to set a sampling model on all MCP servers, use [`agent.set_mcp_sampling_model()`][pydantic_ai.agent.Agent.set_mcp_sampling_model].\n\nReturns: a context manager to start and shutdown the servers.", "url": "https://ai.pydantic.dev/api/run-mcp-servers/", "page": "pydantic_ai_slim/pydantic_ai/agent/__init__", "source_site": "pydantic_ai"}
{"title": "WrapperAgent", "anchor": "wrapperagent", "heading_level": 0, "md_text": "Agent which wraps another agent.\n\nDoes nothing on its own, used as a base class.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/agent/wrapper", "source_site": "pydantic_ai"}
{"title": "WrapperAgent.iter", "anchor": "wrapperagent-iter", "heading_level": 0, "md_text": "A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.\n\nThis method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an\n`AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are\nexecuted. This is the API to use if you want to consume the outputs coming from each LLM model response, or the\nstream of events coming from the execution of tools.\n\nThe `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,\nand the final result of the run once it has completed.\n\nFor more details, see the documentation of `AgentRun`.\n\nExample:\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    nodes = []\n    async with agent.iter('What is the capital of France?') as agent_run:\n        async for node in agent_run:\n            nodes.append(node)\n    print(nodes)\n    '''\n    [\n        UserPromptNode(\n            user_prompt='What is the capital of France?',\n            instructions_functions=[],\n            system_prompts=(),\n            system_prompt_functions=[],\n            system_prompt_dynamic_functions={},\n        ),\n        ModelRequestNode(\n            request=ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='What is the capital of France?',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            )\n        ),\n        CallToolsNode(\n            model_response=ModelResponse(\n                parts=[TextPart(content='The capital of France is Paris.')],\n                usage=RequestUsage(input_tokens=56, output_tokens=7),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            )\n        ),\n        End(data=FinalResult(output='The capital of France is Paris.')),\n    ]\n    '''\n    print(agent_run.result.output)\n    #> The capital of France is Paris.\n```\n\nArgs:\n    user_prompt: User input to start/continue the conversation.\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    builtin_tools: Optional additional builtin tools for this run.\n\nReturns:\n    The result of the run.", "url": "https://ai.pydantic.dev/api/iter/", "page": "pydantic_ai_slim/pydantic_ai/agent/wrapper", "source_site": "pydantic_ai"}
{"title": "WrapperAgent.override", "anchor": "wrapperagent-override", "heading_level": 0, "md_text": "Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.\n\nThis is particularly useful when testing.\nYou can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).\n\nArgs:\n    name: The name to use instead of the name passed to the agent constructor and agent run.\n    deps: The dependencies to use instead of the dependencies passed to the agent run.\n    model: The model to use instead of the model passed to the agent run.\n    toolsets: The toolsets to use instead of the toolsets passed to the agent constructor and agent run.\n    tools: The tools to use instead of the tools registered with the agent.\n    instructions: The instructions to use instead of the instructions registered with the agent.", "url": "https://ai.pydantic.dev/api/override/", "page": "pydantic_ai_slim/pydantic_ai/agent/wrapper", "source_site": "pydantic_ai"}
{"title": "AbstractAgent", "anchor": "abstractagent", "heading_level": 0, "md_text": "Abstract superclass for [`Agent`][pydantic_ai.agent.Agent], [`WrapperAgent`][pydantic_ai.agent.WrapperAgent], and your own custom agent implementations.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/agent/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractAgent.model", "anchor": "abstractagent-model", "heading_level": 0, "md_text": "The default model configured for this agent.", "url": "https://ai.pydantic.dev/api/model/", "page": "pydantic_ai_slim/pydantic_ai/agent/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractAgent.name", "anchor": "abstractagent-name", "heading_level": 0, "md_text": "The name of the agent, used for logging.\n\nIf `None`, we try to infer the agent name from the call frame when the agent is first run.", "url": "https://ai.pydantic.dev/api/name/", "page": "pydantic_ai_slim/pydantic_ai/agent/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractAgent.name", "anchor": "abstractagent-name", "heading_level": 0, "md_text": "Set the name of the agent, used for logging.", "url": "https://ai.pydantic.dev/api/name/", "page": "pydantic_ai_slim/pydantic_ai/agent/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractAgent.deps_type", "anchor": "abstractagent-deps_type", "heading_level": 0, "md_text": "The type of dependencies used by the agent.", "url": "https://ai.pydantic.dev/api/deps-type/", "page": "pydantic_ai_slim/pydantic_ai/agent/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractAgent.output_type", "anchor": "abstractagent-output_type", "heading_level": 0, "md_text": "The type of data output by agent runs, used to validate the data returned by the model, defaults to `str`.", "url": "https://ai.pydantic.dev/api/output-type/", "page": "pydantic_ai_slim/pydantic_ai/agent/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractAgent.event_stream_handler", "anchor": "abstractagent-event_stream_handler", "heading_level": 0, "md_text": "Optional handler for events from the model's streaming response and the agent's execution of tools.", "url": "https://ai.pydantic.dev/api/event-stream-handler/", "page": "pydantic_ai_slim/pydantic_ai/agent/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractAgent.toolsets", "anchor": "abstractagent-toolsets", "heading_level": 0, "md_text": "All toolsets registered on the agent.\n\nOutput tools are not included.", "url": "https://ai.pydantic.dev/api/toolsets/", "page": "pydantic_ai_slim/pydantic_ai/agent/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractAgent.run", "anchor": "abstractagent-run", "heading_level": 0, "md_text": "Run the agent with a user prompt in async mode.\n\nThis method builds an internal agent graph (using system prompts, tools and output schemas) and then\nruns the graph to completion. The result of the run is returned.\n\nExample:\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    agent_run = await agent.run('What is the capital of France?')\n    print(agent_run.output)\n    #> The capital of France is Paris.\n```\n\nArgs:\n    user_prompt: User input to start/continue the conversation.\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    event_stream_handler: Optional handler for events from the model's streaming response and the agent's execution of tools to use for this run.\n    builtin_tools: Optional additional builtin tools for this run.\n\nReturns:\n    The result of the run.", "url": "https://ai.pydantic.dev/api/run/", "page": "pydantic_ai_slim/pydantic_ai/agent/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractAgent.run_sync", "anchor": "abstractagent-run_sync", "heading_level": 0, "md_text": "Synchronously run the agent with a user prompt.\n\nThis is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] with `loop.run_until_complete(...)`.\nYou therefore can't use this method inside async code or if there's an active event loop.\n\nExample:\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nresult_sync = agent.run_sync('What is the capital of Italy?')\nprint(result_sync.output)\n#> The capital of Italy is Rome.\n```\n\nArgs:\n    user_prompt: User input to start/continue the conversation.\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    event_stream_handler: Optional handler for events from the model's streaming response and the agent's execution of tools to use for this run.\n    builtin_tools: Optional additional builtin tools for this run.\n\nReturns:\n    The result of the run.", "url": "https://ai.pydantic.dev/api/run-sync/", "page": "pydantic_ai_slim/pydantic_ai/agent/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractAgent.run_stream", "anchor": "abstractagent-run_stream", "heading_level": 0, "md_text": "Run the agent with a user prompt in async streaming mode.\n\nThis method builds an internal agent graph (using system prompts, tools and output schemas) and then\nruns the graph until the model produces output matching the `output_type`, for example text or structured data.\nAt this point, a streaming run result object is yielded from which you can stream the output as it comes in,\nand -- once this output has completed streaming -- get the complete output, message history, and usage.\n\nAs this method will consider the first output matching the `output_type` to be the final output,\nit will stop running the agent graph and will not execute any tool calls made by the model after this \"final\" output.\nIf you want to always run the agent graph to completion and stream events and output at the same time,\nuse [`agent.run()`][pydantic_ai.agent.AbstractAgent.run] with an `event_stream_handler` or [`agent.iter()`][pydantic_ai.agent.AbstractAgent.iter] instead.\n\nExample:\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    async with agent.run_stream('What is the capital of the UK?') as response:\n        print(await response.get_output())\n        #> The capital of the UK is London.\n```\n\nArgs:\n    user_prompt: User input to start/continue the conversation.\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    builtin_tools: Optional additional builtin tools for this run.\n    event_stream_handler: Optional handler for events from the model's streaming response and the agent's execution of tools to use for this run.\n        It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager.\n        Note that it does _not_ receive any events after the final result is found.\n\nReturns:\n    The result of the run.", "url": "https://ai.pydantic.dev/api/run-stream/", "page": "pydantic_ai_slim/pydantic_ai/agent/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractAgent.run_stream_events", "anchor": "abstractagent-run_stream_events", "heading_level": 0, "md_text": "Run the agent with a user prompt in async mode and stream events from the run.\n\nThis is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] and\nuses the `event_stream_handler` kwarg to get a stream of events from the run.\n\nExample:\n```python\nfrom pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    events: list[AgentStreamEvent | AgentRunResultEvent] = []\n    async for event in agent.run_stream_events('What is the capital of France?'):\n        events.append(event)\n    print(events)\n    '''\n    [\n        PartStartEvent(index=0, part=TextPart(content='The capital of ')),\n        FinalResultEvent(tool_name=None, tool_call_id=None),\n        PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='France is Paris. ')),\n        PartEndEvent(\n            index=0, part=TextPart(content='The capital of France is Paris. ')\n        ),\n        AgentRunResultEvent(\n            result=AgentRunResult(output='The capital of France is Paris. ')\n        ),\n    ]\n    '''\n```\n\nArguments are the same as for [`self.run`][pydantic_ai.agent.AbstractAgent.run],\nexcept that `event_stream_handler` is now allowed.\n\nArgs:\n    user_prompt: User input to start/continue the conversation.\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    builtin_tools: Optional additional builtin tools for this run.\n\nReturns:\n    An async iterable of stream events `AgentStreamEvent` and finally a `AgentRunResultEvent` with the final\n    run result.", "url": "https://ai.pydantic.dev/api/run-stream-events/", "page": "pydantic_ai_slim/pydantic_ai/agent/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractAgent.iter", "anchor": "abstractagent-iter", "heading_level": 0, "md_text": "A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.\n\nThis method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an\n`AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are\nexecuted. This is the API to use if you want to consume the outputs coming from each LLM model response, or the\nstream of events coming from the execution of tools.\n\nThe `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,\nand the final result of the run once it has completed.\n\nFor more details, see the documentation of `AgentRun`.\n\nExample:\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    nodes = []\n    async with agent.iter('What is the capital of France?') as agent_run:\n        async for node in agent_run:\n            nodes.append(node)\n    print(nodes)\n    '''\n    [\n        UserPromptNode(\n            user_prompt='What is the capital of France?',\n            instructions_functions=[],\n            system_prompts=(),\n            system_prompt_functions=[],\n            system_prompt_dynamic_functions={},\n        ),\n        ModelRequestNode(\n            request=ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='What is the capital of France?',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            )\n        ),\n        CallToolsNode(\n            model_response=ModelResponse(\n                parts=[TextPart(content='The capital of France is Paris.')],\n                usage=RequestUsage(input_tokens=56, output_tokens=7),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            )\n        ),\n        End(data=FinalResult(output='The capital of France is Paris.')),\n    ]\n    '''\n    print(agent_run.result.output)\n    #> The capital of France is Paris.\n```\n\nArgs:\n    user_prompt: User input to start/continue the conversation.\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    builtin_tools: Optional additional builtin tools for this run.\n\nReturns:\n    The result of the run.", "url": "https://ai.pydantic.dev/api/iter/", "page": "pydantic_ai_slim/pydantic_ai/agent/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractAgent.override", "anchor": "abstractagent-override", "heading_level": 0, "md_text": "Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.\n\nThis is particularly useful when testing.\nYou can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).\n\nArgs:\n    name: The name to use instead of the name passed to the agent constructor and agent run.\n    deps: The dependencies to use instead of the dependencies passed to the agent run.\n    model: The model to use instead of the model passed to the agent run.\n    toolsets: The toolsets to use instead of the toolsets passed to the agent constructor and agent run.\n    tools: The tools to use instead of the tools registered with the agent.\n    instructions: The instructions to use instead of the instructions registered with the agent.", "url": "https://ai.pydantic.dev/api/override/", "page": "pydantic_ai_slim/pydantic_ai/agent/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractAgent._infer_name", "anchor": "abstractagent-_infer_name", "heading_level": 0, "md_text": "Infer the agent name from the call frame.\n\nRunUsage should be `self._infer_name(inspect.currentframe())`.", "url": "https://ai.pydantic.dev/api/-infer-name/", "page": "pydantic_ai_slim/pydantic_ai/agent/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractAgent.sequential_tool_calls", "anchor": "abstractagent-sequential_tool_calls", "heading_level": 0, "md_text": "Run tool calls sequentially during the context.", "url": "https://ai.pydantic.dev/api/sequential-tool-calls/", "page": "pydantic_ai_slim/pydantic_ai/agent/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractAgent.is_model_request_node", "anchor": "abstractagent-is_model_request_node", "heading_level": 0, "md_text": "Check if the node is a `ModelRequestNode`, narrowing the type if it is.\n\nThis method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.", "url": "https://ai.pydantic.dev/api/is-model-request-node/", "page": "pydantic_ai_slim/pydantic_ai/agent/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractAgent.is_call_tools_node", "anchor": "abstractagent-is_call_tools_node", "heading_level": 0, "md_text": "Check if the node is a `CallToolsNode`, narrowing the type if it is.\n\nThis method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.", "url": "https://ai.pydantic.dev/api/is-call-tools-node/", "page": "pydantic_ai_slim/pydantic_ai/agent/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractAgent.is_user_prompt_node", "anchor": "abstractagent-is_user_prompt_node", "heading_level": 0, "md_text": "Check if the node is a `UserPromptNode`, narrowing the type if it is.\n\nThis method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.", "url": "https://ai.pydantic.dev/api/is-user-prompt-node/", "page": "pydantic_ai_slim/pydantic_ai/agent/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractAgent.is_end_node", "anchor": "abstractagent-is_end_node", "heading_level": 0, "md_text": "Check if the node is a `End`, narrowing the type if it is.\n\nThis method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.", "url": "https://ai.pydantic.dev/api/is-end-node/", "page": "pydantic_ai_slim/pydantic_ai/agent/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractAgent.to_ag_ui", "anchor": "abstractagent-to_ag_ui", "heading_level": 0, "md_text": "Returns an ASGI application that handles every AG-UI request by running the agent.\n\nNote that the `deps` will be the same for each request, with the exception of the AG-UI state that's\ninjected into the `state` field of a `deps` object that implements the [`StateHandler`][pydantic_ai.ag_ui.StateHandler] protocol.\nTo provide different `deps` for each request (e.g. based on the authenticated user),\nuse [`pydantic_ai.ag_ui.run_ag_ui`][pydantic_ai.ag_ui.run_ag_ui] or\n[`pydantic_ai.ag_ui.handle_ag_ui_request`][pydantic_ai.ag_ui.handle_ag_ui_request] instead.\n\nExample:\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\napp = agent.to_ag_ui()\n```\n\nThe `app` is an ASGI application that can be used with any ASGI server.\n\nTo run the application, you can use the following command:\n\n```bash\nuvicorn app:app --host 0.0.0.0 --port 8000\n```\n\nSee [AG-UI docs](../ui/ag-ui.md) for more information.\n\nArgs:\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has\n        no output validators since output validators would expect an argument that matches the agent's\n        output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n\n    debug: Boolean indicating if debug tracebacks should be returned on errors.\n    routes: A list of routes to serve incoming HTTP and WebSocket requests.\n    middleware: A list of middleware to run for every request. A starlette application will always\n        automatically include two middleware classes. `ServerErrorMiddleware` is added as the very\n        outermost middleware, to handle any uncaught errors occurring anywhere in the entire stack.\n        `ExceptionMiddleware` is added as the very innermost middleware, to deal with handled\n        exception cases occurring in the routing or endpoints.\n    exception_handlers: A mapping of either integer status codes, or exception class types onto\n        callables which handle the exceptions. Exception handler callables should be of the form\n        `handler(request, exc) -> response` and may be either standard functions, or async functions.\n    on_startup: A list of callables to run on application startup. Startup handler callables do not\n        take any arguments, and may be either standard functions, or async functions.\n    on_shutdown: A list of callables to run on application shutdown. Shutdown handler callables do\n        not take any arguments, and may be either standard functions, or async functions.\n    lifespan: A lifespan context function, which can be used to perform startup and shutdown tasks.\n        This is a newer style that replaces the `on_startup` and `on_shutdown` handlers. Use one or\n        the other, not both.\n\nReturns:\n    An ASGI application for running Pydantic AI agents with AG-UI protocol support.", "url": "https://ai.pydantic.dev/api/to-ag-ui/", "page": "pydantic_ai_slim/pydantic_ai/agent/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractAgent.to_a2a", "anchor": "abstractagent-to_a2a", "heading_level": 0, "md_text": "Convert the agent to a FastA2A application.\n\nExample:\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\napp = agent.to_a2a()\n```\n\nThe `app` is an ASGI application that can be used with any ASGI server.\n\nTo run the application, you can use the following command:\n\n```bash\nuvicorn app:app --host 0.0.0.0 --port 8000\n```", "url": "https://ai.pydantic.dev/api/to-a2a/", "page": "pydantic_ai_slim/pydantic_ai/agent/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractAgent.to_cli", "anchor": "abstractagent-to_cli", "heading_level": 0, "md_text": "Run the agent in a CLI chat interface.\n\nArgs:\n    deps: The dependencies to pass to the agent.\n    prog_name: The name of the program to use for the CLI. Defaults to 'pydantic-ai'.\n    message_history: History of the conversation so far.\n\nExample:\n```python {title=\"agent_to_cli.py\" test=\"skip\"}\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o', instructions='You always respond in Italian.')\n\nasync def main():\n    await agent.to_cli()\n```", "url": "https://ai.pydantic.dev/api/to-cli/", "page": "pydantic_ai_slim/pydantic_ai/agent/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractAgent.to_cli_sync", "anchor": "abstractagent-to_cli_sync", "heading_level": 0, "md_text": "Run the agent in a CLI chat interface with the non-async interface.\n\nArgs:\n    deps: The dependencies to pass to the agent.\n    prog_name: The name of the program to use for the CLI. Defaults to 'pydantic-ai'.\n    message_history: History of the conversation so far.\n\n```python {title=\"agent_to_cli_sync.py\" test=\"skip\"}\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o', instructions='You always respond in Italian.')\nagent.to_cli_sync()\nagent.to_cli_sync(prog_name='assistant')\n```", "url": "https://ai.pydantic.dev/api/to-cli-sync/", "page": "pydantic_ai_slim/pydantic_ai/agent/abstract", "source_site": "pydantic_ai"}
{"title": "on_complete", "anchor": "on_complete", "heading_level": 0, "md_text": "Called when the stream has completed.\n\nThe model response will have been added to messages by now\nby `StreamedRunResult._marked_completed`.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/agent/abstract", "source_site": "pydantic_ai"}
{"title": "CerebrasProvider", "anchor": "cerebrasprovider", "heading_level": 0, "md_text": "Provider for Cerebras API.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/cerebras", "source_site": "pydantic_ai"}
{"title": "AzureProvider", "anchor": "azureprovider", "heading_level": 0, "md_text": "Provider for Azure OpenAI API.\n\nSee <https://azure.microsoft.com/en-us/products/ai-foundry> for more information.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/azure", "source_site": "pydantic_ai"}
{"title": "AzureProvider.__init__", "anchor": "azureprovider-__init__", "heading_level": 0, "md_text": "Create a new Azure provider.\n\nArgs:\n    azure_endpoint: The Azure endpoint to use for authentication, if not provided, the `AZURE_OPENAI_ENDPOINT`\n        environment variable will be used if available.\n    api_version: The API version to use for authentication, if not provided, the `OPENAI_API_VERSION`\n        environment variable will be used if available.\n    api_key: The API key to use for authentication, if not provided, the `AZURE_OPENAI_API_KEY` environment variable\n        will be used if available.\n    openai_client: An existing\n        [`AsyncAzureOpenAI`](https://github.com/openai/openai-python#microsoft-azure-openai)\n        client to use. If provided, `base_url`, `api_key`, and `http_client` must be `None`.\n    http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/providers/azure", "source_site": "pydantic_ai"}
{"title": "OpenAIProvider", "anchor": "openaiprovider", "heading_level": 0, "md_text": "Provider for OpenAI API.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIProvider.__init__", "anchor": "openaiprovider-__init__", "heading_level": 0, "md_text": "Create a new OpenAI provider.\n\nArgs:\n    base_url: The base url for the OpenAI requests. If not provided, the `OPENAI_BASE_URL` environment variable\n        will be used if available. Otherwise, defaults to OpenAI's base url.\n    api_key: The API key to use for authentication, if not provided, the `OPENAI_API_KEY` environment variable\n        will be used if available.\n    openai_client: An existing\n        [`AsyncOpenAI`](https://github.com/openai/openai-python?tab=readme-ov-file#async-usage)\n        client to use. If provided, `base_url`, `api_key`, and `http_client` must be `None`.\n    http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/providers/openai", "source_site": "pydantic_ai"}
{"title": "gateway", "anchor": "gateway", "heading_level": 0, "md_text": "This module implements the Pydantic AI Gateway provider.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/gateway", "source_site": "pydantic_ai"}
{"title": "gateway_provider", "anchor": "gateway_provider", "heading_level": 0, "md_text": "Create a new Gateway provider.\n\nArgs:\n    upstream_provider: The upstream provider to use.\n    api_key: The API key to use for authentication. If not provided, the `PYDANTIC_AI_GATEWAY_API_KEY`\n        environment variable will be used if available.\n    base_url: The base URL to use for the Gateway. If not provided, the `PYDANTIC_AI_GATEWAY_BASE_URL`\n        environment variable will be used if available. Otherwise, defaults to `https://gateway.pydantic.dev/proxy`.\n    http_client: The HTTP client to use for the Gateway.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/gateway", "source_site": "pydantic_ai"}
{"title": "_request_hook", "anchor": "_request_hook", "heading_level": 0, "md_text": "Request hook for the gateway provider.\n\nIt adds the `\"traceparent\"` and `\"Authorization\"` headers to the request.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/gateway", "source_site": "pydantic_ai"}
{"title": "_merge_url_path", "anchor": "_merge_url_path", "heading_level": 0, "md_text": "Merge a base URL and a path.\n\nArgs:\n    base_url: The base URL to merge.\n    path: The path to merge.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/gateway", "source_site": "pydantic_ai"}
{"title": "MistralProvider", "anchor": "mistralprovider", "heading_level": 0, "md_text": "Provider for Mistral API.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/mistral", "source_site": "pydantic_ai"}
{"title": "MistralProvider.__init__", "anchor": "mistralprovider-__init__", "heading_level": 0, "md_text": "Create a new Mistral provider.\n\nArgs:\n    api_key: The API key to use for authentication, if not provided, the `MISTRAL_API_KEY` environment variable\n        will be used if available.\n    mistral_client: An existing `Mistral` client to use, if provided, `api_key` and `http_client` must be `None`.\n    base_url: The base url for the Mistral requests.\n    http_client: An existing async client to use for making HTTP requests.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/providers/mistral", "source_site": "pydantic_ai"}
{"title": "OVHcloudProvider", "anchor": "ovhcloudprovider", "heading_level": 0, "md_text": "Provider for OVHcloud AI Endpoints.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/ovhcloud", "source_site": "pydantic_ai"}
{"title": "HerokuProvider", "anchor": "herokuprovider", "heading_level": 0, "md_text": "Provider for Heroku API.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/heroku", "source_site": "pydantic_ai"}
{"title": "GitHubProvider", "anchor": "githubprovider", "heading_level": 0, "md_text": "Provider for GitHub Models API.\n\nGitHub Models provides access to various AI models through an OpenAI-compatible API.\nSee <https://docs.github.com/en/github-models> for more information.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/github", "source_site": "pydantic_ai"}
{"title": "GitHubProvider.__init__", "anchor": "githubprovider-__init__", "heading_level": 0, "md_text": "Create a new GitHub Models provider.\n\nArgs:\n    api_key: The GitHub token to use for authentication. If not provided, the `GITHUB_API_KEY`\n        environment variable will be used if available.\n    openai_client: An existing `AsyncOpenAI` client to use. If provided, `api_key` and `http_client` must be `None`.\n    http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/providers/github", "source_site": "pydantic_ai"}
{"title": "OllamaProvider", "anchor": "ollamaprovider", "heading_level": 0, "md_text": "Provider for local or remote Ollama API.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/ollama", "source_site": "pydantic_ai"}
{"title": "OllamaProvider.__init__", "anchor": "ollamaprovider-__init__", "heading_level": 0, "md_text": "Create a new Ollama provider.\n\nArgs:\n    base_url: The base url for the Ollama requests. If not provided, the `OLLAMA_BASE_URL` environment variable\n        will be used if available.\n    api_key: The API key to use for authentication, if not provided, the `OLLAMA_API_KEY` environment variable\n        will be used if available.\n    openai_client: An existing\n        [`AsyncOpenAI`](https://github.com/openai/openai-python?tab=readme-ov-file#async-usage)\n        client to use. If provided, `base_url`, `api_key`, and `http_client` must be `None`.\n    http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/providers/ollama", "source_site": "pydantic_ai"}
{"title": "OutlinesProvider", "anchor": "outlinesprovider", "heading_level": 0, "md_text": "Provider for Outlines API.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesProvider.name", "anchor": "outlinesprovider-name", "heading_level": 0, "md_text": "The provider name.", "url": "https://ai.pydantic.dev/api/name/", "page": "pydantic_ai_slim/pydantic_ai/providers/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesProvider.base_url", "anchor": "outlinesprovider-base_url", "heading_level": 0, "md_text": "The base URL for the provider API.", "url": "https://ai.pydantic.dev/api/base-url/", "page": "pydantic_ai_slim/pydantic_ai/providers/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesProvider.client", "anchor": "outlinesprovider-client", "heading_level": 0, "md_text": "The client for the provider.", "url": "https://ai.pydantic.dev/api/client/", "page": "pydantic_ai_slim/pydantic_ai/providers/outlines", "source_site": "pydantic_ai"}
{"title": "OutlinesProvider.model_profile", "anchor": "outlinesprovider-model_profile", "heading_level": 0, "md_text": "The model profile for the named model, if available.", "url": "https://ai.pydantic.dev/api/model-profile/", "page": "pydantic_ai_slim/pydantic_ai/providers/outlines", "source_site": "pydantic_ai"}
{"title": "HuggingFaceProvider", "anchor": "huggingfaceprovider", "heading_level": 0, "md_text": "Provider for Hugging Face.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/huggingface", "source_site": "pydantic_ai"}
{"title": "HuggingFaceProvider.__init__", "anchor": "huggingfaceprovider-__init__", "heading_level": 0, "md_text": "Create a new Hugging Face provider.\n\nArgs:\n    base_url: The base url for the Hugging Face requests.\n    api_key: The API key to use for authentication, if not provided, the `HF_TOKEN` environment variable\n        will be used if available.\n    hf_client: An existing\n        [`AsyncInferenceClient`](https://huggingface.co/docs/huggingface_hub/v0.29.3/en/package_reference/inference_client#huggingface_hub.AsyncInferenceClient)\n        client to use. If not provided, a new instance will be created.\n    http_client: (currently ignored) An existing `httpx.AsyncClient` to use for making HTTP requests.\n    provider_name : Name of the provider to use for inference. available providers can be found in the [HF Inference Providers documentation](https://huggingface.co/docs/inference-providers/index#partners).\n        defaults to \"auto\", which will select the first available provider for the model, the first of the providers available for the model, sorted by the user's order in https://hf.co/settings/inference-providers.\n        If `base_url` is passed, then `provider_name` is not used.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/providers/huggingface", "source_site": "pydantic_ai"}
{"title": "FireworksProvider", "anchor": "fireworksprovider", "heading_level": 0, "md_text": "Provider for Fireworks AI API.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/fireworks", "source_site": "pydantic_ai"}
{"title": "groq_moonshotai_model_profile", "anchor": "groq_moonshotai_model_profile", "heading_level": 0, "md_text": "Get the model profile for an MoonshotAI model used with the Groq provider.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/groq", "source_site": "pydantic_ai"}
{"title": "meta_groq_model_profile", "anchor": "meta_groq_model_profile", "heading_level": 0, "md_text": "Get the model profile for a Meta model used with the Groq provider.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/groq", "source_site": "pydantic_ai"}
{"title": "GroqProvider", "anchor": "groqprovider", "heading_level": 0, "md_text": "Provider for Groq API.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/groq", "source_site": "pydantic_ai"}
{"title": "GroqProvider.__init__", "anchor": "groqprovider-__init__", "heading_level": 0, "md_text": "Create a new Groq provider.\n\nArgs:\n    api_key: The API key to use for authentication, if not provided, the `GROQ_API_KEY` environment variable\n        will be used if available.\n    base_url: The base url for the Groq requests. If not provided, the `GROQ_BASE_URL` environment variable\n        will be used if available. Otherwise, defaults to Groq's base url.\n    groq_client: An existing\n        [`AsyncGroq`](https://github.com/groq/groq-python?tab=readme-ov-file#async-usage)\n        client to use. If provided, `api_key` and `http_client` must be `None`.\n    http_client: An existing `AsyncHTTPClient` to use for making HTTP requests.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/providers/groq", "source_site": "pydantic_ai"}
{"title": "GrokProvider", "anchor": "grokprovider", "heading_level": 0, "md_text": "Provider for Grok API.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/grok", "source_site": "pydantic_ai"}
{"title": "VercelProvider", "anchor": "vercelprovider", "heading_level": 0, "md_text": "Provider for Vercel AI Gateway API.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/vercel", "source_site": "pydantic_ai"}
{"title": "TogetherProvider", "anchor": "togetherprovider", "heading_level": 0, "md_text": "Provider for Together AI API.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/together", "source_site": "pydantic_ai"}
{"title": "GoogleVertexProvider", "anchor": "googlevertexprovider", "heading_level": 0, "md_text": "Provider for Vertex AI API.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/google_vertex", "source_site": "pydantic_ai"}
{"title": "_VertexAIAuth", "anchor": "_vertexaiauth", "heading_level": 0, "md_text": "Auth class for Vertex AI API.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/google_vertex", "source_site": "pydantic_ai"}
{"title": "GoogleVertexProvider.__init__", "anchor": "googlevertexprovider-__init__", "heading_level": 0, "md_text": "Create a new Vertex AI provider.\n\nArgs:\n    service_account_file: Path to a service account file.\n        If not provided, the service_account_info or default environment credentials will be used.\n    service_account_info: The loaded service_account_file contents.\n        If not provided, the service_account_file or default environment credentials will be used.\n    project_id: The project ID to use, if not provided it will be taken from the credentials.\n    region: The region to make requests to.\n    model_publisher: The model publisher to use, I couldn't find a good list of available publishers,\n        and from trial and error it seems non-google models don't work with the `generateContent` and\n        `streamGenerateContent` functions, hence only `google` is currently supported.\n        Please create an issue or PR if you know how to use other publishers.\n    http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/providers/google_vertex", "source_site": "pydantic_ai"}
{"title": "GoogleGLAProvider", "anchor": "googleglaprovider", "heading_level": 0, "md_text": "Provider for Google Generative Language AI API.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/google_gla", "source_site": "pydantic_ai"}
{"title": "GoogleGLAProvider.__init__", "anchor": "googleglaprovider-__init__", "heading_level": 0, "md_text": "Create a new Google GLA provider.\n\nArgs:\n    api_key: The API key to use for authentication, if not provided, the `GEMINI_API_KEY` environment variable\n        will be used if available.\n    http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/providers/google_gla", "source_site": "pydantic_ai"}
{"title": "__init__", "anchor": "__init__", "heading_level": 0, "md_text": "Providers for the API clients.\n\nThe providers are in charge of providing an authenticated client to the API.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/__init__", "source_site": "pydantic_ai"}
{"title": "Provider", "anchor": "provider", "heading_level": 0, "md_text": "Abstract class for a provider.\n\nThe provider is in charge of providing an authenticated client to the API.\n\nEach provider only supports a specific interface. A interface can be supported by multiple providers.\n\nFor example, the `OpenAIChatModel` interface can be supported by the `OpenAIProvider` and the `DeepSeekProvider`.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/__init__", "source_site": "pydantic_ai"}
{"title": "infer_provider_class", "anchor": "infer_provider_class", "heading_level": 0, "md_text": "Infers the provider class from the provider name.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/__init__", "source_site": "pydantic_ai"}
{"title": "infer_provider", "anchor": "infer_provider", "heading_level": 0, "md_text": "Infer the provider from the provider name.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/__init__", "source_site": "pydantic_ai"}
{"title": "Provider.name", "anchor": "provider-name", "heading_level": 0, "md_text": "The provider name.", "url": "https://ai.pydantic.dev/api/name/", "page": "pydantic_ai_slim/pydantic_ai/providers/__init__", "source_site": "pydantic_ai"}
{"title": "Provider.base_url", "anchor": "provider-base_url", "heading_level": 0, "md_text": "The base URL for the provider API.", "url": "https://ai.pydantic.dev/api/base-url/", "page": "pydantic_ai_slim/pydantic_ai/providers/__init__", "source_site": "pydantic_ai"}
{"title": "Provider.client", "anchor": "provider-client", "heading_level": 0, "md_text": "The client for the provider.", "url": "https://ai.pydantic.dev/api/client/", "page": "pydantic_ai_slim/pydantic_ai/providers/__init__", "source_site": "pydantic_ai"}
{"title": "Provider.model_profile", "anchor": "provider-model_profile", "heading_level": 0, "md_text": "The model profile for the named model, if available.", "url": "https://ai.pydantic.dev/api/model-profile/", "page": "pydantic_ai_slim/pydantic_ai/providers/__init__", "source_site": "pydantic_ai"}
{"title": "AnthropicProvider", "anchor": "anthropicprovider", "heading_level": 0, "md_text": "Provider for Anthropic API.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/anthropic", "source_site": "pydantic_ai"}
{"title": "AnthropicProvider.__init__", "anchor": "anthropicprovider-__init__", "heading_level": 0, "md_text": "Create a new Anthropic provider.\n\nArgs:\n    api_key: The API key to use for authentication, if not provided, the `ANTHROPIC_API_KEY` environment variable\n        will be used if available.\n    base_url: The base URL to use for the Anthropic API.\n    anthropic_client: An existing [`AsyncAnthropic`](https://github.com/anthropics/anthropic-sdk-python)\n        client to use. If provided, the `api_key` and `http_client` arguments will be ignored.\n    http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/providers/anthropic", "source_site": "pydantic_ai"}
{"title": "CohereProvider", "anchor": "cohereprovider", "heading_level": 0, "md_text": "Provider for Cohere API.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/cohere", "source_site": "pydantic_ai"}
{"title": "CohereProvider.__init__", "anchor": "cohereprovider-__init__", "heading_level": 0, "md_text": "Create a new Cohere provider.\n\nArgs:\n    api_key: The API key to use for authentication, if not provided, the `CO_API_KEY` environment variable\n        will be used if available.\n    cohere_client: An existing\n        [AsyncClientV2](https://github.com/cohere-ai/cohere-python)\n        client to use. If provided, `api_key` and `http_client` must be `None`.\n    http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/providers/cohere", "source_site": "pydantic_ai"}
{"title": "OpenRouterProvider", "anchor": "openrouterprovider", "heading_level": 0, "md_text": "Provider for OpenRouter API.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/openrouter", "source_site": "pydantic_ai"}
{"title": "LiteLLMProvider", "anchor": "litellmprovider", "heading_level": 0, "md_text": "Provider for LiteLLM API.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/litellm", "source_site": "pydantic_ai"}
{"title": "LiteLLMProvider.__init__", "anchor": "litellmprovider-__init__", "heading_level": 0, "md_text": "Initialize a LiteLLM provider.\n\nArgs:\n    api_key: API key for the model provider. If None, LiteLLM will try to get it from environment variables.\n    api_base: Base URL for the model provider. Use this for custom endpoints or self-hosted models.\n    openai_client: Pre-configured OpenAI client. If provided, other parameters are ignored.\n    http_client: Custom HTTP client to use.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/providers/litellm", "source_site": "pydantic_ai"}
{"title": "MoonshotAIProvider", "anchor": "moonshotaiprovider", "heading_level": 0, "md_text": "Provider for MoonshotAI platform (Kimi models).", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/moonshotai", "source_site": "pydantic_ai"}
{"title": "DeepSeekProvider", "anchor": "deepseekprovider", "heading_level": 0, "md_text": "Provider for DeepSeek API.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/deepseek", "source_site": "pydantic_ai"}
{"title": "BedrockModelProfile", "anchor": "bedrockmodelprofile", "heading_level": 0, "md_text": "Profile for models used with BedrockModel.\n\nALL FIELDS MUST BE `bedrock_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/bedrock", "source_site": "pydantic_ai"}
{"title": "bedrock_amazon_model_profile", "anchor": "bedrock_amazon_model_profile", "heading_level": 0, "md_text": "Get the model profile for an Amazon model used via Bedrock.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/bedrock", "source_site": "pydantic_ai"}
{"title": "bedrock_deepseek_model_profile", "anchor": "bedrock_deepseek_model_profile", "heading_level": 0, "md_text": "Get the model profile for a DeepSeek model used via Bedrock.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/bedrock", "source_site": "pydantic_ai"}
{"title": "BedrockProvider", "anchor": "bedrockprovider", "heading_level": 0, "md_text": "Provider for AWS Bedrock.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/bedrock", "source_site": "pydantic_ai"}
{"title": "BedrockProvider.__init__", "anchor": "bedrockprovider-__init__", "heading_level": 0, "md_text": "Initialize the Bedrock provider.\n\nArgs:\n    bedrock_client: A boto3 client for Bedrock Runtime. If provided, other arguments are ignored.\n    aws_access_key_id: The AWS access key ID. If not set, the `AWS_ACCESS_KEY_ID` environment variable will be used if available.\n    aws_secret_access_key: The AWS secret access key. If not set, the `AWS_SECRET_ACCESS_KEY` environment variable will be used if available.\n    aws_session_token: The AWS session token. If not set, the `AWS_SESSION_TOKEN` environment variable will be used if available.\n    api_key: The API key for Bedrock client. Can be used instead of `aws_access_key_id`, `aws_secret_access_key`, and `aws_session_token`. If not set, the `AWS_BEARER_TOKEN_BEDROCK` environment variable will be used if available.\n    base_url: The base URL for the Bedrock client.\n    region_name: The AWS region name. If not set, the `AWS_DEFAULT_REGION` environment variable will be used if available.\n    profile_name: The AWS profile name.\n    aws_read_timeout: The read timeout for Bedrock client.\n    aws_connect_timeout: The connect timeout for Bedrock client.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/providers/bedrock", "source_site": "pydantic_ai"}
{"title": "NebiusProvider", "anchor": "nebiusprovider", "heading_level": 0, "md_text": "Provider for Nebius AI Studio API.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/nebius", "source_site": "pydantic_ai"}
{"title": "GoogleProvider", "anchor": "googleprovider", "heading_level": 0, "md_text": "Provider for Google.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/providers/google", "source_site": "pydantic_ai"}
{"title": "GoogleProvider.__init__", "anchor": "googleprovider-__init__", "heading_level": 0, "md_text": "Create a new Google provider.\n\nArgs:\n    api_key: The `API key <https://ai.google.dev/gemini-api/docs/api-key>`_ to\n        use for authentication. It can also be set via the `GOOGLE_API_KEY` environment variable.\n    credentials: The credentials to use for authentication when calling the Vertex AI APIs. Credentials can be\n        obtained from environment variables and default credentials. For more information, see Set up\n        Application Default Credentials. Applies to the Vertex AI API only.\n    project: The Google Cloud project ID to use for quota. Can be obtained from environment variables\n        (for example, GOOGLE_CLOUD_PROJECT). Applies to the Vertex AI API only.\n    location: The location to send API requests to (for example, us-central1). Can be obtained from environment variables.\n        Applies to the Vertex AI API only.\n    vertexai: Force the use of the Vertex AI API. If `False`, the Google Generative Language API will be used.\n        Defaults to `False` unless `location`, `project`, or `credentials` are provided.\n    client: A pre-initialized client to use.\n    http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.\n    base_url: The base URL for the Google API.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/providers/google", "source_site": "pydantic_ai"}
{"title": "tool_from_langchain", "anchor": "tool_from_langchain", "heading_level": 0, "md_text": "Creates a Pydantic AI tool proxy from a LangChain tool.\n\nArgs:\n    langchain_tool: The LangChain tool to wrap.\n\nReturns:\n    A Pydantic AI tool that corresponds to the LangChain tool.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ext/langchain", "source_site": "pydantic_ai"}
{"title": "LangChainToolset", "anchor": "langchaintoolset", "heading_level": 0, "md_text": "A toolset that wraps LangChain tools.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ext/langchain", "source_site": "pydantic_ai"}
{"title": "tool_from_aci", "anchor": "tool_from_aci", "heading_level": 0, "md_text": "Creates a Pydantic AI tool proxy from an ACI.dev function.\n\nArgs:\n    aci_function: The ACI.dev function to wrap.\n    linked_account_owner_id: The ACI user ID to execute the function on behalf of.\n\nReturns:\n    A Pydantic AI tool that corresponds to the ACI.dev tool.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ext/aci", "source_site": "pydantic_ai"}
{"title": "ACIToolset", "anchor": "acitoolset", "heading_level": 0, "md_text": "A toolset that wraps ACI.dev tools.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ext/aci", "source_site": "pydantic_ai"}
{"title": "UIEventStream", "anchor": "uieventstream", "heading_level": 0, "md_text": "Base class for UI event stream transformers.\n\nThis class is responsible for transforming Pydantic AI events into protocol-specific events.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.new_message_id", "anchor": "uieventstream-new_message_id", "heading_level": 0, "md_text": "Generate and store a new message ID.", "url": "https://ai.pydantic.dev/api/new-message-id/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.response_headers", "anchor": "uieventstream-response_headers", "heading_level": 0, "md_text": "Response headers to return to the frontend.", "url": "https://ai.pydantic.dev/api/response-headers/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.content_type", "anchor": "uieventstream-content_type", "heading_level": 0, "md_text": "Get the content type for the event stream, compatible with the `Accept` header value.\n\nBy default, this returns the Server-Sent Events content type (`text/event-stream`).\nIf a subclass supports other types as well, it should consider `self.accept` in [`encode_event()`][pydantic_ai.ui.UIEventStream.encode_event] and return the resulting content type.", "url": "https://ai.pydantic.dev/api/content-type/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.encode_event", "anchor": "uieventstream-encode_event", "heading_level": 0, "md_text": "Encode a protocol-specific event as a string.", "url": "https://ai.pydantic.dev/api/encode-event/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.encode_stream", "anchor": "uieventstream-encode_stream", "heading_level": 0, "md_text": "Encode a stream of protocol-specific events as strings according to the `Accept` header value.", "url": "https://ai.pydantic.dev/api/encode-stream/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.streaming_response", "anchor": "uieventstream-streaming_response", "heading_level": 0, "md_text": "Generate a streaming response from a stream of protocol-specific events.", "url": "https://ai.pydantic.dev/api/streaming-response/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.transform_stream", "anchor": "uieventstream-transform_stream", "heading_level": 0, "md_text": "Transform a stream of Pydantic AI events into protocol-specific events.\n\nThis method dispatches to specific hooks and `handle_*` methods that subclasses can override:\n- [`before_stream()`][pydantic_ai.ui.UIEventStream.before_stream]\n- [`after_stream()`][pydantic_ai.ui.UIEventStream.after_stream]\n- [`on_error()`][pydantic_ai.ui.UIEventStream.on_error]\n- [`before_request()`][pydantic_ai.ui.UIEventStream.before_request]\n- [`after_request()`][pydantic_ai.ui.UIEventStream.after_request]\n- [`before_response()`][pydantic_ai.ui.UIEventStream.before_response]\n- [`after_response()`][pydantic_ai.ui.UIEventStream.after_response]\n- [`handle_event()`][pydantic_ai.ui.UIEventStream.handle_event]\n\nArgs:\n    stream: The stream of Pydantic AI events to transform.\n    on_complete: Optional callback function called when the agent run completes successfully.\n        The callback receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can optionally yield additional protocol-specific events.", "url": "https://ai.pydantic.dev/api/transform-stream/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream._turn_to", "anchor": "uieventstream-_turn_to", "heading_level": 0, "md_text": "Fire hooks when turning from request to response or vice versa.", "url": "https://ai.pydantic.dev/api/-turn-to/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.handle_event", "anchor": "uieventstream-handle_event", "heading_level": 0, "md_text": "Transform a Pydantic AI event into one or more protocol-specific events.\n\nThis method dispatches to specific `handle_*` methods based on event type:\n\n- [`PartStartEvent`][pydantic_ai.messages.PartStartEvent] -> [`handle_part_start()`][pydantic_ai.ui.UIEventStream.handle_part_start]\n- [`PartDeltaEvent`][pydantic_ai.messages.PartDeltaEvent] -> `handle_part_delta`\n- [`PartEndEvent`][pydantic_ai.messages.PartEndEvent] -> `handle_part_end`\n- [`FinalResultEvent`][pydantic_ai.messages.FinalResultEvent] -> `handle_final_result`\n- [`FunctionToolCallEvent`][pydantic_ai.messages.FunctionToolCallEvent] -> `handle_function_tool_call`\n- [`FunctionToolResultEvent`][pydantic_ai.messages.FunctionToolResultEvent] -> `handle_function_tool_result`\n- [`AgentRunResultEvent`][pydantic_ai.run.AgentRunResultEvent] -> `handle_run_result`\n\nSubclasses are encouraged to override the individual `handle_*` methods rather than this one.\nIf you need specific behavior for all events, make sure you call the super method.", "url": "https://ai.pydantic.dev/api/handle-event/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.handle_part_start", "anchor": "uieventstream-handle_part_start", "heading_level": 0, "md_text": "Handle a `PartStartEvent`.\n\nThis method dispatches to specific `handle_*` methods based on part type:\n\n- [`TextPart`][pydantic_ai.messages.TextPart] -> [`handle_text_start()`][pydantic_ai.ui.UIEventStream.handle_text_start]\n- [`ThinkingPart`][pydantic_ai.messages.ThinkingPart] -> [`handle_thinking_start()`][pydantic_ai.ui.UIEventStream.handle_thinking_start]\n- [`ToolCallPart`][pydantic_ai.messages.ToolCallPart] -> [`handle_tool_call_start()`][pydantic_ai.ui.UIEventStream.handle_tool_call_start]\n- [`BuiltinToolCallPart`][pydantic_ai.messages.BuiltinToolCallPart] -> [`handle_builtin_tool_call_start()`][pydantic_ai.ui.UIEventStream.handle_builtin_tool_call_start]\n- [`BuiltinToolReturnPart`][pydantic_ai.messages.BuiltinToolReturnPart] -> [`handle_builtin_tool_return()`][pydantic_ai.ui.UIEventStream.handle_builtin_tool_return]\n- [`FilePart`][pydantic_ai.messages.FilePart] -> [`handle_file()`][pydantic_ai.ui.UIEventStream.handle_file]\n\nSubclasses are encouraged to override the individual `handle_*` methods rather than this one.\nIf you need specific behavior for all part start events, make sure you call the super method.\n\nArgs:\n    event: The part start event.", "url": "https://ai.pydantic.dev/api/handle-part-start/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.handle_part_delta", "anchor": "uieventstream-handle_part_delta", "heading_level": 0, "md_text": "Handle a PartDeltaEvent.\n\nThis method dispatches to specific `handle_*_delta` methods based on part delta type:\n\n- [`TextPartDelta`][pydantic_ai.messages.TextPartDelta] -> [`handle_text_delta()`][pydantic_ai.ui.UIEventStream.handle_text_delta]\n- [`ThinkingPartDelta`][pydantic_ai.messages.ThinkingPartDelta] -> [`handle_thinking_delta()`][pydantic_ai.ui.UIEventStream.handle_thinking_delta]\n- [`ToolCallPartDelta`][pydantic_ai.messages.ToolCallPartDelta] -> [`handle_tool_call_delta()`][pydantic_ai.ui.UIEventStream.handle_tool_call_delta]\n\nSubclasses are encouraged to override the individual `handle_*_delta` methods rather than this one.\nIf you need specific behavior for all part delta events, make sure you call the super method.\n\nArgs:\n    event: The PartDeltaEvent.", "url": "https://ai.pydantic.dev/api/handle-part-delta/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.handle_part_end", "anchor": "uieventstream-handle_part_end", "heading_level": 0, "md_text": "Handle a `PartEndEvent`.\n\nThis method dispatches to specific `handle_*_end` methods based on part type:\n\n- [`TextPart`][pydantic_ai.messages.TextPart] -> [`handle_text_end()`][pydantic_ai.ui.UIEventStream.handle_text_end]\n- [`ThinkingPart`][pydantic_ai.messages.ThinkingPart] -> [`handle_thinking_end()`][pydantic_ai.ui.UIEventStream.handle_thinking_end]\n- [`ToolCallPart`][pydantic_ai.messages.ToolCallPart] -> [`handle_tool_call_end()`][pydantic_ai.ui.UIEventStream.handle_tool_call_end]\n- [`BuiltinToolCallPart`][pydantic_ai.messages.BuiltinToolCallPart] -> [`handle_builtin_tool_call_end()`][pydantic_ai.ui.UIEventStream.handle_builtin_tool_call_end]\n\nSubclasses are encouraged to override the individual `handle_*_end` methods rather than this one.\nIf you need specific behavior for all part end events, make sure you call the super method.\n\nArgs:\n    event: The part end event.", "url": "https://ai.pydantic.dev/api/handle-part-end/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.before_stream", "anchor": "uieventstream-before_stream", "heading_level": 0, "md_text": "Yield events before agent streaming starts.\n\nThis hook is called before any agent events are processed.\nOverride this to inject custom events at the start of the stream.", "url": "https://ai.pydantic.dev/api/before-stream/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.after_stream", "anchor": "uieventstream-after_stream", "heading_level": 0, "md_text": "Yield events after agent streaming completes.\n\nThis hook is called after all agent events have been processed.\nOverride this to inject custom events at the end of the stream.", "url": "https://ai.pydantic.dev/api/after-stream/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.on_error", "anchor": "uieventstream-on_error", "heading_level": 0, "md_text": "Handle errors that occur during streaming.\n\nArgs:\n    error: The error that occurred during streaming.", "url": "https://ai.pydantic.dev/api/on-error/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.before_request", "anchor": "uieventstream-before_request", "heading_level": 0, "md_text": "Yield events before a model request is processed.\n\nOverride this to inject custom events at the start of the request.", "url": "https://ai.pydantic.dev/api/before-request/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.after_request", "anchor": "uieventstream-after_request", "heading_level": 0, "md_text": "Yield events after a model request is processed.\n\nOverride this to inject custom events at the end of the request.", "url": "https://ai.pydantic.dev/api/after-request/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.before_response", "anchor": "uieventstream-before_response", "heading_level": 0, "md_text": "Yield events before a model response is processed.\n\nOverride this to inject custom events at the start of the response.", "url": "https://ai.pydantic.dev/api/before-response/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.after_response", "anchor": "uieventstream-after_response", "heading_level": 0, "md_text": "Yield events after a model response is processed.\n\nOverride this to inject custom events at the end of the response.", "url": "https://ai.pydantic.dev/api/after-response/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.handle_text_start", "anchor": "uieventstream-handle_text_start", "heading_level": 0, "md_text": "Handle the start of a `TextPart`.\n\nArgs:\n    part: The text part.\n    follows_text: Whether the part is directly preceded by another text part. In this case, you may want to yield a \"text-delta\" event instead of a \"text-start\" event.", "url": "https://ai.pydantic.dev/api/handle-text-start/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.handle_text_delta", "anchor": "uieventstream-handle_text_delta", "heading_level": 0, "md_text": "Handle a `TextPartDelta`.\n\nArgs:\n    delta: The text part delta.", "url": "https://ai.pydantic.dev/api/handle-text-delta/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.handle_text_end", "anchor": "uieventstream-handle_text_end", "heading_level": 0, "md_text": "Handle the end of a `TextPart`.\n\nArgs:\n    part: The text part.\n    followed_by_text: Whether the part is directly followed by another text part. In this case, you may not want to yield a \"text-end\" event yet.", "url": "https://ai.pydantic.dev/api/handle-text-end/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.handle_thinking_start", "anchor": "uieventstream-handle_thinking_start", "heading_level": 0, "md_text": "Handle the start of a `ThinkingPart`.\n\nArgs:\n    part: The thinking part.\n    follows_thinking: Whether the part is directly preceded by another thinking part. In this case, you may want to yield a \"thinking-delta\" event instead of a \"thinking-start\" event.", "url": "https://ai.pydantic.dev/api/handle-thinking-start/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.handle_thinking_delta", "anchor": "uieventstream-handle_thinking_delta", "heading_level": 0, "md_text": "Handle a `ThinkingPartDelta`.\n\nArgs:\n    delta: The thinking part delta.", "url": "https://ai.pydantic.dev/api/handle-thinking-delta/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.handle_thinking_end", "anchor": "uieventstream-handle_thinking_end", "heading_level": 0, "md_text": "Handle the end of a `ThinkingPart`.\n\nArgs:\n    part: The thinking part.\n    followed_by_thinking: Whether the part is directly followed by another thinking part. In this case, you may not want to yield a \"thinking-end\" event yet.", "url": "https://ai.pydantic.dev/api/handle-thinking-end/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.handle_tool_call_start", "anchor": "uieventstream-handle_tool_call_start", "heading_level": 0, "md_text": "Handle the start of a `ToolCallPart`.\n\nArgs:\n    part: The tool call part.", "url": "https://ai.pydantic.dev/api/handle-tool-call-start/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.handle_tool_call_delta", "anchor": "uieventstream-handle_tool_call_delta", "heading_level": 0, "md_text": "Handle a `ToolCallPartDelta`.\n\nArgs:\n    delta: The tool call part delta.", "url": "https://ai.pydantic.dev/api/handle-tool-call-delta/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.handle_tool_call_end", "anchor": "uieventstream-handle_tool_call_end", "heading_level": 0, "md_text": "Handle the end of a `ToolCallPart`.\n\nArgs:\n    part: The tool call part.", "url": "https://ai.pydantic.dev/api/handle-tool-call-end/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.handle_builtin_tool_call_start", "anchor": "uieventstream-handle_builtin_tool_call_start", "heading_level": 0, "md_text": "Handle a `BuiltinToolCallPart` at start.\n\nArgs:\n    part: The builtin tool call part.", "url": "https://ai.pydantic.dev/api/handle-builtin-tool-call-start/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.handle_builtin_tool_call_end", "anchor": "uieventstream-handle_builtin_tool_call_end", "heading_level": 0, "md_text": "Handle the end of a `BuiltinToolCallPart`.\n\nArgs:\n    part: The builtin tool call part.", "url": "https://ai.pydantic.dev/api/handle-builtin-tool-call-end/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.handle_builtin_tool_return", "anchor": "uieventstream-handle_builtin_tool_return", "heading_level": 0, "md_text": "Handle a `BuiltinToolReturnPart`.\n\nArgs:\n    part: The builtin tool return part.", "url": "https://ai.pydantic.dev/api/handle-builtin-tool-return/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.handle_file", "anchor": "uieventstream-handle_file", "heading_level": 0, "md_text": "Handle a `FilePart`.\n\nArgs:\n    part: The file part.", "url": "https://ai.pydantic.dev/api/handle-file/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.handle_final_result", "anchor": "uieventstream-handle_final_result", "heading_level": 0, "md_text": "Handle a `FinalResultEvent`.\n\nArgs:\n    event: The final result event.", "url": "https://ai.pydantic.dev/api/handle-final-result/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.handle_function_tool_call", "anchor": "uieventstream-handle_function_tool_call", "heading_level": 0, "md_text": "Handle a `FunctionToolCallEvent`.\n\nArgs:\n    event: The function tool call event.", "url": "https://ai.pydantic.dev/api/handle-function-tool-call/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.handle_function_tool_result", "anchor": "uieventstream-handle_function_tool_result", "heading_level": 0, "md_text": "Handle a `FunctionToolResultEvent`.\n\nArgs:\n    event: The function tool result event.", "url": "https://ai.pydantic.dev/api/handle-function-tool-result/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "UIEventStream.handle_run_result", "anchor": "uieventstream-handle_run_result", "heading_level": 0, "md_text": "Handle an `AgentRunResultEvent`.\n\nArgs:\n    event: The agent run result event.", "url": "https://ai.pydantic.dev/api/handle-run-result/", "page": "pydantic_ai_slim/pydantic_ai/ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "StateHandler", "anchor": "statehandler", "heading_level": 0, "md_text": "Protocol for state handlers in agent runs. Requires the class to be a dataclass with a `state` field.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/_adapter", "source_site": "pydantic_ai"}
{"title": "StateDeps", "anchor": "statedeps", "heading_level": 0, "md_text": "Dependency type that holds state.\n\nThis class is used to manage the state of an agent run. It allows setting\nthe state of the agent run with a specific type of state model, which must\nbe a subclass of `BaseModel`.\n\nThe state is set using the `state` setter by the `Adapter` when the run starts.\n\nImplements the `StateHandler` protocol.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/_adapter", "source_site": "pydantic_ai"}
{"title": "UIAdapter", "anchor": "uiadapter", "heading_level": 0, "md_text": "Base class for UI adapters.\n\nThis class is responsible for transforming agent run input received from the frontend into arguments for [`Agent.run_stream_events()`][pydantic_ai.Agent.run_stream_events], running the agent, and then transforming Pydantic AI events into protocol-specific events.\n\nThe event stream transformation is handled by a protocol-specific [`UIEventStream`][pydantic_ai.ui.UIEventStream] subclass.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/_adapter", "source_site": "pydantic_ai"}
{"title": "StateHandler.state", "anchor": "statehandler-state", "heading_level": 0, "md_text": "Get the current state of the agent run.", "url": "https://ai.pydantic.dev/api/state/", "page": "pydantic_ai_slim/pydantic_ai/ui/_adapter", "source_site": "pydantic_ai"}
{"title": "StateHandler.state", "anchor": "statehandler-state", "heading_level": 0, "md_text": "Set the state of the agent run.\n\nThis method is called to update the state of the agent run with the\nprovided state.\n\nArgs:\n    state: The run state.", "url": "https://ai.pydantic.dev/api/state/", "page": "pydantic_ai_slim/pydantic_ai/ui/_adapter", "source_site": "pydantic_ai"}
{"title": "UIAdapter.from_request", "anchor": "uiadapter-from_request", "heading_level": 0, "md_text": "Create an adapter from a request.", "url": "https://ai.pydantic.dev/api/from-request/", "page": "pydantic_ai_slim/pydantic_ai/ui/_adapter", "source_site": "pydantic_ai"}
{"title": "UIAdapter.build_run_input", "anchor": "uiadapter-build_run_input", "heading_level": 0, "md_text": "Build a protocol-specific run input object from the request body.", "url": "https://ai.pydantic.dev/api/build-run-input/", "page": "pydantic_ai_slim/pydantic_ai/ui/_adapter", "source_site": "pydantic_ai"}
{"title": "UIAdapter.load_messages", "anchor": "uiadapter-load_messages", "heading_level": 0, "md_text": "Transform protocol-specific messages into Pydantic AI messages.", "url": "https://ai.pydantic.dev/api/load-messages/", "page": "pydantic_ai_slim/pydantic_ai/ui/_adapter", "source_site": "pydantic_ai"}
{"title": "UIAdapter.build_event_stream", "anchor": "uiadapter-build_event_stream", "heading_level": 0, "md_text": "Build a protocol-specific event stream transformer.", "url": "https://ai.pydantic.dev/api/build-event-stream/", "page": "pydantic_ai_slim/pydantic_ai/ui/_adapter", "source_site": "pydantic_ai"}
{"title": "UIAdapter.messages", "anchor": "uiadapter-messages", "heading_level": 0, "md_text": "Pydantic AI messages from the protocol-specific run input.", "url": "https://ai.pydantic.dev/api/messages/", "page": "pydantic_ai_slim/pydantic_ai/ui/_adapter", "source_site": "pydantic_ai"}
{"title": "UIAdapter.toolset", "anchor": "uiadapter-toolset", "heading_level": 0, "md_text": "Toolset representing frontend tools from the protocol-specific run input.", "url": "https://ai.pydantic.dev/api/toolset/", "page": "pydantic_ai_slim/pydantic_ai/ui/_adapter", "source_site": "pydantic_ai"}
{"title": "UIAdapter.state", "anchor": "uiadapter-state", "heading_level": 0, "md_text": "Frontend state from the protocol-specific run input.", "url": "https://ai.pydantic.dev/api/state/", "page": "pydantic_ai_slim/pydantic_ai/ui/_adapter", "source_site": "pydantic_ai"}
{"title": "UIAdapter.transform_stream", "anchor": "uiadapter-transform_stream", "heading_level": 0, "md_text": "Transform a stream of Pydantic AI events into protocol-specific events.\n\nArgs:\n    stream: The stream of Pydantic AI events to transform.\n    on_complete: Optional callback function called when the agent run completes successfully.\n        The callback receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can optionally yield additional protocol-specific events.", "url": "https://ai.pydantic.dev/api/transform-stream/", "page": "pydantic_ai_slim/pydantic_ai/ui/_adapter", "source_site": "pydantic_ai"}
{"title": "UIAdapter.encode_stream", "anchor": "uiadapter-encode_stream", "heading_level": 0, "md_text": "Encode a stream of protocol-specific events as strings according to the `Accept` header value.\n\nArgs:\n    stream: The stream of protocol-specific events to encode.", "url": "https://ai.pydantic.dev/api/encode-stream/", "page": "pydantic_ai_slim/pydantic_ai/ui/_adapter", "source_site": "pydantic_ai"}
{"title": "UIAdapter.streaming_response", "anchor": "uiadapter-streaming_response", "heading_level": 0, "md_text": "Generate a streaming response from a stream of protocol-specific events.\n\nArgs:\n    stream: The stream of protocol-specific events to encode.", "url": "https://ai.pydantic.dev/api/streaming-response/", "page": "pydantic_ai_slim/pydantic_ai/ui/_adapter", "source_site": "pydantic_ai"}
{"title": "UIAdapter.run_stream_native", "anchor": "uiadapter-run_stream_native", "heading_level": 0, "md_text": "Run the agent with the protocol-specific run input and stream Pydantic AI events.\n\nArgs:\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    builtin_tools: Optional additional builtin tools to use for this run.", "url": "https://ai.pydantic.dev/api/run-stream-native/", "page": "pydantic_ai_slim/pydantic_ai/ui/_adapter", "source_site": "pydantic_ai"}
{"title": "UIAdapter.run_stream", "anchor": "uiadapter-run_stream", "heading_level": 0, "md_text": "Run the agent with the protocol-specific run input and stream protocol-specific events.\n\nArgs:\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    builtin_tools: Optional additional builtin tools to use for this run.\n    on_complete: Optional callback function called when the agent run completes successfully.\n        The callback receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can optionally yield additional protocol-specific events.", "url": "https://ai.pydantic.dev/api/run-stream/", "page": "pydantic_ai_slim/pydantic_ai/ui/_adapter", "source_site": "pydantic_ai"}
{"title": "UIAdapter.dispatch_request", "anchor": "uiadapter-dispatch_request", "heading_level": 0, "md_text": "Handle a protocol-specific HTTP request by running the agent and returning a streaming response of protocol-specific events.\n\nArgs:\n    request: The incoming Starlette/FastAPI request.\n    agent: The agent to run.\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    builtin_tools: Optional additional builtin tools to use for this run.\n    on_complete: Optional callback function called when the agent run completes successfully.\n        The callback receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can optionally yield additional protocol-specific events.\n\nReturns:\n    A streaming Starlette response with protocol-specific events encoded per the request's `Accept` header value.", "url": "https://ai.pydantic.dev/api/dispatch-request/", "page": "pydantic_ai_slim/pydantic_ai/ui/_adapter", "source_site": "pydantic_ai"}
{"title": "MessagesBuilder", "anchor": "messagesbuilder", "heading_level": 0, "md_text": "Helper class to build Pydantic AI messages from request/response parts.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/_messages_builder", "source_site": "pydantic_ai"}
{"title": "MessagesBuilder.add", "anchor": "messagesbuilder-add", "heading_level": 0, "md_text": "Add a new part, creating a new request or response message if necessary.", "url": "https://ai.pydantic.dev/api/add/", "page": "pydantic_ai_slim/pydantic_ai/ui/_messages_builder", "source_site": "pydantic_ai"}
{"title": "DuckDuckGoResult", "anchor": "duckduckgoresult", "heading_level": 0, "md_text": "A DuckDuckGo search result.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/common_tools/duckduckgo", "source_site": "pydantic_ai"}
{"title": "DuckDuckGoSearchTool", "anchor": "duckduckgosearchtool", "heading_level": 0, "md_text": "The DuckDuckGo search tool.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/common_tools/duckduckgo", "source_site": "pydantic_ai"}
{"title": "duckduckgo_search_tool", "anchor": "duckduckgo_search_tool", "heading_level": 0, "md_text": "Creates a DuckDuckGo search tool.\n\nArgs:\n    duckduckgo_client: The DuckDuckGo search client.\n    max_results: The maximum number of results. If None, returns results only from the first response.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/common_tools/duckduckgo", "source_site": "pydantic_ai"}
{"title": "DuckDuckGoSearchTool.__call__", "anchor": "duckduckgosearchtool-__call__", "heading_level": 0, "md_text": "Searches DuckDuckGo for the given query and returns the results.\n\nArgs:\n    query: The query to search for.\n\nReturns:\n    The search results.", "url": "https://ai.pydantic.dev/api/--call--/", "page": "pydantic_ai_slim/pydantic_ai/common_tools/duckduckgo", "source_site": "pydantic_ai"}
{"title": "TavilySearchResult", "anchor": "tavilysearchresult", "heading_level": 0, "md_text": "A Tavily search result.\n\nSee [Tavily Search Endpoint documentation](https://docs.tavily.com/api-reference/endpoint/search)\nfor more information.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/common_tools/tavily", "source_site": "pydantic_ai"}
{"title": "TavilySearchTool", "anchor": "tavilysearchtool", "heading_level": 0, "md_text": "The Tavily search tool.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/common_tools/tavily", "source_site": "pydantic_ai"}
{"title": "tavily_search_tool", "anchor": "tavily_search_tool", "heading_level": 0, "md_text": "Creates a Tavily search tool.\n\nArgs:\n    api_key: The Tavily API key.\n\n        You can get one by signing up at [https://app.tavily.com/home](https://app.tavily.com/home).", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/common_tools/tavily", "source_site": "pydantic_ai"}
{"title": "TavilySearchTool.__call__", "anchor": "tavilysearchtool-__call__", "heading_level": 0, "md_text": "Searches Tavily for the given query and returns the results.\n\nArgs:\n    query: The search query to execute with Tavily.\n    search_deep: The depth of the search.\n    topic: The category of the search.\n    time_range: The time range back from the current date to filter results.\n\nReturns:\n    The search results.", "url": "https://ai.pydantic.dev/api/--call--/", "page": "pydantic_ai_slim/pydantic_ai/common_tools/tavily", "source_site": "pydantic_ai"}
{"title": "PrefixedToolset", "anchor": "prefixedtoolset", "heading_level": 0, "md_text": "A toolset that prefixes the names of the tools it contains.\n\nSee [toolset docs](../toolsets.md#prefixing-tool-names) for more information.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/prefixed", "source_site": "pydantic_ai"}
{"title": "DynamicToolset", "anchor": "dynamictoolset", "heading_level": 0, "md_text": "A toolset that dynamically builds a toolset using a function that takes the run context.\n\nIt should only be used during a single agent run as it stores the generated toolset.\nTo use it multiple times, copy it using `dataclasses.replace`.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/_dynamic", "source_site": "pydantic_ai"}
{"title": "FilteredToolset", "anchor": "filteredtoolset", "heading_level": 0, "md_text": "A toolset that filters the tools it contains using a filter function that takes the agent context and the tool definition.\n\nSee [toolset docs](../toolsets.md#filtering-tools) for more information.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/filtered", "source_site": "pydantic_ai"}
{"title": "FunctionToolsetTool", "anchor": "functiontoolsettool", "heading_level": 0, "md_text": "A tool definition for a function toolset tool that keeps track of the function to call.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/function", "source_site": "pydantic_ai"}
{"title": "FunctionToolset", "anchor": "functiontoolset", "heading_level": 0, "md_text": "A toolset that lets Python functions be used as tools.\n\nSee [toolset docs](../toolsets.md#function-toolset) for more information.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/function", "source_site": "pydantic_ai"}
{"title": "FunctionToolset.__init__", "anchor": "functiontoolset-__init__", "heading_level": 0, "md_text": "Build a new function toolset.\n\nArgs:\n    tools: The tools to add to the toolset.\n    max_retries: The maximum number of retries for each tool during a run.\n        Applies to all tools, unless overridden when adding a tool.\n    docstring_format: Format of tool docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].\n        Defaults to `'auto'`, such that the format is inferred from the structure of the docstring.\n        Applies to all tools, unless overridden when adding a tool.\n    require_parameter_descriptions: If True, raise an error if a parameter description is missing. Defaults to False.\n        Applies to all tools, unless overridden when adding a tool.\n    schema_generator: The JSON schema generator class to use for this tool. Defaults to `GenerateToolJsonSchema`.\n        Applies to all tools, unless overridden when adding a tool.\n    strict: Whether to enforce JSON schema compliance (only affects OpenAI).\n        See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.\n    sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.\n        Applies to all tools, unless overridden when adding a tool.\n    requires_approval: Whether this tool requires human-in-the-loop approval. Defaults to False.\n        See the [tools documentation](../deferred-tools.md#human-in-the-loop-tool-approval) for more info.\n        Applies to all tools, unless overridden when adding a tool.\n    metadata: Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization.\n        Applies to all tools, unless overridden when adding a tool, which will be merged with the toolset's metadata.\n    id: An optional unique ID for the toolset. A toolset needs to have an ID in order to be used in a durable execution environment like Temporal,\n        in which case the ID will be used to identify the toolset's activities within the workflow.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/function", "source_site": "pydantic_ai"}
{"title": "FunctionToolset.tool", "anchor": "functiontoolset-tool", "heading_level": 0, "md_text": "Decorator to register a tool function which takes [`RunContext`][pydantic_ai.tools.RunContext] as its first argument.\n\nCan decorate a sync or async functions.\n\nThe docstring is inspected to extract both the tool description and description of each parameter,\n[learn more](../tools.md#function-tools-and-schema).\n\nWe can't add overloads for every possible signature of tool, since the return type is a recursive union\nso the signature of functions decorated with `@toolset.tool` is obscured.\n\nExample:\n```python\nfrom pydantic_ai import Agent, FunctionToolset, RunContext\n\ntoolset = FunctionToolset()\n\n@toolset.tool\ndef foobar(ctx: RunContext[int], x: int) -> int:\n    return ctx.deps + x\n\n@toolset.tool(retries=2)\nasync def spam(ctx: RunContext[str], y: float) -> float:\n    return ctx.deps + y\n\nagent = Agent('test', toolsets=[toolset], deps_type=int)\nresult = agent.run_sync('foobar', deps=1)\nprint(result.output)\n#> {\"foobar\":1,\"spam\":1.0}\n```\n\nArgs:\n    func: The tool function to register.\n    name: The name of the tool, defaults to the function name.\n    description: The description of the tool,defaults to the function docstring.\n    retries: The number of retries to allow for this tool, defaults to the agent's default retries,\n        which defaults to 1.\n    prepare: custom method to prepare the tool definition for each step, return `None` to omit this\n        tool from a given step. This is useful if you want to customise a tool at call time,\n        or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].\n    docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].\n        If `None`, the default value is determined by the toolset.\n    require_parameter_descriptions: If True, raise an error if a parameter description is missing.\n        If `None`, the default value is determined by the toolset.\n    schema_generator: The JSON schema generator class to use for this tool.\n        If `None`, the default value is determined by the toolset.\n    strict: Whether to enforce JSON schema compliance (only affects OpenAI).\n        See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.\n        If `None`, the default value is determined by the toolset.\n    sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.\n        If `None`, the default value is determined by the toolset.\n    requires_approval: Whether this tool requires human-in-the-loop approval. Defaults to False.\n        See the [tools documentation](../deferred-tools.md#human-in-the-loop-tool-approval) for more info.\n        If `None`, the default value is determined by the toolset.\n    metadata: Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization.\n        If `None`, the default value is determined by the toolset. If provided, it will be merged with the toolset's metadata.", "url": "https://ai.pydantic.dev/api/tool/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/function", "source_site": "pydantic_ai"}
{"title": "FunctionToolset.add_function", "anchor": "functiontoolset-add_function", "heading_level": 0, "md_text": "Add a function as a tool to the toolset.\n\nCan take a sync or async function.\n\nThe docstring is inspected to extract both the tool description and description of each parameter,\n[learn more](../tools.md#function-tools-and-schema).\n\nArgs:\n    func: The tool function to register.\n    takes_ctx: Whether the function takes a [`RunContext`][pydantic_ai.tools.RunContext] as its first argument. If `None`, this is inferred from the function signature.\n    name: The name of the tool, defaults to the function name.\n    description: The description of the tool, defaults to the function docstring.\n    retries: The number of retries to allow for this tool, defaults to the agent's default retries,\n        which defaults to 1.\n    prepare: custom method to prepare the tool definition for each step, return `None` to omit this\n        tool from a given step. This is useful if you want to customise a tool at call time,\n        or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].\n    docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].\n        If `None`, the default value is determined by the toolset.\n    require_parameter_descriptions: If True, raise an error if a parameter description is missing.\n        If `None`, the default value is determined by the toolset.\n    schema_generator: The JSON schema generator class to use for this tool.\n        If `None`, the default value is determined by the toolset.\n    strict: Whether to enforce JSON schema compliance (only affects OpenAI).\n        See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.\n        If `None`, the default value is determined by the toolset.\n    sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.\n        If `None`, the default value is determined by the toolset.\n    requires_approval: Whether this tool requires human-in-the-loop approval. Defaults to False.\n        See the [tools documentation](../deferred-tools.md#human-in-the-loop-tool-approval) for more info.\n        If `None`, the default value is determined by the toolset.\n    metadata: Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization.\n        If `None`, the default value is determined by the toolset. If provided, it will be merged with the toolset's metadata.", "url": "https://ai.pydantic.dev/api/add-function/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/function", "source_site": "pydantic_ai"}
{"title": "FunctionToolset.add_tool", "anchor": "functiontoolset-add_tool", "heading_level": 0, "md_text": "Add a tool to the toolset.\n\nArgs:\n    tool: The tool to add.", "url": "https://ai.pydantic.dev/api/add-tool/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/function", "source_site": "pydantic_ai"}
{"title": "ApprovalRequiredToolset", "anchor": "approvalrequiredtoolset", "heading_level": 0, "md_text": "A toolset that requires (some) calls to tools it contains to be approved.\n\nSee [toolset docs](../toolsets.md#requiring-tool-approval) for more information.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/approval_required", "source_site": "pydantic_ai"}
{"title": "WrapperToolset", "anchor": "wrappertoolset", "heading_level": 0, "md_text": "A toolset that wraps another toolset and delegates to it.\n\nSee [toolset docs](../toolsets.md#wrapping-a-toolset) for more information.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/wrapper", "source_site": "pydantic_ai"}
{"title": "PreparedToolset", "anchor": "preparedtoolset", "heading_level": 0, "md_text": "A toolset that prepares the tools it contains using a prepare function that takes the agent context and the original tool definitions.\n\nSee [toolset docs](../toolsets.md#preparing-tool-definitions) for more information.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/prepared", "source_site": "pydantic_ai"}
{"title": "FastMCPToolset", "anchor": "fastmcptoolset", "heading_level": 0, "md_text": "A FastMCP Toolset that uses the FastMCP Client to call tools from a local or remote MCP Server.\n\nThe Toolset can accept a FastMCP Client, a FastMCP Transport, or any other object which a FastMCP Transport can be created from.\n\nSee https://gofastmcp.com/clients/transports for a full list of transports available.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/fastmcp", "source_site": "pydantic_ai"}
{"title": "_convert_mcp_tool_to_toolset_tool", "anchor": "_convert_mcp_tool_to_toolset_tool", "heading_level": 0, "md_text": "Convert an MCP tool to a toolset tool.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/fastmcp", "source_site": "pydantic_ai"}
{"title": "_map_fastmcp_tool_results", "anchor": "_map_fastmcp_tool_results", "heading_level": 0, "md_text": "Map FastMCP tool results to toolset tool results.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/fastmcp", "source_site": "pydantic_ai"}
{"title": "RenamedToolset", "anchor": "renamedtoolset", "heading_level": 0, "md_text": "A toolset that renames the tools it contains using a dictionary mapping new names to original names.\n\nSee [toolset docs](../toolsets.md#renaming-tools) for more information.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/renamed", "source_site": "pydantic_ai"}
{"title": "_CombinedToolsetTool", "anchor": "_combinedtoolsettool", "heading_level": 0, "md_text": "A tool definition for a combined toolset tools that keeps track of the source toolset and tool.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/combined", "source_site": "pydantic_ai"}
{"title": "CombinedToolset", "anchor": "combinedtoolset", "heading_level": 0, "md_text": "A toolset that combines multiple toolsets.\n\nSee [toolset docs](../toolsets.md#combining-toolsets) for more information.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/combined", "source_site": "pydantic_ai"}
{"title": "ExternalToolset", "anchor": "externaltoolset", "heading_level": 0, "md_text": "A toolset that holds tools whose results will be produced outside of the Pydantic AI agent run in which they were called.\n\nSee [toolset docs](../toolsets.md#external-toolset) for more information.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/external", "source_site": "pydantic_ai"}
{"title": "DeferredToolset", "anchor": "deferredtoolset", "heading_level": 0, "md_text": "Deprecated alias for `ExternalToolset`.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/external", "source_site": "pydantic_ai"}
{"title": "SchemaValidatorProt", "anchor": "schemavalidatorprot", "heading_level": 0, "md_text": "Protocol for a Pydantic Core `SchemaValidator` or `PluggableSchemaValidator` (which is private but API-compatible).", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/abstract", "source_site": "pydantic_ai"}
{"title": "ToolsetTool", "anchor": "toolsettool", "heading_level": 0, "md_text": "Definition of a tool available on a toolset.\n\nThis is a wrapper around a plain tool definition that includes information about:\n\n- the toolset that provided it, for use in error messages\n- the maximum number of retries to attempt if the tool call fails\n- the validator for the tool's arguments", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractToolset", "anchor": "abstracttoolset", "heading_level": 0, "md_text": "A toolset is a collection of tools that can be used by an agent.\n\nIt is responsible for:\n\n- Listing the tools it contains\n- Validating the arguments of the tools\n- Calling the tools\n\nSee [toolset docs](../toolsets.md) for more information.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractToolset.id", "anchor": "abstracttoolset-id", "heading_level": 0, "md_text": "An ID for the toolset that is unique among all toolsets registered with the same agent.\n\nIf you're implementing a concrete implementation that users can instantiate more than once, you should let them optionally pass a custom ID to the constructor and return that here.\n\nA toolset needs to have an ID in order to be used in a durable execution environment like Temporal, in which case the ID will be used to identify the toolset's activities within the workflow.", "url": "https://ai.pydantic.dev/api/id/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractToolset.label", "anchor": "abstracttoolset-label", "heading_level": 0, "md_text": "The name of the toolset for use in error messages.", "url": "https://ai.pydantic.dev/api/label/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractToolset.tool_name_conflict_hint", "anchor": "abstracttoolset-tool_name_conflict_hint", "heading_level": 0, "md_text": "A hint for how to avoid name conflicts with other toolsets for use in error messages.", "url": "https://ai.pydantic.dev/api/tool-name-conflict-hint/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractToolset.__aenter__", "anchor": "abstracttoolset-__aenter__", "heading_level": 0, "md_text": "Enter the toolset context.\n\nThis is where you can set up network connections in a concrete implementation.", "url": "https://ai.pydantic.dev/api/--aenter--/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractToolset.__aexit__", "anchor": "abstracttoolset-__aexit__", "heading_level": 0, "md_text": "Exit the toolset context.\n\nThis is where you can tear down network connections in a concrete implementation.", "url": "https://ai.pydantic.dev/api/--aexit--/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractToolset.get_tools", "anchor": "abstracttoolset-get_tools", "heading_level": 0, "md_text": "The tools that are available in this toolset.", "url": "https://ai.pydantic.dev/api/get-tools/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractToolset.call_tool", "anchor": "abstracttoolset-call_tool", "heading_level": 0, "md_text": "Call a tool with the given arguments.\n\nArgs:\n    name: The name of the tool to call.\n    tool_args: The arguments to pass to the tool.\n    ctx: The run context.\n    tool: The tool definition returned by [`get_tools`][pydantic_ai.toolsets.AbstractToolset.get_tools] that was called.", "url": "https://ai.pydantic.dev/api/call-tool/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractToolset.apply", "anchor": "abstracttoolset-apply", "heading_level": 0, "md_text": "Run a visitor function on all \"leaf\" toolsets (i.e. those that implement their own tool listing and calling).", "url": "https://ai.pydantic.dev/api/apply/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractToolset.visit_and_replace", "anchor": "abstracttoolset-visit_and_replace", "heading_level": 0, "md_text": "Run a visitor function on all \"leaf\" toolsets (i.e. those that implement their own tool listing and calling) and replace them in the hierarchy with the result of the function.", "url": "https://ai.pydantic.dev/api/visit-and-replace/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractToolset.filtered", "anchor": "abstracttoolset-filtered", "heading_level": 0, "md_text": "Returns a new toolset that filters this toolset's tools using a filter function that takes the agent context and the tool definition.\n\nSee [toolset docs](../toolsets.md#filtering-tools) for more information.", "url": "https://ai.pydantic.dev/api/filtered/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractToolset.prefixed", "anchor": "abstracttoolset-prefixed", "heading_level": 0, "md_text": "Returns a new toolset that prefixes the names of this toolset's tools.\n\nSee [toolset docs](../toolsets.md#prefixing-tool-names) for more information.", "url": "https://ai.pydantic.dev/api/prefixed/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractToolset.prepared", "anchor": "abstracttoolset-prepared", "heading_level": 0, "md_text": "Returns a new toolset that prepares this toolset's tools using a prepare function that takes the agent context and the original tool definitions.\n\nSee [toolset docs](../toolsets.md#preparing-tool-definitions) for more information.", "url": "https://ai.pydantic.dev/api/prepared/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractToolset.renamed", "anchor": "abstracttoolset-renamed", "heading_level": 0, "md_text": "Returns a new toolset that renames this toolset's tools using a dictionary mapping new names to original names.\n\nSee [toolset docs](../toolsets.md#renaming-tools) for more information.", "url": "https://ai.pydantic.dev/api/renamed/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/abstract", "source_site": "pydantic_ai"}
{"title": "AbstractToolset.approval_required", "anchor": "abstracttoolset-approval_required", "heading_level": 0, "md_text": "Returns a new toolset that requires (some) calls to tools it contains to be approved.\n\nSee [toolset docs](../toolsets.md#requiring-tool-approval) for more information.", "url": "https://ai.pydantic.dev/api/approval-required/", "page": "pydantic_ai_slim/pydantic_ai/toolsets/abstract", "source_site": "pydantic_ai"}
{"title": "OpenAIModelProfile", "anchor": "openaimodelprofile", "heading_level": 0, "md_text": "Profile for models used with `OpenAIChatModel`.\n\nALL FIELDS MUST BE `openai_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/profiles/openai", "source_site": "pydantic_ai"}
{"title": "openai_model_profile", "anchor": "openai_model_profile", "heading_level": 0, "md_text": "Get the model profile for an OpenAI model.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/profiles/openai", "source_site": "pydantic_ai"}
{"title": "OpenAIJsonSchemaTransformer", "anchor": "openaijsonschematransformer", "heading_level": 0, "md_text": "Recursively handle the schema to make it compatible with OpenAI strict mode.\n\nSee https://platform.openai.com/docs/guides/function-calling?api-mode=responses#strict-mode for more details,\nbut this basically just requires:\n* `additionalProperties` must be set to false for each object in the parameters\n* all fields in properties must be marked as required", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/profiles/openai", "source_site": "pydantic_ai"}
{"title": "mistral_model_profile", "anchor": "mistral_model_profile", "heading_level": 0, "md_text": "Get the model profile for a Mistral model.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/profiles/mistral", "source_site": "pydantic_ai"}
{"title": "amazon_model_profile", "anchor": "amazon_model_profile", "heading_level": 0, "md_text": "Get the model profile for an Amazon model.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/profiles/amazon", "source_site": "pydantic_ai"}
{"title": "GroqModelProfile", "anchor": "groqmodelprofile", "heading_level": 0, "md_text": "Profile for models used with GroqModel.\n\nALL FIELDS MUST BE `groq_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/profiles/groq", "source_site": "pydantic_ai"}
{"title": "groq_model_profile", "anchor": "groq_model_profile", "heading_level": 0, "md_text": "Get the model profile for a Groq model.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/profiles/groq", "source_site": "pydantic_ai"}
{"title": "grok_model_profile", "anchor": "grok_model_profile", "heading_level": 0, "md_text": "Get the model profile for a Grok model.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/profiles/grok", "source_site": "pydantic_ai"}
{"title": "harmony_model_profile", "anchor": "harmony_model_profile", "heading_level": 0, "md_text": "The model profile for the OpenAI Harmony Response format.\n\nSee <https://cookbook.openai.com/articles/openai-harmony> for more details.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/profiles/harmony", "source_site": "pydantic_ai"}
{"title": "ModelProfile", "anchor": "modelprofile", "heading_level": 0, "md_text": "Describes how requests to and responses from specific models or families of models need to be constructed and processed to get the best results, independent of the model and provider classes used.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/profiles/__init__", "source_site": "pydantic_ai"}
{"title": "ModelProfile.from_profile", "anchor": "modelprofile-from_profile", "heading_level": 0, "md_text": "Build a ModelProfile subclass instance from a ModelProfile instance.", "url": "https://ai.pydantic.dev/api/from-profile/", "page": "pydantic_ai_slim/pydantic_ai/profiles/__init__", "source_site": "pydantic_ai"}
{"title": "ModelProfile.update", "anchor": "modelprofile-update", "heading_level": 0, "md_text": "Update this ModelProfile (subclass) instance with the non-default values from another ModelProfile instance.", "url": "https://ai.pydantic.dev/api/update/", "page": "pydantic_ai_slim/pydantic_ai/profiles/__init__", "source_site": "pydantic_ai"}
{"title": "meta_model_profile", "anchor": "meta_model_profile", "heading_level": 0, "md_text": "Get the model profile for a Meta model.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/profiles/meta", "source_site": "pydantic_ai"}
{"title": "anthropic_model_profile", "anchor": "anthropic_model_profile", "heading_level": 0, "md_text": "Get the model profile for an Anthropic model.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/profiles/anthropic", "source_site": "pydantic_ai"}
{"title": "cohere_model_profile", "anchor": "cohere_model_profile", "heading_level": 0, "md_text": "Get the model profile for a Cohere model.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/profiles/cohere", "source_site": "pydantic_ai"}
{"title": "moonshotai_model_profile", "anchor": "moonshotai_model_profile", "heading_level": 0, "md_text": "Get the model profile for a MoonshotAI model.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/profiles/moonshotai", "source_site": "pydantic_ai"}
{"title": "deepseek_model_profile", "anchor": "deepseek_model_profile", "heading_level": 0, "md_text": "Get the model profile for a DeepSeek model.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/profiles/deepseek", "source_site": "pydantic_ai"}
{"title": "google_model_profile", "anchor": "google_model_profile", "heading_level": 0, "md_text": "Get the model profile for a Google model.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/profiles/google", "source_site": "pydantic_ai"}
{"title": "GoogleJsonSchemaTransformer", "anchor": "googlejsonschematransformer", "heading_level": 0, "md_text": "Transforms the JSON Schema from Pydantic to be suitable for Gemini.\n\nGemini which [supports](https://ai.google.dev/gemini-api/docs/function-calling#function_declarations)\na subset of OpenAPI v3.0.3.\n\nSpecifically:\n* gemini doesn't allow the `title` keyword to be set\n* gemini doesn't allow `$defs` \u2014 we need to inline the definitions where possible", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/profiles/google", "source_site": "pydantic_ai"}
{"title": "qwen_model_profile", "anchor": "qwen_model_profile", "heading_level": 0, "md_text": "Get the model profile for a Qwen model.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/profiles/qwen", "source_site": "pydantic_ai"}
{"title": "PrefectMCPServer", "anchor": "prefectmcpserver", "heading_level": 0, "md_text": "A wrapper for MCPServer that integrates with Prefect, turning call_tool and get_tools into Prefect tasks.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_mcp_server", "source_site": "pydantic_ai"}
{"title": "PrefectMCPServer.call_tool", "anchor": "prefectmcpserver-call_tool", "heading_level": 0, "md_text": "Call an MCP tool, wrapped as a Prefect task with a descriptive name.", "url": "https://ai.pydantic.dev/api/call-tool/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_mcp_server", "source_site": "pydantic_ai"}
{"title": "PrefectFunctionToolset", "anchor": "prefectfunctiontoolset", "heading_level": 0, "md_text": "A wrapper for FunctionToolset that integrates with Prefect, turning tool calls into Prefect tasks.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_function_toolset", "source_site": "pydantic_ai"}
{"title": "PrefectFunctionToolset.call_tool", "anchor": "prefectfunctiontoolset-call_tool", "heading_level": 0, "md_text": "Call a tool, wrapped as a Prefect task with a descriptive name.", "url": "https://ai.pydantic.dev/api/call-tool/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_function_toolset", "source_site": "pydantic_ai"}
{"title": "PrefectStreamedResponse", "anchor": "prefectstreamedresponse", "heading_level": 0, "md_text": "A non-streaming response wrapper for Prefect tasks.\n\nWhen a model request is executed inside a Prefect flow, the entire stream\nis consumed within the task, and this wrapper is returned containing the\nfinal response.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model", "source_site": "pydantic_ai"}
{"title": "PrefectModel", "anchor": "prefectmodel", "heading_level": 0, "md_text": "A wrapper for Model that integrates with Prefect, turning request and request_stream into Prefect tasks.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model", "source_site": "pydantic_ai"}
{"title": "PrefectStreamedResponse._get_event_iterator", "anchor": "prefectstreamedresponse-_get_event_iterator", "heading_level": 0, "md_text": "Return an empty iterator since the stream has already been consumed.", "url": "https://ai.pydantic.dev/api/-get-event-iterator/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model", "source_site": "pydantic_ai"}
{"title": "PrefectModel.request", "anchor": "prefectmodel-request", "heading_level": 0, "md_text": "Make a model request, wrapped as a Prefect task when in a flow.", "url": "https://ai.pydantic.dev/api/request/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model", "source_site": "pydantic_ai"}
{"title": "PrefectModel.request_stream", "anchor": "prefectmodel-request_stream", "heading_level": 0, "md_text": "Make a streaming model request.\n\nWhen inside a Prefect flow, the stream is consumed within a task and\na non-streaming response is returned. When not in a flow, behaves normally.", "url": "https://ai.pydantic.dev/api/request-stream/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model", "source_site": "pydantic_ai"}
{"title": "_replace_run_context", "anchor": "_replace_run_context", "heading_level": 0, "md_text": "Replace RunContext objects with a dict containing only hashable fields.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_cache_policies", "source_site": "pydantic_ai"}
{"title": "_strip_timestamps", "anchor": "_strip_timestamps", "heading_level": 0, "md_text": "Recursively convert dataclasses to dicts, excluding timestamp fields.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_cache_policies", "source_site": "pydantic_ai"}
{"title": "_replace_toolsets", "anchor": "_replace_toolsets", "heading_level": 0, "md_text": "Replace Toolset objects with a dict containing only hashable fields.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_cache_policies", "source_site": "pydantic_ai"}
{"title": "PrefectAgentInputs", "anchor": "prefectagentinputs", "heading_level": 0, "md_text": "Cache policy designed to handle input hashing for PrefectAgent cache keys.\n\nComputes a cache key based on inputs, ignoring nested 'timestamp' fields\nand serializing RunContext objects to only include hashable fields.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_cache_policies", "source_site": "pydantic_ai"}
{"title": "PrefectAgentInputs.compute_key", "anchor": "prefectagentinputs-compute_key", "heading_level": 0, "md_text": "Compute cache key from inputs with timestamps removed and RunContext serialized.", "url": "https://ai.pydantic.dev/api/compute-key/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_cache_policies", "source_site": "pydantic_ai"}
{"title": "PrefectAgent.__init__", "anchor": "prefectagent-__init__", "heading_level": 0, "md_text": "Wrap an agent to enable it with Prefect durable flows, by automatically offloading model requests, tool calls, and MCP server communication to Prefect tasks.\n\nAfter wrapping, the original agent can still be used as normal outside of the Prefect flow.\n\nArgs:\n    wrapped: The agent to wrap.\n    name: Optional unique agent name to use as the Prefect flow name prefix. If not provided, the agent's `name` will be used.\n    event_stream_handler: Optional event stream handler to use instead of the one set on the wrapped agent.\n    mcp_task_config: The base Prefect task config to use for MCP server tasks. If no config is provided, use the default settings of Prefect.\n    model_task_config: The Prefect task config to use for model request tasks. If no config is provided, use the default settings of Prefect.\n    tool_task_config: The default Prefect task config to use for tool calls. If no config is provided, use the default settings of Prefect.\n    tool_task_config_by_name: Per-tool task configuration. Keys are tool names, values are TaskConfig or None (None disables task wrapping for that tool).\n    event_stream_handler_task_config: The Prefect task config to use for the event stream handler task. If no config is provided, use the default settings of Prefect.\n    prefectify_toolset_func: Optional function to use to prepare toolsets for Prefect by wrapping them in a `PrefectWrapperToolset` that moves methods that require IO to Prefect tasks.\n        If not provided, only `FunctionToolset` and `MCPServer` will be prepared for Prefect.\n        The function takes the toolset, the task config, the tool-specific task config, and the tool-specific task config by name.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent", "source_site": "pydantic_ai"}
{"title": "PrefectAgent.run", "anchor": "prefectagent-run", "heading_level": 0, "md_text": "Run the agent with a user prompt in async mode.\n\nThis method builds an internal agent graph (using system prompts, tools and result schemas) and then\nruns the graph to completion. The result of the run is returned.\n\nExample:\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    agent_run = await agent.run('What is the capital of France?')\n    print(agent_run.output)\n    #> The capital of France is Paris.\n```\n\nArgs:\n    user_prompt: User input to start/continue the conversation.\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    event_stream_handler: Optional event stream handler to use for this run.\n    builtin_tools: Optional additional builtin tools for this run.\n\nReturns:\n    The result of the run.", "url": "https://ai.pydantic.dev/api/run/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent", "source_site": "pydantic_ai"}
{"title": "PrefectAgent.run_sync", "anchor": "prefectagent-run_sync", "heading_level": 0, "md_text": "Synchronously run the agent with a user prompt.\n\nThis is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] with `loop.run_until_complete(...)`.\nYou therefore can't use this method inside async code or if there's an active event loop.\n\nExample:\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nresult_sync = agent.run_sync('What is the capital of Italy?')\nprint(result_sync.output)\n#> The capital of Italy is Rome.\n```\n\nArgs:\n    user_prompt: User input to start/continue the conversation.\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    event_stream_handler: Optional event stream handler to use for this run.\n    builtin_tools: Optional additional builtin tools for this run.\n\nReturns:\n    The result of the run.", "url": "https://ai.pydantic.dev/api/run-sync/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent", "source_site": "pydantic_ai"}
{"title": "PrefectAgent.run_stream", "anchor": "prefectagent-run_stream", "heading_level": 0, "md_text": "Run the agent with a user prompt in async mode, returning a streamed response.\n\nExample:\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    async with agent.run_stream('What is the capital of the UK?') as response:\n        print(await response.get_output())\n        #> The capital of the UK is London.\n```\n\nArgs:\n    user_prompt: User input to start/continue the conversation.\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    builtin_tools: Optional additional builtin tools for this run.\n    event_stream_handler: Optional event stream handler to use for this run. It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager.\n\nReturns:\n    The result of the run.", "url": "https://ai.pydantic.dev/api/run-stream/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent", "source_site": "pydantic_ai"}
{"title": "PrefectAgent.run_stream_events", "anchor": "prefectagent-run_stream_events", "heading_level": 0, "md_text": "Run the agent with a user prompt in async mode and stream events from the run.\n\nThis is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] and\nuses the `event_stream_handler` kwarg to get a stream of events from the run.\n\nExample:\n```python\nfrom pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    events: list[AgentStreamEvent | AgentRunResultEvent] = []\n    async for event in agent.run_stream_events('What is the capital of France?'):\n        events.append(event)\n    print(events)\n    '''\n    [\n        PartStartEvent(index=0, part=TextPart(content='The capital of ')),\n        FinalResultEvent(tool_name=None, tool_call_id=None),\n        PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='France is Paris. ')),\n        PartEndEvent(\n            index=0, part=TextPart(content='The capital of France is Paris. ')\n        ),\n        AgentRunResultEvent(\n            result=AgentRunResult(output='The capital of France is Paris. ')\n        ),\n    ]\n    '''\n```\n\nArguments are the same as for [`self.run`][pydantic_ai.agent.AbstractAgent.run],\nexcept that `event_stream_handler` is now allowed.\n\nArgs:\n    user_prompt: User input to start/continue the conversation.\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    builtin_tools: Optional additional builtin tools for this run.\n\nReturns:\n    An async iterable of stream events `AgentStreamEvent` and finally a `AgentRunResultEvent` with the final\n    run result.", "url": "https://ai.pydantic.dev/api/run-stream-events/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent", "source_site": "pydantic_ai"}
{"title": "PrefectAgent.iter", "anchor": "prefectagent-iter", "heading_level": 0, "md_text": "A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.\n\nThis method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an\n`AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are\nexecuted. This is the API to use if you want to consume the outputs coming from each LLM model response, or the\nstream of events coming from the execution of tools.\n\nThe `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,\nand the final result of the run once it has completed.\n\nFor more details, see the documentation of `AgentRun`.\n\nExample:\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    nodes = []\n    async with agent.iter('What is the capital of France?') as agent_run:\n        async for node in agent_run:\n            nodes.append(node)\n    print(nodes)\n    '''\n    [\n        UserPromptNode(\n            user_prompt='What is the capital of France?',\n            instructions_functions=[],\n            system_prompts=(),\n            system_prompt_functions=[],\n            system_prompt_dynamic_functions={},\n        ),\n        ModelRequestNode(\n            request=ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='What is the capital of France?',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            )\n        ),\n        CallToolsNode(\n            model_response=ModelResponse(\n                parts=[TextPart(content='The capital of France is Paris.')],\n                usage=RequestUsage(input_tokens=56, output_tokens=7),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            )\n        ),\n        End(data=FinalResult(output='The capital of France is Paris.')),\n    ]\n    '''\n    print(agent_run.result.output)\n    #> The capital of France is Paris.\n```\n\nArgs:\n    user_prompt: User input to start/continue the conversation.\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    builtin_tools: Optional additional builtin tools for this run.\n\nReturns:\n    The result of the run.", "url": "https://ai.pydantic.dev/api/iter/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent", "source_site": "pydantic_ai"}
{"title": "PrefectAgent.override", "anchor": "prefectagent-override", "heading_level": 0, "md_text": "Context manager to temporarily override agent dependencies, model, toolsets, tools, or instructions.\n\nThis is particularly useful when testing.\nYou can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).\n\nArgs:\n    name: The name to use instead of the name passed to the agent constructor and agent run.\n    deps: The dependencies to use instead of the dependencies passed to the agent run.\n    model: The model to use instead of the model passed to the agent run.\n    toolsets: The toolsets to use instead of the toolsets passed to the agent constructor and agent run.\n    tools: The tools to use instead of the tools registered with the agent.\n    instructions: The instructions to use instead of the instructions registered with the agent.", "url": "https://ai.pydantic.dev/api/override/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent", "source_site": "pydantic_ai"}
{"title": "_prefectify_toolset", "anchor": "_prefectify_toolset", "heading_level": 0, "md_text": "Convert a toolset to its Prefect equivalent.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent", "source_site": "pydantic_ai"}
{"title": "TaskConfig", "anchor": "taskconfig", "heading_level": 0, "md_text": "Configuration for a task in Prefect.\n\nThese options are passed to the `@task` decorator.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_types", "source_site": "pydantic_ai"}
{"title": "PrefectWrapperToolset", "anchor": "prefectwrappertoolset", "heading_level": 0, "md_text": "Base class for Prefect-wrapped toolsets.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_toolset", "source_site": "pydantic_ai"}
{"title": "prefectify_toolset", "anchor": "prefectify_toolset", "heading_level": 0, "md_text": "Wrap a toolset to integrate it with Prefect.\n\nArgs:\n    toolset: The toolset to wrap.\n    mcp_task_config: The Prefect task config to use for MCP server tasks.\n    tool_task_config: The default Prefect task config to use for tool calls.\n    tool_task_config_by_name: Per-tool task configuration. Keys are tool names, values are TaskConfig or None.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_toolset", "source_site": "pydantic_ai"}
{"title": "LogfirePlugin", "anchor": "logfireplugin", "heading_level": 0, "md_text": "Temporal client plugin for Logfire.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_logfire", "source_site": "pydantic_ai"}
{"title": "PydanticAIPlugin", "anchor": "pydanticaiplugin", "heading_level": 0, "md_text": "Temporal client and worker plugin for Pydantic AI.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/temporal/__init__", "source_site": "pydantic_ai"}
{"title": "AgentPlugin", "anchor": "agentplugin", "heading_level": 0, "md_text": "Temporal worker plugin for a specific Pydantic AI agent.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/temporal/__init__", "source_site": "pydantic_ai"}
{"title": "TemporalAgent.__init__", "anchor": "temporalagent-__init__", "heading_level": 0, "md_text": "Wrap an agent to enable it to be used inside a Temporal workflow, by automatically offloading model requests, tool calls, and MCP server communication to Temporal activities.\n\nAfter wrapping, the original agent can still be used as normal outside of the Temporal workflow, but any changes to its model or toolsets after wrapping will not be reflected in the durable agent.\n\nArgs:\n    wrapped: The agent to wrap.\n    name: Optional unique agent name to use in the Temporal activities' names. If not provided, the agent's `name` will be used.\n    event_stream_handler: Optional event stream handler to use instead of the one set on the wrapped agent.\n    activity_config: The base Temporal activity config to use for all activities. If no config is provided, a `start_to_close_timeout` of 60 seconds is used.\n    model_activity_config: The Temporal activity config to use for model request activities. This is merged with the base activity config.\n    toolset_activity_config: The Temporal activity config to use for get-tools and call-tool activities for specific toolsets identified by ID. This is merged with the base activity config.\n    tool_activity_config: The Temporal activity config to use for specific tool call activities identified by toolset ID and tool name.\n        This is merged with the base and toolset-specific activity configs.\n        If a tool does not use IO, you can specify `False` to disable using an activity.\n        Note that the tool is required to be defined as an `async` function as non-async tools are run in threads which are non-deterministic and thus not supported outside of activities.\n    run_context_type: The `TemporalRunContext` subclass to use to serialize and deserialize the run context for use inside a Temporal activity.\n        By default, only the `deps`, `retries`, `tool_call_id`, `tool_name`, `retry` and `run_step` attributes will be available.\n        To make another attribute available, create a `TemporalRunContext` subclass with a custom `serialize_run_context` class method that returns a dictionary that includes the attribute.\n    temporalize_toolset_func: Optional function to use to prepare \"leaf\" toolsets (i.e. those that implement their own tool listing and calling) for Temporal by wrapping them in a `TemporalWrapperToolset` that moves methods that require IO to Temporal activities.\n        If not provided, only `FunctionToolset` and `MCPServer` will be prepared for Temporal.\n        The function takes the toolset, the activity name prefix, the toolset-specific activity config, the tool-specific activity configs and the run context type.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent", "source_site": "pydantic_ai"}
{"title": "TemporalAgent.run", "anchor": "temporalagent-run", "heading_level": 0, "md_text": "Run the agent with a user prompt in async mode.\n\nThis method builds an internal agent graph (using system prompts, tools and result schemas) and then\nruns the graph to completion. The result of the run is returned.\n\nExample:\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    agent_run = await agent.run('What is the capital of France?')\n    print(agent_run.output)\n    #> The capital of France is Paris.\n```\n\nArgs:\n    user_prompt: User input to start/continue the conversation.\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    event_stream_handler: Optional event stream handler to use for this run.\n    builtin_tools: Optional additional builtin tools for this run.\n\nReturns:\n    The result of the run.", "url": "https://ai.pydantic.dev/api/run/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent", "source_site": "pydantic_ai"}
{"title": "TemporalAgent.run_sync", "anchor": "temporalagent-run_sync", "heading_level": 0, "md_text": "Synchronously run the agent with a user prompt.\n\nThis is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] with `loop.run_until_complete(...)`.\nYou therefore can't use this method inside async code or if there's an active event loop.\n\nExample:\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nresult_sync = agent.run_sync('What is the capital of Italy?')\nprint(result_sync.output)\n#> The capital of Italy is Rome.\n```\n\nArgs:\n    user_prompt: User input to start/continue the conversation.\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    event_stream_handler: Optional event stream handler to use for this run.\n    builtin_tools: Optional additional builtin tools for this run.\n\nReturns:\n    The result of the run.", "url": "https://ai.pydantic.dev/api/run-sync/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent", "source_site": "pydantic_ai"}
{"title": "TemporalAgent.run_stream", "anchor": "temporalagent-run_stream", "heading_level": 0, "md_text": "Run the agent with a user prompt in async mode, returning a streamed response.\n\nExample:\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    async with agent.run_stream('What is the capital of the UK?') as response:\n        print(await response.get_output())\n        #> The capital of the UK is London.\n```\n\nArgs:\n    user_prompt: User input to start/continue the conversation.\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    builtin_tools: Optional additional builtin tools for this run.\n    event_stream_handler: Optional event stream handler to use for this run. It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager.\n\nReturns:\n    The result of the run.", "url": "https://ai.pydantic.dev/api/run-stream/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent", "source_site": "pydantic_ai"}
{"title": "TemporalAgent.run_stream_events", "anchor": "temporalagent-run_stream_events", "heading_level": 0, "md_text": "Run the agent with a user prompt in async mode and stream events from the run.\n\nThis is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] and\nuses the `event_stream_handler` kwarg to get a stream of events from the run.\n\nExample:\n```python\nfrom pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    events: list[AgentStreamEvent | AgentRunResultEvent] = []\n    async for event in agent.run_stream_events('What is the capital of France?'):\n        events.append(event)\n    print(events)\n    '''\n    [\n        PartStartEvent(index=0, part=TextPart(content='The capital of ')),\n        FinalResultEvent(tool_name=None, tool_call_id=None),\n        PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='France is Paris. ')),\n        PartEndEvent(\n            index=0, part=TextPart(content='The capital of France is Paris. ')\n        ),\n        AgentRunResultEvent(\n            result=AgentRunResult(output='The capital of France is Paris. ')\n        ),\n    ]\n    '''\n```\n\nArguments are the same as for [`self.run`][pydantic_ai.agent.AbstractAgent.run],\nexcept that `event_stream_handler` is now allowed.\n\nArgs:\n    user_prompt: User input to start/continue the conversation.\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    builtin_tools: Optional additional builtin tools for this run.\n\nReturns:\n    An async iterable of stream events `AgentStreamEvent` and finally a `AgentRunResultEvent` with the final\n    run result.", "url": "https://ai.pydantic.dev/api/run-stream-events/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent", "source_site": "pydantic_ai"}
{"title": "TemporalAgent.iter", "anchor": "temporalagent-iter", "heading_level": 0, "md_text": "A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.\n\nThis method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an\n`AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are\nexecuted. This is the API to use if you want to consume the outputs coming from each LLM model response, or the\nstream of events coming from the execution of tools.\n\nThe `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,\nand the final result of the run once it has completed.\n\nFor more details, see the documentation of `AgentRun`.\n\nExample:\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    nodes = []\n    async with agent.iter('What is the capital of France?') as agent_run:\n        async for node in agent_run:\n            nodes.append(node)\n    print(nodes)\n    '''\n    [\n        UserPromptNode(\n            user_prompt='What is the capital of France?',\n            instructions_functions=[],\n            system_prompts=(),\n            system_prompt_functions=[],\n            system_prompt_dynamic_functions={},\n        ),\n        ModelRequestNode(\n            request=ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='What is the capital of France?',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            )\n        ),\n        CallToolsNode(\n            model_response=ModelResponse(\n                parts=[TextPart(content='The capital of France is Paris.')],\n                usage=RequestUsage(input_tokens=56, output_tokens=7),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            )\n        ),\n        End(data=FinalResult(output='The capital of France is Paris.')),\n    ]\n    '''\n    print(agent_run.result.output)\n    #> The capital of France is Paris.\n```\n\nArgs:\n    user_prompt: User input to start/continue the conversation.\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    builtin_tools: Optional additional builtin tools for this run.\n\nReturns:\n    The result of the run.", "url": "https://ai.pydantic.dev/api/iter/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent", "source_site": "pydantic_ai"}
{"title": "TemporalAgent.override", "anchor": "temporalagent-override", "heading_level": 0, "md_text": "Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.\n\nThis is particularly useful when testing.\nYou can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).\n\nArgs:\n    name: The name to use instead of the name passed to the agent constructor and agent run.\n    deps: The dependencies to use instead of the dependencies passed to the agent run.\n    model: The model to use instead of the model passed to the agent run.\n    toolsets: The toolsets to use instead of the toolsets passed to the agent constructor and agent run.\n    tools: The tools to use instead of the tools registered with the agent.\n    instructions: The instructions to use instead of the instructions registered with the agent.", "url": "https://ai.pydantic.dev/api/override/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent", "source_site": "pydantic_ai"}
{"title": "TemporalRunContext", "anchor": "temporalruncontext", "heading_level": 0, "md_text": "The [`RunContext`][pydantic_ai.tools.RunContext] subclass to use to serialize and deserialize the run context for use inside a Temporal activity.\n\nBy default, only the `deps`, `retries`, `tool_call_id`, `tool_name`, `tool_call_approved`, `retry`, `max_retries` and `run_step` attributes will be available.\nTo make another attribute available, create a `TemporalRunContext` subclass with a custom `serialize_run_context` class method that returns a dictionary that includes the attribute and pass it to [`TemporalAgent`][pydantic_ai.durable_exec.temporal.TemporalAgent].", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_run_context", "source_site": "pydantic_ai"}
{"title": "TemporalRunContext.serialize_run_context", "anchor": "temporalruncontext-serialize_run_context", "heading_level": 0, "md_text": "Serialize the run context to a `dict[str, Any]`.", "url": "https://ai.pydantic.dev/api/serialize-run-context/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_run_context", "source_site": "pydantic_ai"}
{"title": "TemporalRunContext.deserialize_run_context", "anchor": "temporalruncontext-deserialize_run_context", "heading_level": 0, "md_text": "Deserialize the run context from a `dict[str, Any]`.", "url": "https://ai.pydantic.dev/api/deserialize-run-context/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_run_context", "source_site": "pydantic_ai"}
{"title": "temporalize_toolset", "anchor": "temporalize_toolset", "heading_level": 0, "md_text": "Temporalize a toolset.\n\nArgs:\n    toolset: The toolset to temporalize.\n    activity_name_prefix: Prefix for Temporal activity names.\n    activity_config: The Temporal activity config to use.\n    tool_activity_config: The Temporal activity config to use for specific tools identified by tool name.\n    deps_type: The type of agent's dependencies object. It needs to be serializable using Pydantic's `TypeAdapter`.\n    run_context_type: The `TemporalRunContext` (sub)class that's used to serialize and deserialize the run context.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_toolset", "source_site": "pydantic_ai"}
{"title": "DBOSMCPServer", "anchor": "dbosmcpserver", "heading_level": 0, "md_text": "A wrapper for MCPServer that integrates with DBOS, turning call_tool and get_tools to DBOS steps.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_mcp_server", "source_site": "pydantic_ai"}
{"title": "DBOSModel", "anchor": "dbosmodel", "heading_level": 0, "md_text": "A wrapper for Model that integrates with DBOS, turning request and request_stream to DBOS steps.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_model", "source_site": "pydantic_ai"}
{"title": "StepConfig", "anchor": "stepconfig", "heading_level": 0, "md_text": "Configuration for a step in the DBOS workflow.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_utils", "source_site": "pydantic_ai"}
{"title": "DBOSAgent.__init__", "anchor": "dbosagent-__init__", "heading_level": 0, "md_text": "Wrap an agent to enable it with DBOS durable workflows, by automatically offloading model requests, tool calls, and MCP server communication to DBOS steps.\n\nAfter wrapping, the original agent can still be used as normal outside of the DBOS workflow.\n\nArgs:\n    wrapped: The agent to wrap.\n    name: Optional unique agent name to use as the DBOS configured instance name. If not provided, the agent's `name` will be used.\n    event_stream_handler: Optional event stream handler to use instead of the one set on the wrapped agent.\n    mcp_step_config: The base DBOS step config to use for MCP server steps. If no config is provided, use the default settings of DBOS.\n    model_step_config: The DBOS step config to use for model request steps. If no config is provided, use the default settings of DBOS.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent", "source_site": "pydantic_ai"}
{"title": "DBOSAgent.run", "anchor": "dbosagent-run", "heading_level": 0, "md_text": "Run the agent with a user prompt in async mode.\n\nThis method builds an internal agent graph (using system prompts, tools and result schemas) and then\nruns the graph to completion. The result of the run is returned.\n\nExample:\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    agent_run = await agent.run('What is the capital of France?')\n    print(agent_run.output)\n    #> The capital of France is Paris.\n```\n\nArgs:\n    user_prompt: User input to start/continue the conversation.\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    builtin_tools: Optional additional builtin tools for this run.\n    event_stream_handler: Optional event stream handler to use for this run.\n\nReturns:\n    The result of the run.", "url": "https://ai.pydantic.dev/api/run/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent", "source_site": "pydantic_ai"}
{"title": "DBOSAgent.run_sync", "anchor": "dbosagent-run_sync", "heading_level": 0, "md_text": "Synchronously run the agent with a user prompt.\n\nThis is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] with `loop.run_until_complete(...)`.\nYou therefore can't use this method inside async code or if there's an active event loop.\n\nExample:\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nresult_sync = agent.run_sync('What is the capital of Italy?')\nprint(result_sync.output)\n#> The capital of Italy is Rome.\n```\n\nArgs:\n    user_prompt: User input to start/continue the conversation.\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    builtin_tools: Optional additional builtin tools for this run.\n    event_stream_handler: Optional event stream handler to use for this run.\n\nReturns:\n    The result of the run.", "url": "https://ai.pydantic.dev/api/run-sync/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent", "source_site": "pydantic_ai"}
{"title": "DBOSAgent.run_stream", "anchor": "dbosagent-run_stream", "heading_level": 0, "md_text": "Run the agent with a user prompt in async mode, returning a streamed response.\n\nExample:\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    async with agent.run_stream('What is the capital of the UK?') as response:\n        print(await response.get_output())\n        #> The capital of the UK is London.\n```\n\nArgs:\n    user_prompt: User input to start/continue the conversation.\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    builtin_tools: Optional additional builtin tools for this run.\n    event_stream_handler: Optional event stream handler to use for this run. It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager.\n\nReturns:\n    The result of the run.", "url": "https://ai.pydantic.dev/api/run-stream/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent", "source_site": "pydantic_ai"}
{"title": "DBOSAgent.run_stream_events", "anchor": "dbosagent-run_stream_events", "heading_level": 0, "md_text": "Run the agent with a user prompt in async mode and stream events from the run.\n\nThis is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] and\nuses the `event_stream_handler` kwarg to get a stream of events from the run.\n\nExample:\n```python\nfrom pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    events: list[AgentStreamEvent | AgentRunResultEvent] = []\n    async for event in agent.run_stream_events('What is the capital of France?'):\n        events.append(event)\n    print(events)\n    '''\n    [\n        PartStartEvent(index=0, part=TextPart(content='The capital of ')),\n        FinalResultEvent(tool_name=None, tool_call_id=None),\n        PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='France is Paris. ')),\n        PartEndEvent(\n            index=0, part=TextPart(content='The capital of France is Paris. ')\n        ),\n        AgentRunResultEvent(\n            result=AgentRunResult(output='The capital of France is Paris. ')\n        ),\n    ]\n    '''\n```\n\nArguments are the same as for [`self.run`][pydantic_ai.agent.AbstractAgent.run],\nexcept that `event_stream_handler` is now allowed.\n\nArgs:\n    user_prompt: User input to start/continue the conversation.\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    builtin_tools: Optional additional builtin tools for this run.\n\nReturns:\n    An async iterable of stream events `AgentStreamEvent` and finally a `AgentRunResultEvent` with the final\n    run result.", "url": "https://ai.pydantic.dev/api/run-stream-events/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent", "source_site": "pydantic_ai"}
{"title": "DBOSAgent.iter", "anchor": "dbosagent-iter", "heading_level": 0, "md_text": "A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.\n\nThis method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an\n`AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are\nexecuted. This is the API to use if you want to consume the outputs coming from each LLM model response, or the\nstream of events coming from the execution of tools.\n\nThe `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,\nand the final result of the run once it has completed.\n\nFor more details, see the documentation of `AgentRun`.\n\nExample:\n```python\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    nodes = []\n    async with agent.iter('What is the capital of France?') as agent_run:\n        async for node in agent_run:\n            nodes.append(node)\n    print(nodes)\n    '''\n    [\n        UserPromptNode(\n            user_prompt='What is the capital of France?',\n            instructions_functions=[],\n            system_prompts=(),\n            system_prompt_functions=[],\n            system_prompt_dynamic_functions={},\n        ),\n        ModelRequestNode(\n            request=ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='What is the capital of France?',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            )\n        ),\n        CallToolsNode(\n            model_response=ModelResponse(\n                parts=[TextPart(content='The capital of France is Paris.')],\n                usage=RequestUsage(input_tokens=56, output_tokens=7),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            )\n        ),\n        End(data=FinalResult(output='The capital of France is Paris.')),\n    ]\n    '''\n    print(agent_run.result.output)\n    #> The capital of France is Paris.\n```\n\nArgs:\n    user_prompt: User input to start/continue the conversation.\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no\n        output validators since output validators would expect an argument that matches the agent's output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    builtin_tools: Optional additional builtin tools for this run.\n\nReturns:\n    The result of the run.", "url": "https://ai.pydantic.dev/api/iter/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent", "source_site": "pydantic_ai"}
{"title": "DBOSAgent.override", "anchor": "dbosagent-override", "heading_level": 0, "md_text": "Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.\n\nThis is particularly useful when testing.\nYou can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).\n\nArgs:\n    name: The name to use instead of the name passed to the agent constructor and agent run.\n    deps: The dependencies to use instead of the dependencies passed to the agent run.\n    model: The model to use instead of the model passed to the agent run.\n    toolsets: The toolsets to use instead of the toolsets passed to the agent constructor and agent run.\n    tools: The tools to use instead of the tools registered with the agent.\n    instructions: The instructions to use instead of the instructions registered with the agent.", "url": "https://ai.pydantic.dev/api/override/", "page": "pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent", "source_site": "pydantic_ai"}
{"title": "__init__", "anchor": "__init__", "heading_level": 0, "md_text": "AG-UI protocol integration for Pydantic AI agents.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/ag_ui/__init__", "source_site": "pydantic_ai"}
{"title": "_event_stream", "anchor": "_event_stream", "heading_level": 0, "md_text": "AG-UI protocol adapter for Pydantic AI agents.\n\nThis module provides classes for integrating Pydantic AI agents with the AG-UI protocol,\nenabling streaming event-based communication for interactive AI applications.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/ag_ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "AGUIEventStream", "anchor": "aguieventstream", "heading_level": 0, "md_text": "UI event stream transformer for the Agent-User Interaction (AG-UI) protocol.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/ag_ui/_event_stream", "source_site": "pydantic_ai"}
{"title": "_adapter", "anchor": "_adapter", "heading_level": 0, "md_text": "AG-UI adapter for handling requests.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/ag_ui/_adapter", "source_site": "pydantic_ai"}
{"title": "_AGUIFrontendToolset", "anchor": "_aguifrontendtoolset", "heading_level": 0, "md_text": "Toolset for AG-UI frontend tools.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/ag_ui/_adapter", "source_site": "pydantic_ai"}
{"title": "AGUIAdapter", "anchor": "aguiadapter", "heading_level": 0, "md_text": "UI adapter for the Agent-User Interaction (AG-UI) protocol.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/ag_ui/_adapter", "source_site": "pydantic_ai"}
{"title": "_AGUIFrontendToolset.__init__", "anchor": "_aguifrontendtoolset-__init__", "heading_level": 0, "md_text": "Initialize the toolset with AG-UI tools.\n\nArgs:\n    tools: List of AG-UI tool definitions.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/ui/ag_ui/_adapter", "source_site": "pydantic_ai"}
{"title": "_AGUIFrontendToolset.label", "anchor": "_aguifrontendtoolset-label", "heading_level": 0, "md_text": "Return the label for this toolset.", "url": "https://ai.pydantic.dev/api/label/", "page": "pydantic_ai_slim/pydantic_ai/ui/ag_ui/_adapter", "source_site": "pydantic_ai"}
{"title": "AGUIAdapter.build_run_input", "anchor": "aguiadapter-build_run_input", "heading_level": 0, "md_text": "Build an AG-UI run input object from the request body.", "url": "https://ai.pydantic.dev/api/build-run-input/", "page": "pydantic_ai_slim/pydantic_ai/ui/ag_ui/_adapter", "source_site": "pydantic_ai"}
{"title": "AGUIAdapter.build_event_stream", "anchor": "aguiadapter-build_event_stream", "heading_level": 0, "md_text": "Build an AG-UI event stream transformer.", "url": "https://ai.pydantic.dev/api/build-event-stream/", "page": "pydantic_ai_slim/pydantic_ai/ui/ag_ui/_adapter", "source_site": "pydantic_ai"}
{"title": "AGUIAdapter.messages", "anchor": "aguiadapter-messages", "heading_level": 0, "md_text": "Pydantic AI messages from the AG-UI run input.", "url": "https://ai.pydantic.dev/api/messages/", "page": "pydantic_ai_slim/pydantic_ai/ui/ag_ui/_adapter", "source_site": "pydantic_ai"}
{"title": "AGUIAdapter.toolset", "anchor": "aguiadapter-toolset", "heading_level": 0, "md_text": "Toolset representing frontend tools from the AG-UI run input.", "url": "https://ai.pydantic.dev/api/toolset/", "page": "pydantic_ai_slim/pydantic_ai/ui/ag_ui/_adapter", "source_site": "pydantic_ai"}
{"title": "AGUIAdapter.state", "anchor": "aguiadapter-state", "heading_level": 0, "md_text": "Frontend state from the AG-UI run input.", "url": "https://ai.pydantic.dev/api/state/", "page": "pydantic_ai_slim/pydantic_ai/ui/ag_ui/_adapter", "source_site": "pydantic_ai"}
{"title": "AGUIAdapter.load_messages", "anchor": "aguiadapter-load_messages", "heading_level": 0, "md_text": "Transform AG-UI messages into Pydantic AI messages.", "url": "https://ai.pydantic.dev/api/load-messages/", "page": "pydantic_ai_slim/pydantic_ai/ui/ag_ui/_adapter", "source_site": "pydantic_ai"}
{"title": "app", "anchor": "app", "heading_level": 0, "md_text": "AG-UI protocol integration for Pydantic AI agents.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/ag_ui/app", "source_site": "pydantic_ai"}
{"title": "AGUIApp", "anchor": "aguiapp", "heading_level": 0, "md_text": "ASGI application for running Pydantic AI agents with AG-UI protocol support.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/ag_ui/app", "source_site": "pydantic_ai"}
{"title": "AGUIApp.__init__", "anchor": "aguiapp-__init__", "heading_level": 0, "md_text": "An ASGI application that handles every request by running the agent and streaming the response.\n\nNote that the `deps` will be the same for each request, with the exception of the frontend state that's\ninjected into the `state` field of a `deps` object that implements the [`StateHandler`][pydantic_ai.ui.StateHandler] protocol.\nTo provide different `deps` for each request (e.g. based on the authenticated user),\nuse [`AGUIAdapter.run_stream()`][pydantic_ai.ui.ag_ui.AGUIAdapter.run_stream] or\n[`AGUIAdapter.dispatch_request()`][pydantic_ai.ui.ag_ui.AGUIAdapter.dispatch_request] instead.\n\nArgs:\n    agent: The agent to run.\n\n    output_type: Custom output type to use for this run, `output_type` may only be used if the agent has\n        no output validators since output validators would expect an argument that matches the agent's\n        output type.\n    message_history: History of the conversation so far.\n    deferred_tool_results: Optional results for deferred tool calls in the message history.\n    model: Optional model to use for this run, required if `model` was not set when creating the agent.\n    deps: Optional dependencies to use for this run.\n    model_settings: Optional settings to use for this model's request.\n    usage_limits: Optional limits on model request count or token usage.\n    usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.\n    infer_name: Whether to try to infer the agent name from the call frame if it's not set.\n    toolsets: Optional additional toolsets for this run.\n    builtin_tools: Optional additional builtin tools for this run.\n    on_complete: Optional callback function called when the agent run completes successfully.\n        The callback receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can access `all_messages()` and other result data.\n\n    debug: Boolean indicating if debug tracebacks should be returned on errors.\n    routes: A list of routes to serve incoming HTTP and WebSocket requests.\n    middleware: A list of middleware to run for every request. A starlette application will always\n        automatically include two middleware classes. `ServerErrorMiddleware` is added as the very\n        outermost middleware, to handle any uncaught errors occurring anywhere in the entire stack.\n        `ExceptionMiddleware` is added as the very innermost middleware, to deal with handled\n        exception cases occurring in the routing or endpoints.\n    exception_handlers: A mapping of either integer status codes, or exception class types onto\n        callables which handle the exceptions. Exception handler callables should be of the form\n        `handler(request, exc) -> response` and may be either standard functions, or async functions.\n    on_startup: A list of callables to run on application startup. Startup handler callables do not\n        take any arguments, and may be either standard functions, or async functions.\n    on_shutdown: A list of callables to run on application shutdown. Shutdown handler callables do\n        not take any arguments, and may be either standard functions, or async functions.\n    lifespan: A lifespan context function, which can be used to perform startup and shutdown tasks.\n        This is a newer style that replaces the `on_startup` and `on_shutdown` handlers. Use one or\n        the other, not both.", "url": "https://ai.pydantic.dev/api/--init--/", "page": "pydantic_ai_slim/pydantic_ai/ui/ag_ui/app", "source_site": "pydantic_ai"}
{"title": "run_agent", "anchor": "run_agent", "heading_level": 0, "md_text": "Endpoint to run the agent with the provided input data.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/ag_ui/app", "source_site": "pydantic_ai"}
{"title": "response_types", "anchor": "response_types", "heading_level": 0, "md_text": "Vercel AI response types (SSE chunks).\n\nConverted to Python from:\nhttps://github.com/vercel/ai/blob/ai%405.0.59/packages/ai/src/ui-message-stream/ui-message-chunks.ts", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "BaseChunk", "anchor": "basechunk", "heading_level": 0, "md_text": "Abstract base class for response SSE events.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "TextStartChunk", "anchor": "textstartchunk", "heading_level": 0, "md_text": "Text start chunk.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "TextDeltaChunk", "anchor": "textdeltachunk", "heading_level": 0, "md_text": "Text delta chunk.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "TextEndChunk", "anchor": "textendchunk", "heading_level": 0, "md_text": "Text end chunk.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "ReasoningStartChunk", "anchor": "reasoningstartchunk", "heading_level": 0, "md_text": "Reasoning start chunk.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "ReasoningDeltaChunk", "anchor": "reasoningdeltachunk", "heading_level": 0, "md_text": "Reasoning delta chunk.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "ReasoningEndChunk", "anchor": "reasoningendchunk", "heading_level": 0, "md_text": "Reasoning end chunk.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "ErrorChunk", "anchor": "errorchunk", "heading_level": 0, "md_text": "Error chunk.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "ToolInputStartChunk", "anchor": "toolinputstartchunk", "heading_level": 0, "md_text": "Tool input start chunk.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "ToolInputDeltaChunk", "anchor": "toolinputdeltachunk", "heading_level": 0, "md_text": "Tool input delta chunk.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "ToolOutputAvailableChunk", "anchor": "tooloutputavailablechunk", "heading_level": 0, "md_text": "Tool output available chunk.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "ToolInputAvailableChunk", "anchor": "toolinputavailablechunk", "heading_level": 0, "md_text": "Tool input available chunk.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "ToolInputErrorChunk", "anchor": "toolinputerrorchunk", "heading_level": 0, "md_text": "Tool input error chunk.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "ToolOutputErrorChunk", "anchor": "tooloutputerrorchunk", "heading_level": 0, "md_text": "Tool output error chunk.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "SourceUrlChunk", "anchor": "sourceurlchunk", "heading_level": 0, "md_text": "Source URL chunk.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "SourceDocumentChunk", "anchor": "sourcedocumentchunk", "heading_level": 0, "md_text": "Source document chunk.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "FileChunk", "anchor": "filechunk", "heading_level": 0, "md_text": "File chunk.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "DataChunk", "anchor": "datachunk", "heading_level": 0, "md_text": "Data chunk with dynamic type.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "StartStepChunk", "anchor": "startstepchunk", "heading_level": 0, "md_text": "Start step chunk.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "FinishStepChunk", "anchor": "finishstepchunk", "heading_level": 0, "md_text": "Finish step chunk.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "StartChunk", "anchor": "startchunk", "heading_level": 0, "md_text": "Start chunk.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "FinishChunk", "anchor": "finishchunk", "heading_level": 0, "md_text": "Finish chunk.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "AbortChunk", "anchor": "abortchunk", "heading_level": 0, "md_text": "Abort chunk.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "MessageMetadataChunk", "anchor": "messagemetadatachunk", "heading_level": 0, "md_text": "Message metadata chunk.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "DoneChunk", "anchor": "donechunk", "heading_level": 0, "md_text": "Done chunk.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/response_types", "source_site": "pydantic_ai"}
{"title": "__init__", "anchor": "__init__", "heading_level": 0, "md_text": "Vercel AI protocol adapter for Pydantic AI agents.\n\nThis module provides classes for integrating Pydantic AI agents with the Vercel AI protocol,\nenabling streaming event-based communication for interactive AI applications.\n\nConverted to Python from:\nhttps://github.com/vercel/ai/blob/ai%405.0.34/packages/ai/src/ui/ui-messages.ts", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/__init__", "source_site": "pydantic_ai"}
{"title": "_utils", "anchor": "_utils", "heading_level": 0, "md_text": "Utilities for Vercel AI protocol.\n\nConverted to Python from:\nhttps://github.com/vercel/ai/blob/ai%405.0.34/packages/ai/src/ui/ui-messages.ts", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_utils", "source_site": "pydantic_ai"}
{"title": "CamelBaseModel", "anchor": "camelbasemodel", "heading_level": 0, "md_text": "Base model with camelCase aliases.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_utils", "source_site": "pydantic_ai"}
{"title": "_event_stream", "anchor": "_event_stream", "heading_level": 0, "md_text": "Vercel AI event stream implementation.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_event_stream", "source_site": "pydantic_ai"}
{"title": "_json_dumps", "anchor": "_json_dumps", "heading_level": 0, "md_text": "Dump an object to JSON string.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_event_stream", "source_site": "pydantic_ai"}
{"title": "VercelAIEventStream", "anchor": "vercelaieventstream", "heading_level": 0, "md_text": "UI event stream transformer for the Vercel AI protocol.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_event_stream", "source_site": "pydantic_ai"}
{"title": "_adapter", "anchor": "_adapter", "heading_level": 0, "md_text": "Vercel AI adapter for handling requests.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_adapter", "source_site": "pydantic_ai"}
{"title": "VercelAIAdapter", "anchor": "vercelaiadapter", "heading_level": 0, "md_text": "UI adapter for the Vercel AI protocol.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_adapter", "source_site": "pydantic_ai"}
{"title": "VercelAIAdapter.build_run_input", "anchor": "vercelaiadapter-build_run_input", "heading_level": 0, "md_text": "Build a Vercel AI run input object from the request body.", "url": "https://ai.pydantic.dev/api/build-run-input/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_adapter", "source_site": "pydantic_ai"}
{"title": "VercelAIAdapter.build_event_stream", "anchor": "vercelaiadapter-build_event_stream", "heading_level": 0, "md_text": "Build a Vercel AI event stream transformer.", "url": "https://ai.pydantic.dev/api/build-event-stream/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_adapter", "source_site": "pydantic_ai"}
{"title": "VercelAIAdapter.messages", "anchor": "vercelaiadapter-messages", "heading_level": 0, "md_text": "Pydantic AI messages from the Vercel AI run input.", "url": "https://ai.pydantic.dev/api/messages/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_adapter", "source_site": "pydantic_ai"}
{"title": "VercelAIAdapter.load_messages", "anchor": "vercelaiadapter-load_messages", "heading_level": 0, "md_text": "Transform Vercel AI messages into Pydantic AI messages.", "url": "https://ai.pydantic.dev/api/load-messages/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/_adapter", "source_site": "pydantic_ai"}
{"title": "request_types", "anchor": "request_types", "heading_level": 0, "md_text": "Vercel AI request types (UI messages).\n\nConverted to Python from:\nhttps://github.com/vercel/ai/blob/ai%405.0.59/packages/ai/src/ui/ui-messages.ts", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types", "source_site": "pydantic_ai"}
{"title": "BaseUIPart", "anchor": "baseuipart", "heading_level": 0, "md_text": "Abstract base class for all UI parts.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types", "source_site": "pydantic_ai"}
{"title": "TextUIPart", "anchor": "textuipart", "heading_level": 0, "md_text": "A text part of a message.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types", "source_site": "pydantic_ai"}
{"title": "ReasoningUIPart", "anchor": "reasoninguipart", "heading_level": 0, "md_text": "A reasoning part of a message.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types", "source_site": "pydantic_ai"}
{"title": "SourceUrlUIPart", "anchor": "sourceurluipart", "heading_level": 0, "md_text": "A source part of a message.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types", "source_site": "pydantic_ai"}
{"title": "SourceDocumentUIPart", "anchor": "sourcedocumentuipart", "heading_level": 0, "md_text": "A document source part of a message.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types", "source_site": "pydantic_ai"}
{"title": "FileUIPart", "anchor": "fileuipart", "heading_level": 0, "md_text": "A file part of a message.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types", "source_site": "pydantic_ai"}
{"title": "StepStartUIPart", "anchor": "stepstartuipart", "heading_level": 0, "md_text": "A step boundary part of a message.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types", "source_site": "pydantic_ai"}
{"title": "DataUIPart", "anchor": "datauipart", "heading_level": 0, "md_text": "Data part with dynamic type based on data name.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types", "source_site": "pydantic_ai"}
{"title": "ToolInputStreamingPart", "anchor": "toolinputstreamingpart", "heading_level": 0, "md_text": "Tool part in input-streaming state.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types", "source_site": "pydantic_ai"}
{"title": "ToolInputAvailablePart", "anchor": "toolinputavailablepart", "heading_level": 0, "md_text": "Tool part in input-available state.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types", "source_site": "pydantic_ai"}
{"title": "ToolOutputAvailablePart", "anchor": "tooloutputavailablepart", "heading_level": 0, "md_text": "Tool part in output-available state.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types", "source_site": "pydantic_ai"}
{"title": "ToolOutputErrorPart", "anchor": "tooloutputerrorpart", "heading_level": 0, "md_text": "Tool part in output-error state.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types", "source_site": "pydantic_ai"}
{"title": "DynamicToolInputStreamingPart", "anchor": "dynamictoolinputstreamingpart", "heading_level": 0, "md_text": "Dynamic tool part in input-streaming state.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types", "source_site": "pydantic_ai"}
{"title": "DynamicToolInputAvailablePart", "anchor": "dynamictoolinputavailablepart", "heading_level": 0, "md_text": "Dynamic tool part in input-available state.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types", "source_site": "pydantic_ai"}
{"title": "DynamicToolOutputAvailablePart", "anchor": "dynamictooloutputavailablepart", "heading_level": 0, "md_text": "Dynamic tool part in output-available state.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types", "source_site": "pydantic_ai"}
{"title": "DynamicToolOutputErrorPart", "anchor": "dynamictooloutputerrorpart", "heading_level": 0, "md_text": "Dynamic tool part in output-error state.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types", "source_site": "pydantic_ai"}
{"title": "UIMessage", "anchor": "uimessage", "heading_level": 0, "md_text": "A message as displayed in the UI by Vercel AI Elements.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types", "source_site": "pydantic_ai"}
{"title": "SubmitMessage", "anchor": "submitmessage", "heading_level": 0, "md_text": "Submit message request.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types", "source_site": "pydantic_ai"}
{"title": "RegenerateMessage", "anchor": "regeneratemessage", "heading_level": 0, "md_text": "Ask the agent to regenerate a message.", "url": "https://ai.pydantic.dev/api/", "page": "pydantic_ai_slim/pydantic_ai/ui/vercel_ai/request_types", "source_site": "pydantic_ai"}
