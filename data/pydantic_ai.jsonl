{"title": "Installation and Setup", "anchor": "installation-and-setup", "md_text": "Clone your fork and cd into the repo directory\n\n```\ngit clone git@github.com:<your username>/pydantic-ai.git\ncd pydantic-ai\n```\n\nInstall `uv` (version 0.4.30 or later), `pre-commit` and `deno`:\n\n* [`uv` install docs](https://docs.astral.sh/uv/getting-started/installation/)\n* [`pre-commit` install docs](https://pre-commit.com/#install)\n* [`deno` install docs](https://docs.deno.com/runtime/getting_started/installation/)\n\nTo install `pre-commit` you can run the following command:\n\n```\nuv tool install pre-commit\n```\n\nFor `deno`, you can run the following, or check\n[their documentation](https://docs.deno.com/runtime/getting_started/installation/) for alternative\ninstallation methods:\n\n```\ncurl -fsSL https://deno.land/install.sh | sh\n```\n\nInstall `pydantic-ai`, all dependencies and pre-commit hooks\n\n```\nmake install\n```", "url": "https://ai.pydantic.dev/index.html#installation-and-setup", "page": "index.html", "source_site": "pydantic_ai"}
{"title": "Running Tests etc.", "anchor": "running-tests-etc", "md_text": "We use `make` to manage most commands you'll need to run.\n\nFor details on available commands, run:\n\n```\nmake help\n```\n\nTo run code formatting, linting, static type checks, and tests with coverage report generation, run:\n\n```\nmake\n```", "url": "https://ai.pydantic.dev/index.html#running-tests-etc", "page": "index.html", "source_site": "pydantic_ai"}
{"title": "Documentation Changes", "anchor": "documentation-changes", "md_text": "To run the documentation page locally, run:\n\n```\nuv run mkdocs serve\n```", "url": "https://ai.pydantic.dev/index.html#documentation-changes", "page": "index.html", "source_site": "pydantic_ai"}
{"title": "Rules for adding new models to Pydantic AI", "anchor": "rules-for-adding-new-models-to-pydantic-ai", "md_text": "To avoid an excessive workload for the maintainers of Pydantic AI, we can't accept all model contributions, so we're setting the following rules for when we'll accept new models and when we won't. This should hopefully reduce the chances of disappointment and wasted work.\n\n* To add a new model with an extra dependency, that dependency needs > 500k monthly downloads from PyPI consistently over 3 months or more\n* To add a new model which uses another models logic internally and has no extra dependencies, that model's GitHub org needs > 20k stars in total\n* For any other model that's just a custom URL and API key, we're happy to add a one-paragraph description with a link and instructions on the URL to use\n* For any other model that requires more logic, we recommend you release your own Python package `pydantic-ai-xxx`, which depends on [`pydantic-ai-slim`](https://ai.pydantic.dev/install/#slim-install) and implements a model that inherits from our [`Model`](models/base/index.html#pydantic_ai.models.Model) ABC\n\nIf you're unsure about adding a model, please [create an issue](https://github.com/pydantic/pydantic-ai/issues).", "url": "https://ai.pydantic.dev/index.html#rules-for-adding-new-models-to-pydantic-ai", "page": "index.html", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "md_text": "You need to either install [`pydantic-ai`](https://ai.pydantic.dev/install/), or [`pydantic-ai-slim`](https://ai.pydantic.dev/install/#slim-install) with the `mcp` optional group:\n\npipuv\n\n```\npip install \"pydantic-ai-slim[mcp]\"\n\nuv add \"pydantic-ai-slim[mcp]\"\n```", "url": "https://ai.pydantic.dev/client/index.html#install", "page": "client/index.html", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "md_text": "Pydantic AI comes with three ways to connect to MCP servers:\n\n* [`MCPServerStreamableHTTP`](../mcp/index.html#pydantic_ai.mcp.MCPServerStreamableHTTP) which connects to an MCP server using the [Streamable HTTP](https://modelcontextprotocol.io/introduction#streamable-http) transport\n* [`MCPServerSSE`](../mcp/index.html#pydantic_ai.mcp.MCPServerSSE) which connects to an MCP server using the [HTTP SSE](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse) transport\n* [`MCPServerStdio`](../mcp/index.html#pydantic_ai.mcp.MCPServerStdio) which runs the server as a subprocess and connects to it using the [stdio](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio) transport\n\nExamples of all three are shown below.\n\nEach MCP server instance is a [toolset](https://ai.pydantic.dev/toolsets/) and can be registered with an [`Agent`](../agent/index.html#pydantic_ai.agent.Agent) using the `toolsets` argument.\n\nYou can use the [`async with agent`](../agent/index.html#pydantic_ai.agent.Agent.__aenter__) context manager to open and close connections to all registered servers (and in the case of stdio servers, start and stop the subprocesses) around the context where they'll be used in agent runs. You can also use [`async with server`](../mcp/index.html#pydantic_ai.mcp.MCPServer.__aenter__) to manage the connection or subprocess of a specific server, for example if you'd like to use it with multiple agents. If you don't explicitly enter one of these context managers to set up the server, this will be done automatically when it's needed (e.g. to list the available tools or call a specific tool), but it's more efficient to do so around the entire context where you expect the servers to be used.", "url": "https://ai.pydantic.dev/client/index.html#usage", "page": "client/index.html", "source_site": "pydantic_ai"}
{"title": "Streamable HTTP Client", "anchor": "streamable-http-client", "md_text": "[`MCPServerStreamableHTTP`](../mcp/index.html#pydantic_ai.mcp.MCPServerStreamableHTTP) connects over HTTP using the\n[Streamable HTTP](https://modelcontextprotocol.io/introduction#streamable-http) transport to a server.\n\n[`MCPServerStreamableHTTP`](../mcp/index.html#pydantic_ai.mcp.MCPServerStreamableHTTP) requires an MCP server to be running and accepting HTTP connections before running the agent. Running the server is not managed by Pydantic AI.\n\nBefore creating the Streamable HTTP client, we need to run a server that supports the Streamable HTTP transport.\n\nstreamable\\_http\\_server.py\n\n```\nfrom mcp.server.fastmcp import FastMCP\n\napp = FastMCP()\n\n@app.tool()\ndef add(a: int, b: int) -> int:\n    return a + b\n\nif __name__ == '__main__':\n    app.run(transport='streamable-http')\n```\n\nThen we can create the client:\n\nmcp\\_streamable\\_http\\_client.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStreamableHTTP\n\nserver = MCPServerStreamableHTTP('http://localhost:8000/mcp')  # (1)!\nagent = Agent('openai:gpt-4o', toolsets=[server])  # (2)!\n\nasync def main():\n    result = await agent.run('What is 7 plus 5?')\n    print(result.output)\n    #> The answer is 12.\n```\n\n1. Define the MCP server with the URL used to connect.\n2. Create an agent with the MCP server attached.\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\n**What's happening here?**\n\n* The model receives the prompt \"What is 7 plus 5?\"\n* The model decides \"Oh, I've got this `add` tool, that will be a good way to answer this question\"\n* The model returns a tool call\n* Pydantic AI sends the tool call to the MCP server using the Streamable HTTP transport\n* The model is called again with the return value of running the `add` tool (12)\n* The model returns the final answer\n\nYou can visualise this clearly, and even see the tool call, by adding three lines of code to instrument the example with [logfire](https://logfire.pydantic.dev/docs):\n\nmcp\\_sse\\_client\\_logfire.py\n\n```\nimport logfire\n\nlogfire.configure()\nlogfire.instrument_pydantic_ai()\n```", "url": "https://ai.pydantic.dev/client/index.html#streamable-http-client", "page": "client/index.html", "source_site": "pydantic_ai"}
{"title": "SSE Client", "anchor": "sse-client", "md_text": "[`MCPServerSSE`](../mcp/index.html#pydantic_ai.mcp.MCPServerSSE) connects over HTTP using the [HTTP + Server Sent Events transport](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse) to a server.\n\nThe SSE transport in MCP is deprecated, you should use Streamable HTTP instead.\n\nBefore creating the SSE client, we need to run a server that supports the SSE transport.\n\nsse\\_server.py\n\n```\nfrom mcp.server.fastmcp import FastMCP\n\napp = FastMCP()\n\n@app.tool()\ndef add(a: int, b: int) -> int:\n    return a + b\n\nif __name__ == '__main__':\n    app.run(transport='sse')\n```\n\nThen we can create the client:\n\nmcp\\_sse\\_client.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerSSE\n\nserver = MCPServerSSE('http://localhost:3001/sse')  # (1)!\nagent = Agent('openai:gpt-4o', toolsets=[server])  # (2)!\n\n\nasync def main():\n    result = await agent.run('What is 7 plus 5?')\n    print(result.output)\n    #> The answer is 12.\n```\n\n1. Define the MCP server with the URL used to connect.\n2. Create an agent with the MCP server attached.\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/client/index.html#sse-client", "page": "client/index.html", "source_site": "pydantic_ai"}
{"title": "MCP \"stdio\" Server", "anchor": "mcp-stdio-server", "md_text": "MCP also offers [stdio transport](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio) where the server is run as a subprocess and communicates with the client over `stdin` and `stdout`. In this case, you'd use the [`MCPServerStdio`](../mcp/index.html#pydantic_ai.mcp.MCPServerStdio) class.\n\nIn this example [mcp-run-python](https://github.com/pydantic/mcp-run-python) is used as the MCP server.\n\nmcp\\_stdio\\_client.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\n\nserver = MCPServerStdio(  # (1)!\n    'uv', args=['run', 'mcp-run-python', 'stdio'], timeout=10\n)\nagent = Agent('openai:gpt-4o', toolsets=[server])\n\n\nasync def main():\n    result = await agent.run('How many days between 2000-01-01 and 2025-03-18?')\n    print(result.output)\n    #> There are 9,208 days between January 1, 2000, and March 18, 2025.\n```\n\n1. See [MCP Run Python](https://github.com/pydantic/mcp-run-python) for more information.", "url": "https://ai.pydantic.dev/client/index.html#mcp-stdio-server", "page": "client/index.html", "source_site": "pydantic_ai"}
{"title": "Loading MCP Servers from Configuration", "anchor": "loading-mcp-servers-from-configuration", "md_text": "Instead of creating MCP server instances individually in code, you can load multiple servers from a JSON configuration file using [`load_mcp_servers()`](../mcp/index.html#pydantic_ai.mcp.load_mcp_servers).\n\nThis is particularly useful when you need to manage multiple MCP servers or want to configure servers externally without modifying code.", "url": "https://ai.pydantic.dev/client/index.html#loading-mcp-servers-from-configuration", "page": "client/index.html", "source_site": "pydantic_ai"}
{"title": "Configuration Format", "anchor": "configuration-format", "md_text": "The configuration file should be a JSON file with an `mcpServers` object containing server definitions. Each server is identified by a unique key and contains the configuration for that server type:\n\nmcp\\_config.json\n\n```\n{\n  \"mcpServers\": {\n    \"python-runner\": {\n      \"command\": \"uv\",\n      \"args\": [\"run\", \"mcp-run-python\", \"stdio\"]\n    },\n    \"weather-api\": {\n      \"url\": \"http://localhost:3001/sse\"\n    },\n    \"calculator\": {\n      \"url\": \"http://localhost:8000/mcp\"\n    }\n  }\n}\n```\n\nThe MCP server is only inferred to be an SSE server because of the `/sse` suffix.\nAny other server with the \"url\" field will be inferred to be a Streamable HTTP server.\n\nWe made this decision given that the SSE transport is deprecated.", "url": "https://ai.pydantic.dev/client/index.html#configuration-format", "page": "client/index.html", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "md_text": "mcp\\_config\\_loader.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import load_mcp_servers\n\n# Load all servers from configuration file\nservers = load_mcp_servers('mcp_config.json')\n\n# Create agent with all loaded servers\nagent = Agent('openai:gpt-5', toolsets=servers)\n\nasync def main():\n    result = await agent.run('What is 7 plus 5?')\n    print(result.output)\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/client/index.html#usage", "page": "client/index.html", "source_site": "pydantic_ai"}
{"title": "Tool call customization", "anchor": "tool-call-customization", "md_text": "The MCP servers provide the ability to set a `process_tool_call` which allows\nthe customization of tool call requests and their responses.\n\nA common use case for this is to inject metadata to the requests which the server\ncall needs:\n\nmcp\\_process\\_tool\\_call.py\n\n```\nfrom typing import Any\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.mcp import CallToolFunc, MCPServerStdio, ToolResult\nfrom pydantic_ai.models.test import TestModel\n\n\nasync def process_tool_call(\n    ctx: RunContext[int],\n    call_tool: CallToolFunc,\n    name: str,\n    tool_args: dict[str, Any],\n) -> ToolResult:\n    \"\"\"A tool call processor that passes along the deps.\"\"\"\n    return await call_tool(name, tool_args, {'deps': ctx.deps})\n\n\nserver = MCPServerStdio('python', args=['mcp_server.py'], process_tool_call=process_tool_call)\nagent = Agent(\n    model=TestModel(call_tools=['echo_deps']),\n    deps_type=int,\n    toolsets=[server]\n)\n\n\nasync def main():\n    result = await agent.run('Echo with deps set to 42', deps=42)\n    print(result.output)\n    #> {\"echo_deps\":{\"echo\":\"This is an echo message\",\"deps\":42}}\n```\n\nHow to access the metadata is MCP server SDK specific. For example with the [MCP Python\nSDK](https://github.com/modelcontextprotocol/python-sdk), it is accessible via the\n[`ctx: Context`](https://github.com/modelcontextprotocol/python-sdk#context)\nargument that can be included on tool call handlers:\n\nmcp\\_server.py\n\n```\nfrom typing import Any\n\nfrom mcp.server.fastmcp import Context, FastMCP\nfrom mcp.server.session import ServerSession\n\nmcp = FastMCP('Pydantic AI MCP Server')\nlog_level = 'unset'\n\n\n@mcp.tool()\nasync def echo_deps(ctx: Context[ServerSession, None]) -> dict[str, Any]:\n    \"\"\"Echo the run context.\n\n    Args:\n        ctx: Context object containing request and session information.\n\n    Returns:\n        Dictionary with an echo message and the deps.\n    \"\"\"\n    await ctx.info('This is an info message')\n\n    deps: Any = getattr(ctx.request_context.meta, 'deps')\n    return {'echo': 'This is an echo message', 'deps': deps}\n\nif __name__ == '__main__':\n    mcp.run()\n```", "url": "https://ai.pydantic.dev/client/index.html#tool-call-customization", "page": "client/index.html", "source_site": "pydantic_ai"}
{"title": "Using Tool Prefixes to Avoid Naming Conflicts", "anchor": "using-tool-prefixes-to-avoid-naming-conflicts", "md_text": "When connecting to multiple MCP servers that might provide tools with the same name, you can use the `tool_prefix` parameter to avoid naming conflicts. This parameter adds a prefix to all tool names from a specific server.\n\nThis allows you to use multiple servers that might have overlapping tool names without conflicts:\n\nmcp\\_tool\\_prefix\\_http\\_client.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerSSE\n\n# Create two servers with different prefixes\nweather_server = MCPServerSSE(\n    'http://localhost:3001/sse',\n    tool_prefix='weather'  # Tools will be prefixed with 'weather_'\n)\n\ncalculator_server = MCPServerSSE(\n    'http://localhost:3002/sse',\n    tool_prefix='calc'  # Tools will be prefixed with 'calc_'\n)\n\n# Both servers might have a tool named 'get_data', but they'll be exposed as:\n# - 'weather_get_data'\n# - 'calc_get_data'\nagent = Agent('openai:gpt-4o', toolsets=[weather_server, calculator_server])\n```", "url": "https://ai.pydantic.dev/client/index.html#using-tool-prefixes-to-avoid-naming-conflicts", "page": "client/index.html", "source_site": "pydantic_ai"}
{"title": "Tool metadata", "anchor": "tool-metadata", "md_text": "MCP tools can include metadata that provides additional information about the tool's characteristics, which can be useful when [filtering tools](../toolsets/index.html#pydantic_ai.toolsets.FilteredToolset). The `meta`, `annotations`, and `output_schema` fields can be found on the `metadata` dict on the [`ToolDefinition`](../tools/index.html#pydantic_ai.tools.ToolDefinition) object that's passed to filter functions.", "url": "https://ai.pydantic.dev/client/index.html#tool-metadata", "page": "client/index.html", "source_site": "pydantic_ai"}
{"title": "Custom TLS / SSL configuration", "anchor": "custom-tls--ssl-configuration", "md_text": "In some environments you need to tweak how HTTPS connections are established –\nfor example to trust an internal Certificate Authority, present a client\ncertificate for **mTLS**, or (during local development only!) disable\ncertificate verification altogether.\nAll HTTP-based MCP client classes\n([`MCPServerStreamableHTTP`](../mcp/index.html#pydantic_ai.mcp.MCPServerStreamableHTTP) and\n[`MCPServerSSE`](../mcp/index.html#pydantic_ai.mcp.MCPServerSSE)) expose an `http_client`\nparameter that lets you pass your own pre-configured\n[`httpx.AsyncClient`](https://www.python-httpx.org/async/).\n\nmcp\\_custom\\_tls\\_client.py\n\n```\nimport ssl\n\nimport httpx\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerSSE\n\n# Trust an internal / self-signed CA\nssl_ctx = ssl.create_default_context(cafile='/etc/ssl/private/my_company_ca.pem')\n\n# OPTIONAL: if the server requires **mutual TLS** load your client certificate\nssl_ctx.load_cert_chain(certfile='/etc/ssl/certs/client.crt', keyfile='/etc/ssl/private/client.key',)\n\nhttp_client = httpx.AsyncClient(\n    verify=ssl_ctx,\n    timeout=httpx.Timeout(10.0),\n)\n\nserver = MCPServerSSE(\n    'http://localhost:3001/sse',\n    http_client=http_client,  # (1)!\n)\nagent = Agent('openai:gpt-4o', toolsets=[server])\n\nasync def main():\n    result = await agent.run('How many days between 2000-01-01 and 2025-03-18?')\n    print(result.output)\n    #> There are 9,208 days between January 1, 2000, and March 18, 2025.\n```\n\n1. When you supply `http_client`, Pydantic AI re-uses this client for every\n   request. Anything supported by **httpx** (`verify`, `cert`, custom\n   proxies, timeouts, etc.) therefore applies to all MCP traffic.", "url": "https://ai.pydantic.dev/client/index.html#custom-tls--ssl-configuration", "page": "client/index.html", "source_site": "pydantic_ai"}
{"title": "MCP Sampling", "anchor": "mcp-sampling", "md_text": "In MCP [sampling](https://modelcontextprotocol.io/docs/concepts/sampling) is a system by which an MCP server can make LLM calls via the MCP client - effectively proxying requests to an LLM via the client over whatever transport is being used.\n\nSampling is extremely useful when MCP servers need to use Gen AI but you don't want to provision them each with their own LLM credentials or when a public MCP server would like the connecting client to pay for LLM calls.\n\nConfusingly it has nothing to do with the concept of \"sampling\" in observability, or frankly the concept of \"sampling\" in any other domain.\n\nSampling Diagram\n\nHere's a mermaid diagram that may or may not make the data flow clearer:\n\n```\nsequenceDiagram\n    participant LLM\n    participant MCP_Client as MCP client\n    participant MCP_Server as MCP server\n\n    MCP_Client->>LLM: LLM call\n    LLM->>MCP_Client: LLM tool call response\n\n    MCP_Client->>MCP_Server: tool call\n    MCP_Server->>MCP_Client: sampling \"create message\"\n\n    MCP_Client->>LLM: LLM call\n    LLM->>MCP_Client: LLM text response\n\n    MCP_Client->>MCP_Server: sampling response\n    MCP_Server->>MCP_Client: tool call response\n```\n\nPydantic AI supports sampling as both a client and server. See the [server](../server/index.html#mcp-sampling) documentation for details on how to use sampling within a server.\n\nSampling is automatically supported by Pydantic AI agents when they act as a client.\n\nTo be able to use sampling, an MCP server instance needs to have a [`sampling_model`](../mcp/index.html#pydantic_ai.mcp.MCPServer.sampling_model) set. This can be done either directly on the server using the constructor keyword argument or the property, or by using [`agent.set_mcp_sampling_model()`](../agent/index.html#pydantic_ai.agent.Agent.set_mcp_sampling_model) to set the agent's model or one specified as an argument as the sampling model on all MCP servers registered with that agent.\n\nLet's say we have an MCP server that wants to use sampling (in this case to generate an SVG as per the tool arguments).\n\nSampling MCP Server\n\ngenerate\\_svg.py\n\n```\nimport re\nfrom pathlib import Path\n\nfrom mcp import SamplingMessage\nfrom mcp.server.fastmcp import Context, FastMCP\nfrom mcp.types import TextContent\n\napp = FastMCP()\n\n\n@app.tool()\nasync def image_generator(ctx: Context, subject: str, style: str) -> str:\n    prompt = f'{subject=} {style=}'\n    # `ctx.session.create_message` is the sampling call\n    result = await ctx.session.create_message(\n        [SamplingMessage(role='user', content=TextContent(type='text', text=prompt))],\n        max_tokens=1_024,\n        system_prompt='Generate an SVG image as per the user input',\n    )\n    assert isinstance(result.content, TextContent)\n\n    path = Path(f'{subject}_{style}.svg')\n    # remove triple backticks if the svg was returned within markdown\n    if m := re.search(r'^```\\w*$(.+?)```$', result.content.text, re.S | re.M):\n        path.write_text(m.group(1))\n    else:\n        path.write_text(result.content.text)\n    return f'See {path}'\n\n\nif __name__ == '__main__':\n    # run the server via stdio\n    app.run()\n```\n\nUsing this server with an `Agent` will automatically allow sampling:\n\nsampling\\_mcp\\_client.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\n\nserver = MCPServerStdio('python', args=['generate_svg.py'])\nagent = Agent('openai:gpt-4o', toolsets=[server])\n\n\nasync def main():\n    agent.set_mcp_sampling_model()\n    result = await agent.run('Create an image of a robot in a punk style.')\n    print(result.output)\n    #> Image file written to robot_punk.svg.\n```\n\n*(This example is complete, it can be run \"as is\")*\n\nYou can disallow sampling by setting [`allow_sampling=False`](../mcp/index.html#pydantic_ai.mcp.MCPServer.allow_sampling) when creating the server reference, e.g.:\n\nsampling\\_disallowed.py\n\n```\nfrom pydantic_ai.mcp import MCPServerStdio\n\nserver = MCPServerStdio(\n    'python',\n    args=['generate_svg.py'],\n    allow_sampling=False,\n)\n```", "url": "https://ai.pydantic.dev/client/index.html#mcp-sampling", "page": "client/index.html", "source_site": "pydantic_ai"}
{"title": "Elicitation", "anchor": "elicitation", "md_text": "In MCP, [elicitation](https://modelcontextprotocol.io/docs/concepts/elicitation) allows a server to request for [structured input](https://modelcontextprotocol.io/specification/2025-06-18/client/elicitation#supported-schema-types) from the client for missing or additional context during a session.\n\nElicitation let models essentially say \"Hold on - I need to know X before i can continue\" rather than requiring everything upfront or taking a shot in the dark.", "url": "https://ai.pydantic.dev/client/index.html#elicitation", "page": "client/index.html", "source_site": "pydantic_ai"}
{"title": "How Elicitation works", "anchor": "how-elicitation-works", "md_text": "Elicitation introduces a new protocol message type called [`ElicitRequest`](https://modelcontextprotocol.io/specification/2025-06-18/schema#elicitrequest), which is sent from the server to the client when it needs additional information. The client can then respond with an [`ElicitResult`](https://modelcontextprotocol.io/specification/2025-06-18/schema#elicitresult) or an `ErrorData` message.\n\nHere's a typical interaction:\n\n* User makes a request to the MCP server (e.g. \"Book a table at that Italian place\")\n* The server identifies that it needs more information (e.g. \"Which Italian place?\", \"What date and time?\")\n* The server sends an `ElicitRequest` to the client asking for the missing information.\n* The client receives the request, presents it to the user (e.g. via a terminal prompt, GUI dialog, or web interface).\n* User provides the requested information, `decline` or `cancel` the request.\n* The client sends an `ElicitResult` back to the server with the user's response.\n* With the structured data, the server can continue processing the original request.\n\nThis allows for a more interactive and user-friendly experience, especially for multi-staged workflows. Instead of requiring all information upfront, the server can ask for it as needed, making the interaction feel more natural.", "url": "https://ai.pydantic.dev/client/index.html#how-elicitation-works", "page": "client/index.html", "source_site": "pydantic_ai"}
{"title": "Setting up Elicitation", "anchor": "setting-up-elicitation", "md_text": "To enable elicitation, provide an [`elicitation_callback`](../mcp/index.html#pydantic_ai.mcp.MCPServer.elicitation_callback) function when creating your MCP server instance:\n\nrestaurant\\_server.py\n\n```\nfrom mcp.server.fastmcp import Context, FastMCP\nfrom pydantic import BaseModel, Field\n\nmcp = FastMCP(name='Restaurant Booking')\n\n\nclass BookingDetails(BaseModel):\n    \"\"\"Schema for restaurant booking information.\"\"\"\n\n    restaurant: str = Field(description='Choose a restaurant')\n    party_size: int = Field(description='Number of people', ge=1, le=8)\n    date: str = Field(description='Reservation date (DD-MM-YYYY)')\n\n\n@mcp.tool()\nasync def book_table(ctx: Context) -> str:\n    \"\"\"Book a restaurant table with user input.\"\"\"\n    # Ask user for booking details using Pydantic schema\n    result = await ctx.elicit(message='Please provide your booking details:', schema=BookingDetails)\n\n    if result.action == 'accept' and result.data:\n        booking = result.data\n        return f'✅ Booked table for {booking.party_size} at {booking.restaurant} on {booking.date}'\n    elif result.action == 'decline':\n        return 'No problem! Maybe another time.'\n    else:  # cancel\n        return 'Booking cancelled.'\n\n\nif __name__ == '__main__':\n    mcp.run(transport='stdio')\n```\n\nThis server demonstrates elicitation by requesting structured booking details from the client when the `book_table` tool is called. Here's how to create a client that handles these elicitation requests:\n\nclient\\_example.py\n\n```\nimport asyncio\nfrom typing import Any\n\nfrom mcp.client.session import ClientSession\nfrom mcp.shared.context import RequestContext\nfrom mcp.types import ElicitRequestParams, ElicitResult\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\n\n\nasync def handle_elicitation(\n    context: RequestContext[ClientSession, Any, Any],\n    params: ElicitRequestParams,\n) -> ElicitResult:\n    \"\"\"Handle elicitation requests from MCP server.\"\"\"\n    print(f'\\n{params.message}')\n\n    if not params.requestedSchema:\n        response = input('Response: ')\n        return ElicitResult(action='accept', content={'response': response})\n\n    # Collect data for each field\n    properties = params.requestedSchema['properties']\n    data = {}\n\n    for field, info in properties.items():\n        description = info.get('description', field)\n\n        value = input(f'{description}: ')\n\n        # Convert to proper type based on JSON schema\n        if info.get('type') == 'integer':\n            data[field] = int(value)\n        else:\n            data[field] = value\n\n    # Confirm\n    confirm = input('\\nConfirm booking? (y/n/c): ').lower()\n\n    if confirm == 'y':\n        print('Booking details:', data)\n        return ElicitResult(action='accept', content=data)\n    elif confirm == 'n':\n        return ElicitResult(action='decline')\n    else:\n        return ElicitResult(action='cancel')\n\n\n# Set up MCP server connection\nrestaurant_server = MCPServerStdio(\n    'python', args=['restaurant_server.py'], elicitation_callback=handle_elicitation\n)\n\n# Create agent\nagent = Agent('openai:gpt-4o', toolsets=[restaurant_server])\n\n\nasync def main():\n    \"\"\"Run the agent to book a restaurant table.\"\"\"\n    result = await agent.run('Book me a table')\n    print(f'\\nResult: {result.output}')\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```", "url": "https://ai.pydantic.dev/client/index.html#setting-up-elicitation", "page": "client/index.html", "source_site": "pydantic_ai"}
{"title": "Supported Schema Types", "anchor": "supported-schema-types", "md_text": "MCP elicitation supports string, number, boolean, and enum types with flat object structures only. These limitations ensure reliable cross-client compatibility. See [supported schema types](https://modelcontextprotocol.io/specification/2025-06-18/client/elicitation#supported-schema-types) for details.", "url": "https://ai.pydantic.dev/client/index.html#supported-schema-types", "page": "client/index.html", "source_site": "pydantic_ai"}
{"title": "Security", "anchor": "security", "md_text": "MCP Elicitation requires careful handling - servers must not request sensitive information, and clients must implement user approval controls with clear explanations. See [security considerations](https://modelcontextprotocol.io/specification/2025-06-18/client/elicitation#security-considerations) for details.", "url": "https://ai.pydantic.dev/client/index.html#security", "page": "client/index.html", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "md_text": "The resulting SQL is validated by running it as an `EXPLAIN` query on PostgreSQL. To run the example, you first need to run PostgreSQL, e.g. via Docker:\n\n```\ndocker run --rm -e POSTGRES_PASSWORD=postgres -p 54320:5432 postgres\n```\n\n*(we run postgres on port `54320` to avoid conflicts with any other postgres instances you may have running)*\n\nWith [dependencies installed and environment variables set](../setup/index.html#usage), run:\n\npipuv\n\n```\npython -m pydantic_ai_examples.sql_gen\n\nuv run -m pydantic_ai_examples.sql_gen\n```\n\nor to use a custom prompt:\n\npipuv\n\n```\npython -m pydantic_ai_examples.sql_gen \"find me errors\"\n\nuv run -m pydantic_ai_examples.sql_gen \"find me errors\"\n```\n\nThis model uses `gemini-1.5-flash` by default since Gemini is good at single shot queries of this kind.", "url": "https://ai.pydantic.dev/sql-gen/index.html#running-the-example", "page": "sql-gen/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "md_text": "[sql\\_gen.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/sql_gen.py)\n\n```\n\"\"\"Example demonstrating how to use Pydantic AI to generate SQL queries based on user input.\n\nRun postgres with:\n\n    mkdir postgres-data\n    docker run --rm -e POSTGRES_PASSWORD=postgres -p 54320:5432 postgres\n\nRun with:\n\n    uv run -m pydantic_ai_examples.sql_gen \"show me logs from yesterday, with level 'error'\"\n\"\"\"\n\nimport asyncio\nimport sys\nfrom collections.abc import AsyncGenerator\nfrom contextlib import asynccontextmanager\nfrom dataclasses import dataclass\nfrom datetime import date\nfrom typing import Annotated, Any, TypeAlias\n\nimport asyncpg\nimport logfire\nfrom annotated_types import MinLen\nfrom devtools import debug\nfrom pydantic import BaseModel, Field\n\nfrom pydantic_ai import Agent, ModelRetry, RunContext, format_as_xml\n\n# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\nlogfire.configure(send_to_logfire='if-token-present')\nlogfire.instrument_asyncpg()\nlogfire.instrument_pydantic_ai()\n\nDB_SCHEMA = \"\"\"\nCREATE TABLE records (\n    created_at timestamptz,\n    start_timestamp timestamptz,\n    end_timestamp timestamptz,\n    trace_id text,\n    span_id text,\n    parent_span_id text,\n    level log_level,\n    span_name text,\n    message text,\n    attributes_json_schema text,\n    attributes jsonb,\n    tags text[],\n    is_exception boolean,\n    otel_status_message text,\n    service_name text\n);\n\"\"\"\nSQL_EXAMPLES = [\n    {\n        'request': 'show me records where foobar is false',\n        'response': \"SELECT * FROM records WHERE attributes->>'foobar' = false\",\n    },\n    {\n        'request': 'show me records where attributes include the key \"foobar\"',\n        'response': \"SELECT * FROM records WHERE attributes ? 'foobar'\",\n    },\n    {\n        'request': 'show me records from yesterday',\n        'response': \"SELECT * FROM records WHERE start_timestamp::date > CURRENT_TIMESTAMP - INTERVAL '1 day'\",\n    },\n    {\n        'request': 'show me error records with the tag \"foobar\"',\n        'response': \"SELECT * FROM records WHERE level = 'error' and 'foobar' = ANY(tags)\",\n    },\n]\n\n\n@dataclass\nclass Deps:\n    conn: asyncpg.Connection\n\n\nclass Success(BaseModel):\n    \"\"\"Response when SQL could be successfully generated.\"\"\"\n\n    sql_query: Annotated[str, MinLen(1)]\n    explanation: str = Field(\n        '', description='Explanation of the SQL query, as markdown'\n    )\n\n\nclass InvalidRequest(BaseModel):\n    \"\"\"Response the user input didn't include enough information to generate SQL.\"\"\"\n\n    error_message: str\n\n\nResponse: TypeAlias = Success | InvalidRequest\nagent = Agent[Deps, Response](\n    'google-gla:gemini-1.5-flash',\n    # Type ignore while we wait for PEP-0747, nonetheless unions will work fine everywhere else\n    output_type=Response,  # type: ignore\n    deps_type=Deps,\n)\n\n\n@agent.system_prompt\nasync def system_prompt() -> str:\n    return f\"\"\"\\\nGiven the following PostgreSQL table of records, your job is to\nwrite a SQL query that suits the user's request.\n\nDatabase schema:\n\n{DB_SCHEMA}\n\ntoday's date = {date.today()}\n\n{format_as_xml(SQL_EXAMPLES)}\n\"\"\"\n\n\n@agent.output_validator\nasync def validate_output(ctx: RunContext[Deps], output: Response) -> Response:\n    if isinstance(output, InvalidRequest):\n        return output\n\n    # gemini often adds extraneous backslashes to SQL\n    output.sql_query = output.sql_query.replace('\\\\', '')\n    if not output.sql_query.upper().startswith('SELECT'):\n        raise ModelRetry('Please create a SELECT query')\n\n    try:\n        await ctx.deps.conn.execute(f'EXPLAIN {output.sql_query}')\n    except asyncpg.exceptions.PostgresError as e:\n        raise ModelRetry(f'Invalid query: {e}') from e\n    else:\n        return output\n\n\nasync def main():\n    if len(sys.argv) == 1:\n        prompt = 'show me logs from yesterday, with level \"error\"'\n    else:\n        prompt = sys.argv[1]\n\n    async with database_connect(\n        'postgresql://postgres:postgres@localhost:54320', 'pydantic_ai_sql_gen'\n    ) as conn:\n        deps = Deps(conn)\n        result = await agent.run(prompt, deps=deps)\n    debug(result.output)\n\n\n# pyright: reportUnknownMemberType=false\n# pyright: reportUnknownVariableType=false\n@asynccontextmanager\nasync def database_connect(server_dsn: str, database: str) -> AsyncGenerator[Any, None]:\n    with logfire.span('check and create DB'):\n        conn = await asyncpg.connect(server_dsn)\n        try:\n            db_exists = await conn.fetchval(\n                'SELECT 1 FROM pg_database WHERE datname = $1', database\n            )\n            if not db_exists:\n                await conn.execute(f'CREATE DATABASE {database}')\n        finally:\n            await conn.close()\n\n    conn = await asyncpg.connect(f'{server_dsn}/{database}')\n    try:\n        with logfire.span('create schema'):\n            async with conn.transaction():\n                if not db_exists:\n                    await conn.execute(\n                        \"CREATE TYPE log_level AS ENUM ('debug', 'info', 'warning', 'error', 'critical')\"\n                    )\n                    await conn.execute(DB_SCHEMA)\n        yield conn\n    finally:\n        await conn.close()", "url": "https://ai.pydantic.dev/sql-gen/index.html#example-code", "page": "sql-gen/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "md_text": "\nif __name__ == '__main__':\n    asyncio.run(main())\n```", "url": "https://ai.pydantic.dev/sql-gen/index.html#example-code", "page": "sql-gen/index.html", "source_site": "pydantic_ai"}
{"title": "AgentDepsT `module-attribute`", "anchor": "agentdepst-module-attribute", "md_text": "```\nAgentDepsT = TypeVar(\n    \"AgentDepsT\", default=None, contravariant=True\n)\n```\n\nType variable for agent dependencies.", "url": "https://ai.pydantic.dev/tools/index.html#agentdepst-module-attribute", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "RunContext `dataclass`", "anchor": "runcontext-dataclass", "md_text": "Bases: `Generic[AgentDepsT]`\n\nInformation about the current call.\n\nSource code in `pydantic_ai_slim/pydantic_ai/_run_context.py`\n\n|  |  |\n| --- | --- |\n| ``` 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 ``` | ``` @dataclasses.dataclass(repr=False, kw_only=True) class RunContext(Generic[AgentDepsT]):     \"\"\"Information about the current call.\"\"\"      deps: AgentDepsT     \"\"\"Dependencies for the agent.\"\"\"     model: Model     \"\"\"The model used in this run.\"\"\"     usage: RunUsage     \"\"\"LLM usage associated with the run.\"\"\"     prompt: str | Sequence[_messages.UserContent] | None = None     \"\"\"The original user prompt passed to the run.\"\"\"     messages: list[_messages.ModelMessage] = field(default_factory=list)     \"\"\"Messages exchanged in the conversation so far.\"\"\"     tracer: Tracer = field(default_factory=NoOpTracer)     \"\"\"The tracer to use for tracing the run.\"\"\"     trace_include_content: bool = False     \"\"\"Whether to include the content of the messages in the trace.\"\"\"     instrumentation_version: int = DEFAULT_INSTRUMENTATION_VERSION     \"\"\"Instrumentation settings version, if instrumentation is enabled.\"\"\"     retries: dict[str, int] = field(default_factory=dict)     \"\"\"Number of retries for each tool so far.\"\"\"     tool_call_id: str | None = None     \"\"\"The ID of the tool call.\"\"\"     tool_name: str | None = None     \"\"\"Name of the tool being called.\"\"\"     retry: int = 0     \"\"\"Number of retries of this tool so far.\"\"\"     max_retries: int = 0     \"\"\"The maximum number of retries of this tool.\"\"\"     run_step: int = 0     \"\"\"The current step in the run.\"\"\"     tool_call_approved: bool = False     \"\"\"Whether a tool call that required approval has now been approved.\"\"\"      @property     def last_attempt(self) -> bool:         \"\"\"Whether this is the last attempt at running this tool before an error is raised.\"\"\"         return self.retry == self.max_retries      __repr__ = _utils.dataclasses_no_defaults_repr ``` |\n\n#### deps `instance-attribute`\n\n```\ndeps: AgentDepsT\n```\n\nDependencies for the agent.\n\n#### model `instance-attribute`\n\n```\nmodel: Model\n```\n\nThe model used in this run.\n\n#### usage `instance-attribute`\n\n```\nusage: RunUsage\n```\n\nLLM usage associated with the run.\n\n#### prompt `class-attribute` `instance-attribute`\n\n```\nprompt: str | Sequence[UserContent] | None = None\n```\n\nThe original user prompt passed to the run.\n\n#### messages `class-attribute` `instance-attribute`\n\n```\nmessages: list[ModelMessage] = field(default_factory=list)\n```\n\nMessages exchanged in the conversation so far.\n\n#### tracer `class-attribute` `instance-attribute`\n\n```\ntracer: Tracer = field(default_factory=NoOpTracer)\n```\n\nThe tracer to use for tracing the run.\n\n#### trace\\_include\\_content `class-attribute` `instance-attribute`\n\n```\ntrace_include_content: bool = False\n```\n\nWhether to include the content of the messages in the trace.\n\n#### instrumentation\\_version `class-attribute` `instance-attribute`\n\n```\ninstrumentation_version: int = (\n    DEFAULT_INSTRUMENTATION_VERSION\n)\n```\n\nInstrumentation settings version, if instrumentation is enabled.\n\n#### retries `class-attribute` `instance-attribute`\n\n```\nretries: dict[str, int] = field(default_factory=dict)\n```\n\nNumber of retries for each tool so far.\n\n#### tool\\_call\\_id `class-attribute` `instance-attribute`\n\n```\ntool_call_id: str | None = None\n```\n\nThe ID of the tool call.\n\n#### tool\\_name `class-attribute` `instance-attribute`\n\n```\ntool_name: str | None = None\n```\n\nName of the tool being called.\n\n#### retry `class-attribute` `instance-attribute`\n\n```\nretry: int = 0\n```\n\nNumber of retries of this tool so far.\n\n#### max\\_retries `class-attribute` `instance-attribute`\n\n```\nmax_retries: int = 0\n```\n\nThe maximum number of retries of this tool.\n\n#### run\\_step `class-attribute` `instance-attribute`\n\n```\nrun_step: int = 0\n```\n\nThe current step in the run.\n\n#### tool\\_call\\_approved `class-attribute` `instance-attribute`\n\n```\ntool_call_approved: bool = False\n```\n\nWhether a tool call that required approval has now been approved.\n\n#### last\\_attempt `property`\n\n```\nlast_attempt: bool\n```\n\nWhether this is the last attempt at running this tool before an error is raised.", "url": "https://ai.pydantic.dev/tools/index.html#runcontext-dataclass", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "ToolParams `module-attribute`", "anchor": "toolparams-module-attribute", "md_text": "```\nToolParams = ParamSpec('ToolParams', default=...)\n```\n\nRetrieval function param spec.", "url": "https://ai.pydantic.dev/tools/index.html#toolparams-module-attribute", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "SystemPromptFunc `module-attribute`", "anchor": "systempromptfunc-module-attribute", "md_text": "```\nSystemPromptFunc: TypeAlias = (\n    Callable[[RunContext[AgentDepsT]], str]\n    | Callable[[RunContext[AgentDepsT]], Awaitable[str]]\n    | Callable[[], str]\n    | Callable[[], Awaitable[str]]\n)\n```\n\nA function that may or maybe not take `RunContext` as an argument, and may or may not be async.\n\nUsage `SystemPromptFunc[AgentDepsT]`.", "url": "https://ai.pydantic.dev/tools/index.html#systempromptfunc-module-attribute", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "ToolFuncContext `module-attribute`", "anchor": "toolfunccontext-module-attribute", "md_text": "```\nToolFuncContext: TypeAlias = Callable[\n    Concatenate[RunContext[AgentDepsT], ToolParams], Any\n]\n```\n\nA tool function that takes `RunContext` as the first argument.\n\nUsage `ToolContextFunc[AgentDepsT, ToolParams]`.", "url": "https://ai.pydantic.dev/tools/index.html#toolfunccontext-module-attribute", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "ToolFuncPlain `module-attribute`", "anchor": "toolfuncplain-module-attribute", "md_text": "```\nToolFuncPlain: TypeAlias = Callable[ToolParams, Any]\n```\n\nA tool function that does not take `RunContext` as the first argument.\n\nUsage `ToolPlainFunc[ToolParams]`.", "url": "https://ai.pydantic.dev/tools/index.html#toolfuncplain-module-attribute", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "ToolFuncEither `module-attribute`", "anchor": "toolfunceither-module-attribute", "md_text": "```\nToolFuncEither: TypeAlias = (\n    ToolFuncContext[AgentDepsT, ToolParams]\n    | ToolFuncPlain[ToolParams]\n)\n```\n\nEither kind of tool function.\n\nThis is just a union of [`ToolFuncContext`](index.html#pydantic_ai.tools.ToolFuncContext) and\n[`ToolFuncPlain`](index.html#pydantic_ai.tools.ToolFuncPlain).\n\nUsage `ToolFuncEither[AgentDepsT, ToolParams]`.", "url": "https://ai.pydantic.dev/tools/index.html#toolfunceither-module-attribute", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "ToolPrepareFunc `module-attribute`", "anchor": "toolpreparefunc-module-attribute", "md_text": "```\nToolPrepareFunc: TypeAlias = Callable[\n    [RunContext[AgentDepsT], \"ToolDefinition\"],\n    Awaitable[\"ToolDefinition | None\"],\n]\n```\n\nDefinition of a function that can prepare a tool definition at call time.\n\nSee [tool docs](https://ai.pydantic.dev/tools-advanced/#tool-prepare) for more information.\n\nExample — here `only_if_42` is valid as a `ToolPrepareFunc`:\n\n```\nfrom pydantic_ai import RunContext, Tool\nfrom pydantic_ai.tools import ToolDefinition\n\nasync def only_if_42(\n    ctx: RunContext[int], tool_def: ToolDefinition\n) -> ToolDefinition | None:\n    if ctx.deps == 42:\n        return tool_def\n\ndef hitchhiker(ctx: RunContext[int], answer: str) -> str:\n    return f'{ctx.deps} {answer}'\n\nhitchhiker = Tool(hitchhiker, prepare=only_if_42)\n```\n\nUsage `ToolPrepareFunc[AgentDepsT]`.", "url": "https://ai.pydantic.dev/tools/index.html#toolpreparefunc-module-attribute", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "ToolsPrepareFunc `module-attribute`", "anchor": "toolspreparefunc-module-attribute", "md_text": "```\nToolsPrepareFunc: TypeAlias = Callable[\n    [RunContext[AgentDepsT], list[\"ToolDefinition\"]],\n    Awaitable[\"list[ToolDefinition] | None\"],\n]\n```\n\nDefinition of a function that can prepare the tool definition of all tools for each step.\nThis is useful if you want to customize the definition of multiple tools or you want to register\na subset of tools for a given step.\n\nExample — here `turn_on_strict_if_openai` is valid as a `ToolsPrepareFunc`:\n\n```\nfrom dataclasses import replace\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.tools import ToolDefinition\n\n\nasync def turn_on_strict_if_openai(\n    ctx: RunContext[None], tool_defs: list[ToolDefinition]\n) -> list[ToolDefinition] | None:\n    if ctx.model.system == 'openai':\n        return [replace(tool_def, strict=True) for tool_def in tool_defs]\n    return tool_defs\n\nagent = Agent('openai:gpt-4o', prepare_tools=turn_on_strict_if_openai)\n```\n\nUsage `ToolsPrepareFunc[AgentDepsT]`.", "url": "https://ai.pydantic.dev/tools/index.html#toolspreparefunc-module-attribute", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "DocstringFormat `module-attribute`", "anchor": "docstringformat-module-attribute", "md_text": "```\nDocstringFormat: TypeAlias = Literal[\n    \"google\", \"numpy\", \"sphinx\", \"auto\"\n]\n```\n\nSupported docstring formats.\n\n* `'google'` — [Google-style](https://google.github.io/styleguide/pyguide.html#381-docstrings) docstrings.\n* `'numpy'` — [Numpy-style](https://numpydoc.readthedocs.io/en/latest/format.html) docstrings.\n* `'sphinx'` — [Sphinx-style](https://sphinx-rtd-tutorial.readthedocs.io/en/latest/docstrings.html#the-sphinx-docstring-format) docstrings.\n* `'auto'` — Automatically infer the format based on the structure of the docstring.", "url": "https://ai.pydantic.dev/tools/index.html#docstringformat-module-attribute", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "DeferredToolRequests `dataclass`", "anchor": "deferredtoolrequests-dataclass", "md_text": "Tool calls that require approval or external execution.\n\nThis can be used as an agent's `output_type` and will be used as the output of the agent run if the model called any deferred tools.\n\nResults can be passed to the next agent run using a [`DeferredToolResults`](index.html#pydantic_ai.tools.DeferredToolResults) object with the same tool call IDs.\n\nSee [deferred tools docs](https://ai.pydantic.dev/deferred-tools/#deferred-tools) for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/tools.py`\n\n|  |  |\n| --- | --- |\n| ``` 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 ``` | ``` @dataclass(kw_only=True) class DeferredToolRequests:     \"\"\"Tool calls that require approval or external execution.      This can be used as an agent's `output_type` and will be used as the output of the agent run if the model called any deferred tools.      Results can be passed to the next agent run using a [`DeferredToolResults`][pydantic_ai.tools.DeferredToolResults] object with the same tool call IDs.      See [deferred tools docs](../deferred-tools.md#deferred-tools) for more information.     \"\"\"      calls: list[ToolCallPart] = field(default_factory=list)     \"\"\"Tool calls that require external execution.\"\"\"     approvals: list[ToolCallPart] = field(default_factory=list)     \"\"\"Tool calls that require human-in-the-loop approval.\"\"\" ``` |\n\n#### calls `class-attribute` `instance-attribute`\n\n```\ncalls: list[ToolCallPart] = field(default_factory=list)\n```\n\nTool calls that require external execution.\n\n#### approvals `class-attribute` `instance-attribute`\n\n```\napprovals: list[ToolCallPart] = field(default_factory=list)\n```\n\nTool calls that require human-in-the-loop approval.", "url": "https://ai.pydantic.dev/tools/index.html#deferredtoolrequests-dataclass", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "ToolApproved `dataclass`", "anchor": "toolapproved-dataclass", "md_text": "Indicates that a tool call has been approved and that the tool function should be executed.\n\nSource code in `pydantic_ai_slim/pydantic_ai/tools.py`\n\n|  |  |\n| --- | --- |\n| ``` 152 153 154 155 156 157 158 159 ``` | ``` @dataclass(kw_only=True) class ToolApproved:     \"\"\"Indicates that a tool call has been approved and that the tool function should be executed.\"\"\"      override_args: dict[str, Any] | None = None     \"\"\"Optional tool call arguments to use instead of the original arguments.\"\"\"      kind: Literal['tool-approved'] = 'tool-approved' ``` |\n\n#### override\\_args `class-attribute` `instance-attribute`\n\n```\noverride_args: dict[str, Any] | None = None\n```\n\nOptional tool call arguments to use instead of the original arguments.", "url": "https://ai.pydantic.dev/tools/index.html#toolapproved-dataclass", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "ToolDenied `dataclass`", "anchor": "tooldenied-dataclass", "md_text": "Indicates that a tool call has been denied and that a denial message should be returned to the model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/tools.py`\n\n|  |  |\n| --- | --- |\n| ``` 162 163 164 165 166 167 168 169 170 171 ``` | ``` @dataclass class ToolDenied:     \"\"\"Indicates that a tool call has been denied and that a denial message should be returned to the model.\"\"\"      message: str = 'The tool call was denied.'     \"\"\"The message to return to the model.\"\"\"      _: KW_ONLY      kind: Literal['tool-denied'] = 'tool-denied' ``` |\n\n#### message `class-attribute` `instance-attribute`\n\n```\nmessage: str = 'The tool call was denied.'\n```\n\nThe message to return to the model.", "url": "https://ai.pydantic.dev/tools/index.html#tooldenied-dataclass", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "DeferredToolResults `dataclass`", "anchor": "deferredtoolresults-dataclass", "md_text": "Results for deferred tool calls from a previous run that required approval or external execution.\n\nThe tool call IDs need to match those from the [`DeferredToolRequests`](../output/index.html#pydantic_ai.output.DeferredToolRequests) output object from the previous run.\n\nSee [deferred tools docs](https://ai.pydantic.dev/deferred-tools/#deferred-tools) for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/tools.py`\n\n|  |  |\n| --- | --- |\n| ``` 201 202 203 204 205 206 207 208 209 210 211 212 213 ``` | ``` @dataclass(kw_only=True) class DeferredToolResults:     \"\"\"Results for deferred tool calls from a previous run that required approval or external execution.      The tool call IDs need to match those from the [`DeferredToolRequests`][pydantic_ai.output.DeferredToolRequests] output object from the previous run.      See [deferred tools docs](../deferred-tools.md#deferred-tools) for more information.     \"\"\"      calls: dict[str, DeferredToolCallResult | Any] = field(default_factory=dict)     \"\"\"Map of tool call IDs to results for tool calls that required external execution.\"\"\"     approvals: dict[str, bool | DeferredToolApprovalResult] = field(default_factory=dict)     \"\"\"Map of tool call IDs to results for tool calls that required human-in-the-loop approval.\"\"\" ``` |\n\n#### calls `class-attribute` `instance-attribute`\n\n```\ncalls: dict[str, DeferredToolCallResult | Any] = field(\n    default_factory=dict\n)\n```\n\nMap of tool call IDs to results for tool calls that required external execution.\n\n#### approvals `class-attribute` `instance-attribute`\n\n```\napprovals: dict[str, bool | DeferredToolApprovalResult] = (\n    field(default_factory=dict)\n)\n```\n\nMap of tool call IDs to results for tool calls that required human-in-the-loop approval.", "url": "https://ai.pydantic.dev/tools/index.html#deferredtoolresults-dataclass", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "Tool `dataclass`", "anchor": "tool-dataclass", "md_text": "Bases: `Generic[AgentDepsT]`\n\nA tool function for an agent.\n\nSource code in `pydantic_ai_slim/pydantic_ai/tools.py`", "url": "https://ai.pydantic.dev/tools/index.html#tool-dataclass", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "Tool `dataclass`", "anchor": "tool-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 ``` | ``` @dataclass(init=False) class Tool(Generic[AgentDepsT]):     \"\"\"A tool function for an agent.\"\"\"      function: ToolFuncEither[AgentDepsT]     takes_ctx: bool     max_retries: int | None     name: str     description: str | None     prepare: ToolPrepareFunc[AgentDepsT] | None     docstring_format: DocstringFormat     require_parameter_descriptions: bool     strict: bool | None     sequential: bool     requires_approval: bool     metadata: dict[str, Any] | None     function_schema: _function_schema.FunctionSchema     \"\"\"     The base JSON schema for the tool's parameters.      This schema may be modified by the `prepare` function or by the Model class prior to including it in an API request.     \"\"\"      def __init__(         self,         function: ToolFuncEither[AgentDepsT],         *,         takes_ctx: bool | None = None,         max_retries: int | None = None,         name: str | None = None,         description: str | None = None,         prepare: ToolPrepareFunc[AgentDepsT] | None = None,         docstring_format: DocstringFormat = 'auto',         require_parameter_descriptions: bool = False,         schema_generator: type[GenerateJsonSchema] = GenerateToolJsonSchema,         strict: bool | None = None,         sequential: bool = False,         requires_approval: bool = False,         metadata: dict[str, Any] | None = None,         function_schema: _function_schema.FunctionSchema | None = None,     ):         \"\"\"Create a new tool instance.          Example usage:          ```python {noqa=\"I001\"}         from pydantic_ai import Agent, RunContext, Tool          async def my_tool(ctx: RunContext[int], x: int, y: int) -> str:             return f'{ctx.deps} {x} {y}'          agent = Agent('test', tools=[Tool(my_tool)])         ```          or with a custom prepare method:          ```python {noqa=\"I001\"}          from pydantic_ai import Agent, RunContext, Tool         from pydantic_ai.tools import ToolDefinition          async def my_tool(ctx: RunContext[int], x: int, y: int) -> str:             return f'{ctx.deps} {x} {y}'          async def prep_my_tool(             ctx: RunContext[int], tool_def: ToolDefinition         ) -> ToolDefinition | None:             # only register the tool if `deps == 42`             if ctx.deps == 42:                 return tool_def          agent = Agent('test', tools=[Tool(my_tool, prepare=prep_my_tool)])         ```           Args:             function: The Python function to call as the tool.             takes_ctx: Whether the function takes a [`RunContext`][pydantic_ai.tools.RunContext] first argument,                 this is inferred if unset.             max_retries: Maximum number of retries allowed for this tool, set to the agent default if `None`.             name: Name of the tool, inferred from the function if `None`.             description: Description of the tool, inferred from the function if `None`.             prepare: custom method to prepare the tool definition for each step, return `None` to omit this                 tool from a given step. This is useful if you want to customise a tool at call time,                 or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].             docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].                 Defaults to `'auto'`, such that the format is inferred from the structure of the docstring.             require_parameter_descriptions: If True, raise an error if a parameter description is missing. Defaults to False.             schema_generator: The JSON schema generator class to use. Defaults to `GenerateToolJsonSchema`.             strict: Whether to enforce JSON schema compliance (only affects OpenAI).                 See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.             sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.             requires_approval: Whether this tool requires human-in-the-loop approval. Defaults to False.                 See the [tools documentation](../deferred-tools.md#human-in-the-loop-tool-approval) for more info.             metadata: Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization.             function_schema: The function schema to use for the tool. If not provided, it will be generated.         \"\"\"         self.function = function         self.function_schema = function_schema or _function_schema.function_schema(             function,             schema_generator,             takes_ctx=takes_ctx,             docstring_format=docstring_format,             require_parameter_descriptions=require_parameter_descriptions,         )         self.takes_ctx = self.function_schema.takes_ctx         self.max_retries = max_retries         self.name = name or function.__name__         self.description = description or self.function_schema.description         self.prepare = prepare         self.docstring_format = docstring_format         self.require_parameter_descriptions = require_parameter_descriptions         self.strict = strict         self.sequential = sequential         self.requires_approval = requires_approval         self.metadata = metadata      @classmethod     def from_schema(         cls,         function: Callable[..., Any],         name: str,         description: str | None,         json_schema: JsonSchemaValue,         takes_ctx: bool = False,         sequential: bool = False,     ) -> Self:         \"\"\"Creates a Pydantic tool from a function and a JSON schema.          Args:             function: The function to call.                 This will be called with keywords only, and no validation of                 the arguments will be performed.             name: The unique name of the tool that clearly communicates its purpose             description: Used to tell the model how/when/why to use the tool.                 You can provide few-shot examples as a part of the description.             json_schema: The schema for the function arguments             takes_ctx: An optional boolean parameter indicating whether the function                 accepts the context object as an argument.             sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.          Returns:             A Pydantic tool that calls the function         \"\"\"         function_schema = _function_schema.FunctionSchema(             function=function,             description=description,             validator=SchemaValidator(schema=core_schema.any_schema()),             json_schema=json_schema,             takes_ctx=takes_ctx,             is_async=_utils.is_async_callable(function),         )          return cls(             function,             takes_ctx=takes_ctx,             name=name,             description=description,             function_schema=function_schema,             sequential=sequential,         )      @property     def tool_def(self):         return ToolDefinition(             name=self.name,             description=self.description,             parameters_json_schema=self.function_schema.json_schema,             strict=self.strict,             sequential=self.sequential,             metadata=self.metadata,         )      async def prepare_tool_def(self, ctx: RunContext[AgentDepsT]) -> ToolDefinition | None:         \"\"\"Get the tool definition.          By default, this method creates a tool definition, then either returns it, or calls `self.prepare`         if it's set.          Returns:             return a `ToolDefinition` or `None` if the tools should not be registered for this run.         \"\"\"         base_tool_def = self.tool_def          if self.requires_approval and not ctx.tool_call_approved:             base_tool_def = replace(base_tool_def, kind='unapproved')          if self.prepare is not None:             return await self.prepare(ctx, base_tool_def)         else:             return base_tool_def ``` |", "url": "https://ai.pydantic.dev/tools/index.html#tool-dataclass", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "Tool `dataclass`", "anchor": "tool-dataclass", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    function: ToolFuncEither[AgentDepsT],\n    *,\n    takes_ctx: bool | None = None,\n    max_retries: int | None = None,\n    name: str | None = None,\n    description: str | None = None,\n    prepare: ToolPrepareFunc[AgentDepsT] | None = None,\n    docstring_format: DocstringFormat = \"auto\",\n    require_parameter_descriptions: bool = False,\n    schema_generator: type[\n        GenerateJsonSchema\n    ] = GenerateToolJsonSchema,\n    strict: bool | None = None,\n    sequential: bool = False,\n    requires_approval: bool = False,\n    metadata: dict[str, Any] | None = None,\n    function_schema: FunctionSchema | None = None\n)\n```\n\nCreate a new tool instance.\n\nExample usage:\n\n```\nfrom pydantic_ai import Agent, RunContext, Tool\n\nasync def my_tool(ctx: RunContext[int], x: int, y: int) -> str:\n    return f'{ctx.deps} {x} {y}'\n\nagent = Agent('test', tools=[Tool(my_tool)])\n```\n\nor with a custom prepare method:\n\n```\nfrom pydantic_ai import Agent, RunContext, Tool\nfrom pydantic_ai.tools import ToolDefinition\n\nasync def my_tool(ctx: RunContext[int], x: int, y: int) -> str:\n    return f'{ctx.deps} {x} {y}'\n\nasync def prep_my_tool(\n    ctx: RunContext[int], tool_def: ToolDefinition\n) -> ToolDefinition | None:\n    # only register the tool if `deps == 42`\n    if ctx.deps == 42:\n        return tool_def\n\nagent = Agent('test', tools=[Tool(my_tool, prepare=prep_my_tool)])\n```\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `function` | `ToolFuncEither[AgentDepsT]` | The Python function to call as the tool. | *required* |\n| `takes_ctx` | `bool | None` | Whether the function takes a [`RunContext`](index.html#pydantic_ai.tools.RunContext) first argument, this is inferred if unset. | `None` |\n| `max_retries` | `int | None` | Maximum number of retries allowed for this tool, set to the agent default if `None`. | `None` |\n| `name` | `str | None` | Name of the tool, inferred from the function if `None`. | `None` |\n| `description` | `str | None` | Description of the tool, inferred from the function if `None`. | `None` |\n| `prepare` | `ToolPrepareFunc[AgentDepsT] | None` | custom method to prepare the tool definition for each step, return `None` to omit this tool from a given step. This is useful if you want to customise a tool at call time, or omit it completely from a step. See [`ToolPrepareFunc`](index.html#pydantic_ai.tools.ToolPrepareFunc). | `None` |\n| `docstring_format` | `DocstringFormat` | The format of the docstring, see [`DocstringFormat`](index.html#pydantic_ai.tools.DocstringFormat). Defaults to `'auto'`, such that the format is inferred from the structure of the docstring. | `'auto'` |\n| `require_parameter_descriptions` | `bool` | If True, raise an error if a parameter description is missing. Defaults to False. | `False` |\n| `schema_generator` | `type[GenerateJsonSchema]` | The JSON schema generator class to use. Defaults to `GenerateToolJsonSchema`. | `GenerateToolJsonSchema` |\n| `strict` | `bool | None` | Whether to enforce JSON schema compliance (only affects OpenAI). See [`ToolDefinition`](index.html#pydantic_ai.tools.ToolDefinition) for more info. | `None` |\n| `sequential` | `bool` | Whether the function requires a sequential/serial execution environment. Defaults to False. | `False` |\n| `requires_approval` | `bool` | Whether this tool requires human-in-the-loop approval. Defaults to False. See the [tools documentation](https://ai.pydantic.dev/deferred-tools/#human-in-the-loop-tool-approval) for more info. | `False` |\n| `metadata` | `dict[str, Any] | None` | Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization. | `None` |\n| `function_schema` | `FunctionSchema | None` | The function schema to use for the tool. If not provided, it will be generated. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/tools.py`", "url": "https://ai.pydantic.dev/tools/index.html#tool-dataclass", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "Tool `dataclass`", "anchor": "tool-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 ``` | ``` def __init__(     self,     function: ToolFuncEither[AgentDepsT],     *,     takes_ctx: bool | None = None,     max_retries: int | None = None,     name: str | None = None,     description: str | None = None,     prepare: ToolPrepareFunc[AgentDepsT] | None = None,     docstring_format: DocstringFormat = 'auto',     require_parameter_descriptions: bool = False,     schema_generator: type[GenerateJsonSchema] = GenerateToolJsonSchema,     strict: bool | None = None,     sequential: bool = False,     requires_approval: bool = False,     metadata: dict[str, Any] | None = None,     function_schema: _function_schema.FunctionSchema | None = None, ):     \"\"\"Create a new tool instance.      Example usage:      ```python {noqa=\"I001\"}     from pydantic_ai import Agent, RunContext, Tool      async def my_tool(ctx: RunContext[int], x: int, y: int) -> str:         return f'{ctx.deps} {x} {y}'      agent = Agent('test', tools=[Tool(my_tool)])     ```      or with a custom prepare method:      ```python {noqa=\"I001\"}      from pydantic_ai import Agent, RunContext, Tool     from pydantic_ai.tools import ToolDefinition      async def my_tool(ctx: RunContext[int], x: int, y: int) -> str:         return f'{ctx.deps} {x} {y}'      async def prep_my_tool(         ctx: RunContext[int], tool_def: ToolDefinition     ) -> ToolDefinition | None:         # only register the tool if `deps == 42`         if ctx.deps == 42:             return tool_def      agent = Agent('test', tools=[Tool(my_tool, prepare=prep_my_tool)])     ```       Args:         function: The Python function to call as the tool.         takes_ctx: Whether the function takes a [`RunContext`][pydantic_ai.tools.RunContext] first argument,             this is inferred if unset.         max_retries: Maximum number of retries allowed for this tool, set to the agent default if `None`.         name: Name of the tool, inferred from the function if `None`.         description: Description of the tool, inferred from the function if `None`.         prepare: custom method to prepare the tool definition for each step, return `None` to omit this             tool from a given step. This is useful if you want to customise a tool at call time,             or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].         docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].             Defaults to `'auto'`, such that the format is inferred from the structure of the docstring.         require_parameter_descriptions: If True, raise an error if a parameter description is missing. Defaults to False.         schema_generator: The JSON schema generator class to use. Defaults to `GenerateToolJsonSchema`.         strict: Whether to enforce JSON schema compliance (only affects OpenAI).             See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.         sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.         requires_approval: Whether this tool requires human-in-the-loop approval. Defaults to False.             See the [tools documentation](../deferred-tools.md#human-in-the-loop-tool-approval) for more info.         metadata: Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization.         function_schema: The function schema to use for the tool. If not provided, it will be generated.     \"\"\"     self.function = function     self.function_schema = function_schema or _function_schema.function_schema(         function,         schema_generator,         takes_ctx=takes_ctx,         docstring_format=docstring_format,         require_parameter_descriptions=require_parameter_descriptions,     )     self.takes_ctx = self.function_schema.takes_ctx     self.max_retries = max_retries     self.name = name or function.__name__     self.description = description or self.function_schema.description     self.prepare = prepare     self.docstring_format = docstring_format     self.require_parameter_descriptions = require_parameter_descriptions     self.strict = strict     self.sequential = sequential     self.requires_approval = requires_approval     self.metadata = metadata ``` |\n\n#### function\\_schema `instance-attribute`", "url": "https://ai.pydantic.dev/tools/index.html#tool-dataclass", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "Tool `dataclass`", "anchor": "tool-dataclass", "md_text": "```\nfunction_schema: FunctionSchema = (\n    function_schema\n    or function_schema(\n        function,\n        schema_generator,\n        takes_ctx=takes_ctx,\n        docstring_format=docstring_format,\n        require_parameter_descriptions=require_parameter_descriptions,\n    )\n)\n```\n\nThe base JSON schema for the tool's parameters.\n\nThis schema may be modified by the `prepare` function or by the Model class prior to including it in an API request.\n\n#### from\\_schema `classmethod`\n\n```\nfrom_schema(\n    function: Callable[..., Any],\n    name: str,\n    description: str | None,\n    json_schema: JsonSchemaValue,\n    takes_ctx: bool = False,\n    sequential: bool = False,\n) -> Self\n```\n\nCreates a Pydantic tool from a function and a JSON schema.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `function` | `Callable[..., Any]` | The function to call. This will be called with keywords only, and no validation of the arguments will be performed. | *required* |\n| `name` | `str` | The unique name of the tool that clearly communicates its purpose | *required* |\n| `description` | `str | None` | Used to tell the model how/when/why to use the tool. You can provide few-shot examples as a part of the description. | *required* |\n| `json_schema` | `JsonSchemaValue` | The schema for the function arguments | *required* |\n| `takes_ctx` | `bool` | An optional boolean parameter indicating whether the function accepts the context object as an argument. | `False` |\n| `sequential` | `bool` | Whether the function requires a sequential/serial execution environment. Defaults to False. | `False` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `Self` | A Pydantic tool that calls the function |\n\nSource code in `pydantic_ai_slim/pydantic_ai/tools.py`\n\n|  |  |\n| --- | --- |\n| ``` 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 ``` | ``` @classmethod def from_schema(     cls,     function: Callable[..., Any],     name: str,     description: str | None,     json_schema: JsonSchemaValue,     takes_ctx: bool = False,     sequential: bool = False, ) -> Self:     \"\"\"Creates a Pydantic tool from a function and a JSON schema.      Args:         function: The function to call.             This will be called with keywords only, and no validation of             the arguments will be performed.         name: The unique name of the tool that clearly communicates its purpose         description: Used to tell the model how/when/why to use the tool.             You can provide few-shot examples as a part of the description.         json_schema: The schema for the function arguments         takes_ctx: An optional boolean parameter indicating whether the function             accepts the context object as an argument.         sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.      Returns:         A Pydantic tool that calls the function     \"\"\"     function_schema = _function_schema.FunctionSchema(         function=function,         description=description,         validator=SchemaValidator(schema=core_schema.any_schema()),         json_schema=json_schema,         takes_ctx=takes_ctx,         is_async=_utils.is_async_callable(function),     )      return cls(         function,         takes_ctx=takes_ctx,         name=name,         description=description,         function_schema=function_schema,         sequential=sequential,     ) ``` |\n\n#### prepare\\_tool\\_def `async`\n\n```\nprepare_tool_def(\n    ctx: RunContext[AgentDepsT],\n) -> ToolDefinition | None\n```\n\nGet the tool definition.\n\nBy default, this method creates a tool definition, then either returns it, or calls `self.prepare`\nif it's set.\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `ToolDefinition | None` | return a `ToolDefinition` or `None` if the tools should not be registered for this run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/tools.py`\n\n|  |  |\n| --- | --- |\n| ``` 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 ``` | ``` async def prepare_tool_def(self, ctx: RunContext[AgentDepsT]) -> ToolDefinition | None:     \"\"\"Get the tool definition.      By default, this method creates a tool definition, then either returns it, or calls `self.prepare`     if it's set.      Returns:         return a `ToolDefinition` or `None` if the tools should not be registered for this run.     \"\"\"     base_tool_def = self.tool_def      if self.requires_approval and not ctx.tool_call_approved:         base_tool_def = replace(base_tool_def, kind='unapproved')      if self.prepare is not None:         return await self.prepare(ctx, base_tool_def)     else:         return base_tool_def ``` |", "url": "https://ai.pydantic.dev/tools/index.html#tool-dataclass", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "ObjectJsonSchema `module-attribute`", "anchor": "objectjsonschema-module-attribute", "md_text": "```\nObjectJsonSchema: TypeAlias = dict[str, Any]\n```\n\nType representing JSON schema of an object, e.g. where `\"type\": \"object\"`.\n\nThis type is used to define tools parameters (aka arguments) in [ToolDefinition](index.html#pydantic_ai.tools.ToolDefinition).\n\nWith PEP-728 this should be a TypedDict with `type: Literal['object']`, and `extra_parts=Any`", "url": "https://ai.pydantic.dev/tools/index.html#objectjsonschema-module-attribute", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "ToolDefinition `dataclass`", "anchor": "tooldefinition-dataclass", "md_text": "Definition of a tool passed to a model.\n\nThis is used for both function tools and output tools.\n\nSource code in `pydantic_ai_slim/pydantic_ai/tools.py`\n\n|  |  |\n| --- | --- |\n| ``` 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 ``` | ``` @dataclass(repr=False, kw_only=True) class ToolDefinition:     \"\"\"Definition of a tool passed to a model.      This is used for both function tools and output tools.     \"\"\"      name: str     \"\"\"The name of the tool.\"\"\"      parameters_json_schema: ObjectJsonSchema = field(default_factory=lambda: {'type': 'object', 'properties': {}})     \"\"\"The JSON schema for the tool's parameters.\"\"\"      description: str | None = None     \"\"\"The description of the tool.\"\"\"      outer_typed_dict_key: str | None = None     \"\"\"The key in the outer [TypedDict] that wraps an output tool.      This will only be set for output tools which don't have an `object` JSON schema.     \"\"\"      strict: bool | None = None     \"\"\"Whether to enforce (vendor-specific) strict JSON schema validation for tool calls.      Setting this to `True` while using a supported model generally imposes some restrictions on the tool's JSON schema     in exchange for guaranteeing the API responses strictly match that schema.      When `False`, the model may be free to generate other properties or types (depending on the vendor).     When `None` (the default), the value will be inferred based on the compatibility of the parameters_json_schema.      Note: this is currently only supported by OpenAI models.     \"\"\"      sequential: bool = False     \"\"\"Whether this tool requires a sequential/serial execution environment.\"\"\"      kind: ToolKind = field(default='function')     \"\"\"The kind of tool:      - `'function'`: a tool that will be executed by Pydantic AI during an agent run and has its result returned to the model     - `'output'`: a tool that passes through an output value that ends the run     - `'external'`: a tool whose result will be produced outside of the Pydantic AI agent run in which it was called, because it depends on an upstream service (or user) or could take longer to generate than it's reasonable to keep the agent process running.         See the [tools documentation](../deferred-tools.md#deferred-tools) for more info.     - `'unapproved'`: a tool that requires human-in-the-loop approval.         See the [tools documentation](../deferred-tools.md#human-in-the-loop-tool-approval) for more info.     \"\"\"      metadata: dict[str, Any] | None = None     \"\"\"Tool metadata that can be set by the toolset this tool came from. It is not sent to the model, but can be used for filtering and tool behavior customization.      For MCP tools, this contains the `meta`, `annotations`, and `output_schema` fields from the tool definition.     \"\"\"      @property     def defer(self) -> bool:         \"\"\"Whether calls to this tool will be deferred.          See the [tools documentation](../deferred-tools.md#deferred-tools) for more info.         \"\"\"         return self.kind in ('external', 'unapproved')      __repr__ = _utils.dataclasses_no_defaults_repr ``` |\n\n#### name `instance-attribute`\n\n```\nname: str\n```\n\nThe name of the tool.\n\n#### parameters\\_json\\_schema `class-attribute` `instance-attribute`\n\n```\nparameters_json_schema: ObjectJsonSchema = field(\n    default_factory=lambda: {\n        \"type\": \"object\",\n        \"properties\": {},\n    }\n)\n```\n\nThe JSON schema for the tool's parameters.\n\n#### description `class-attribute` `instance-attribute`\n\n```\ndescription: str | None = None\n```\n\nThe description of the tool.\n\n#### outer\\_typed\\_dict\\_key `class-attribute` `instance-attribute`\n\n```\nouter_typed_dict_key: str | None = None\n```\n\nThe key in the outer [TypedDict] that wraps an output tool.\n\nThis will only be set for output tools which don't have an `object` JSON schema.\n\n#### strict `class-attribute` `instance-attribute`\n\n```\nstrict: bool | None = None\n```\n\nWhether to enforce (vendor-specific) strict JSON schema validation for tool calls.\n\nSetting this to `True` while using a supported model generally imposes some restrictions on the tool's JSON schema\nin exchange for guaranteeing the API responses strictly match that schema.\n\nWhen `False`, the model may be free to generate other properties or types (depending on the vendor).\nWhen `None` (the default), the value will be inferred based on the compatibility of the parameters\\_json\\_schema.\n\nNote: this is currently only supported by OpenAI models.\n\n#### sequential `class-attribute` `instance-attribute`\n\n```\nsequential: bool = False\n```\n\nWhether this tool requires a sequential/serial execution environment.\n\n#### kind `class-attribute` `instance-attribute`\n\n```\nkind: ToolKind = field(default='function')\n```\n\nThe kind of tool:", "url": "https://ai.pydantic.dev/tools/index.html#tooldefinition-dataclass", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "ToolDefinition `dataclass`", "anchor": "tooldefinition-dataclass", "md_text": "* `'function'`: a tool that will be executed by Pydantic AI during an agent run and has its result returned to the model\n* `'output'`: a tool that passes through an output value that ends the run\n* `'external'`: a tool whose result will be produced outside of the Pydantic AI agent run in which it was called, because it depends on an upstream service (or user) or could take longer to generate than it's reasonable to keep the agent process running.\n  See the [tools documentation](https://ai.pydantic.dev/deferred-tools/#deferred-tools) for more info.\n* `'unapproved'`: a tool that requires human-in-the-loop approval.\n  See the [tools documentation](https://ai.pydantic.dev/deferred-tools/#human-in-the-loop-tool-approval) for more info.\n\n#### metadata `class-attribute` `instance-attribute`\n\n```\nmetadata: dict[str, Any] | None = None\n```\n\nTool metadata that can be set by the toolset this tool came from. It is not sent to the model, but can be used for filtering and tool behavior customization.\n\nFor MCP tools, this contains the `meta`, `annotations`, and `output_schema` fields from the tool definition.\n\n#### defer `property`\n\n```\ndefer: bool\n```\n\nWhether calls to this tool will be deferred.\n\nSee the [tools documentation](https://ai.pydantic.dev/deferred-tools/#deferred-tools) for more info.", "url": "https://ai.pydantic.dev/tools/index.html#tooldefinition-dataclass", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "Durable Execution", "anchor": "durable-execution", "md_text": "DBOS workflows make your program **durable** by checkpointing its state in a database. If your program ever fails, when it restarts all your workflows will automatically resume from the last completed step.\n\n* **Workflows** must be deterministic and generally cannot include I/O.\n* **Steps** may perform I/O (network, disk, API calls). If a step fails, it restarts from the beginning.\n\nEvery workflow input and step output is durably stored in the system database. When workflow execution fails, whether from crashes, network issues, or server restarts, DBOS leverages these checkpoints to recover workflows from their last completed step.\n\nDBOS **queues** provide durable, database-backed alternatives to systems like Celery or BullMQ, supporting features such as concurrency limits, rate limits, timeouts, and prioritization. See the [DBOS docs](https://docs.dbos.dev/architecture) for details.\n\nThe diagram below shows the overall architecture of an agentic application in DBOS.\nDBOS runs fully in-process as a library. Functions remain normal Python functions but are checkpointed into a database (Postgres or SQLite).\n\n```\n                    Clients\n            (HTTP, RPC, Kafka, etc.)\n                        |\n                        v\n+------------------------------------------------------+\n|               Application Servers                    |\n|                                                      |\n|   +----------------------------------------------+   |\n|   |        Pydantic AI + DBOS Libraries          |   |\n|   |                                              |   |\n|   |  [ Workflows (Agent Run Loop) ]              |   |\n|   |  [ Steps (Tool, MCP, Model) ]                |   |\n|   |  [ Queues ]   [ Cron Jobs ]   [ Messaging ]  |   |\n|   +----------------------------------------------+   |\n|                                                      |\n+------------------------------------------------------+\n                        |\n                        v\n+------------------------------------------------------+\n|                      Database                        |\n|   (Stores workflow and step state, schedules tasks)  |\n+------------------------------------------------------+\n```\n\nSee the [DBOS documentation](https://docs.dbos.dev/architecture) for more information.", "url": "https://ai.pydantic.dev/dbos/index.html#durable-execution", "page": "dbos/index.html", "source_site": "pydantic_ai"}
{"title": "Durable Agent", "anchor": "durable-agent", "md_text": "Any agent can be wrapped in a [`DBOSAgent`](../durable_exec/index.html#pydantic_ai.durable_exec.dbos.DBOSAgent) to get durable execution. `DBOSAgent` automatically:,\n\n* Wraps `Agent.run` and `Agent.run_sync` as DBOS workflows.\n* Wraps [model requests](https://ai.pydantic.dev/models/overview/) and [MCP communication](../client/index.html) as DBOS steps.\n\nCustom tool functions and event stream handlers are **not automatically wrapped** by DBOS.\nIf they involve non-deterministic behavior or perform I/O, you should explicitly decorate them with `@DBOS.step`.\n\nThe original agent, model, and MCP server can still be used as normal outside the DBOS workflow.\n\nHere is a simple but complete example of wrapping an agent for durable execution. All it requires is to install Pydantic AI with the DBOS [open-source library](https://github.com/dbos-inc/dbos-transact-py):\n\npipuv\n\n```\npip install pydantic-ai[dbos]\n\nuv add pydantic-ai[dbos]\n```\n\nOr if you're using the slim package, you can install it with the `dbos` optional group:\n\npipuv\n\n```\npip install pydantic-ai-slim[dbos]\n\nuv add pydantic-ai-slim[dbos]\n```\n\ndbos\\_agent.py\n\n```\nfrom dbos import DBOS, DBOSConfig\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.durable_exec.dbos import DBOSAgent\n\ndbos_config: DBOSConfig = {\n    'name': 'pydantic_dbos_agent',\n    'system_database_url': 'sqlite:///dbostest.sqlite',  # (3)!\n}\nDBOS(config=dbos_config)\n\nagent = Agent(\n    'gpt-5',\n    instructions=\"You're an expert in geography.\",\n    name='geography',  # (4)!\n)\n\ndbos_agent = DBOSAgent(agent)  # (1)!\n\nasync def main():\n    DBOS.launch()\n    result = await dbos_agent.run('What is the capital of Mexico?')  # (2)!\n    print(result.output)\n    #> Mexico City (Ciudad de México, CDMX)\n```\n\n1. Workflows and `DBOSAgent` must be defined before `DBOS.launch()` so that recovery can correctly find all workflows.\n2. [`DBOSAgent.run()`](../durable_exec/index.html#pydantic_ai.durable_exec.dbos.DBOSAgent.run) works like [`Agent.run()`](../agent/index.html#pydantic_ai.agent.AbstractAgent.run), but runs as a DBOS workflow and executes model requests, decorated tool calls, and MCP communication as DBOS steps.\n3. This example uses SQLite. Postgres is recommended for production.\n4. The agent's `name` is used to uniquely identify its workflows.\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\nBecause DBOS workflows need to be defined before calling `DBOS.launch()` and the `DBOSAgent` instance automatically registers `run` and `run_sync` as workflows, it needs to be defined before calling `DBOS.launch()` as well.\n\nFor more information on how to use DBOS in Python applications, see their [Python SDK guide](https://docs.dbos.dev/python/programming-guide).", "url": "https://ai.pydantic.dev/dbos/index.html#durable-agent", "page": "dbos/index.html", "source_site": "pydantic_ai"}
{"title": "DBOS Integration Considerations", "anchor": "dbos-integration-considerations", "md_text": "When using DBOS with Pydantic AI agents, there are a few important considerations to ensure workflows and toolsets behave correctly.", "url": "https://ai.pydantic.dev/dbos/index.html#dbos-integration-considerations", "page": "dbos/index.html", "source_site": "pydantic_ai"}
{"title": "Agent and Toolset Requirements", "anchor": "agent-and-toolset-requirements", "md_text": "Each agent instance must have a unique `name` so DBOS can correctly resume workflows after a failure or restart.\n\nTools and event stream handlers are not automatically wrapped by DBOS. You can decide how to integrate them:\n\n* Decorate with `@DBOS.step` if the function involves non-determinism or I/O.\n* Skip the decorator if durability isn't needed, so you avoid the extra DB checkpoint write.\n* If the function needs to enqueue tasks or invoke other DBOS workflows, run it inside the agent's main workflow (not as a step).\n\nOther than that, any agent and toolset will just work!", "url": "https://ai.pydantic.dev/dbos/index.html#agent-and-toolset-requirements", "page": "dbos/index.html", "source_site": "pydantic_ai"}
{"title": "Agent Run Context and Dependencies", "anchor": "agent-run-context-and-dependencies", "md_text": "DBOS checkpoints workflow inputs/outputs and step outputs into a database using [`pickle`](https://docs.python.org/3/library/pickle.html). This means you need to make sure [dependencies](https://ai.pydantic.dev/dependencies/) object provided to [`DBOSAgent.run()`](../durable_exec/index.html#pydantic_ai.durable_exec.dbos.DBOSAgent.run) or [`DBOSAgent.run_sync()`](../durable_exec/index.html#pydantic_ai.durable_exec.dbos.DBOSAgent.run_sync), and tool outputs can be serialized using pickle. You may also want to keep the inputs and outputs small (under ~2 MB). PostgreSQL and SQLite support up to 1 GB per field, but large objects may impact performance.", "url": "https://ai.pydantic.dev/dbos/index.html#agent-run-context-and-dependencies", "page": "dbos/index.html", "source_site": "pydantic_ai"}
{"title": "Streaming", "anchor": "streaming", "md_text": "Because DBOS cannot stream output directly to the workflow or step call site, [`Agent.run_stream()`](../agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream) and [`Agent.run_stream_events()`](../agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream_events) are not supported when running inside of a DBOS workflow.\n\nInstead, you can implement streaming by setting an [`event_stream_handler`](../agent/index.html#pydantic_ai.agent.EventStreamHandler) on the `Agent` or `DBOSAgent` instance and using [`DBOSAgent.run()`](../durable_exec/index.html#pydantic_ai.durable_exec.dbos.DBOSAgent.run).\nThe event stream handler function will receive the agent [run context](../tools/index.html#pydantic_ai.tools.RunContext) and an async iterable of events from the model's streaming response and the agent's execution of tools. For examples, see the [streaming docs](https://ai.pydantic.dev/agents/#streaming-all-events).", "url": "https://ai.pydantic.dev/dbos/index.html#streaming", "page": "dbos/index.html", "source_site": "pydantic_ai"}
{"title": "Step Configuration", "anchor": "step-configuration", "md_text": "You can customize DBOS step behavior, such as retries, by passing [`StepConfig`](../durable_exec/index.html#pydantic_ai.durable_exec.dbos.StepConfig) objects to the `DBOSAgent` constructor:\n\n* `mcp_step_config`: The DBOS step config to use for MCP server communication. No retries if omitted.\n* `model_step_config`: The DBOS step config to use for model request steps. No retries if omitted.\n\nFor custom tools, you can annotate them directly with [`@DBOS.step`](https://docs.dbos.dev/python/reference/decorators#step) or [`@DBOS.workflow`](https://docs.dbos.dev/python/reference/decorators#workflow) decorators as needed. These decorators have no effect outside DBOS workflows, so tools remain usable in non-DBOS agents.", "url": "https://ai.pydantic.dev/dbos/index.html#step-configuration", "page": "dbos/index.html", "source_site": "pydantic_ai"}
{"title": "Step Retries", "anchor": "step-retries", "md_text": "On top of the automatic retries for request failures that DBOS will perform, Pydantic AI and various provider API clients also have their own request retry logic. Enabling these at the same time may cause the request to be retried more often than expected, with improper `Retry-After` handling.\n\nWhen using DBOS, it's recommended to not use [HTTP Request Retries](https://ai.pydantic.dev/retries/) and to turn off your provider API client's own retry logic, for example by setting `max_retries=0` on a [custom `OpenAIProvider` API client](../openai/index.html#custom-openai-client).\n\nYou can customize DBOS's retry policy using [step configuration](index.html#step-configuration).", "url": "https://ai.pydantic.dev/dbos/index.html#step-retries", "page": "dbos/index.html", "source_site": "pydantic_ai"}
{"title": "Observability with Logfire", "anchor": "observability-with-logfire", "md_text": "DBOS can be configured to generate OpenTelemetry spans for each workflow and step execution, and Pydantic AI emits spans for each agent run, model request, and tool invocation. You can send these spans to [Pydantic Logfire](https://ai.pydantic.dev/logfire/) to get a full, end-to-end view of what's happening in your application.\n\nFor more information about DBOS logging and tracing, please see the [DBOS docs](https://docs.dbos.dev/python/tutorials/logging-and-tracing) for details.", "url": "https://ai.pydantic.dev/dbos/index.html#observability-with-logfire", "page": "dbos/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractToolset", "anchor": "abstracttoolset", "md_text": "Bases: `ABC`, `Generic[AgentDepsT]`\n\nA toolset is a collection of tools that can be used by an agent.\n\nIt is responsible for:\n\n* Listing the tools it contains\n* Validating the arguments of the tools\n* Calling the tools\n\nSee [toolset docs](https://ai.pydantic.dev/toolsets/) for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/toolsets/abstract.py`", "url": "https://ai.pydantic.dev/toolsets/index.html#abstracttoolset", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractToolset", "anchor": "abstracttoolset", "md_text": "|  |  |\n| --- | --- |\n| ```  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 ``` | ``` class AbstractToolset(ABC, Generic[AgentDepsT]):     \"\"\"A toolset is a collection of tools that can be used by an agent.      It is responsible for:      - Listing the tools it contains     - Validating the arguments of the tools     - Calling the tools      See [toolset docs](../toolsets.md) for more information.     \"\"\"      @property     @abstractmethod     def id(self) -> str | None:         \"\"\"An ID for the toolset that is unique among all toolsets registered with the same agent.          If you're implementing a concrete implementation that users can instantiate more than once, you should let them optionally pass a custom ID to the constructor and return that here.          A toolset needs to have an ID in order to be used in a durable execution environment like Temporal, in which case the ID will be used to identify the toolset's activities within the workflow.         \"\"\"         raise NotImplementedError()      @property     def label(self) -> str:         \"\"\"The name of the toolset for use in error messages.\"\"\"         label = self.__class__.__name__         if self.id:  # pragma: no branch             label += f' {self.id!r}'         return label      @property     def tool_name_conflict_hint(self) -> str:         \"\"\"A hint for how to avoid name conflicts with other toolsets for use in error messages.\"\"\"         return 'Rename the tool or wrap the toolset in a `PrefixedToolset` to avoid name conflicts.'      async def __aenter__(self) -> Self:         \"\"\"Enter the toolset context.          This is where you can set up network connections in a concrete implementation.         \"\"\"         return self      async def __aexit__(self, *args: Any) -> bool | None:         \"\"\"Exit the toolset context.          This is where you can tear down network connections in a concrete implementation.         \"\"\"         return None      @abstractmethod     async def get_tools(self, ctx: RunContext[AgentDepsT]) -> dict[str, ToolsetTool[AgentDepsT]]:         \"\"\"The tools that are available in this toolset.\"\"\"         raise NotImplementedError()      @abstractmethod     async def call_tool(         self, name: str, tool_args: dict[str, Any], ctx: RunContext[AgentDepsT], tool: ToolsetTool[AgentDepsT]     ) -> Any:         \"\"\"Call a tool with the given arguments.          Args:             name: The name of the tool to call.             tool_args: The arguments to pass to the tool.             ctx: The run context.             tool: The tool definition returned by [`get_tools`][pydantic_ai.toolsets.AbstractToolset.get_tools] that was called.         \"\"\"         raise NotImplementedError()      def apply(self, visitor: Callable[[AbstractToolset[AgentDepsT]], None]) -> None:         \"\"\"Run a visitor function on all \"leaf\" toolsets (i.e. those that implement their own tool listing and calling).\"\"\"         visitor(self)      def visit_and_replace(         self, visitor: Callable[[AbstractToolset[AgentDepsT]], AbstractToolset[AgentDepsT]]     ) -> AbstractToolset[AgentDepsT]:         \"\"\"Run a visitor function on all \"leaf\" toolsets (i.e. those that implement their own tool listing and calling) and replace them in the hierarchy with the result of the function.\"\"\"         return visitor(self)      def filtered(         self, filter_func: Callable[[RunContext[AgentDepsT], ToolDefinition], bool]     ) -> FilteredToolset[AgentDepsT]:         \"\"\"Returns a new toolset that filters this toolset's tools using a filter function that takes the agent context and the tool definition.          See [toolset docs](../toolsets.md#filtering-tools) for more information.         \"\"\"         from .filtered import FilteredToolset          return FilteredToolset(self, filter_func)      def prefixed(self, prefix: str) -> PrefixedToolset[AgentDepsT]:         \"\"\"Returns a new toolset that prefixes the names of this toolset's tools.          See [toolset docs](../toolsets.md#prefixing-tool-names) for more information.         \"\"\"         from .prefixed import PrefixedToolset          return PrefixedToolset(self, prefix)      def prepared(self, prepare_func: ToolsPrepareFunc[AgentDepsT]) -> PreparedToolset[AgentDepsT]:         \"\"\"Returns a new toolset that prepares this toolset's tools using a prepare function that takes the agent context and the original tool definitions.          See [toolset docs](../toolsets.md#preparing-tool-definitions) for more information.         \"\"\"         from .prepared import PreparedToolset          return PreparedToolset(self, prepare_func)      def renamed(self, name_map: dict[str, str]) -> RenamedToolset[AgentDepsT]:         \"\"\"Returns a new toolset that renames this toolset's tools using a dictionary mapping new names to original names.          See [toolset docs](../toolsets.md#renaming-tools) for more information.         \"\"\"         from .renamed import RenamedToolset          return RenamedToolset(self, name_map)      def approval_required(         self,         approval_required_func: Callable[[RunContext[AgentDepsT], ToolDefinition, dict[str, Any]], bool] = (             lambda ctx, tool_def, tool_args: True         ),     ) -> ApprovalRequiredToolset[AgentDepsT]:         \"\"\"Returns a new toolset that requires (some) calls to tools it contains to be approved.          See [toolset docs](../toolsets.md#requiring-tool-approval) for more information.         \"\"\"         from .approval_required import ApprovalRequiredToolset          return ApprovalRequiredToolset(self, approval_required_func) ``` |", "url": "https://ai.pydantic.dev/toolsets/index.html#abstracttoolset", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractToolset", "anchor": "abstracttoolset", "md_text": "#### id `abstractmethod` `property`\n\n```\nid: str | None\n```\n\nAn ID for the toolset that is unique among all toolsets registered with the same agent.\n\nIf you're implementing a concrete implementation that users can instantiate more than once, you should let them optionally pass a custom ID to the constructor and return that here.\n\nA toolset needs to have an ID in order to be used in a durable execution environment like Temporal, in which case the ID will be used to identify the toolset's activities within the workflow.\n\n#### label `property`\n\n```\nlabel: str\n```\n\nThe name of the toolset for use in error messages.\n\n#### tool\\_name\\_conflict\\_hint `property`\n\n```\ntool_name_conflict_hint: str\n```\n\nA hint for how to avoid name conflicts with other toolsets for use in error messages.\n\n#### \\_\\_aenter\\_\\_ `async`\n\n```\n__aenter__() -> Self\n```\n\nEnter the toolset context.\n\nThis is where you can set up network connections in a concrete implementation.\n\nSource code in `pydantic_ai_slim/pydantic_ai/toolsets/abstract.py`\n\n|  |  |\n| --- | --- |\n| ```  98  99 100 101 102 103 ``` | ``` async def __aenter__(self) -> Self:     \"\"\"Enter the toolset context.      This is where you can set up network connections in a concrete implementation.     \"\"\"     return self ``` |\n\n#### \\_\\_aexit\\_\\_ `async`\n\n```\n__aexit__(*args: Any) -> bool | None\n```\n\nExit the toolset context.\n\nThis is where you can tear down network connections in a concrete implementation.\n\nSource code in `pydantic_ai_slim/pydantic_ai/toolsets/abstract.py`\n\n|  |  |\n| --- | --- |\n| ``` 105 106 107 108 109 110 ``` | ``` async def __aexit__(self, *args: Any) -> bool | None:     \"\"\"Exit the toolset context.      This is where you can tear down network connections in a concrete implementation.     \"\"\"     return None ``` |\n\n#### get\\_tools `abstractmethod` `async`\n\n```\nget_tools(\n    ctx: RunContext[AgentDepsT],\n) -> dict[str, ToolsetTool[AgentDepsT]]\n```\n\nThe tools that are available in this toolset.\n\nSource code in `pydantic_ai_slim/pydantic_ai/toolsets/abstract.py`\n\n|  |  |\n| --- | --- |\n| ``` 112 113 114 115 ``` | ``` @abstractmethod async def get_tools(self, ctx: RunContext[AgentDepsT]) -> dict[str, ToolsetTool[AgentDepsT]]:     \"\"\"The tools that are available in this toolset.\"\"\"     raise NotImplementedError() ``` |\n\n#### call\\_tool `abstractmethod` `async`\n\n```\ncall_tool(\n    name: str,\n    tool_args: dict[str, Any],\n    ctx: RunContext[AgentDepsT],\n    tool: ToolsetTool[AgentDepsT],\n) -> Any\n```\n\nCall a tool with the given arguments.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `name` | `str` | The name of the tool to call. | *required* |\n| `tool_args` | `dict[str, Any]` | The arguments to pass to the tool. | *required* |\n| `ctx` | `RunContext[AgentDepsT]` | The run context. | *required* |\n| `tool` | `ToolsetTool[AgentDepsT]` | The tool definition returned by [`get_tools`](index.html#pydantic_ai.toolsets.AbstractToolset.get_tools) that was called. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/toolsets/abstract.py`\n\n|  |  |\n| --- | --- |\n| ``` 117 118 119 120 121 122 123 124 125 126 127 128 129 ``` | ``` @abstractmethod async def call_tool(     self, name: str, tool_args: dict[str, Any], ctx: RunContext[AgentDepsT], tool: ToolsetTool[AgentDepsT] ) -> Any:     \"\"\"Call a tool with the given arguments.      Args:         name: The name of the tool to call.         tool_args: The arguments to pass to the tool.         ctx: The run context.         tool: The tool definition returned by [`get_tools`][pydantic_ai.toolsets.AbstractToolset.get_tools] that was called.     \"\"\"     raise NotImplementedError() ``` |\n\n#### apply\n\n```\napply(\n    visitor: Callable[[AbstractToolset[AgentDepsT]], None],\n) -> None\n```\n\nRun a visitor function on all \"leaf\" toolsets (i.e. those that implement their own tool listing and calling).\n\nSource code in `pydantic_ai_slim/pydantic_ai/toolsets/abstract.py`\n\n|  |  |\n| --- | --- |\n| ``` 131 132 133 ``` | ``` def apply(self, visitor: Callable[[AbstractToolset[AgentDepsT]], None]) -> None:     \"\"\"Run a visitor function on all \"leaf\" toolsets (i.e. those that implement their own tool listing and calling).\"\"\"     visitor(self) ``` |\n\n#### visit\\_and\\_replace\n\n```\nvisit_and_replace(\n    visitor: Callable[\n        [AbstractToolset[AgentDepsT]],\n        AbstractToolset[AgentDepsT],\n    ],\n) -> AbstractToolset[AgentDepsT]\n```", "url": "https://ai.pydantic.dev/toolsets/index.html#abstracttoolset", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractToolset", "anchor": "abstracttoolset", "md_text": "Run a visitor function on all \"leaf\" toolsets (i.e. those that implement their own tool listing and calling) and replace them in the hierarchy with the result of the function.\n\nSource code in `pydantic_ai_slim/pydantic_ai/toolsets/abstract.py`\n\n|  |  |\n| --- | --- |\n| ``` 135 136 137 138 139 ``` | ``` def visit_and_replace(     self, visitor: Callable[[AbstractToolset[AgentDepsT]], AbstractToolset[AgentDepsT]] ) -> AbstractToolset[AgentDepsT]:     \"\"\"Run a visitor function on all \"leaf\" toolsets (i.e. those that implement their own tool listing and calling) and replace them in the hierarchy with the result of the function.\"\"\"     return visitor(self) ``` |\n\n#### filtered\n\n```\nfiltered(\n    filter_func: Callable[\n        [RunContext[AgentDepsT], ToolDefinition], bool\n    ],\n) -> FilteredToolset[AgentDepsT]\n```\n\nReturns a new toolset that filters this toolset's tools using a filter function that takes the agent context and the tool definition.\n\nSee [toolset docs](https://ai.pydantic.dev/toolsets/#filtering-tools) for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/toolsets/abstract.py`\n\n|  |  |\n| --- | --- |\n| ``` 141 142 143 144 145 146 147 148 149 150 ``` | ``` def filtered(     self, filter_func: Callable[[RunContext[AgentDepsT], ToolDefinition], bool] ) -> FilteredToolset[AgentDepsT]:     \"\"\"Returns a new toolset that filters this toolset's tools using a filter function that takes the agent context and the tool definition.      See [toolset docs](../toolsets.md#filtering-tools) for more information.     \"\"\"     from .filtered import FilteredToolset      return FilteredToolset(self, filter_func) ``` |\n\n#### prefixed\n\n```\nprefixed(prefix: str) -> PrefixedToolset[AgentDepsT]\n```\n\nReturns a new toolset that prefixes the names of this toolset's tools.\n\nSee [toolset docs](https://ai.pydantic.dev/toolsets/#prefixing-tool-names) for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/toolsets/abstract.py`\n\n|  |  |\n| --- | --- |\n| ``` 152 153 154 155 156 157 158 159 ``` | ``` def prefixed(self, prefix: str) -> PrefixedToolset[AgentDepsT]:     \"\"\"Returns a new toolset that prefixes the names of this toolset's tools.      See [toolset docs](../toolsets.md#prefixing-tool-names) for more information.     \"\"\"     from .prefixed import PrefixedToolset      return PrefixedToolset(self, prefix) ``` |\n\n#### prepared\n\n```\nprepared(\n    prepare_func: ToolsPrepareFunc[AgentDepsT],\n) -> PreparedToolset[AgentDepsT]\n```\n\nReturns a new toolset that prepares this toolset's tools using a prepare function that takes the agent context and the original tool definitions.\n\nSee [toolset docs](https://ai.pydantic.dev/toolsets/#preparing-tool-definitions) for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/toolsets/abstract.py`\n\n|  |  |\n| --- | --- |\n| ``` 161 162 163 164 165 166 167 168 ``` | ``` def prepared(self, prepare_func: ToolsPrepareFunc[AgentDepsT]) -> PreparedToolset[AgentDepsT]:     \"\"\"Returns a new toolset that prepares this toolset's tools using a prepare function that takes the agent context and the original tool definitions.      See [toolset docs](../toolsets.md#preparing-tool-definitions) for more information.     \"\"\"     from .prepared import PreparedToolset      return PreparedToolset(self, prepare_func) ``` |\n\n#### renamed\n\n```\nrenamed(\n    name_map: dict[str, str],\n) -> RenamedToolset[AgentDepsT]\n```\n\nReturns a new toolset that renames this toolset's tools using a dictionary mapping new names to original names.\n\nSee [toolset docs](https://ai.pydantic.dev/toolsets/#renaming-tools) for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/toolsets/abstract.py`\n\n|  |  |\n| --- | --- |\n| ``` 170 171 172 173 174 175 176 177 ``` | ``` def renamed(self, name_map: dict[str, str]) -> RenamedToolset[AgentDepsT]:     \"\"\"Returns a new toolset that renames this toolset's tools using a dictionary mapping new names to original names.      See [toolset docs](../toolsets.md#renaming-tools) for more information.     \"\"\"     from .renamed import RenamedToolset      return RenamedToolset(self, name_map) ``` |\n\n#### approval\\_required\n\n```\napproval_required(\n    approval_required_func: Callable[\n        [\n            RunContext[AgentDepsT],\n            ToolDefinition,\n            dict[str, Any],\n        ],\n        bool,\n    ] = lambda ctx, tool_def, tool_args: True\n) -> ApprovalRequiredToolset[AgentDepsT]\n```\n\nReturns a new toolset that requires (some) calls to tools it contains to be approved.\n\nSee [toolset docs](https://ai.pydantic.dev/toolsets/#requiring-tool-approval) for more information.", "url": "https://ai.pydantic.dev/toolsets/index.html#abstracttoolset", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractToolset", "anchor": "abstracttoolset", "md_text": "Source code in `pydantic_ai_slim/pydantic_ai/toolsets/abstract.py`\n\n|  |  |\n| --- | --- |\n| ``` 179 180 181 182 183 184 185 186 187 188 189 190 191 ``` | ``` def approval_required(     self,     approval_required_func: Callable[[RunContext[AgentDepsT], ToolDefinition, dict[str, Any]], bool] = (         lambda ctx, tool_def, tool_args: True     ), ) -> ApprovalRequiredToolset[AgentDepsT]:     \"\"\"Returns a new toolset that requires (some) calls to tools it contains to be approved.      See [toolset docs](../toolsets.md#requiring-tool-approval) for more information.     \"\"\"     from .approval_required import ApprovalRequiredToolset      return ApprovalRequiredToolset(self, approval_required_func) ``` |", "url": "https://ai.pydantic.dev/toolsets/index.html#abstracttoolset", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "CombinedToolset `dataclass`", "anchor": "combinedtoolset-dataclass", "md_text": "Bases: `AbstractToolset[AgentDepsT]`\n\nA toolset that combines multiple toolsets.\n\nSee [toolset docs](https://ai.pydantic.dev/toolsets/#combining-toolsets) for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/toolsets/combined.py`\n\n|  |  |\n| --- | --- |\n| ``` 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 ``` | ``` @dataclass class CombinedToolset(AbstractToolset[AgentDepsT]):     \"\"\"A toolset that combines multiple toolsets.      See [toolset docs](../toolsets.md#combining-toolsets) for more information.     \"\"\"      toolsets: Sequence[AbstractToolset[AgentDepsT]]      _enter_lock: Lock = field(compare=False, init=False, default_factory=Lock)     _entered_count: int = field(init=False, default=0)     _exit_stack: AsyncExitStack | None = field(init=False, default=None)      @property     def id(self) -> str | None:         return None  # pragma: no cover      @property     def label(self) -> str:         return f'{self.__class__.__name__}({\", \".join(toolset.label for toolset in self.toolsets)})'  # pragma: no cover      async def __aenter__(self) -> Self:         async with self._enter_lock:             if self._entered_count == 0:                 async with AsyncExitStack() as exit_stack:                     for toolset in self.toolsets:                         await exit_stack.enter_async_context(toolset)                     self._exit_stack = exit_stack.pop_all()             self._entered_count += 1         return self      async def __aexit__(self, *args: Any) -> bool | None:         async with self._enter_lock:             self._entered_count -= 1             if self._entered_count == 0 and self._exit_stack is not None:                 await self._exit_stack.aclose()                 self._exit_stack = None      async def get_tools(self, ctx: RunContext[AgentDepsT]) -> dict[str, ToolsetTool[AgentDepsT]]:         toolsets_tools = await asyncio.gather(*(toolset.get_tools(ctx) for toolset in self.toolsets))         all_tools: dict[str, ToolsetTool[AgentDepsT]] = {}          for toolset, tools in zip(self.toolsets, toolsets_tools):             for name, tool in tools.items():                 tool_toolset = tool.toolset                 if existing_tool := all_tools.get(name):                     capitalized_toolset_label = tool_toolset.label[0].upper() + tool_toolset.label[1:]                     raise UserError(                         f'{capitalized_toolset_label} defines a tool whose name conflicts with existing tool from {existing_tool.toolset.label}: {name!r}. {toolset.tool_name_conflict_hint}'                     )                  all_tools[name] = _CombinedToolsetTool(                     toolset=tool_toolset,                     tool_def=tool.tool_def,                     max_retries=tool.max_retries,                     args_validator=tool.args_validator,                     source_toolset=toolset,                     source_tool=tool,                 )         return all_tools      async def call_tool(         self, name: str, tool_args: dict[str, Any], ctx: RunContext[AgentDepsT], tool: ToolsetTool[AgentDepsT]     ) -> Any:         assert isinstance(tool, _CombinedToolsetTool)         return await tool.source_toolset.call_tool(name, tool_args, ctx, tool.source_tool)      def apply(self, visitor: Callable[[AbstractToolset[AgentDepsT]], None]) -> None:         for toolset in self.toolsets:             toolset.apply(visitor)      def visit_and_replace(         self, visitor: Callable[[AbstractToolset[AgentDepsT]], AbstractToolset[AgentDepsT]]     ) -> AbstractToolset[AgentDepsT]:         return replace(self, toolsets=[toolset.visit_and_replace(visitor) for toolset in self.toolsets]) ``` |", "url": "https://ai.pydantic.dev/toolsets/index.html#combinedtoolset-dataclass", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "ExternalToolset", "anchor": "externaltoolset", "md_text": "Bases: `AbstractToolset[AgentDepsT]`\n\nA toolset that holds tools whose results will be produced outside of the Pydantic AI agent run in which they were called.\n\nSee [toolset docs](https://ai.pydantic.dev/toolsets/#external-toolset) for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/toolsets/external.py`\n\n|  |  |\n| --- | --- |\n| ``` 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 ``` | ``` class ExternalToolset(AbstractToolset[AgentDepsT]):     \"\"\"A toolset that holds tools whose results will be produced outside of the Pydantic AI agent run in which they were called.      See [toolset docs](../toolsets.md#external-toolset) for more information.     \"\"\"      tool_defs: list[ToolDefinition]     _id: str | None      def __init__(self, tool_defs: list[ToolDefinition], *, id: str | None = None):         self.tool_defs = tool_defs         self._id = id      @property     def id(self) -> str | None:         return self._id      async def get_tools(self, ctx: RunContext[AgentDepsT]) -> dict[str, ToolsetTool[AgentDepsT]]:         return {             tool_def.name: ToolsetTool(                 toolset=self,                 tool_def=replace(tool_def, kind='external'),                 max_retries=0,                 args_validator=TOOL_SCHEMA_VALIDATOR,             )             for tool_def in self.tool_defs         }      async def call_tool(         self, name: str, tool_args: dict[str, Any], ctx: RunContext[AgentDepsT], tool: ToolsetTool[AgentDepsT]     ) -> Any:         raise NotImplementedError('External tools cannot be called directly') ``` |", "url": "https://ai.pydantic.dev/toolsets/index.html#externaltoolset", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "ApprovalRequiredToolset `dataclass`", "anchor": "approvalrequiredtoolset-dataclass", "md_text": "Bases: `WrapperToolset[AgentDepsT]`\n\nA toolset that requires (some) calls to tools it contains to be approved.\n\nSee [toolset docs](https://ai.pydantic.dev/toolsets/#requiring-tool-approval) for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/toolsets/approval_required.py`\n\n|  |  |\n| --- | --- |\n| ``` 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 ``` | ``` @dataclass class ApprovalRequiredToolset(WrapperToolset[AgentDepsT]):     \"\"\"A toolset that requires (some) calls to tools it contains to be approved.      See [toolset docs](../toolsets.md#requiring-tool-approval) for more information.     \"\"\"      approval_required_func: Callable[[RunContext[AgentDepsT], ToolDefinition, dict[str, Any]], bool] = (         lambda ctx, tool_def, tool_args: True     )      async def call_tool(         self, name: str, tool_args: dict[str, Any], ctx: RunContext[AgentDepsT], tool: ToolsetTool[AgentDepsT]     ) -> Any:         if not ctx.tool_call_approved and self.approval_required_func(ctx, tool.tool_def, tool_args):             raise ApprovalRequired          return await super().call_tool(name, tool_args, ctx, tool) ``` |", "url": "https://ai.pydantic.dev/toolsets/index.html#approvalrequiredtoolset-dataclass", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "FilteredToolset `dataclass`", "anchor": "filteredtoolset-dataclass", "md_text": "Bases: `WrapperToolset[AgentDepsT]`\n\nA toolset that filters the tools it contains using a filter function that takes the agent context and the tool definition.\n\nSee [toolset docs](https://ai.pydantic.dev/toolsets/#filtering-tools) for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/toolsets/filtered.py`\n\n|  |  |\n| --- | --- |\n| ``` 12 13 14 15 16 17 18 19 20 21 22 23 24 ``` | ``` @dataclass class FilteredToolset(WrapperToolset[AgentDepsT]):     \"\"\"A toolset that filters the tools it contains using a filter function that takes the agent context and the tool definition.      See [toolset docs](../toolsets.md#filtering-tools) for more information.     \"\"\"      filter_func: Callable[[RunContext[AgentDepsT], ToolDefinition], bool]      async def get_tools(self, ctx: RunContext[AgentDepsT]) -> dict[str, ToolsetTool[AgentDepsT]]:         return {             name: tool for name, tool in (await super().get_tools(ctx)).items() if self.filter_func(ctx, tool.tool_def)         } ``` |", "url": "https://ai.pydantic.dev/toolsets/index.html#filteredtoolset-dataclass", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "FunctionToolset", "anchor": "functiontoolset", "md_text": "Bases: `AbstractToolset[AgentDepsT]`\n\nA toolset that lets Python functions be used as tools.\n\nSee [toolset docs](https://ai.pydantic.dev/toolsets/#function-toolset) for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/toolsets/function.py`", "url": "https://ai.pydantic.dev/toolsets/index.html#functiontoolset", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "FunctionToolset", "anchor": "functiontoolset", "md_text": "|  |  |\n| --- | --- |\n| ```  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 ``` | ``` class FunctionToolset(AbstractToolset[AgentDepsT]):     \"\"\"A toolset that lets Python functions be used as tools.      See [toolset docs](../toolsets.md#function-toolset) for more information.     \"\"\"      tools: dict[str, Tool[Any]]     max_retries: int     _id: str | None     docstring_format: DocstringFormat     require_parameter_descriptions: bool     schema_generator: type[GenerateJsonSchema]      def __init__(         self,         tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] = [],         *,         max_retries: int = 1,         docstring_format: DocstringFormat = 'auto',         require_parameter_descriptions: bool = False,         schema_generator: type[GenerateJsonSchema] = GenerateToolJsonSchema,         strict: bool | None = None,         sequential: bool = False,         requires_approval: bool = False,         metadata: dict[str, Any] | None = None,         id: str | None = None,     ):         \"\"\"Build a new function toolset.          Args:             tools: The tools to add to the toolset.             max_retries: The maximum number of retries for each tool during a run.                 Applies to all tools, unless overridden when adding a tool.             docstring_format: Format of tool docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].                 Defaults to `'auto'`, such that the format is inferred from the structure of the docstring.                 Applies to all tools, unless overridden when adding a tool.             require_parameter_descriptions: If True, raise an error if a parameter description is missing. Defaults to False.                 Applies to all tools, unless overridden when adding a tool.             schema_generator: The JSON schema generator class to use for this tool. Defaults to `GenerateToolJsonSchema`.                 Applies to all tools, unless overridden when adding a tool.             strict: Whether to enforce JSON schema compliance (only affects OpenAI).                 See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.             sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.                 Applies to all tools, unless overridden when adding a tool.             requires_approval: Whether this tool requires human-in-the-loop approval. Defaults to False.                 See the [tools documentation](../deferred-tools.md#human-in-the-loop-tool-approval) for more info.                 Applies to all tools, unless overridden when adding a tool.             metadata: Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization.                 Applies to all tools, unless overridden when adding a tool, which will be merged with the toolset's metadata.             id: An optional unique ID for the toolset. A toolset needs to have an ID in order to be used in a durable execution environment like Temporal,                 in which case the ID will be used to identify the toolset's activities within the workflow.         \"\"\"         self.max_retries = max_retries         self._id = id         self.docstring_format = docstring_format         self.require_parameter_descriptions = require_parameter_descriptions         self.schema_generator = schema_generator         self.strict = strict         self.sequential = sequential         self.requires_approval = requires_approval         self.metadata = metadata          self.tools = {}         for tool in tools:             if isinstance(tool, Tool):                 self.add_tool(tool)             else:                 self.add_function(tool)      @property     def id(self) -> str | None:         return self._id      @overload     def tool(self, func: ToolFuncEither[AgentDepsT, ToolParams], /) -> ToolFuncEither[AgentDepsT, ToolParams]: ...      @overload     def tool(         self,         /,         *,         name: str | None = None,         description: str | None = None,         retries: int | None = None,         prepare: ToolPrepareFunc[AgentDepsT] | None = None,         docstring_format: DocstringFormat | None = None,         require_parameter_descriptions: bool | None = None,         schema_generator: type[GenerateJsonSchema] | None = None,         strict: bool | None = None,         sequential: bool | None = None,         requires_approval: bool | None = None,         metadata: dict[str, Any] | None = None,     ) -> Callable[[ToolFuncEither[AgentDepsT, ToolParams]], ToolFuncEither[AgentDepsT, ToolParams]]: ...      def tool(         self,         func: ToolFuncEither[AgentDepsT, ToolParams] | None = None,         /,         *,         name: str | None = None,         description: str | None = None,         retries: int | None = None,         prepare: ToolPrepareFunc[AgentDepsT] | None = None,         docstring_format: DocstringFormat | None = None,         require_parameter_descriptions: bool | None = None,         schema_generator: type[GenerateJsonSchema] | None = None,         strict: bool | None = None,         sequential: bool | None = None,         requires_approval: bool | None = None,         metadata: dict[str, Any] | None = None,     ) -> Any:         \"\"\"Decorator to register a tool function which takes [`RunContext`][pydantic_ai.tools.RunContext] as its first argument.          Can decorate a sync or async functions.          The docstring is inspected to extract both the tool description and description of each parameter,         [learn more](../tools.md#function-tools-and-schema).          We can't add overloads for every possible signature of tool, since the return type is a recursive union         so the signature of functions decorated with `@toolset.tool` is obscured.          Example:         ```python         from pydantic_ai import Agent, FunctionToolset, RunContext          toolset = FunctionToolset()          @toolset.tool         def foobar(ctx: RunContext[int], x: int) -> int:             return ctx.deps + x          @toolset.tool(retries=2)         async def spam(ctx: RunContext[str], y: float) -> float:             return ctx.deps + y          agent = Agent('test', toolsets=[toolset], deps_type=int)         result = agent.run_sync('foobar', deps=1)         print(result.output)         #> {\"foobar\":1,\"spam\":1.0}         ```          Args:             func: The tool function to register.             name: The name of the tool, defaults to the function name.             description: The description of the tool,defaults to the function docstring.             retries: The number of retries to allow for this tool, defaults to the agent's default retries,                 which defaults to 1.             prepare: custom method to prepare the tool definition for each step, return `None` to omit this                 tool from a given step. This is useful if you want to customise a tool at call time,                 or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].             docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].                 If `None`, the default value is determined by the toolset.             require_parameter_descriptions: If True, raise an error if a parameter description is missing.                 If `None`, the default value is determined by the toolset.             schema_generator: The JSON schema generator class to use for this tool.                 If `None`, the default value is determined by the toolset.             strict: Whether to enforce JSON schema compliance (only affects OpenAI).                 See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.                 If `None`, the default value is determined by the toolset.             sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.                 If `None`, the default value is determined by the toolset.             requires_approval: Whether this tool requires human-in-the-loop approval. Defaults to False.                 See the [tools documentation](../deferred-tools.md#human-in-the-loop-tool-approval) for more info.                 If `None`, the default value is determined by the toolset.             metadata: Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization.                 If `None`, the default value is determined by the toolset. If provided, it will be merged with the toolset's metadata.         \"\"\"          def tool_decorator(             func_: ToolFuncEither[AgentDepsT, ToolParams],         ) -> ToolFuncEither[AgentDepsT, ToolParams]:             # noinspection PyTypeChecker             self.add_function(                 func=func_,                 takes_ctx=None,                 name=name,                 description=description,                 retries=retries,                 prepare=prepare,                 docstring_format=docstring_format,                 require_parameter_descriptions=require_parameter_descriptions,                 schema_generator=schema_generator,                 strict=strict,                 sequential=sequential,                 requires_approval=requires_approval,                 metadata=metadata,             )             return func_          return tool_decorator if func is None else tool_decorator(func)      def add_function(         self,         func: ToolFuncEither[AgentDepsT, ToolParams],         takes_ctx: bool | None = None,         name: str | None = None,         description: str | None = None,         retries: int | None = None,         prepare: ToolPrepareFunc[AgentDepsT] | None = None,         docstring_format: DocstringFormat | None = None,         require_parameter_descriptions: bool | None = None,         schema_generator: type[GenerateJsonSchema] | None = None,         strict: bool | None = None,         sequential: bool | None = None,         requires_approval: bool | None = None,         metadata: dict[str, Any] | None = None,     ) -> None:         \"\"\"Add a function as a tool to the toolset.          Can take a sync or async function.          The docstring is inspected to extract both the tool description and description of each parameter,         [learn more](../tools.md#function-tools-and-schema).          Args:             func: The tool function to register.             takes_ctx: Whether the function takes a [`RunContext`][pydantic_ai.tools.RunContext] as its first argument. If `None`, this is inferred from the function signature.             name: The name of the tool, defaults to the function name.             description: The description of the tool, defaults to the function docstring.             retries: The number of retries to allow for this tool, defaults to the agent's default retries,                 which defaults to 1.             prepare: custom method to prepare the tool definition for each step, return `None` to omit this                 tool from a given step. This is useful if you want to customise a tool at call time,                 or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].             docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].                 If `None`, the default value is determined by the toolset.             require_parameter_descriptions: If True, raise an error if a parameter description is missing.                 If `None`, the default value is determined by the toolset.             schema_generator: The JSON schema generator class to use for this tool.                 If `None`, the default value is determined by the toolset.             strict: Whether to enforce JSON schema compliance (only affects OpenAI).                 See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.                 If `None`, the default value is determined by the toolset.             sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.                 If `None`, the default value is determined by the toolset.             requires_approval: Whether this tool requires human-in-the-loop approval. Defaults to False.                 See the [tools documentation](../deferred-tools.md#human-in-the-loop-tool-approval) for more info.                 If `None`, the default value is determined by the toolset.             metadata: Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization.                 If `None`, the default value is determined by the toolset. If provided, it will be merged with the toolset's metadata.         \"\"\"         if docstring_format is None:             docstring_format = self.docstring_format         if require_parameter_descriptions is None:             require_parameter_descriptions = self.require_parameter_descriptions         if schema_generator is None:             schema_generator = self.schema_generator         if strict is None:             strict = self.strict         if sequential is None:             sequential = self.sequential         if requires_approval is None:             requires_approval = self.requires_approval          tool = Tool[AgentDepsT](             func,             takes_ctx=takes_ctx,             name=name,             description=description,             max_retries=retries,             prepare=prepare,             docstring_format=docstring_format,             require_parameter_descriptions=require_parameter_descriptions,             schema_generator=schema_generator,             strict=strict,             sequential=sequential,             requires_approval=requires_approval,             metadata=metadata,         )         self.add_tool(tool)      def add_tool(self, tool: Tool[AgentDepsT]) -> None:         \"\"\"Add a tool to the toolset.          Args:             tool: The tool to add.         \"\"\"         if tool.name in self.tools:             raise UserError(f'Tool name conflicts with existing tool: {tool.name!r}')         if tool.max_retries is None:             tool.max_retries = self.max_retries         if self.metadata is not None:             tool.metadata = self.metadata | (tool.metadata or {})         self.tools[tool.name] = tool      async def get_tools(self, ctx: RunContext[AgentDepsT]) -> dict[str, ToolsetTool[AgentDepsT]]:         tools: dict[str, ToolsetTool[AgentDepsT]] = {}         for original_name, tool in self.tools.items():             max_retries = tool.max_retries if tool.max_retries is not None else self.max_retries             run_context = replace(                 ctx,                 tool_name=original_name,                 retry=ctx.retries.get(original_name, 0),                 max_retries=max_retries,             )             tool_def = await tool.prepare_tool_def(run_context)             if not tool_def:                 continue              new_name = tool_def.name             if new_name in tools:                 if new_name != original_name:                     raise UserError(f'Renaming tool {original_name!r} to {new_name!r} conflicts with existing tool.')                 else:                     raise UserError(f'Tool name conflicts with previously renamed tool: {new_name!r}.')              tools[new_name] = FunctionToolsetTool(                 toolset=self,                 tool_def=tool_def,                 max_retries=max_retries,                 args_validator=tool.function_schema.validator,                 call_func=tool.function_schema.call,                 is_async=tool.function_schema.is_async,             )         return tools      async def call_tool(         self, name: str, tool_args: dict[str, Any], ctx: RunContext[AgentDepsT], tool: ToolsetTool[AgentDepsT]     ) -> Any:         assert isinstance(tool, FunctionToolsetTool)         return await tool.call_func(tool_args, ctx) ``` |", "url": "https://ai.pydantic.dev/toolsets/index.html#functiontoolset", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "FunctionToolset", "anchor": "functiontoolset", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    tools: Sequence[\n        Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]\n    ] = [],\n    *,\n    max_retries: int = 1,\n    docstring_format: DocstringFormat = \"auto\",\n    require_parameter_descriptions: bool = False,\n    schema_generator: type[\n        GenerateJsonSchema\n    ] = GenerateToolJsonSchema,\n    strict: bool | None = None,\n    sequential: bool = False,\n    requires_approval: bool = False,\n    metadata: dict[str, Any] | None = None,\n    id: str | None = None\n)\n```\n\nBuild a new function toolset.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `tools` | `Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]]` | The tools to add to the toolset. | `[]` |\n| `max_retries` | `int` | The maximum number of retries for each tool during a run. Applies to all tools, unless overridden when adding a tool. | `1` |\n| `docstring_format` | `DocstringFormat` | Format of tool docstring, see [`DocstringFormat`](../tools/index.html#pydantic_ai.tools.DocstringFormat). Defaults to `'auto'`, such that the format is inferred from the structure of the docstring. Applies to all tools, unless overridden when adding a tool. | `'auto'` |\n| `require_parameter_descriptions` | `bool` | If True, raise an error if a parameter description is missing. Defaults to False. Applies to all tools, unless overridden when adding a tool. | `False` |\n| `schema_generator` | `type[GenerateJsonSchema]` | The JSON schema generator class to use for this tool. Defaults to `GenerateToolJsonSchema`. Applies to all tools, unless overridden when adding a tool. | `GenerateToolJsonSchema` |\n| `strict` | `bool | None` | Whether to enforce JSON schema compliance (only affects OpenAI). See [`ToolDefinition`](../tools/index.html#pydantic_ai.tools.ToolDefinition) for more info. | `None` |\n| `sequential` | `bool` | Whether the function requires a sequential/serial execution environment. Defaults to False. Applies to all tools, unless overridden when adding a tool. | `False` |\n| `requires_approval` | `bool` | Whether this tool requires human-in-the-loop approval. Defaults to False. See the [tools documentation](https://ai.pydantic.dev/deferred-tools/#human-in-the-loop-tool-approval) for more info. Applies to all tools, unless overridden when adding a tool. | `False` |\n| `metadata` | `dict[str, Any] | None` | Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization. Applies to all tools, unless overridden when adding a tool, which will be merged with the toolset's metadata. | `None` |\n| `id` | `str | None` | An optional unique ID for the toolset. A toolset needs to have an ID in order to be used in a durable execution environment like Temporal, in which case the ID will be used to identify the toolset's activities within the workflow. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/toolsets/function.py`", "url": "https://ai.pydantic.dev/toolsets/index.html#functiontoolset", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "FunctionToolset", "anchor": "functiontoolset", "md_text": "|  |  |\n| --- | --- |\n| ``` 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 ``` | ``` def __init__(     self,     tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] = [],     *,     max_retries: int = 1,     docstring_format: DocstringFormat = 'auto',     require_parameter_descriptions: bool = False,     schema_generator: type[GenerateJsonSchema] = GenerateToolJsonSchema,     strict: bool | None = None,     sequential: bool = False,     requires_approval: bool = False,     metadata: dict[str, Any] | None = None,     id: str | None = None, ):     \"\"\"Build a new function toolset.      Args:         tools: The tools to add to the toolset.         max_retries: The maximum number of retries for each tool during a run.             Applies to all tools, unless overridden when adding a tool.         docstring_format: Format of tool docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].             Defaults to `'auto'`, such that the format is inferred from the structure of the docstring.             Applies to all tools, unless overridden when adding a tool.         require_parameter_descriptions: If True, raise an error if a parameter description is missing. Defaults to False.             Applies to all tools, unless overridden when adding a tool.         schema_generator: The JSON schema generator class to use for this tool. Defaults to `GenerateToolJsonSchema`.             Applies to all tools, unless overridden when adding a tool.         strict: Whether to enforce JSON schema compliance (only affects OpenAI).             See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.         sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.             Applies to all tools, unless overridden when adding a tool.         requires_approval: Whether this tool requires human-in-the-loop approval. Defaults to False.             See the [tools documentation](../deferred-tools.md#human-in-the-loop-tool-approval) for more info.             Applies to all tools, unless overridden when adding a tool.         metadata: Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization.             Applies to all tools, unless overridden when adding a tool, which will be merged with the toolset's metadata.         id: An optional unique ID for the toolset. A toolset needs to have an ID in order to be used in a durable execution environment like Temporal,             in which case the ID will be used to identify the toolset's activities within the workflow.     \"\"\"     self.max_retries = max_retries     self._id = id     self.docstring_format = docstring_format     self.require_parameter_descriptions = require_parameter_descriptions     self.schema_generator = schema_generator     self.strict = strict     self.sequential = sequential     self.requires_approval = requires_approval     self.metadata = metadata      self.tools = {}     for tool in tools:         if isinstance(tool, Tool):             self.add_tool(tool)         else:             self.add_function(tool) ``` |\n\n#### tool\n\n```\ntool(\n    func: ToolFuncEither[AgentDepsT, ToolParams],\n) -> ToolFuncEither[AgentDepsT, ToolParams]\n\ntool(\n    *,\n    name: str | None = None,\n    description: str | None = None,\n    retries: int | None = None,\n    prepare: ToolPrepareFunc[AgentDepsT] | None = None,\n    docstring_format: DocstringFormat | None = None,\n    require_parameter_descriptions: bool | None = None,\n    schema_generator: (\n        type[GenerateJsonSchema] | None\n    ) = None,\n    strict: bool | None = None,\n    sequential: bool | None = None,\n    requires_approval: bool | None = None,\n    metadata: dict[str, Any] | None = None\n) -> Callable[\n    [ToolFuncEither[AgentDepsT, ToolParams]],\n    ToolFuncEither[AgentDepsT, ToolParams],\n]\n\ntool(\n    func: (\n        ToolFuncEither[AgentDepsT, ToolParams] | None\n    ) = None,\n    /,\n    *,\n    name: str | None = None,\n    description: str | None = None,\n    retries: int | None = None,\n    prepare: ToolPrepareFunc[AgentDepsT] | None = None,\n    docstring_format: DocstringFormat | None = None,\n    require_parameter_descriptions: bool | None = None,\n    schema_generator: (\n        type[GenerateJsonSchema] | None\n    ) = None,\n    strict: bool | None = None,\n    sequential: bool | None = None,\n    requires_approval: bool | None = None,\n    metadata: dict[str, Any] | None = None,\n) -> Any\n```\n\nDecorator to register a tool function which takes [`RunContext`](../tools/index.html#pydantic_ai.tools.RunContext) as its first argument.\n\nCan decorate a sync or async functions.", "url": "https://ai.pydantic.dev/toolsets/index.html#functiontoolset", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "FunctionToolset", "anchor": "functiontoolset", "md_text": "The docstring is inspected to extract both the tool description and description of each parameter,\n[learn more](https://ai.pydantic.dev/tools/#function-tools-and-schema).\n\nWe can't add overloads for every possible signature of tool, since the return type is a recursive union\nso the signature of functions decorated with `@toolset.tool` is obscured.\n\nExample:\n\n```\nfrom pydantic_ai import Agent, FunctionToolset, RunContext\n\ntoolset = FunctionToolset()\n\n@toolset.tool\ndef foobar(ctx: RunContext[int], x: int) -> int:\n    return ctx.deps + x\n\n@toolset.tool(retries=2)\nasync def spam(ctx: RunContext[str], y: float) -> float:\n    return ctx.deps + y\n\nagent = Agent('test', toolsets=[toolset], deps_type=int)\nresult = agent.run_sync('foobar', deps=1)\nprint(result.output)\n#> {\"foobar\":1,\"spam\":1.0}\n```\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `func` | `ToolFuncEither[AgentDepsT, ToolParams] | None` | The tool function to register. | `None` |\n| `name` | `str | None` | The name of the tool, defaults to the function name. | `None` |\n| `description` | `str | None` | The description of the tool,defaults to the function docstring. | `None` |\n| `retries` | `int | None` | The number of retries to allow for this tool, defaults to the agent's default retries, which defaults to 1. | `None` |\n| `prepare` | `ToolPrepareFunc[AgentDepsT] | None` | custom method to prepare the tool definition for each step, return `None` to omit this tool from a given step. This is useful if you want to customise a tool at call time, or omit it completely from a step. See [`ToolPrepareFunc`](../tools/index.html#pydantic_ai.tools.ToolPrepareFunc). | `None` |\n| `docstring_format` | `DocstringFormat | None` | The format of the docstring, see [`DocstringFormat`](../tools/index.html#pydantic_ai.tools.DocstringFormat). If `None`, the default value is determined by the toolset. | `None` |\n| `require_parameter_descriptions` | `bool | None` | If True, raise an error if a parameter description is missing. If `None`, the default value is determined by the toolset. | `None` |\n| `schema_generator` | `type[GenerateJsonSchema] | None` | The JSON schema generator class to use for this tool. If `None`, the default value is determined by the toolset. | `None` |\n| `strict` | `bool | None` | Whether to enforce JSON schema compliance (only affects OpenAI). See [`ToolDefinition`](../tools/index.html#pydantic_ai.tools.ToolDefinition) for more info. If `None`, the default value is determined by the toolset. | `None` |\n| `sequential` | `bool | None` | Whether the function requires a sequential/serial execution environment. Defaults to False. If `None`, the default value is determined by the toolset. | `None` |\n| `requires_approval` | `bool | None` | Whether this tool requires human-in-the-loop approval. Defaults to False. See the [tools documentation](https://ai.pydantic.dev/deferred-tools/#human-in-the-loop-tool-approval) for more info. If `None`, the default value is determined by the toolset. | `None` |\n| `metadata` | `dict[str, Any] | None` | Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization. If `None`, the default value is determined by the toolset. If provided, it will be merged with the toolset's metadata. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/toolsets/function.py`", "url": "https://ai.pydantic.dev/toolsets/index.html#functiontoolset", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "FunctionToolset", "anchor": "functiontoolset", "md_text": "|  |  |\n| --- | --- |\n| ``` 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 ``` | ``` def tool(     self,     func: ToolFuncEither[AgentDepsT, ToolParams] | None = None,     /,     *,     name: str | None = None,     description: str | None = None,     retries: int | None = None,     prepare: ToolPrepareFunc[AgentDepsT] | None = None,     docstring_format: DocstringFormat | None = None,     require_parameter_descriptions: bool | None = None,     schema_generator: type[GenerateJsonSchema] | None = None,     strict: bool | None = None,     sequential: bool | None = None,     requires_approval: bool | None = None,     metadata: dict[str, Any] | None = None, ) -> Any:     \"\"\"Decorator to register a tool function which takes [`RunContext`][pydantic_ai.tools.RunContext] as its first argument.      Can decorate a sync or async functions.      The docstring is inspected to extract both the tool description and description of each parameter,     [learn more](../tools.md#function-tools-and-schema).      We can't add overloads for every possible signature of tool, since the return type is a recursive union     so the signature of functions decorated with `@toolset.tool` is obscured.      Example:     ```python     from pydantic_ai import Agent, FunctionToolset, RunContext      toolset = FunctionToolset()      @toolset.tool     def foobar(ctx: RunContext[int], x: int) -> int:         return ctx.deps + x      @toolset.tool(retries=2)     async def spam(ctx: RunContext[str], y: float) -> float:         return ctx.deps + y      agent = Agent('test', toolsets=[toolset], deps_type=int)     result = agent.run_sync('foobar', deps=1)     print(result.output)     #> {\"foobar\":1,\"spam\":1.0}     ```      Args:         func: The tool function to register.         name: The name of the tool, defaults to the function name.         description: The description of the tool,defaults to the function docstring.         retries: The number of retries to allow for this tool, defaults to the agent's default retries,             which defaults to 1.         prepare: custom method to prepare the tool definition for each step, return `None` to omit this             tool from a given step. This is useful if you want to customise a tool at call time,             or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].         docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].             If `None`, the default value is determined by the toolset.         require_parameter_descriptions: If True, raise an error if a parameter description is missing.             If `None`, the default value is determined by the toolset.         schema_generator: The JSON schema generator class to use for this tool.             If `None`, the default value is determined by the toolset.         strict: Whether to enforce JSON schema compliance (only affects OpenAI).             See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.             If `None`, the default value is determined by the toolset.         sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.             If `None`, the default value is determined by the toolset.         requires_approval: Whether this tool requires human-in-the-loop approval. Defaults to False.             See the [tools documentation](../deferred-tools.md#human-in-the-loop-tool-approval) for more info.             If `None`, the default value is determined by the toolset.         metadata: Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization.             If `None`, the default value is determined by the toolset. If provided, it will be merged with the toolset's metadata.     \"\"\"      def tool_decorator(         func_: ToolFuncEither[AgentDepsT, ToolParams],     ) -> ToolFuncEither[AgentDepsT, ToolParams]:         # noinspection PyTypeChecker         self.add_function(             func=func_,             takes_ctx=None,             name=name,             description=description,             retries=retries,             prepare=prepare,             docstring_format=docstring_format,             require_parameter_descriptions=require_parameter_descriptions,             schema_generator=schema_generator,             strict=strict,             sequential=sequential,             requires_approval=requires_approval,             metadata=metadata,         )         return func_      return tool_decorator if func is None else tool_decorator(func) ``` |", "url": "https://ai.pydantic.dev/toolsets/index.html#functiontoolset", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "FunctionToolset", "anchor": "functiontoolset", "md_text": "#### add\\_function\n\n```\nadd_function(\n    func: ToolFuncEither[AgentDepsT, ToolParams],\n    takes_ctx: bool | None = None,\n    name: str | None = None,\n    description: str | None = None,\n    retries: int | None = None,\n    prepare: ToolPrepareFunc[AgentDepsT] | None = None,\n    docstring_format: DocstringFormat | None = None,\n    require_parameter_descriptions: bool | None = None,\n    schema_generator: (\n        type[GenerateJsonSchema] | None\n    ) = None,\n    strict: bool | None = None,\n    sequential: bool | None = None,\n    requires_approval: bool | None = None,\n    metadata: dict[str, Any] | None = None,\n) -> None\n```\n\nAdd a function as a tool to the toolset.\n\nCan take a sync or async function.\n\nThe docstring is inspected to extract both the tool description and description of each parameter,\n[learn more](https://ai.pydantic.dev/tools/#function-tools-and-schema).\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `func` | `ToolFuncEither[AgentDepsT, ToolParams]` | The tool function to register. | *required* |\n| `takes_ctx` | `bool | None` | Whether the function takes a [`RunContext`](../tools/index.html#pydantic_ai.tools.RunContext) as its first argument. If `None`, this is inferred from the function signature. | `None` |\n| `name` | `str | None` | The name of the tool, defaults to the function name. | `None` |\n| `description` | `str | None` | The description of the tool, defaults to the function docstring. | `None` |\n| `retries` | `int | None` | The number of retries to allow for this tool, defaults to the agent's default retries, which defaults to 1. | `None` |\n| `prepare` | `ToolPrepareFunc[AgentDepsT] | None` | custom method to prepare the tool definition for each step, return `None` to omit this tool from a given step. This is useful if you want to customise a tool at call time, or omit it completely from a step. See [`ToolPrepareFunc`](../tools/index.html#pydantic_ai.tools.ToolPrepareFunc). | `None` |\n| `docstring_format` | `DocstringFormat | None` | The format of the docstring, see [`DocstringFormat`](../tools/index.html#pydantic_ai.tools.DocstringFormat). If `None`, the default value is determined by the toolset. | `None` |\n| `require_parameter_descriptions` | `bool | None` | If True, raise an error if a parameter description is missing. If `None`, the default value is determined by the toolset. | `None` |\n| `schema_generator` | `type[GenerateJsonSchema] | None` | The JSON schema generator class to use for this tool. If `None`, the default value is determined by the toolset. | `None` |\n| `strict` | `bool | None` | Whether to enforce JSON schema compliance (only affects OpenAI). See [`ToolDefinition`](../tools/index.html#pydantic_ai.tools.ToolDefinition) for more info. If `None`, the default value is determined by the toolset. | `None` |\n| `sequential` | `bool | None` | Whether the function requires a sequential/serial execution environment. Defaults to False. If `None`, the default value is determined by the toolset. | `None` |\n| `requires_approval` | `bool | None` | Whether this tool requires human-in-the-loop approval. Defaults to False. See the [tools documentation](https://ai.pydantic.dev/deferred-tools/#human-in-the-loop-tool-approval) for more info. If `None`, the default value is determined by the toolset. | `None` |\n| `metadata` | `dict[str, Any] | None` | Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization. If `None`, the default value is determined by the toolset. If provided, it will be merged with the toolset's metadata. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/toolsets/function.py`", "url": "https://ai.pydantic.dev/toolsets/index.html#functiontoolset", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "FunctionToolset", "anchor": "functiontoolset", "md_text": "|  |  |\n| --- | --- |\n| ``` 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 ``` | ``` def add_function(     self,     func: ToolFuncEither[AgentDepsT, ToolParams],     takes_ctx: bool | None = None,     name: str | None = None,     description: str | None = None,     retries: int | None = None,     prepare: ToolPrepareFunc[AgentDepsT] | None = None,     docstring_format: DocstringFormat | None = None,     require_parameter_descriptions: bool | None = None,     schema_generator: type[GenerateJsonSchema] | None = None,     strict: bool | None = None,     sequential: bool | None = None,     requires_approval: bool | None = None,     metadata: dict[str, Any] | None = None, ) -> None:     \"\"\"Add a function as a tool to the toolset.      Can take a sync or async function.      The docstring is inspected to extract both the tool description and description of each parameter,     [learn more](../tools.md#function-tools-and-schema).      Args:         func: The tool function to register.         takes_ctx: Whether the function takes a [`RunContext`][pydantic_ai.tools.RunContext] as its first argument. If `None`, this is inferred from the function signature.         name: The name of the tool, defaults to the function name.         description: The description of the tool, defaults to the function docstring.         retries: The number of retries to allow for this tool, defaults to the agent's default retries,             which defaults to 1.         prepare: custom method to prepare the tool definition for each step, return `None` to omit this             tool from a given step. This is useful if you want to customise a tool at call time,             or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].         docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].             If `None`, the default value is determined by the toolset.         require_parameter_descriptions: If True, raise an error if a parameter description is missing.             If `None`, the default value is determined by the toolset.         schema_generator: The JSON schema generator class to use for this tool.             If `None`, the default value is determined by the toolset.         strict: Whether to enforce JSON schema compliance (only affects OpenAI).             See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.             If `None`, the default value is determined by the toolset.         sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.             If `None`, the default value is determined by the toolset.         requires_approval: Whether this tool requires human-in-the-loop approval. Defaults to False.             See the [tools documentation](../deferred-tools.md#human-in-the-loop-tool-approval) for more info.             If `None`, the default value is determined by the toolset.         metadata: Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization.             If `None`, the default value is determined by the toolset. If provided, it will be merged with the toolset's metadata.     \"\"\"     if docstring_format is None:         docstring_format = self.docstring_format     if require_parameter_descriptions is None:         require_parameter_descriptions = self.require_parameter_descriptions     if schema_generator is None:         schema_generator = self.schema_generator     if strict is None:         strict = self.strict     if sequential is None:         sequential = self.sequential     if requires_approval is None:         requires_approval = self.requires_approval      tool = Tool[AgentDepsT](         func,         takes_ctx=takes_ctx,         name=name,         description=description,         max_retries=retries,         prepare=prepare,         docstring_format=docstring_format,         require_parameter_descriptions=require_parameter_descriptions,         schema_generator=schema_generator,         strict=strict,         sequential=sequential,         requires_approval=requires_approval,         metadata=metadata,     )     self.add_tool(tool) ``` |\n\n#### add\\_tool\n\n```\nadd_tool(tool: Tool[AgentDepsT]) -> None\n```\n\nAdd a tool to the toolset.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `tool` | `Tool[AgentDepsT]` | The tool to add. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/toolsets/function.py`", "url": "https://ai.pydantic.dev/toolsets/index.html#functiontoolset", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "FunctionToolset", "anchor": "functiontoolset", "md_text": "|  |  |\n| --- | --- |\n| ``` 301 302 303 304 305 306 307 308 309 310 311 312 313 ``` | ``` def add_tool(self, tool: Tool[AgentDepsT]) -> None:     \"\"\"Add a tool to the toolset.      Args:         tool: The tool to add.     \"\"\"     if tool.name in self.tools:         raise UserError(f'Tool name conflicts with existing tool: {tool.name!r}')     if tool.max_retries is None:         tool.max_retries = self.max_retries     if self.metadata is not None:         tool.metadata = self.metadata | (tool.metadata or {})     self.tools[tool.name] = tool ``` |", "url": "https://ai.pydantic.dev/toolsets/index.html#functiontoolset", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "PrefixedToolset `dataclass`", "anchor": "prefixedtoolset-dataclass", "md_text": "Bases: `WrapperToolset[AgentDepsT]`\n\nA toolset that prefixes the names of the tools it contains.\n\nSee [toolset docs](https://ai.pydantic.dev/toolsets/#prefixing-tool-names) for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/toolsets/prefixed.py`\n\n|  |  |\n| --- | --- |\n| ``` 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 ``` | ``` @dataclass class PrefixedToolset(WrapperToolset[AgentDepsT]):     \"\"\"A toolset that prefixes the names of the tools it contains.      See [toolset docs](../toolsets.md#prefixing-tool-names) for more information.     \"\"\"      prefix: str      @property     def tool_name_conflict_hint(self) -> str:         return 'Change the `prefix` attribute to avoid name conflicts.'      async def get_tools(self, ctx: RunContext[AgentDepsT]) -> dict[str, ToolsetTool[AgentDepsT]]:         return {             new_name: replace(                 tool,                 toolset=self,                 tool_def=replace(tool.tool_def, name=new_name),             )             for name, tool in (await super().get_tools(ctx)).items()             if (new_name := f'{self.prefix}_{name}')         }      async def call_tool(         self, name: str, tool_args: dict[str, Any], ctx: RunContext[AgentDepsT], tool: ToolsetTool[AgentDepsT]     ) -> Any:         original_name = name.removeprefix(self.prefix + '_')         ctx = replace(ctx, tool_name=original_name)         tool = replace(tool, tool_def=replace(tool.tool_def, name=original_name))         return await super().call_tool(original_name, tool_args, ctx, tool) ``` |", "url": "https://ai.pydantic.dev/toolsets/index.html#prefixedtoolset-dataclass", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "RenamedToolset `dataclass`", "anchor": "renamedtoolset-dataclass", "md_text": "Bases: `WrapperToolset[AgentDepsT]`\n\nA toolset that renames the tools it contains using a dictionary mapping new names to original names.\n\nSee [toolset docs](https://ai.pydantic.dev/toolsets/#renaming-tools) for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/toolsets/renamed.py`\n\n|  |  |\n| --- | --- |\n| ``` 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 ``` | ``` @dataclass class RenamedToolset(WrapperToolset[AgentDepsT]):     \"\"\"A toolset that renames the tools it contains using a dictionary mapping new names to original names.      See [toolset docs](../toolsets.md#renaming-tools) for more information.     \"\"\"      name_map: dict[str, str]      async def get_tools(self, ctx: RunContext[AgentDepsT]) -> dict[str, ToolsetTool[AgentDepsT]]:         original_to_new_name_map = {v: k for k, v in self.name_map.items()}         original_tools = await super().get_tools(ctx)         tools: dict[str, ToolsetTool[AgentDepsT]] = {}         for original_name, tool in original_tools.items():             new_name = original_to_new_name_map.get(original_name, None)             if new_name:                 tools[new_name] = replace(                     tool,                     toolset=self,                     tool_def=replace(tool.tool_def, name=new_name),                 )             else:                 tools[original_name] = tool         return tools      async def call_tool(         self, name: str, tool_args: dict[str, Any], ctx: RunContext[AgentDepsT], tool: ToolsetTool[AgentDepsT]     ) -> Any:         original_name = self.name_map.get(name, name)         ctx = replace(ctx, tool_name=original_name)         tool = replace(tool, tool_def=replace(tool.tool_def, name=original_name))         return await super().call_tool(original_name, tool_args, ctx, tool) ``` |", "url": "https://ai.pydantic.dev/toolsets/index.html#renamedtoolset-dataclass", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "PreparedToolset `dataclass`", "anchor": "preparedtoolset-dataclass", "md_text": "Bases: `WrapperToolset[AgentDepsT]`\n\nA toolset that prepares the tools it contains using a prepare function that takes the agent context and the original tool definitions.\n\nSee [toolset docs](https://ai.pydantic.dev/toolsets/#preparing-tool-definitions) for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/toolsets/prepared.py`\n\n|  |  |\n| --- | --- |\n| ``` 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 ``` | ``` @dataclass class PreparedToolset(WrapperToolset[AgentDepsT]):     \"\"\"A toolset that prepares the tools it contains using a prepare function that takes the agent context and the original tool definitions.      See [toolset docs](../toolsets.md#preparing-tool-definitions) for more information.     \"\"\"      prepare_func: ToolsPrepareFunc[AgentDepsT]      async def get_tools(self, ctx: RunContext[AgentDepsT]) -> dict[str, ToolsetTool[AgentDepsT]]:         original_tools = await super().get_tools(ctx)         original_tool_defs = [tool.tool_def for tool in original_tools.values()]         prepared_tool_defs_by_name = {             tool_def.name: tool_def for tool_def in (await self.prepare_func(ctx, original_tool_defs) or [])         }          if len(prepared_tool_defs_by_name.keys() - original_tools.keys()) > 0:             raise UserError(                 'Prepare function cannot add or rename tools. Use `FunctionToolset.add_function()` or `RenamedToolset` instead.'             )          return {             name: replace(original_tools[name], tool_def=tool_def)             for name, tool_def in prepared_tool_defs_by_name.items()         } ``` |", "url": "https://ai.pydantic.dev/toolsets/index.html#preparedtoolset-dataclass", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "WrapperToolset `dataclass`", "anchor": "wrappertoolset-dataclass", "md_text": "Bases: `AbstractToolset[AgentDepsT]`\n\nA toolset that wraps another toolset and delegates to it.\n\nSee [toolset docs](https://ai.pydantic.dev/toolsets/#wrapping-a-toolset) for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/toolsets/wrapper.py`\n\n|  |  |\n| --- | --- |\n| ``` 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 ``` | ``` @dataclass class WrapperToolset(AbstractToolset[AgentDepsT]):     \"\"\"A toolset that wraps another toolset and delegates to it.      See [toolset docs](../toolsets.md#wrapping-a-toolset) for more information.     \"\"\"      wrapped: AbstractToolset[AgentDepsT]      @property     def id(self) -> str | None:         return None  # pragma: no cover      @property     def label(self) -> str:         return f'{self.__class__.__name__}({self.wrapped.label})'      async def __aenter__(self) -> Self:         await self.wrapped.__aenter__()         return self      async def __aexit__(self, *args: Any) -> bool | None:         return await self.wrapped.__aexit__(*args)      async def get_tools(self, ctx: RunContext[AgentDepsT]) -> dict[str, ToolsetTool[AgentDepsT]]:         return await self.wrapped.get_tools(ctx)      async def call_tool(         self, name: str, tool_args: dict[str, Any], ctx: RunContext[AgentDepsT], tool: ToolsetTool[AgentDepsT]     ) -> Any:         return await self.wrapped.call_tool(name, tool_args, ctx, tool)      def apply(self, visitor: Callable[[AbstractToolset[AgentDepsT]], None]) -> None:         self.wrapped.apply(visitor)      def visit_and_replace(         self, visitor: Callable[[AbstractToolset[AgentDepsT]], AbstractToolset[AgentDepsT]]     ) -> AbstractToolset[AgentDepsT]:         return replace(self, wrapped=self.wrapped.visit_and_replace(visitor)) ``` |", "url": "https://ai.pydantic.dev/toolsets/index.html#wrappertoolset-dataclass", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "ToolsetFunc `module-attribute`", "anchor": "toolsetfunc-module-attribute", "md_text": "```\nToolsetFunc: TypeAlias = Callable[\n    [RunContext[AgentDepsT]],\n    AbstractToolset[AgentDepsT]\n    | None\n    | Awaitable[AbstractToolset[AgentDepsT] | None],\n]\n```\n\nA sync/async function which takes a run context and returns a toolset.", "url": "https://ai.pydantic.dev/toolsets/index.html#toolsetfunc-module-attribute", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "FastMCPToolset `dataclass`", "anchor": "fastmcptoolset-dataclass", "md_text": "Bases: `AbstractToolset[AgentDepsT]`\n\nA FastMCP Toolset that uses the FastMCP Client to call tools from a local or remote MCP Server.\n\nThe Toolset can accept a FastMCP Client, a FastMCP Transport, or any other object which a FastMCP Transport can be created from.\n\nSee https://gofastmcp.com/clients/transports for a full list of transports available.\n\nSource code in `pydantic_ai_slim/pydantic_ai/toolsets/fastmcp.py`\n\n|  |  |\n| --- | --- |\n| ```  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 ``` | ``` @dataclass(init=False) class FastMCPToolset(AbstractToolset[AgentDepsT]):     \"\"\"A FastMCP Toolset that uses the FastMCP Client to call tools from a local or remote MCP Server.      The Toolset can accept a FastMCP Client, a FastMCP Transport, or any other object which a FastMCP Transport can be created from.      See https://gofastmcp.com/clients/transports for a full list of transports available.     \"\"\"      client: Client[Any]     \"\"\"The FastMCP client to use.\"\"\"      _: KW_ONLY      tool_error_behavior: Literal['model_retry', 'error']     \"\"\"The behavior to take when a tool error occurs.\"\"\"      max_retries: int     \"\"\"The maximum number of retries to attempt if a tool call fails.\"\"\"      _id: str | None      def __init__(         self,         client: Client[Any]         | ClientTransport         | FastMCP         | FastMCP1Server         | AnyUrl         | Path         | MCPConfig         | dict[str, Any]         | str,         *,         max_retries: int = 1,         tool_error_behavior: Literal['model_retry', 'error'] = 'model_retry',         id: str | None = None,     ) -> None:         if isinstance(client, Client):             self.client = client         else:             self.client = Client[Any](transport=client)          self._id = id         self.max_retries = max_retries         self.tool_error_behavior = tool_error_behavior          self._enter_lock: Lock = Lock()         self._running_count: int = 0         self._exit_stack: AsyncExitStack | None = None      @property     def id(self) -> str | None:         return self._id      async def __aenter__(self) -> Self:         async with self._enter_lock:             if self._running_count == 0:                 self._exit_stack = AsyncExitStack()                 await self._exit_stack.enter_async_context(self.client)              self._running_count += 1          return self      async def __aexit__(self, *args: Any) -> bool | None:         async with self._enter_lock:             self._running_count -= 1             if self._running_count == 0 and self._exit_stack:                 await self._exit_stack.aclose()                 self._exit_stack = None          return None      async def get_tools(self, ctx: RunContext[AgentDepsT]) -> dict[str, ToolsetTool[AgentDepsT]]:         async with self:             mcp_tools: list[MCPTool] = await self.client.list_tools()              return {                 tool.name: _convert_mcp_tool_to_toolset_tool(toolset=self, mcp_tool=tool, retries=self.max_retries)                 for tool in mcp_tools             }      async def call_tool(         self, name: str, tool_args: dict[str, Any], ctx: RunContext[AgentDepsT], tool: ToolsetTool[AgentDepsT]     ) -> Any:         async with self:             try:                 call_tool_result: CallToolResult = await self.client.call_tool(name=name, arguments=tool_args)             except ToolError as e:                 if self.tool_error_behavior == 'model_retry':                     raise ModelRetry(message=str(e)) from e                 else:                     raise e          # If we have structured content, return that         if call_tool_result.structured_content:             return call_tool_result.structured_content          # Otherwise, return the content         return _map_fastmcp_tool_results(parts=call_tool_result.content) ``` |\n\n#### client `instance-attribute`\n\n```\nclient: Client[Any]\n```\n\nThe FastMCP client to use.\n\n#### max\\_retries `instance-attribute`\n\n```\nmax_retries: int = max_retries\n```\n\nThe maximum number of retries to attempt if a tool call fails.", "url": "https://ai.pydantic.dev/toolsets/index.html#fastmcptoolset-dataclass", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "FastMCPToolset `dataclass`", "anchor": "fastmcptoolset-dataclass", "md_text": "#### tool\\_error\\_behavior `instance-attribute`\n\n```\ntool_error_behavior: Literal[\"model_retry\", \"error\"] = (\n    tool_error_behavior\n)\n```\n\nThe behavior to take when a tool error occurs.", "url": "https://ai.pydantic.dev/toolsets/index.html#fastmcptoolset-dataclass", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "md_text": "To use `HuggingFaceModel`, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `huggingface` optional group:\n\npipuv\n\n```\npip install \"pydantic-ai-slim[huggingface]\"\n\nuv add \"pydantic-ai-slim[huggingface]\"\n```", "url": "https://ai.pydantic.dev/huggingface/index.html#install", "page": "huggingface/index.html", "source_site": "pydantic_ai"}
{"title": "Configuration", "anchor": "configuration", "md_text": "To use [Hugging Face](https://huggingface.co/) inference, you'll need to set up an account which will give you [free tier](https://huggingface.co/docs/inference-providers/pricing) allowance on [Inference Providers](https://huggingface.co/docs/inference-providers). To setup inference, follow these steps:\n\n1. Go to [Hugging Face](https://huggingface.co/join) and sign up for an account.\n2. Create a new access token in [Hugging Face](https://huggingface.co/settings/tokens).\n3. Set the `HF_TOKEN` environment variable to the token you just created.\n\nOnce you have a Hugging Face access token, you can set it as an environment variable:\n\n```\nexport HF_TOKEN='hf_token'\n```", "url": "https://ai.pydantic.dev/huggingface/index.html#configuration", "page": "huggingface/index.html", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "md_text": "You can then use [`HuggingFaceModel`](../models/huggingface/index.html#pydantic_ai.models.huggingface.HuggingFaceModel) by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('huggingface:Qwen/Qwen3-235B-A22B')\n...\n```\n\nOr initialise the model directly with just the model name:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.huggingface import HuggingFaceModel\n\nmodel = HuggingFaceModel('Qwen/Qwen3-235B-A22B')\nagent = Agent(model)\n...\n```\n\nBy default, the [`HuggingFaceModel`](../models/huggingface/index.html#pydantic_ai.models.huggingface.HuggingFaceModel) uses the\n[`HuggingFaceProvider`](../providers/index.html#pydantic_ai.providers.huggingface.HuggingFaceProvider) that will select automatically\nthe first of the inference providers (Cerebras, Together AI, Cohere..etc) available for the model, sorted by your\npreferred order in https://hf.co/settings/inference-providers.", "url": "https://ai.pydantic.dev/huggingface/index.html#usage", "page": "huggingface/index.html", "source_site": "pydantic_ai"}
{"title": "Configure the provider", "anchor": "configure-the-provider", "md_text": "If you want to pass parameters in code to the provider, you can programmatically instantiate the\n[`HuggingFaceProvider`](../providers/index.html#pydantic_ai.providers.huggingface.HuggingFaceProvider) and pass it to the model:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.huggingface import HuggingFaceModel\nfrom pydantic_ai.providers.huggingface import HuggingFaceProvider\n\nmodel = HuggingFaceModel('Qwen/Qwen3-235B-A22B', provider=HuggingFaceProvider(api_key='hf_token', provider_name='nebius'))\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/huggingface/index.html#configure-the-provider", "page": "huggingface/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Hugging Face client", "anchor": "custom-hugging-face-client", "md_text": "[`HuggingFaceProvider`](../providers/index.html#pydantic_ai.providers.huggingface.HuggingFaceProvider) also accepts a custom\n[`AsyncInferenceClient`](https://huggingface.co/docs/huggingface_hub/v0.29.3/en/package_reference/inference_client#huggingface_hub.AsyncInferenceClient) client via the `hf_client` parameter, so you can customise\nthe `headers`, `bill_to` (billing to an HF organization you're a member of), `base_url` etc. as defined in the\n[Hugging Face Hub python library docs](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client).\n\n```\nfrom huggingface_hub import AsyncInferenceClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.huggingface import HuggingFaceModel\nfrom pydantic_ai.providers.huggingface import HuggingFaceProvider\n\nclient = AsyncInferenceClient(\n    bill_to='openai',\n    api_key='hf_token',\n    provider='fireworks-ai',\n)\n\nmodel = HuggingFaceModel(\n    'Qwen/Qwen3-235B-A22B',\n    provider=HuggingFaceProvider(hf_client=client),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/huggingface/index.html#custom-hugging-face-client", "page": "huggingface/index.html", "source_site": "pydantic_ai"}
{"title": "SSE\\_CONTENT\\_TYPE `module-attribute`", "anchor": "ssecontenttype-module-attribute", "md_text": "```\nSSE_CONTENT_TYPE: Final[str] = 'text/event-stream'\n```\n\nContent type header value for Server-Sent Events (SSE).", "url": "https://ai.pydantic.dev/ag_ui/index.html#ssecontenttype-module-attribute", "page": "ag_ui/index.html", "source_site": "pydantic_ai"}
{"title": "OnCompleteFunc `module-attribute`", "anchor": "oncompletefunc-module-attribute", "md_text": "```\nOnCompleteFunc: TypeAlias = (\n    Callable[[AgentRunResult[Any]], None]\n    | Callable[[AgentRunResult[Any]], Awaitable[None]]\n)\n```\n\nCallback function type that receives the `AgentRunResult` of the completed run. Can be sync or async.", "url": "https://ai.pydantic.dev/ag_ui/index.html#oncompletefunc-module-attribute", "page": "ag_ui/index.html", "source_site": "pydantic_ai"}
{"title": "AGUIApp", "anchor": "aguiapp", "md_text": "Bases: `Generic[AgentDepsT, OutputDataT]`, `Starlette`\n\nASGI application for running Pydantic AI agents with AG-UI protocol support.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ag_ui.py`", "url": "https://ai.pydantic.dev/ag_ui/index.html#aguiapp", "page": "ag_ui/index.html", "source_site": "pydantic_ai"}
{"title": "AGUIApp", "anchor": "aguiapp", "md_text": "|  |  |\n| --- | --- |\n| ``` 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 ``` | ``` class AGUIApp(Generic[AgentDepsT, OutputDataT], Starlette):     \"\"\"ASGI application for running Pydantic AI agents with AG-UI protocol support.\"\"\"      def __init__(         self,         agent: AbstractAgent[AgentDepsT, OutputDataT],         *,         # Agent.iter parameters.         output_type: OutputSpec[Any] | None = None,         model: Model | KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: UsageLimits | None = None,         usage: RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         # Starlette parameters.         debug: bool = False,         routes: Sequence[BaseRoute] | None = None,         middleware: Sequence[Middleware] | None = None,         exception_handlers: Mapping[Any, ExceptionHandler] | None = None,         on_startup: Sequence[Callable[[], Any]] | None = None,         on_shutdown: Sequence[Callable[[], Any]] | None = None,         lifespan: Lifespan[AGUIApp[AgentDepsT, OutputDataT]] | None = None,     ) -> None:         \"\"\"An ASGI application that handles every AG-UI request by running the agent.          Note that the `deps` will be the same for each request, with the exception of the AG-UI state that's         injected into the `state` field of a `deps` object that implements the [`StateHandler`][pydantic_ai.ag_ui.StateHandler] protocol.         To provide different `deps` for each request (e.g. based on the authenticated user),         use [`pydantic_ai.ag_ui.run_ag_ui`][pydantic_ai.ag_ui.run_ag_ui] or         [`pydantic_ai.ag_ui.handle_ag_ui_request`][pydantic_ai.ag_ui.handle_ag_ui_request] instead.          Args:             agent: The agent to run.              output_type: Custom output type to use for this run, `output_type` may only be used if the agent has                 no output validators since output validators would expect an argument that matches the agent's                 output type.             model: Optional model to use for this run, required if `model` was not set when creating the agent.             deps: Optional dependencies to use for this run.             model_settings: Optional settings to use for this model's request.             usage_limits: Optional limits on model request count or token usage.             usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.             infer_name: Whether to try to infer the agent name from the call frame if it's not set.             toolsets: Optional additional toolsets for this run.              debug: Boolean indicating if debug tracebacks should be returned on errors.             routes: A list of routes to serve incoming HTTP and WebSocket requests.             middleware: A list of middleware to run for every request. A starlette application will always                 automatically include two middleware classes. `ServerErrorMiddleware` is added as the very                 outermost middleware, to handle any uncaught errors occurring anywhere in the entire stack.                 `ExceptionMiddleware` is added as the very innermost middleware, to deal with handled                 exception cases occurring in the routing or endpoints.             exception_handlers: A mapping of either integer status codes, or exception class types onto                 callables which handle the exceptions. Exception handler callables should be of the form                 `handler(request, exc) -> response` and may be either standard functions, or async functions.             on_startup: A list of callables to run on application startup. Startup handler callables do not                 take any arguments, and may be either standard functions, or async functions.             on_shutdown: A list of callables to run on application shutdown. Shutdown handler callables do                 not take any arguments, and may be either standard functions, or async functions.             lifespan: A lifespan context function, which can be used to perform startup and shutdown tasks.                 This is a newer style that replaces the `on_startup` and `on_shutdown` handlers. Use one or                 the other, not both.         \"\"\"         super().__init__(             debug=debug,             routes=routes,             middleware=middleware,             exception_handlers=exception_handlers,             on_startup=on_startup,             on_shutdown=on_shutdown,             lifespan=lifespan,         )          async def endpoint(request: Request) -> Response:             \"\"\"Endpoint to run the agent with the provided input data.\"\"\"             return await handle_ag_ui_request(                 agent,                 request,                 output_type=output_type,                 model=model,                 deps=deps,                 model_settings=model_settings,                 usage_limits=usage_limits,                 usage=usage,                 infer_name=infer_name,                 toolsets=toolsets,             )          self.router.add_route('/', endpoint, methods=['POST'], name='run_agent') ``` |", "url": "https://ai.pydantic.dev/ag_ui/index.html#aguiapp", "page": "ag_ui/index.html", "source_site": "pydantic_ai"}
{"title": "AGUIApp", "anchor": "aguiapp", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    agent: AbstractAgent[AgentDepsT, OutputDataT],\n    *,\n    output_type: OutputSpec[Any] | None = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    debug: bool = False,\n    routes: Sequence[BaseRoute] | None = None,\n    middleware: Sequence[Middleware] | None = None,\n    exception_handlers: (\n        Mapping[Any, ExceptionHandler] | None\n    ) = None,\n    on_startup: Sequence[Callable[[], Any]] | None = None,\n    on_shutdown: Sequence[Callable[[], Any]] | None = None,\n    lifespan: (\n        Lifespan[AGUIApp[AgentDepsT, OutputDataT]] | None\n    ) = None\n) -> None\n```\n\nAn ASGI application that handles every AG-UI request by running the agent.\n\nNote that the `deps` will be the same for each request, with the exception of the AG-UI state that's\ninjected into the `state` field of a `deps` object that implements the [`StateHandler`](index.html#pydantic_ai.ag_ui.StateHandler) protocol.\nTo provide different `deps` for each request (e.g. based on the authenticated user),\nuse [`pydantic_ai.ag_ui.run_ag_ui`](index.html#pydantic_ai.ag_ui.run_ag_ui) or\n[`pydantic_ai.ag_ui.handle_ag_ui_request`](index.html#pydantic_ai.ag_ui.handle_ag_ui_request) instead.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `agent` | `AbstractAgent[AgentDepsT, OutputDataT]` | The agent to run. | *required* |\n| `output_type` | `OutputSpec[Any] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `debug` | `bool` | Boolean indicating if debug tracebacks should be returned on errors. | `False` |\n| `routes` | `Sequence[BaseRoute] | None` | A list of routes to serve incoming HTTP and WebSocket requests. | `None` |\n| `middleware` | `Sequence[Middleware] | None` | A list of middleware to run for every request. A starlette application will always automatically include two middleware classes. `ServerErrorMiddleware` is added as the very outermost middleware, to handle any uncaught errors occurring anywhere in the entire stack. `ExceptionMiddleware` is added as the very innermost middleware, to deal with handled exception cases occurring in the routing or endpoints. | `None` |\n| `exception_handlers` | `Mapping[Any, ExceptionHandler] | None` | A mapping of either integer status codes, or exception class types onto callables which handle the exceptions. Exception handler callables should be of the form `handler(request, exc) -> response` and may be either standard functions, or async functions. | `None` |\n| `on_startup` | `Sequence[Callable[[], Any]] | None` | A list of callables to run on application startup. Startup handler callables do not take any arguments, and may be either standard functions, or async functions. | `None` |\n| `on_shutdown` | `Sequence[Callable[[], Any]] | None` | A list of callables to run on application shutdown. Shutdown handler callables do not take any arguments, and may be either standard functions, or async functions. | `None` |\n| `lifespan` | `Lifespan[AGUIApp[AgentDepsT, OutputDataT]] | None` | A lifespan context function, which can be used to perform startup and shutdown tasks. This is a newer style that replaces the `on_startup` and `on_shutdown` handlers. Use one or the other, not both. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ag_ui.py`", "url": "https://ai.pydantic.dev/ag_ui/index.html#aguiapp", "page": "ag_ui/index.html", "source_site": "pydantic_ai"}
{"title": "AGUIApp", "anchor": "aguiapp", "md_text": "|  |  |\n| --- | --- |\n| ``` 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 ``` | ``` def __init__(     self,     agent: AbstractAgent[AgentDepsT, OutputDataT],     *,     # Agent.iter parameters.     output_type: OutputSpec[Any] | None = None,     model: Model | KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: UsageLimits | None = None,     usage: RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     # Starlette parameters.     debug: bool = False,     routes: Sequence[BaseRoute] | None = None,     middleware: Sequence[Middleware] | None = None,     exception_handlers: Mapping[Any, ExceptionHandler] | None = None,     on_startup: Sequence[Callable[[], Any]] | None = None,     on_shutdown: Sequence[Callable[[], Any]] | None = None,     lifespan: Lifespan[AGUIApp[AgentDepsT, OutputDataT]] | None = None, ) -> None:     \"\"\"An ASGI application that handles every AG-UI request by running the agent.      Note that the `deps` will be the same for each request, with the exception of the AG-UI state that's     injected into the `state` field of a `deps` object that implements the [`StateHandler`][pydantic_ai.ag_ui.StateHandler] protocol.     To provide different `deps` for each request (e.g. based on the authenticated user),     use [`pydantic_ai.ag_ui.run_ag_ui`][pydantic_ai.ag_ui.run_ag_ui] or     [`pydantic_ai.ag_ui.handle_ag_ui_request`][pydantic_ai.ag_ui.handle_ag_ui_request] instead.      Args:         agent: The agent to run.          output_type: Custom output type to use for this run, `output_type` may only be used if the agent has             no output validators since output validators would expect an argument that matches the agent's             output type.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.          debug: Boolean indicating if debug tracebacks should be returned on errors.         routes: A list of routes to serve incoming HTTP and WebSocket requests.         middleware: A list of middleware to run for every request. A starlette application will always             automatically include two middleware classes. `ServerErrorMiddleware` is added as the very             outermost middleware, to handle any uncaught errors occurring anywhere in the entire stack.             `ExceptionMiddleware` is added as the very innermost middleware, to deal with handled             exception cases occurring in the routing or endpoints.         exception_handlers: A mapping of either integer status codes, or exception class types onto             callables which handle the exceptions. Exception handler callables should be of the form             `handler(request, exc) -> response` and may be either standard functions, or async functions.         on_startup: A list of callables to run on application startup. Startup handler callables do not             take any arguments, and may be either standard functions, or async functions.         on_shutdown: A list of callables to run on application shutdown. Shutdown handler callables do             not take any arguments, and may be either standard functions, or async functions.         lifespan: A lifespan context function, which can be used to perform startup and shutdown tasks.             This is a newer style that replaces the `on_startup` and `on_shutdown` handlers. Use one or             the other, not both.     \"\"\"     super().__init__(         debug=debug,         routes=routes,         middleware=middleware,         exception_handlers=exception_handlers,         on_startup=on_startup,         on_shutdown=on_shutdown,         lifespan=lifespan,     )      async def endpoint(request: Request) -> Response:         \"\"\"Endpoint to run the agent with the provided input data.\"\"\"         return await handle_ag_ui_request(             agent,             request,             output_type=output_type,             model=model,             deps=deps,             model_settings=model_settings,             usage_limits=usage_limits,             usage=usage,             infer_name=infer_name,             toolsets=toolsets,         )      self.router.add_route('/', endpoint, methods=['POST'], name='run_agent') ``` |", "url": "https://ai.pydantic.dev/ag_ui/index.html#aguiapp", "page": "ag_ui/index.html", "source_site": "pydantic_ai"}
{"title": "handle\\_ag\\_ui\\_request `async`", "anchor": "handleaguirequest-async", "md_text": "```\nhandle_ag_ui_request(\n    agent: AbstractAgent[AgentDepsT, Any],\n    request: Request,\n    *,\n    output_type: OutputSpec[Any] | None = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    on_complete: OnCompleteFunc | None = None\n) -> Response\n```\n\nHandle an AG-UI request by running the agent and returning a streaming response.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `agent` | `AbstractAgent[AgentDepsT, Any]` | The agent to run. | *required* |\n| `request` | `Request` | The Starlette request (e.g. from FastAPI) containing the AG-UI run input. | *required* |\n| `output_type` | `OutputSpec[Any] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `on_complete` | `OnCompleteFunc | None` | Optional callback function called when the agent run completes successfully. The callback receives the completed [`AgentRunResult`](../agent/index.html#pydantic_ai.agent.AgentRunResult) and can access `all_messages()` and other result data. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `Response` | A streaming Starlette response with AG-UI protocol events. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ag_ui.py`", "url": "https://ai.pydantic.dev/ag_ui/index.html#handleaguirequest-async", "page": "ag_ui/index.html", "source_site": "pydantic_ai"}
{"title": "handle\\_ag\\_ui\\_request `async`", "anchor": "handleaguirequest-async", "md_text": "|  |  |\n| --- | --- |\n| ``` 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 ``` | ``` async def handle_ag_ui_request(     agent: AbstractAgent[AgentDepsT, Any],     request: Request,     *,     output_type: OutputSpec[Any] | None = None,     model: Model | KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: UsageLimits | None = None,     usage: RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     on_complete: OnCompleteFunc | None = None, ) -> Response:     \"\"\"Handle an AG-UI request by running the agent and returning a streaming response.      Args:         agent: The agent to run.         request: The Starlette request (e.g. from FastAPI) containing the AG-UI run input.          output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no             output validators since output validators would expect an argument that matches the agent's output type.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.         on_complete: Optional callback function called when the agent run completes successfully.             The callback receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can access `all_messages()` and other result data.      Returns:         A streaming Starlette response with AG-UI protocol events.     \"\"\"     accept = request.headers.get('accept', SSE_CONTENT_TYPE)     try:         input_data = RunAgentInput.model_validate(await request.json())     except ValidationError as e:  # pragma: no cover         return Response(             content=json.dumps(e.json()),             media_type='application/json',             status_code=HTTPStatus.UNPROCESSABLE_ENTITY,         )      return StreamingResponse(         run_ag_ui(             agent,             input_data,             accept,             output_type=output_type,             model=model,             deps=deps,             model_settings=model_settings,             usage_limits=usage_limits,             usage=usage,             infer_name=infer_name,             toolsets=toolsets,             on_complete=on_complete,         ),         media_type=accept,     ) ``` |", "url": "https://ai.pydantic.dev/ag_ui/index.html#handleaguirequest-async", "page": "ag_ui/index.html", "source_site": "pydantic_ai"}
{"title": "run\\_ag\\_ui `async`", "anchor": "runagui-async", "md_text": "```\nrun_ag_ui(\n    agent: AbstractAgent[AgentDepsT, Any],\n    run_input: RunAgentInput,\n    accept: str = SSE_CONTENT_TYPE,\n    *,\n    output_type: OutputSpec[Any] | None = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    on_complete: OnCompleteFunc | None = None\n) -> AsyncIterator[str]\n```\n\nRun the agent with the AG-UI run input and stream AG-UI protocol events.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `agent` | `AbstractAgent[AgentDepsT, Any]` | The agent to run. | *required* |\n| `run_input` | `RunAgentInput` | The AG-UI run input containing thread\\_id, run\\_id, messages, etc. | *required* |\n| `accept` | `str` | The accept header value for the run. | `SSE_CONTENT_TYPE` |\n| `output_type` | `OutputSpec[Any] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `on_complete` | `OnCompleteFunc | None` | Optional callback function called when the agent run completes successfully. The callback receives the completed [`AgentRunResult`](../agent/index.html#pydantic_ai.agent.AgentRunResult) and can access `all_messages()` and other result data. | `None` |\n\nYields:\n\n| Type | Description |\n| --- | --- |\n| `AsyncIterator[str]` | Streaming event chunks encoded as strings according to the accept header value. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ag_ui.py`", "url": "https://ai.pydantic.dev/ag_ui/index.html#runagui-async", "page": "ag_ui/index.html", "source_site": "pydantic_ai"}
{"title": "run\\_ag\\_ui `async`", "anchor": "runagui-async", "md_text": "|  |  |\n| --- | --- |\n| ``` 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 ``` | ``` async def run_ag_ui(     agent: AbstractAgent[AgentDepsT, Any],     run_input: RunAgentInput,     accept: str = SSE_CONTENT_TYPE,     *,     output_type: OutputSpec[Any] | None = None,     model: Model | KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: UsageLimits | None = None,     usage: RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     on_complete: OnCompleteFunc | None = None, ) -> AsyncIterator[str]:     \"\"\"Run the agent with the AG-UI run input and stream AG-UI protocol events.      Args:         agent: The agent to run.         run_input: The AG-UI run input containing thread_id, run_id, messages, etc.         accept: The accept header value for the run.          output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no             output validators since output validators would expect an argument that matches the agent's output type.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.         on_complete: Optional callback function called when the agent run completes successfully.             The callback receives the completed [`AgentRunResult`][pydantic_ai.agent.AgentRunResult] and can access `all_messages()` and other result data.      Yields:         Streaming event chunks encoded as strings according to the accept header value.     \"\"\"     encoder = EventEncoder(accept=accept)     if run_input.tools:         # AG-UI tools can't be prefixed as that would result in a mismatch between the tool names in the         # Pydantic AI events and actual AG-UI tool names, preventing the tool from being called. If any         # conflicts arise, the AG-UI tool should be renamed or a `PrefixedToolset` used for local toolsets.         toolset = _AGUIFrontendToolset[AgentDepsT](run_input.tools)         toolsets = [*toolsets, toolset] if toolsets else [toolset]      try:         yield encoder.encode(             RunStartedEvent(                 thread_id=run_input.thread_id,                 run_id=run_input.run_id,             ),         )          if not run_input.messages:             raise _NoMessagesError          raw_state: dict[str, Any] = run_input.state or {}         if isinstance(deps, StateHandler):             if isinstance(deps.state, BaseModel):                 try:                     state = type(deps.state).model_validate(raw_state)                 except ValidationError as e:  # pragma: no cover                     raise _InvalidStateError from e             else:                 state = raw_state              deps = replace(deps, state=state)         elif raw_state:             raise UserError(                 f'AG-UI state is provided but `deps` of type `{type(deps).__name__}` does not implement the `StateHandler` protocol: it needs to be a dataclass with a non-optional `state` field.'             )         else:             # `deps` not being a `StateHandler` is OK if there is no state.             pass          messages = _messages_from_ag_ui(run_input.messages)          async with agent.iter(             user_prompt=None,             output_type=[output_type or agent.output_type, DeferredToolRequests],             message_history=messages,             model=model,             deps=deps,             model_settings=model_settings,             usage_limits=usage_limits,             usage=usage,             infer_name=infer_name,             toolsets=toolsets,         ) as run:             async for event in _agent_stream(run):                 yield encoder.encode(event)          if on_complete is not None and run.result is not None:             if _utils.is_async_callable(on_complete):                 await on_complete(run.result)             else:                 await _utils.run_in_executor(on_complete, run.result)     except _RunError as e:         yield encoder.encode(             RunErrorEvent(message=e.message, code=e.code),         )     except Exception as e:         yield encoder.encode(             RunErrorEvent(message=str(e)),         )         raise e     else:         yield encoder.encode(             RunFinishedEvent(                 thread_id=run_input.thread_id,                 run_id=run_input.run_id,             ),         ) ``` |", "url": "https://ai.pydantic.dev/ag_ui/index.html#runagui-async", "page": "ag_ui/index.html", "source_site": "pydantic_ai"}
{"title": "StateHandler", "anchor": "statehandler", "md_text": "Bases: `Protocol`\n\nProtocol for state handlers in agent runs. Requires the class to be a dataclass with a `state` field.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ag_ui.py`\n\n|  |  |\n| --- | --- |\n| ``` 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 ``` | ``` @runtime_checkable class StateHandler(Protocol):     \"\"\"Protocol for state handlers in agent runs. Requires the class to be a dataclass with a `state` field.\"\"\"      # Has to be a dataclass so we can use `replace` to update the state.     # From https://github.com/python/typeshed/blob/9ab7fde0a0cd24ed7a72837fcb21093b811b80d8/stdlib/_typeshed/__init__.pyi#L352     __dataclass_fields__: ClassVar[dict[str, Field[Any]]]      @property     def state(self) -> State:         \"\"\"Get the current state of the agent run.\"\"\"         ...      @state.setter     def state(self, state: State) -> None:         \"\"\"Set the state of the agent run.          This method is called to update the state of the agent run with the         provided state.          Args:             state: The run state.          Raises:             InvalidStateError: If `state` does not match the expected model.         \"\"\"         ... ``` |\n\n#### state `property` `writable`\n\n```\nstate: State\n```\n\nGet the current state of the agent run.", "url": "https://ai.pydantic.dev/ag_ui/index.html#statehandler", "page": "ag_ui/index.html", "source_site": "pydantic_ai"}
{"title": "StateDeps `dataclass`", "anchor": "statedeps-dataclass", "md_text": "Bases: `Generic[StateT]`\n\nProvides AG-UI state management.\n\nThis class is used to manage the state of an agent run. It allows setting\nthe state of the agent run with a specific type of state model, which must\nbe a subclass of `BaseModel`.\n\nThe state is set using the `state` setter by the `Adapter` when the run starts.\n\nImplements the `StateHandler` protocol.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ag_ui.py`\n\n|  |  |\n| --- | --- |\n| ``` 719 720 721 722 723 724 725 726 727 728 729 730 731 732 ``` | ``` @dataclass class StateDeps(Generic[StateT]):     \"\"\"Provides AG-UI state management.      This class is used to manage the state of an agent run. It allows setting     the state of the agent run with a specific type of state model, which must     be a subclass of `BaseModel`.      The state is set using the `state` setter by the `Adapter` when the run starts.      Implements the `StateHandler` protocol.     \"\"\"      state: StateT ``` |", "url": "https://ai.pydantic.dev/ag_ui/index.html#statedeps-dataclass", "page": "ag_ui/index.html", "source_site": "pydantic_ai"}
{"title": "model\\_request `async`", "anchor": "modelrequest-async", "md_text": "```\nmodel_request(\n    model: Model | KnownModelName | str,\n    messages: Sequence[ModelMessage],\n    *,\n    model_settings: ModelSettings | None = None,\n    model_request_parameters: (\n        ModelRequestParameters | None\n    ) = None,\n    instrument: InstrumentationSettings | bool | None = None\n) -> ModelResponse\n```\n\nMake a non-streamed request to a model.\n\nmodel\\_request\\_example.py\n\n```\nfrom pydantic_ai import ModelRequest\nfrom pydantic_ai.direct import model_request\n\n\nasync def main():\n    model_response = await model_request(\n        'anthropic:claude-3-5-haiku-latest',\n        [ModelRequest.user_text_prompt('What is the capital of France?')]  # (1)!\n    )\n    print(model_response)\n    '''\n    ModelResponse(\n        parts=[TextPart(content='The capital of France is Paris.')],\n        usage=RequestUsage(input_tokens=56, output_tokens=7),\n        model_name='claude-3-5-haiku-latest',\n        timestamp=datetime.datetime(...),\n    )\n    '''\n```\n\n1. See [`ModelRequest.user_text_prompt`](../messages/index.html#pydantic_ai.messages.ModelRequest.user_text_prompt) for details.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model` | `Model | KnownModelName | str` | The model to make a request to. We allow `str` here since the actual list of allowed models changes frequently. | *required* |\n| `messages` | `Sequence[ModelMessage]` | Messages to send to the model | *required* |\n| `model_settings` | `ModelSettings | None` | optional model settings | `None` |\n| `model_request_parameters` | `ModelRequestParameters | None` | optional model request parameters | `None` |\n| `instrument` | `InstrumentationSettings | bool | None` | Whether to instrument the request with OpenTelemetry/Logfire, if `None` the value from [`logfire.instrument_pydantic_ai`](https://logfire.pydantic.dev/docs/reference/api/logfire/#logfire.Logfire.instrument_pydantic_ai) is used. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `ModelResponse` | The model response and token usage associated with the request. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/direct.py`\n\n|  |  |\n| --- | --- |\n| ``` 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 ``` | ``` async def model_request(     model: models.Model | models.KnownModelName | str,     messages: Sequence[messages.ModelMessage],     *,     model_settings: settings.ModelSettings | None = None,     model_request_parameters: models.ModelRequestParameters | None = None,     instrument: instrumented_models.InstrumentationSettings | bool | None = None, ) -> messages.ModelResponse:     \"\"\"Make a non-streamed request to a model.      ```py title=\"model_request_example.py\"     from pydantic_ai import ModelRequest     from pydantic_ai.direct import model_request       async def main():         model_response = await model_request(             'anthropic:claude-3-5-haiku-latest',             [ModelRequest.user_text_prompt('What is the capital of France?')]  # (1)!         )         print(model_response)         '''         ModelResponse(             parts=[TextPart(content='The capital of France is Paris.')],             usage=RequestUsage(input_tokens=56, output_tokens=7),             model_name='claude-3-5-haiku-latest',             timestamp=datetime.datetime(...),         )         '''     ```      1. See [`ModelRequest.user_text_prompt`][pydantic_ai.messages.ModelRequest.user_text_prompt] for details.      Args:         model: The model to make a request to. We allow `str` here since the actual list of allowed models changes frequently.         messages: Messages to send to the model         model_settings: optional model settings         model_request_parameters: optional model request parameters         instrument: Whether to instrument the request with OpenTelemetry/Logfire, if `None` the value from             [`logfire.instrument_pydantic_ai`][logfire.Logfire.instrument_pydantic_ai] is used.      Returns:         The model response and token usage associated with the request.     \"\"\"     model_instance = _prepare_model(model, instrument)     return await model_instance.request(         list(messages),         model_settings,         model_request_parameters or models.ModelRequestParameters(),     ) ``` |", "url": "https://ai.pydantic.dev/direct/index.html#modelrequest-async", "page": "direct/index.html", "source_site": "pydantic_ai"}
{"title": "model\\_request\\_sync", "anchor": "modelrequestsync", "md_text": "```\nmodel_request_sync(\n    model: Model | KnownModelName | str,\n    messages: Sequence[ModelMessage],\n    *,\n    model_settings: ModelSettings | None = None,\n    model_request_parameters: (\n        ModelRequestParameters | None\n    ) = None,\n    instrument: InstrumentationSettings | bool | None = None\n) -> ModelResponse\n```\n\nMake a Synchronous, non-streamed request to a model.\n\nThis is a convenience method that wraps [`model_request`](index.html#pydantic_ai.direct.model_request) with\n`loop.run_until_complete(...)`. You therefore can't use this method inside async code or if there's an active event loop.\n\nmodel\\_request\\_sync\\_example.py\n\n```\nfrom pydantic_ai import ModelRequest\nfrom pydantic_ai.direct import model_request_sync\n\nmodel_response = model_request_sync(\n    'anthropic:claude-3-5-haiku-latest',\n    [ModelRequest.user_text_prompt('What is the capital of France?')]  # (1)!\n)\nprint(model_response)\n'''\nModelResponse(\n    parts=[TextPart(content='The capital of France is Paris.')],\n    usage=RequestUsage(input_tokens=56, output_tokens=7),\n    model_name='claude-3-5-haiku-latest',\n    timestamp=datetime.datetime(...),\n)\n'''\n```\n\n1. See [`ModelRequest.user_text_prompt`](../messages/index.html#pydantic_ai.messages.ModelRequest.user_text_prompt) for details.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model` | `Model | KnownModelName | str` | The model to make a request to. We allow `str` here since the actual list of allowed models changes frequently. | *required* |\n| `messages` | `Sequence[ModelMessage]` | Messages to send to the model | *required* |\n| `model_settings` | `ModelSettings | None` | optional model settings | `None` |\n| `model_request_parameters` | `ModelRequestParameters | None` | optional model request parameters | `None` |\n| `instrument` | `InstrumentationSettings | bool | None` | Whether to instrument the request with OpenTelemetry/Logfire, if `None` the value from [`logfire.instrument_pydantic_ai`](https://logfire.pydantic.dev/docs/reference/api/logfire/#logfire.Logfire.instrument_pydantic_ai) is used. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `ModelResponse` | The model response and token usage associated with the request. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/direct.py`\n\n|  |  |\n| --- | --- |\n| ```  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 ``` | ``` def model_request_sync(     model: models.Model | models.KnownModelName | str,     messages: Sequence[messages.ModelMessage],     *,     model_settings: settings.ModelSettings | None = None,     model_request_parameters: models.ModelRequestParameters | None = None,     instrument: instrumented_models.InstrumentationSettings | bool | None = None, ) -> messages.ModelResponse:     \"\"\"Make a Synchronous, non-streamed request to a model.      This is a convenience method that wraps [`model_request`][pydantic_ai.direct.model_request] with     `loop.run_until_complete(...)`. You therefore can't use this method inside async code or if there's an active event loop.      ```py title=\"model_request_sync_example.py\"     from pydantic_ai import ModelRequest     from pydantic_ai.direct import model_request_sync      model_response = model_request_sync(         'anthropic:claude-3-5-haiku-latest',         [ModelRequest.user_text_prompt('What is the capital of France?')]  # (1)!     )     print(model_response)     '''     ModelResponse(         parts=[TextPart(content='The capital of France is Paris.')],         usage=RequestUsage(input_tokens=56, output_tokens=7),         model_name='claude-3-5-haiku-latest',         timestamp=datetime.datetime(...),     )     '''     ```      1. See [`ModelRequest.user_text_prompt`][pydantic_ai.messages.ModelRequest.user_text_prompt] for details.      Args:         model: The model to make a request to. We allow `str` here since the actual list of allowed models changes frequently.         messages: Messages to send to the model         model_settings: optional model settings         model_request_parameters: optional model request parameters         instrument: Whether to instrument the request with OpenTelemetry/Logfire, if `None` the value from             [`logfire.instrument_pydantic_ai`][logfire.Logfire.instrument_pydantic_ai] is used.      Returns:         The model response and token usage associated with the request.     \"\"\"     return _get_event_loop().run_until_complete(         model_request(             model,             list(messages),             model_settings=model_settings,             model_request_parameters=model_request_parameters,             instrument=instrument,         )     ) ``` |", "url": "https://ai.pydantic.dev/direct/index.html#modelrequestsync", "page": "direct/index.html", "source_site": "pydantic_ai"}
{"title": "model\\_request\\_stream", "anchor": "modelrequeststream", "md_text": "```\nmodel_request_stream(\n    model: Model | KnownModelName | str,\n    messages: Sequence[ModelMessage],\n    *,\n    model_settings: ModelSettings | None = None,\n    model_request_parameters: (\n        ModelRequestParameters | None\n    ) = None,\n    instrument: InstrumentationSettings | bool | None = None\n) -> AbstractAsyncContextManager[StreamedResponse]\n```\n\nMake a streamed async request to a model.\n\nmodel\\_request\\_stream\\_example.py\n\n```\nfrom pydantic_ai import ModelRequest\nfrom pydantic_ai.direct import model_request_stream\n\n\nasync def main():\n    messages = [ModelRequest.user_text_prompt('Who was Albert Einstein?')]  # (1)!\n    async with model_request_stream('openai:gpt-4.1-mini', messages) as stream:\n        chunks = []\n        async for chunk in stream:\n            chunks.append(chunk)\n        print(chunks)\n        '''\n        [\n            PartStartEvent(index=0, part=TextPart(content='Albert Einstein was ')),\n            FinalResultEvent(tool_name=None, tool_call_id=None),\n            PartDeltaEvent(\n                index=0, delta=TextPartDelta(content_delta='a German-born theoretical ')\n            ),\n            PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='physicist.')),\n        ]\n        '''\n```\n\n1. See [`ModelRequest.user_text_prompt`](../messages/index.html#pydantic_ai.messages.ModelRequest.user_text_prompt) for details.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model` | `Model | KnownModelName | str` | The model to make a request to. We allow `str` here since the actual list of allowed models changes frequently. | *required* |\n| `messages` | `Sequence[ModelMessage]` | Messages to send to the model | *required* |\n| `model_settings` | `ModelSettings | None` | optional model settings | `None` |\n| `model_request_parameters` | `ModelRequestParameters | None` | optional model request parameters | `None` |\n| `instrument` | `InstrumentationSettings | bool | None` | Whether to instrument the request with OpenTelemetry/Logfire, if `None` the value from [`logfire.instrument_pydantic_ai`](https://logfire.pydantic.dev/docs/reference/api/logfire/#logfire.Logfire.instrument_pydantic_ai) is used. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AbstractAsyncContextManager[StreamedResponse]` | A [stream response](../models/base/index.html#pydantic_ai.models.StreamedResponse) async context manager. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/direct.py`\n\n|  |  |\n| --- | --- |\n| ``` 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 ``` | ``` def model_request_stream(     model: models.Model | models.KnownModelName | str,     messages: Sequence[messages.ModelMessage],     *,     model_settings: settings.ModelSettings | None = None,     model_request_parameters: models.ModelRequestParameters | None = None,     instrument: instrumented_models.InstrumentationSettings | bool | None = None, ) -> AbstractAsyncContextManager[models.StreamedResponse]:     \"\"\"Make a streamed async request to a model.      ```py {title=\"model_request_stream_example.py\"}      from pydantic_ai import ModelRequest     from pydantic_ai.direct import model_request_stream       async def main():         messages = [ModelRequest.user_text_prompt('Who was Albert Einstein?')]  # (1)!         async with model_request_stream('openai:gpt-4.1-mini', messages) as stream:             chunks = []             async for chunk in stream:                 chunks.append(chunk)             print(chunks)             '''             [                 PartStartEvent(index=0, part=TextPart(content='Albert Einstein was ')),                 FinalResultEvent(tool_name=None, tool_call_id=None),                 PartDeltaEvent(                     index=0, delta=TextPartDelta(content_delta='a German-born theoretical ')                 ),                 PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='physicist.')),             ]             '''     ```      1. See [`ModelRequest.user_text_prompt`][pydantic_ai.messages.ModelRequest.user_text_prompt] for details.      Args:         model: The model to make a request to. We allow `str` here since the actual list of allowed models changes frequently.         messages: Messages to send to the model         model_settings: optional model settings         model_request_parameters: optional model request parameters         instrument: Whether to instrument the request with OpenTelemetry/Logfire, if `None` the value from             [`logfire.instrument_pydantic_ai`][logfire.Logfire.instrument_pydantic_ai] is used.      Returns:         A [stream response][pydantic_ai.models.StreamedResponse] async context manager.     \"\"\"     model_instance = _prepare_model(model, instrument)     return model_instance.request_stream(         list(messages),         model_settings,         model_request_parameters or models.ModelRequestParameters(),     ) ``` |", "url": "https://ai.pydantic.dev/direct/index.html#modelrequeststream", "page": "direct/index.html", "source_site": "pydantic_ai"}
{"title": "model\\_request\\_stream\\_sync", "anchor": "modelrequeststreamsync", "md_text": "```\nmodel_request_stream_sync(\n    model: Model | KnownModelName | str,\n    messages: Sequence[ModelMessage],\n    *,\n    model_settings: ModelSettings | None = None,\n    model_request_parameters: (\n        ModelRequestParameters | None\n    ) = None,\n    instrument: InstrumentationSettings | bool | None = None\n) -> StreamedResponseSync\n```\n\nMake a streamed synchronous request to a model.\n\nThis is the synchronous version of [`model_request_stream`](index.html#pydantic_ai.direct.model_request_stream).\nIt uses threading to run the asynchronous stream in the background while providing a synchronous iterator interface.\n\nmodel\\_request\\_stream\\_sync\\_example.py\n\n```\nfrom pydantic_ai import ModelRequest\nfrom pydantic_ai.direct import model_request_stream_sync\n\nmessages = [ModelRequest.user_text_prompt('Who was Albert Einstein?')]\nwith model_request_stream_sync('openai:gpt-4.1-mini', messages) as stream:\n    chunks = []\n    for chunk in stream:\n        chunks.append(chunk)\n    print(chunks)\n    '''\n    [\n        PartStartEvent(index=0, part=TextPart(content='Albert Einstein was ')),\n        FinalResultEvent(tool_name=None, tool_call_id=None),\n        PartDeltaEvent(\n            index=0, delta=TextPartDelta(content_delta='a German-born theoretical ')\n        ),\n        PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='physicist.')),\n    ]\n    '''\n```\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model` | `Model | KnownModelName | str` | The model to make a request to. We allow `str` here since the actual list of allowed models changes frequently. | *required* |\n| `messages` | `Sequence[ModelMessage]` | Messages to send to the model | *required* |\n| `model_settings` | `ModelSettings | None` | optional model settings | `None` |\n| `model_request_parameters` | `ModelRequestParameters | None` | optional model request parameters | `None` |\n| `instrument` | `InstrumentationSettings | bool | None` | Whether to instrument the request with OpenTelemetry/Logfire, if `None` the value from [`logfire.instrument_pydantic_ai`](https://logfire.pydantic.dev/docs/reference/api/logfire/#logfire.Logfire.instrument_pydantic_ai) is used. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `StreamedResponseSync` | A [sync stream response](index.html#pydantic_ai.direct.StreamedResponseSync) context manager. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/direct.py`\n\n|  |  |\n| --- | --- |\n| ``` 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 ``` | ``` def model_request_stream_sync(     model: models.Model | models.KnownModelName | str,     messages: Sequence[messages.ModelMessage],     *,     model_settings: settings.ModelSettings | None = None,     model_request_parameters: models.ModelRequestParameters | None = None,     instrument: instrumented_models.InstrumentationSettings | bool | None = None, ) -> StreamedResponseSync:     \"\"\"Make a streamed synchronous request to a model.      This is the synchronous version of [`model_request_stream`][pydantic_ai.direct.model_request_stream].     It uses threading to run the asynchronous stream in the background while providing a synchronous iterator interface.      ```py {title=\"model_request_stream_sync_example.py\"}      from pydantic_ai import ModelRequest     from pydantic_ai.direct import model_request_stream_sync      messages = [ModelRequest.user_text_prompt('Who was Albert Einstein?')]     with model_request_stream_sync('openai:gpt-4.1-mini', messages) as stream:         chunks = []         for chunk in stream:             chunks.append(chunk)         print(chunks)         '''         [             PartStartEvent(index=0, part=TextPart(content='Albert Einstein was ')),             FinalResultEvent(tool_name=None, tool_call_id=None),             PartDeltaEvent(                 index=0, delta=TextPartDelta(content_delta='a German-born theoretical ')             ),             PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='physicist.')),         ]         '''     ```      Args:         model: The model to make a request to. We allow `str` here since the actual list of allowed models changes frequently.         messages: Messages to send to the model         model_settings: optional model settings         model_request_parameters: optional model request parameters         instrument: Whether to instrument the request with OpenTelemetry/Logfire, if `None` the value from             [`logfire.instrument_pydantic_ai`][logfire.Logfire.instrument_pydantic_ai] is used.      Returns:         A [sync stream response][pydantic_ai.direct.StreamedResponseSync] context manager.     \"\"\"     async_stream_cm = model_request_stream(         model=model,         messages=list(messages),         model_settings=model_settings,         model_request_parameters=model_request_parameters,         instrument=instrument,     )      return StreamedResponseSync(async_stream_cm) ``` |", "url": "https://ai.pydantic.dev/direct/index.html#modelrequeststreamsync", "page": "direct/index.html", "source_site": "pydantic_ai"}
{"title": "StreamedResponseSync `dataclass`", "anchor": "streamedresponsesync-dataclass", "md_text": "Synchronous wrapper to async streaming responses by running the async producer in a background thread and providing a synchronous iterator.\n\nThis class must be used as a context manager with the `with` statement.\n\nSource code in `pydantic_ai_slim/pydantic_ai/direct.py`", "url": "https://ai.pydantic.dev/direct/index.html#streamedresponsesync-dataclass", "page": "direct/index.html", "source_site": "pydantic_ai"}
{"title": "StreamedResponseSync `dataclass`", "anchor": "streamedresponsesync-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 ``` | ``` @dataclass class StreamedResponseSync:     \"\"\"Synchronous wrapper to async streaming responses by running the async producer in a background thread and providing a synchronous iterator.      This class must be used as a context manager with the `with` statement.     \"\"\"      _async_stream_cm: AbstractAsyncContextManager[StreamedResponse]     _queue: queue.Queue[messages.ModelResponseStreamEvent | Exception | None] = field(         default_factory=queue.Queue, init=False     )     _thread: threading.Thread | None = field(default=None, init=False)     _stream_response: StreamedResponse | None = field(default=None, init=False)     _exception: Exception | None = field(default=None, init=False)     _context_entered: bool = field(default=False, init=False)     _stream_ready: threading.Event = field(default_factory=threading.Event, init=False)      def __enter__(self) -> StreamedResponseSync:         self._context_entered = True         self._start_producer()         return self      def __exit__(         self,         _exc_type: type[BaseException] | None,         _exc_val: BaseException | None,         _exc_tb: TracebackType | None,     ) -> None:         self._cleanup()      def __iter__(self) -> Iterator[messages.ModelResponseStreamEvent]:         \"\"\"Stream the response as an iterable of [`ModelResponseStreamEvent`][pydantic_ai.messages.ModelResponseStreamEvent]s.\"\"\"         self._check_context_manager_usage()          while True:             item = self._queue.get()             if item is None:  # End of stream                 break             elif isinstance(item, Exception):                 raise item             else:                 yield item      def __repr__(self) -> str:         if self._stream_response:             return repr(self._stream_response)         else:             return f'{self.__class__.__name__}(context_entered={self._context_entered})'      __str__ = __repr__      def _check_context_manager_usage(self) -> None:         if not self._context_entered:             raise RuntimeError(                 'StreamedResponseSync must be used as a context manager. '                 'Use: `with model_request_stream_sync(...) as stream:`'             )      def _ensure_stream_ready(self) -> StreamedResponse:         self._check_context_manager_usage()          if self._stream_response is None:             # Wait for the background thread to signal that the stream is ready             if not self._stream_ready.wait(timeout=STREAM_INITIALIZATION_TIMEOUT):                 raise RuntimeError('Stream failed to initialize within timeout')              if self._stream_response is None:  # pragma: no cover                 raise RuntimeError('Stream failed to initialize')          return self._stream_response      def _start_producer(self):         self._thread = threading.Thread(target=self._async_producer, daemon=True)         self._thread.start()      def _async_producer(self):         async def _consume_async_stream():             try:                 async with self._async_stream_cm as stream:                     self._stream_response = stream                     # Signal that the stream is ready                     self._stream_ready.set()                     async for event in stream:                         self._queue.put(event)             except Exception as e:                 # Signal ready even on error so waiting threads don't hang                 self._stream_ready.set()                 self._queue.put(e)             finally:                 self._queue.put(None)  # Signal end          _get_event_loop().run_until_complete(_consume_async_stream())      def _cleanup(self):         if self._thread and self._thread.is_alive():             self._thread.join()      # TODO (v2): Drop in favor of `response` property     def get(self) -> messages.ModelResponse:         \"\"\"Build a ModelResponse from the data received from the stream so far.\"\"\"         return self._ensure_stream_ready().get()      @property     def response(self) -> messages.ModelResponse:         \"\"\"Get the current state of the response.\"\"\"         return self.get()      # TODO (v2): Make this a property     def usage(self) -> RequestUsage:         \"\"\"Get the usage of the response so far.\"\"\"         return self._ensure_stream_ready().usage()      @property     def model_name(self) -> str:         \"\"\"Get the model name of the response.\"\"\"         return self._ensure_stream_ready().model_name      @property     def timestamp(self) -> datetime:         \"\"\"Get the timestamp of the response.\"\"\"         return self._ensure_stream_ready().timestamp ``` |", "url": "https://ai.pydantic.dev/direct/index.html#streamedresponsesync-dataclass", "page": "direct/index.html", "source_site": "pydantic_ai"}
{"title": "StreamedResponseSync `dataclass`", "anchor": "streamedresponsesync-dataclass", "md_text": "#### \\_\\_iter\\_\\_\n\n```\n__iter__() -> Iterator[ModelResponseStreamEvent]\n```\n\nStream the response as an iterable of [`ModelResponseStreamEvent`](../messages/index.html#pydantic_ai.messages.ModelResponseStreamEvent)s.\n\nSource code in `pydantic_ai_slim/pydantic_ai/direct.py`\n\n|  |  |\n| --- | --- |\n| ``` 300 301 302 303 304 305 306 307 308 309 310 311 ``` | ``` def __iter__(self) -> Iterator[messages.ModelResponseStreamEvent]:     \"\"\"Stream the response as an iterable of [`ModelResponseStreamEvent`][pydantic_ai.messages.ModelResponseStreamEvent]s.\"\"\"     self._check_context_manager_usage()      while True:         item = self._queue.get()         if item is None:  # End of stream             break         elif isinstance(item, Exception):             raise item         else:             yield item ``` |\n\n#### get\n\n```\nget() -> ModelResponse\n```\n\nBuild a ModelResponse from the data received from the stream so far.\n\nSource code in `pydantic_ai_slim/pydantic_ai/direct.py`\n\n|  |  |\n| --- | --- |\n| ``` 368 369 370 ``` | ``` def get(self) -> messages.ModelResponse:     \"\"\"Build a ModelResponse from the data received from the stream so far.\"\"\"     return self._ensure_stream_ready().get() ``` |\n\n#### response `property`\n\n```\nresponse: ModelResponse\n```\n\nGet the current state of the response.\n\n#### usage\n\n```\nusage() -> RequestUsage\n```\n\nGet the usage of the response so far.\n\nSource code in `pydantic_ai_slim/pydantic_ai/direct.py`\n\n|  |  |\n| --- | --- |\n| ``` 378 379 380 ``` | ``` def usage(self) -> RequestUsage:     \"\"\"Get the usage of the response so far.\"\"\"     return self._ensure_stream_ready().usage() ``` |\n\n#### model\\_name `property`\n\n```\nmodel_name: str\n```\n\nGet the model name of the response.\n\n#### timestamp `property`\n\n```\ntimestamp: datetime\n```\n\nGet the timestamp of the response.", "url": "https://ai.pydantic.dev/direct/index.html#streamedresponsesync-dataclass", "page": "direct/index.html", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "md_text": "To use `CohereModel`, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `cohere` optional group:\n\npipuv\n\n```\npip install \"pydantic-ai-slim[cohere]\"\n\nuv add \"pydantic-ai-slim[cohere]\"\n```", "url": "https://ai.pydantic.dev/cohere/index.html#install", "page": "cohere/index.html", "source_site": "pydantic_ai"}
{"title": "Configuration", "anchor": "configuration", "md_text": "To use [Cohere](https://cohere.com/) through their API, go to [dashboard.cohere.com/api-keys](https://dashboard.cohere.com/api-keys) and follow your nose until you find the place to generate an API key.\n\n`CohereModelName` contains a list of the most popular Cohere models.", "url": "https://ai.pydantic.dev/cohere/index.html#configuration", "page": "cohere/index.html", "source_site": "pydantic_ai"}
{"title": "Environment variable", "anchor": "environment-variable", "md_text": "Once you have the API key, you can set it as an environment variable:\n\n```\nexport CO_API_KEY='your-api-key'\n```\n\nYou can then use `CohereModel` by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('cohere:command-r7b-12-2024')\n...\n```\n\nOr initialise the model directly with just the model name:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.cohere import CohereModel\n\nmodel = CohereModel('command-r7b-12-2024')\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/cohere/index.html#environment-variable", "page": "cohere/index.html", "source_site": "pydantic_ai"}
{"title": "`provider` argument", "anchor": "provider-argument", "md_text": "You can provide a custom `Provider` via the `provider` argument:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.cohere import CohereModel\nfrom pydantic_ai.providers.cohere import CohereProvider\n\nmodel = CohereModel('command-r7b-12-2024', provider=CohereProvider(api_key='your-api-key'))\nagent = Agent(model)\n...\n```\n\nYou can also customize the `CohereProvider` with a custom `http_client`:\n\n```\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.cohere import CohereModel\nfrom pydantic_ai.providers.cohere import CohereProvider\n\ncustom_http_client = AsyncClient(timeout=30)\nmodel = CohereModel(\n    'command-r7b-12-2024',\n    provider=CohereProvider(api_key='your-api-key', http_client=custom_http_client),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/cohere/index.html#provider-argument", "page": "cohere/index.html", "source_site": "pydantic_ai"}
{"title": "RequestUsage `dataclass`", "anchor": "requestusage-dataclass", "md_text": "Bases: `UsageBase`\n\nLLM usage associated with a single request.\n\nThis is an implementation of `genai_prices.types.AbstractUsage` so it can be used to calculate the price of the\nrequest using [genai-prices](https://github.com/pydantic/genai-prices).\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\n\n|  |  |\n| --- | --- |\n| ``` 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 ``` | ``` @dataclass(repr=False, kw_only=True) class RequestUsage(UsageBase):     \"\"\"LLM usage associated with a single request.      This is an implementation of `genai_prices.types.AbstractUsage` so it can be used to calculate the price of the     request using [genai-prices](https://github.com/pydantic/genai-prices).     \"\"\"      @property     def requests(self):         return 1      def incr(self, incr_usage: RequestUsage) -> None:         \"\"\"Increment the usage in place.          Args:             incr_usage: The usage to increment by.         \"\"\"         return _incr_usage_tokens(self, incr_usage)      def __add__(self, other: RequestUsage) -> RequestUsage:         \"\"\"Add two RequestUsages together.          This is provided so it's trivial to sum usage information from multiple parts of a response.          **WARNING:** this CANNOT be used to sum multiple requests without breaking some pricing calculations.         \"\"\"         new_usage = copy(self)         new_usage.incr(other)         return new_usage      @classmethod     def extract(         cls,         data: Any,         *,         provider: str,         provider_url: str,         provider_fallback: str,         api_flavor: str = 'default',         details: dict[str, Any] | None = None,     ) -> RequestUsage:         \"\"\"Extract usage information from the response data using genai-prices.          Args:             data: The response data from the model API.             provider: The actual provider ID             provider_url: The provider base_url             provider_fallback: The fallback provider ID to use if the actual provider is not found in genai-prices.                 For example, an OpenAI model should set this to \"openai\" in case it has an obscure provider ID.             api_flavor: The API flavor to use when extracting usage information,                 e.g. 'chat' or 'responses' for OpenAI.             details: Becomes the `details` field on the returned `RequestUsage` for convenience.         \"\"\"         details = details or {}         for provider_id, provider_api_url in [(None, provider_url), (provider, None), (provider_fallback, None)]:             try:                 provider_obj = get_snapshot().find_provider(None, provider_id, provider_api_url)                 _model_ref, extracted_usage = provider_obj.extract_usage(data, api_flavor=api_flavor)                 return cls(**{k: v for k, v in extracted_usage.__dict__.items() if v is not None}, details=details)             except Exception:                 pass         return cls(details=details) ``` |\n\n#### incr\n\n```\nincr(incr_usage: RequestUsage) -> None\n```\n\nIncrement the usage in place.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `incr_usage` | `RequestUsage` | The usage to increment by. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\n\n|  |  |\n| --- | --- |\n| ``` 116 117 118 119 120 121 122 ``` | ``` def incr(self, incr_usage: RequestUsage) -> None:     \"\"\"Increment the usage in place.      Args:         incr_usage: The usage to increment by.     \"\"\"     return _incr_usage_tokens(self, incr_usage) ``` |\n\n#### \\_\\_add\\_\\_\n\n```\n__add__(other: RequestUsage) -> RequestUsage\n```\n\nAdd two RequestUsages together.\n\nThis is provided so it's trivial to sum usage information from multiple parts of a response.\n\n**WARNING:** this CANNOT be used to sum multiple requests without breaking some pricing calculations.\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\n\n|  |  |\n| --- | --- |\n| ``` 124 125 126 127 128 129 130 131 132 133 ``` | ``` def __add__(self, other: RequestUsage) -> RequestUsage:     \"\"\"Add two RequestUsages together.      This is provided so it's trivial to sum usage information from multiple parts of a response.      **WARNING:** this CANNOT be used to sum multiple requests without breaking some pricing calculations.     \"\"\"     new_usage = copy(self)     new_usage.incr(other)     return new_usage ``` |\n\n#### extract `classmethod`\n\n```\nextract(\n    data: Any,\n    *,\n    provider: str,\n    provider_url: str,\n    provider_fallback: str,\n    api_flavor: str = \"default\",\n    details: dict[str, Any] | None = None\n) -> RequestUsage\n```", "url": "https://ai.pydantic.dev/usage/index.html#requestusage-dataclass", "page": "usage/index.html", "source_site": "pydantic_ai"}
{"title": "RequestUsage `dataclass`", "anchor": "requestusage-dataclass", "md_text": "Extract usage information from the response data using genai-prices.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `data` | `Any` | The response data from the model API. | *required* |\n| `provider` | `str` | The actual provider ID | *required* |\n| `provider_url` | `str` | The provider base\\_url | *required* |\n| `provider_fallback` | `str` | The fallback provider ID to use if the actual provider is not found in genai-prices. For example, an OpenAI model should set this to \"openai\" in case it has an obscure provider ID. | *required* |\n| `api_flavor` | `str` | The API flavor to use when extracting usage information, e.g. 'chat' or 'responses' for OpenAI. | `'default'` |\n| `details` | `dict[str, Any] | None` | Becomes the `details` field on the returned `RequestUsage` for convenience. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\n\n|  |  |\n| --- | --- |\n| ``` 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 ``` | ``` @classmethod def extract(     cls,     data: Any,     *,     provider: str,     provider_url: str,     provider_fallback: str,     api_flavor: str = 'default',     details: dict[str, Any] | None = None, ) -> RequestUsage:     \"\"\"Extract usage information from the response data using genai-prices.      Args:         data: The response data from the model API.         provider: The actual provider ID         provider_url: The provider base_url         provider_fallback: The fallback provider ID to use if the actual provider is not found in genai-prices.             For example, an OpenAI model should set this to \"openai\" in case it has an obscure provider ID.         api_flavor: The API flavor to use when extracting usage information,             e.g. 'chat' or 'responses' for OpenAI.         details: Becomes the `details` field on the returned `RequestUsage` for convenience.     \"\"\"     details = details or {}     for provider_id, provider_api_url in [(None, provider_url), (provider, None), (provider_fallback, None)]:         try:             provider_obj = get_snapshot().find_provider(None, provider_id, provider_api_url)             _model_ref, extracted_usage = provider_obj.extract_usage(data, api_flavor=api_flavor)             return cls(**{k: v for k, v in extracted_usage.__dict__.items() if v is not None}, details=details)         except Exception:             pass     return cls(details=details) ``` |", "url": "https://ai.pydantic.dev/usage/index.html#requestusage-dataclass", "page": "usage/index.html", "source_site": "pydantic_ai"}
{"title": "RunUsage `dataclass`", "anchor": "runusage-dataclass", "md_text": "Bases: `UsageBase`\n\nLLM usage associated with an agent run.\n\nResponsibility for calculating request usage is on the model; Pydantic AI simply sums the usage information across requests.\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\n\n|  |  |\n| --- | --- |\n| ``` 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 ``` | ``` @dataclass(repr=False, kw_only=True) class RunUsage(UsageBase):     \"\"\"LLM usage associated with an agent run.      Responsibility for calculating request usage is on the model; Pydantic AI simply sums the usage information across requests.     \"\"\"      requests: int = 0     \"\"\"Number of requests made to the LLM API.\"\"\"      tool_calls: int = 0     \"\"\"Number of successful tool calls executed during the run.\"\"\"      input_tokens: int = 0     \"\"\"Total number of input/prompt tokens.\"\"\"      cache_write_tokens: int = 0     \"\"\"Total number of tokens written to the cache.\"\"\"      cache_read_tokens: int = 0     \"\"\"Total number of tokens read from the cache.\"\"\"      input_audio_tokens: int = 0     \"\"\"Total number of audio input tokens.\"\"\"      cache_audio_read_tokens: int = 0     \"\"\"Total number of audio tokens read from the cache.\"\"\"      output_tokens: int = 0     \"\"\"Total number of output/completion tokens.\"\"\"      details: dict[str, int] = dataclasses.field(default_factory=dict)     \"\"\"Any extra details returned by the model.\"\"\"      def incr(self, incr_usage: RunUsage | RequestUsage) -> None:         \"\"\"Increment the usage in place.          Args:             incr_usage: The usage to increment by.         \"\"\"         if isinstance(incr_usage, RunUsage):             self.requests += incr_usage.requests             self.tool_calls += incr_usage.tool_calls         return _incr_usage_tokens(self, incr_usage)      def __add__(self, other: RunUsage | RequestUsage) -> RunUsage:         \"\"\"Add two RunUsages together.          This is provided so it's trivial to sum usage information from multiple runs.         \"\"\"         new_usage = copy(self)         new_usage.incr(other)         return new_usage ``` |\n\n#### requests `class-attribute` `instance-attribute`\n\n```\nrequests: int = 0\n```\n\nNumber of requests made to the LLM API.\n\n#### tool\\_calls `class-attribute` `instance-attribute`\n\n```\ntool_calls: int = 0\n```\n\nNumber of successful tool calls executed during the run.\n\n#### input\\_tokens `class-attribute` `instance-attribute`\n\n```\ninput_tokens: int = 0\n```\n\nTotal number of input/prompt tokens.\n\n#### cache\\_write\\_tokens `class-attribute` `instance-attribute`\n\n```\ncache_write_tokens: int = 0\n```\n\nTotal number of tokens written to the cache.\n\n#### cache\\_read\\_tokens `class-attribute` `instance-attribute`\n\n```\ncache_read_tokens: int = 0\n```\n\nTotal number of tokens read from the cache.\n\n#### input\\_audio\\_tokens `class-attribute` `instance-attribute`\n\n```\ninput_audio_tokens: int = 0\n```\n\nTotal number of audio input tokens.\n\n#### cache\\_audio\\_read\\_tokens `class-attribute` `instance-attribute`\n\n```\ncache_audio_read_tokens: int = 0\n```\n\nTotal number of audio tokens read from the cache.\n\n#### output\\_tokens `class-attribute` `instance-attribute`\n\n```\noutput_tokens: int = 0\n```\n\nTotal number of output/completion tokens.\n\n#### details `class-attribute` `instance-attribute`\n\n```\ndetails: dict[str, int] = field(default_factory=dict)\n```\n\nAny extra details returned by the model.\n\n#### incr\n\n```\nincr(incr_usage: RunUsage | RequestUsage) -> None\n```\n\nIncrement the usage in place.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `incr_usage` | `RunUsage | RequestUsage` | The usage to increment by. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\n\n|  |  |\n| --- | --- |\n| ``` 203 204 205 206 207 208 209 210 211 212 ``` | ``` def incr(self, incr_usage: RunUsage | RequestUsage) -> None:     \"\"\"Increment the usage in place.      Args:         incr_usage: The usage to increment by.     \"\"\"     if isinstance(incr_usage, RunUsage):         self.requests += incr_usage.requests         self.tool_calls += incr_usage.tool_calls     return _incr_usage_tokens(self, incr_usage) ``` |\n\n#### \\_\\_add\\_\\_\n\n```\n__add__(other: RunUsage | RequestUsage) -> RunUsage\n```\n\nAdd two RunUsages together.\n\nThis is provided so it's trivial to sum usage information from multiple runs.\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`", "url": "https://ai.pydantic.dev/usage/index.html#runusage-dataclass", "page": "usage/index.html", "source_site": "pydantic_ai"}
{"title": "RunUsage `dataclass`", "anchor": "runusage-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 214 215 216 217 218 219 220 221 ``` | ``` def __add__(self, other: RunUsage | RequestUsage) -> RunUsage:     \"\"\"Add two RunUsages together.      This is provided so it's trivial to sum usage information from multiple runs.     \"\"\"     new_usage = copy(self)     new_usage.incr(other)     return new_usage ``` |", "url": "https://ai.pydantic.dev/usage/index.html#runusage-dataclass", "page": "usage/index.html", "source_site": "pydantic_ai"}
{"title": "Usage `dataclass` `deprecated`", "anchor": "usage-dataclass-deprecated", "md_text": "Bases: `RunUsage`\n\nDeprecated\n\n`Usage` is deprecated, use `RunUsage` instead\n\nDeprecated alias for `RunUsage`.\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\n\n|  |  |\n| --- | --- |\n| ``` 242 243 244 245 ``` | ``` @dataclass(repr=False, kw_only=True) @deprecated('`Usage` is deprecated, use `RunUsage` instead') class Usage(RunUsage):     \"\"\"Deprecated alias for `RunUsage`.\"\"\" ``` |", "url": "https://ai.pydantic.dev/usage/index.html#usage-dataclass-deprecated", "page": "usage/index.html", "source_site": "pydantic_ai"}
{"title": "UsageLimits `dataclass`", "anchor": "usagelimits-dataclass", "md_text": "Limits on model usage.\n\nThe request count is tracked by pydantic\\_ai, and the request limit is checked before each request to the model.\nToken counts are provided in responses from the model, and the token limits are checked after each response.\n\nEach of the limits can be set to `None` to disable that limit.\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`", "url": "https://ai.pydantic.dev/usage/index.html#usagelimits-dataclass", "page": "usage/index.html", "source_site": "pydantic_ai"}
{"title": "UsageLimits `dataclass`", "anchor": "usagelimits-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 ``` | ``` @dataclass(repr=False, kw_only=True) class UsageLimits:     \"\"\"Limits on model usage.      The request count is tracked by pydantic_ai, and the request limit is checked before each request to the model.     Token counts are provided in responses from the model, and the token limits are checked after each response.      Each of the limits can be set to `None` to disable that limit.     \"\"\"      request_limit: int | None = 50     \"\"\"The maximum number of requests allowed to the model.\"\"\"     tool_calls_limit: int | None = None     \"\"\"The maximum number of successful tool calls allowed to be executed.\"\"\"     input_tokens_limit: int | None = None     \"\"\"The maximum number of input/prompt tokens allowed.\"\"\"     output_tokens_limit: int | None = None     \"\"\"The maximum number of output/response tokens allowed.\"\"\"     total_tokens_limit: int | None = None     \"\"\"The maximum number of tokens allowed in requests and responses combined.\"\"\"     count_tokens_before_request: bool = False     \"\"\"If True, perform a token counting pass before sending the request to the model,     to enforce `request_tokens_limit` ahead of time. This may incur additional overhead     (from calling the model's `count_tokens` API before making the actual request) and is disabled by default.\"\"\"      @property     @deprecated('`request_tokens_limit` is deprecated, use `input_tokens_limit` instead')     def request_tokens_limit(self) -> int | None:         return self.input_tokens_limit      @property     @deprecated('`response_tokens_limit` is deprecated, use `output_tokens_limit` instead')     def response_tokens_limit(self) -> int | None:         return self.output_tokens_limit      @overload     def __init__(         self,         *,         request_limit: int | None = 50,         tool_calls_limit: int | None = None,         input_tokens_limit: int | None = None,         output_tokens_limit: int | None = None,         total_tokens_limit: int | None = None,         count_tokens_before_request: bool = False,     ) -> None:         self.request_limit = request_limit         self.tool_calls_limit = tool_calls_limit         self.input_tokens_limit = input_tokens_limit         self.output_tokens_limit = output_tokens_limit         self.total_tokens_limit = total_tokens_limit         self.count_tokens_before_request = count_tokens_before_request      @overload     @deprecated(         'Use `input_tokens_limit` instead of `request_tokens_limit` and `output_tokens_limit` and `total_tokens_limit`'     )     def __init__(         self,         *,         request_limit: int | None = 50,         tool_calls_limit: int | None = None,         request_tokens_limit: int | None = None,         response_tokens_limit: int | None = None,         total_tokens_limit: int | None = None,         count_tokens_before_request: bool = False,     ) -> None:         self.request_limit = request_limit         self.tool_calls_limit = tool_calls_limit         self.input_tokens_limit = request_tokens_limit         self.output_tokens_limit = response_tokens_limit         self.total_tokens_limit = total_tokens_limit         self.count_tokens_before_request = count_tokens_before_request      def __init__(         self,         *,         request_limit: int | None = 50,         tool_calls_limit: int | None = None,         input_tokens_limit: int | None = None,         output_tokens_limit: int | None = None,         total_tokens_limit: int | None = None,         count_tokens_before_request: bool = False,         # deprecated:         request_tokens_limit: int | None = None,         response_tokens_limit: int | None = None,     ):         self.request_limit = request_limit         self.tool_calls_limit = tool_calls_limit         self.input_tokens_limit = input_tokens_limit or request_tokens_limit         self.output_tokens_limit = output_tokens_limit or response_tokens_limit         self.total_tokens_limit = total_tokens_limit         self.count_tokens_before_request = count_tokens_before_request      def has_token_limits(self) -> bool:         \"\"\"Returns `True` if this instance places any limits on token counts.          If this returns `False`, the `check_tokens` method will never raise an error.          This is useful because if we have token limits, we need to check them after receiving each streamed message.         If there are no limits, we can skip that processing in the streaming response iterator.         \"\"\"         return any(             limit is not None for limit in (self.input_tokens_limit, self.output_tokens_limit, self.total_tokens_limit)         )      def check_before_request(self, usage: RunUsage) -> None:         \"\"\"Raises a `UsageLimitExceeded` exception if the next request would exceed any of the limits.\"\"\"         request_limit = self.request_limit         if request_limit is not None and usage.requests >= request_limit:             raise UsageLimitExceeded(f'The next request would exceed the request_limit of {request_limit}')          input_tokens = usage.input_tokens         if self.input_tokens_limit is not None and input_tokens > self.input_tokens_limit:             raise UsageLimitExceeded(                 f'The next request would exceed the input_tokens_limit of {self.input_tokens_limit} ({input_tokens=})'             )          total_tokens = usage.total_tokens         if self.total_tokens_limit is not None and total_tokens > self.total_tokens_limit:             raise UsageLimitExceeded(  # pragma: lax no cover                 f'The next request would exceed the total_tokens_limit of {self.total_tokens_limit} ({total_tokens=})'             )      def check_tokens(self, usage: RunUsage) -> None:         \"\"\"Raises a `UsageLimitExceeded` exception if the usage exceeds any of the token limits.\"\"\"         input_tokens = usage.input_tokens         if self.input_tokens_limit is not None and input_tokens > self.input_tokens_limit:             raise UsageLimitExceeded(f'Exceeded the input_tokens_limit of {self.input_tokens_limit} ({input_tokens=})')          output_tokens = usage.output_tokens         if self.output_tokens_limit is not None and output_tokens > self.output_tokens_limit:             raise UsageLimitExceeded(                 f'Exceeded the output_tokens_limit of {self.output_tokens_limit} ({output_tokens=})'             )          total_tokens = usage.total_tokens         if self.total_tokens_limit is not None and total_tokens > self.total_tokens_limit:             raise UsageLimitExceeded(f'Exceeded the total_tokens_limit of {self.total_tokens_limit} ({total_tokens=})')      def check_before_tool_call(self, projected_usage: RunUsage) -> None:         \"\"\"Raises a `UsageLimitExceeded` exception if the next tool call(s) would exceed the tool call limit.\"\"\"         tool_calls_limit = self.tool_calls_limit         tool_calls = projected_usage.tool_calls         if tool_calls_limit is not None and tool_calls > tool_calls_limit:             raise UsageLimitExceeded(                 f'The next tool call(s) would exceed the tool_calls_limit of {tool_calls_limit} ({tool_calls=}).'             )      __repr__ = _utils.dataclasses_no_defaults_repr ``` |", "url": "https://ai.pydantic.dev/usage/index.html#usagelimits-dataclass", "page": "usage/index.html", "source_site": "pydantic_ai"}
{"title": "UsageLimits `dataclass`", "anchor": "usagelimits-dataclass", "md_text": "#### request\\_limit `class-attribute` `instance-attribute`\n\n```\nrequest_limit: int | None = request_limit\n```\n\nThe maximum number of requests allowed to the model.\n\n#### tool\\_calls\\_limit `class-attribute` `instance-attribute`\n\n```\ntool_calls_limit: int | None = tool_calls_limit\n```\n\nThe maximum number of successful tool calls allowed to be executed.\n\n#### input\\_tokens\\_limit `class-attribute` `instance-attribute`\n\n```\ninput_tokens_limit: int | None = (\n    input_tokens_limit or request_tokens_limit\n)\n```\n\nThe maximum number of input/prompt tokens allowed.\n\n#### output\\_tokens\\_limit `class-attribute` `instance-attribute`\n\n```\noutput_tokens_limit: int | None = (\n    output_tokens_limit or response_tokens_limit\n)\n```\n\nThe maximum number of output/response tokens allowed.\n\n#### total\\_tokens\\_limit `class-attribute` `instance-attribute`\n\n```\ntotal_tokens_limit: int | None = total_tokens_limit\n```\n\nThe maximum number of tokens allowed in requests and responses combined.\n\n#### count\\_tokens\\_before\\_request `class-attribute` `instance-attribute`\n\n```\ncount_tokens_before_request: bool = (\n    count_tokens_before_request\n)\n```\n\nIf True, perform a token counting pass before sending the request to the model,\nto enforce `request_tokens_limit` ahead of time. This may incur additional overhead\n(from calling the model's `count_tokens` API before making the actual request) and is disabled by default.\n\n#### has\\_token\\_limits\n\n```\nhas_token_limits() -> bool\n```\n\nReturns `True` if this instance places any limits on token counts.\n\nIf this returns `False`, the `check_tokens` method will never raise an error.\n\nThis is useful because if we have token limits, we need to check them after receiving each streamed message.\nIf there are no limits, we can skip that processing in the streaming response iterator.\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\n\n|  |  |\n| --- | --- |\n| ``` 342 343 344 345 346 347 348 349 350 351 352 ``` | ``` def has_token_limits(self) -> bool:     \"\"\"Returns `True` if this instance places any limits on token counts.      If this returns `False`, the `check_tokens` method will never raise an error.      This is useful because if we have token limits, we need to check them after receiving each streamed message.     If there are no limits, we can skip that processing in the streaming response iterator.     \"\"\"     return any(         limit is not None for limit in (self.input_tokens_limit, self.output_tokens_limit, self.total_tokens_limit)     ) ``` |\n\n#### check\\_before\\_request\n\n```\ncheck_before_request(usage: RunUsage) -> None\n```\n\nRaises a `UsageLimitExceeded` exception if the next request would exceed any of the limits.\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\n\n|  |  |\n| --- | --- |\n| ``` 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 ``` | ``` def check_before_request(self, usage: RunUsage) -> None:     \"\"\"Raises a `UsageLimitExceeded` exception if the next request would exceed any of the limits.\"\"\"     request_limit = self.request_limit     if request_limit is not None and usage.requests >= request_limit:         raise UsageLimitExceeded(f'The next request would exceed the request_limit of {request_limit}')      input_tokens = usage.input_tokens     if self.input_tokens_limit is not None and input_tokens > self.input_tokens_limit:         raise UsageLimitExceeded(             f'The next request would exceed the input_tokens_limit of {self.input_tokens_limit} ({input_tokens=})'         )      total_tokens = usage.total_tokens     if self.total_tokens_limit is not None and total_tokens > self.total_tokens_limit:         raise UsageLimitExceeded(  # pragma: lax no cover             f'The next request would exceed the total_tokens_limit of {self.total_tokens_limit} ({total_tokens=})'         ) ``` |\n\n#### check\\_tokens\n\n```\ncheck_tokens(usage: RunUsage) -> None\n```\n\nRaises a `UsageLimitExceeded` exception if the usage exceeds any of the token limits.\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\n\n|  |  |\n| --- | --- |\n| ``` 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 ``` | ``` def check_tokens(self, usage: RunUsage) -> None:     \"\"\"Raises a `UsageLimitExceeded` exception if the usage exceeds any of the token limits.\"\"\"     input_tokens = usage.input_tokens     if self.input_tokens_limit is not None and input_tokens > self.input_tokens_limit:         raise UsageLimitExceeded(f'Exceeded the input_tokens_limit of {self.input_tokens_limit} ({input_tokens=})')      output_tokens = usage.output_tokens     if self.output_tokens_limit is not None and output_tokens > self.output_tokens_limit:         raise UsageLimitExceeded(             f'Exceeded the output_tokens_limit of {self.output_tokens_limit} ({output_tokens=})'         )      total_tokens = usage.total_tokens     if self.total_tokens_limit is not None and total_tokens > self.total_tokens_limit:         raise UsageLimitExceeded(f'Exceeded the total_tokens_limit of {self.total_tokens_limit} ({total_tokens=})') ``` |\n\n#### check\\_before\\_tool\\_call", "url": "https://ai.pydantic.dev/usage/index.html#usagelimits-dataclass", "page": "usage/index.html", "source_site": "pydantic_ai"}
{"title": "UsageLimits `dataclass`", "anchor": "usagelimits-dataclass", "md_text": "```\ncheck_before_tool_call(projected_usage: RunUsage) -> None\n```\n\nRaises a `UsageLimitExceeded` exception if the next tool call(s) would exceed the tool call limit.\n\nSource code in `pydantic_ai_slim/pydantic_ai/usage.py`\n\n|  |  |\n| --- | --- |\n| ``` 388 389 390 391 392 393 394 395 ``` | ``` def check_before_tool_call(self, projected_usage: RunUsage) -> None:     \"\"\"Raises a `UsageLimitExceeded` exception if the next tool call(s) would exceed the tool call limit.\"\"\"     tool_calls_limit = self.tool_calls_limit     tool_calls = projected_usage.tool_calls     if tool_calls_limit is not None and tool_calls > tool_calls_limit:         raise UsageLimitExceeded(             f'The next tool call(s) would exceed the tool_calls_limit of {tool_calls_limit} ({tool_calls=}).'         ) ``` |", "url": "https://ai.pydantic.dev/usage/index.html#usagelimits-dataclass", "page": "usage/index.html", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "md_text": "These examples are distributed with `pydantic-ai` so you can run them either by cloning the [pydantic-ai repo](https://github.com/pydantic/pydantic-ai) or by simply installing `pydantic-ai` from PyPI with `pip` or `uv`.", "url": "https://ai.pydantic.dev/setup/index.html#usage", "page": "setup/index.html", "source_site": "pydantic_ai"}
{"title": "Installing required dependencies", "anchor": "installing-required-dependencies", "md_text": "Either way you'll need to install extra dependencies to run some examples, you just need to install the `examples` optional dependency group.\n\nIf you've installed `pydantic-ai` via pip/uv, you can install the extra dependencies with:\n\npipuv\n\n```\npip install \"pydantic-ai[examples]\"\n\nuv add \"pydantic-ai[examples]\"\n```\n\nIf you clone the repo, you should instead use `uv sync --extra examples` to install extra dependencies.", "url": "https://ai.pydantic.dev/setup/index.html#installing-required-dependencies", "page": "setup/index.html", "source_site": "pydantic_ai"}
{"title": "Setting model environment variables", "anchor": "setting-model-environment-variables", "md_text": "These examples will need you to set up authentication with one or more of the LLMs, see the [model configuration](https://ai.pydantic.dev/models/overview/) docs for details on how to do this.\n\nTL;DR: in most cases you'll need to set one of the following environment variables:\n\nOpenAIGoogle Gemini\n\n```\nexport OPENAI_API_KEY=your-api-key\n\nexport GEMINI_API_KEY=your-api-key\n```", "url": "https://ai.pydantic.dev/setup/index.html#setting-model-environment-variables", "page": "setup/index.html", "source_site": "pydantic_ai"}
{"title": "Running Examples", "anchor": "running-examples", "md_text": "To run the examples (this will work whether you installed `pydantic_ai`, or cloned the repo), run:\n\npipuv\n\n```\npython -m pydantic_ai_examples.<example_module_name>\n\nuv run -m pydantic_ai_examples.<example_module_name>\n```\n\nFor examples, to run the very simple [`pydantic_model`](../pydantic-model/index.html) example:\n\npipuv\n\n```\npython -m pydantic_ai_examples.pydantic_model\n\nuv run -m pydantic_ai_examples.pydantic_model\n```\n\nIf you like one-liners and you're using uv, you can run a pydantic-ai example with zero setup:\n\n```\nOPENAI_API_KEY='your-api-key' \\\n  uv run --with \"pydantic-ai[examples]\" \\\n  -m pydantic_ai_examples.pydantic_model\n```\n\n---\n\nYou'll probably want to edit examples in addition to just running them. You can copy the examples to a new directory with:\n\npipuv\n\n```\npython -m pydantic_ai_examples --copy-to examples/\n\nuv run -m pydantic_ai_examples --copy-to examples/\n```", "url": "https://ai.pydantic.dev/setup/index.html#running-examples", "page": "setup/index.html", "source_site": "pydantic_ai"}
{"title": "duckduckgo\\_search\\_tool", "anchor": "duckduckgosearchtool", "md_text": "```\nduckduckgo_search_tool(\n    duckduckgo_client: DDGS | None = None,\n    max_results: int | None = None,\n)\n```\n\nCreates a DuckDuckGo search tool.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `duckduckgo_client` | `DDGS | None` | The DuckDuckGo search client. | `None` |\n| `max_results` | `int | None` | The maximum number of results. If None, returns results only from the first response. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/common_tools/duckduckgo.py`\n\n|  |  |\n| --- | --- |\n| ``` 65 66 67 68 69 70 71 72 73 74 75 76 ``` | ``` def duckduckgo_search_tool(duckduckgo_client: DDGS | None = None, max_results: int | None = None):     \"\"\"Creates a DuckDuckGo search tool.      Args:         duckduckgo_client: The DuckDuckGo search client.         max_results: The maximum number of results. If None, returns results only from the first response.     \"\"\"     return Tool[Any](         DuckDuckGoSearchTool(client=duckduckgo_client or DDGS(), max_results=max_results).__call__,         name='duckduckgo_search',         description='Searches DuckDuckGo for the given query and returns the results.',     ) ``` |", "url": "https://ai.pydantic.dev/common_tools/index.html#duckduckgosearchtool", "page": "common_tools/index.html", "source_site": "pydantic_ai"}
{"title": "tavily\\_search\\_tool", "anchor": "tavilysearchtool", "md_text": "```\ntavily_search_tool(api_key: str)\n```\n\nCreates a Tavily search tool.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `api_key` | `str` | The Tavily API key.  You can get one by signing up at <https://app.tavily.com/home>. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/common_tools/tavily.py`\n\n|  |  |\n| --- | --- |\n| ``` 69 70 71 72 73 74 75 76 77 78 79 80 81 ``` | ``` def tavily_search_tool(api_key: str):     \"\"\"Creates a Tavily search tool.      Args:         api_key: The Tavily API key.              You can get one by signing up at [https://app.tavily.com/home](https://app.tavily.com/home).     \"\"\"     return Tool[Any](         TavilySearchTool(client=AsyncTavilyClient(api_key)).__call__,         name='tavily_search',         description='Searches Tavily for the given query and returns the results.',     ) ``` |", "url": "https://ai.pydantic.dev/common_tools/index.html#tavilysearchtool", "page": "common_tools/index.html", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "md_text": "With [dependencies installed and environment variables set](../setup/index.html#usage), run:\n\npipuv\n\n```\npython -m pydantic_ai_examples.flight_booking\n\nuv run -m pydantic_ai_examples.flight_booking\n```", "url": "https://ai.pydantic.dev/flight-booking/index.html#running-the-example", "page": "flight-booking/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "md_text": "[flight\\_booking.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/flight_booking.py)\n\n```\n\"\"\"Example of a multi-agent flow where one agent delegates work to another.\n\nIn this scenario, a group of agents work together to find flights for a user.\n\"\"\"\n\nimport datetime\nfrom dataclasses import dataclass\nfrom typing import Literal\n\nimport logfire\nfrom pydantic import BaseModel, Field\nfrom rich.prompt import Prompt\n\nfrom pydantic_ai import (\n    Agent,\n    ModelMessage,\n    ModelRetry,\n    RunContext,\n    RunUsage,\n    UsageLimits,\n)\n\n# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\nlogfire.configure(send_to_logfire='if-token-present')\nlogfire.instrument_pydantic_ai()\n\n\nclass FlightDetails(BaseModel):\n    \"\"\"Details of the most suitable flight.\"\"\"\n\n    flight_number: str\n    price: int\n    origin: str = Field(description='Three-letter airport code')\n    destination: str = Field(description='Three-letter airport code')\n    date: datetime.date\n\n\nclass NoFlightFound(BaseModel):\n    \"\"\"When no valid flight is found.\"\"\"\n\n\n@dataclass\nclass Deps:\n    web_page_text: str\n    req_origin: str\n    req_destination: str\n    req_date: datetime.date\n\n\n# This agent is responsible for controlling the flow of the conversation.\nsearch_agent = Agent[Deps, FlightDetails | NoFlightFound](\n    'openai:gpt-4o',\n    output_type=FlightDetails | NoFlightFound,  # type: ignore\n    retries=4,\n    system_prompt=(\n        'Your job is to find the cheapest flight for the user on the given date. '\n    ),\n)\n\n\n# This agent is responsible for extracting flight details from web page text.\nextraction_agent = Agent(\n    'openai:gpt-4o',\n    output_type=list[FlightDetails],\n    system_prompt='Extract all the flight details from the given text.',\n)\n\n\n@search_agent.tool\nasync def extract_flights(ctx: RunContext[Deps]) -> list[FlightDetails]:\n    \"\"\"Get details of all flights.\"\"\"\n    # we pass the usage to the search agent so requests within this agent are counted\n    result = await extraction_agent.run(ctx.deps.web_page_text, usage=ctx.usage)\n    logfire.info('found {flight_count} flights', flight_count=len(result.output))\n    return result.output\n\n\n@search_agent.output_validator\nasync def validate_output(\n    ctx: RunContext[Deps], output: FlightDetails | NoFlightFound\n) -> FlightDetails | NoFlightFound:\n    \"\"\"Procedural validation that the flight meets the constraints.\"\"\"\n    if isinstance(output, NoFlightFound):\n        return output\n\n    errors: list[str] = []\n    if output.origin != ctx.deps.req_origin:\n        errors.append(\n            f'Flight should have origin {ctx.deps.req_origin}, not {output.origin}'\n        )\n    if output.destination != ctx.deps.req_destination:\n        errors.append(\n            f'Flight should have destination {ctx.deps.req_destination}, not {output.destination}'\n        )\n    if output.date != ctx.deps.req_date:\n        errors.append(f'Flight should be on {ctx.deps.req_date}, not {output.date}')\n\n    if errors:\n        raise ModelRetry('\\n'.join(errors))\n    else:\n        return output\n\n\nclass SeatPreference(BaseModel):\n    row: int = Field(ge=1, le=30)\n    seat: Literal['A', 'B', 'C', 'D', 'E', 'F']\n\n\nclass Failed(BaseModel):\n    \"\"\"Unable to extract a seat selection.\"\"\"\n\n\n# This agent is responsible for extracting the user's seat selection\nseat_preference_agent = Agent[None, SeatPreference | Failed](\n    'openai:gpt-4o',\n    output_type=SeatPreference | Failed,\n    system_prompt=(\n        \"Extract the user's seat preference. \"\n        'Seats A and F are window seats. '\n        'Row 1 is the front row and has extra leg room. '\n        'Rows 14, and 20 also have extra leg room. '\n    ),\n)\n\n\n# in reality this would be downloaded from a booking site,\n# potentially using another agent to navigate the site\nflights_web_page = \"\"\"\n1. Flight SFO-AK123\n- Price: $350\n- Origin: San Francisco International Airport (SFO)\n- Destination: Ted Stevens Anchorage International Airport (ANC)\n- Date: January 10, 2025\n\n2. Flight SFO-AK456\n- Price: $370\n- Origin: San Francisco International Airport (SFO)\n- Destination: Fairbanks International Airport (FAI)\n- Date: January 10, 2025\n\n3. Flight SFO-AK789\n- Price: $400\n- Origin: San Francisco International Airport (SFO)\n- Destination: Juneau International Airport (JNU)\n- Date: January 20, 2025\n\n4. Flight NYC-LA101\n- Price: $250\n- Origin: San Francisco International Airport (SFO)\n- Destination: Ted Stevens Anchorage International Airport (ANC)\n- Date: January 10, 2025\n\n5. Flight CHI-MIA202\n- Price: $200\n- Origin: Chicago O'Hare International Airport (ORD)\n- Destination: Miami International Airport (MIA)\n- Date: January 12, 2025\n\n6. Flight BOS-SEA303\n- Price: $120\n- Origin: Boston Logan International Airport (BOS)\n- Destination: Ted Stevens Anchorage International Airport (ANC)\n- Date: January 12, 2025", "url": "https://ai.pydantic.dev/flight-booking/index.html#example-code", "page": "flight-booking/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "md_text": "7. Flight DFW-DEN404\n- Price: $150\n- Origin: Dallas/Fort Worth International Airport (DFW)\n- Destination: Denver International Airport (DEN)\n- Date: January 10, 2025\n\n8. Flight ATL-HOU505\n- Price: $180\n- Origin: Hartsfield-Jackson Atlanta International Airport (ATL)\n- Destination: George Bush Intercontinental Airport (IAH)\n- Date: January 10, 2025\n\"\"\"\n\n# restrict how many requests this app can make to the LLM\nusage_limits = UsageLimits(request_limit=15)\n\n\nasync def main():\n    deps = Deps(\n        web_page_text=flights_web_page,\n        req_origin='SFO',\n        req_destination='ANC',\n        req_date=datetime.date(2025, 1, 10),\n    )\n    message_history: list[ModelMessage] | None = None\n    usage: RunUsage = RunUsage()\n    # run the agent until a satisfactory flight is found\n    while True:\n        result = await search_agent.run(\n            f'Find me a flight from {deps.req_origin} to {deps.req_destination} on {deps.req_date}',\n            deps=deps,\n            usage=usage,\n            message_history=message_history,\n            usage_limits=usage_limits,\n        )\n        if isinstance(result.output, NoFlightFound):\n            print('No flight found')\n            break\n        else:\n            flight = result.output\n            print(f'Flight found: {flight}')\n            answer = Prompt.ask(\n                'Do you want to buy this flight, or keep searching? (buy/*search)',\n                choices=['buy', 'search', ''],\n                show_choices=False,\n            )\n            if answer == 'buy':\n                seat = await find_seat(usage)\n                await buy_tickets(flight, seat)\n                break\n            else:\n                message_history = result.all_messages(\n                    output_tool_return_content='Please suggest another flight'\n                )\n\n\nasync def find_seat(usage: RunUsage) -> SeatPreference:\n    message_history: list[ModelMessage] | None = None\n    while True:\n        answer = Prompt.ask('What seat would you like?')\n\n        result = await seat_preference_agent.run(\n            answer,\n            message_history=message_history,\n            usage=usage,\n            usage_limits=usage_limits,\n        )\n        if isinstance(result.output, SeatPreference):\n            return result.output\n        else:\n            print('Could not understand seat preference. Please try again.')\n            message_history = result.all_messages()\n\n\nasync def buy_tickets(flight_details: FlightDetails, seat: SeatPreference):\n    print(f'Purchasing flight {flight_details=!r} {seat=!r}...')\n\n\nif __name__ == '__main__':\n    import asyncio\n\n    asyncio.run(main())\n```", "url": "https://ai.pydantic.dev/flight-booking/index.html#example-code", "page": "flight-booking/index.html", "source_site": "pydantic_ai"}
{"title": "Prerequisites", "anchor": "prerequisites", "md_text": "* An [OpenAI API key](https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key)", "url": "https://ai.pydantic.dev/ag-ui/index.html#prerequisites", "page": "ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "md_text": "With [dependencies installed and environment variables set](../setup/index.html#usage)\nyou will need two command line windows.", "url": "https://ai.pydantic.dev/ag-ui/index.html#running-the-example", "page": "ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Pydantic AI AG-UI backend", "anchor": "pydantic-ai-ag-ui-backend", "md_text": "Setup your OpenAI API Key\n\n```\nexport OPENAI_API_KEY=<your api key>\n```\n\nStart the Pydantic AI AG-UI example backend.\n\npipuv\n\n```\npython -m pydantic_ai_examples.ag_ui\n\nuv run -m pydantic_ai_examples.ag_ui\n```", "url": "https://ai.pydantic.dev/ag-ui/index.html#pydantic-ai-ag-ui-backend", "page": "ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "AG-UI Dojo example frontend", "anchor": "ag-ui-dojo-example-frontend", "md_text": "Next run the AG-UI Dojo example frontend.\n\n1. Clone the [AG-UI repository](https://github.com/ag-ui-protocol/ag-ui)\n\n   ```\n   git clone https://github.com/ag-ui-protocol/ag-ui.git\n   ```\n2. Change into to the `ag-ui/typescript-sdk` directory\n\n   ```\n   cd ag-ui/typescript-sdk\n   ```\n3. Run the Dojo app following the [official instructions](https://github.com/ag-ui-protocol/ag-ui/tree/main/typescript-sdk/apps/dojo#development-setup)\n4. Visit <http://localhost:3000/pydantic-ai>\n5. Select View `Pydantic AI` from the sidebar", "url": "https://ai.pydantic.dev/ag-ui/index.html#ag-ui-dojo-example-frontend", "page": "ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Feature Examples", "anchor": "feature-examples", "md_text": "", "url": "https://ai.pydantic.dev/ag-ui/index.html#feature-examples", "page": "ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Agentic Chat", "anchor": "agentic-chat", "md_text": "This demonstrates a basic agent interaction including Pydantic AI server side\ntools and AG-UI client side tools.\n\nIf you've [run the example](index.html#running-the-example), you can view it at <http://localhost:3000/pydantic-ai/feature/agentic_chat>.\n\n#### Agent Tools\n\n* `time` - Pydantic AI tool to check the current time for a time zone\n* `background` - AG-UI tool to set the background color of the client window\n\n#### Agent Prompts\n\n```\nWhat is the time in New York?\n\nChange the background to blue\n```\n\nA complex example which mixes both AG-UI and Pydantic AI tools:\n\n```\nPerform the following steps, waiting for the response of each step before continuing:\n1. Get the time\n2. Set the background to red\n3. Get the time\n4. Report how long the background set took by diffing the two times\n```\n\n#### Agentic Chat - Code\n\n[ag\\_ui/api/agentic\\_chat.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/agentic_chat.py)\n\n```\n\"\"\"Agentic Chat feature.\"\"\"\n\nfrom __future__ import annotations\n\nfrom datetime import datetime\nfrom zoneinfo import ZoneInfo\n\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o-mini')\napp = agent.to_ag_ui()\n\n\n@agent.tool_plain\nasync def current_time(timezone: str = 'UTC') -> str:\n    \"\"\"Get the current time in ISO format.\n\n    Args:\n        timezone: The timezone to use.\n\n    Returns:\n        The current time in ISO format string.\n    \"\"\"\n    tz: ZoneInfo = ZoneInfo(timezone)\n    return datetime.now(tz=tz).isoformat()\n```", "url": "https://ai.pydantic.dev/ag-ui/index.html#agentic-chat", "page": "ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Agentic Generative UI", "anchor": "agentic-generative-ui", "md_text": "Demonstrates a long running task where the agent sends updates to the frontend\nto let the user know what's happening.\n\nIf you've [run the example](index.html#running-the-example), you can view it at <http://localhost:3000/pydantic-ai/feature/agentic_generative_ui>.\n\n#### Plan Prompts\n\n```\nCreate a plan for breakfast and execute it\n```\n\n#### Agentic Generative UI - Code\n\n[ag\\_ui/api/agentic\\_generative\\_ui.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py)\n\n```\n\"\"\"Agentic Generative UI feature.\"\"\"\n\nfrom __future__ import annotations\n\nfrom textwrap import dedent\nfrom typing import Any, Literal\n\nfrom pydantic import BaseModel, Field\n\nfrom ag_ui.core import EventType, StateDeltaEvent, StateSnapshotEvent\nfrom pydantic_ai import Agent\n\nStepStatus = Literal['pending', 'completed']\n\n\nclass Step(BaseModel):\n    \"\"\"Represents a step in a plan.\"\"\"\n\n    description: str = Field(description='The description of the step')\n    status: StepStatus = Field(\n        default='pending',\n        description='The status of the step (e.g., pending, completed)',\n    )\n\n\nclass Plan(BaseModel):\n    \"\"\"Represents a plan with multiple steps.\"\"\"\n\n    steps: list[Step] = Field(default_factory=list, description='The steps in the plan')\n\n\nclass JSONPatchOp(BaseModel):\n    \"\"\"A class representing a JSON Patch operation (RFC 6902).\"\"\"\n\n    op: Literal['add', 'remove', 'replace', 'move', 'copy', 'test'] = Field(\n        description='The operation to perform: add, remove, replace, move, copy, or test',\n    )\n    path: str = Field(description='JSON Pointer (RFC 6901) to the target location')\n    value: Any = Field(\n        default=None,\n        description='The value to apply (for add, replace operations)',\n    )\n    from_: str | None = Field(\n        default=None,\n        alias='from',\n        description='Source path (for move, copy operations)',\n    )\n\n\nagent = Agent(\n    'openai:gpt-4o-mini',\n    instructions=dedent(\n        \"\"\"\n        When planning use tools only, without any other messages.\n        IMPORTANT:\n        - Use the `create_plan` tool to set the initial state of the steps\n        - Use the `update_plan_step` tool to update the status of each step\n        - Do NOT repeat the plan or summarise it in a message\n        - Do NOT confirm the creation or updates in a message\n        - Do NOT ask the user for additional information or next steps\n\n        Only one plan can be active at a time, so do not call the `create_plan` tool\n        again until all the steps in current plan are completed.\n        \"\"\"\n    ),\n)\n\n\n@agent.tool_plain\nasync def create_plan(steps: list[str]) -> StateSnapshotEvent:\n    \"\"\"Create a plan with multiple steps.\n\n    Args:\n        steps: List of step descriptions to create the plan.\n\n    Returns:\n        StateSnapshotEvent containing the initial state of the steps.\n    \"\"\"\n    plan: Plan = Plan(\n        steps=[Step(description=step) for step in steps],\n    )\n    return StateSnapshotEvent(\n        type=EventType.STATE_SNAPSHOT,\n        snapshot=plan.model_dump(),\n    )\n\n\n@agent.tool_plain\nasync def update_plan_step(\n    index: int, description: str | None = None, status: StepStatus | None = None\n) -> StateDeltaEvent:\n    \"\"\"Update the plan with new steps or changes.\n\n    Args:\n        index: The index of the step to update.\n        description: The new description for the step.\n        status: The new status for the step.\n\n    Returns:\n        StateDeltaEvent containing the changes made to the plan.\n    \"\"\"\n    changes: list[JSONPatchOp] = []\n    if description is not None:\n        changes.append(\n            JSONPatchOp(\n                op='replace', path=f'/steps/{index}/description', value=description\n            )\n        )\n    if status is not None:\n        changes.append(\n            JSONPatchOp(op='replace', path=f'/steps/{index}/status', value=status)\n        )\n    return StateDeltaEvent(\n        type=EventType.STATE_DELTA,\n        delta=changes,\n    )\n\n\napp = agent.to_ag_ui()\n```", "url": "https://ai.pydantic.dev/ag-ui/index.html#agentic-generative-ui", "page": "ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Human in the Loop", "anchor": "human-in-the-loop", "md_text": "Demonstrates simple human in the loop workflow where the agent comes up with a\nplan and the user can approve it using checkboxes.\n\n#### Task Planning Tools\n\n* `generate_task_steps` - AG-UI tool to generate and confirm steps\n\n#### Task Planning Prompt\n\n```\nGenerate a list of steps for cleaning a car for me to review\n```\n\n#### Human in the Loop - Code\n\n[ag\\_ui/api/human\\_in\\_the\\_loop.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/human_in_the_loop.py)\n\n```\n\"\"\"Human in the Loop Feature.\n\nNo special handling is required for this feature.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom textwrap import dedent\n\nfrom pydantic_ai import Agent\n\nagent = Agent(\n    'openai:gpt-4o-mini',\n    instructions=dedent(\n        \"\"\"\n        When planning tasks use tools only, without any other messages.\n        IMPORTANT:\n        - Use the `generate_task_steps` tool to display the suggested steps to the user\n        - Never repeat the plan, or send a message detailing steps\n        - If accepted, confirm the creation of the plan and the number of selected (enabled) steps only\n        - If not accepted, ask the user for more information, DO NOT use the `generate_task_steps` tool again\n        \"\"\"\n    ),\n)\n\napp = agent.to_ag_ui()\n```", "url": "https://ai.pydantic.dev/ag-ui/index.html#human-in-the-loop", "page": "ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Predictive State Updates", "anchor": "predictive-state-updates", "md_text": "Demonstrates how to use the predictive state updates feature to update the state\nof the UI based on agent responses, including user interaction via user\nconfirmation.\n\nIf you've [run the example](index.html#running-the-example), you can view it at <http://localhost:3000/pydantic-ai/feature/predictive_state_updates>.\n\n#### Story Tools\n\n* `write_document` - AG-UI tool to write the document to a window\n* `document_predict_state` - Pydantic AI tool that enables document state\n  prediction for the `write_document` tool\n\nThis also shows how to use custom instructions based on shared state information.\n\n#### Story Example\n\nStarting document text\n\n```\nBruce was a good dog,\n```\n\nAgent prompt\n\n```\nHelp me complete my story about bruce the dog, is should be no longer than a sentence.\n```\n\n#### Predictive State Updates - Code\n\n[ag\\_ui/api/predictive\\_state\\_updates.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/predictive_state_updates.py)\n\n```\n\"\"\"Predictive State feature.\"\"\"\n\nfrom __future__ import annotations\n\nfrom textwrap import dedent\n\nfrom pydantic import BaseModel\n\nfrom ag_ui.core import CustomEvent, EventType\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.ag_ui import StateDeps\n\n\nclass DocumentState(BaseModel):\n    \"\"\"State for the document being written.\"\"\"\n\n    document: str = ''\n\n\nagent = Agent('openai:gpt-4o-mini', deps_type=StateDeps[DocumentState])\n\n\n# Tools which return AG-UI events will be sent to the client as part of the\n# event stream, single events and iterables of events are supported.\n@agent.tool_plain\nasync def document_predict_state() -> list[CustomEvent]:\n    \"\"\"Enable document state prediction.\n\n    Returns:\n        CustomEvent containing the event to enable state prediction.\n    \"\"\"\n    return [\n        CustomEvent(\n            type=EventType.CUSTOM,\n            name='PredictState',\n            value=[\n                {\n                    'state_key': 'document',\n                    'tool': 'write_document',\n                    'tool_argument': 'document',\n                },\n            ],\n        ),\n    ]\n\n\n@agent.instructions()\nasync def story_instructions(ctx: RunContext[StateDeps[DocumentState]]) -> str:\n    \"\"\"Provide instructions for writing document if present.\n\n    Args:\n        ctx: The run context containing document state information.\n\n    Returns:\n        Instructions string for the document writing agent.\n    \"\"\"\n    return dedent(\n        f\"\"\"You are a helpful assistant for writing documents.\n\n        Before you start writing, you MUST call the `document_predict_state`\n        tool to enable state prediction.\n\n        To present the document to the user for review, you MUST use the\n        `write_document` tool.\n\n        When you have written the document, DO NOT repeat it as a message.\n        If accepted briefly summarize the changes you made, 2 sentences\n        max, otherwise ask the user to clarify what they want to change.\n\n        This is the current document:\n\n        {ctx.deps.state.document}\n        \"\"\"\n    )\n\n\napp = agent.to_ag_ui(deps=StateDeps(DocumentState()))\n```", "url": "https://ai.pydantic.dev/ag-ui/index.html#predictive-state-updates", "page": "ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Shared State", "anchor": "shared-state", "md_text": "Demonstrates how to use the shared state between the UI and the agent.\n\nState sent to the agent is detected by a function based instruction. This then\nvalidates the data using a custom pydantic model before using to create the\ninstructions for the agent to follow and send to the client using a AG-UI tool.\n\nIf you've [run the example](index.html#running-the-example), you can view it at <http://localhost:3000/pydantic-ai/feature/shared_state>.\n\n#### Recipe Tools\n\n* `display_recipe` - AG-UI tool to display the recipe in a graphical format\n\n#### Recipe Example\n\n1. Customise the basic settings of your recipe\n2. Click `Improve with AI`\n\n#### Shared State - Code\n\n[ag\\_ui/api/shared\\_state.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/shared_state.py)\n\n```\n\"\"\"Shared State feature.\"\"\"\n\nfrom __future__ import annotations\n\nfrom enum import Enum\nfrom textwrap import dedent\n\nfrom pydantic import BaseModel, Field\n\nfrom ag_ui.core import EventType, StateSnapshotEvent\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_ai.ag_ui import StateDeps\n\n\nclass SkillLevel(str, Enum):\n    \"\"\"The level of skill required for the recipe.\"\"\"\n\n    BEGINNER = 'Beginner'\n    INTERMEDIATE = 'Intermediate'\n    ADVANCED = 'Advanced'\n\n\nclass SpecialPreferences(str, Enum):\n    \"\"\"Special preferences for the recipe.\"\"\"\n\n    HIGH_PROTEIN = 'High Protein'\n    LOW_CARB = 'Low Carb'\n    SPICY = 'Spicy'\n    BUDGET_FRIENDLY = 'Budget-Friendly'\n    ONE_POT_MEAL = 'One-Pot Meal'\n    VEGETARIAN = 'Vegetarian'\n    VEGAN = 'Vegan'\n\n\nclass CookingTime(str, Enum):\n    \"\"\"The cooking time of the recipe.\"\"\"\n\n    FIVE_MIN = '5 min'\n    FIFTEEN_MIN = '15 min'\n    THIRTY_MIN = '30 min'\n    FORTY_FIVE_MIN = '45 min'\n    SIXTY_PLUS_MIN = '60+ min'\n\n\nclass Ingredient(BaseModel):\n    \"\"\"A class representing an ingredient in a recipe.\"\"\"\n\n    icon: str = Field(\n        default='ingredient',\n        description=\"The icon emoji (not emoji code like '\\x1f35e', but the actual emoji like 🥕) of the ingredient\",\n    )\n    name: str\n    amount: str\n\n\nclass Recipe(BaseModel):\n    \"\"\"A class representing a recipe.\"\"\"\n\n    skill_level: SkillLevel = Field(\n        default=SkillLevel.BEGINNER,\n        description='The skill level required for the recipe',\n    )\n    special_preferences: list[SpecialPreferences] = Field(\n        default_factory=list,\n        description='Any special preferences for the recipe',\n    )\n    cooking_time: CookingTime = Field(\n        default=CookingTime.FIVE_MIN, description='The cooking time of the recipe'\n    )\n    ingredients: list[Ingredient] = Field(\n        default_factory=list,\n        description='Ingredients for the recipe',\n    )\n    instructions: list[str] = Field(\n        default_factory=list, description='Instructions for the recipe'\n    )\n\n\nclass RecipeSnapshot(BaseModel):\n    \"\"\"A class representing the state of the recipe.\"\"\"\n\n    recipe: Recipe = Field(\n        default_factory=Recipe, description='The current state of the recipe'\n    )\n\n\nagent = Agent('openai:gpt-4o-mini', deps_type=StateDeps[RecipeSnapshot])\n\n\n@agent.tool_plain\nasync def display_recipe(recipe: Recipe) -> StateSnapshotEvent:\n    \"\"\"Display the recipe to the user.\n\n    Args:\n        recipe: The recipe to display.\n\n    Returns:\n        StateSnapshotEvent containing the recipe snapshot.\n    \"\"\"\n    return StateSnapshotEvent(\n        type=EventType.STATE_SNAPSHOT,\n        snapshot={'recipe': recipe},\n    )\n\n\n@agent.instructions\nasync def recipe_instructions(ctx: RunContext[StateDeps[RecipeSnapshot]]) -> str:\n    \"\"\"Instructions for the recipe generation agent.\n\n    Args:\n        ctx: The run context containing recipe state information.\n\n    Returns:\n        Instructions string for the recipe generation agent.\n    \"\"\"\n    return dedent(\n        f\"\"\"\n        You are a helpful assistant for creating recipes.\n\n        IMPORTANT:\n        - Create a complete recipe using the existing ingredients\n        - Append new ingredients to the existing ones\n        - Use the `display_recipe` tool to present the recipe to the user\n        - Do NOT repeat the recipe in the message, use the tool instead\n        - Do NOT run the `display_recipe` tool multiple times in a row\n\n        Once you have created the updated recipe and displayed it to the user,\n        summarise the changes in one sentence, don't describe the recipe in\n        detail or send it as a message to the user.\n\n        The current state of the recipe is:\n\n        {ctx.deps.state.recipe.model_dump_json(indent=2)}\n        \"\"\",\n    )\n\n\napp = agent.to_ag_ui(deps=StateDeps(RecipeSnapshot()))\n```", "url": "https://ai.pydantic.dev/ag-ui/index.html#shared-state", "page": "ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Tool Based Generative UI", "anchor": "tool-based-generative-ui", "md_text": "Demonstrates customised rendering for tool output with used confirmation.\n\nIf you've [run the example](index.html#running-the-example), you can view it at <http://localhost:3000/pydantic-ai/feature/tool_based_generative_ui>.\n\n#### Haiku Tools\n\n* `generate_haiku` - AG-UI tool to display a haiku in English and Japanese\n\n#### Haiku Prompt\n\n```\nGenerate a haiku about formula 1\n```\n\n#### Tool Based Generative UI - Code\n\n[ag\\_ui/api/tool\\_based\\_generative\\_ui.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/tool_based_generative_ui.py)\n\n```\n\"\"\"Tool Based Generative UI feature.\n\nNo special handling is required for this feature.\n\"\"\"\n\nfrom __future__ import annotations\n\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o-mini')\napp = agent.to_ag_ui()\n```", "url": "https://ai.pydantic.dev/ag-ui/index.html#tool-based-generative-ui", "page": "ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "name `abstractmethod` `property`", "anchor": "name-abstractmethod-property", "md_text": "```\nname: str\n```\n\nThe provider name.", "url": "https://ai.pydantic.dev/providers/index.html#name-abstractmethod-property", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "base\\_url `abstractmethod` `property`", "anchor": "baseurl-abstractmethod-property", "md_text": "```\nbase_url: str\n```\n\nThe base URL for the provider API.", "url": "https://ai.pydantic.dev/providers/index.html#baseurl-abstractmethod-property", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "client `abstractmethod` `property`", "anchor": "client-abstractmethod-property", "md_text": "```\nclient: InterfaceClient\n```\n\nThe client for the provider.", "url": "https://ai.pydantic.dev/providers/index.html#client-abstractmethod-property", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "model\\_profile", "anchor": "modelprofile", "md_text": "```\nmodel_profile(model_name: str) -> ModelProfile | None\n```\n\nThe model profile for the named model, if available.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 46 47 48 ``` | ``` def model_profile(self, model_name: str) -> ModelProfile | None:     \"\"\"The model profile for the named model, if available.\"\"\"     return None  # pragma: no cover ``` |", "url": "https://ai.pydantic.dev/providers/index.html#modelprofile", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "GoogleProvider", "anchor": "googleprovider", "md_text": "Bases: `Provider[Client]`\n\nProvider for Google.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/google.py`", "url": "https://ai.pydantic.dev/providers/index.html#googleprovider", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "GoogleProvider", "anchor": "googleprovider", "md_text": "|  |  |\n| --- | --- |\n| ```  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 ``` | ``` class GoogleProvider(Provider[Client]):     \"\"\"Provider for Google.\"\"\"      @property     def name(self) -> str:         return 'google-vertex' if self._client._api_client.vertexai else 'google-gla'  # type: ignore[reportPrivateUsage]      @property     def base_url(self) -> str:         return str(self._client._api_client._http_options.base_url)  # type: ignore[reportPrivateUsage]      @property     def client(self) -> Client:         return self._client      def model_profile(self, model_name: str) -> ModelProfile | None:         return google_model_profile(model_name)      @overload     def __init__(         self, *, api_key: str, http_client: httpx.AsyncClient | None = None, base_url: str | None = None     ) -> None: ...      @overload     def __init__(         self,         *,         credentials: Credentials | None = None,         project: str | None = None,         location: VertexAILocation | Literal['global'] | str | None = None,         http_client: httpx.AsyncClient | None = None,         base_url: str | None = None,     ) -> None: ...      @overload     def __init__(self, *, client: Client) -> None: ...      @overload     def __init__(         self,         *,         vertexai: bool = False,         api_key: str | None = None,         http_client: httpx.AsyncClient | None = None,         base_url: str | None = None,     ) -> None: ...      def __init__(         self,         *,         api_key: str | None = None,         credentials: Credentials | None = None,         project: str | None = None,         location: VertexAILocation | Literal['global'] | str | None = None,         vertexai: bool | None = None,         client: Client | None = None,         http_client: httpx.AsyncClient | None = None,         base_url: str | None = None,     ) -> None:         \"\"\"Create a new Google provider.          Args:             api_key: The `API key <https://ai.google.dev/gemini-api/docs/api-key>`_ to                 use for authentication. It can also be set via the `GOOGLE_API_KEY` environment variable.             credentials: The credentials to use for authentication when calling the Vertex AI APIs. Credentials can be                 obtained from environment variables and default credentials. For more information, see Set up                 Application Default Credentials. Applies to the Vertex AI API only.             project: The Google Cloud project ID to use for quota. Can be obtained from environment variables                 (for example, GOOGLE_CLOUD_PROJECT). Applies to the Vertex AI API only.             location: The location to send API requests to (for example, us-central1). Can be obtained from environment variables.                 Applies to the Vertex AI API only.             vertexai: Force the use of the Vertex AI API. If `False`, the Google Generative Language API will be used.                 Defaults to `False` unless `location`, `project`, or `credentials` are provided.             client: A pre-initialized client to use.             http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.             base_url: The base URL for the Google API.         \"\"\"         if client is None:             # NOTE: We are keeping GEMINI_API_KEY for backwards compatibility.             api_key = api_key or os.getenv('GOOGLE_API_KEY') or os.getenv('GEMINI_API_KEY')              vertex_ai_args_used = bool(location or project or credentials)             if vertexai is None:                 vertexai = vertex_ai_args_used              http_client = http_client or cached_async_http_client(                 provider='google-vertex' if vertexai else 'google-gla'             )             http_options = HttpOptions(                 base_url=base_url,                 headers={'User-Agent': get_user_agent()},                 httpx_async_client=http_client,                 # TODO: Remove once https://github.com/googleapis/python-genai/issues/1565 is solved.                 async_client_args={'transport': httpx.AsyncHTTPTransport()},             )             if not vertexai:                 if api_key is None:                     raise UserError(                         'Set the `GOOGLE_API_KEY` environment variable or pass it via `GoogleProvider(api_key=...)`'                         'to use the Google Generative Language API.'                     )                 self._client = _SafelyClosingClient(vertexai=False, api_key=api_key, http_options=http_options)             else:                 if vertex_ai_args_used:                     api_key = None                  if api_key is None:                     project = project or os.getenv('GOOGLE_CLOUD_PROJECT')                     # From https://github.com/pydantic/pydantic-ai/pull/2031/files#r2169682149:                     # Currently `us-central1` supports the most models by far of any region including `global`, but not                     # all of them. `us-central1` has all google models but is missing some Anthropic partner models,                     # which use `us-east5` instead. `global` has fewer models but higher availability.                     # For more details, check: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations#available-regions                     location = location or os.getenv('GOOGLE_CLOUD_LOCATION') or 'us-central1'                  self._client = _SafelyClosingClient(                     vertexai=True,                     api_key=api_key,                     project=project,                     location=location,                     credentials=credentials,                     http_options=http_options,                 )         else:             self._client = client  # pragma: no cover ``` |", "url": "https://ai.pydantic.dev/providers/index.html#googleprovider", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "GoogleProvider", "anchor": "googleprovider", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    *,\n    api_key: str,\n    http_client: AsyncClient | None = None,\n    base_url: str | None = None\n) -> None\n\n__init__(\n    *,\n    credentials: Credentials | None = None,\n    project: str | None = None,\n    location: (\n        VertexAILocation | Literal[\"global\"] | str | None\n    ) = None,\n    http_client: AsyncClient | None = None,\n    base_url: str | None = None\n) -> None\n\n__init__(*, client: Client) -> None\n\n__init__(\n    *,\n    vertexai: bool = False,\n    api_key: str | None = None,\n    http_client: AsyncClient | None = None,\n    base_url: str | None = None\n) -> None\n\n__init__(\n    *,\n    api_key: str | None = None,\n    credentials: Credentials | None = None,\n    project: str | None = None,\n    location: (\n        VertexAILocation | Literal[\"global\"] | str | None\n    ) = None,\n    vertexai: bool | None = None,\n    client: Client | None = None,\n    http_client: AsyncClient | None = None,\n    base_url: str | None = None\n) -> None\n```\n\nCreate a new Google provider.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `api_key` | `str | None` | The `API key <https://ai.google.dev/gemini-api/docs/api-key>`\\_ to use for authentication. It can also be set via the `GOOGLE_API_KEY` environment variable. | `None` |\n| `credentials` | `Credentials | None` | The credentials to use for authentication when calling the Vertex AI APIs. Credentials can be obtained from environment variables and default credentials. For more information, see Set up Application Default Credentials. Applies to the Vertex AI API only. | `None` |\n| `project` | `str | None` | The Google Cloud project ID to use for quota. Can be obtained from environment variables (for example, GOOGLE\\_CLOUD\\_PROJECT). Applies to the Vertex AI API only. | `None` |\n| `location` | `VertexAILocation | Literal['global'] | str | None` | The location to send API requests to (for example, us-central1). Can be obtained from environment variables. Applies to the Vertex AI API only. | `None` |\n| `vertexai` | `bool | None` | Force the use of the Vertex AI API. If `False`, the Google Generative Language API will be used. Defaults to `False` unless `location`, `project`, or `credentials` are provided. | `None` |\n| `client` | `Client | None` | A pre-initialized client to use. | `None` |\n| `http_client` | `AsyncClient | None` | An existing `httpx.AsyncClient` to use for making HTTP requests. | `None` |\n| `base_url` | `str | None` | The base URL for the Google API. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/google.py`", "url": "https://ai.pydantic.dev/providers/index.html#googleprovider", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "GoogleProvider", "anchor": "googleprovider", "md_text": "|  |  |\n| --- | --- |\n| ```  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 ``` | ``` def __init__(     self,     *,     api_key: str | None = None,     credentials: Credentials | None = None,     project: str | None = None,     location: VertexAILocation | Literal['global'] | str | None = None,     vertexai: bool | None = None,     client: Client | None = None,     http_client: httpx.AsyncClient | None = None,     base_url: str | None = None, ) -> None:     \"\"\"Create a new Google provider.      Args:         api_key: The `API key <https://ai.google.dev/gemini-api/docs/api-key>`_ to             use for authentication. It can also be set via the `GOOGLE_API_KEY` environment variable.         credentials: The credentials to use for authentication when calling the Vertex AI APIs. Credentials can be             obtained from environment variables and default credentials. For more information, see Set up             Application Default Credentials. Applies to the Vertex AI API only.         project: The Google Cloud project ID to use for quota. Can be obtained from environment variables             (for example, GOOGLE_CLOUD_PROJECT). Applies to the Vertex AI API only.         location: The location to send API requests to (for example, us-central1). Can be obtained from environment variables.             Applies to the Vertex AI API only.         vertexai: Force the use of the Vertex AI API. If `False`, the Google Generative Language API will be used.             Defaults to `False` unless `location`, `project`, or `credentials` are provided.         client: A pre-initialized client to use.         http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.         base_url: The base URL for the Google API.     \"\"\"     if client is None:         # NOTE: We are keeping GEMINI_API_KEY for backwards compatibility.         api_key = api_key or os.getenv('GOOGLE_API_KEY') or os.getenv('GEMINI_API_KEY')          vertex_ai_args_used = bool(location or project or credentials)         if vertexai is None:             vertexai = vertex_ai_args_used          http_client = http_client or cached_async_http_client(             provider='google-vertex' if vertexai else 'google-gla'         )         http_options = HttpOptions(             base_url=base_url,             headers={'User-Agent': get_user_agent()},             httpx_async_client=http_client,             # TODO: Remove once https://github.com/googleapis/python-genai/issues/1565 is solved.             async_client_args={'transport': httpx.AsyncHTTPTransport()},         )         if not vertexai:             if api_key is None:                 raise UserError(                     'Set the `GOOGLE_API_KEY` environment variable or pass it via `GoogleProvider(api_key=...)`'                     'to use the Google Generative Language API.'                 )             self._client = _SafelyClosingClient(vertexai=False, api_key=api_key, http_options=http_options)         else:             if vertex_ai_args_used:                 api_key = None              if api_key is None:                 project = project or os.getenv('GOOGLE_CLOUD_PROJECT')                 # From https://github.com/pydantic/pydantic-ai/pull/2031/files#r2169682149:                 # Currently `us-central1` supports the most models by far of any region including `global`, but not                 # all of them. `us-central1` has all google models but is missing some Anthropic partner models,                 # which use `us-east5` instead. `global` has fewer models but higher availability.                 # For more details, check: https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations#available-regions                 location = location or os.getenv('GOOGLE_CLOUD_LOCATION') or 'us-central1'              self._client = _SafelyClosingClient(                 vertexai=True,                 api_key=api_key,                 project=project,                 location=location,                 credentials=credentials,                 http_options=http_options,             )     else:         self._client = client  # pragma: no cover ``` |", "url": "https://ai.pydantic.dev/providers/index.html#googleprovider", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "VertexAILocation `module-attribute`", "anchor": "vertexailocation-module-attribute", "md_text": "```\nVertexAILocation = Literal[\n    \"asia-east1\",\n    \"asia-east2\",\n    \"asia-northeast1\",\n    \"asia-northeast3\",\n    \"asia-south1\",\n    \"asia-southeast1\",\n    \"australia-southeast1\",\n    \"europe-central2\",\n    \"europe-north1\",\n    \"europe-southwest1\",\n    \"europe-west1\",\n    \"europe-west2\",\n    \"europe-west3\",\n    \"europe-west4\",\n    \"europe-west6\",\n    \"europe-west8\",\n    \"europe-west9\",\n    \"me-central1\",\n    \"me-central2\",\n    \"me-west1\",\n    \"northamerica-northeast1\",\n    \"southamerica-east1\",\n    \"us-central1\",\n    \"us-east1\",\n    \"us-east4\",\n    \"us-east5\",\n    \"us-south1\",\n    \"us-west1\",\n    \"us-west4\",\n]\n```\n\nRegions available for Vertex AI.\nMore details [here](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations#genai-locations).", "url": "https://ai.pydantic.dev/providers/index.html#vertexailocation-module-attribute", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIProvider", "anchor": "openaiprovider", "md_text": "Bases: `Provider[AsyncOpenAI]`\n\nProvider for OpenAI API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/openai.py`\n\n|  |  |\n| --- | --- |\n| ``` 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 ``` | ``` class OpenAIProvider(Provider[AsyncOpenAI]):     \"\"\"Provider for OpenAI API.\"\"\"      @property     def name(self) -> str:         return 'openai'      @property     def base_url(self) -> str:         return str(self.client.base_url)      @property     def client(self) -> AsyncOpenAI:         return self._client      def model_profile(self, model_name: str) -> ModelProfile | None:         return openai_model_profile(model_name)      @overload     def __init__(self, *, openai_client: AsyncOpenAI) -> None: ...      @overload     def __init__(         self,         base_url: str | None = None,         api_key: str | None = None,         openai_client: None = None,         http_client: httpx.AsyncClient | None = None,     ) -> None: ...      def __init__(         self,         base_url: str | None = None,         api_key: str | None = None,         openai_client: AsyncOpenAI | None = None,         http_client: httpx.AsyncClient | None = None,     ) -> None:         \"\"\"Create a new OpenAI provider.          Args:             base_url: The base url for the OpenAI requests. If not provided, the `OPENAI_BASE_URL` environment variable                 will be used if available. Otherwise, defaults to OpenAI's base url.             api_key: The API key to use for authentication, if not provided, the `OPENAI_API_KEY` environment variable                 will be used if available.             openai_client: An existing                 [`AsyncOpenAI`](https://github.com/openai/openai-python?tab=readme-ov-file#async-usage)                 client to use. If provided, `base_url`, `api_key`, and `http_client` must be `None`.             http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.         \"\"\"         # This is a workaround for the OpenAI client requiring an API key, whilst locally served,         # openai compatible models do not always need an API key, but a placeholder (non-empty) key is required.         if api_key is None and 'OPENAI_API_KEY' not in os.environ and base_url is not None and openai_client is None:             api_key = 'api-key-not-set'          if openai_client is not None:             assert base_url is None, 'Cannot provide both `openai_client` and `base_url`'             assert http_client is None, 'Cannot provide both `openai_client` and `http_client`'             assert api_key is None, 'Cannot provide both `openai_client` and `api_key`'             self._client = openai_client         elif http_client is not None:             self._client = AsyncOpenAI(base_url=base_url, api_key=api_key, http_client=http_client)         else:             http_client = cached_async_http_client(provider='openai')             self._client = AsyncOpenAI(base_url=base_url, api_key=api_key, http_client=http_client) ``` |\n\n#### \\_\\_init\\_\\_\n\n```\n__init__(*, openai_client: AsyncOpenAI) -> None\n\n__init__(\n    base_url: str | None = None,\n    api_key: str | None = None,\n    openai_client: None = None,\n    http_client: AsyncClient | None = None,\n) -> None\n\n__init__(\n    base_url: str | None = None,\n    api_key: str | None = None,\n    openai_client: AsyncOpenAI | None = None,\n    http_client: AsyncClient | None = None,\n) -> None\n```\n\nCreate a new OpenAI provider.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `base_url` | `str | None` | The base url for the OpenAI requests. If not provided, the `OPENAI_BASE_URL` environment variable will be used if available. Otherwise, defaults to OpenAI's base url. | `None` |\n| `api_key` | `str | None` | The API key to use for authentication, if not provided, the `OPENAI_API_KEY` environment variable will be used if available. | `None` |\n| `openai_client` | `AsyncOpenAI | None` | An existing [`AsyncOpenAI`](https://github.com/openai/openai-python?tab=readme-ov-file#async-usage) client to use. If provided, `base_url`, `api_key`, and `http_client` must be `None`. | `None` |\n| `http_client` | `AsyncClient | None` | An existing `httpx.AsyncClient` to use for making HTTP requests. | `None` |", "url": "https://ai.pydantic.dev/providers/index.html#openaiprovider", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIProvider", "anchor": "openaiprovider", "md_text": "Source code in `pydantic_ai_slim/pydantic_ai/providers/openai.py`\n\n|  |  |\n| --- | --- |\n| ``` 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 ``` | ``` def __init__(     self,     base_url: str | None = None,     api_key: str | None = None,     openai_client: AsyncOpenAI | None = None,     http_client: httpx.AsyncClient | None = None, ) -> None:     \"\"\"Create a new OpenAI provider.      Args:         base_url: The base url for the OpenAI requests. If not provided, the `OPENAI_BASE_URL` environment variable             will be used if available. Otherwise, defaults to OpenAI's base url.         api_key: The API key to use for authentication, if not provided, the `OPENAI_API_KEY` environment variable             will be used if available.         openai_client: An existing             [`AsyncOpenAI`](https://github.com/openai/openai-python?tab=readme-ov-file#async-usage)             client to use. If provided, `base_url`, `api_key`, and `http_client` must be `None`.         http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.     \"\"\"     # This is a workaround for the OpenAI client requiring an API key, whilst locally served,     # openai compatible models do not always need an API key, but a placeholder (non-empty) key is required.     if api_key is None and 'OPENAI_API_KEY' not in os.environ and base_url is not None and openai_client is None:         api_key = 'api-key-not-set'      if openai_client is not None:         assert base_url is None, 'Cannot provide both `openai_client` and `base_url`'         assert http_client is None, 'Cannot provide both `openai_client` and `http_client`'         assert api_key is None, 'Cannot provide both `openai_client` and `api_key`'         self._client = openai_client     elif http_client is not None:         self._client = AsyncOpenAI(base_url=base_url, api_key=api_key, http_client=http_client)     else:         http_client = cached_async_http_client(provider='openai')         self._client = AsyncOpenAI(base_url=base_url, api_key=api_key, http_client=http_client) ``` |", "url": "https://ai.pydantic.dev/providers/index.html#openaiprovider", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "DeepSeekProvider", "anchor": "deepseekprovider", "md_text": "Bases: `Provider[AsyncOpenAI]`\n\nProvider for DeepSeek API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/deepseek.py`\n\n|  |  |\n| --- | --- |\n| ``` 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 ``` | ``` class DeepSeekProvider(Provider[AsyncOpenAI]):     \"\"\"Provider for DeepSeek API.\"\"\"      @property     def name(self) -> str:         return 'deepseek'      @property     def base_url(self) -> str:         return 'https://api.deepseek.com'      @property     def client(self) -> AsyncOpenAI:         return self._client      def model_profile(self, model_name: str) -> ModelProfile | None:         profile = deepseek_model_profile(model_name)          # As DeepSeekProvider is always used with OpenAIChatModel, which used to unconditionally use OpenAIJsonSchemaTransformer,         # we need to maintain that behavior unless json_schema_transformer is set explicitly.         # This was not the case when using a DeepSeek model with another model class (e.g. BedrockConverseModel or GroqModel),         # so we won't do this in `deepseek_model_profile` unless we learn it's always needed.         return OpenAIModelProfile(json_schema_transformer=OpenAIJsonSchemaTransformer).update(profile)      @overload     def __init__(self) -> None: ...      @overload     def __init__(self, *, api_key: str) -> None: ...      @overload     def __init__(self, *, api_key: str, http_client: httpx.AsyncClient) -> None: ...      @overload     def __init__(self, *, openai_client: AsyncOpenAI | None = None) -> None: ...      def __init__(         self,         *,         api_key: str | None = None,         openai_client: AsyncOpenAI | None = None,         http_client: httpx.AsyncClient | None = None,     ) -> None:         api_key = api_key or os.getenv('DEEPSEEK_API_KEY')         if not api_key and openai_client is None:             raise UserError(                 'Set the `DEEPSEEK_API_KEY` environment variable or pass it via `DeepSeekProvider(api_key=...)`'                 'to use the DeepSeek provider.'             )          if openai_client is not None:             self._client = openai_client         elif http_client is not None:             self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)         else:             http_client = cached_async_http_client(provider='deepseek')             self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client) ``` |", "url": "https://ai.pydantic.dev/providers/index.html#deepseekprovider", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "BedrockModelProfile `dataclass`", "anchor": "bedrockmodelprofile-dataclass", "md_text": "Bases: `ModelProfile`\n\nProfile for models used with BedrockModel.\n\nALL FIELDS MUST BE `bedrock_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/bedrock.py`\n\n|  |  |\n| --- | --- |\n| ``` 33 34 35 36 37 38 39 40 41 42 ``` | ``` @dataclass(kw_only=True) class BedrockModelProfile(ModelProfile):     \"\"\"Profile for models used with BedrockModel.      ALL FIELDS MUST BE `bedrock_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.     \"\"\"      bedrock_supports_tool_choice: bool = False     bedrock_tool_result_format: Literal['text', 'json'] = 'text'     bedrock_send_back_thinking_parts: bool = False ``` |", "url": "https://ai.pydantic.dev/providers/index.html#bedrockmodelprofile-dataclass", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "bedrock\\_amazon\\_model\\_profile", "anchor": "bedrockamazonmodelprofile", "md_text": "```\nbedrock_amazon_model_profile(\n    model_name: str,\n) -> ModelProfile | None\n```\n\nGet the model profile for an Amazon model used via Bedrock.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/bedrock.py`\n\n|  |  |\n| --- | --- |\n| ``` 45 46 47 48 49 50 ``` | ``` def bedrock_amazon_model_profile(model_name: str) -> ModelProfile | None:     \"\"\"Get the model profile for an Amazon model used via Bedrock.\"\"\"     profile = amazon_model_profile(model_name)     if 'nova' in model_name:         return BedrockModelProfile(bedrock_supports_tool_choice=True).update(profile)     return profile ``` |", "url": "https://ai.pydantic.dev/providers/index.html#bedrockamazonmodelprofile", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "bedrock\\_deepseek\\_model\\_profile", "anchor": "bedrockdeepseekmodelprofile", "md_text": "```\nbedrock_deepseek_model_profile(\n    model_name: str,\n) -> ModelProfile | None\n```\n\nGet the model profile for a DeepSeek model used via Bedrock.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/bedrock.py`\n\n|  |  |\n| --- | --- |\n| ``` 53 54 55 56 57 58 ``` | ``` def bedrock_deepseek_model_profile(model_name: str) -> ModelProfile | None:     \"\"\"Get the model profile for a DeepSeek model used via Bedrock.\"\"\"     profile = deepseek_model_profile(model_name)     if 'r1' in model_name:         return BedrockModelProfile(bedrock_send_back_thinking_parts=True).update(profile)     return profile  # pragma: no cover ``` |", "url": "https://ai.pydantic.dev/providers/index.html#bedrockdeepseekmodelprofile", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "BedrockProvider", "anchor": "bedrockprovider", "md_text": "Bases: `Provider[BaseClient]`\n\nProvider for AWS Bedrock.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/bedrock.py`", "url": "https://ai.pydantic.dev/providers/index.html#bedrockprovider", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "BedrockProvider", "anchor": "bedrockprovider", "md_text": "|  |  |\n| --- | --- |\n| ```  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 ``` | ``` class BedrockProvider(Provider[BaseClient]):     \"\"\"Provider for AWS Bedrock.\"\"\"      @property     def name(self) -> str:         return 'bedrock'      @property     def base_url(self) -> str:         return self._client.meta.endpoint_url      @property     def client(self) -> BaseClient:         return self._client      def model_profile(self, model_name: str) -> ModelProfile | None:         provider_to_profile: dict[str, Callable[[str], ModelProfile | None]] = {             'anthropic': lambda model_name: BedrockModelProfile(                 bedrock_supports_tool_choice=True, bedrock_send_back_thinking_parts=True             ).update(anthropic_model_profile(model_name)),             'mistral': lambda model_name: BedrockModelProfile(bedrock_tool_result_format='json').update(                 mistral_model_profile(model_name)             ),             'cohere': cohere_model_profile,             'amazon': bedrock_amazon_model_profile,             'meta': meta_model_profile,             'deepseek': bedrock_deepseek_model_profile,         }          # Split the model name into parts         parts = model_name.split('.', 2)          # Handle regional prefixes (e.g. \"us.\")         if len(parts) > 2 and len(parts[0]) == 2:             parts = parts[1:]          if len(parts) < 2:             return None          provider = parts[0]         model_name_with_version = parts[1]          # Remove version suffix if it matches the format (e.g. \"-v1:0\" or \"-v14\")         version_match = re.match(r'(.+)-v\\d+(?::\\d+)?$', model_name_with_version)         if version_match:             model_name = version_match.group(1)         else:             model_name = model_name_with_version          if provider in provider_to_profile:             return provider_to_profile[provider](model_name)          return None      @overload     def __init__(self, *, bedrock_client: BaseClient) -> None: ...      @overload     def __init__(         self,         *,         api_key: str,         base_url: str | None = None,         region_name: str | None = None,         profile_name: str | None = None,         aws_read_timeout: float | None = None,         aws_connect_timeout: float | None = None,     ) -> None: ...      @overload     def __init__(         self,         *,         aws_access_key_id: str | None = None,         aws_secret_access_key: str | None = None,         aws_session_token: str | None = None,         base_url: str | None = None,         region_name: str | None = None,         profile_name: str | None = None,         aws_read_timeout: float | None = None,         aws_connect_timeout: float | None = None,     ) -> None: ...      def __init__(         self,         *,         bedrock_client: BaseClient | None = None,         aws_access_key_id: str | None = None,         aws_secret_access_key: str | None = None,         aws_session_token: str | None = None,         base_url: str | None = None,         region_name: str | None = None,         profile_name: str | None = None,         api_key: str | None = None,         aws_read_timeout: float | None = None,         aws_connect_timeout: float | None = None,     ) -> None:         \"\"\"Initialize the Bedrock provider.          Args:             bedrock_client: A boto3 client for Bedrock Runtime. If provided, other arguments are ignored.             aws_access_key_id: The AWS access key ID. If not set, the `AWS_ACCESS_KEY_ID` environment variable will be used if available.             aws_secret_access_key: The AWS secret access key. If not set, the `AWS_SECRET_ACCESS_KEY` environment variable will be used if available.             aws_session_token: The AWS session token. If not set, the `AWS_SESSION_TOKEN` environment variable will be used if available.             api_key: The API key for Bedrock client. Can be used instead of `aws_access_key_id`, `aws_secret_access_key`, and `aws_session_token`. If not set, the `AWS_BEARER_TOKEN_BEDROCK` environment variable will be used if available.             base_url: The base URL for the Bedrock client.             region_name: The AWS region name. If not set, the `AWS_DEFAULT_REGION` environment variable will be used if available.             profile_name: The AWS profile name.             aws_read_timeout: The read timeout for Bedrock client.             aws_connect_timeout: The connect timeout for Bedrock client.         \"\"\"         if bedrock_client is not None:             self._client = bedrock_client         else:             read_timeout = aws_read_timeout or float(os.getenv('AWS_READ_TIMEOUT', 300))             connect_timeout = aws_connect_timeout or float(os.getenv('AWS_CONNECT_TIMEOUT', 60))             config: dict[str, Any] = {                 'read_timeout': read_timeout,                 'connect_timeout': connect_timeout,             }             try:                 if api_key is not None:                     session = boto3.Session(                         botocore_session=_BearerTokenSession(api_key),                         region_name=region_name,                         profile_name=profile_name,                     )                     config['signature_version'] = 'bearer'                 else:                     session = boto3.Session(                         aws_access_key_id=aws_access_key_id,                         aws_secret_access_key=aws_secret_access_key,                         aws_session_token=aws_session_token,                         region_name=region_name,                         profile_name=profile_name,                     )                 self._client = session.client(  # type: ignore[reportUnknownMemberType]                     'bedrock-runtime',                     config=Config(**config),                     endpoint_url=base_url,                 )             except NoRegionError as exc:  # pragma: no cover                 raise UserError('You must provide a `region_name` or a boto3 client for Bedrock Runtime.') from exc ``` |", "url": "https://ai.pydantic.dev/providers/index.html#bedrockprovider", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "BedrockProvider", "anchor": "bedrockprovider", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(*, bedrock_client: BaseClient) -> None\n\n__init__(\n    *,\n    api_key: str,\n    base_url: str | None = None,\n    region_name: str | None = None,\n    profile_name: str | None = None,\n    aws_read_timeout: float | None = None,\n    aws_connect_timeout: float | None = None\n) -> None\n\n__init__(\n    *,\n    aws_access_key_id: str | None = None,\n    aws_secret_access_key: str | None = None,\n    aws_session_token: str | None = None,\n    base_url: str | None = None,\n    region_name: str | None = None,\n    profile_name: str | None = None,\n    aws_read_timeout: float | None = None,\n    aws_connect_timeout: float | None = None\n) -> None\n\n__init__(\n    *,\n    bedrock_client: BaseClient | None = None,\n    aws_access_key_id: str | None = None,\n    aws_secret_access_key: str | None = None,\n    aws_session_token: str | None = None,\n    base_url: str | None = None,\n    region_name: str | None = None,\n    profile_name: str | None = None,\n    api_key: str | None = None,\n    aws_read_timeout: float | None = None,\n    aws_connect_timeout: float | None = None\n) -> None\n```\n\nInitialize the Bedrock provider.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `bedrock_client` | `BaseClient | None` | A boto3 client for Bedrock Runtime. If provided, other arguments are ignored. | `None` |\n| `aws_access_key_id` | `str | None` | The AWS access key ID. If not set, the `AWS_ACCESS_KEY_ID` environment variable will be used if available. | `None` |\n| `aws_secret_access_key` | `str | None` | The AWS secret access key. If not set, the `AWS_SECRET_ACCESS_KEY` environment variable will be used if available. | `None` |\n| `aws_session_token` | `str | None` | The AWS session token. If not set, the `AWS_SESSION_TOKEN` environment variable will be used if available. | `None` |\n| `api_key` | `str | None` | The API key for Bedrock client. Can be used instead of `aws_access_key_id`, `aws_secret_access_key`, and `aws_session_token`. If not set, the `AWS_BEARER_TOKEN_BEDROCK` environment variable will be used if available. | `None` |\n| `base_url` | `str | None` | The base URL for the Bedrock client. | `None` |\n| `region_name` | `str | None` | The AWS region name. If not set, the `AWS_DEFAULT_REGION` environment variable will be used if available. | `None` |\n| `profile_name` | `str | None` | The AWS profile name. | `None` |\n| `aws_read_timeout` | `float | None` | The read timeout for Bedrock client. | `None` |\n| `aws_connect_timeout` | `float | None` | The connect timeout for Bedrock client. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/bedrock.py`", "url": "https://ai.pydantic.dev/providers/index.html#bedrockprovider", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "BedrockProvider", "anchor": "bedrockprovider", "md_text": "|  |  |\n| --- | --- |\n| ``` 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 ``` | ``` def __init__(     self,     *,     bedrock_client: BaseClient | None = None,     aws_access_key_id: str | None = None,     aws_secret_access_key: str | None = None,     aws_session_token: str | None = None,     base_url: str | None = None,     region_name: str | None = None,     profile_name: str | None = None,     api_key: str | None = None,     aws_read_timeout: float | None = None,     aws_connect_timeout: float | None = None, ) -> None:     \"\"\"Initialize the Bedrock provider.      Args:         bedrock_client: A boto3 client for Bedrock Runtime. If provided, other arguments are ignored.         aws_access_key_id: The AWS access key ID. If not set, the `AWS_ACCESS_KEY_ID` environment variable will be used if available.         aws_secret_access_key: The AWS secret access key. If not set, the `AWS_SECRET_ACCESS_KEY` environment variable will be used if available.         aws_session_token: The AWS session token. If not set, the `AWS_SESSION_TOKEN` environment variable will be used if available.         api_key: The API key for Bedrock client. Can be used instead of `aws_access_key_id`, `aws_secret_access_key`, and `aws_session_token`. If not set, the `AWS_BEARER_TOKEN_BEDROCK` environment variable will be used if available.         base_url: The base URL for the Bedrock client.         region_name: The AWS region name. If not set, the `AWS_DEFAULT_REGION` environment variable will be used if available.         profile_name: The AWS profile name.         aws_read_timeout: The read timeout for Bedrock client.         aws_connect_timeout: The connect timeout for Bedrock client.     \"\"\"     if bedrock_client is not None:         self._client = bedrock_client     else:         read_timeout = aws_read_timeout or float(os.getenv('AWS_READ_TIMEOUT', 300))         connect_timeout = aws_connect_timeout or float(os.getenv('AWS_CONNECT_TIMEOUT', 60))         config: dict[str, Any] = {             'read_timeout': read_timeout,             'connect_timeout': connect_timeout,         }         try:             if api_key is not None:                 session = boto3.Session(                     botocore_session=_BearerTokenSession(api_key),                     region_name=region_name,                     profile_name=profile_name,                 )                 config['signature_version'] = 'bearer'             else:                 session = boto3.Session(                     aws_access_key_id=aws_access_key_id,                     aws_secret_access_key=aws_secret_access_key,                     aws_session_token=aws_session_token,                     region_name=region_name,                     profile_name=profile_name,                 )             self._client = session.client(  # type: ignore[reportUnknownMemberType]                 'bedrock-runtime',                 config=Config(**config),                 endpoint_url=base_url,             )         except NoRegionError as exc:  # pragma: no cover             raise UserError('You must provide a `region_name` or a boto3 client for Bedrock Runtime.') from exc ``` |", "url": "https://ai.pydantic.dev/providers/index.html#bedrockprovider", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "groq\\_moonshotai\\_model\\_profile", "anchor": "groqmoonshotaimodelprofile", "md_text": "```\ngroq_moonshotai_model_profile(\n    model_name: str,\n) -> ModelProfile | None\n```\n\nGet the model profile for an MoonshotAI model used with the Groq provider.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/groq.py`\n\n|  |  |\n| --- | --- |\n| ``` 30 31 32 33 34 ``` | ``` def groq_moonshotai_model_profile(model_name: str) -> ModelProfile | None:     \"\"\"Get the model profile for an MoonshotAI model used with the Groq provider.\"\"\"     return ModelProfile(supports_json_object_output=True, supports_json_schema_output=True).update(         moonshotai_model_profile(model_name)     ) ``` |", "url": "https://ai.pydantic.dev/providers/index.html#groqmoonshotaimodelprofile", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "meta\\_groq\\_model\\_profile", "anchor": "metagroqmodelprofile", "md_text": "```\nmeta_groq_model_profile(\n    model_name: str,\n) -> ModelProfile | None\n```\n\nGet the model profile for a Meta model used with the Groq provider.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/groq.py`\n\n|  |  |\n| --- | --- |\n| ``` 37 38 39 40 41 42 43 44 ``` | ``` def meta_groq_model_profile(model_name: str) -> ModelProfile | None:     \"\"\"Get the model profile for a Meta model used with the Groq provider.\"\"\"     if model_name in {'llama-4-maverick-17b-128e-instruct', 'llama-4-scout-17b-16e-instruct'}:         return ModelProfile(supports_json_object_output=True, supports_json_schema_output=True).update(             meta_model_profile(model_name)         )     else:         return meta_model_profile(model_name) ``` |", "url": "https://ai.pydantic.dev/providers/index.html#metagroqmodelprofile", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "GroqProvider", "anchor": "groqprovider", "md_text": "Bases: `Provider[AsyncGroq]`\n\nProvider for Groq API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/groq.py`\n\n|  |  |\n| --- | --- |\n| ```  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 ``` | ``` class GroqProvider(Provider[AsyncGroq]):     \"\"\"Provider for Groq API.\"\"\"      @property     def name(self) -> str:         return 'groq'      @property     def base_url(self) -> str:         return str(self.client.base_url)      @property     def client(self) -> AsyncGroq:         return self._client      def model_profile(self, model_name: str) -> ModelProfile | None:         prefix_to_profile = {             'llama': meta_model_profile,             'meta-llama/': meta_groq_model_profile,             'gemma': google_model_profile,             'qwen': qwen_model_profile,             'deepseek': deepseek_model_profile,             'mistral': mistral_model_profile,             'moonshotai/': groq_moonshotai_model_profile,             'compound-': groq_model_profile,             'openai/': openai_model_profile,         }          for prefix, profile_func in prefix_to_profile.items():             model_name = model_name.lower()             if model_name.startswith(prefix):                 if prefix.endswith('/'):                     model_name = model_name[len(prefix) :]                 return profile_func(model_name)          return None      @overload     def __init__(self, *, groq_client: AsyncGroq | None = None) -> None: ...      @overload     def __init__(         self, *, api_key: str | None = None, base_url: str | None = None, http_client: httpx.AsyncClient | None = None     ) -> None: ...      def __init__(         self,         *,         api_key: str | None = None,         base_url: str | None = None,         groq_client: AsyncGroq | None = None,         http_client: httpx.AsyncClient | None = None,     ) -> None:         \"\"\"Create a new Groq provider.          Args:             api_key: The API key to use for authentication, if not provided, the `GROQ_API_KEY` environment variable                 will be used if available.             base_url: The base url for the Groq requests. If not provided, the `GROQ_BASE_URL` environment variable                 will be used if available. Otherwise, defaults to Groq's base url.             groq_client: An existing                 [`AsyncGroq`](https://github.com/groq/groq-python?tab=readme-ov-file#async-usage)                 client to use. If provided, `api_key` and `http_client` must be `None`.             http_client: An existing `AsyncHTTPClient` to use for making HTTP requests.         \"\"\"         if groq_client is not None:             assert http_client is None, 'Cannot provide both `groq_client` and `http_client`'             assert api_key is None, 'Cannot provide both `groq_client` and `api_key`'             assert base_url is None, 'Cannot provide both `groq_client` and `base_url`'             self._client = groq_client         else:             api_key = api_key or os.getenv('GROQ_API_KEY')             base_url = base_url or os.getenv('GROQ_BASE_URL', 'https://api.groq.com')              if not api_key:                 raise UserError(                     'Set the `GROQ_API_KEY` environment variable or pass it via `GroqProvider(api_key=...)`'                     'to use the Groq provider.'                 )             elif http_client is not None:                 self._client = AsyncGroq(base_url=base_url, api_key=api_key, http_client=http_client)             else:                 http_client = cached_async_http_client(provider='groq')                 self._client = AsyncGroq(base_url=base_url, api_key=api_key, http_client=http_client) ``` |\n\n#### \\_\\_init\\_\\_\n\n```\n__init__(*, groq_client: AsyncGroq | None = None) -> None\n\n__init__(\n    *,\n    api_key: str | None = None,\n    base_url: str | None = None,\n    http_client: AsyncClient | None = None\n) -> None", "url": "https://ai.pydantic.dev/providers/index.html#groqprovider", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "GroqProvider", "anchor": "groqprovider", "md_text": "__init__(\n    *,\n    api_key: str | None = None,\n    base_url: str | None = None,\n    groq_client: AsyncGroq | None = None,\n    http_client: AsyncClient | None = None\n) -> None\n```\n\nCreate a new Groq provider.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `api_key` | `str | None` | The API key to use for authentication, if not provided, the `GROQ_API_KEY` environment variable will be used if available. | `None` |\n| `base_url` | `str | None` | The base url for the Groq requests. If not provided, the `GROQ_BASE_URL` environment variable will be used if available. Otherwise, defaults to Groq's base url. | `None` |\n| `groq_client` | `AsyncGroq | None` | An existing [`AsyncGroq`](https://github.com/groq/groq-python?tab=readme-ov-file#async-usage) client to use. If provided, `api_key` and `http_client` must be `None`. | `None` |\n| `http_client` | `AsyncClient | None` | An existing `AsyncHTTPClient` to use for making HTTP requests. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/groq.py`\n\n|  |  |\n| --- | --- |\n| ```  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 ``` | ``` def __init__(     self,     *,     api_key: str | None = None,     base_url: str | None = None,     groq_client: AsyncGroq | None = None,     http_client: httpx.AsyncClient | None = None, ) -> None:     \"\"\"Create a new Groq provider.      Args:         api_key: The API key to use for authentication, if not provided, the `GROQ_API_KEY` environment variable             will be used if available.         base_url: The base url for the Groq requests. If not provided, the `GROQ_BASE_URL` environment variable             will be used if available. Otherwise, defaults to Groq's base url.         groq_client: An existing             [`AsyncGroq`](https://github.com/groq/groq-python?tab=readme-ov-file#async-usage)             client to use. If provided, `api_key` and `http_client` must be `None`.         http_client: An existing `AsyncHTTPClient` to use for making HTTP requests.     \"\"\"     if groq_client is not None:         assert http_client is None, 'Cannot provide both `groq_client` and `http_client`'         assert api_key is None, 'Cannot provide both `groq_client` and `api_key`'         assert base_url is None, 'Cannot provide both `groq_client` and `base_url`'         self._client = groq_client     else:         api_key = api_key or os.getenv('GROQ_API_KEY')         base_url = base_url or os.getenv('GROQ_BASE_URL', 'https://api.groq.com')          if not api_key:             raise UserError(                 'Set the `GROQ_API_KEY` environment variable or pass it via `GroqProvider(api_key=...)`'                 'to use the Groq provider.'             )         elif http_client is not None:             self._client = AsyncGroq(base_url=base_url, api_key=api_key, http_client=http_client)         else:             http_client = cached_async_http_client(provider='groq')             self._client = AsyncGroq(base_url=base_url, api_key=api_key, http_client=http_client) ``` |", "url": "https://ai.pydantic.dev/providers/index.html#groqprovider", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "AzureProvider", "anchor": "azureprovider", "md_text": "Bases: `Provider[AsyncOpenAI]`\n\nProvider for Azure OpenAI API.\n\nSee <https://azure.microsoft.com/en-us/products/ai-foundry> for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/azure.py`", "url": "https://ai.pydantic.dev/providers/index.html#azureprovider", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "AzureProvider", "anchor": "azureprovider", "md_text": "|  |  |\n| --- | --- |\n| ```  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 ``` | ``` class AzureProvider(Provider[AsyncOpenAI]):     \"\"\"Provider for Azure OpenAI API.      See <https://azure.microsoft.com/en-us/products/ai-foundry> for more information.     \"\"\"      @property     def name(self) -> str:         return 'azure'      @property     def base_url(self) -> str:         assert self._base_url is not None         return self._base_url      @property     def client(self) -> AsyncOpenAI:         return self._client      def model_profile(self, model_name: str) -> ModelProfile | None:         model_name = model_name.lower()          prefix_to_profile = {             'llama': meta_model_profile,             'meta-': meta_model_profile,             'deepseek': deepseek_model_profile,             'mistralai-': mistral_model_profile,             'mistral': mistral_model_profile,             'cohere-': cohere_model_profile,             'grok': grok_model_profile,         }          for prefix, profile_func in prefix_to_profile.items():             if model_name.startswith(prefix):                 if prefix.endswith('-'):                     model_name = model_name[len(prefix) :]                  profile = profile_func(model_name)                  # As AzureProvider is always used with OpenAIChatModel, which used to unconditionally use OpenAIJsonSchemaTransformer,                 # we need to maintain that behavior unless json_schema_transformer is set explicitly                 return OpenAIModelProfile(json_schema_transformer=OpenAIJsonSchemaTransformer).update(profile)          # OpenAI models are unprefixed         return openai_model_profile(model_name)      @overload     def __init__(self, *, openai_client: AsyncAzureOpenAI) -> None: ...      @overload     def __init__(         self,         *,         azure_endpoint: str | None = None,         api_version: str | None = None,         api_key: str | None = None,         http_client: httpx.AsyncClient | None = None,     ) -> None: ...      def __init__(         self,         *,         azure_endpoint: str | None = None,         api_version: str | None = None,         api_key: str | None = None,         openai_client: AsyncAzureOpenAI | None = None,         http_client: httpx.AsyncClient | None = None,     ) -> None:         \"\"\"Create a new Azure provider.          Args:             azure_endpoint: The Azure endpoint to use for authentication, if not provided, the `AZURE_OPENAI_ENDPOINT`                 environment variable will be used if available.             api_version: The API version to use for authentication, if not provided, the `OPENAI_API_VERSION`                 environment variable will be used if available.             api_key: The API key to use for authentication, if not provided, the `AZURE_OPENAI_API_KEY` environment variable                 will be used if available.             openai_client: An existing                 [`AsyncAzureOpenAI`](https://github.com/openai/openai-python#microsoft-azure-openai)                 client to use. If provided, `base_url`, `api_key`, and `http_client` must be `None`.             http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.         \"\"\"         if openai_client is not None:             assert azure_endpoint is None, 'Cannot provide both `openai_client` and `azure_endpoint`'             assert http_client is None, 'Cannot provide both `openai_client` and `http_client`'             assert api_key is None, 'Cannot provide both `openai_client` and `api_key`'             self._base_url = str(openai_client.base_url)             self._client = openai_client         else:             azure_endpoint = azure_endpoint or os.getenv('AZURE_OPENAI_ENDPOINT')             if not azure_endpoint:                 raise UserError(                     'Must provide one of the `azure_endpoint` argument or the `AZURE_OPENAI_ENDPOINT` environment variable'                 )              if not api_key and 'AZURE_OPENAI_API_KEY' not in os.environ:  # pragma: no cover                 raise UserError(                     'Must provide one of the `api_key` argument or the `AZURE_OPENAI_API_KEY` environment variable'                 )              if not api_version and 'OPENAI_API_VERSION' not in os.environ:  # pragma: no cover                 raise UserError(                     'Must provide one of the `api_version` argument or the `OPENAI_API_VERSION` environment variable'                 )              http_client = http_client or cached_async_http_client(provider='azure')             self._client = AsyncAzureOpenAI(                 azure_endpoint=azure_endpoint,                 api_key=api_key,                 api_version=api_version,                 http_client=http_client,             )             self._base_url = str(self._client.base_url) ``` |", "url": "https://ai.pydantic.dev/providers/index.html#azureprovider", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "AzureProvider", "anchor": "azureprovider", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(*, openai_client: AsyncAzureOpenAI) -> None\n\n__init__(\n    *,\n    azure_endpoint: str | None = None,\n    api_version: str | None = None,\n    api_key: str | None = None,\n    http_client: AsyncClient | None = None\n) -> None\n\n__init__(\n    *,\n    azure_endpoint: str | None = None,\n    api_version: str | None = None,\n    api_key: str | None = None,\n    openai_client: AsyncAzureOpenAI | None = None,\n    http_client: AsyncClient | None = None\n) -> None\n```\n\nCreate a new Azure provider.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `azure_endpoint` | `str | None` | The Azure endpoint to use for authentication, if not provided, the `AZURE_OPENAI_ENDPOINT` environment variable will be used if available. | `None` |\n| `api_version` | `str | None` | The API version to use for authentication, if not provided, the `OPENAI_API_VERSION` environment variable will be used if available. | `None` |\n| `api_key` | `str | None` | The API key to use for authentication, if not provided, the `AZURE_OPENAI_API_KEY` environment variable will be used if available. | `None` |\n| `openai_client` | `AsyncAzureOpenAI | None` | An existing [`AsyncAzureOpenAI`](https://github.com/openai/openai-python#microsoft-azure-openai) client to use. If provided, `base_url`, `api_key`, and `http_client` must be `None`. | `None` |\n| `http_client` | `AsyncClient | None` | An existing `httpx.AsyncClient` to use for making HTTP requests. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/azure.py`\n\n|  |  |\n| --- | --- |\n| ```  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 ``` | ``` def __init__(     self,     *,     azure_endpoint: str | None = None,     api_version: str | None = None,     api_key: str | None = None,     openai_client: AsyncAzureOpenAI | None = None,     http_client: httpx.AsyncClient | None = None, ) -> None:     \"\"\"Create a new Azure provider.      Args:         azure_endpoint: The Azure endpoint to use for authentication, if not provided, the `AZURE_OPENAI_ENDPOINT`             environment variable will be used if available.         api_version: The API version to use for authentication, if not provided, the `OPENAI_API_VERSION`             environment variable will be used if available.         api_key: The API key to use for authentication, if not provided, the `AZURE_OPENAI_API_KEY` environment variable             will be used if available.         openai_client: An existing             [`AsyncAzureOpenAI`](https://github.com/openai/openai-python#microsoft-azure-openai)             client to use. If provided, `base_url`, `api_key`, and `http_client` must be `None`.         http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.     \"\"\"     if openai_client is not None:         assert azure_endpoint is None, 'Cannot provide both `openai_client` and `azure_endpoint`'         assert http_client is None, 'Cannot provide both `openai_client` and `http_client`'         assert api_key is None, 'Cannot provide both `openai_client` and `api_key`'         self._base_url = str(openai_client.base_url)         self._client = openai_client     else:         azure_endpoint = azure_endpoint or os.getenv('AZURE_OPENAI_ENDPOINT')         if not azure_endpoint:             raise UserError(                 'Must provide one of the `azure_endpoint` argument or the `AZURE_OPENAI_ENDPOINT` environment variable'             )          if not api_key and 'AZURE_OPENAI_API_KEY' not in os.environ:  # pragma: no cover             raise UserError(                 'Must provide one of the `api_key` argument or the `AZURE_OPENAI_API_KEY` environment variable'             )          if not api_version and 'OPENAI_API_VERSION' not in os.environ:  # pragma: no cover             raise UserError(                 'Must provide one of the `api_version` argument or the `OPENAI_API_VERSION` environment variable'             )          http_client = http_client or cached_async_http_client(provider='azure')         self._client = AsyncAzureOpenAI(             azure_endpoint=azure_endpoint,             api_key=api_key,             api_version=api_version,             http_client=http_client,         )         self._base_url = str(self._client.base_url) ``` |", "url": "https://ai.pydantic.dev/providers/index.html#azureprovider", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "CohereProvider", "anchor": "cohereprovider", "md_text": "Bases: `Provider[AsyncClientV2]`\n\nProvider for Cohere API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/cohere.py`\n\n|  |  |\n| --- | --- |\n| ``` 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 ``` | ``` class CohereProvider(Provider[AsyncClientV2]):     \"\"\"Provider for Cohere API.\"\"\"      @property     def name(self) -> str:         return 'cohere'      @property     def base_url(self) -> str:         client_wrapper = self.client._client_wrapper  # type: ignore         return str(client_wrapper.get_base_url())      @property     def client(self) -> AsyncClientV2:         return self._client      def model_profile(self, model_name: str) -> ModelProfile | None:         return cohere_model_profile(model_name)      def __init__(         self,         *,         api_key: str | None = None,         cohere_client: AsyncClientV2 | None = None,         http_client: httpx.AsyncClient | None = None,     ) -> None:         \"\"\"Create a new Cohere provider.          Args:             api_key: The API key to use for authentication, if not provided, the `CO_API_KEY` environment variable                 will be used if available.             cohere_client: An existing                 [AsyncClientV2](https://github.com/cohere-ai/cohere-python)                 client to use. If provided, `api_key` and `http_client` must be `None`.             http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.         \"\"\"         if cohere_client is not None:             assert http_client is None, 'Cannot provide both `cohere_client` and `http_client`'             assert api_key is None, 'Cannot provide both `cohere_client` and `api_key`'             self._client = cohere_client         else:             api_key = api_key or os.getenv('CO_API_KEY')             if not api_key:                 raise UserError(                     'Set the `CO_API_KEY` environment variable or pass it via `CohereProvider(api_key=...)`'                     'to use the Cohere provider.'                 )              base_url = os.getenv('CO_BASE_URL')             if http_client is not None:                 self._client = AsyncClientV2(api_key=api_key, httpx_client=http_client, base_url=base_url)             else:                 http_client = cached_async_http_client(provider='cohere')                 self._client = AsyncClientV2(api_key=api_key, httpx_client=http_client, base_url=base_url) ``` |\n\n#### \\_\\_init\\_\\_\n\n```\n__init__(\n    *,\n    api_key: str | None = None,\n    cohere_client: AsyncClientV2 | None = None,\n    http_client: AsyncClient | None = None\n) -> None\n```\n\nCreate a new Cohere provider.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `api_key` | `str | None` | The API key to use for authentication, if not provided, the `CO_API_KEY` environment variable will be used if available. | `None` |\n| `cohere_client` | `AsyncClientV2 | None` | An existing [AsyncClientV2](https://github.com/cohere-ai/cohere-python) client to use. If provided, `api_key` and `http_client` must be `None`. | `None` |\n| `http_client` | `AsyncClient | None` | An existing `httpx.AsyncClient` to use for making HTTP requests. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/cohere.py`", "url": "https://ai.pydantic.dev/providers/index.html#cohereprovider", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "CohereProvider", "anchor": "cohereprovider", "md_text": "|  |  |\n| --- | --- |\n| ``` 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 ``` | ``` def __init__(     self,     *,     api_key: str | None = None,     cohere_client: AsyncClientV2 | None = None,     http_client: httpx.AsyncClient | None = None, ) -> None:     \"\"\"Create a new Cohere provider.      Args:         api_key: The API key to use for authentication, if not provided, the `CO_API_KEY` environment variable             will be used if available.         cohere_client: An existing             [AsyncClientV2](https://github.com/cohere-ai/cohere-python)             client to use. If provided, `api_key` and `http_client` must be `None`.         http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.     \"\"\"     if cohere_client is not None:         assert http_client is None, 'Cannot provide both `cohere_client` and `http_client`'         assert api_key is None, 'Cannot provide both `cohere_client` and `api_key`'         self._client = cohere_client     else:         api_key = api_key or os.getenv('CO_API_KEY')         if not api_key:             raise UserError(                 'Set the `CO_API_KEY` environment variable or pass it via `CohereProvider(api_key=...)`'                 'to use the Cohere provider.'             )          base_url = os.getenv('CO_BASE_URL')         if http_client is not None:             self._client = AsyncClientV2(api_key=api_key, httpx_client=http_client, base_url=base_url)         else:             http_client = cached_async_http_client(provider='cohere')             self._client = AsyncClientV2(api_key=api_key, httpx_client=http_client, base_url=base_url) ``` |\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for Cerebras API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/cerebras.py`", "url": "https://ai.pydantic.dev/providers/index.html#cohereprovider", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "CohereProvider", "anchor": "cohereprovider", "md_text": "|  |  |\n| --- | --- |\n| ``` 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 ``` | ``` class CerebrasProvider(Provider[AsyncOpenAI]):     \"\"\"Provider for Cerebras API.\"\"\"      @property     def name(self) -> str:         return 'cerebras'      @property     def base_url(self) -> str:         return 'https://api.cerebras.ai/v1'      @property     def client(self) -> AsyncOpenAI:         return self._client      def model_profile(self, model_name: str) -> ModelProfile | None:         prefix_to_profile = {'llama': meta_model_profile, 'qwen': qwen_model_profile, 'gpt-oss': harmony_model_profile}          profile = None         for prefix, profile_func in prefix_to_profile.items():             model_name = model_name.lower()             if model_name.startswith(prefix):                 profile = profile_func(model_name)          # According to https://inference-docs.cerebras.ai/resources/openai#currently-unsupported-openai-features,         # Cerebras doesn't support some model settings.         unsupported_model_settings = (             'frequency_penalty',             'logit_bias',             'presence_penalty',             'parallel_tool_calls',             'service_tier',         )         return OpenAIModelProfile(             json_schema_transformer=OpenAIJsonSchemaTransformer,             openai_unsupported_model_settings=unsupported_model_settings,         ).update(profile)      @overload     def __init__(self) -> None: ...      @overload     def __init__(self, *, api_key: str) -> None: ...      @overload     def __init__(self, *, api_key: str, http_client: httpx.AsyncClient) -> None: ...      @overload     def __init__(self, *, openai_client: AsyncOpenAI | None = None) -> None: ...      def __init__(         self,         *,         api_key: str | None = None,         openai_client: AsyncOpenAI | None = None,         http_client: httpx.AsyncClient | None = None,     ) -> None:         api_key = api_key or os.getenv('CEREBRAS_API_KEY')         if not api_key and openai_client is None:             raise UserError(                 'Set the `CEREBRAS_API_KEY` environment variable or pass it via `CerebrasProvider(api_key=...)` '                 'to use the Cerebras provider.'             )          if openai_client is not None:             self._client = openai_client         elif http_client is not None:             self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)         else:             http_client = cached_async_http_client(provider='cerebras')             self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client) ``` |\n\nBases: `Provider[Mistral]`\n\nProvider for Mistral API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/mistral.py`", "url": "https://ai.pydantic.dev/providers/index.html#cohereprovider", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "CohereProvider", "anchor": "cohereprovider", "md_text": "|  |  |\n| --- | --- |\n| ``` 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 ``` | ``` class MistralProvider(Provider[Mistral]):     \"\"\"Provider for Mistral API.\"\"\"      @property     def name(self) -> str:         return 'mistral'      @property     def base_url(self) -> str:         return self.client.sdk_configuration.get_server_details()[0]      @property     def client(self) -> Mistral:         return self._client      def model_profile(self, model_name: str) -> ModelProfile | None:         return mistral_model_profile(model_name)      @overload     def __init__(self, *, mistral_client: Mistral | None = None) -> None: ...      @overload     def __init__(self, *, api_key: str | None = None, http_client: httpx.AsyncClient | None = None) -> None: ...      def __init__(         self,         *,         api_key: str | None = None,         mistral_client: Mistral | None = None,         base_url: str | None = None,         http_client: httpx.AsyncClient | None = None,     ) -> None:         \"\"\"Create a new Mistral provider.          Args:             api_key: The API key to use for authentication, if not provided, the `MISTRAL_API_KEY` environment variable                 will be used if available.             mistral_client: An existing `Mistral` client to use, if provided, `api_key` and `http_client` must be `None`.             base_url: The base url for the Mistral requests.             http_client: An existing async client to use for making HTTP requests.         \"\"\"         if mistral_client is not None:             assert http_client is None, 'Cannot provide both `mistral_client` and `http_client`'             assert api_key is None, 'Cannot provide both `mistral_client` and `api_key`'             assert base_url is None, 'Cannot provide both `mistral_client` and `base_url`'             self._client = mistral_client         else:             api_key = api_key or os.getenv('MISTRAL_API_KEY')              if not api_key:                 raise UserError(                     'Set the `MISTRAL_API_KEY` environment variable or pass it via `MistralProvider(api_key=...)`'                     'to use the Mistral provider.'                 )             elif http_client is not None:                 self._client = Mistral(api_key=api_key, async_client=http_client, server_url=base_url)             else:                 http_client = cached_async_http_client(provider='mistral')                 self._client = Mistral(api_key=api_key, async_client=http_client, server_url=base_url) ``` |", "url": "https://ai.pydantic.dev/providers/index.html#cohereprovider", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "\\_\\_init\\_\\_", "anchor": "init", "md_text": "```\n__init__(*, mistral_client: Mistral | None = None) -> None\n\n__init__(\n    *,\n    api_key: str | None = None,\n    http_client: AsyncClient | None = None\n) -> None\n\n__init__(\n    *,\n    api_key: str | None = None,\n    mistral_client: Mistral | None = None,\n    base_url: str | None = None,\n    http_client: AsyncClient | None = None\n) -> None\n```\n\nCreate a new Mistral provider.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `api_key` | `str | None` | The API key to use for authentication, if not provided, the `MISTRAL_API_KEY` environment variable will be used if available. | `None` |\n| `mistral_client` | `Mistral | None` | An existing `Mistral` client to use, if provided, `api_key` and `http_client` must be `None`. | `None` |\n| `base_url` | `str | None` | The base url for the Mistral requests. | `None` |\n| `http_client` | `AsyncClient | None` | An existing async client to use for making HTTP requests. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/mistral.py`\n\n|  |  |\n| --- | --- |\n| ``` 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 ``` | ``` def __init__(     self,     *,     api_key: str | None = None,     mistral_client: Mistral | None = None,     base_url: str | None = None,     http_client: httpx.AsyncClient | None = None, ) -> None:     \"\"\"Create a new Mistral provider.      Args:         api_key: The API key to use for authentication, if not provided, the `MISTRAL_API_KEY` environment variable             will be used if available.         mistral_client: An existing `Mistral` client to use, if provided, `api_key` and `http_client` must be `None`.         base_url: The base url for the Mistral requests.         http_client: An existing async client to use for making HTTP requests.     \"\"\"     if mistral_client is not None:         assert http_client is None, 'Cannot provide both `mistral_client` and `http_client`'         assert api_key is None, 'Cannot provide both `mistral_client` and `api_key`'         assert base_url is None, 'Cannot provide both `mistral_client` and `base_url`'         self._client = mistral_client     else:         api_key = api_key or os.getenv('MISTRAL_API_KEY')          if not api_key:             raise UserError(                 'Set the `MISTRAL_API_KEY` environment variable or pass it via `MistralProvider(api_key=...)`'                 'to use the Mistral provider.'             )         elif http_client is not None:             self._client = Mistral(api_key=api_key, async_client=http_client, server_url=base_url)         else:             http_client = cached_async_http_client(provider='mistral')             self._client = Mistral(api_key=api_key, async_client=http_client, server_url=base_url) ``` |\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for Fireworks AI API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/fireworks.py`", "url": "https://ai.pydantic.dev/providers/index.html#init", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "\\_\\_init\\_\\_", "anchor": "init", "md_text": "|  |  |\n| --- | --- |\n| ``` 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 ``` | ``` class FireworksProvider(Provider[AsyncOpenAI]):     \"\"\"Provider for Fireworks AI API.\"\"\"      @property     def name(self) -> str:         return 'fireworks'      @property     def base_url(self) -> str:         return 'https://api.fireworks.ai/inference/v1'      @property     def client(self) -> AsyncOpenAI:         return self._client      def model_profile(self, model_name: str) -> ModelProfile | None:         prefix_to_profile = {             'llama': meta_model_profile,             'qwen': qwen_model_profile,             'deepseek': deepseek_model_profile,             'mistral': mistral_model_profile,             'gemma': google_model_profile,         }          prefix = 'accounts/fireworks/models/'          profile = None         if model_name.startswith(prefix):             model_name = model_name[len(prefix) :]             for provider, profile_func in prefix_to_profile.items():                 if model_name.startswith(provider):                     profile = profile_func(model_name)                     break          # As the Fireworks API is OpenAI-compatible, let's assume we also need OpenAIJsonSchemaTransformer,         # unless json_schema_transformer is set explicitly         return OpenAIModelProfile(json_schema_transformer=OpenAIJsonSchemaTransformer).update(profile)      @overload     def __init__(self) -> None: ...      @overload     def __init__(self, *, api_key: str) -> None: ...      @overload     def __init__(self, *, api_key: str, http_client: httpx.AsyncClient) -> None: ...      @overload     def __init__(self, *, openai_client: AsyncOpenAI | None = None) -> None: ...      def __init__(         self,         *,         api_key: str | None = None,         openai_client: AsyncOpenAI | None = None,         http_client: httpx.AsyncClient | None = None,     ) -> None:         api_key = api_key or os.getenv('FIREWORKS_API_KEY')         if not api_key and openai_client is None:             raise UserError(                 'Set the `FIREWORKS_API_KEY` environment variable or pass it via `FireworksProvider(api_key=...)`'                 'to use the Fireworks AI provider.'             )          if openai_client is not None:             self._client = openai_client         elif http_client is not None:             self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)         else:             http_client = cached_async_http_client(provider='fireworks')             self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client) ``` |\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for Grok API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/grok.py`", "url": "https://ai.pydantic.dev/providers/index.html#init", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "\\_\\_init\\_\\_", "anchor": "init", "md_text": "|  |  |\n| --- | --- |\n| ``` 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 ``` | ``` class GrokProvider(Provider[AsyncOpenAI]):     \"\"\"Provider for Grok API.\"\"\"      @property     def name(self) -> str:         return 'grok'      @property     def base_url(self) -> str:         return 'https://api.x.ai/v1'      @property     def client(self) -> AsyncOpenAI:         return self._client      def model_profile(self, model_name: str) -> ModelProfile | None:         profile = grok_model_profile(model_name)          # As the Grok API is OpenAI-compatible, let's assume we also need OpenAIJsonSchemaTransformer,         # unless json_schema_transformer is set explicitly.         # Also, Grok does not support strict tool definitions: https://github.com/pydantic/pydantic-ai/issues/1846         return OpenAIModelProfile(             json_schema_transformer=OpenAIJsonSchemaTransformer, openai_supports_strict_tool_definition=False         ).update(profile)      @overload     def __init__(self) -> None: ...      @overload     def __init__(self, *, api_key: str) -> None: ...      @overload     def __init__(self, *, api_key: str, http_client: httpx.AsyncClient) -> None: ...      @overload     def __init__(self, *, openai_client: AsyncOpenAI | None = None) -> None: ...      def __init__(         self,         *,         api_key: str | None = None,         openai_client: AsyncOpenAI | None = None,         http_client: httpx.AsyncClient | None = None,     ) -> None:         api_key = api_key or os.getenv('GROK_API_KEY')         if not api_key and openai_client is None:             raise UserError(                 'Set the `GROK_API_KEY` environment variable or pass it via `GrokProvider(api_key=...)`'                 'to use the Grok provider.'             )          if openai_client is not None:             self._client = openai_client         elif http_client is not None:             self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)         else:             http_client = cached_async_http_client(provider='grok')             self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client) ``` |\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for Together AI API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/together.py`", "url": "https://ai.pydantic.dev/providers/index.html#init", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "\\_\\_init\\_\\_", "anchor": "init", "md_text": "|  |  |\n| --- | --- |\n| ``` 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 ``` | ``` class TogetherProvider(Provider[AsyncOpenAI]):     \"\"\"Provider for Together AI API.\"\"\"      @property     def name(self) -> str:         return 'together'      @property     def base_url(self) -> str:         return 'https://api.together.xyz/v1'      @property     def client(self) -> AsyncOpenAI:         return self._client      def model_profile(self, model_name: str) -> ModelProfile | None:         provider_to_profile = {             'deepseek-ai': deepseek_model_profile,             'google': google_model_profile,             'qwen': qwen_model_profile,             'meta-llama': meta_model_profile,             'mistralai': mistral_model_profile,         }          profile = None          model_name = model_name.lower()         provider, model_name = model_name.split('/', 1)         if provider in provider_to_profile:             profile = provider_to_profile[provider](model_name)          # As the Together API is OpenAI-compatible, let's assume we also need OpenAIJsonSchemaTransformer,         # unless json_schema_transformer is set explicitly         return OpenAIModelProfile(json_schema_transformer=OpenAIJsonSchemaTransformer).update(profile)      @overload     def __init__(self) -> None: ...      @overload     def __init__(self, *, api_key: str) -> None: ...      @overload     def __init__(self, *, api_key: str, http_client: httpx.AsyncClient) -> None: ...      @overload     def __init__(self, *, openai_client: AsyncOpenAI | None = None) -> None: ...      def __init__(         self,         *,         api_key: str | None = None,         openai_client: AsyncOpenAI | None = None,         http_client: httpx.AsyncClient | None = None,     ) -> None:         api_key = api_key or os.getenv('TOGETHER_API_KEY')         if not api_key and openai_client is None:             raise UserError(                 'Set the `TOGETHER_API_KEY` environment variable or pass it via `TogetherProvider(api_key=...)`'                 'to use the Together AI provider.'             )          if openai_client is not None:             self._client = openai_client         elif http_client is not None:             self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)         else:             http_client = cached_async_http_client(provider='together')             self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client) ``` |\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for Heroku API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/heroku.py`", "url": "https://ai.pydantic.dev/providers/index.html#init", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "\\_\\_init\\_\\_", "anchor": "init", "md_text": "|  |  |\n| --- | --- |\n| ``` 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 ``` | ``` class HerokuProvider(Provider[AsyncOpenAI]):     \"\"\"Provider for Heroku API.\"\"\"      @property     def name(self) -> str:         return 'heroku'      @property     def base_url(self) -> str:         return str(self.client.base_url)      @property     def client(self) -> AsyncOpenAI:         return self._client      def model_profile(self, model_name: str) -> ModelProfile | None:         # As the Heroku API is OpenAI-compatible, let's assume we also need OpenAIJsonSchemaTransformer.         return OpenAIModelProfile(json_schema_transformer=OpenAIJsonSchemaTransformer)      @overload     def __init__(self) -> None: ...      @overload     def __init__(self, *, api_key: str) -> None: ...      @overload     def __init__(self, *, api_key: str, http_client: httpx.AsyncClient) -> None: ...      @overload     def __init__(self, *, openai_client: AsyncOpenAI | None = None) -> None: ...      def __init__(         self,         *,         base_url: str | None = None,         api_key: str | None = None,         openai_client: AsyncOpenAI | None = None,         http_client: httpx.AsyncClient | None = None,     ) -> None:         if openai_client is not None:             assert http_client is None, 'Cannot provide both `openai_client` and `http_client`'             assert api_key is None, 'Cannot provide both `openai_client` and `api_key`'             self._client = openai_client         else:             api_key = api_key or os.getenv('HEROKU_INFERENCE_KEY')             if not api_key:                 raise UserError(                     'Set the `HEROKU_INFERENCE_KEY` environment variable or pass it via `HerokuProvider(api_key=...)`'                     'to use the Heroku provider.'                 )              base_url = base_url or os.getenv('HEROKU_INFERENCE_URL', 'https://us.inference.heroku.com')             base_url = base_url.rstrip('/') + '/v1'              if http_client is not None:                 self._client = AsyncOpenAI(api_key=api_key, http_client=http_client, base_url=base_url)             else:                 http_client = cached_async_http_client(provider='heroku')                 self._client = AsyncOpenAI(api_key=api_key, http_client=http_client, base_url=base_url) ``` |\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for GitHub Models API.\n\nGitHub Models provides access to various AI models through an OpenAI-compatible API.\nSee <https://docs.github.com/en/github-models> for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/github.py`", "url": "https://ai.pydantic.dev/providers/index.html#init", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "\\_\\_init\\_\\_", "anchor": "init", "md_text": "|  |  |\n| --- | --- |\n| ```  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 ``` | ``` class GitHubProvider(Provider[AsyncOpenAI]):     \"\"\"Provider for GitHub Models API.      GitHub Models provides access to various AI models through an OpenAI-compatible API.     See <https://docs.github.com/en/github-models> for more information.     \"\"\"      @property     def name(self) -> str:         return 'github'      @property     def base_url(self) -> str:         return 'https://models.github.ai/inference'      @property     def client(self) -> AsyncOpenAI:         return self._client      def model_profile(self, model_name: str) -> ModelProfile | None:         provider_to_profile = {             'xai': grok_model_profile,             'meta': meta_model_profile,             'microsoft': openai_model_profile,             'mistral-ai': mistral_model_profile,             'cohere': cohere_model_profile,             'deepseek': deepseek_model_profile,         }          profile = None          # If the model name does not contain a provider prefix, we assume it's an OpenAI model         if '/' not in model_name:             return openai_model_profile(model_name)          provider, model_name = model_name.lower().split('/', 1)         if provider in provider_to_profile:             model_name, *_ = model_name.split(':', 1)  # drop tags             profile = provider_to_profile[provider](model_name)          # As GitHubProvider is always used with OpenAIChatModel, which used to unconditionally use OpenAIJsonSchemaTransformer,         # we need to maintain that behavior unless json_schema_transformer is set explicitly         return OpenAIModelProfile(json_schema_transformer=OpenAIJsonSchemaTransformer).update(profile)      @overload     def __init__(self) -> None: ...      @overload     def __init__(self, *, api_key: str) -> None: ...      @overload     def __init__(self, *, api_key: str, http_client: httpx.AsyncClient) -> None: ...      @overload     def __init__(self, *, openai_client: AsyncOpenAI | None = None) -> None: ...      def __init__(         self,         *,         api_key: str | None = None,         openai_client: AsyncOpenAI | None = None,         http_client: httpx.AsyncClient | None = None,     ) -> None:         \"\"\"Create a new GitHub Models provider.          Args:             api_key: The GitHub token to use for authentication. If not provided, the `GITHUB_API_KEY`                 environment variable will be used if available.             openai_client: An existing `AsyncOpenAI` client to use. If provided, `api_key` and `http_client` must be `None`.             http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.         \"\"\"         api_key = api_key or os.getenv('GITHUB_API_KEY')         if not api_key and openai_client is None:             raise UserError(                 'Set the `GITHUB_API_KEY` environment variable or pass it via `GitHubProvider(api_key=...)`'                 ' to use the GitHub Models provider.'             )          if openai_client is not None:             self._client = openai_client         elif http_client is not None:             self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)         else:             http_client = cached_async_http_client(provider='github')             self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client) ``` |", "url": "https://ai.pydantic.dev/providers/index.html#init", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "\\_\\_init\\_\\_", "anchor": "init", "md_text": "```\n__init__() -> None\n\n__init__(*, api_key: str) -> None\n\n__init__(*, api_key: str, http_client: AsyncClient) -> None\n\n__init__(\n    *, openai_client: AsyncOpenAI | None = None\n) -> None\n\n__init__(\n    *,\n    api_key: str | None = None,\n    openai_client: AsyncOpenAI | None = None,\n    http_client: AsyncClient | None = None\n) -> None\n```\n\nCreate a new GitHub Models provider.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `api_key` | `str | None` | The GitHub token to use for authentication. If not provided, the `GITHUB_API_KEY` environment variable will be used if available. | `None` |\n| `openai_client` | `AsyncOpenAI | None` | An existing `AsyncOpenAI` client to use. If provided, `api_key` and `http_client` must be `None`. | `None` |\n| `http_client` | `AsyncClient | None` | An existing `httpx.AsyncClient` to use for making HTTP requests. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/github.py`\n\n|  |  |\n| --- | --- |\n| ```  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 ``` | ``` def __init__(     self,     *,     api_key: str | None = None,     openai_client: AsyncOpenAI | None = None,     http_client: httpx.AsyncClient | None = None, ) -> None:     \"\"\"Create a new GitHub Models provider.      Args:         api_key: The GitHub token to use for authentication. If not provided, the `GITHUB_API_KEY`             environment variable will be used if available.         openai_client: An existing `AsyncOpenAI` client to use. If provided, `api_key` and `http_client` must be `None`.         http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.     \"\"\"     api_key = api_key or os.getenv('GITHUB_API_KEY')     if not api_key and openai_client is None:         raise UserError(             'Set the `GITHUB_API_KEY` environment variable or pass it via `GitHubProvider(api_key=...)`'             ' to use the GitHub Models provider.'         )      if openai_client is not None:         self._client = openai_client     elif http_client is not None:         self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)     else:         http_client = cached_async_http_client(provider='github')         self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client) ``` |\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for OpenRouter API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/openrouter.py`", "url": "https://ai.pydantic.dev/providers/index.html#init", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "\\_\\_init\\_\\_", "anchor": "init", "md_text": "|  |  |\n| --- | --- |\n| ```  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 ``` | ``` class OpenRouterProvider(Provider[AsyncOpenAI]):     \"\"\"Provider for OpenRouter API.\"\"\"      @property     def name(self) -> str:         return 'openrouter'      @property     def base_url(self) -> str:         return 'https://openrouter.ai/api/v1'      @property     def client(self) -> AsyncOpenAI:         return self._client      def model_profile(self, model_name: str) -> ModelProfile | None:         provider_to_profile = {             'google': google_model_profile,             'openai': openai_model_profile,             'anthropic': anthropic_model_profile,             'mistralai': mistral_model_profile,             'qwen': qwen_model_profile,             'x-ai': grok_model_profile,             'cohere': cohere_model_profile,             'amazon': amazon_model_profile,             'deepseek': deepseek_model_profile,             'meta-llama': meta_model_profile,             'moonshotai': moonshotai_model_profile,         }          profile = None          provider, model_name = model_name.split('/', 1)         if provider in provider_to_profile:             model_name, *_ = model_name.split(':', 1)  # drop tags             profile = provider_to_profile[provider](model_name)          # As OpenRouterProvider is always used with OpenAIChatModel, which used to unconditionally use OpenAIJsonSchemaTransformer,         # we need to maintain that behavior unless json_schema_transformer is set explicitly         return OpenAIModelProfile(json_schema_transformer=OpenAIJsonSchemaTransformer).update(profile)      @overload     def __init__(self) -> None: ...      @overload     def __init__(self, *, api_key: str) -> None: ...      @overload     def __init__(self, *, api_key: str, http_client: httpx.AsyncClient) -> None: ...      @overload     def __init__(self, *, openai_client: AsyncOpenAI | None = None) -> None: ...      def __init__(         self,         *,         api_key: str | None = None,         openai_client: AsyncOpenAI | None = None,         http_client: httpx.AsyncClient | None = None,     ) -> None:         api_key = api_key or os.getenv('OPENROUTER_API_KEY')         if not api_key and openai_client is None:             raise UserError(                 'Set the `OPENROUTER_API_KEY` environment variable or pass it via `OpenRouterProvider(api_key=...)`'                 'to use the OpenRouter provider.'             )          if openai_client is not None:             self._client = openai_client         elif http_client is not None:             self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)         else:             http_client = cached_async_http_client(provider='openrouter')             self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client) ``` |\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for Vercel AI Gateway API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/vercel.py`", "url": "https://ai.pydantic.dev/providers/index.html#init", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "\\_\\_init\\_\\_", "anchor": "init", "md_text": "|  |  |\n| --- | --- |\n| ```  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 ``` | ``` class VercelProvider(Provider[AsyncOpenAI]):     \"\"\"Provider for Vercel AI Gateway API.\"\"\"      @property     def name(self) -> str:         return 'vercel'      @property     def base_url(self) -> str:         return 'https://ai-gateway.vercel.sh/v1'      @property     def client(self) -> AsyncOpenAI:         return self._client      def model_profile(self, model_name: str) -> ModelProfile | None:         provider_to_profile = {             'anthropic': anthropic_model_profile,             'bedrock': amazon_model_profile,             'cohere': cohere_model_profile,             'deepseek': deepseek_model_profile,             'mistral': mistral_model_profile,             'openai': openai_model_profile,             'vertex': google_model_profile,             'xai': grok_model_profile,         }          profile = None          try:             provider, model_name = model_name.split('/', 1)         except ValueError:             raise UserError(f\"Model name must be in 'provider/model' format, got: {model_name!r}\")          if provider in provider_to_profile:             profile = provider_to_profile[provider](model_name)          # As VercelProvider is always used with OpenAIChatModel, which used to unconditionally use OpenAIJsonSchemaTransformer,         # we need to maintain that behavior unless json_schema_transformer is set explicitly         return OpenAIModelProfile(             json_schema_transformer=OpenAIJsonSchemaTransformer,         ).update(profile)      @overload     def __init__(self) -> None: ...      @overload     def __init__(self, *, api_key: str) -> None: ...      @overload     def __init__(self, *, api_key: str, http_client: httpx.AsyncClient) -> None: ...      @overload     def __init__(self, *, openai_client: AsyncOpenAI | None = None) -> None: ...      def __init__(         self,         *,         api_key: str | None = None,         openai_client: AsyncOpenAI | None = None,         http_client: httpx.AsyncClient | None = None,     ) -> None:         # Support Vercel AI Gateway's standard environment variables         api_key = api_key or os.getenv('VERCEL_AI_GATEWAY_API_KEY') or os.getenv('VERCEL_OIDC_TOKEN')          if not api_key and openai_client is None:             raise UserError(                 'Set the `VERCEL_AI_GATEWAY_API_KEY` or `VERCEL_OIDC_TOKEN` environment variable '                 'or pass the API key via `VercelProvider(api_key=...)` to use the Vercel provider.'             )          default_headers = {'http-referer': 'https://ai.pydantic.dev/', 'x-title': 'pydantic-ai'}          if openai_client is not None:             self._client = openai_client         elif http_client is not None:             self._client = AsyncOpenAI(                 base_url=self.base_url, api_key=api_key, http_client=http_client, default_headers=default_headers             )         else:             http_client = cached_async_http_client(provider='vercel')             self._client = AsyncOpenAI(                 base_url=self.base_url, api_key=api_key, http_client=http_client, default_headers=default_headers             ) ``` |\n\nBases: `Provider[AsyncInferenceClient]`\n\nProvider for Hugging Face.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/huggingface.py`", "url": "https://ai.pydantic.dev/providers/index.html#init", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "\\_\\_init\\_\\_", "anchor": "init", "md_text": "|  |  |\n| --- | --- |\n| ```  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 ``` | ``` class HuggingFaceProvider(Provider[AsyncInferenceClient]):     \"\"\"Provider for Hugging Face.\"\"\"      @property     def name(self) -> str:         return 'huggingface'      @property     def base_url(self) -> str:         return self.client.model  # type: ignore      @property     def client(self) -> AsyncInferenceClient:         return self._client      def model_profile(self, model_name: str) -> ModelProfile | None:         provider_to_profile = {             'deepseek-ai': deepseek_model_profile,             'google': google_model_profile,             'qwen': qwen_model_profile,             'meta-llama': meta_model_profile,             'mistralai': mistral_model_profile,             'moonshotai': moonshotai_model_profile,         }          if '/' not in model_name:             return None          model_name = model_name.lower()         provider, model_name = model_name.split('/', 1)         if provider in provider_to_profile:             return provider_to_profile[provider](model_name)          return None      @overload     def __init__(self, *, base_url: str, api_key: str | None = None) -> None: ...     @overload     def __init__(self, *, provider_name: str, api_key: str | None = None) -> None: ...     @overload     def __init__(self, *, hf_client: AsyncInferenceClient, api_key: str | None = None) -> None: ...     @overload     def __init__(self, *, hf_client: AsyncInferenceClient, base_url: str, api_key: str | None = None) -> None: ...     @overload     def __init__(self, *, hf_client: AsyncInferenceClient, provider_name: str, api_key: str | None = None) -> None: ...     @overload     def __init__(self, *, api_key: str | None = None) -> None: ...      def __init__(         self,         base_url: str | None = None,         api_key: str | None = None,         hf_client: AsyncInferenceClient | None = None,         http_client: AsyncClient | None = None,         provider_name: str | None = None,     ) -> None:         \"\"\"Create a new Hugging Face provider.          Args:             base_url: The base url for the Hugging Face requests.             api_key: The API key to use for authentication, if not provided, the `HF_TOKEN` environment variable                 will be used if available.             hf_client: An existing                 [`AsyncInferenceClient`](https://huggingface.co/docs/huggingface_hub/v0.29.3/en/package_reference/inference_client#huggingface_hub.AsyncInferenceClient)                 client to use. If not provided, a new instance will be created.             http_client: (currently ignored) An existing `httpx.AsyncClient` to use for making HTTP requests.             provider_name : Name of the provider to use for inference. available providers can be found in the [HF Inference Providers documentation](https://huggingface.co/docs/inference-providers/index#partners).                 defaults to \"auto\", which will select the first available provider for the model, the first of the providers available for the model, sorted by the user's order in https://hf.co/settings/inference-providers.                 If `base_url` is passed, then `provider_name` is not used.         \"\"\"         api_key = api_key or os.getenv('HF_TOKEN')          if api_key is None:             raise UserError(                 'Set the `HF_TOKEN` environment variable or pass it via `HuggingFaceProvider(api_key=...)`'                 'to use the HuggingFace provider.'             )          if http_client is not None:             raise ValueError('`http_client` is ignored for HuggingFace provider, please use `hf_client` instead.')          if base_url is not None and provider_name is not None:             raise ValueError('Cannot provide both `base_url` and `provider_name`.')          if hf_client is None:             self._client = AsyncInferenceClient(api_key=api_key, provider=provider_name, base_url=base_url)  # type: ignore         else:             self._client = hf_client ``` |", "url": "https://ai.pydantic.dev/providers/index.html#init", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "\\_\\_init\\_\\_", "anchor": "init", "md_text": "```\n__init__(\n    *, base_url: str, api_key: str | None = None\n) -> None\n\n__init__(\n    *, provider_name: str, api_key: str | None = None\n) -> None\n\n__init__(\n    *,\n    hf_client: AsyncInferenceClient,\n    api_key: str | None = None\n) -> None\n\n__init__(\n    *,\n    hf_client: AsyncInferenceClient,\n    base_url: str,\n    api_key: str | None = None\n) -> None\n\n__init__(\n    *,\n    hf_client: AsyncInferenceClient,\n    provider_name: str,\n    api_key: str | None = None\n) -> None\n\n__init__(*, api_key: str | None = None) -> None\n\n__init__(\n    base_url: str | None = None,\n    api_key: str | None = None,\n    hf_client: AsyncInferenceClient | None = None,\n    http_client: AsyncClient | None = None,\n    provider_name: str | None = None,\n) -> None\n```\n\nCreate a new Hugging Face provider.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `base_url` | `str | None` | The base url for the Hugging Face requests. | `None` |\n| `api_key` | `str | None` | The API key to use for authentication, if not provided, the `HF_TOKEN` environment variable will be used if available. | `None` |\n| `hf_client` | `AsyncInferenceClient | None` | An existing [`AsyncInferenceClient`](https://huggingface.co/docs/huggingface_hub/v0.29.3/en/package_reference/inference_client#huggingface_hub.AsyncInferenceClient) client to use. If not provided, a new instance will be created. | `None` |\n| `http_client` | `AsyncClient | None` | (currently ignored) An existing `httpx.AsyncClient` to use for making HTTP requests. | `None` |\n| `provider_name` |  | Name of the provider to use for inference. available providers can be found in the [HF Inference Providers documentation](https://huggingface.co/docs/inference-providers/index#partners). defaults to \"auto\", which will select the first available provider for the model, the first of the providers available for the model, sorted by the user's order in https://hf.co/settings/inference-providers. If `base_url` is passed, then `provider_name` is not used. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/huggingface.py`\n\n|  |  |\n| --- | --- |\n| ```  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 ``` | ``` def __init__(     self,     base_url: str | None = None,     api_key: str | None = None,     hf_client: AsyncInferenceClient | None = None,     http_client: AsyncClient | None = None,     provider_name: str | None = None, ) -> None:     \"\"\"Create a new Hugging Face provider.      Args:         base_url: The base url for the Hugging Face requests.         api_key: The API key to use for authentication, if not provided, the `HF_TOKEN` environment variable             will be used if available.         hf_client: An existing             [`AsyncInferenceClient`](https://huggingface.co/docs/huggingface_hub/v0.29.3/en/package_reference/inference_client#huggingface_hub.AsyncInferenceClient)             client to use. If not provided, a new instance will be created.         http_client: (currently ignored) An existing `httpx.AsyncClient` to use for making HTTP requests.         provider_name : Name of the provider to use for inference. available providers can be found in the [HF Inference Providers documentation](https://huggingface.co/docs/inference-providers/index#partners).             defaults to \"auto\", which will select the first available provider for the model, the first of the providers available for the model, sorted by the user's order in https://hf.co/settings/inference-providers.             If `base_url` is passed, then `provider_name` is not used.     \"\"\"     api_key = api_key or os.getenv('HF_TOKEN')      if api_key is None:         raise UserError(             'Set the `HF_TOKEN` environment variable or pass it via `HuggingFaceProvider(api_key=...)`'             'to use the HuggingFace provider.'         )      if http_client is not None:         raise ValueError('`http_client` is ignored for HuggingFace provider, please use `hf_client` instead.')      if base_url is not None and provider_name is not None:         raise ValueError('Cannot provide both `base_url` and `provider_name`.')      if hf_client is None:         self._client = AsyncInferenceClient(api_key=api_key, provider=provider_name, base_url=base_url)  # type: ignore     else:         self._client = hf_client ``` |\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for MoonshotAI platform (Kimi models).", "url": "https://ai.pydantic.dev/providers/index.html#init", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "\\_\\_init\\_\\_", "anchor": "init", "md_text": "Source code in `pydantic_ai_slim/pydantic_ai/providers/moonshotai.py`\n\n|  |  |\n| --- | --- |\n| ``` 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 ``` | ``` class MoonshotAIProvider(Provider[AsyncOpenAI]):     \"\"\"Provider for MoonshotAI platform (Kimi models).\"\"\"      @property     def name(self) -> str:         return 'moonshotai'      @property     def base_url(self) -> str:         # OpenAI-compatible endpoint, see MoonshotAI docs         return 'https://api.moonshot.ai/v1'      @property     def client(self) -> AsyncOpenAI:         return self._client      def model_profile(self, model_name: str) -> ModelProfile | None:         profile = moonshotai_model_profile(model_name)          # As the MoonshotAI API is OpenAI-compatible, let's assume we also need OpenAIJsonSchemaTransformer,         # unless json_schema_transformer is set explicitly.         # Also, MoonshotAI does not support strict tool definitions         # https://platform.moonshot.ai/docs/guide/migrating-from-openai-to-kimi#about-tool_choice         # \"Please note that the current version of Kimi API does not support the tool_choice=required parameter.\"         return OpenAIModelProfile(             json_schema_transformer=OpenAIJsonSchemaTransformer,             openai_supports_tool_choice_required=False,             supports_json_object_output=True,         ).update(profile)      @overload     def __init__(self) -> None: ...      @overload     def __init__(self, *, api_key: str) -> None: ...      @overload     def __init__(self, *, api_key: str, http_client: httpx.AsyncClient) -> None: ...      @overload     def __init__(self, *, openai_client: AsyncOpenAI | None = None) -> None: ...      def __init__(         self,         *,         api_key: str | None = None,         openai_client: AsyncOpenAI | None = None,         http_client: httpx.AsyncClient | None = None,     ) -> None:         api_key = api_key or os.getenv('MOONSHOTAI_API_KEY')         if not api_key and openai_client is None:             raise UserError(                 'Set the `MOONSHOTAI_API_KEY` environment variable or pass it via '                 '`MoonshotAIProvider(api_key=...)` to use the MoonshotAI provider.'             )          if openai_client is not None:             self._client = openai_client         elif http_client is not None:             self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)         else:             http_client = cached_async_http_client(provider='moonshotai')             self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client) ``` |\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for local or remote Ollama API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/ollama.py`", "url": "https://ai.pydantic.dev/providers/index.html#init", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "\\_\\_init\\_\\_", "anchor": "init", "md_text": "|  |  |\n| --- | --- |\n| ```  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 ``` | ``` class OllamaProvider(Provider[AsyncOpenAI]):     \"\"\"Provider for local or remote Ollama API.\"\"\"      @property     def name(self) -> str:         return 'ollama'      @property     def base_url(self) -> str:         return str(self.client.base_url)      @property     def client(self) -> AsyncOpenAI:         return self._client      def model_profile(self, model_name: str) -> ModelProfile | None:         prefix_to_profile = {             'llama': meta_model_profile,             'gemma': google_model_profile,             'qwen': qwen_model_profile,             'qwq': qwen_model_profile,             'deepseek': deepseek_model_profile,             'mistral': mistral_model_profile,             'command': cohere_model_profile,             'gpt-oss': harmony_model_profile,         }          profile = None         for prefix, profile_func in prefix_to_profile.items():             model_name = model_name.lower()             if model_name.startswith(prefix):                 profile = profile_func(model_name)          # As OllamaProvider is always used with OpenAIChatModel, which used to unconditionally use OpenAIJsonSchemaTransformer,         # we need to maintain that behavior unless json_schema_transformer is set explicitly         return OpenAIModelProfile(json_schema_transformer=OpenAIJsonSchemaTransformer).update(profile)      def __init__(         self,         base_url: str | None = None,         api_key: str | None = None,         openai_client: AsyncOpenAI | None = None,         http_client: httpx.AsyncClient | None = None,     ) -> None:         \"\"\"Create a new Ollama provider.          Args:             base_url: The base url for the Ollama requests. If not provided, the `OLLAMA_BASE_URL` environment variable                 will be used if available.             api_key: The API key to use for authentication, if not provided, the `OLLAMA_API_KEY` environment variable                 will be used if available.             openai_client: An existing                 [`AsyncOpenAI`](https://github.com/openai/openai-python?tab=readme-ov-file#async-usage)                 client to use. If provided, `base_url`, `api_key`, and `http_client` must be `None`.             http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.         \"\"\"         if openai_client is not None:             assert base_url is None, 'Cannot provide both `openai_client` and `base_url`'             assert http_client is None, 'Cannot provide both `openai_client` and `http_client`'             assert api_key is None, 'Cannot provide both `openai_client` and `api_key`'             self._client = openai_client         else:             base_url = base_url or os.getenv('OLLAMA_BASE_URL')             if not base_url:                 raise UserError(                     'Set the `OLLAMA_BASE_URL` environment variable or pass it via `OllamaProvider(base_url=...)`'                     'to use the Ollama provider.'                 )              # This is a workaround for the OpenAI client requiring an API key, whilst locally served,             # openai compatible models do not always need an API key, but a placeholder (non-empty) key is required.             api_key = api_key or os.getenv('OLLAMA_API_KEY') or 'api-key-not-set'              if http_client is not None:                 self._client = AsyncOpenAI(base_url=base_url, api_key=api_key, http_client=http_client)             else:                 http_client = cached_async_http_client(provider='ollama')                 self._client = AsyncOpenAI(base_url=base_url, api_key=api_key, http_client=http_client) ``` |", "url": "https://ai.pydantic.dev/providers/index.html#init", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "\\_\\_init\\_\\_", "anchor": "init", "md_text": "```\n__init__(\n    base_url: str | None = None,\n    api_key: str | None = None,\n    openai_client: AsyncOpenAI | None = None,\n    http_client: AsyncClient | None = None,\n) -> None\n```\n\nCreate a new Ollama provider.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `base_url` | `str | None` | The base url for the Ollama requests. If not provided, the `OLLAMA_BASE_URL` environment variable will be used if available. | `None` |\n| `api_key` | `str | None` | The API key to use for authentication, if not provided, the `OLLAMA_API_KEY` environment variable will be used if available. | `None` |\n| `openai_client` | `AsyncOpenAI | None` | An existing [`AsyncOpenAI`](https://github.com/openai/openai-python?tab=readme-ov-file#async-usage) client to use. If provided, `base_url`, `api_key`, and `http_client` must be `None`. | `None` |\n| `http_client` | `AsyncClient | None` | An existing `httpx.AsyncClient` to use for making HTTP requests. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/ollama.py`\n\n|  |  |\n| --- | --- |\n| ```  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 ``` | ``` def __init__(     self,     base_url: str | None = None,     api_key: str | None = None,     openai_client: AsyncOpenAI | None = None,     http_client: httpx.AsyncClient | None = None, ) -> None:     \"\"\"Create a new Ollama provider.      Args:         base_url: The base url for the Ollama requests. If not provided, the `OLLAMA_BASE_URL` environment variable             will be used if available.         api_key: The API key to use for authentication, if not provided, the `OLLAMA_API_KEY` environment variable             will be used if available.         openai_client: An existing             [`AsyncOpenAI`](https://github.com/openai/openai-python?tab=readme-ov-file#async-usage)             client to use. If provided, `base_url`, `api_key`, and `http_client` must be `None`.         http_client: An existing `httpx.AsyncClient` to use for making HTTP requests.     \"\"\"     if openai_client is not None:         assert base_url is None, 'Cannot provide both `openai_client` and `base_url`'         assert http_client is None, 'Cannot provide both `openai_client` and `http_client`'         assert api_key is None, 'Cannot provide both `openai_client` and `api_key`'         self._client = openai_client     else:         base_url = base_url or os.getenv('OLLAMA_BASE_URL')         if not base_url:             raise UserError(                 'Set the `OLLAMA_BASE_URL` environment variable or pass it via `OllamaProvider(base_url=...)`'                 'to use the Ollama provider.'             )          # This is a workaround for the OpenAI client requiring an API key, whilst locally served,         # openai compatible models do not always need an API key, but a placeholder (non-empty) key is required.         api_key = api_key or os.getenv('OLLAMA_API_KEY') or 'api-key-not-set'          if http_client is not None:             self._client = AsyncOpenAI(base_url=base_url, api_key=api_key, http_client=http_client)         else:             http_client = cached_async_http_client(provider='ollama')             self._client = AsyncOpenAI(base_url=base_url, api_key=api_key, http_client=http_client) ``` |\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for LiteLLM API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/litellm.py`", "url": "https://ai.pydantic.dev/providers/index.html#init", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "\\_\\_init\\_\\_", "anchor": "init", "md_text": "|  |  |\n| --- | --- |\n| ```  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 ``` | ``` class LiteLLMProvider(Provider[AsyncOpenAI]):     \"\"\"Provider for LiteLLM API.\"\"\"      @property     def name(self) -> str:         return 'litellm'      @property     def base_url(self) -> str:         return str(self.client.base_url)      @property     def client(self) -> AsyncOpenAI:         return self._client      def model_profile(self, model_name: str) -> ModelProfile | None:         # Map provider prefixes to their profile functions         provider_to_profile = {             'anthropic': anthropic_model_profile,             'openai': openai_model_profile,             'google': google_model_profile,             'mistralai': mistral_model_profile,             'mistral': mistral_model_profile,             'cohere': cohere_model_profile,             'amazon': amazon_model_profile,             'bedrock': amazon_model_profile,             'meta-llama': meta_model_profile,             'meta': meta_model_profile,             'groq': groq_model_profile,             'deepseek': deepseek_model_profile,             'moonshotai': moonshotai_model_profile,             'x-ai': grok_model_profile,             'qwen': qwen_model_profile,         }          profile = None          # Check if model name contains a provider prefix (e.g., \"anthropic/claude-3\")         if '/' in model_name:             provider_prefix, model_suffix = model_name.split('/', 1)             if provider_prefix in provider_to_profile:                 profile = provider_to_profile[provider_prefix](model_suffix)          # If no profile found, default to OpenAI profile         if profile is None:             profile = openai_model_profile(model_name)          # As LiteLLMProvider is used with OpenAIModel, which uses OpenAIJsonSchemaTransformer,         # we maintain that behavior         return OpenAIModelProfile(json_schema_transformer=OpenAIJsonSchemaTransformer).update(profile)      @overload     def __init__(         self,         *,         api_key: str | None = None,         api_base: str | None = None,     ) -> None: ...      @overload     def __init__(         self,         *,         api_key: str | None = None,         api_base: str | None = None,         http_client: AsyncHTTPClient,     ) -> None: ...      @overload     def __init__(self, *, openai_client: AsyncOpenAI) -> None: ...      def __init__(         self,         *,         api_key: str | None = None,         api_base: str | None = None,         openai_client: AsyncOpenAI | None = None,         http_client: AsyncHTTPClient | None = None,     ) -> None:         \"\"\"Initialize a LiteLLM provider.          Args:             api_key: API key for the model provider. If None, LiteLLM will try to get it from environment variables.             api_base: Base URL for the model provider. Use this for custom endpoints or self-hosted models.             openai_client: Pre-configured OpenAI client. If provided, other parameters are ignored.             http_client: Custom HTTP client to use.         \"\"\"         if openai_client is not None:             self._client = openai_client             return          # Create OpenAI client that will be used with LiteLLM's completion function         # The actual API calls will be intercepted and routed through LiteLLM         if http_client is not None:             self._client = AsyncOpenAI(                 base_url=api_base, api_key=api_key or 'litellm-placeholder', http_client=http_client             )         else:             http_client = cached_async_http_client(provider='litellm')             self._client = AsyncOpenAI(                 base_url=api_base, api_key=api_key or 'litellm-placeholder', http_client=http_client             ) ``` |", "url": "https://ai.pydantic.dev/providers/index.html#init", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "\\_\\_init\\_\\_", "anchor": "init", "md_text": "```\n__init__(\n    *,\n    api_key: str | None = None,\n    api_base: str | None = None\n) -> None\n\n__init__(\n    *,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    http_client: AsyncClient\n) -> None\n\n__init__(*, openai_client: AsyncOpenAI) -> None\n\n__init__(\n    *,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    openai_client: AsyncOpenAI | None = None,\n    http_client: AsyncClient | None = None\n) -> None\n```\n\nInitialize a LiteLLM provider.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `api_key` | `str | None` | API key for the model provider. If None, LiteLLM will try to get it from environment variables. | `None` |\n| `api_base` | `str | None` | Base URL for the model provider. Use this for custom endpoints or self-hosted models. | `None` |\n| `openai_client` | `AsyncOpenAI | None` | Pre-configured OpenAI client. If provided, other parameters are ignored. | `None` |\n| `http_client` | `AsyncClient | None` | Custom HTTP client to use. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/litellm.py`\n\n|  |  |\n| --- | --- |\n| ``` 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 ``` | ``` def __init__(     self,     *,     api_key: str | None = None,     api_base: str | None = None,     openai_client: AsyncOpenAI | None = None,     http_client: AsyncHTTPClient | None = None, ) -> None:     \"\"\"Initialize a LiteLLM provider.      Args:         api_key: API key for the model provider. If None, LiteLLM will try to get it from environment variables.         api_base: Base URL for the model provider. Use this for custom endpoints or self-hosted models.         openai_client: Pre-configured OpenAI client. If provided, other parameters are ignored.         http_client: Custom HTTP client to use.     \"\"\"     if openai_client is not None:         self._client = openai_client         return      # Create OpenAI client that will be used with LiteLLM's completion function     # The actual API calls will be intercepted and routed through LiteLLM     if http_client is not None:         self._client = AsyncOpenAI(             base_url=api_base, api_key=api_key or 'litellm-placeholder', http_client=http_client         )     else:         http_client = cached_async_http_client(provider='litellm')         self._client = AsyncOpenAI(             base_url=api_base, api_key=api_key or 'litellm-placeholder', http_client=http_client         ) ``` |\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for Nebius AI Studio API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/nebius.py`", "url": "https://ai.pydantic.dev/providers/index.html#init", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "\\_\\_init\\_\\_", "anchor": "init", "md_text": "|  |  |\n| --- | --- |\n| ```  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 ``` | ``` class NebiusProvider(Provider[AsyncOpenAI]):     \"\"\"Provider for Nebius AI Studio API.\"\"\"      @property     def name(self) -> str:         return 'nebius'      @property     def base_url(self) -> str:         return 'https://api.studio.nebius.com/v1'      @property     def client(self) -> AsyncOpenAI:         return self._client      def model_profile(self, model_name: str) -> ModelProfile | None:         provider_to_profile = {             'meta-llama': meta_model_profile,             'deepseek-ai': deepseek_model_profile,             'qwen': qwen_model_profile,             'google': google_model_profile,             'openai': harmony_model_profile,  # used for gpt-oss models on Nebius             'mistralai': mistral_model_profile,             'moonshotai': moonshotai_model_profile,         }          profile = None          try:             model_name = model_name.lower()             provider, model_name = model_name.split('/', 1)         except ValueError:             raise UserError(f\"Model name must be in 'provider/model' format, got: {model_name!r}\")         if provider in provider_to_profile:             profile = provider_to_profile[provider](model_name)          # As NebiusProvider is always used with OpenAIChatModel, which used to unconditionally use OpenAIJsonSchemaTransformer,         # we need to maintain that behavior unless json_schema_transformer is set explicitly         return OpenAIModelProfile(json_schema_transformer=OpenAIJsonSchemaTransformer).update(profile)      @overload     def __init__(self) -> None: ...      @overload     def __init__(self, *, api_key: str) -> None: ...      @overload     def __init__(self, *, api_key: str, http_client: httpx.AsyncClient) -> None: ...      @overload     def __init__(self, *, openai_client: AsyncOpenAI | None = None) -> None: ...      def __init__(         self,         *,         api_key: str | None = None,         openai_client: AsyncOpenAI | None = None,         http_client: httpx.AsyncClient | None = None,     ) -> None:         api_key = api_key or os.getenv('NEBIUS_API_KEY')         if not api_key and openai_client is None:             raise UserError(                 'Set the `NEBIUS_API_KEY` environment variable or pass it via '                 '`NebiusProvider(api_key=...)` to use the Nebius AI Studio provider.'             )          if openai_client is not None:             self._client = openai_client         elif http_client is not None:             self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)         else:             http_client = cached_async_http_client(provider='nebius')             self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client) ``` |\n\nBases: `Provider[AsyncOpenAI]`\n\nProvider for OVHcloud AI Endpoints.\n\nSource code in `pydantic_ai_slim/pydantic_ai/providers/ovhcloud.py`", "url": "https://ai.pydantic.dev/providers/index.html#init", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "\\_\\_init\\_\\_", "anchor": "init", "md_text": "|  |  |\n| --- | --- |\n| ``` 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 ``` | ``` class OVHcloudProvider(Provider[AsyncOpenAI]):     \"\"\"Provider for OVHcloud AI Endpoints.\"\"\"      @property     def name(self) -> str:         return 'ovhcloud'      @property     def base_url(self) -> str:         return 'https://oai.endpoints.kepler.ai.cloud.ovh.net/v1'      @property     def client(self) -> AsyncOpenAI:         return self._client      def model_profile(self, model_name: str) -> ModelProfile | None:         model_name = model_name.lower()          prefix_to_profile = {             'llama': meta_model_profile,             'meta-': meta_model_profile,             'deepseek': deepseek_model_profile,             'mistral': mistral_model_profile,             'gpt': harmony_model_profile,             'qwen': qwen_model_profile,         }          profile = None         for prefix, profile_func in prefix_to_profile.items():             if model_name.startswith(prefix):                 profile = profile_func(model_name)          # As the OVHcloud AI Endpoints API is OpenAI-compatible, let's assume we also need OpenAIJsonSchemaTransformer.         return OpenAIModelProfile(json_schema_transformer=OpenAIJsonSchemaTransformer).update(profile)      @overload     def __init__(self) -> None: ...      @overload     def __init__(self, *, api_key: str) -> None: ...      @overload     def __init__(self, *, api_key: str, http_client: httpx.AsyncClient) -> None: ...      @overload     def __init__(self, *, openai_client: AsyncOpenAI | None = None) -> None: ...      def __init__(         self,         *,         api_key: str | None = None,         openai_client: AsyncOpenAI | None = None,         http_client: httpx.AsyncClient | None = None,     ) -> None:         api_key = api_key or os.getenv('OVHCLOUD_API_KEY')         if not api_key and openai_client is None:             raise UserError(                 'Set the `OVHCLOUD_API_KEY` environment variable or pass it via '                 '`OVHcloudProvider(api_key=...)` to use OVHcloud AI Endpoints provider.'             )          if openai_client is not None:             self._client = openai_client         elif http_client is not None:             self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client)         else:             http_client = cached_async_http_client(provider='ovhcloud')             self._client = AsyncOpenAI(base_url=self.base_url, api_key=api_key, http_client=http_client) ``` |", "url": "https://ai.pydantic.dev/providers/index.html#init", "page": "providers/index.html", "source_site": "pydantic_ai"}
{"title": "supports\\_tools `class-attribute` `instance-attribute`", "anchor": "supportstools-class-attribute-instance-attribute", "md_text": "```\nsupports_tools: bool = True\n```\n\nWhether the model supports tools.", "url": "https://ai.pydantic.dev/profiles/index.html#supportstools-class-attribute-instance-attribute", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "supports\\_json\\_schema\\_output `class-attribute` `instance-attribute`", "anchor": "supportsjsonschemaoutput-class-attribute-instance-attribute", "md_text": "```\nsupports_json_schema_output: bool = False\n```\n\nWhether the model supports JSON schema output.", "url": "https://ai.pydantic.dev/profiles/index.html#supportsjsonschemaoutput-class-attribute-instance-attribute", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "supports\\_json\\_object\\_output `class-attribute` `instance-attribute`", "anchor": "supportsjsonobjectoutput-class-attribute-instance-attribute", "md_text": "```\nsupports_json_object_output: bool = False\n```\n\nWhether the model supports JSON object output.", "url": "https://ai.pydantic.dev/profiles/index.html#supportsjsonobjectoutput-class-attribute-instance-attribute", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "supports\\_image\\_output `class-attribute` `instance-attribute`", "anchor": "supportsimageoutput-class-attribute-instance-attribute", "md_text": "```\nsupports_image_output: bool = False\n```\n\nWhether the model supports image output.", "url": "https://ai.pydantic.dev/profiles/index.html#supportsimageoutput-class-attribute-instance-attribute", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "default\\_structured\\_output\\_mode `class-attribute` `instance-attribute`", "anchor": "defaultstructuredoutputmode-class-attribute-instance-attribute", "md_text": "```\ndefault_structured_output_mode: StructuredOutputMode = (\n    \"tool\"\n)\n```\n\nThe default structured output mode to use for the model.", "url": "https://ai.pydantic.dev/profiles/index.html#defaultstructuredoutputmode-class-attribute-instance-attribute", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "prompted\\_output\\_template `class-attribute` `instance-attribute`", "anchor": "promptedoutputtemplate-class-attribute-instance-attribute", "md_text": "```\nprompted_output_template: str = dedent(\n    \"\\n        Always respond with a JSON object that's compatible with this schema:\\n\\n        {schema}\\n\\n        Don't include any text or Markdown fencing before or after.\\n        \"\n)\n```\n\nThe instructions template to use for prompted structured output. The '{schema}' placeholder will be replaced with the JSON schema for the output.", "url": "https://ai.pydantic.dev/profiles/index.html#promptedoutputtemplate-class-attribute-instance-attribute", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "json\\_schema\\_transformer `class-attribute` `instance-attribute`", "anchor": "jsonschematransformer-class-attribute-instance-attribute", "md_text": "```\njson_schema_transformer: (\n    type[JsonSchemaTransformer] | None\n) = None\n```\n\nThe transformer to use to make JSON schemas for tools and structured output compatible with the model.", "url": "https://ai.pydantic.dev/profiles/index.html#jsonschematransformer-class-attribute-instance-attribute", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "thinking\\_tags `class-attribute` `instance-attribute`", "anchor": "thinkingtags-class-attribute-instance-attribute", "md_text": "```\nthinking_tags: tuple[str, str] = ('<think>', '</think>')\n```\n\nThe tags used to indicate thinking parts in the model's output. Defaults to ('', '').", "url": "https://ai.pydantic.dev/profiles/index.html#thinkingtags-class-attribute-instance-attribute", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "ignore\\_streamed\\_leading\\_whitespace `class-attribute` `instance-attribute`", "anchor": "ignorestreamedleadingwhitespace-class-attribute-instance-attribute", "md_text": "```\nignore_streamed_leading_whitespace: bool = False\n```\n\nWhether to ignore leading whitespace when streaming a response.\n\n```\nThis is a workaround for models that emit `<think>\n```\n\n`or an empty text part ahead of tool calls (e.g. Ollama + Qwen3),\nwhich we don't want to end up treating as a final result when using`run\\_stream`with`str`a valid`output\\_type`.\n\n```\nThis is currently only used by `OpenAIChatModel`, `HuggingFaceModel`, and `GroqModel`.\n```", "url": "https://ai.pydantic.dev/profiles/index.html#ignorestreamedleadingwhitespace-class-attribute-instance-attribute", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "from\\_profile `classmethod`", "anchor": "fromprofile-classmethod", "md_text": "```\nfrom_profile(profile: ModelProfile | None) -> Self\n```\n\nBuild a ModelProfile subclass instance from a ModelProfile instance.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 60 61 62 63 64 65 ``` | ``` @classmethod def from_profile(cls, profile: ModelProfile | None) -> Self:     \"\"\"Build a ModelProfile subclass instance from a ModelProfile instance.\"\"\"     if isinstance(profile, cls):         return profile     return cls().update(profile) ``` |", "url": "https://ai.pydantic.dev/profiles/index.html#fromprofile-classmethod", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "update", "anchor": "update", "md_text": "```\nupdate(profile: ModelProfile | None) -> Self\n```\n\nUpdate this ModelProfile (subclass) instance with the non-default values from another ModelProfile instance.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 67 68 69 70 71 72 73 74 75 76 77 ``` | ``` def update(self, profile: ModelProfile | None) -> Self:     \"\"\"Update this ModelProfile (subclass) instance with the non-default values from another ModelProfile instance.\"\"\"     if not profile:         return self     field_names = set(f.name for f in fields(self))     non_default_attrs = {         f.name: getattr(profile, f.name)         for f in fields(profile)         if f.name in field_names and getattr(profile, f.name) != f.default     }     return replace(self, **non_default_attrs) ``` |", "url": "https://ai.pydantic.dev/profiles/index.html#update", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIModelProfile `dataclass`", "anchor": "openaimodelprofile-dataclass", "md_text": "Bases: `ModelProfile`\n\nProfile for models used with `OpenAIChatModel`.\n\nALL FIELDS MUST BE `openai_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/openai.py`\n\n|  |  |\n| --- | --- |\n| ``` 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 ``` | ``` @dataclass(kw_only=True) class OpenAIModelProfile(ModelProfile):     \"\"\"Profile for models used with `OpenAIChatModel`.      ALL FIELDS MUST BE `openai_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.     \"\"\"      openai_supports_strict_tool_definition: bool = True     \"\"\"This can be set by a provider or user if the OpenAI-\"compatible\" API doesn't support strict tool definitions.\"\"\"      openai_supports_sampling_settings: bool = True     \"\"\"Turn off to don't send sampling settings like `temperature` and `top_p` to models that don't support them, like OpenAI's o-series reasoning models.\"\"\"      openai_unsupported_model_settings: Sequence[str] = ()     \"\"\"A list of model settings that are not supported by this model.\"\"\"      # Some OpenAI-compatible providers (e.g. MoonshotAI) currently do **not** accept     # `tool_choice=\"required\"`.  This flag lets the calling model know whether it's     # safe to pass that value along.  Default is `True` to preserve existing     # behaviour for OpenAI itself and most providers.     openai_supports_tool_choice_required: bool = True     \"\"\"Whether the provider accepts the value ``tool_choice='required'`` in the request payload.\"\"\"      openai_system_prompt_role: OpenAISystemPromptRole | None = None     \"\"\"The role to use for the system prompt message. If not provided, defaults to `'system'`.\"\"\"      openai_chat_supports_web_search: bool = False     \"\"\"Whether the model supports web search in Chat Completions API.\"\"\"      openai_supports_encrypted_reasoning_content: bool = False     \"\"\"Whether the model supports including encrypted reasoning content in the response.\"\"\"      openai_responses_requires_function_call_status_none: bool = False     \"\"\"Whether the Responses API requires the `status` field on function tool calls to be `None`.      This is required by vLLM Responses API versions before https://github.com/vllm-project/vllm/pull/26706.     See https://github.com/pydantic/pydantic-ai/issues/3245 for more details.     \"\"\"      def __post_init__(self):  # pragma: no cover         if not self.openai_supports_sampling_settings:             warnings.warn(                 'The `openai_supports_sampling_settings` has no effect, and it will be removed in future versions. '                 'Use `openai_unsupported_model_settings` instead.',                 DeprecationWarning,             ) ``` |\n\n#### openai\\_supports\\_strict\\_tool\\_definition `class-attribute` `instance-attribute`\n\n```\nopenai_supports_strict_tool_definition: bool = True\n```\n\nThis can be set by a provider or user if the OpenAI-\"compatible\" API doesn't support strict tool definitions.\n\n#### openai\\_supports\\_sampling\\_settings `class-attribute` `instance-attribute`\n\n```\nopenai_supports_sampling_settings: bool = True\n```\n\nTurn off to don't send sampling settings like `temperature` and `top_p` to models that don't support them, like OpenAI's o-series reasoning models.\n\n#### openai\\_unsupported\\_model\\_settings `class-attribute` `instance-attribute`\n\n```\nopenai_unsupported_model_settings: Sequence[str] = ()\n```\n\nA list of model settings that are not supported by this model.\n\n#### openai\\_supports\\_tool\\_choice\\_required `class-attribute` `instance-attribute`\n\n```\nopenai_supports_tool_choice_required: bool = True\n```\n\nWhether the provider accepts the value `tool_choice='required'` in the request payload.\n\n#### openai\\_system\\_prompt\\_role `class-attribute` `instance-attribute`\n\n```\nopenai_system_prompt_role: OpenAISystemPromptRole | None = (\n    None\n)\n```\n\nThe role to use for the system prompt message. If not provided, defaults to `'system'`.\n\n#### openai\\_chat\\_supports\\_web\\_search `class-attribute` `instance-attribute`\n\n```\nopenai_chat_supports_web_search: bool = False\n```\n\nWhether the model supports web search in Chat Completions API.\n\n#### openai\\_supports\\_encrypted\\_reasoning\\_content `class-attribute` `instance-attribute`\n\n```\nopenai_supports_encrypted_reasoning_content: bool = False\n```\n\nWhether the model supports including encrypted reasoning content in the response.\n\n#### openai\\_responses\\_requires\\_function\\_call\\_status\\_none `class-attribute` `instance-attribute`\n\n```\nopenai_responses_requires_function_call_status_none: (\n    bool\n) = False\n```\n\nWhether the Responses API requires the `status` field on function tool calls to be `None`.", "url": "https://ai.pydantic.dev/profiles/index.html#openaimodelprofile-dataclass", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIModelProfile `dataclass`", "anchor": "openaimodelprofile-dataclass", "md_text": "This is required by vLLM Responses API versions before https://github.com/vllm-project/vllm/pull/26706.\nSee https://github.com/pydantic/pydantic-ai/issues/3245 for more details.", "url": "https://ai.pydantic.dev/profiles/index.html#openaimodelprofile-dataclass", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "openai\\_model\\_profile", "anchor": "openaimodelprofile", "md_text": "```\nopenai_model_profile(model_name: str) -> ModelProfile\n```\n\nGet the model profile for an OpenAI model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/openai.py`\n\n|  |  |\n| --- | --- |\n| ``` 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 ``` | ``` def openai_model_profile(model_name: str) -> ModelProfile:     \"\"\"Get the model profile for an OpenAI model.\"\"\"     is_reasoning_model = model_name.startswith('o') or model_name.startswith('gpt-5')     # Check if the model supports web search (only specific search-preview models)     supports_web_search = '-search-preview' in model_name      # Structured Outputs (output mode 'native') is only supported with the gpt-4o-mini, gpt-4o-mini-2024-07-18, and gpt-4o-2024-08-06 model snapshots and later.     # We leave it in here for all models because the `default_structured_output_mode` is `'tool'`, so `native` is only used     # when the user specifically uses the `NativeOutput` marker, so an error from the API is acceptable.      if is_reasoning_model:         openai_unsupported_model_settings = (             'temperature',             'top_p',             'presence_penalty',             'frequency_penalty',             'logit_bias',             'logprobs',             'top_logprobs',         )     else:         openai_unsupported_model_settings = ()      # The o1-mini model doesn't support the `system` role, so we default to `user`.     # See https://github.com/pydantic/pydantic-ai/issues/974 for more details.     openai_system_prompt_role = 'user' if model_name.startswith('o1-mini') else None      return OpenAIModelProfile(         json_schema_transformer=OpenAIJsonSchemaTransformer,         supports_json_schema_output=True,         supports_json_object_output=True,         supports_image_output=is_reasoning_model or '4.1' in model_name or '4o' in model_name,         openai_unsupported_model_settings=openai_unsupported_model_settings,         openai_system_prompt_role=openai_system_prompt_role,         openai_chat_supports_web_search=supports_web_search,         openai_supports_encrypted_reasoning_content=is_reasoning_model,     ) ``` |", "url": "https://ai.pydantic.dev/profiles/index.html#openaimodelprofile", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIJsonSchemaTransformer `dataclass`", "anchor": "openaijsonschematransformer-dataclass", "md_text": "Bases: `JsonSchemaTransformer`\n\nRecursively handle the schema to make it compatible with OpenAI strict mode.\n\nSee https://platform.openai.com/docs/guides/function-calling?api-mode=responses#strict-mode for more details,\nbut this basically just requires:\n\\* `additionalProperties` must be set to false for each object in the parameters\n\\* all fields in properties must be marked as required\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/openai.py`", "url": "https://ai.pydantic.dev/profiles/index.html#openaijsonschematransformer-dataclass", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIJsonSchemaTransformer `dataclass`", "anchor": "openaijsonschematransformer-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 ``` | ``` @dataclass(init=False) class OpenAIJsonSchemaTransformer(JsonSchemaTransformer):     \"\"\"Recursively handle the schema to make it compatible with OpenAI strict mode.      See https://platform.openai.com/docs/guides/function-calling?api-mode=responses#strict-mode for more details,     but this basically just requires:     * `additionalProperties` must be set to false for each object in the parameters     * all fields in properties must be marked as required     \"\"\"      def __init__(self, schema: JsonSchema, *, strict: bool | None = None):         super().__init__(schema, strict=strict)         self.root_ref = schema.get('$ref')      def walk(self) -> JsonSchema:         # Note: OpenAI does not support anyOf at the root in strict mode         # However, we don't need to check for it here because we ensure in pydantic_ai._utils.check_object_json_schema         # that the root schema either has type 'object' or is recursive.         result = super().walk()          # For recursive models, we need to tweak the schema to make it compatible with strict mode.         # Because the following should never change the semantics of the schema we apply it unconditionally.         if self.root_ref is not None:             result.pop('$ref', None)  # We replace references to the self.root_ref with just '#' in the transform method             root_key = re.sub(r'^#/\\$defs/', '', self.root_ref)             result.update(self.defs.get(root_key) or {})          return result      def transform(self, schema: JsonSchema) -> JsonSchema:  # noqa C901         # Remove unnecessary keys         schema.pop('title', None)         schema.pop('$schema', None)         schema.pop('discriminator', None)          default = schema.get('default', _sentinel)         if default is not _sentinel:             # the \"default\" keyword is not allowed in strict mode, but including it makes some Ollama models behave             # better, so we keep it around when not strict             if self.strict is True:                 schema.pop('default', None)             elif self.strict is None:  # pragma: no branch                 self.is_strict_compatible = False          if schema_ref := schema.get('$ref'):             if schema_ref == self.root_ref:                 schema['$ref'] = '#'             if len(schema) > 1:                 # OpenAI Strict mode doesn't support siblings to \"$ref\", but _does_ allow siblings to \"anyOf\".                 # So if there is a \"description\" field or any other extra info, we move the \"$ref\" into an \"anyOf\":                 schema['anyOf'] = [{'$ref': schema.pop('$ref')}]          # Track strict-incompatible keys         incompatible_values: dict[str, Any] = {}         for key in _STRICT_INCOMPATIBLE_KEYS:             value = schema.get(key, _sentinel)             if value is not _sentinel:                 incompatible_values[key] = value         if format := schema.get('format'):             if format not in _STRICT_COMPATIBLE_STRING_FORMATS:                 incompatible_values['format'] = format         description = schema.get('description')         if incompatible_values:             if self.strict is True:                 notes: list[str] = []                 for key, value in incompatible_values.items():                     schema.pop(key)                     notes.append(f'{key}={value}')                 notes_string = ', '.join(notes)                 schema['description'] = notes_string if not description else f'{description} ({notes_string})'             elif self.strict is None:  # pragma: no branch                 self.is_strict_compatible = False          schema_type = schema.get('type')         if 'oneOf' in schema:             # OpenAI does not support oneOf in strict mode             if self.strict is True:                 schema['anyOf'] = schema.pop('oneOf')             else:                 self.is_strict_compatible = False          if schema_type == 'object':             if self.strict is True:                 # additional properties are disallowed                 schema['additionalProperties'] = False                  # all properties are required                 if 'properties' not in schema:                     schema['properties'] = dict[str, Any]()                 schema['required'] = list(schema['properties'].keys())              elif self.strict is None:                 if schema.get('additionalProperties', None) not in (None, False):                     self.is_strict_compatible = False                 else:                     # additional properties are disallowed by default                     schema['additionalProperties'] = False                  if 'properties' not in schema or 'required' not in schema:                     self.is_strict_compatible = False                 else:                     required = schema['required']                     for k in schema['properties'].keys():                         if k not in required:                             self.is_strict_compatible = False         return schema ``` |", "url": "https://ai.pydantic.dev/profiles/index.html#openaijsonschematransformer-dataclass", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "anthropic\\_model\\_profile", "anchor": "anthropicmodelprofile", "md_text": "```\nanthropic_model_profile(\n    model_name: str,\n) -> ModelProfile | None\n```\n\nGet the model profile for an Anthropic model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/anthropic.py`\n\n|  |  |\n| --- | --- |\n| ``` 6 7 8 ``` | ``` def anthropic_model_profile(model_name: str) -> ModelProfile | None:     \"\"\"Get the model profile for an Anthropic model.\"\"\"     return ModelProfile(thinking_tags=('<thinking>', '</thinking>')) ``` |", "url": "https://ai.pydantic.dev/profiles/index.html#anthropicmodelprofile", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "google\\_model\\_profile", "anchor": "googlemodelprofile", "md_text": "```\ngoogle_model_profile(\n    model_name: str,\n) -> ModelProfile | None\n```\n\nGet the model profile for a Google model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/google.py`\n\n|  |  |\n| --- | --- |\n| ``` 11 12 13 14 15 16 17 18 19 20 ``` | ``` def google_model_profile(model_name: str) -> ModelProfile | None:     \"\"\"Get the model profile for a Google model.\"\"\"     is_image_model = 'image' in model_name     return ModelProfile(         json_schema_transformer=GoogleJsonSchemaTransformer,         supports_image_output=is_image_model,         supports_json_schema_output=not is_image_model,         supports_json_object_output=not is_image_model,         supports_tools=not is_image_model,     ) ``` |", "url": "https://ai.pydantic.dev/profiles/index.html#googlemodelprofile", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "GoogleJsonSchemaTransformer", "anchor": "googlejsonschematransformer", "md_text": "Bases: `JsonSchemaTransformer`\n\nTransforms the JSON Schema from Pydantic to be suitable for Gemini.\n\nGemini which [supports](https://ai.google.dev/gemini-api/docs/function-calling#function_declarations)\na subset of OpenAPI v3.0.3.\n\nSpecifically:\n\\* gemini doesn't allow the `title` keyword to be set\n\\* gemini doesn't allow `$defs` — we need to inline the definitions where possible\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/google.py`", "url": "https://ai.pydantic.dev/profiles/index.html#googlejsonschematransformer", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "GoogleJsonSchemaTransformer", "anchor": "googlejsonschematransformer", "md_text": "|  |  |\n| --- | --- |\n| ```  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 ``` | ``` class GoogleJsonSchemaTransformer(JsonSchemaTransformer):     \"\"\"Transforms the JSON Schema from Pydantic to be suitable for Gemini.      Gemini which [supports](https://ai.google.dev/gemini-api/docs/function-calling#function_declarations)     a subset of OpenAPI v3.0.3.      Specifically:     * gemini doesn't allow the `title` keyword to be set     * gemini doesn't allow `$defs` — we need to inline the definitions where possible     \"\"\"      def __init__(self, schema: JsonSchema, *, strict: bool | None = None):         super().__init__(schema, strict=strict, prefer_inlined_defs=True, simplify_nullable_unions=True)      def transform(self, schema: JsonSchema) -> JsonSchema:         # Note: we need to remove `additionalProperties: False` since it is currently mishandled by Gemini         additional_properties = schema.pop(             'additionalProperties', None         )  # don't pop yet so it's included in the warning         if additional_properties:             original_schema = {**schema, 'additionalProperties': additional_properties}             warnings.warn(                 '`additionalProperties` is not supported by Gemini; it will be removed from the tool JSON schema.'                 f' Full schema: {self.schema}\\n\\n'                 f'Source of additionalProperties within the full schema: {original_schema}\\n\\n'                 'If this came from a field with a type like `dict[str, MyType]`, that field will always be empty.\\n\\n'                 \"If Google's APIs are updated to support this properly, please create an issue on the Pydantic AI GitHub\"                 ' and we will fix this behavior.',                 UserWarning,             )          schema.pop('title', None)         schema.pop('$schema', None)         if (const := schema.pop('const', None)) is not None:             # Gemini doesn't support const, but it does support enum with a single value             schema['enum'] = [const]         schema.pop('discriminator', None)         schema.pop('examples', None)          # TODO: Should we use the trick from pydantic_ai.models.openai._OpenAIJsonSchema         #   where we add notes about these properties to the field description?         schema.pop('exclusiveMaximum', None)         schema.pop('exclusiveMinimum', None)          # Gemini only supports string enums, so we need to convert any enum values to strings.         # Pydantic will take care of transforming the transformed string values to the correct type.         if enum := schema.get('enum'):             schema['type'] = 'string'             schema['enum'] = [str(val) for val in enum]          type_ = schema.get('type')         if 'oneOf' in schema and 'type' not in schema:  # pragma: no cover             # This gets hit when we have a discriminated union             # Gemini returns an API error in this case even though it says in its error message it shouldn't...             # Changing the oneOf to an anyOf prevents the API error and I think is functionally equivalent             schema['anyOf'] = schema.pop('oneOf')          if type_ == 'string' and (fmt := schema.pop('format', None)):             description = schema.get('description')             if description:                 schema['description'] = f'{description} (format: {fmt})'             else:                 schema['description'] = f'Format: {fmt}'          if '$ref' in schema:             raise UserError(f'Recursive `$ref`s in JSON Schema are not supported by Gemini: {schema[\"$ref\"]}')          if 'prefixItems' in schema:             # prefixItems is not currently supported in Gemini, so we convert it to items for best compatibility             prefix_items = schema.pop('prefixItems')             items = schema.get('items')             unique_items = [items] if items is not None else []             for item in prefix_items:                 if item not in unique_items:                     unique_items.append(item)             if len(unique_items) > 1:  # pragma: no cover                 schema['items'] = {'anyOf': unique_items}             elif len(unique_items) == 1:  # pragma: no branch                 schema['items'] = unique_items[0]             schema.setdefault('minItems', len(prefix_items))             if items is None:  # pragma: no branch                 schema.setdefault('maxItems', len(prefix_items))          return schema ``` |", "url": "https://ai.pydantic.dev/profiles/index.html#googlejsonschematransformer", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "meta\\_model\\_profile", "anchor": "metamodelprofile", "md_text": "```\nmeta_model_profile(model_name: str) -> ModelProfile | None\n```\n\nGet the model profile for a Meta model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/meta.py`\n\n|  |  |\n| --- | --- |\n| ``` 6 7 8 ``` | ``` def meta_model_profile(model_name: str) -> ModelProfile | None:     \"\"\"Get the model profile for a Meta model.\"\"\"     return ModelProfile(json_schema_transformer=InlineDefsJsonSchemaTransformer) ``` |", "url": "https://ai.pydantic.dev/profiles/index.html#metamodelprofile", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "amazon\\_model\\_profile", "anchor": "amazonmodelprofile", "md_text": "```\namazon_model_profile(\n    model_name: str,\n) -> ModelProfile | None\n```\n\nGet the model profile for an Amazon model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/amazon.py`\n\n|  |  |\n| --- | --- |\n| ``` 6 7 8 ``` | ``` def amazon_model_profile(model_name: str) -> ModelProfile | None:     \"\"\"Get the model profile for an Amazon model.\"\"\"     return ModelProfile(json_schema_transformer=InlineDefsJsonSchemaTransformer) ``` |", "url": "https://ai.pydantic.dev/profiles/index.html#amazonmodelprofile", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "deepseek\\_model\\_profile", "anchor": "deepseekmodelprofile", "md_text": "```\ndeepseek_model_profile(\n    model_name: str,\n) -> ModelProfile | None\n```\n\nGet the model profile for a DeepSeek model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/deepseek.py`\n\n|  |  |\n| --- | --- |\n| ``` 6 7 8 ``` | ``` def deepseek_model_profile(model_name: str) -> ModelProfile | None:     \"\"\"Get the model profile for a DeepSeek model.\"\"\"     return ModelProfile(ignore_streamed_leading_whitespace='r1' in model_name) ``` |", "url": "https://ai.pydantic.dev/profiles/index.html#deepseekmodelprofile", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "grok\\_model\\_profile", "anchor": "grokmodelprofile", "md_text": "```\ngrok_model_profile(model_name: str) -> ModelProfile | None\n```\n\nGet the model profile for a Grok model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/grok.py`\n\n|  |  |\n| --- | --- |\n| ``` 6 7 8 ``` | ``` def grok_model_profile(model_name: str) -> ModelProfile | None:     \"\"\"Get the model profile for a Grok model.\"\"\"     return None ``` |", "url": "https://ai.pydantic.dev/profiles/index.html#grokmodelprofile", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "mistral\\_model\\_profile", "anchor": "mistralmodelprofile", "md_text": "```\nmistral_model_profile(\n    model_name: str,\n) -> ModelProfile | None\n```\n\nGet the model profile for a Mistral model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/mistral.py`\n\n|  |  |\n| --- | --- |\n| ``` 6 7 8 ``` | ``` def mistral_model_profile(model_name: str) -> ModelProfile | None:     \"\"\"Get the model profile for a Mistral model.\"\"\"     return None ``` |", "url": "https://ai.pydantic.dev/profiles/index.html#mistralmodelprofile", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "qwen\\_model\\_profile", "anchor": "qwenmodelprofile", "md_text": "```\nqwen_model_profile(model_name: str) -> ModelProfile | None\n```\n\nGet the model profile for a Qwen model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/profiles/qwen.py`\n\n|  |  |\n| --- | --- |\n| ```  7  8  9 10 11 12 13 14 15 16 17 18 19 ``` | ``` def qwen_model_profile(model_name: str) -> ModelProfile | None:     \"\"\"Get the model profile for a Qwen model.\"\"\"     if model_name.startswith('qwen-3-coder'):         return OpenAIModelProfile(             json_schema_transformer=InlineDefsJsonSchemaTransformer,             openai_supports_tool_choice_required=False,             openai_supports_strict_tool_definition=False,             ignore_streamed_leading_whitespace=True,         )     return ModelProfile(         json_schema_transformer=InlineDefsJsonSchemaTransformer,         ignore_streamed_leading_whitespace=True,     ) ``` |", "url": "https://ai.pydantic.dev/profiles/index.html#qwenmodelprofile", "page": "profiles/index.html", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "md_text": "With [dependencies installed and environment variables set](../setup/index.html#usage), run:\n\npipuv\n\n```\npython -m pydantic_ai_examples.bank_support\n\nuv run -m pydantic_ai_examples.bank_support\n```\n\n(or `PYDANTIC_AI_MODEL=gemini-1.5-flash ...`)", "url": "https://ai.pydantic.dev/bank-support/index.html#running-the-example", "page": "bank-support/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "md_text": "[bank\\_support.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/bank_support.py)\n\n```\n\"\"\"Small but complete example of using Pydantic AI to build a support agent for a bank.\n\nRun with:\n\n    uv run -m pydantic_ai_examples.bank_support\n\"\"\"\n\nfrom dataclasses import dataclass\n\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, RunContext\n\n\nclass DatabaseConn:\n    \"\"\"This is a fake database for example purposes.\n\n    In reality, you'd be connecting to an external database\n    (e.g. PostgreSQL) to get information about customers.\n    \"\"\"\n\n    @classmethod\n    async def customer_name(cls, *, id: int) -> str | None:\n        if id == 123:\n            return 'John'\n\n    @classmethod\n    async def customer_balance(cls, *, id: int, include_pending: bool) -> float:\n        if id == 123:\n            if include_pending:\n                return 123.45\n            else:\n                return 100.00\n        else:\n            raise ValueError('Customer not found')\n\n\n@dataclass\nclass SupportDependencies:\n    customer_id: int\n    db: DatabaseConn\n\n\nclass SupportOutput(BaseModel):\n    support_advice: str\n    \"\"\"Advice returned to the customer\"\"\"\n    block_card: bool\n    \"\"\"Whether to block their card or not\"\"\"\n    risk: int\n    \"\"\"Risk level of query\"\"\"\n\n\nsupport_agent = Agent(\n    'openai:gpt-4o',\n    deps_type=SupportDependencies,\n    output_type=SupportOutput,\n    instructions=(\n        'You are a support agent in our bank, give the '\n        'customer support and judge the risk level of their query. '\n        \"Reply using the customer's name.\"\n    ),\n)\n\n\n@support_agent.instructions\nasync def add_customer_name(ctx: RunContext[SupportDependencies]) -> str:\n    customer_name = await ctx.deps.db.customer_name(id=ctx.deps.customer_id)\n    return f\"The customer's name is {customer_name!r}\"\n\n\n@support_agent.tool\nasync def customer_balance(\n    ctx: RunContext[SupportDependencies], include_pending: bool\n) -> str:\n    \"\"\"Returns the customer's current account balance.\"\"\"\n    balance = await ctx.deps.db.customer_balance(\n        id=ctx.deps.customer_id,\n        include_pending=include_pending,\n    )\n    return f'${balance:.2f}'\n\n\nif __name__ == '__main__':\n    deps = SupportDependencies(customer_id=123, db=DatabaseConn())\n    result = support_agent.run_sync('What is my balance?', deps=deps)\n    print(result.output)\n    \"\"\"\n    support_advice='Hello John, your current account balance, including pending transactions, is $123.45.' block_card=False risk=1\n    \"\"\"\n\n    result = support_agent.run_sync('I just lost my card!', deps=deps)\n    print(result.output)\n    \"\"\"\n    support_advice=\"I'm sorry to hear that, John. We are temporarily blocking your card to prevent unauthorized transactions.\" block_card=True risk=8\n    \"\"\"\n```", "url": "https://ai.pydantic.dev/bank-support/index.html#example-code", "page": "bank-support/index.html", "source_site": "pydantic_ai"}
{"title": "Agent `dataclass`", "anchor": "agent-dataclass", "md_text": "Bases: `AbstractAgent[AgentDepsT, OutputDataT]`\n\nClass for defining \"agents\" - a way to have a specific type of \"conversation\" with an LLM.\n\nAgents are generic in the dependency type they take [`AgentDepsT`](../tools/index.html#pydantic_ai.tools.AgentDepsT)\nand the output type they return, [`OutputDataT`](../output/index.html#pydantic_ai.output.OutputDataT).\n\nBy default, if neither generic parameter is customised, agents have type `Agent[None, str]`.\n\nMinimal usage example:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n```\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/__init__.py`", "url": "https://ai.pydantic.dev/agent/index.html#agent-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "Agent `dataclass`", "anchor": "agent-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ```   94   95   96   97   98   99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144  145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160  161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176  177  178  179  180  181  182  183  184  185  186  187  188  189  190  191  192  193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208  209  210  211  212  213  214  215  216  217  218  219  220  221  222  223  224  225  226  227  228  229  230  231  232  233  234  235  236  237  238  239  240  241  242  243  244  245  246  247  248  249  250  251  252  253  254  255  256  257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272  273  274  275  276  277  278  279  280  281  282  283  284  285  286  287  288  289  290  291  292  293  294  295  296  297  298  299  300  301  302  303  304  305  306  307  308  309  310  311  312  313  314  315  316  317  318  319  320  321  322  323  324  325  326  327  328  329  330  331  332  333  334  335  336  337  338  339  340  341  342  343  344  345  346  347  348  349  350  351  352  353  354  355  356  357  358  359  360  361  362  363  364  365  366  367  368  369  370  371  372  373  374  375  376  377  378  379  380  381  382  383  384  385  386  387  388  389  390  391  392  393  394  395  396  397  398  399  400  401  402  403  404  405  406  407  408  409  410  411  412  413  414  415  416  417  418  419  420  421  422  423  424  425  426  427  428  429  430  431  432  433  434  435  436  437  438  439  440  441  442  443  444  445  446  447  448  449  450  451  452  453  454  455  456  457  458  459  460  461  462  463  464  465  466  467  468  469  470  471  472  473  474  475  476  477  478  479  480  481  482  483  484  485  486  487  488  489  490  491  492  493  494  495  496  497  498  499  500  501  502  503  504  505  506  507  508  509  510  511  512  513  514  515  516  517  518  519  520  521  522  523  524  525  526  527  528  529  530  531  532  533  534  535  536  537  538  539  540  541  542  543  544  545  546  547  548  549  550  551  552  553  554  555  556  557  558  559  560  561  562  563  564  565  566  567  568  569  570  571  572  573  574  575  576  577  578  579  580  581  582  583  584  585  586  587  588  589  590  591  592  593  594  595  596  597  598  599  600  601  602  603  604  605  606  607  608  609  610  611  612  613  614  615  616  617  618  619  620  621  622  623  624  625  626  627  628  629  630  631  632  633  634  635  636  637  638  639  640  641  642  643  644  645  646  647  648  649  650  651  652  653  654  655  656  657  658  659  660  661  662  663  664  665  666  667  668  669  670  671  672  673  674  675  676  677  678  679  680  681  682  683  684  685  686  687  688  689  690  691  692  693  694  695  696  697  698  699  700  701  702  703  704  705  706  707  708  709  710  711  712  713  714  715  716  717  718  719  720  721  722  723  724  725  726  727  728  729  730  731  732  733  734  735  736  737  738  739  740  741  742  743  744  745  746  747  748  749  750  751  752  753  754  755  756  757  758  759  760  761  762  763  764  765  766  767  768  769  770  771  772  773  774  775  776  777  778  779  780  781  782  783  784  785  786  787  788  789  790  791  792  793  794  795  796  797  798  799  800  801  802  803  804  805  806  807  808  809  810  811  812  813  814  815  816  817  818  819  820  821  822  823  824  825  826  827  828  829  830  831  832  833  834  835  836  837  838  839  840  841  842  843  844  845  846  847  848  849  850  851  852  853  854  855  856  857  858  859  860  861  862  863  864  865  866  867  868  869  870  871  872  873  874  875  876  877  878  879  880  881  882  883  884  885  886  887  888  889  890  891  892  893  894  895  896  897  898  899  900  901  902  903  904  905  906  907  908  909  910  911  912  913  914  915  916  917  918  919  920  921  922  923  924  925  926  927  928  929  930  931  932  933  934  935  936  937  938  939  940  941  942  943  944  945  946  947  948  949  950  951  952  953  954  955  956  957  958  959  960  961  962  963  964  965  966  967  968  969  970  971  972  973  974  975  976  977  978  979  980  981  982  983  984  985  986  987  988  989  990  991  992  993  994  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 ``` | ``` @dataclasses.dataclass(init=False) class Agent(AbstractAgent[AgentDepsT, OutputDataT]):     \"\"\"Class for defining \"agents\" - a way to have a specific type of \"conversation\" with an LLM.      Agents are generic in the dependency type they take [`AgentDepsT`][pydantic_ai.tools.AgentDepsT]     and the output type they return, [`OutputDataT`][pydantic_ai.output.OutputDataT].      By default, if neither generic parameter is customised, agents have type `Agent[None, str]`.      Minimal usage example:      ```python     from pydantic_ai import Agent      agent = Agent('openai:gpt-4o')     result = agent.run_sync('What is the capital of France?')     print(result.output)     #> The capital of France is Paris.     ```     \"\"\"      _model: models.Model | models.KnownModelName | str | None      _name: str | None     end_strategy: EndStrategy     \"\"\"Strategy for handling tool calls when a final result is found.\"\"\"      model_settings: ModelSettings | None     \"\"\"Optional model request settings to use for this agents's runs, by default.      Note, if `model_settings` is provided by `run`, `run_sync`, or `run_stream`, those settings will     be merged with this value, with the runtime argument taking priority.     \"\"\"      _output_type: OutputSpec[OutputDataT]      instrument: InstrumentationSettings | bool | None     \"\"\"Options to automatically instrument with OpenTelemetry.\"\"\"      _instrument_default: ClassVar[InstrumentationSettings | bool] = False      _deps_type: type[AgentDepsT] = dataclasses.field(repr=False)     _output_schema: _output.BaseOutputSchema[OutputDataT] = dataclasses.field(repr=False)     _output_validators: list[_output.OutputValidator[AgentDepsT, OutputDataT]] = dataclasses.field(repr=False)     _instructions: list[str | _system_prompt.SystemPromptFunc[AgentDepsT]] = dataclasses.field(repr=False)     _system_prompts: tuple[str, ...] = dataclasses.field(repr=False)     _system_prompt_functions: list[_system_prompt.SystemPromptRunner[AgentDepsT]] = dataclasses.field(repr=False)     _system_prompt_dynamic_functions: dict[str, _system_prompt.SystemPromptRunner[AgentDepsT]] = dataclasses.field(         repr=False     )     _function_toolset: FunctionToolset[AgentDepsT] = dataclasses.field(repr=False)     _output_toolset: OutputToolset[AgentDepsT] | None = dataclasses.field(repr=False)     _user_toolsets: list[AbstractToolset[AgentDepsT]] = dataclasses.field(repr=False)     _prepare_tools: ToolsPrepareFunc[AgentDepsT] | None = dataclasses.field(repr=False)     _prepare_output_tools: ToolsPrepareFunc[AgentDepsT] | None = dataclasses.field(repr=False)     _max_result_retries: int = dataclasses.field(repr=False)     _max_tool_retries: int = dataclasses.field(repr=False)      _event_stream_handler: EventStreamHandler[AgentDepsT] | None = dataclasses.field(repr=False)      _enter_lock: Lock = dataclasses.field(repr=False)     _entered_count: int = dataclasses.field(repr=False)     _exit_stack: AsyncExitStack | None = dataclasses.field(repr=False)      @overload     def __init__(         self,         model: models.Model | models.KnownModelName | str | None = None,         *,         output_type: OutputSpec[OutputDataT] = str,         instructions: Instructions[AgentDepsT] = None,         system_prompt: str | Sequence[str] = (),         deps_type: type[AgentDepsT] = NoneType,         name: str | None = None,         model_settings: ModelSettings | None = None,         retries: int = 1,         output_retries: int | None = None,         tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] = (),         builtin_tools: Sequence[AbstractBuiltinTool] = (),         prepare_tools: ToolsPrepareFunc[AgentDepsT] | None = None,         prepare_output_tools: ToolsPrepareFunc[AgentDepsT] | None = None,         toolsets: Sequence[AbstractToolset[AgentDepsT] | ToolsetFunc[AgentDepsT]] | None = None,         defer_model_check: bool = False,         end_strategy: EndStrategy = 'early',         instrument: InstrumentationSettings | bool | None = None,         history_processors: Sequence[HistoryProcessor[AgentDepsT]] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> None: ...      @overload     @deprecated('`mcp_servers` is deprecated, use `toolsets` instead.')     def __init__(         self,         model: models.Model | models.KnownModelName | str | None = None,         *,         output_type: OutputSpec[OutputDataT] = str,         instructions: Instructions[AgentDepsT] = None,         system_prompt: str | Sequence[str] = (),         deps_type: type[AgentDepsT] = NoneType,         name: str | None = None,         model_settings: ModelSettings | None = None,         retries: int = 1,         output_retries: int | None = None,         tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] = (),         builtin_tools: Sequence[AbstractBuiltinTool] = (),         prepare_tools: ToolsPrepareFunc[AgentDepsT] | None = None,         prepare_output_tools: ToolsPrepareFunc[AgentDepsT] | None = None,         mcp_servers: Sequence[MCPServer] = (),         defer_model_check: bool = False,         end_strategy: EndStrategy = 'early',         instrument: InstrumentationSettings | bool | None = None,         history_processors: Sequence[HistoryProcessor[AgentDepsT]] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> None: ...      def __init__(         self,         model: models.Model | models.KnownModelName | str | None = None,         *,         output_type: OutputSpec[OutputDataT] = str,         instructions: Instructions[AgentDepsT] = None,         system_prompt: str | Sequence[str] = (),         deps_type: type[AgentDepsT] = NoneType,         name: str | None = None,         model_settings: ModelSettings | None = None,         retries: int = 1,         output_retries: int | None = None,         tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] = (),         builtin_tools: Sequence[AbstractBuiltinTool] = (),         prepare_tools: ToolsPrepareFunc[AgentDepsT] | None = None,         prepare_output_tools: ToolsPrepareFunc[AgentDepsT] | None = None,         toolsets: Sequence[AbstractToolset[AgentDepsT] | ToolsetFunc[AgentDepsT]] | None = None,         defer_model_check: bool = False,         end_strategy: EndStrategy = 'early',         instrument: InstrumentationSettings | bool | None = None,         history_processors: Sequence[HistoryProcessor[AgentDepsT]] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,         **_deprecated_kwargs: Any,     ):         \"\"\"Create an agent.          Args:             model: The default model to use for this agent, if not provided,                 you must provide the model when calling it. We allow `str` here since the actual list of allowed models changes frequently.             output_type: The type of the output data, used to validate the data returned by the model,                 defaults to `str`.             instructions: Instructions to use for this agent, you can also register instructions via a function with                 [`instructions`][pydantic_ai.Agent.instructions].             system_prompt: Static system prompts to use for this agent, you can also register system                 prompts via a function with [`system_prompt`][pydantic_ai.Agent.system_prompt].             deps_type: The type used for dependency injection, this parameter exists solely to allow you to fully                 parameterize the agent, and therefore get the best out of static type checking.                 If you're not using deps, but want type checking to pass, you can set `deps=None` to satisfy Pyright                 or add a type hint `: Agent[None, <return type>]`.             name: The name of the agent, used for logging. If `None`, we try to infer the agent name from the call frame                 when the agent is first run.             model_settings: Optional model request settings to use for this agent's runs, by default.             retries: The default number of retries to allow for tool calls and output validation, before raising an error.                 For model request retries, see the [HTTP Request Retries](../retries.md) documentation.             output_retries: The maximum number of retries to allow for output validation, defaults to `retries`.             tools: Tools to register with the agent, you can also register tools via the decorators                 [`@agent.tool`][pydantic_ai.Agent.tool] and [`@agent.tool_plain`][pydantic_ai.Agent.tool_plain].             builtin_tools: The builtin tools that the agent will use. This depends on the model, as some models may not                 support certain tools. If the model doesn't support the builtin tools, an error will be raised.             prepare_tools: Custom function to prepare the tool definition of all tools for each step, except output tools.                 This is useful if you want to customize the definition of multiple tools or you want to register                 a subset of tools for a given step. See [`ToolsPrepareFunc`][pydantic_ai.tools.ToolsPrepareFunc]             prepare_output_tools: Custom function to prepare the tool definition of all output tools for each step.                 This is useful if you want to customize the definition of multiple output tools or you want to register                 a subset of output tools for a given step. See [`ToolsPrepareFunc`][pydantic_ai.tools.ToolsPrepareFunc]             toolsets: Toolsets to register with the agent, including MCP servers and functions which take a run context                 and return a toolset. See [`ToolsetFunc`][pydantic_ai.toolsets.ToolsetFunc] for more information.             defer_model_check: by default, if you provide a [named][pydantic_ai.models.KnownModelName] model,                 it's evaluated to create a [`Model`][pydantic_ai.models.Model] instance immediately,                 which checks for the necessary environment variables. Set this to `false`                 to defer the evaluation until the first run. Useful if you want to                 [override the model][pydantic_ai.Agent.override] for testing.             end_strategy: Strategy for handling tool calls that are requested alongside a final result.                 See [`EndStrategy`][pydantic_ai.agent.EndStrategy] for more information.             instrument: Set to True to automatically instrument with OpenTelemetry,                 which will use Logfire if it's configured.                 Set to an instance of [`InstrumentationSettings`][pydantic_ai.agent.InstrumentationSettings] to customize.                 If this isn't set, then the last value set by                 [`Agent.instrument_all()`][pydantic_ai.Agent.instrument_all]                 will be used, which defaults to False.                 See the [Debugging and Monitoring guide](https://ai.pydantic.dev/logfire/) for more info.             history_processors: Optional list of callables to process the message history before sending it to the model.                 Each processor takes a list of messages and returns a modified list of messages.                 Processors can be sync or async and are applied in sequence.             event_stream_handler: Optional handler for events from the model's streaming response and the agent's execution of tools.         \"\"\"         if model is None or defer_model_check:             self._model = model         else:             self._model = models.infer_model(model)          self._name = name         self.end_strategy = end_strategy         self.model_settings = model_settings          self._output_type = output_type         self.instrument = instrument         self._deps_type = deps_type          if mcp_servers := _deprecated_kwargs.pop('mcp_servers', None):             if toolsets is not None:  # pragma: no cover                 raise TypeError('`mcp_servers` and `toolsets` cannot be set at the same time.')             warnings.warn('`mcp_servers` is deprecated, use `toolsets` instead', DeprecationWarning)             toolsets = mcp_servers          _utils.validate_empty_kwargs(_deprecated_kwargs)          default_output_mode = (             self.model.profile.default_structured_output_mode if isinstance(self.model, models.Model) else None         )          self._output_schema = _output.OutputSchema[OutputDataT].build(output_type, default_mode=default_output_mode)         self._output_validators = []          self._instructions = self._normalize_instructions(instructions)          self._system_prompts = (system_prompt,) if isinstance(system_prompt, str) else tuple(system_prompt)         self._system_prompt_functions = []         self._system_prompt_dynamic_functions = {}          self._max_result_retries = output_retries if output_retries is not None else retries         self._max_tool_retries = retries          self._builtin_tools = builtin_tools          self._prepare_tools = prepare_tools         self._prepare_output_tools = prepare_output_tools          self._output_toolset = self._output_schema.toolset         if self._output_toolset:             self._output_toolset.max_retries = self._max_result_retries          self._function_toolset = _AgentFunctionToolset(             tools, max_retries=self._max_tool_retries, output_schema=self._output_schema         )         self._dynamic_toolsets = [             DynamicToolset[AgentDepsT](toolset_func=toolset)             for toolset in toolsets or []             if not isinstance(toolset, AbstractToolset)         ]         self._user_toolsets = [toolset for toolset in toolsets or [] if isinstance(toolset, AbstractToolset)]          self.history_processors = history_processors or []          self._event_stream_handler = event_stream_handler          self._override_name: ContextVar[_utils.Option[str]] = ContextVar('_override_name', default=None)         self._override_deps: ContextVar[_utils.Option[AgentDepsT]] = ContextVar('_override_deps', default=None)         self._override_model: ContextVar[_utils.Option[models.Model]] = ContextVar('_override_model', default=None)         self._override_toolsets: ContextVar[_utils.Option[Sequence[AbstractToolset[AgentDepsT]]]] = ContextVar(             '_override_toolsets', default=None         )         self._override_tools: ContextVar[             _utils.Option[Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]]]         ] = ContextVar('_override_tools', default=None)         self._override_instructions: ContextVar[             _utils.Option[list[str | _system_prompt.SystemPromptFunc[AgentDepsT]]]         ] = ContextVar('_override_instructions', default=None)          self._enter_lock = Lock()         self._entered_count = 0         self._exit_stack = None      @staticmethod     def instrument_all(instrument: InstrumentationSettings | bool = True) -> None:         \"\"\"Set the instrumentation options for all agents where `instrument` is not set.\"\"\"         Agent._instrument_default = instrument      @property     def model(self) -> models.Model | models.KnownModelName | str | None:         \"\"\"The default model configured for this agent.\"\"\"         return self._model      @model.setter     def model(self, value: models.Model | models.KnownModelName | str | None) -> None:         \"\"\"Set the default model configured for this agent.          We allow `str` here since the actual list of allowed models changes frequently.         \"\"\"         self._model = value      @property     def name(self) -> str | None:         \"\"\"The name of the agent, used for logging.          If `None`, we try to infer the agent name from the call frame when the agent is first run.         \"\"\"         name_ = self._override_name.get()         return name_.value if name_ else self._name      @name.setter     def name(self, value: str | None) -> None:         \"\"\"Set the name of the agent, used for logging.\"\"\"         self._name = value      @property     def deps_type(self) -> type:         \"\"\"The type of dependencies used by the agent.\"\"\"         return self._deps_type      @property     def output_type(self) -> OutputSpec[OutputDataT]:         \"\"\"The type of data output by agent runs, used to validate the data returned by the model, defaults to `str`.\"\"\"         return self._output_type      @property     def event_stream_handler(self) -> EventStreamHandler[AgentDepsT] | None:         \"\"\"Optional handler for events from the model's streaming response and the agent's execution of tools.\"\"\"         return self._event_stream_handler      def __repr__(self) -> str:         return f'{type(self).__name__}(model={self.model!r}, name={self.name!r}, end_strategy={self.end_strategy!r}, model_settings={self.model_settings!r}, output_type={self.output_type!r}, instrument={self.instrument!r})'      @overload     def iter(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     ) -> AbstractAsyncContextManager[AgentRun[AgentDepsT, OutputDataT]]: ...      @overload     def iter(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT],         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     ) -> AbstractAsyncContextManager[AgentRun[AgentDepsT, RunOutputDataT]]: ...      @asynccontextmanager     async def iter(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT] | None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     ) -> AsyncIterator[AgentRun[AgentDepsT, Any]]:         \"\"\"A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.          This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an         `AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are         executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the         stream of events coming from the execution of tools.          The `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,         and the final result of the run once it has completed.          For more details, see the documentation of `AgentRun`.          Example:         ```python         from pydantic_ai import Agent          agent = Agent('openai:gpt-4o')          async def main():             nodes = []             async with agent.iter('What is the capital of France?') as agent_run:                 async for node in agent_run:                     nodes.append(node)             print(nodes)             '''             [                 UserPromptNode(                     user_prompt='What is the capital of France?',                     instructions_functions=[],                     system_prompts=(),                     system_prompt_functions=[],                     system_prompt_dynamic_functions={},                 ),                 ModelRequestNode(                     request=ModelRequest(                         parts=[                             UserPromptPart(                                 content='What is the capital of France?',                                 timestamp=datetime.datetime(...),                             )                         ]                     )                 ),                 CallToolsNode(                     model_response=ModelResponse(                         parts=[TextPart(content='The capital of France is Paris.')],                         usage=RequestUsage(input_tokens=56, output_tokens=7),                         model_name='gpt-4o',                         timestamp=datetime.datetime(...),                     )                 ),                 End(data=FinalResult(output='The capital of France is Paris.')),             ]             '''             print(agent_run.result.output)             #> The capital of France is Paris.         ```          Args:             user_prompt: User input to start/continue the conversation.             output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no                 output validators since output validators would expect an argument that matches the agent's output type.             message_history: History of the conversation so far.             deferred_tool_results: Optional results for deferred tool calls in the message history.             model: Optional model to use for this run, required if `model` was not set when creating the agent.             deps: Optional dependencies to use for this run.             model_settings: Optional settings to use for this model's request.             usage_limits: Optional limits on model request count or token usage.             usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.             infer_name: Whether to try to infer the agent name from the call frame if it's not set.             toolsets: Optional additional toolsets for this run.             builtin_tools: Optional additional builtin tools for this run.          Returns:             The result of the run.         \"\"\"         if infer_name and self.name is None:             self._infer_name(inspect.currentframe())          model_used = self._get_model(model)         del model          deps = self._get_deps(deps)         output_schema = self._prepare_output_schema(output_type, model_used.profile)          output_type_ = output_type or self.output_type          # We consider it a user error if a user tries to restrict the result type while having an output validator that         # may change the result type from the restricted type to something else. Therefore, we consider the following         # typecast reasonable, even though it is possible to violate it with otherwise-type-checked code.         output_validators = cast(list[_output.OutputValidator[AgentDepsT, RunOutputDataT]], self._output_validators)          output_toolset = self._output_toolset         if output_schema != self._output_schema or output_validators:             output_toolset = cast(OutputToolset[AgentDepsT], output_schema.toolset)             if output_toolset:                 output_toolset.max_retries = self._max_result_retries                 output_toolset.output_validators = output_validators         toolset = self._get_toolset(output_toolset=output_toolset, additional_toolsets=toolsets)         tool_manager = ToolManager[AgentDepsT](toolset)          # Build the graph         graph = _agent_graph.build_agent_graph(self.name, self._deps_type, output_type_)          # Build the initial state         usage = usage or _usage.RunUsage()         state = _agent_graph.GraphAgentState(             message_history=list(message_history) if message_history else [],             usage=usage,             retries=0,             run_step=0,         )          # Merge model settings in order of precedence: run > agent > model         merged_settings = merge_model_settings(model_used.settings, self.model_settings)         model_settings = merge_model_settings(merged_settings, model_settings)         usage_limits = usage_limits or _usage.UsageLimits()          instructions_literal, instructions_functions = self._get_instructions()          async def get_instructions(run_context: RunContext[AgentDepsT]) -> str | None:             parts = [                 instructions_literal,                 *[await func.run(run_context) for func in instructions_functions],             ]              model_profile = model_used.profile             if isinstance(output_schema, _output.PromptedOutputSchema):                 instructions = output_schema.instructions(model_profile.prompted_output_template)                 parts.append(instructions)              parts = [p for p in parts if p]             if not parts:                 return None             return '\\n\\n'.join(parts).strip()          if isinstance(model_used, InstrumentedModel):             instrumentation_settings = model_used.instrumentation_settings             tracer = model_used.instrumentation_settings.tracer         else:             instrumentation_settings = None             tracer = NoOpTracer()          graph_deps = _agent_graph.GraphAgentDeps[AgentDepsT, RunOutputDataT](             user_deps=deps,             prompt=user_prompt,             new_message_index=len(message_history) if message_history else 0,             model=model_used,             model_settings=model_settings,             usage_limits=usage_limits,             max_result_retries=self._max_result_retries,             end_strategy=self.end_strategy,             output_schema=output_schema,             output_validators=output_validators,             history_processors=self.history_processors,             builtin_tools=[*self._builtin_tools, *(builtin_tools or [])],             tool_manager=tool_manager,             tracer=tracer,             get_instructions=get_instructions,             instrumentation_settings=instrumentation_settings,         )          user_prompt_node = _agent_graph.UserPromptNode[AgentDepsT](             user_prompt=user_prompt,             deferred_tool_results=deferred_tool_results,             instructions=instructions_literal,             instructions_functions=instructions_functions,             system_prompts=self._system_prompts,             system_prompt_functions=self._system_prompt_functions,             system_prompt_dynamic_functions=self._system_prompt_dynamic_functions,         )          agent_name = self.name or 'agent'         instrumentation_names = InstrumentationNames.for_version(             instrumentation_settings.version if instrumentation_settings else DEFAULT_INSTRUMENTATION_VERSION         )          run_span = tracer.start_span(             instrumentation_names.get_agent_run_span_name(agent_name),             attributes={                 'model_name': model_used.model_name if model_used else 'no-model',                 'agent_name': agent_name,                 'gen_ai.agent.name': agent_name,                 'logfire.msg': f'{agent_name} run',             },         )          try:             async with graph.iter(                 inputs=user_prompt_node,                 state=state,                 deps=graph_deps,                 span=use_span(run_span) if run_span.is_recording() else None,                 infer_name=False,             ) as graph_run:                 async with toolset:                     agent_run = AgentRun(graph_run)                     yield agent_run                     if (final_result := agent_run.result) is not None and run_span.is_recording():                         if instrumentation_settings and instrumentation_settings.include_content:                             run_span.set_attribute(                                 'final_result',                                 (                                     final_result.output                                     if isinstance(final_result.output, str)                                     else json.dumps(InstrumentedModel.serialize_any(final_result.output))                                 ),                             )         finally:             try:                 if instrumentation_settings and run_span.is_recording():                     run_span.set_attributes(                         self._run_span_end_attributes(                             instrumentation_settings, usage, state.message_history, graph_deps.new_message_index                         )                     )             finally:                 run_span.end()      def _run_span_end_attributes(         self,         settings: InstrumentationSettings,         usage: _usage.RunUsage,         message_history: list[_messages.ModelMessage],         new_message_index: int,     ):         if settings.version == 1:             attrs = {                 'all_messages_events': json.dumps(                     [InstrumentedModel.event_to_dict(e) for e in settings.messages_to_otel_events(message_history)]                 )             }         else:             # Store the last instructions here for convenience             last_instructions = InstrumentedModel._get_instructions(message_history)  # pyright: ignore[reportPrivateUsage]             attrs: dict[str, Any] = {                 'pydantic_ai.all_messages': json.dumps(settings.messages_to_otel_messages(list(message_history))),                 **settings.system_instructions_attributes(last_instructions),             }              # If this agent run was provided with existing history, store an attribute indicating the point at which the             # new messages begin.             if new_message_index > 0:                 attrs['pydantic_ai.new_message_index'] = new_message_index              # If the instructions for this agent run were not always the same, store an attribute that indicates that.             # This can signal to an observability UI that different steps in the agent run had different instructions.             # Note: We purposely only look at \"new\" messages because they are the only ones produced by this agent run.             if any(                 (                     isinstance(m, _messages.ModelRequest)                     and m.instructions is not None                     and m.instructions != last_instructions                 )                 for m in message_history[new_message_index:]             ):                 attrs['pydantic_ai.variable_instructions'] = True          return {             **usage.opentelemetry_attributes(),             **attrs,             'logfire.json_schema': json.dumps(                 {                     'type': 'object',                     'properties': {                         **{k: {'type': 'array'} if isinstance(v, str) else {} for k, v in attrs.items()},                         'final_result': {'type': 'object'},                     },                 }             ),         }      @contextmanager     def override(         self,         *,         name: str | _utils.Unset = _utils.UNSET,         deps: AgentDepsT | _utils.Unset = _utils.UNSET,         model: models.Model | models.KnownModelName | str | _utils.Unset = _utils.UNSET,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | _utils.Unset = _utils.UNSET,         tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | _utils.Unset = _utils.UNSET,         instructions: Instructions[AgentDepsT] | _utils.Unset = _utils.UNSET,     ) -> Iterator[None]:         \"\"\"Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.          This is particularly useful when testing.         You can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).          Args:             name: The name to use instead of the name passed to the agent constructor and agent run.             deps: The dependencies to use instead of the dependencies passed to the agent run.             model: The model to use instead of the model passed to the agent run.             toolsets: The toolsets to use instead of the toolsets passed to the agent constructor and agent run.             tools: The tools to use instead of the tools registered with the agent.             instructions: The instructions to use instead of the instructions registered with the agent.         \"\"\"         if _utils.is_set(name):             name_token = self._override_name.set(_utils.Some(name))         else:             name_token = None          if _utils.is_set(deps):             deps_token = self._override_deps.set(_utils.Some(deps))         else:             deps_token = None          if _utils.is_set(model):             model_token = self._override_model.set(_utils.Some(models.infer_model(model)))         else:             model_token = None          if _utils.is_set(toolsets):             toolsets_token = self._override_toolsets.set(_utils.Some(toolsets))         else:             toolsets_token = None          if _utils.is_set(tools):             tools_token = self._override_tools.set(_utils.Some(tools))         else:             tools_token = None          if _utils.is_set(instructions):             normalized_instructions = self._normalize_instructions(instructions)             instructions_token = self._override_instructions.set(_utils.Some(normalized_instructions))         else:             instructions_token = None          try:             yield         finally:             if name_token is not None:                 self._override_name.reset(name_token)             if deps_token is not None:                 self._override_deps.reset(deps_token)             if model_token is not None:                 self._override_model.reset(model_token)             if toolsets_token is not None:                 self._override_toolsets.reset(toolsets_token)             if tools_token is not None:                 self._override_tools.reset(tools_token)             if instructions_token is not None:                 self._override_instructions.reset(instructions_token)      @overload     def instructions(         self, func: Callable[[RunContext[AgentDepsT]], str], /     ) -> Callable[[RunContext[AgentDepsT]], str]: ...      @overload     def instructions(         self, func: Callable[[RunContext[AgentDepsT]], Awaitable[str]], /     ) -> Callable[[RunContext[AgentDepsT]], Awaitable[str]]: ...      @overload     def instructions(self, func: Callable[[], str], /) -> Callable[[], str]: ...      @overload     def instructions(self, func: Callable[[], Awaitable[str]], /) -> Callable[[], Awaitable[str]]: ...      @overload     def instructions(         self, /     ) -> Callable[[_system_prompt.SystemPromptFunc[AgentDepsT]], _system_prompt.SystemPromptFunc[AgentDepsT]]: ...      def instructions(         self,         func: _system_prompt.SystemPromptFunc[AgentDepsT] | None = None,         /,     ) -> (         Callable[[_system_prompt.SystemPromptFunc[AgentDepsT]], _system_prompt.SystemPromptFunc[AgentDepsT]]         | _system_prompt.SystemPromptFunc[AgentDepsT]     ):         \"\"\"Decorator to register an instructions function.          Optionally takes [`RunContext`][pydantic_ai.tools.RunContext] as its only argument.         Can decorate a sync or async functions.          The decorator can be used bare (`agent.instructions`).          Overloads for every possible signature of `instructions` are included so the decorator doesn't obscure         the type of the function.          Example:         ```python         from pydantic_ai import Agent, RunContext          agent = Agent('test', deps_type=str)          @agent.instructions         def simple_instructions() -> str:             return 'foobar'          @agent.instructions         async def async_instructions(ctx: RunContext[str]) -> str:             return f'{ctx.deps} is the best'         ```         \"\"\"         if func is None:              def decorator(                 func_: _system_prompt.SystemPromptFunc[AgentDepsT],             ) -> _system_prompt.SystemPromptFunc[AgentDepsT]:                 self._instructions.append(func_)                 return func_              return decorator         else:             self._instructions.append(func)             return func      @overload     def system_prompt(         self, func: Callable[[RunContext[AgentDepsT]], str], /     ) -> Callable[[RunContext[AgentDepsT]], str]: ...      @overload     def system_prompt(         self, func: Callable[[RunContext[AgentDepsT]], Awaitable[str]], /     ) -> Callable[[RunContext[AgentDepsT]], Awaitable[str]]: ...      @overload     def system_prompt(self, func: Callable[[], str], /) -> Callable[[], str]: ...      @overload     def system_prompt(self, func: Callable[[], Awaitable[str]], /) -> Callable[[], Awaitable[str]]: ...      @overload     def system_prompt(         self, /, *, dynamic: bool = False     ) -> Callable[[_system_prompt.SystemPromptFunc[AgentDepsT]], _system_prompt.SystemPromptFunc[AgentDepsT]]: ...      def system_prompt(         self,         func: _system_prompt.SystemPromptFunc[AgentDepsT] | None = None,         /,         *,         dynamic: bool = False,     ) -> (         Callable[[_system_prompt.SystemPromptFunc[AgentDepsT]], _system_prompt.SystemPromptFunc[AgentDepsT]]         | _system_prompt.SystemPromptFunc[AgentDepsT]     ):         \"\"\"Decorator to register a system prompt function.          Optionally takes [`RunContext`][pydantic_ai.tools.RunContext] as its only argument.         Can decorate a sync or async functions.          The decorator can be used either bare (`agent.system_prompt`) or as a function call         (`agent.system_prompt(...)`), see the examples below.          Overloads for every possible signature of `system_prompt` are included so the decorator doesn't obscure         the type of the function, see `tests/typed_agent.py` for tests.          Args:             func: The function to decorate             dynamic: If True, the system prompt will be reevaluated even when `messages_history` is provided,                 see [`SystemPromptPart.dynamic_ref`][pydantic_ai.messages.SystemPromptPart.dynamic_ref]          Example:         ```python         from pydantic_ai import Agent, RunContext          agent = Agent('test', deps_type=str)          @agent.system_prompt         def simple_system_prompt() -> str:             return 'foobar'          @agent.system_prompt(dynamic=True)         async def async_system_prompt(ctx: RunContext[str]) -> str:             return f'{ctx.deps} is the best'         ```         \"\"\"         if func is None:              def decorator(                 func_: _system_prompt.SystemPromptFunc[AgentDepsT],             ) -> _system_prompt.SystemPromptFunc[AgentDepsT]:                 runner = _system_prompt.SystemPromptRunner[AgentDepsT](func_, dynamic=dynamic)                 self._system_prompt_functions.append(runner)                 if dynamic:  # pragma: lax no cover                     self._system_prompt_dynamic_functions[func_.__qualname__] = runner                 return func_              return decorator         else:             assert not dynamic, \"dynamic can't be True in this case\"             self._system_prompt_functions.append(_system_prompt.SystemPromptRunner[AgentDepsT](func, dynamic=dynamic))             return func      @overload     def output_validator(         self, func: Callable[[RunContext[AgentDepsT], OutputDataT], OutputDataT], /     ) -> Callable[[RunContext[AgentDepsT], OutputDataT], OutputDataT]: ...      @overload     def output_validator(         self, func: Callable[[RunContext[AgentDepsT], OutputDataT], Awaitable[OutputDataT]], /     ) -> Callable[[RunContext[AgentDepsT], OutputDataT], Awaitable[OutputDataT]]: ...      @overload     def output_validator(         self, func: Callable[[OutputDataT], OutputDataT], /     ) -> Callable[[OutputDataT], OutputDataT]: ...      @overload     def output_validator(         self, func: Callable[[OutputDataT], Awaitable[OutputDataT]], /     ) -> Callable[[OutputDataT], Awaitable[OutputDataT]]: ...      def output_validator(         self, func: _output.OutputValidatorFunc[AgentDepsT, OutputDataT], /     ) -> _output.OutputValidatorFunc[AgentDepsT, OutputDataT]:         \"\"\"Decorator to register an output validator function.          Optionally takes [`RunContext`][pydantic_ai.tools.RunContext] as its first argument.         Can decorate a sync or async functions.          Overloads for every possible signature of `output_validator` are included so the decorator doesn't obscure         the type of the function, see `tests/typed_agent.py` for tests.          Example:         ```python         from pydantic_ai import Agent, ModelRetry, RunContext          agent = Agent('test', deps_type=str)          @agent.output_validator         def output_validator_simple(data: str) -> str:             if 'wrong' in data:                 raise ModelRetry('wrong response')             return data          @agent.output_validator         async def output_validator_deps(ctx: RunContext[str], data: str) -> str:             if ctx.deps in data:                 raise ModelRetry('wrong response')             return data          result = agent.run_sync('foobar', deps='spam')         print(result.output)         #> success (no tool calls)         ```         \"\"\"         self._output_validators.append(_output.OutputValidator[AgentDepsT, Any](func))         return func      @overload     def tool(self, func: ToolFuncContext[AgentDepsT, ToolParams], /) -> ToolFuncContext[AgentDepsT, ToolParams]: ...      @overload     def tool(         self,         /,         *,         name: str | None = None,         description: str | None = None,         retries: int | None = None,         prepare: ToolPrepareFunc[AgentDepsT] | None = None,         docstring_format: DocstringFormat = 'auto',         require_parameter_descriptions: bool = False,         schema_generator: type[GenerateJsonSchema] = GenerateToolJsonSchema,         strict: bool | None = None,         sequential: bool = False,         requires_approval: bool = False,         metadata: dict[str, Any] | None = None,     ) -> Callable[[ToolFuncContext[AgentDepsT, ToolParams]], ToolFuncContext[AgentDepsT, ToolParams]]: ...      def tool(         self,         func: ToolFuncContext[AgentDepsT, ToolParams] | None = None,         /,         *,         name: str | None = None,         description: str | None = None,         retries: int | None = None,         prepare: ToolPrepareFunc[AgentDepsT] | None = None,         docstring_format: DocstringFormat = 'auto',         require_parameter_descriptions: bool = False,         schema_generator: type[GenerateJsonSchema] = GenerateToolJsonSchema,         strict: bool | None = None,         sequential: bool = False,         requires_approval: bool = False,         metadata: dict[str, Any] | None = None,     ) -> Any:         \"\"\"Decorator to register a tool function which takes [`RunContext`][pydantic_ai.tools.RunContext] as its first argument.          Can decorate a sync or async functions.          The docstring is inspected to extract both the tool description and description of each parameter,         [learn more](../tools.md#function-tools-and-schema).          We can't add overloads for every possible signature of tool, since the return type is a recursive union         so the signature of functions decorated with `@agent.tool` is obscured.          Example:         ```python         from pydantic_ai import Agent, RunContext          agent = Agent('test', deps_type=int)          @agent.tool         def foobar(ctx: RunContext[int], x: int) -> int:             return ctx.deps + x          @agent.tool(retries=2)         async def spam(ctx: RunContext[str], y: float) -> float:             return ctx.deps + y          result = agent.run_sync('foobar', deps=1)         print(result.output)         #> {\"foobar\":1,\"spam\":1.0}         ```          Args:             func: The tool function to register.             name: The name of the tool, defaults to the function name.             description: The description of the tool, defaults to the function docstring.             retries: The number of retries to allow for this tool, defaults to the agent's default retries,                 which defaults to 1.             prepare: custom method to prepare the tool definition for each step, return `None` to omit this                 tool from a given step. This is useful if you want to customise a tool at call time,                 or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].             docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].                 Defaults to `'auto'`, such that the format is inferred from the structure of the docstring.             require_parameter_descriptions: If True, raise an error if a parameter description is missing. Defaults to False.             schema_generator: The JSON schema generator class to use for this tool. Defaults to `GenerateToolJsonSchema`.             strict: Whether to enforce JSON schema compliance (only affects OpenAI).                 See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.             sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.             requires_approval: Whether this tool requires human-in-the-loop approval. Defaults to False.                 See the [tools documentation](../deferred-tools.md#human-in-the-loop-tool-approval) for more info.             metadata: Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization.         \"\"\"          def tool_decorator(             func_: ToolFuncContext[AgentDepsT, ToolParams],         ) -> ToolFuncContext[AgentDepsT, ToolParams]:             # noinspection PyTypeChecker             self._function_toolset.add_function(                 func_,                 takes_ctx=True,                 name=name,                 description=description,                 retries=retries,                 prepare=prepare,                 docstring_format=docstring_format,                 require_parameter_descriptions=require_parameter_descriptions,                 schema_generator=schema_generator,                 strict=strict,                 sequential=sequential,                 requires_approval=requires_approval,                 metadata=metadata,             )             return func_          return tool_decorator if func is None else tool_decorator(func)      @overload     def tool_plain(self, func: ToolFuncPlain[ToolParams], /) -> ToolFuncPlain[ToolParams]: ...      @overload     def tool_plain(         self,         /,         *,         name: str | None = None,         description: str | None = None,         retries: int | None = None,         prepare: ToolPrepareFunc[AgentDepsT] | None = None,         docstring_format: DocstringFormat = 'auto',         require_parameter_descriptions: bool = False,         schema_generator: type[GenerateJsonSchema] = GenerateToolJsonSchema,         strict: bool | None = None,         sequential: bool = False,         requires_approval: bool = False,         metadata: dict[str, Any] | None = None,     ) -> Callable[[ToolFuncPlain[ToolParams]], ToolFuncPlain[ToolParams]]: ...      def tool_plain(         self,         func: ToolFuncPlain[ToolParams] | None = None,         /,         *,         name: str | None = None,         description: str | None = None,         retries: int | None = None,         prepare: ToolPrepareFunc[AgentDepsT] | None = None,         docstring_format: DocstringFormat = 'auto',         require_parameter_descriptions: bool = False,         schema_generator: type[GenerateJsonSchema] = GenerateToolJsonSchema,         strict: bool | None = None,         sequential: bool = False,         requires_approval: bool = False,         metadata: dict[str, Any] | None = None,     ) -> Any:         \"\"\"Decorator to register a tool function which DOES NOT take `RunContext` as an argument.          Can decorate a sync or async functions.          The docstring is inspected to extract both the tool description and description of each parameter,         [learn more](../tools.md#function-tools-and-schema).          We can't add overloads for every possible signature of tool, since the return type is a recursive union         so the signature of functions decorated with `@agent.tool` is obscured.          Example:         ```python         from pydantic_ai import Agent, RunContext          agent = Agent('test')          @agent.tool         def foobar(ctx: RunContext[int]) -> int:             return 123          @agent.tool(retries=2)         async def spam(ctx: RunContext[str]) -> float:             return 3.14          result = agent.run_sync('foobar', deps=1)         print(result.output)         #> {\"foobar\":123,\"spam\":3.14}         ```          Args:             func: The tool function to register.             name: The name of the tool, defaults to the function name.             description: The description of the tool, defaults to the function docstring.             retries: The number of retries to allow for this tool, defaults to the agent's default retries,                 which defaults to 1.             prepare: custom method to prepare the tool definition for each step, return `None` to omit this                 tool from a given step. This is useful if you want to customise a tool at call time,                 or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].             docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].                 Defaults to `'auto'`, such that the format is inferred from the structure of the docstring.             require_parameter_descriptions: If True, raise an error if a parameter description is missing. Defaults to False.             schema_generator: The JSON schema generator class to use for this tool. Defaults to `GenerateToolJsonSchema`.             strict: Whether to enforce JSON schema compliance (only affects OpenAI).                 See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.             sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.             requires_approval: Whether this tool requires human-in-the-loop approval. Defaults to False.                 See the [tools documentation](../deferred-tools.md#human-in-the-loop-tool-approval) for more info.             metadata: Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization.         \"\"\"          def tool_decorator(func_: ToolFuncPlain[ToolParams]) -> ToolFuncPlain[ToolParams]:             # noinspection PyTypeChecker             self._function_toolset.add_function(                 func_,                 takes_ctx=False,                 name=name,                 description=description,                 retries=retries,                 prepare=prepare,                 docstring_format=docstring_format,                 require_parameter_descriptions=require_parameter_descriptions,                 schema_generator=schema_generator,                 strict=strict,                 sequential=sequential,                 requires_approval=requires_approval,                 metadata=metadata,             )             return func_          return tool_decorator if func is None else tool_decorator(func)      @overload     def toolset(self, func: ToolsetFunc[AgentDepsT], /) -> ToolsetFunc[AgentDepsT]: ...      @overload     def toolset(         self,         /,         *,         per_run_step: bool = True,     ) -> Callable[[ToolsetFunc[AgentDepsT]], ToolsetFunc[AgentDepsT]]: ...      def toolset(         self,         func: ToolsetFunc[AgentDepsT] | None = None,         /,         *,         per_run_step: bool = True,     ) -> Any:         \"\"\"Decorator to register a toolset function which takes [`RunContext`][pydantic_ai.tools.RunContext] as its only argument.          Can decorate a sync or async functions.          The decorator can be used bare (`agent.toolset`).          Example:         ```python         from pydantic_ai import AbstractToolset, Agent, FunctionToolset, RunContext          agent = Agent('test', deps_type=str)          @agent.toolset         async def simple_toolset(ctx: RunContext[str]) -> AbstractToolset[str]:             return FunctionToolset()         ```          Args:             func: The toolset function to register.             per_run_step: Whether to re-evaluate the toolset for each run step. Defaults to True.         \"\"\"          def toolset_decorator(func_: ToolsetFunc[AgentDepsT]) -> ToolsetFunc[AgentDepsT]:             self._dynamic_toolsets.append(DynamicToolset(func_, per_run_step=per_run_step))             return func_          return toolset_decorator if func is None else toolset_decorator(func)      def _get_model(self, model: models.Model | models.KnownModelName | str | None) -> models.Model:         \"\"\"Create a model configured for this agent.          Args:             model: model to use for this run, required if `model` was not set when creating the agent.          Returns:             The model used         \"\"\"         model_: models.Model         if some_model := self._override_model.get():             # we don't want `override()` to cover up errors from the model not being defined, hence this check             if model is None and self.model is None:                 raise exceptions.UserError(                     '`model` must either be set on the agent or included when calling it. '                     '(Even when `override(model=...)` is customizing the model that will actually be called)'                 )             model_ = some_model.value         elif model is not None:             model_ = models.infer_model(model)         elif self.model is not None:             # noinspection PyTypeChecker             model_ = self.model = models.infer_model(self.model)         else:             raise exceptions.UserError('`model` must either be set on the agent or included when calling it.')          instrument = self.instrument         if instrument is None:             instrument = self._instrument_default          return instrument_model(model_, instrument)      def _get_deps(self: Agent[T, OutputDataT], deps: T) -> T:         \"\"\"Get deps for a run.          If we've overridden deps via `_override_deps`, use that, otherwise use the deps passed to the call.          We could do runtime type checking of deps against `self._deps_type`, but that's a slippery slope.         \"\"\"         if some_deps := self._override_deps.get():             return some_deps.value         else:             return deps      def _normalize_instructions(         self,         instructions: Instructions[AgentDepsT],     ) -> list[str | _system_prompt.SystemPromptFunc[AgentDepsT]]:         if instructions is None:             return []         if isinstance(instructions, str) or callable(instructions):             return [instructions]         return list(instructions)      def _get_instructions(         self,     ) -> tuple[str | None, list[_system_prompt.SystemPromptRunner[AgentDepsT]]]:         override_instructions = self._override_instructions.get()         instructions = override_instructions.value if override_instructions else self._instructions          literal_parts: list[str] = []         functions: list[_system_prompt.SystemPromptRunner[AgentDepsT]] = []          for instruction in instructions:             if isinstance(instruction, str):                 literal_parts.append(instruction)             else:                 functions.append(_system_prompt.SystemPromptRunner[AgentDepsT](instruction))          literal = '\\n'.join(literal_parts).strip() or None         return literal, functions      def _get_toolset(         self,         output_toolset: AbstractToolset[AgentDepsT] | None | _utils.Unset = _utils.UNSET,         additional_toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     ) -> AbstractToolset[AgentDepsT]:         \"\"\"Get the complete toolset.          Args:             output_toolset: The output toolset to use instead of the one built at agent construction time.             additional_toolsets: Additional toolsets to add, unless toolsets have been overridden.         \"\"\"         toolsets = self.toolsets         # Don't add additional toolsets if the toolsets have been overridden         if additional_toolsets and self._override_toolsets.get() is None:             toolsets = [*toolsets, *additional_toolsets]          toolset = CombinedToolset(toolsets)          # Copy the dynamic toolsets to ensure each run has its own instances         def copy_dynamic_toolsets(toolset: AbstractToolset[AgentDepsT]) -> AbstractToolset[AgentDepsT]:             if isinstance(toolset, DynamicToolset):                 return dataclasses.replace(toolset)             else:                 return toolset          toolset = toolset.visit_and_replace(copy_dynamic_toolsets)          if self._prepare_tools:             toolset = PreparedToolset(toolset, self._prepare_tools)          output_toolset = output_toolset if _utils.is_set(output_toolset) else self._output_toolset         if output_toolset is not None:             if self._prepare_output_tools:                 output_toolset = PreparedToolset(output_toolset, self._prepare_output_tools)             toolset = CombinedToolset([output_toolset, toolset])          return toolset      @property     def toolsets(self) -> Sequence[AbstractToolset[AgentDepsT]]:         \"\"\"All toolsets registered on the agent, including a function toolset holding tools that were registered on the agent directly.          Output tools are not included.         \"\"\"         toolsets: list[AbstractToolset[AgentDepsT]] = []          if some_tools := self._override_tools.get():             function_toolset = _AgentFunctionToolset(                 some_tools.value, max_retries=self._max_tool_retries, output_schema=self._output_schema             )         else:             function_toolset = self._function_toolset         toolsets.append(function_toolset)          if some_user_toolsets := self._override_toolsets.get():             user_toolsets = some_user_toolsets.value         else:             user_toolsets = [*self._user_toolsets, *self._dynamic_toolsets]         toolsets.extend(user_toolsets)          return toolsets      def _prepare_output_schema(         self, output_type: OutputSpec[RunOutputDataT] | None, model_profile: ModelProfile     ) -> _output.OutputSchema[RunOutputDataT]:         if output_type is not None:             if self._output_validators:                 raise exceptions.UserError('Cannot set a custom run `output_type` when the agent has output validators')             schema = _output.OutputSchema[RunOutputDataT].build(                 output_type, default_mode=model_profile.default_structured_output_mode             )         else:             schema = self._output_schema.with_default_mode(model_profile.default_structured_output_mode)          schema.raise_if_unsupported(model_profile)          return schema  # pyright: ignore[reportReturnType]      async def __aenter__(self) -> Self:         \"\"\"Enter the agent context.          This will start all [`MCPServerStdio`s][pydantic_ai.mcp.MCPServerStdio] registered as `toolsets` so they are ready to be used.          This is a no-op if the agent has already been entered.         \"\"\"         async with self._enter_lock:             if self._entered_count == 0:                 async with AsyncExitStack() as exit_stack:                     toolset = self._get_toolset()                     await exit_stack.enter_async_context(toolset)                      self._exit_stack = exit_stack.pop_all()             self._entered_count += 1         return self      async def __aexit__(self, *args: Any) -> bool | None:         async with self._enter_lock:             self._entered_count -= 1             if self._entered_count == 0 and self._exit_stack is not None:                 await self._exit_stack.aclose()                 self._exit_stack = None      def set_mcp_sampling_model(self, model: models.Model | models.KnownModelName | str | None = None) -> None:         \"\"\"Set the sampling model on all MCP servers registered with the agent.          If no sampling model is provided, the agent's model will be used.         \"\"\"         try:             sampling_model = models.infer_model(model) if model else self._get_model(None)         except exceptions.UserError as e:             raise exceptions.UserError('No sampling model provided and no model set on the agent.') from e          from ..mcp import MCPServer          def _set_sampling_model(toolset: AbstractToolset[AgentDepsT]) -> None:             if isinstance(toolset, MCPServer):                 toolset.sampling_model = sampling_model          self._get_toolset().apply(_set_sampling_model)      @asynccontextmanager     @deprecated(         '`run_mcp_servers` is deprecated, use `async with agent:` instead. If you need to set a sampling model on all MCP servers, use `agent.set_mcp_sampling_model()`.'     )     async def run_mcp_servers(         self, model: models.Model | models.KnownModelName | str | None = None     ) -> AsyncIterator[None]:         \"\"\"Run [`MCPServerStdio`s][pydantic_ai.mcp.MCPServerStdio] so they can be used by the agent.          Deprecated: use [`async with agent`][pydantic_ai.agent.Agent.__aenter__] instead.         If you need to set a sampling model on all MCP servers, use [`agent.set_mcp_sampling_model()`][pydantic_ai.agent.Agent.set_mcp_sampling_model].          Returns: a context manager to start and shutdown the servers.         \"\"\"         try:             self.set_mcp_sampling_model(model)         except exceptions.UserError:             if model is not None:                 raise          async with self:             yield ``` |", "url": "https://ai.pydantic.dev/agent/index.html#agent-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "Agent `dataclass`", "anchor": "agent-dataclass", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    model: Model | KnownModelName | str | None = None,\n    *,\n    output_type: OutputSpec[OutputDataT] = str,\n    instructions: Instructions[AgentDepsT] = None,\n    system_prompt: str | Sequence[str] = (),\n    deps_type: type[AgentDepsT] = NoneType,\n    name: str | None = None,\n    model_settings: ModelSettings | None = None,\n    retries: int = 1,\n    output_retries: int | None = None,\n    tools: Sequence[\n        Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]\n    ] = (),\n    builtin_tools: Sequence[AbstractBuiltinTool] = (),\n    prepare_tools: (\n        ToolsPrepareFunc[AgentDepsT] | None\n    ) = None,\n    prepare_output_tools: (\n        ToolsPrepareFunc[AgentDepsT] | None\n    ) = None,\n    toolsets: (\n        Sequence[\n            AbstractToolset[AgentDepsT]\n            | ToolsetFunc[AgentDepsT]\n        ]\n        | None\n    ) = None,\n    defer_model_check: bool = False,\n    end_strategy: EndStrategy = \"early\",\n    instrument: (\n        InstrumentationSettings | bool | None\n    ) = None,\n    history_processors: (\n        Sequence[HistoryProcessor[AgentDepsT]] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> None\n\n__init__(\n    model: Model | KnownModelName | str | None = None,\n    *,\n    output_type: OutputSpec[OutputDataT] = str,\n    instructions: Instructions[AgentDepsT] = None,\n    system_prompt: str | Sequence[str] = (),\n    deps_type: type[AgentDepsT] = NoneType,\n    name: str | None = None,\n    model_settings: ModelSettings | None = None,\n    retries: int = 1,\n    output_retries: int | None = None,\n    tools: Sequence[\n        Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]\n    ] = (),\n    builtin_tools: Sequence[AbstractBuiltinTool] = (),\n    prepare_tools: (\n        ToolsPrepareFunc[AgentDepsT] | None\n    ) = None,\n    prepare_output_tools: (\n        ToolsPrepareFunc[AgentDepsT] | None\n    ) = None,\n    mcp_servers: Sequence[MCPServer] = (),\n    defer_model_check: bool = False,\n    end_strategy: EndStrategy = \"early\",\n    instrument: (\n        InstrumentationSettings | bool | None\n    ) = None,\n    history_processors: (\n        Sequence[HistoryProcessor[AgentDepsT]] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> None\n\n__init__(\n    model: Model | KnownModelName | str | None = None,\n    *,\n    output_type: OutputSpec[OutputDataT] = str,\n    instructions: Instructions[AgentDepsT] = None,\n    system_prompt: str | Sequence[str] = (),\n    deps_type: type[AgentDepsT] = NoneType,\n    name: str | None = None,\n    model_settings: ModelSettings | None = None,\n    retries: int = 1,\n    output_retries: int | None = None,\n    tools: Sequence[\n        Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]\n    ] = (),\n    builtin_tools: Sequence[AbstractBuiltinTool] = (),\n    prepare_tools: (\n        ToolsPrepareFunc[AgentDepsT] | None\n    ) = None,\n    prepare_output_tools: (\n        ToolsPrepareFunc[AgentDepsT] | None\n    ) = None,\n    toolsets: (\n        Sequence[\n            AbstractToolset[AgentDepsT]\n            | ToolsetFunc[AgentDepsT]\n        ]\n        | None\n    ) = None,\n    defer_model_check: bool = False,\n    end_strategy: EndStrategy = \"early\",\n    instrument: (\n        InstrumentationSettings | bool | None\n    ) = None,\n    history_processors: (\n        Sequence[HistoryProcessor[AgentDepsT]] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None,\n    **_deprecated_kwargs: Any\n)\n```\n\nCreate an agent.\n\nParameters:", "url": "https://ai.pydantic.dev/agent/index.html#agent-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "Agent `dataclass`", "anchor": "agent-dataclass", "md_text": "| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model` | `Model | KnownModelName | str | None` | The default model to use for this agent, if not provided, you must provide the model when calling it. We allow `str` here since the actual list of allowed models changes frequently. | `None` |\n| `output_type` | `OutputSpec[OutputDataT]` | The type of the output data, used to validate the data returned by the model, defaults to `str`. | `str` |\n| `instructions` | `Instructions[AgentDepsT]` | Instructions to use for this agent, you can also register instructions via a function with [`instructions`](index.html#pydantic_ai.agent.Agent.instructions). | `None` |\n| `system_prompt` | `str | Sequence[str]` | Static system prompts to use for this agent, you can also register system prompts via a function with [`system_prompt`](index.html#pydantic_ai.agent.Agent.system_prompt). | `()` |\n| `deps_type` | `type[AgentDepsT]` | The type used for dependency injection, this parameter exists solely to allow you to fully parameterize the agent, and therefore get the best out of static type checking. If you're not using deps, but want type checking to pass, you can set `deps=None` to satisfy Pyright or add a type hint `: Agent[None, <return type>]`. | `NoneType` |\n| `name` | `str | None` | The name of the agent, used for logging. If `None`, we try to infer the agent name from the call frame when the agent is first run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional model request settings to use for this agent's runs, by default. | `None` |\n| `retries` | `int` | The default number of retries to allow for tool calls and output validation, before raising an error. For model request retries, see the [HTTP Request Retries](https://ai.pydantic.dev/retries/) documentation. | `1` |\n| `output_retries` | `int | None` | The maximum number of retries to allow for output validation, defaults to `retries`. | `None` |\n| `tools` | `Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]]` | Tools to register with the agent, you can also register tools via the decorators [`@agent.tool`](index.html#pydantic_ai.agent.Agent.tool) and [`@agent.tool_plain`](index.html#pydantic_ai.agent.Agent.tool_plain). | `()` |\n| `builtin_tools` | `Sequence[AbstractBuiltinTool]` | The builtin tools that the agent will use. This depends on the model, as some models may not support certain tools. If the model doesn't support the builtin tools, an error will be raised. | `()` |\n| `prepare_tools` | `ToolsPrepareFunc[AgentDepsT] | None` | Custom function to prepare the tool definition of all tools for each step, except output tools. This is useful if you want to customize the definition of multiple tools or you want to register a subset of tools for a given step. See [`ToolsPrepareFunc`](../tools/index.html#pydantic_ai.tools.ToolsPrepareFunc) | `None` |\n| `prepare_output_tools` | `ToolsPrepareFunc[AgentDepsT] | None` | Custom function to prepare the tool definition of all output tools for each step. This is useful if you want to customize the definition of multiple output tools or you want to register a subset of output tools for a given step. See [`ToolsPrepareFunc`](../tools/index.html#pydantic_ai.tools.ToolsPrepareFunc) | `None` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT] | ToolsetFunc[AgentDepsT]] | None` | Toolsets to register with the agent, including MCP servers and functions which take a run context and return a toolset. See [`ToolsetFunc`](../toolsets/index.html#pydantic_ai.toolsets.ToolsetFunc) for more information. | `None` |\n| `defer_model_check` | `bool` | by default, if you provide a [named](../models/base/index.html#pydantic_ai.models.KnownModelName) model, it's evaluated to create a [`Model`](../models/base/index.html#pydantic_ai.models.Model) instance immediately, which checks for the necessary environment variables. Set this to `false` to defer the evaluation until the first run. Useful if you want to [override the model](index.html#pydantic_ai.agent.Agent.override) for testing. | `False` |\n| `end_strategy` | `EndStrategy` | Strategy for handling tool calls that are requested alongside a final result. See [`EndStrategy`](index.html#pydantic_ai.agent.EndStrategy) for more information. | `'early'` |\n| `instrument` | `InstrumentationSettings | bool | None` | Set to True to automatically instrument with OpenTelemetry, which will use Logfire if it's configured. Set to an instance of [`InstrumentationSettings`](index.html#pydantic_ai.agent.InstrumentationSettings) to customize. If this isn't set, then the last value set by [`Agent.instrument_all()`](index.html#pydantic_ai.agent.Agent.instrument_all) will be used, which defaults to False. See the [Debugging and Monitoring guide](https://ai.pydantic.dev/logfire/) for more info. | `None` |\n| `history_processors` | `Sequence[HistoryProcessor[AgentDepsT]] | None` | Optional list of callables to process the message history before sending it to the model. Each processor takes a list of messages and returns a modified list of messages. Processors can be sync or async and are applied in sequence. | `None` |\n| `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional handler for events from the model's streaming response and the agent's execution of tools. | `None` |", "url": "https://ai.pydantic.dev/agent/index.html#agent-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "Agent `dataclass`", "anchor": "agent-dataclass", "md_text": "Source code in `pydantic_ai_slim/pydantic_ai/agent/__init__.py`", "url": "https://ai.pydantic.dev/agent/index.html#agent-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "Agent `dataclass`", "anchor": "agent-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 ``` | ``` def __init__(     self,     model: models.Model | models.KnownModelName | str | None = None,     *,     output_type: OutputSpec[OutputDataT] = str,     instructions: Instructions[AgentDepsT] = None,     system_prompt: str | Sequence[str] = (),     deps_type: type[AgentDepsT] = NoneType,     name: str | None = None,     model_settings: ModelSettings | None = None,     retries: int = 1,     output_retries: int | None = None,     tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] = (),     builtin_tools: Sequence[AbstractBuiltinTool] = (),     prepare_tools: ToolsPrepareFunc[AgentDepsT] | None = None,     prepare_output_tools: ToolsPrepareFunc[AgentDepsT] | None = None,     toolsets: Sequence[AbstractToolset[AgentDepsT] | ToolsetFunc[AgentDepsT]] | None = None,     defer_model_check: bool = False,     end_strategy: EndStrategy = 'early',     instrument: InstrumentationSettings | bool | None = None,     history_processors: Sequence[HistoryProcessor[AgentDepsT]] | None = None,     event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     **_deprecated_kwargs: Any, ):     \"\"\"Create an agent.      Args:         model: The default model to use for this agent, if not provided,             you must provide the model when calling it. We allow `str` here since the actual list of allowed models changes frequently.         output_type: The type of the output data, used to validate the data returned by the model,             defaults to `str`.         instructions: Instructions to use for this agent, you can also register instructions via a function with             [`instructions`][pydantic_ai.Agent.instructions].         system_prompt: Static system prompts to use for this agent, you can also register system             prompts via a function with [`system_prompt`][pydantic_ai.Agent.system_prompt].         deps_type: The type used for dependency injection, this parameter exists solely to allow you to fully             parameterize the agent, and therefore get the best out of static type checking.             If you're not using deps, but want type checking to pass, you can set `deps=None` to satisfy Pyright             or add a type hint `: Agent[None, <return type>]`.         name: The name of the agent, used for logging. If `None`, we try to infer the agent name from the call frame             when the agent is first run.         model_settings: Optional model request settings to use for this agent's runs, by default.         retries: The default number of retries to allow for tool calls and output validation, before raising an error.             For model request retries, see the [HTTP Request Retries](../retries.md) documentation.         output_retries: The maximum number of retries to allow for output validation, defaults to `retries`.         tools: Tools to register with the agent, you can also register tools via the decorators             [`@agent.tool`][pydantic_ai.Agent.tool] and [`@agent.tool_plain`][pydantic_ai.Agent.tool_plain].         builtin_tools: The builtin tools that the agent will use. This depends on the model, as some models may not             support certain tools. If the model doesn't support the builtin tools, an error will be raised.         prepare_tools: Custom function to prepare the tool definition of all tools for each step, except output tools.             This is useful if you want to customize the definition of multiple tools or you want to register             a subset of tools for a given step. See [`ToolsPrepareFunc`][pydantic_ai.tools.ToolsPrepareFunc]         prepare_output_tools: Custom function to prepare the tool definition of all output tools for each step.             This is useful if you want to customize the definition of multiple output tools or you want to register             a subset of output tools for a given step. See [`ToolsPrepareFunc`][pydantic_ai.tools.ToolsPrepareFunc]         toolsets: Toolsets to register with the agent, including MCP servers and functions which take a run context             and return a toolset. See [`ToolsetFunc`][pydantic_ai.toolsets.ToolsetFunc] for more information.         defer_model_check: by default, if you provide a [named][pydantic_ai.models.KnownModelName] model,             it's evaluated to create a [`Model`][pydantic_ai.models.Model] instance immediately,             which checks for the necessary environment variables. Set this to `false`             to defer the evaluation until the first run. Useful if you want to             [override the model][pydantic_ai.Agent.override] for testing.         end_strategy: Strategy for handling tool calls that are requested alongside a final result.             See [`EndStrategy`][pydantic_ai.agent.EndStrategy] for more information.         instrument: Set to True to automatically instrument with OpenTelemetry,             which will use Logfire if it's configured.             Set to an instance of [`InstrumentationSettings`][pydantic_ai.agent.InstrumentationSettings] to customize.             If this isn't set, then the last value set by             [`Agent.instrument_all()`][pydantic_ai.Agent.instrument_all]             will be used, which defaults to False.             See the [Debugging and Monitoring guide](https://ai.pydantic.dev/logfire/) for more info.         history_processors: Optional list of callables to process the message history before sending it to the model.             Each processor takes a list of messages and returns a modified list of messages.             Processors can be sync or async and are applied in sequence.         event_stream_handler: Optional handler for events from the model's streaming response and the agent's execution of tools.     \"\"\"     if model is None or defer_model_check:         self._model = model     else:         self._model = models.infer_model(model)      self._name = name     self.end_strategy = end_strategy     self.model_settings = model_settings      self._output_type = output_type     self.instrument = instrument     self._deps_type = deps_type      if mcp_servers := _deprecated_kwargs.pop('mcp_servers', None):         if toolsets is not None:  # pragma: no cover             raise TypeError('`mcp_servers` and `toolsets` cannot be set at the same time.')         warnings.warn('`mcp_servers` is deprecated, use `toolsets` instead', DeprecationWarning)         toolsets = mcp_servers      _utils.validate_empty_kwargs(_deprecated_kwargs)      default_output_mode = (         self.model.profile.default_structured_output_mode if isinstance(self.model, models.Model) else None     )      self._output_schema = _output.OutputSchema[OutputDataT].build(output_type, default_mode=default_output_mode)     self._output_validators = []      self._instructions = self._normalize_instructions(instructions)      self._system_prompts = (system_prompt,) if isinstance(system_prompt, str) else tuple(system_prompt)     self._system_prompt_functions = []     self._system_prompt_dynamic_functions = {}      self._max_result_retries = output_retries if output_retries is not None else retries     self._max_tool_retries = retries      self._builtin_tools = builtin_tools      self._prepare_tools = prepare_tools     self._prepare_output_tools = prepare_output_tools      self._output_toolset = self._output_schema.toolset     if self._output_toolset:         self._output_toolset.max_retries = self._max_result_retries      self._function_toolset = _AgentFunctionToolset(         tools, max_retries=self._max_tool_retries, output_schema=self._output_schema     )     self._dynamic_toolsets = [         DynamicToolset[AgentDepsT](toolset_func=toolset)         for toolset in toolsets or []         if not isinstance(toolset, AbstractToolset)     ]     self._user_toolsets = [toolset for toolset in toolsets or [] if isinstance(toolset, AbstractToolset)]      self.history_processors = history_processors or []      self._event_stream_handler = event_stream_handler      self._override_name: ContextVar[_utils.Option[str]] = ContextVar('_override_name', default=None)     self._override_deps: ContextVar[_utils.Option[AgentDepsT]] = ContextVar('_override_deps', default=None)     self._override_model: ContextVar[_utils.Option[models.Model]] = ContextVar('_override_model', default=None)     self._override_toolsets: ContextVar[_utils.Option[Sequence[AbstractToolset[AgentDepsT]]]] = ContextVar(         '_override_toolsets', default=None     )     self._override_tools: ContextVar[         _utils.Option[Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]]]     ] = ContextVar('_override_tools', default=None)     self._override_instructions: ContextVar[         _utils.Option[list[str | _system_prompt.SystemPromptFunc[AgentDepsT]]]     ] = ContextVar('_override_instructions', default=None)      self._enter_lock = Lock()     self._entered_count = 0     self._exit_stack = None ``` |", "url": "https://ai.pydantic.dev/agent/index.html#agent-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "Agent `dataclass`", "anchor": "agent-dataclass", "md_text": "#### end\\_strategy `instance-attribute`\n\n```\nend_strategy: EndStrategy = end_strategy\n```\n\nStrategy for handling tool calls when a final result is found.\n\n#### model\\_settings `instance-attribute`\n\n```\nmodel_settings: ModelSettings | None = model_settings\n```\n\nOptional model request settings to use for this agents's runs, by default.\n\nNote, if `model_settings` is provided by `run`, `run_sync`, or `run_stream`, those settings will\nbe merged with this value, with the runtime argument taking priority.\n\n#### instrument `instance-attribute`\n\n```\ninstrument: InstrumentationSettings | bool | None = (\n    instrument\n)\n```\n\nOptions to automatically instrument with OpenTelemetry.\n\n#### instrument\\_all `staticmethod`\n\n```\ninstrument_all(\n    instrument: InstrumentationSettings | bool = True,\n) -> None\n```\n\nSet the instrumentation options for all agents where `instrument` is not set.\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 362 363 364 365 ``` | ``` @staticmethod def instrument_all(instrument: InstrumentationSettings | bool = True) -> None:     \"\"\"Set the instrumentation options for all agents where `instrument` is not set.\"\"\"     Agent._instrument_default = instrument ``` |\n\n#### model `property` `writable`\n\n```\nmodel: Model | KnownModelName | str | None\n```\n\nThe default model configured for this agent.\n\n#### name `property` `writable`\n\n```\nname: str | None\n```\n\nThe name of the agent, used for logging.\n\nIf `None`, we try to infer the agent name from the call frame when the agent is first run.\n\n#### deps\\_type `property`\n\n```\ndeps_type: type\n```\n\nThe type of dependencies used by the agent.\n\n#### output\\_type `property`\n\n```\noutput_type: OutputSpec[OutputDataT]\n```\n\nThe type of data output by agent runs, used to validate the data returned by the model, defaults to `str`.\n\n#### event\\_stream\\_handler `property`\n\n```\nevent_stream_handler: EventStreamHandler[AgentDepsT] | None\n```\n\nOptional handler for events from the model's streaming response and the agent's execution of tools.\n\n#### iter `async`\n\n```\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AbstractAsyncContextManager[\n    AgentRun[AgentDepsT, OutputDataT]\n]\n\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AbstractAsyncContextManager[\n    AgentRun[AgentDepsT, RunOutputDataT]\n]\n\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[AgentRun[AgentDepsT, Any]]\n```\n\nA contextmanager which can be used to iterate over the agent graph's nodes as they are executed.\n\nThis method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an\n`AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are\nexecuted. This is the API to use if you want to consume the outputs coming from each LLM model response, or the\nstream of events coming from the execution of tools.", "url": "https://ai.pydantic.dev/agent/index.html#agent-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "Agent `dataclass`", "anchor": "agent-dataclass", "md_text": "The `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,\nand the final result of the run once it has completed.\n\nFor more details, see the documentation of `AgentRun`.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    nodes = []\n    async with agent.iter('What is the capital of France?') as agent_run:\n        async for node in agent_run:\n            nodes.append(node)\n    print(nodes)\n    '''\n    [\n        UserPromptNode(\n            user_prompt='What is the capital of France?',\n            instructions_functions=[],\n            system_prompts=(),\n            system_prompt_functions=[],\n            system_prompt_dynamic_functions={},\n        ),\n        ModelRequestNode(\n            request=ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='What is the capital of France?',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            )\n        ),\n        CallToolsNode(\n            model_response=ModelResponse(\n                parts=[TextPart(content='The capital of France is Paris.')],\n                usage=RequestUsage(input_tokens=56, output_tokens=7),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            )\n        ),\n        End(data=FinalResult(output='The capital of France is Paris.')),\n    ]\n    '''\n    print(agent_run.result.output)\n    #> The capital of France is Paris.\n```\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` |\n| `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` |\n| `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AsyncIterator[AgentRun[AgentDepsT, Any]]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/__init__.py`", "url": "https://ai.pydantic.dev/agent/index.html#agent-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "Agent `dataclass`", "anchor": "agent-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 ``` | ``` @asynccontextmanager async def iter(     self,     user_prompt: str | Sequence[_messages.UserContent] | None = None,     *,     output_type: OutputSpec[RunOutputDataT] | None = None,     message_history: Sequence[_messages.ModelMessage] | None = None,     deferred_tool_results: DeferredToolResults | None = None,     model: models.Model | models.KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: _usage.UsageLimits | None = None,     usage: _usage.RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     builtin_tools: Sequence[AbstractBuiltinTool] | None = None, ) -> AsyncIterator[AgentRun[AgentDepsT, Any]]:     \"\"\"A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.      This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an     `AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are     executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the     stream of events coming from the execution of tools.      The `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,     and the final result of the run once it has completed.      For more details, see the documentation of `AgentRun`.      Example:     ```python     from pydantic_ai import Agent      agent = Agent('openai:gpt-4o')      async def main():         nodes = []         async with agent.iter('What is the capital of France?') as agent_run:             async for node in agent_run:                 nodes.append(node)         print(nodes)         '''         [             UserPromptNode(                 user_prompt='What is the capital of France?',                 instructions_functions=[],                 system_prompts=(),                 system_prompt_functions=[],                 system_prompt_dynamic_functions={},             ),             ModelRequestNode(                 request=ModelRequest(                     parts=[                         UserPromptPart(                             content='What is the capital of France?',                             timestamp=datetime.datetime(...),                         )                     ]                 )             ),             CallToolsNode(                 model_response=ModelResponse(                     parts=[TextPart(content='The capital of France is Paris.')],                     usage=RequestUsage(input_tokens=56, output_tokens=7),                     model_name='gpt-4o',                     timestamp=datetime.datetime(...),                 )             ),             End(data=FinalResult(output='The capital of France is Paris.')),         ]         '''         print(agent_run.result.output)         #> The capital of France is Paris.     ```      Args:         user_prompt: User input to start/continue the conversation.         output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no             output validators since output validators would expect an argument that matches the agent's output type.         message_history: History of the conversation so far.         deferred_tool_results: Optional results for deferred tool calls in the message history.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.         builtin_tools: Optional additional builtin tools for this run.      Returns:         The result of the run.     \"\"\"     if infer_name and self.name is None:         self._infer_name(inspect.currentframe())      model_used = self._get_model(model)     del model      deps = self._get_deps(deps)     output_schema = self._prepare_output_schema(output_type, model_used.profile)      output_type_ = output_type or self.output_type      # We consider it a user error if a user tries to restrict the result type while having an output validator that     # may change the result type from the restricted type to something else. Therefore, we consider the following     # typecast reasonable, even though it is possible to violate it with otherwise-type-checked code.     output_validators = cast(list[_output.OutputValidator[AgentDepsT, RunOutputDataT]], self._output_validators)      output_toolset = self._output_toolset     if output_schema != self._output_schema or output_validators:         output_toolset = cast(OutputToolset[AgentDepsT], output_schema.toolset)         if output_toolset:             output_toolset.max_retries = self._max_result_retries             output_toolset.output_validators = output_validators     toolset = self._get_toolset(output_toolset=output_toolset, additional_toolsets=toolsets)     tool_manager = ToolManager[AgentDepsT](toolset)      # Build the graph     graph = _agent_graph.build_agent_graph(self.name, self._deps_type, output_type_)      # Build the initial state     usage = usage or _usage.RunUsage()     state = _agent_graph.GraphAgentState(         message_history=list(message_history) if message_history else [],         usage=usage,         retries=0,         run_step=0,     )      # Merge model settings in order of precedence: run > agent > model     merged_settings = merge_model_settings(model_used.settings, self.model_settings)     model_settings = merge_model_settings(merged_settings, model_settings)     usage_limits = usage_limits or _usage.UsageLimits()      instructions_literal, instructions_functions = self._get_instructions()      async def get_instructions(run_context: RunContext[AgentDepsT]) -> str | None:         parts = [             instructions_literal,             *[await func.run(run_context) for func in instructions_functions],         ]          model_profile = model_used.profile         if isinstance(output_schema, _output.PromptedOutputSchema):             instructions = output_schema.instructions(model_profile.prompted_output_template)             parts.append(instructions)          parts = [p for p in parts if p]         if not parts:             return None         return '\\n\\n'.join(parts).strip()      if isinstance(model_used, InstrumentedModel):         instrumentation_settings = model_used.instrumentation_settings         tracer = model_used.instrumentation_settings.tracer     else:         instrumentation_settings = None         tracer = NoOpTracer()      graph_deps = _agent_graph.GraphAgentDeps[AgentDepsT, RunOutputDataT](         user_deps=deps,         prompt=user_prompt,         new_message_index=len(message_history) if message_history else 0,         model=model_used,         model_settings=model_settings,         usage_limits=usage_limits,         max_result_retries=self._max_result_retries,         end_strategy=self.end_strategy,         output_schema=output_schema,         output_validators=output_validators,         history_processors=self.history_processors,         builtin_tools=[*self._builtin_tools, *(builtin_tools or [])],         tool_manager=tool_manager,         tracer=tracer,         get_instructions=get_instructions,         instrumentation_settings=instrumentation_settings,     )      user_prompt_node = _agent_graph.UserPromptNode[AgentDepsT](         user_prompt=user_prompt,         deferred_tool_results=deferred_tool_results,         instructions=instructions_literal,         instructions_functions=instructions_functions,         system_prompts=self._system_prompts,         system_prompt_functions=self._system_prompt_functions,         system_prompt_dynamic_functions=self._system_prompt_dynamic_functions,     )      agent_name = self.name or 'agent'     instrumentation_names = InstrumentationNames.for_version(         instrumentation_settings.version if instrumentation_settings else DEFAULT_INSTRUMENTATION_VERSION     )      run_span = tracer.start_span(         instrumentation_names.get_agent_run_span_name(agent_name),         attributes={             'model_name': model_used.model_name if model_used else 'no-model',             'agent_name': agent_name,             'gen_ai.agent.name': agent_name,             'logfire.msg': f'{agent_name} run',         },     )      try:         async with graph.iter(             inputs=user_prompt_node,             state=state,             deps=graph_deps,             span=use_span(run_span) if run_span.is_recording() else None,             infer_name=False,         ) as graph_run:             async with toolset:                 agent_run = AgentRun(graph_run)                 yield agent_run                 if (final_result := agent_run.result) is not None and run_span.is_recording():                     if instrumentation_settings and instrumentation_settings.include_content:                         run_span.set_attribute(                             'final_result',                             (                                 final_result.output                                 if isinstance(final_result.output, str)                                 else json.dumps(InstrumentedModel.serialize_any(final_result.output))                             ),                         )     finally:         try:             if instrumentation_settings and run_span.is_recording():                 run_span.set_attributes(                     self._run_span_end_attributes(                         instrumentation_settings, usage, state.message_history, graph_deps.new_message_index                     )                 )         finally:             run_span.end() ``` |", "url": "https://ai.pydantic.dev/agent/index.html#agent-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "Agent `dataclass`", "anchor": "agent-dataclass", "md_text": "#### override\n\n```\noverride(\n    *,\n    name: str | Unset = UNSET,\n    deps: AgentDepsT | Unset = UNSET,\n    model: Model | KnownModelName | str | Unset = UNSET,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | Unset\n    ) = UNSET,\n    tools: (\n        Sequence[\n            Tool[AgentDepsT]\n            | ToolFuncEither[AgentDepsT, ...]\n        ]\n        | Unset\n    ) = UNSET,\n    instructions: Instructions[AgentDepsT] | Unset = UNSET\n) -> Iterator[None]\n```\n\nContext manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.\n\nThis is particularly useful when testing.\nYou can find an example of this [here](https://ai.pydantic.dev/testing/#overriding-model-via-pytest-fixtures).\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `name` | `str | Unset` | The name to use instead of the name passed to the agent constructor and agent run. | `UNSET` |\n| `deps` | `AgentDepsT | Unset` | The dependencies to use instead of the dependencies passed to the agent run. | `UNSET` |\n| `model` | `Model | KnownModelName | str | Unset` | The model to use instead of the model passed to the agent run. | `UNSET` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | Unset` | The toolsets to use instead of the toolsets passed to the agent constructor and agent run. | `UNSET` |\n| `tools` | `Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | Unset` | The tools to use instead of the tools registered with the agent. | `UNSET` |\n| `instructions` | `Instructions[AgentDepsT] | Unset` | The instructions to use instead of the instructions registered with the agent. | `UNSET` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/__init__.py`", "url": "https://ai.pydantic.dev/agent/index.html#agent-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "Agent `dataclass`", "anchor": "agent-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 ``` | ``` @contextmanager def override(     self,     *,     name: str | _utils.Unset = _utils.UNSET,     deps: AgentDepsT | _utils.Unset = _utils.UNSET,     model: models.Model | models.KnownModelName | str | _utils.Unset = _utils.UNSET,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | _utils.Unset = _utils.UNSET,     tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | _utils.Unset = _utils.UNSET,     instructions: Instructions[AgentDepsT] | _utils.Unset = _utils.UNSET, ) -> Iterator[None]:     \"\"\"Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.      This is particularly useful when testing.     You can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).      Args:         name: The name to use instead of the name passed to the agent constructor and agent run.         deps: The dependencies to use instead of the dependencies passed to the agent run.         model: The model to use instead of the model passed to the agent run.         toolsets: The toolsets to use instead of the toolsets passed to the agent constructor and agent run.         tools: The tools to use instead of the tools registered with the agent.         instructions: The instructions to use instead of the instructions registered with the agent.     \"\"\"     if _utils.is_set(name):         name_token = self._override_name.set(_utils.Some(name))     else:         name_token = None      if _utils.is_set(deps):         deps_token = self._override_deps.set(_utils.Some(deps))     else:         deps_token = None      if _utils.is_set(model):         model_token = self._override_model.set(_utils.Some(models.infer_model(model)))     else:         model_token = None      if _utils.is_set(toolsets):         toolsets_token = self._override_toolsets.set(_utils.Some(toolsets))     else:         toolsets_token = None      if _utils.is_set(tools):         tools_token = self._override_tools.set(_utils.Some(tools))     else:         tools_token = None      if _utils.is_set(instructions):         normalized_instructions = self._normalize_instructions(instructions)         instructions_token = self._override_instructions.set(_utils.Some(normalized_instructions))     else:         instructions_token = None      try:         yield     finally:         if name_token is not None:             self._override_name.reset(name_token)         if deps_token is not None:             self._override_deps.reset(deps_token)         if model_token is not None:             self._override_model.reset(model_token)         if toolsets_token is not None:             self._override_toolsets.reset(toolsets_token)         if tools_token is not None:             self._override_tools.reset(tools_token)         if instructions_token is not None:             self._override_instructions.reset(instructions_token) ``` |\n\n#### instructions\n\n```\ninstructions(\n    func: Callable[[RunContext[AgentDepsT]], str],\n) -> Callable[[RunContext[AgentDepsT]], str]\n\ninstructions(\n    func: Callable[\n        [RunContext[AgentDepsT]], Awaitable[str]\n    ],\n) -> Callable[[RunContext[AgentDepsT]], Awaitable[str]]\n\ninstructions(func: Callable[[], str]) -> Callable[[], str]\n\ninstructions(\n    func: Callable[[], Awaitable[str]],\n) -> Callable[[], Awaitable[str]]\n\ninstructions() -> Callable[\n    [SystemPromptFunc[AgentDepsT]],\n    SystemPromptFunc[AgentDepsT],\n]\n\ninstructions(\n    func: SystemPromptFunc[AgentDepsT] | None = None,\n) -> (\n    Callable[\n        [SystemPromptFunc[AgentDepsT]],\n        SystemPromptFunc[AgentDepsT],\n    ]\n    | SystemPromptFunc[AgentDepsT]\n)\n```\n\nDecorator to register an instructions function.\n\nOptionally takes [`RunContext`](../tools/index.html#pydantic_ai.tools.RunContext) as its only argument.\nCan decorate a sync or async functions.\n\nThe decorator can be used bare (`agent.instructions`).\n\nOverloads for every possible signature of `instructions` are included so the decorator doesn't obscure\nthe type of the function.\n\nExample:\n\n```\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent('test', deps_type=str)\n\n@agent.instructions\ndef simple_instructions() -> str:\n    return 'foobar'\n\n@agent.instructions\nasync def async_instructions(ctx: RunContext[str]) -> str:\n    return f'{ctx.deps} is the best'\n```\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/__init__.py`", "url": "https://ai.pydantic.dev/agent/index.html#agent-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "Agent `dataclass`", "anchor": "agent-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 ``` | ``` def instructions(     self,     func: _system_prompt.SystemPromptFunc[AgentDepsT] | None = None,     /, ) -> (     Callable[[_system_prompt.SystemPromptFunc[AgentDepsT]], _system_prompt.SystemPromptFunc[AgentDepsT]]     | _system_prompt.SystemPromptFunc[AgentDepsT] ):     \"\"\"Decorator to register an instructions function.      Optionally takes [`RunContext`][pydantic_ai.tools.RunContext] as its only argument.     Can decorate a sync or async functions.      The decorator can be used bare (`agent.instructions`).      Overloads for every possible signature of `instructions` are included so the decorator doesn't obscure     the type of the function.      Example:     ```python     from pydantic_ai import Agent, RunContext      agent = Agent('test', deps_type=str)      @agent.instructions     def simple_instructions() -> str:         return 'foobar'      @agent.instructions     async def async_instructions(ctx: RunContext[str]) -> str:         return f'{ctx.deps} is the best'     ```     \"\"\"     if func is None:          def decorator(             func_: _system_prompt.SystemPromptFunc[AgentDepsT],         ) -> _system_prompt.SystemPromptFunc[AgentDepsT]:             self._instructions.append(func_)             return func_          return decorator     else:         self._instructions.append(func)         return func ``` |\n\n#### system\\_prompt\n\n```\nsystem_prompt(\n    func: Callable[[RunContext[AgentDepsT]], str],\n) -> Callable[[RunContext[AgentDepsT]], str]\n\nsystem_prompt(\n    func: Callable[\n        [RunContext[AgentDepsT]], Awaitable[str]\n    ],\n) -> Callable[[RunContext[AgentDepsT]], Awaitable[str]]\n\nsystem_prompt(func: Callable[[], str]) -> Callable[[], str]\n\nsystem_prompt(\n    func: Callable[[], Awaitable[str]],\n) -> Callable[[], Awaitable[str]]\n\nsystem_prompt(*, dynamic: bool = False) -> Callable[\n    [SystemPromptFunc[AgentDepsT]],\n    SystemPromptFunc[AgentDepsT],\n]\n\nsystem_prompt(\n    func: SystemPromptFunc[AgentDepsT] | None = None,\n    /,\n    *,\n    dynamic: bool = False,\n) -> (\n    Callable[\n        [SystemPromptFunc[AgentDepsT]],\n        SystemPromptFunc[AgentDepsT],\n    ]\n    | SystemPromptFunc[AgentDepsT]\n)\n```\n\nDecorator to register a system prompt function.\n\nOptionally takes [`RunContext`](../tools/index.html#pydantic_ai.tools.RunContext) as its only argument.\nCan decorate a sync or async functions.\n\nThe decorator can be used either bare (`agent.system_prompt`) or as a function call\n(`agent.system_prompt(...)`), see the examples below.\n\nOverloads for every possible signature of `system_prompt` are included so the decorator doesn't obscure\nthe type of the function, see `tests/typed_agent.py` for tests.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `func` | `SystemPromptFunc[AgentDepsT] | None` | The function to decorate | `None` |\n| `dynamic` | `bool` | If True, the system prompt will be reevaluated even when `messages_history` is provided, see [`SystemPromptPart.dynamic_ref`](../messages/index.html#pydantic_ai.messages.SystemPromptPart.dynamic_ref) | `False` |\n\nExample:\n\n```\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent('test', deps_type=str)\n\n@agent.system_prompt\ndef simple_system_prompt() -> str:\n    return 'foobar'\n\n@agent.system_prompt(dynamic=True)\nasync def async_system_prompt(ctx: RunContext[str]) -> str:\n    return f'{ctx.deps} is the best'\n```\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/__init__.py`", "url": "https://ai.pydantic.dev/agent/index.html#agent-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "Agent `dataclass`", "anchor": "agent-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 ``` | ``` def system_prompt(     self,     func: _system_prompt.SystemPromptFunc[AgentDepsT] | None = None,     /,     *,     dynamic: bool = False, ) -> (     Callable[[_system_prompt.SystemPromptFunc[AgentDepsT]], _system_prompt.SystemPromptFunc[AgentDepsT]]     | _system_prompt.SystemPromptFunc[AgentDepsT] ):     \"\"\"Decorator to register a system prompt function.      Optionally takes [`RunContext`][pydantic_ai.tools.RunContext] as its only argument.     Can decorate a sync or async functions.      The decorator can be used either bare (`agent.system_prompt`) or as a function call     (`agent.system_prompt(...)`), see the examples below.      Overloads for every possible signature of `system_prompt` are included so the decorator doesn't obscure     the type of the function, see `tests/typed_agent.py` for tests.      Args:         func: The function to decorate         dynamic: If True, the system prompt will be reevaluated even when `messages_history` is provided,             see [`SystemPromptPart.dynamic_ref`][pydantic_ai.messages.SystemPromptPart.dynamic_ref]      Example:     ```python     from pydantic_ai import Agent, RunContext      agent = Agent('test', deps_type=str)      @agent.system_prompt     def simple_system_prompt() -> str:         return 'foobar'      @agent.system_prompt(dynamic=True)     async def async_system_prompt(ctx: RunContext[str]) -> str:         return f'{ctx.deps} is the best'     ```     \"\"\"     if func is None:          def decorator(             func_: _system_prompt.SystemPromptFunc[AgentDepsT],         ) -> _system_prompt.SystemPromptFunc[AgentDepsT]:             runner = _system_prompt.SystemPromptRunner[AgentDepsT](func_, dynamic=dynamic)             self._system_prompt_functions.append(runner)             if dynamic:  # pragma: lax no cover                 self._system_prompt_dynamic_functions[func_.__qualname__] = runner             return func_          return decorator     else:         assert not dynamic, \"dynamic can't be True in this case\"         self._system_prompt_functions.append(_system_prompt.SystemPromptRunner[AgentDepsT](func, dynamic=dynamic))         return func ``` |\n\n#### output\\_validator\n\n```\noutput_validator(\n    func: Callable[\n        [RunContext[AgentDepsT], OutputDataT], OutputDataT\n    ],\n) -> Callable[\n    [RunContext[AgentDepsT], OutputDataT], OutputDataT\n]\n\noutput_validator(\n    func: Callable[\n        [RunContext[AgentDepsT], OutputDataT],\n        Awaitable[OutputDataT],\n    ],\n) -> Callable[\n    [RunContext[AgentDepsT], OutputDataT],\n    Awaitable[OutputDataT],\n]\n\noutput_validator(\n    func: Callable[[OutputDataT], OutputDataT],\n) -> Callable[[OutputDataT], OutputDataT]\n\noutput_validator(\n    func: Callable[[OutputDataT], Awaitable[OutputDataT]],\n) -> Callable[[OutputDataT], Awaitable[OutputDataT]]\n\noutput_validator(\n    func: OutputValidatorFunc[AgentDepsT, OutputDataT],\n) -> OutputValidatorFunc[AgentDepsT, OutputDataT]\n```\n\nDecorator to register an output validator function.\n\nOptionally takes [`RunContext`](../tools/index.html#pydantic_ai.tools.RunContext) as its first argument.\nCan decorate a sync or async functions.\n\nOverloads for every possible signature of `output_validator` are included so the decorator doesn't obscure\nthe type of the function, see `tests/typed_agent.py` for tests.\n\nExample:\n\n```\nfrom pydantic_ai import Agent, ModelRetry, RunContext\n\nagent = Agent('test', deps_type=str)\n\n@agent.output_validator\ndef output_validator_simple(data: str) -> str:\n    if 'wrong' in data:\n        raise ModelRetry('wrong response')\n    return data\n\n@agent.output_validator\nasync def output_validator_deps(ctx: RunContext[str], data: str) -> str:\n    if ctx.deps in data:\n        raise ModelRetry('wrong response')\n    return data\n\nresult = agent.run_sync('foobar', deps='spam')\nprint(result.output)\n#> success (no tool calls)\n```\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/__init__.py`", "url": "https://ai.pydantic.dev/agent/index.html#agent-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "Agent `dataclass`", "anchor": "agent-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ```  974  975  976  977  978  979  980  981  982  983  984  985  986  987  988  989  990  991  992  993  994  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 ``` | ``` def output_validator(     self, func: _output.OutputValidatorFunc[AgentDepsT, OutputDataT], / ) -> _output.OutputValidatorFunc[AgentDepsT, OutputDataT]:     \"\"\"Decorator to register an output validator function.      Optionally takes [`RunContext`][pydantic_ai.tools.RunContext] as its first argument.     Can decorate a sync or async functions.      Overloads for every possible signature of `output_validator` are included so the decorator doesn't obscure     the type of the function, see `tests/typed_agent.py` for tests.      Example:     ```python     from pydantic_ai import Agent, ModelRetry, RunContext      agent = Agent('test', deps_type=str)      @agent.output_validator     def output_validator_simple(data: str) -> str:         if 'wrong' in data:             raise ModelRetry('wrong response')         return data      @agent.output_validator     async def output_validator_deps(ctx: RunContext[str], data: str) -> str:         if ctx.deps in data:             raise ModelRetry('wrong response')         return data      result = agent.run_sync('foobar', deps='spam')     print(result.output)     #> success (no tool calls)     ```     \"\"\"     self._output_validators.append(_output.OutputValidator[AgentDepsT, Any](func))     return func ``` |\n\n#### tool\n\n```\ntool(\n    func: ToolFuncContext[AgentDepsT, ToolParams],\n) -> ToolFuncContext[AgentDepsT, ToolParams]\n\ntool(\n    *,\n    name: str | None = None,\n    description: str | None = None,\n    retries: int | None = None,\n    prepare: ToolPrepareFunc[AgentDepsT] | None = None,\n    docstring_format: DocstringFormat = \"auto\",\n    require_parameter_descriptions: bool = False,\n    schema_generator: type[\n        GenerateJsonSchema\n    ] = GenerateToolJsonSchema,\n    strict: bool | None = None,\n    sequential: bool = False,\n    requires_approval: bool = False,\n    metadata: dict[str, Any] | None = None\n) -> Callable[\n    [ToolFuncContext[AgentDepsT, ToolParams]],\n    ToolFuncContext[AgentDepsT, ToolParams],\n]\n\ntool(\n    func: (\n        ToolFuncContext[AgentDepsT, ToolParams] | None\n    ) = None,\n    /,\n    *,\n    name: str | None = None,\n    description: str | None = None,\n    retries: int | None = None,\n    prepare: ToolPrepareFunc[AgentDepsT] | None = None,\n    docstring_format: DocstringFormat = \"auto\",\n    require_parameter_descriptions: bool = False,\n    schema_generator: type[\n        GenerateJsonSchema\n    ] = GenerateToolJsonSchema,\n    strict: bool | None = None,\n    sequential: bool = False,\n    requires_approval: bool = False,\n    metadata: dict[str, Any] | None = None,\n) -> Any\n```\n\nDecorator to register a tool function which takes [`RunContext`](../tools/index.html#pydantic_ai.tools.RunContext) as its first argument.\n\nCan decorate a sync or async functions.\n\nThe docstring is inspected to extract both the tool description and description of each parameter,\n[learn more](https://ai.pydantic.dev/tools/#function-tools-and-schema).\n\nWe can't add overloads for every possible signature of tool, since the return type is a recursive union\nso the signature of functions decorated with `@agent.tool` is obscured.\n\nExample:\n\n```\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent('test', deps_type=int)\n\n@agent.tool\ndef foobar(ctx: RunContext[int], x: int) -> int:\n    return ctx.deps + x\n\n@agent.tool(retries=2)\nasync def spam(ctx: RunContext[str], y: float) -> float:\n    return ctx.deps + y\n\nresult = agent.run_sync('foobar', deps=1)\nprint(result.output)\n#> {\"foobar\":1,\"spam\":1.0}\n```\n\nParameters:", "url": "https://ai.pydantic.dev/agent/index.html#agent-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "Agent `dataclass`", "anchor": "agent-dataclass", "md_text": "| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `func` | `ToolFuncContext[AgentDepsT, ToolParams] | None` | The tool function to register. | `None` |\n| `name` | `str | None` | The name of the tool, defaults to the function name. | `None` |\n| `description` | `str | None` | The description of the tool, defaults to the function docstring. | `None` |\n| `retries` | `int | None` | The number of retries to allow for this tool, defaults to the agent's default retries, which defaults to 1. | `None` |\n| `prepare` | `ToolPrepareFunc[AgentDepsT] | None` | custom method to prepare the tool definition for each step, return `None` to omit this tool from a given step. This is useful if you want to customise a tool at call time, or omit it completely from a step. See [`ToolPrepareFunc`](../tools/index.html#pydantic_ai.tools.ToolPrepareFunc). | `None` |\n| `docstring_format` | `DocstringFormat` | The format of the docstring, see [`DocstringFormat`](../tools/index.html#pydantic_ai.tools.DocstringFormat). Defaults to `'auto'`, such that the format is inferred from the structure of the docstring. | `'auto'` |\n| `require_parameter_descriptions` | `bool` | If True, raise an error if a parameter description is missing. Defaults to False. | `False` |\n| `schema_generator` | `type[GenerateJsonSchema]` | The JSON schema generator class to use for this tool. Defaults to `GenerateToolJsonSchema`. | `GenerateToolJsonSchema` |\n| `strict` | `bool | None` | Whether to enforce JSON schema compliance (only affects OpenAI). See [`ToolDefinition`](../tools/index.html#pydantic_ai.tools.ToolDefinition) for more info. | `None` |\n| `sequential` | `bool` | Whether the function requires a sequential/serial execution environment. Defaults to False. | `False` |\n| `requires_approval` | `bool` | Whether this tool requires human-in-the-loop approval. Defaults to False. See the [tools documentation](https://ai.pydantic.dev/deferred-tools/#human-in-the-loop-tool-approval) for more info. | `False` |\n| `metadata` | `dict[str, Any] | None` | Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/__init__.py`", "url": "https://ai.pydantic.dev/agent/index.html#agent-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "Agent `dataclass`", "anchor": "agent-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 ``` | ``` def tool(     self,     func: ToolFuncContext[AgentDepsT, ToolParams] | None = None,     /,     *,     name: str | None = None,     description: str | None = None,     retries: int | None = None,     prepare: ToolPrepareFunc[AgentDepsT] | None = None,     docstring_format: DocstringFormat = 'auto',     require_parameter_descriptions: bool = False,     schema_generator: type[GenerateJsonSchema] = GenerateToolJsonSchema,     strict: bool | None = None,     sequential: bool = False,     requires_approval: bool = False,     metadata: dict[str, Any] | None = None, ) -> Any:     \"\"\"Decorator to register a tool function which takes [`RunContext`][pydantic_ai.tools.RunContext] as its first argument.      Can decorate a sync or async functions.      The docstring is inspected to extract both the tool description and description of each parameter,     [learn more](../tools.md#function-tools-and-schema).      We can't add overloads for every possible signature of tool, since the return type is a recursive union     so the signature of functions decorated with `@agent.tool` is obscured.      Example:     ```python     from pydantic_ai import Agent, RunContext      agent = Agent('test', deps_type=int)      @agent.tool     def foobar(ctx: RunContext[int], x: int) -> int:         return ctx.deps + x      @agent.tool(retries=2)     async def spam(ctx: RunContext[str], y: float) -> float:         return ctx.deps + y      result = agent.run_sync('foobar', deps=1)     print(result.output)     #> {\"foobar\":1,\"spam\":1.0}     ```      Args:         func: The tool function to register.         name: The name of the tool, defaults to the function name.         description: The description of the tool, defaults to the function docstring.         retries: The number of retries to allow for this tool, defaults to the agent's default retries,             which defaults to 1.         prepare: custom method to prepare the tool definition for each step, return `None` to omit this             tool from a given step. This is useful if you want to customise a tool at call time,             or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].         docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].             Defaults to `'auto'`, such that the format is inferred from the structure of the docstring.         require_parameter_descriptions: If True, raise an error if a parameter description is missing. Defaults to False.         schema_generator: The JSON schema generator class to use for this tool. Defaults to `GenerateToolJsonSchema`.         strict: Whether to enforce JSON schema compliance (only affects OpenAI).             See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.         sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.         requires_approval: Whether this tool requires human-in-the-loop approval. Defaults to False.             See the [tools documentation](../deferred-tools.md#human-in-the-loop-tool-approval) for more info.         metadata: Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization.     \"\"\"      def tool_decorator(         func_: ToolFuncContext[AgentDepsT, ToolParams],     ) -> ToolFuncContext[AgentDepsT, ToolParams]:         # noinspection PyTypeChecker         self._function_toolset.add_function(             func_,             takes_ctx=True,             name=name,             description=description,             retries=retries,             prepare=prepare,             docstring_format=docstring_format,             require_parameter_descriptions=require_parameter_descriptions,             schema_generator=schema_generator,             strict=strict,             sequential=sequential,             requires_approval=requires_approval,             metadata=metadata,         )         return func_      return tool_decorator if func is None else tool_decorator(func) ``` |\n\n#### tool\\_plain\n\n```\ntool_plain(\n    func: ToolFuncPlain[ToolParams],\n) -> ToolFuncPlain[ToolParams]", "url": "https://ai.pydantic.dev/agent/index.html#agent-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "Agent `dataclass`", "anchor": "agent-dataclass", "md_text": "tool_plain(\n    *,\n    name: str | None = None,\n    description: str | None = None,\n    retries: int | None = None,\n    prepare: ToolPrepareFunc[AgentDepsT] | None = None,\n    docstring_format: DocstringFormat = \"auto\",\n    require_parameter_descriptions: bool = False,\n    schema_generator: type[\n        GenerateJsonSchema\n    ] = GenerateToolJsonSchema,\n    strict: bool | None = None,\n    sequential: bool = False,\n    requires_approval: bool = False,\n    metadata: dict[str, Any] | None = None\n) -> Callable[\n    [ToolFuncPlain[ToolParams]], ToolFuncPlain[ToolParams]\n]\n\ntool_plain(\n    func: ToolFuncPlain[ToolParams] | None = None,\n    /,\n    *,\n    name: str | None = None,\n    description: str | None = None,\n    retries: int | None = None,\n    prepare: ToolPrepareFunc[AgentDepsT] | None = None,\n    docstring_format: DocstringFormat = \"auto\",\n    require_parameter_descriptions: bool = False,\n    schema_generator: type[\n        GenerateJsonSchema\n    ] = GenerateToolJsonSchema,\n    strict: bool | None = None,\n    sequential: bool = False,\n    requires_approval: bool = False,\n    metadata: dict[str, Any] | None = None,\n) -> Any\n```\n\nDecorator to register a tool function which DOES NOT take `RunContext` as an argument.\n\nCan decorate a sync or async functions.\n\nThe docstring is inspected to extract both the tool description and description of each parameter,\n[learn more](https://ai.pydantic.dev/tools/#function-tools-and-schema).\n\nWe can't add overloads for every possible signature of tool, since the return type is a recursive union\nso the signature of functions decorated with `@agent.tool` is obscured.\n\nExample:\n\n```\nfrom pydantic_ai import Agent, RunContext\n\nagent = Agent('test')\n\n@agent.tool\ndef foobar(ctx: RunContext[int]) -> int:\n    return 123\n\n@agent.tool(retries=2)\nasync def spam(ctx: RunContext[str]) -> float:\n    return 3.14\n\nresult = agent.run_sync('foobar', deps=1)\nprint(result.output)\n#> {\"foobar\":123,\"spam\":3.14}\n```\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `func` | `ToolFuncPlain[ToolParams] | None` | The tool function to register. | `None` |\n| `name` | `str | None` | The name of the tool, defaults to the function name. | `None` |\n| `description` | `str | None` | The description of the tool, defaults to the function docstring. | `None` |\n| `retries` | `int | None` | The number of retries to allow for this tool, defaults to the agent's default retries, which defaults to 1. | `None` |\n| `prepare` | `ToolPrepareFunc[AgentDepsT] | None` | custom method to prepare the tool definition for each step, return `None` to omit this tool from a given step. This is useful if you want to customise a tool at call time, or omit it completely from a step. See [`ToolPrepareFunc`](../tools/index.html#pydantic_ai.tools.ToolPrepareFunc). | `None` |\n| `docstring_format` | `DocstringFormat` | The format of the docstring, see [`DocstringFormat`](../tools/index.html#pydantic_ai.tools.DocstringFormat). Defaults to `'auto'`, such that the format is inferred from the structure of the docstring. | `'auto'` |\n| `require_parameter_descriptions` | `bool` | If True, raise an error if a parameter description is missing. Defaults to False. | `False` |\n| `schema_generator` | `type[GenerateJsonSchema]` | The JSON schema generator class to use for this tool. Defaults to `GenerateToolJsonSchema`. | `GenerateToolJsonSchema` |\n| `strict` | `bool | None` | Whether to enforce JSON schema compliance (only affects OpenAI). See [`ToolDefinition`](../tools/index.html#pydantic_ai.tools.ToolDefinition) for more info. | `None` |\n| `sequential` | `bool` | Whether the function requires a sequential/serial execution environment. Defaults to False. | `False` |\n| `requires_approval` | `bool` | Whether this tool requires human-in-the-loop approval. Defaults to False. See the [tools documentation](https://ai.pydantic.dev/deferred-tools/#human-in-the-loop-tool-approval) for more info. | `False` |\n| `metadata` | `dict[str, Any] | None` | Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/__init__.py`", "url": "https://ai.pydantic.dev/agent/index.html#agent-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "Agent `dataclass`", "anchor": "agent-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 ``` | ``` def tool_plain(     self,     func: ToolFuncPlain[ToolParams] | None = None,     /,     *,     name: str | None = None,     description: str | None = None,     retries: int | None = None,     prepare: ToolPrepareFunc[AgentDepsT] | None = None,     docstring_format: DocstringFormat = 'auto',     require_parameter_descriptions: bool = False,     schema_generator: type[GenerateJsonSchema] = GenerateToolJsonSchema,     strict: bool | None = None,     sequential: bool = False,     requires_approval: bool = False,     metadata: dict[str, Any] | None = None, ) -> Any:     \"\"\"Decorator to register a tool function which DOES NOT take `RunContext` as an argument.      Can decorate a sync or async functions.      The docstring is inspected to extract both the tool description and description of each parameter,     [learn more](../tools.md#function-tools-and-schema).      We can't add overloads for every possible signature of tool, since the return type is a recursive union     so the signature of functions decorated with `@agent.tool` is obscured.      Example:     ```python     from pydantic_ai import Agent, RunContext      agent = Agent('test')      @agent.tool     def foobar(ctx: RunContext[int]) -> int:         return 123      @agent.tool(retries=2)     async def spam(ctx: RunContext[str]) -> float:         return 3.14      result = agent.run_sync('foobar', deps=1)     print(result.output)     #> {\"foobar\":123,\"spam\":3.14}     ```      Args:         func: The tool function to register.         name: The name of the tool, defaults to the function name.         description: The description of the tool, defaults to the function docstring.         retries: The number of retries to allow for this tool, defaults to the agent's default retries,             which defaults to 1.         prepare: custom method to prepare the tool definition for each step, return `None` to omit this             tool from a given step. This is useful if you want to customise a tool at call time,             or omit it completely from a step. See [`ToolPrepareFunc`][pydantic_ai.tools.ToolPrepareFunc].         docstring_format: The format of the docstring, see [`DocstringFormat`][pydantic_ai.tools.DocstringFormat].             Defaults to `'auto'`, such that the format is inferred from the structure of the docstring.         require_parameter_descriptions: If True, raise an error if a parameter description is missing. Defaults to False.         schema_generator: The JSON schema generator class to use for this tool. Defaults to `GenerateToolJsonSchema`.         strict: Whether to enforce JSON schema compliance (only affects OpenAI).             See [`ToolDefinition`][pydantic_ai.tools.ToolDefinition] for more info.         sequential: Whether the function requires a sequential/serial execution environment. Defaults to False.         requires_approval: Whether this tool requires human-in-the-loop approval. Defaults to False.             See the [tools documentation](../deferred-tools.md#human-in-the-loop-tool-approval) for more info.         metadata: Optional metadata for the tool. This is not sent to the model but can be used for filtering and tool behavior customization.     \"\"\"      def tool_decorator(func_: ToolFuncPlain[ToolParams]) -> ToolFuncPlain[ToolParams]:         # noinspection PyTypeChecker         self._function_toolset.add_function(             func_,             takes_ctx=False,             name=name,             description=description,             retries=retries,             prepare=prepare,             docstring_format=docstring_format,             require_parameter_descriptions=require_parameter_descriptions,             schema_generator=schema_generator,             strict=strict,             sequential=sequential,             requires_approval=requires_approval,             metadata=metadata,         )         return func_      return tool_decorator if func is None else tool_decorator(func) ``` |\n\n#### toolset\n\n```\ntoolset(\n    func: ToolsetFunc[AgentDepsT],\n) -> ToolsetFunc[AgentDepsT]\n\ntoolset(\n    *, per_run_step: bool = True\n) -> Callable[\n    [ToolsetFunc[AgentDepsT]], ToolsetFunc[AgentDepsT]\n]", "url": "https://ai.pydantic.dev/agent/index.html#agent-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "Agent `dataclass`", "anchor": "agent-dataclass", "md_text": "toolset(\n    func: ToolsetFunc[AgentDepsT] | None = None,\n    /,\n    *,\n    per_run_step: bool = True,\n) -> Any\n```\n\nDecorator to register a toolset function which takes [`RunContext`](../tools/index.html#pydantic_ai.tools.RunContext) as its only argument.\n\nCan decorate a sync or async functions.\n\nThe decorator can be used bare (`agent.toolset`).\n\nExample:\n\n```\nfrom pydantic_ai import AbstractToolset, Agent, FunctionToolset, RunContext\n\nagent = Agent('test', deps_type=str)\n\n@agent.toolset\nasync def simple_toolset(ctx: RunContext[str]) -> AbstractToolset[str]:\n    return FunctionToolset()\n```\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `func` | `ToolsetFunc[AgentDepsT] | None` | The toolset function to register. | `None` |\n| `per_run_step` | `bool` | Whether to re-evaluate the toolset for each run step. Defaults to True. | `True` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 ``` | ``` def toolset(     self,     func: ToolsetFunc[AgentDepsT] | None = None,     /,     *,     per_run_step: bool = True, ) -> Any:     \"\"\"Decorator to register a toolset function which takes [`RunContext`][pydantic_ai.tools.RunContext] as its only argument.      Can decorate a sync or async functions.      The decorator can be used bare (`agent.toolset`).      Example:     ```python     from pydantic_ai import AbstractToolset, Agent, FunctionToolset, RunContext      agent = Agent('test', deps_type=str)      @agent.toolset     async def simple_toolset(ctx: RunContext[str]) -> AbstractToolset[str]:         return FunctionToolset()     ```      Args:         func: The toolset function to register.         per_run_step: Whether to re-evaluate the toolset for each run step. Defaults to True.     \"\"\"      def toolset_decorator(func_: ToolsetFunc[AgentDepsT]) -> ToolsetFunc[AgentDepsT]:         self._dynamic_toolsets.append(DynamicToolset(func_, per_run_step=per_run_step))         return func_      return toolset_decorator if func is None else toolset_decorator(func) ``` |\n\n#### toolsets `property`\n\n```\ntoolsets: Sequence[AbstractToolset[AgentDepsT]]\n```\n\nAll toolsets registered on the agent, including a function toolset holding tools that were registered on the agent directly.\n\nOutput tools are not included.\n\n#### \\_\\_aenter\\_\\_ `async`\n\n```\n__aenter__() -> Self\n```\n\nEnter the agent context.\n\nThis will start all [`MCPServerStdio`s](../mcp/index.html#pydantic_ai.mcp.MCPServerStdio) registered as `toolsets` so they are ready to be used.\n\nThis is a no-op if the agent has already been entered.\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 ``` | ``` async def __aenter__(self) -> Self:     \"\"\"Enter the agent context.      This will start all [`MCPServerStdio`s][pydantic_ai.mcp.MCPServerStdio] registered as `toolsets` so they are ready to be used.      This is a no-op if the agent has already been entered.     \"\"\"     async with self._enter_lock:         if self._entered_count == 0:             async with AsyncExitStack() as exit_stack:                 toolset = self._get_toolset()                 await exit_stack.enter_async_context(toolset)                  self._exit_stack = exit_stack.pop_all()         self._entered_count += 1     return self ``` |\n\n#### set\\_mcp\\_sampling\\_model\n\n```\nset_mcp_sampling_model(\n    model: Model | KnownModelName | str | None = None,\n) -> None\n```\n\nSet the sampling model on all MCP servers registered with the agent.\n\nIf no sampling model is provided, the agent's model will be used.\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/__init__.py`", "url": "https://ai.pydantic.dev/agent/index.html#agent-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "Agent `dataclass`", "anchor": "agent-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 ``` | ``` def set_mcp_sampling_model(self, model: models.Model | models.KnownModelName | str | None = None) -> None:     \"\"\"Set the sampling model on all MCP servers registered with the agent.      If no sampling model is provided, the agent's model will be used.     \"\"\"     try:         sampling_model = models.infer_model(model) if model else self._get_model(None)     except exceptions.UserError as e:         raise exceptions.UserError('No sampling model provided and no model set on the agent.') from e      from ..mcp import MCPServer      def _set_sampling_model(toolset: AbstractToolset[AgentDepsT]) -> None:         if isinstance(toolset, MCPServer):             toolset.sampling_model = sampling_model      self._get_toolset().apply(_set_sampling_model) ``` |\n\n#### run\\_mcp\\_servers `async` `deprecated`\n\n```\nrun_mcp_servers(\n    model: Model | KnownModelName | str | None = None,\n) -> AsyncIterator[None]\n```\n\nDeprecated\n\n`run_mcp_servers` is deprecated, use `async with agent:` instead. If you need to set a sampling model on all MCP servers, use `agent.set_mcp_sampling_model()`.\n\nRun [`MCPServerStdio`s](../mcp/index.html#pydantic_ai.mcp.MCPServerStdio) so they can be used by the agent.\n\nDeprecated: use [`async with agent`](index.html#pydantic_ai.agent.Agent.__aenter__) instead.\nIf you need to set a sampling model on all MCP servers, use [`agent.set_mcp_sampling_model()`](index.html#pydantic_ai.agent.Agent.set_mcp_sampling_model).\n\nReturns: a context manager to start and shutdown the servers.\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 ``` | ``` @asynccontextmanager @deprecated(     '`run_mcp_servers` is deprecated, use `async with agent:` instead. If you need to set a sampling model on all MCP servers, use `agent.set_mcp_sampling_model()`.' ) async def run_mcp_servers(     self, model: models.Model | models.KnownModelName | str | None = None ) -> AsyncIterator[None]:     \"\"\"Run [`MCPServerStdio`s][pydantic_ai.mcp.MCPServerStdio] so they can be used by the agent.      Deprecated: use [`async with agent`][pydantic_ai.agent.Agent.__aenter__] instead.     If you need to set a sampling model on all MCP servers, use [`agent.set_mcp_sampling_model()`][pydantic_ai.agent.Agent.set_mcp_sampling_model].      Returns: a context manager to start and shutdown the servers.     \"\"\"     try:         self.set_mcp_sampling_model(model)     except exceptions.UserError:         if model is not None:             raise      async with self:         yield ``` |", "url": "https://ai.pydantic.dev/agent/index.html#agent-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractAgent", "anchor": "abstractagent", "md_text": "Bases: `Generic[AgentDepsT, OutputDataT]`, `ABC`\n\nAbstract superclass for [`Agent`](index.html#pydantic_ai.agent.Agent), [`WrapperAgent`](index.html#pydantic_ai.agent.WrapperAgent), and your own custom agent implementations.\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/abstract.py`", "url": "https://ai.pydantic.dev/agent/index.html#abstractagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractAgent", "anchor": "abstractagent", "md_text": "|  |  |\n| --- | --- |\n| ```   75   76   77   78   79   80   81   82   83   84   85   86   87   88   89   90   91   92   93   94   95   96   97   98   99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144  145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160  161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176  177  178  179  180  181  182  183  184  185  186  187  188  189  190  191  192  193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208  209  210  211  212  213  214  215  216  217  218  219  220  221  222  223  224  225  226  227  228  229  230  231  232  233  234  235  236  237  238  239  240  241  242  243  244  245  246  247  248  249  250  251  252  253  254  255  256  257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272  273  274  275  276  277  278  279  280  281  282  283  284  285  286  287  288  289  290  291  292  293  294  295  296  297  298  299  300  301  302  303  304  305  306  307  308  309  310  311  312  313  314  315  316  317  318  319  320  321  322  323  324  325  326  327  328  329  330  331  332  333  334  335  336  337  338  339  340  341  342  343  344  345  346  347  348  349  350  351  352  353  354  355  356  357  358  359  360  361  362  363  364  365  366  367  368  369  370  371  372  373  374  375  376  377  378  379  380  381  382  383  384  385  386  387  388  389  390  391  392  393  394  395  396  397  398  399  400  401  402  403  404  405  406  407  408  409  410  411  412  413  414  415  416  417  418  419  420  421  422  423  424  425  426  427  428  429  430  431  432  433  434  435  436  437  438  439  440  441  442  443  444  445  446  447  448  449  450  451  452  453  454  455  456  457  458  459  460  461  462  463  464  465  466  467  468  469  470  471  472  473  474  475  476  477  478  479  480  481  482  483  484  485  486  487  488  489  490  491  492  493  494  495  496  497  498  499  500  501  502  503  504  505  506  507  508  509  510  511  512  513  514  515  516  517  518  519  520  521  522  523  524  525  526  527  528  529  530  531  532  533  534  535  536  537  538  539  540  541  542  543  544  545  546  547  548  549  550  551  552  553  554  555  556  557  558  559  560  561  562  563  564  565  566  567  568  569  570  571  572  573  574  575  576  577  578  579  580  581  582  583  584  585  586  587  588  589  590  591  592  593  594  595  596  597  598  599  600  601  602  603  604  605  606  607  608  609  610  611  612  613  614  615  616  617  618  619  620  621  622  623  624  625  626  627  628  629  630  631  632  633  634  635  636  637  638  639  640  641  642  643  644  645  646  647  648  649  650  651  652  653  654  655  656  657  658  659  660  661  662  663  664  665  666  667  668  669  670  671  672  673  674  675  676  677  678  679  680  681  682  683  684  685  686  687  688  689  690  691  692  693  694  695  696  697  698  699  700  701  702  703  704  705  706  707  708  709  710  711  712  713  714  715  716  717  718  719  720  721  722  723  724  725  726  727  728  729  730  731  732  733  734  735  736  737  738  739  740  741  742  743  744  745  746  747  748  749  750  751  752  753  754  755  756  757  758  759  760  761  762  763  764  765  766  767  768  769  770  771  772  773  774  775  776  777  778  779  780  781  782  783  784  785  786  787  788  789  790  791  792  793  794  795  796  797  798  799  800  801  802  803  804  805  806  807  808  809  810  811  812  813  814  815  816  817  818  819  820  821  822  823  824  825  826  827  828  829  830  831  832  833  834  835  836  837  838  839  840  841  842  843  844  845  846  847  848  849  850  851  852  853  854  855  856  857  858  859  860  861  862  863  864  865  866  867  868  869  870  871  872  873  874  875  876  877  878  879  880  881  882  883  884  885  886  887  888  889  890  891  892  893  894  895  896  897  898  899  900  901  902  903  904  905  906  907  908  909  910  911  912  913  914  915  916  917  918  919  920  921  922  923  924  925  926  927  928  929  930  931  932  933  934  935  936  937  938  939  940  941  942  943  944  945  946  947  948  949  950  951  952  953  954  955  956  957  958  959  960  961  962  963  964  965  966  967  968  969  970  971  972  973  974  975  976  977  978  979  980  981  982  983  984  985  986  987  988  989  990  991  992  993  994  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 ``` | ``` class AbstractAgent(Generic[AgentDepsT, OutputDataT], ABC):     \"\"\"Abstract superclass for [`Agent`][pydantic_ai.agent.Agent], [`WrapperAgent`][pydantic_ai.agent.WrapperAgent], and your own custom agent implementations.\"\"\"      @property     @abstractmethod     def model(self) -> models.Model | models.KnownModelName | str | None:         \"\"\"The default model configured for this agent.\"\"\"         raise NotImplementedError      @property     @abstractmethod     def name(self) -> str | None:         \"\"\"The name of the agent, used for logging.          If `None`, we try to infer the agent name from the call frame when the agent is first run.         \"\"\"         raise NotImplementedError      @name.setter     @abstractmethod     def name(self, value: str | None) -> None:         \"\"\"Set the name of the agent, used for logging.\"\"\"         raise NotImplementedError      @property     @abstractmethod     def deps_type(self) -> type:         \"\"\"The type of dependencies used by the agent.\"\"\"         raise NotImplementedError      @property     @abstractmethod     def output_type(self) -> OutputSpec[OutputDataT]:         \"\"\"The type of data output by agent runs, used to validate the data returned by the model, defaults to `str`.\"\"\"         raise NotImplementedError      @property     @abstractmethod     def event_stream_handler(self) -> EventStreamHandler[AgentDepsT] | None:         \"\"\"Optional handler for events from the model's streaming response and the agent's execution of tools.\"\"\"         raise NotImplementedError      @property     @abstractmethod     def toolsets(self) -> Sequence[AbstractToolset[AgentDepsT]]:         \"\"\"All toolsets registered on the agent.          Output tools are not included.         \"\"\"         raise NotImplementedError      @overload     async def run(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AgentRunResult[OutputDataT]: ...      @overload     async def run(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT],         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AgentRunResult[RunOutputDataT]: ...      async def run(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT] | None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AgentRunResult[Any]:         \"\"\"Run the agent with a user prompt in async mode.          This method builds an internal agent graph (using system prompts, tools and output schemas) and then         runs the graph to completion. The result of the run is returned.          Example:         ```python         from pydantic_ai import Agent          agent = Agent('openai:gpt-4o')          async def main():             agent_run = await agent.run('What is the capital of France?')             print(agent_run.output)             #> The capital of France is Paris.         ```          Args:             user_prompt: User input to start/continue the conversation.             output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no                 output validators since output validators would expect an argument that matches the agent's output type.             message_history: History of the conversation so far.             deferred_tool_results: Optional results for deferred tool calls in the message history.             model: Optional model to use for this run, required if `model` was not set when creating the agent.             deps: Optional dependencies to use for this run.             model_settings: Optional settings to use for this model's request.             usage_limits: Optional limits on model request count or token usage.             usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.             infer_name: Whether to try to infer the agent name from the call frame if it's not set.             toolsets: Optional additional toolsets for this run.             event_stream_handler: Optional handler for events from the model's streaming response and the agent's execution of tools to use for this run.             builtin_tools: Optional additional builtin tools for this run.          Returns:             The result of the run.         \"\"\"         if infer_name and self.name is None:             self._infer_name(inspect.currentframe())          event_stream_handler = event_stream_handler or self.event_stream_handler          async with self.iter(             user_prompt=user_prompt,             output_type=output_type,             message_history=message_history,             deferred_tool_results=deferred_tool_results,             model=model,             deps=deps,             model_settings=model_settings,             usage_limits=usage_limits,             usage=usage,             toolsets=toolsets,             builtin_tools=builtin_tools,         ) as agent_run:             async for node in agent_run:                 if event_stream_handler is not None and (                     self.is_model_request_node(node) or self.is_call_tools_node(node)                 ):                     async with node.stream(agent_run.ctx) as stream:                         await event_stream_handler(_agent_graph.build_run_context(agent_run.ctx), stream)          assert agent_run.result is not None, 'The graph run did not finish properly'         return agent_run.result      @overload     def run_sync(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AgentRunResult[OutputDataT]: ...      @overload     def run_sync(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT],         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AgentRunResult[RunOutputDataT]: ...      def run_sync(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT] | None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AgentRunResult[Any]:         \"\"\"Synchronously run the agent with a user prompt.          This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] with `loop.run_until_complete(...)`.         You therefore can't use this method inside async code or if there's an active event loop.          Example:         ```python         from pydantic_ai import Agent          agent = Agent('openai:gpt-4o')          result_sync = agent.run_sync('What is the capital of Italy?')         print(result_sync.output)         #> The capital of Italy is Rome.         ```          Args:             user_prompt: User input to start/continue the conversation.             output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no                 output validators since output validators would expect an argument that matches the agent's output type.             message_history: History of the conversation so far.             deferred_tool_results: Optional results for deferred tool calls in the message history.             model: Optional model to use for this run, required if `model` was not set when creating the agent.             deps: Optional dependencies to use for this run.             model_settings: Optional settings to use for this model's request.             usage_limits: Optional limits on model request count or token usage.             usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.             infer_name: Whether to try to infer the agent name from the call frame if it's not set.             toolsets: Optional additional toolsets for this run.             event_stream_handler: Optional handler for events from the model's streaming response and the agent's execution of tools to use for this run.             builtin_tools: Optional additional builtin tools for this run.          Returns:             The result of the run.         \"\"\"         if infer_name and self.name is None:             self._infer_name(inspect.currentframe())          return get_event_loop().run_until_complete(             self.run(                 user_prompt,                 output_type=output_type,                 message_history=message_history,                 deferred_tool_results=deferred_tool_results,                 model=model,                 deps=deps,                 model_settings=model_settings,                 usage_limits=usage_limits,                 usage=usage,                 infer_name=False,                 toolsets=toolsets,                 builtin_tools=builtin_tools,                 event_stream_handler=event_stream_handler,             )         )      @overload     def run_stream(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AbstractAsyncContextManager[result.StreamedRunResult[AgentDepsT, OutputDataT]]: ...      @overload     def run_stream(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT],         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AbstractAsyncContextManager[result.StreamedRunResult[AgentDepsT, RunOutputDataT]]: ...      @asynccontextmanager     async def run_stream(  # noqa C901         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT] | None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AsyncIterator[result.StreamedRunResult[AgentDepsT, Any]]:         \"\"\"Run the agent with a user prompt in async streaming mode.          This method builds an internal agent graph (using system prompts, tools and output schemas) and then         runs the graph until the model produces output matching the `output_type`, for example text or structured data.         At this point, a streaming run result object is yielded from which you can stream the output as it comes in,         and -- once this output has completed streaming -- get the complete output, message history, and usage.          As this method will consider the first output matching the `output_type` to be the final output,         it will stop running the agent graph and will not execute any tool calls made by the model after this \"final\" output.         If you want to always run the agent graph to completion and stream events and output at the same time,         use [`agent.run()`][pydantic_ai.agent.AbstractAgent.run] with an `event_stream_handler` or [`agent.iter()`][pydantic_ai.agent.AbstractAgent.iter] instead.          Example:         ```python         from pydantic_ai import Agent          agent = Agent('openai:gpt-4o')          async def main():             async with agent.run_stream('What is the capital of the UK?') as response:                 print(await response.get_output())                 #> The capital of the UK is London.         ```          Args:             user_prompt: User input to start/continue the conversation.             output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no                 output validators since output validators would expect an argument that matches the agent's output type.             message_history: History of the conversation so far.             deferred_tool_results: Optional results for deferred tool calls in the message history.             model: Optional model to use for this run, required if `model` was not set when creating the agent.             deps: Optional dependencies to use for this run.             model_settings: Optional settings to use for this model's request.             usage_limits: Optional limits on model request count or token usage.             usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.             infer_name: Whether to try to infer the agent name from the call frame if it's not set.             toolsets: Optional additional toolsets for this run.             builtin_tools: Optional additional builtin tools for this run.             event_stream_handler: Optional handler for events from the model's streaming response and the agent's execution of tools to use for this run.                 It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager.                 Note that it does _not_ receive any events after the final result is found.          Returns:             The result of the run.         \"\"\"         if infer_name and self.name is None:             # f_back because `asynccontextmanager` adds one frame             if frame := inspect.currentframe():  # pragma: no branch                 self._infer_name(frame.f_back)          event_stream_handler = event_stream_handler or self.event_stream_handler          yielded = False         async with self.iter(             user_prompt,             output_type=output_type,             message_history=message_history,             deferred_tool_results=deferred_tool_results,             model=model,             deps=deps,             model_settings=model_settings,             usage_limits=usage_limits,             usage=usage,             infer_name=False,             toolsets=toolsets,             builtin_tools=builtin_tools,         ) as agent_run:             first_node = agent_run.next_node  # start with the first node             assert isinstance(first_node, _agent_graph.UserPromptNode)  # the first node should be a user prompt node             node = first_node             while True:                 graph_ctx = agent_run.ctx                 if self.is_model_request_node(node):                     async with node.stream(graph_ctx) as stream:                         final_result_event = None                          async def stream_to_final(                             stream: AgentStream,                         ) -> AsyncIterator[_messages.ModelResponseStreamEvent]:                             nonlocal final_result_event                             async for event in stream:                                 yield event                                 if isinstance(event, _messages.FinalResultEvent):                                     final_result_event = event                                     break                          if event_stream_handler is not None:                             await event_stream_handler(                                 _agent_graph.build_run_context(graph_ctx), stream_to_final(stream)                             )                         else:                             async for _ in stream_to_final(stream):                                 pass                          if final_result_event is not None:                             final_result = FinalResult(                                 None, final_result_event.tool_name, final_result_event.tool_call_id                             )                             if yielded:                                 raise exceptions.AgentRunError('Agent run produced final results')  # pragma: no cover                             yielded = True                              messages = graph_ctx.state.message_history.copy()                              async def on_complete() -> None:                                 \"\"\"Called when the stream has completed.                                  The model response will have been added to messages by now                                 by `StreamedRunResult._marked_completed`.                                 \"\"\"                                 nonlocal final_result                                 final_result = FinalResult(                                     await stream.get_output(), final_result.tool_name, final_result.tool_call_id                                 )                                  # When we get here, the `ModelRequestNode` has completed streaming after the final result was found.                                 # When running an agent with `agent.run`, we'd then move to `CallToolsNode` to execute the tool calls and                                 # find the final result.                                 # We also want to execute tool calls (in case `agent.end_strategy == 'exhaustive'`) here, but                                 # we don't want to use run the `CallToolsNode` logic to determine the final output, as it would be                                 # wasteful and could produce a different result (e.g. when text output is followed by tool calls).                                 # So we call `process_tool_calls` directly and then end the run with the found final result.                                  parts: list[_messages.ModelRequestPart] = []                                 async for _event in _agent_graph.process_tool_calls(                                     tool_manager=graph_ctx.deps.tool_manager,                                     tool_calls=stream.response.tool_calls,                                     tool_call_results=None,                                     final_result=final_result,                                     ctx=graph_ctx,                                     output_parts=parts,                                 ):                                     pass                                  # For backwards compatibility, append a new ModelRequest using the tool returns and retries                                 if parts:                                     messages.append(_messages.ModelRequest(parts))                                  await agent_run.next(_agent_graph.SetFinalResult(final_result))                              yield StreamedRunResult(                                 messages,                                 graph_ctx.deps.new_message_index,                                 stream,                                 on_complete,                             )                             break                 elif self.is_call_tools_node(node) and event_stream_handler is not None:                     async with node.stream(agent_run.ctx) as stream:                         await event_stream_handler(_agent_graph.build_run_context(agent_run.ctx), stream)                  next_node = await agent_run.next(node)                 if isinstance(next_node, End) and agent_run.result is not None:                     # A final output could have been produced by the CallToolsNode rather than the ModelRequestNode,                     # if a tool function raised CallDeferred or ApprovalRequired.                     # In this case there's no response to stream, but we still let the user access the output etc as normal.                     yield StreamedRunResult(                         graph_ctx.state.message_history,                         graph_ctx.deps.new_message_index,                         run_result=agent_run.result,                     )                     yielded = True                     break                 if not isinstance(next_node, _agent_graph.AgentNode):                     raise exceptions.AgentRunError(  # pragma: no cover                         'Should have produced a StreamedRunResult before getting here'                     )                 node = cast(_agent_graph.AgentNode[Any, Any], next_node)          if not yielded:             raise exceptions.AgentRunError('Agent run finished without producing a final result')  # pragma: no cover      @overload     def run_stream_events(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[OutputDataT]]: ...      @overload     def run_stream_events(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT],         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[RunOutputDataT]]: ...      def run_stream_events(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT] | None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[Any]]:         \"\"\"Run the agent with a user prompt in async mode and stream events from the run.          This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] and         uses the `event_stream_handler` kwarg to get a stream of events from the run.          Example:         ```python         from pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent          agent = Agent('openai:gpt-4o')          async def main():             events: list[AgentStreamEvent | AgentRunResultEvent] = []             async for event in agent.run_stream_events('What is the capital of France?'):                 events.append(event)             print(events)             '''             [                 PartStartEvent(index=0, part=TextPart(content='The capital of ')),                 FinalResultEvent(tool_name=None, tool_call_id=None),                 PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='France is Paris. ')),                 AgentRunResultEvent(                     result=AgentRunResult(output='The capital of France is Paris. ')                 ),             ]             '''         ```          Arguments are the same as for [`self.run`][pydantic_ai.agent.AbstractAgent.run],         except that `event_stream_handler` is now allowed.          Args:             user_prompt: User input to start/continue the conversation.             output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no                 output validators since output validators would expect an argument that matches the agent's output type.             message_history: History of the conversation so far.             deferred_tool_results: Optional results for deferred tool calls in the message history.             model: Optional model to use for this run, required if `model` was not set when creating the agent.             deps: Optional dependencies to use for this run.             model_settings: Optional settings to use for this model's request.             usage_limits: Optional limits on model request count or token usage.             usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.             infer_name: Whether to try to infer the agent name from the call frame if it's not set.             toolsets: Optional additional toolsets for this run.             builtin_tools: Optional additional builtin tools for this run.          Returns:             An async iterable of stream events `AgentStreamEvent` and finally a `AgentRunResultEvent` with the final             run result.         \"\"\"         # unfortunately this hack of returning a generator rather than defining it right here is         # required to allow overloads of this method to work in python's typing system, or at least with pyright         # or at least I couldn't make it work without         return self._run_stream_events(             user_prompt,             output_type=output_type,             message_history=message_history,             deferred_tool_results=deferred_tool_results,             model=model,             deps=deps,             model_settings=model_settings,             usage_limits=usage_limits,             usage=usage,             infer_name=infer_name,             toolsets=toolsets,             builtin_tools=builtin_tools,         )      async def _run_stream_events(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT] | None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[Any]]:         send_stream, receive_stream = anyio.create_memory_object_stream[             _messages.AgentStreamEvent | AgentRunResultEvent[Any]         ]()          async def event_stream_handler(             _: RunContext[AgentDepsT], events: AsyncIterable[_messages.AgentStreamEvent]         ) -> None:             async for event in events:                 await send_stream.send(event)          async def run_agent() -> AgentRunResult[Any]:             async with send_stream:                 return await self.run(                     user_prompt,                     output_type=output_type,                     message_history=message_history,                     deferred_tool_results=deferred_tool_results,                     model=model,                     deps=deps,                     model_settings=model_settings,                     usage_limits=usage_limits,                     usage=usage,                     infer_name=infer_name,                     toolsets=toolsets,                     builtin_tools=builtin_tools,                     event_stream_handler=event_stream_handler,                 )          task = asyncio.create_task(run_agent())          async with receive_stream:             async for message in receive_stream:                 yield message          result = await task         yield AgentRunResultEvent(result)      @overload     def iter(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     ) -> AbstractAsyncContextManager[AgentRun[AgentDepsT, OutputDataT]]: ...      @overload     def iter(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT],         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     ) -> AbstractAsyncContextManager[AgentRun[AgentDepsT, RunOutputDataT]]: ...      @asynccontextmanager     @abstractmethod     async def iter(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT] | None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     ) -> AsyncIterator[AgentRun[AgentDepsT, Any]]:         \"\"\"A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.          This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an         `AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are         executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the         stream of events coming from the execution of tools.          The `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,         and the final result of the run once it has completed.          For more details, see the documentation of `AgentRun`.          Example:         ```python         from pydantic_ai import Agent          agent = Agent('openai:gpt-4o')          async def main():             nodes = []             async with agent.iter('What is the capital of France?') as agent_run:                 async for node in agent_run:                     nodes.append(node)             print(nodes)             '''             [                 UserPromptNode(                     user_prompt='What is the capital of France?',                     instructions_functions=[],                     system_prompts=(),                     system_prompt_functions=[],                     system_prompt_dynamic_functions={},                 ),                 ModelRequestNode(                     request=ModelRequest(                         parts=[                             UserPromptPart(                                 content='What is the capital of France?',                                 timestamp=datetime.datetime(...),                             )                         ]                     )                 ),                 CallToolsNode(                     model_response=ModelResponse(                         parts=[TextPart(content='The capital of France is Paris.')],                         usage=RequestUsage(input_tokens=56, output_tokens=7),                         model_name='gpt-4o',                         timestamp=datetime.datetime(...),                     )                 ),                 End(data=FinalResult(output='The capital of France is Paris.')),             ]             '''             print(agent_run.result.output)             #> The capital of France is Paris.         ```          Args:             user_prompt: User input to start/continue the conversation.             output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no                 output validators since output validators would expect an argument that matches the agent's output type.             message_history: History of the conversation so far.             deferred_tool_results: Optional results for deferred tool calls in the message history.             model: Optional model to use for this run, required if `model` was not set when creating the agent.             deps: Optional dependencies to use for this run.             model_settings: Optional settings to use for this model's request.             usage_limits: Optional limits on model request count or token usage.             usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.             infer_name: Whether to try to infer the agent name from the call frame if it's not set.             toolsets: Optional additional toolsets for this run.             builtin_tools: Optional additional builtin tools for this run.          Returns:             The result of the run.         \"\"\"         raise NotImplementedError         yield      @contextmanager     @abstractmethod     def override(         self,         *,         name: str | _utils.Unset = _utils.UNSET,         deps: AgentDepsT | _utils.Unset = _utils.UNSET,         model: models.Model | models.KnownModelName | str | _utils.Unset = _utils.UNSET,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | _utils.Unset = _utils.UNSET,         tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | _utils.Unset = _utils.UNSET,         instructions: Instructions[AgentDepsT] | _utils.Unset = _utils.UNSET,     ) -> Iterator[None]:         \"\"\"Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.          This is particularly useful when testing.         You can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).          Args:             name: The name to use instead of the name passed to the agent constructor and agent run.             deps: The dependencies to use instead of the dependencies passed to the agent run.             model: The model to use instead of the model passed to the agent run.             toolsets: The toolsets to use instead of the toolsets passed to the agent constructor and agent run.             tools: The tools to use instead of the tools registered with the agent.             instructions: The instructions to use instead of the instructions registered with the agent.         \"\"\"         raise NotImplementedError         yield      def _infer_name(self, function_frame: FrameType | None) -> None:         \"\"\"Infer the agent name from the call frame.          RunUsage should be `self._infer_name(inspect.currentframe())`.         \"\"\"         assert self.name is None, 'Name already set'         if function_frame is not None:  # pragma: no branch             if parent_frame := function_frame.f_back:  # pragma: no branch                 for name, item in parent_frame.f_locals.items():                     if item is self:                         self.name = name                         return                 if parent_frame.f_locals != parent_frame.f_globals:  # pragma: no branch                     # if we couldn't find the agent in locals and globals are a different dict, try globals                     for name, item in parent_frame.f_globals.items():                         if item is self:                             self.name = name                             return      @staticmethod     @contextmanager     def sequential_tool_calls() -> Iterator[None]:         \"\"\"Run tool calls sequentially during the context.\"\"\"         with ToolManager.sequential_tool_calls():             yield      @staticmethod     def is_model_request_node(         node: _agent_graph.AgentNode[T, S] | End[result.FinalResult[S]],     ) -> TypeIs[_agent_graph.ModelRequestNode[T, S]]:         \"\"\"Check if the node is a `ModelRequestNode`, narrowing the type if it is.          This method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.         \"\"\"         return isinstance(node, _agent_graph.ModelRequestNode)      @staticmethod     def is_call_tools_node(         node: _agent_graph.AgentNode[T, S] | End[result.FinalResult[S]],     ) -> TypeIs[_agent_graph.CallToolsNode[T, S]]:         \"\"\"Check if the node is a `CallToolsNode`, narrowing the type if it is.          This method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.         \"\"\"         return isinstance(node, _agent_graph.CallToolsNode)      @staticmethod     def is_user_prompt_node(         node: _agent_graph.AgentNode[T, S] | End[result.FinalResult[S]],     ) -> TypeIs[_agent_graph.UserPromptNode[T, S]]:         \"\"\"Check if the node is a `UserPromptNode`, narrowing the type if it is.          This method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.         \"\"\"         return isinstance(node, _agent_graph.UserPromptNode)      @staticmethod     def is_end_node(         node: _agent_graph.AgentNode[T, S] | End[result.FinalResult[S]],     ) -> TypeIs[End[result.FinalResult[S]]]:         \"\"\"Check if the node is a `End`, narrowing the type if it is.          This method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.         \"\"\"         return isinstance(node, End)      @abstractmethod     async def __aenter__(self) -> AbstractAgent[AgentDepsT, OutputDataT]:         raise NotImplementedError      @abstractmethod     async def __aexit__(self, *args: Any) -> bool | None:         raise NotImplementedError      def to_ag_ui(         self,         *,         # Agent.iter parameters         output_type: OutputSpec[OutputDataT] | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: UsageLimits | None = None,         usage: RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         # Starlette         debug: bool = False,         routes: Sequence[BaseRoute] | None = None,         middleware: Sequence[Middleware] | None = None,         exception_handlers: Mapping[Any, ExceptionHandler] | None = None,         on_startup: Sequence[Callable[[], Any]] | None = None,         on_shutdown: Sequence[Callable[[], Any]] | None = None,         lifespan: Lifespan[AGUIApp[AgentDepsT, OutputDataT]] | None = None,     ) -> AGUIApp[AgentDepsT, OutputDataT]:         \"\"\"Returns an ASGI application that handles every AG-UI request by running the agent.          Note that the `deps` will be the same for each request, with the exception of the AG-UI state that's         injected into the `state` field of a `deps` object that implements the [`StateHandler`][pydantic_ai.ag_ui.StateHandler] protocol.         To provide different `deps` for each request (e.g. based on the authenticated user),         use [`pydantic_ai.ag_ui.run_ag_ui`][pydantic_ai.ag_ui.run_ag_ui] or         [`pydantic_ai.ag_ui.handle_ag_ui_request`][pydantic_ai.ag_ui.handle_ag_ui_request] instead.          Example:         ```python         from pydantic_ai import Agent          agent = Agent('openai:gpt-4o')         app = agent.to_ag_ui()         ```          The `app` is an ASGI application that can be used with any ASGI server.          To run the application, you can use the following command:          ```bash         uvicorn app:app --host 0.0.0.0 --port 8000         ```          See [AG-UI docs](../ag-ui.md) for more information.          Args:             output_type: Custom output type to use for this run, `output_type` may only be used if the agent has                 no output validators since output validators would expect an argument that matches the agent's                 output type.             model: Optional model to use for this run, required if `model` was not set when creating the agent.             deps: Optional dependencies to use for this run.             model_settings: Optional settings to use for this model's request.             usage_limits: Optional limits on model request count or token usage.             usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.             infer_name: Whether to try to infer the agent name from the call frame if it's not set.             toolsets: Optional additional toolsets for this run.              debug: Boolean indicating if debug tracebacks should be returned on errors.             routes: A list of routes to serve incoming HTTP and WebSocket requests.             middleware: A list of middleware to run for every request. A starlette application will always                 automatically include two middleware classes. `ServerErrorMiddleware` is added as the very                 outermost middleware, to handle any uncaught errors occurring anywhere in the entire stack.                 `ExceptionMiddleware` is added as the very innermost middleware, to deal with handled                 exception cases occurring in the routing or endpoints.             exception_handlers: A mapping of either integer status codes, or exception class types onto                 callables which handle the exceptions. Exception handler callables should be of the form                 `handler(request, exc) -> response` and may be either standard functions, or async functions.             on_startup: A list of callables to run on application startup. Startup handler callables do not                 take any arguments, and may be either standard functions, or async functions.             on_shutdown: A list of callables to run on application shutdown. Shutdown handler callables do                 not take any arguments, and may be either standard functions, or async functions.             lifespan: A lifespan context function, which can be used to perform startup and shutdown tasks.                 This is a newer style that replaces the `on_startup` and `on_shutdown` handlers. Use one or                 the other, not both.          Returns:             An ASGI application for running Pydantic AI agents with AG-UI protocol support.         \"\"\"         from ..ag_ui import AGUIApp          return AGUIApp(             agent=self,             # Agent.iter parameters             output_type=output_type,             model=model,             deps=deps,             model_settings=model_settings,             usage_limits=usage_limits,             usage=usage,             infer_name=infer_name,             toolsets=toolsets,             # Starlette             debug=debug,             routes=routes,             middleware=middleware,             exception_handlers=exception_handlers,             on_startup=on_startup,             on_shutdown=on_shutdown,             lifespan=lifespan,         )      def to_a2a(         self,         *,         storage: Storage | None = None,         broker: Broker | None = None,         # Agent card         name: str | None = None,         url: str = 'http://localhost:8000',         version: str = '1.0.0',         description: str | None = None,         provider: AgentProvider | None = None,         skills: list[Skill] | None = None,         # Starlette         debug: bool = False,         routes: Sequence[Route] | None = None,         middleware: Sequence[Middleware] | None = None,         exception_handlers: dict[Any, ExceptionHandler] | None = None,         lifespan: Lifespan[FastA2A] | None = None,     ) -> FastA2A:         \"\"\"Convert the agent to a FastA2A application.          Example:         ```python         from pydantic_ai import Agent          agent = Agent('openai:gpt-4o')         app = agent.to_a2a()         ```          The `app` is an ASGI application that can be used with any ASGI server.          To run the application, you can use the following command:          ```bash         uvicorn app:app --host 0.0.0.0 --port 8000         ```         \"\"\"         from .._a2a import agent_to_a2a          return agent_to_a2a(             self,             storage=storage,             broker=broker,             name=name,             url=url,             version=version,             description=description,             provider=provider,             skills=skills,             debug=debug,             routes=routes,             middleware=middleware,             exception_handlers=exception_handlers,             lifespan=lifespan,         )      async def to_cli(         self: Self,         deps: AgentDepsT = None,         prog_name: str = 'pydantic-ai',         message_history: Sequence[_messages.ModelMessage] | None = None,     ) -> None:         \"\"\"Run the agent in a CLI chat interface.          Args:             deps: The dependencies to pass to the agent.             prog_name: The name of the program to use for the CLI. Defaults to 'pydantic-ai'.             message_history: History of the conversation so far.          Example:         ```python {title=\"agent_to_cli.py\" test=\"skip\"}         from pydantic_ai import Agent          agent = Agent('openai:gpt-4o', instructions='You always respond in Italian.')          async def main():             await agent.to_cli()         ```         \"\"\"         from rich.console import Console          from pydantic_ai._cli import run_chat          await run_chat(             stream=True,             agent=self,             deps=deps,             console=Console(),             code_theme='monokai',             prog_name=prog_name,             message_history=message_history,         )      def to_cli_sync(         self: Self,         deps: AgentDepsT = None,         prog_name: str = 'pydantic-ai',         message_history: Sequence[_messages.ModelMessage] | None = None,     ) -> None:         \"\"\"Run the agent in a CLI chat interface with the non-async interface.          Args:             deps: The dependencies to pass to the agent.             prog_name: The name of the program to use for the CLI. Defaults to 'pydantic-ai'.             message_history: History of the conversation so far.          ```python {title=\"agent_to_cli_sync.py\" test=\"skip\"}         from pydantic_ai import Agent          agent = Agent('openai:gpt-4o', instructions='You always respond in Italian.')         agent.to_cli_sync()         agent.to_cli_sync(prog_name='assistant')         ```         \"\"\"         return get_event_loop().run_until_complete(             self.to_cli(deps=deps, prog_name=prog_name, message_history=message_history)         ) ``` |", "url": "https://ai.pydantic.dev/agent/index.html#abstractagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractAgent", "anchor": "abstractagent", "md_text": "#### model `abstractmethod` `property`\n\n```\nmodel: Model | KnownModelName | str | None\n```\n\nThe default model configured for this agent.\n\n#### name `abstractmethod` `property` `writable`\n\n```\nname: str | None\n```\n\nThe name of the agent, used for logging.\n\nIf `None`, we try to infer the agent name from the call frame when the agent is first run.\n\n#### deps\\_type `abstractmethod` `property`\n\n```\ndeps_type: type\n```\n\nThe type of dependencies used by the agent.\n\n#### output\\_type `abstractmethod` `property`\n\n```\noutput_type: OutputSpec[OutputDataT]\n```\n\nThe type of data output by agent runs, used to validate the data returned by the model, defaults to `str`.\n\n#### event\\_stream\\_handler `abstractmethod` `property`\n\n```\nevent_stream_handler: EventStreamHandler[AgentDepsT] | None\n```\n\nOptional handler for events from the model's streaming response and the agent's execution of tools.\n\n#### toolsets `abstractmethod` `property`\n\n```\ntoolsets: Sequence[AbstractToolset[AgentDepsT]]\n```\n\nAll toolsets registered on the agent.\n\nOutput tools are not included.\n\n#### run `async`\n\n```\nrun(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[OutputDataT]\n\nrun(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[RunOutputDataT]\n\nrun(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[Any]\n```\n\nRun the agent with a user prompt in async mode.\n\nThis method builds an internal agent graph (using system prompts, tools and output schemas) and then\nruns the graph to completion. The result of the run is returned.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    agent_run = await agent.run('What is the capital of France?')\n    print(agent_run.output)\n    #> The capital of France is Paris.\n```\n\nParameters:", "url": "https://ai.pydantic.dev/agent/index.html#abstractagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractAgent", "anchor": "abstractagent", "md_text": "| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` |\n| `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` |\n| `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional handler for events from the model's streaming response and the agent's execution of tools to use for this run. | `None` |\n| `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AgentRunResult[Any]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/abstract.py`", "url": "https://ai.pydantic.dev/agent/index.html#abstractagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractAgent", "anchor": "abstractagent", "md_text": "|  |  |\n| --- | --- |\n| ``` 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 ``` | ``` async def run(     self,     user_prompt: str | Sequence[_messages.UserContent] | None = None,     *,     output_type: OutputSpec[RunOutputDataT] | None = None,     message_history: Sequence[_messages.ModelMessage] | None = None,     deferred_tool_results: DeferredToolResults | None = None,     model: models.Model | models.KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: _usage.UsageLimits | None = None,     usage: _usage.RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     event_stream_handler: EventStreamHandler[AgentDepsT] | None = None, ) -> AgentRunResult[Any]:     \"\"\"Run the agent with a user prompt in async mode.      This method builds an internal agent graph (using system prompts, tools and output schemas) and then     runs the graph to completion. The result of the run is returned.      Example:     ```python     from pydantic_ai import Agent      agent = Agent('openai:gpt-4o')      async def main():         agent_run = await agent.run('What is the capital of France?')         print(agent_run.output)         #> The capital of France is Paris.     ```      Args:         user_prompt: User input to start/continue the conversation.         output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no             output validators since output validators would expect an argument that matches the agent's output type.         message_history: History of the conversation so far.         deferred_tool_results: Optional results for deferred tool calls in the message history.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.         event_stream_handler: Optional handler for events from the model's streaming response and the agent's execution of tools to use for this run.         builtin_tools: Optional additional builtin tools for this run.      Returns:         The result of the run.     \"\"\"     if infer_name and self.name is None:         self._infer_name(inspect.currentframe())      event_stream_handler = event_stream_handler or self.event_stream_handler      async with self.iter(         user_prompt=user_prompt,         output_type=output_type,         message_history=message_history,         deferred_tool_results=deferred_tool_results,         model=model,         deps=deps,         model_settings=model_settings,         usage_limits=usage_limits,         usage=usage,         toolsets=toolsets,         builtin_tools=builtin_tools,     ) as agent_run:         async for node in agent_run:             if event_stream_handler is not None and (                 self.is_model_request_node(node) or self.is_call_tools_node(node)             ):                 async with node.stream(agent_run.ctx) as stream:                     await event_stream_handler(_agent_graph.build_run_context(agent_run.ctx), stream)      assert agent_run.result is not None, 'The graph run did not finish properly'     return agent_run.result ``` |\n\n#### run\\_sync\n\n```\nrun_sync(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[OutputDataT]", "url": "https://ai.pydantic.dev/agent/index.html#abstractagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractAgent", "anchor": "abstractagent", "md_text": "run_sync(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[RunOutputDataT]\n\nrun_sync(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[Any]\n```\n\nSynchronously run the agent with a user prompt.\n\nThis is a convenience method that wraps [`self.run`](index.html#pydantic_ai.agent.AbstractAgent.run) with `loop.run_until_complete(...)`.\nYou therefore can't use this method inside async code or if there's an active event loop.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nresult_sync = agent.run_sync('What is the capital of Italy?')\nprint(result_sync.output)\n#> The capital of Italy is Rome.\n```\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` |\n| `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` |\n| `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional handler for events from the model's streaming response and the agent's execution of tools to use for this run. | `None` |\n| `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AgentRunResult[Any]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/abstract.py`", "url": "https://ai.pydantic.dev/agent/index.html#abstractagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractAgent", "anchor": "abstractagent", "md_text": "|  |  |\n| --- | --- |\n| ``` 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 ``` | ``` def run_sync(     self,     user_prompt: str | Sequence[_messages.UserContent] | None = None,     *,     output_type: OutputSpec[RunOutputDataT] | None = None,     message_history: Sequence[_messages.ModelMessage] | None = None,     deferred_tool_results: DeferredToolResults | None = None,     model: models.Model | models.KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: _usage.UsageLimits | None = None,     usage: _usage.RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     event_stream_handler: EventStreamHandler[AgentDepsT] | None = None, ) -> AgentRunResult[Any]:     \"\"\"Synchronously run the agent with a user prompt.      This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] with `loop.run_until_complete(...)`.     You therefore can't use this method inside async code or if there's an active event loop.      Example:     ```python     from pydantic_ai import Agent      agent = Agent('openai:gpt-4o')      result_sync = agent.run_sync('What is the capital of Italy?')     print(result_sync.output)     #> The capital of Italy is Rome.     ```      Args:         user_prompt: User input to start/continue the conversation.         output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no             output validators since output validators would expect an argument that matches the agent's output type.         message_history: History of the conversation so far.         deferred_tool_results: Optional results for deferred tool calls in the message history.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.         event_stream_handler: Optional handler for events from the model's streaming response and the agent's execution of tools to use for this run.         builtin_tools: Optional additional builtin tools for this run.      Returns:         The result of the run.     \"\"\"     if infer_name and self.name is None:         self._infer_name(inspect.currentframe())      return get_event_loop().run_until_complete(         self.run(             user_prompt,             output_type=output_type,             message_history=message_history,             deferred_tool_results=deferred_tool_results,             model=model,             deps=deps,             model_settings=model_settings,             usage_limits=usage_limits,             usage=usage,             infer_name=False,             toolsets=toolsets,             builtin_tools=builtin_tools,             event_stream_handler=event_stream_handler,         )     ) ``` |\n\n#### run\\_stream `async`\n\n```\nrun_stream(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AbstractAsyncContextManager[\n    StreamedRunResult[AgentDepsT, OutputDataT]\n]", "url": "https://ai.pydantic.dev/agent/index.html#abstractagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractAgent", "anchor": "abstractagent", "md_text": "run_stream(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AbstractAsyncContextManager[\n    StreamedRunResult[AgentDepsT, RunOutputDataT]\n]\n\nrun_stream(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AsyncIterator[StreamedRunResult[AgentDepsT, Any]]\n```\n\nRun the agent with a user prompt in async streaming mode.\n\nThis method builds an internal agent graph (using system prompts, tools and output schemas) and then\nruns the graph until the model produces output matching the `output_type`, for example text or structured data.\nAt this point, a streaming run result object is yielded from which you can stream the output as it comes in,\nand -- once this output has completed streaming -- get the complete output, message history, and usage.\n\nAs this method will consider the first output matching the `output_type` to be the final output,\nit will stop running the agent graph and will not execute any tool calls made by the model after this \"final\" output.\nIf you want to always run the agent graph to completion and stream events and output at the same time,\nuse [`agent.run()`](index.html#pydantic_ai.agent.AbstractAgent.run) with an `event_stream_handler` or [`agent.iter()`](index.html#pydantic_ai.agent.AbstractAgent.iter) instead.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    async with agent.run_stream('What is the capital of the UK?') as response:\n        print(await response.get_output())\n        #> The capital of the UK is London.\n```\n\nParameters:", "url": "https://ai.pydantic.dev/agent/index.html#abstractagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractAgent", "anchor": "abstractagent", "md_text": "| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` |\n| `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` |\n| `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n| `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional handler for events from the model's streaming response and the agent's execution of tools to use for this run. It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager. Note that it does *not* receive any events after the final result is found. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AsyncIterator[StreamedRunResult[AgentDepsT, Any]]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/abstract.py`", "url": "https://ai.pydantic.dev/agent/index.html#abstractagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractAgent", "anchor": "abstractagent", "md_text": "|  |  |\n| --- | --- |\n| ``` 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 ``` | ``` @asynccontextmanager async def run_stream(  # noqa C901     self,     user_prompt: str | Sequence[_messages.UserContent] | None = None,     *,     output_type: OutputSpec[RunOutputDataT] | None = None,     message_history: Sequence[_messages.ModelMessage] | None = None,     deferred_tool_results: DeferredToolResults | None = None,     model: models.Model | models.KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: _usage.UsageLimits | None = None,     usage: _usage.RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     event_stream_handler: EventStreamHandler[AgentDepsT] | None = None, ) -> AsyncIterator[result.StreamedRunResult[AgentDepsT, Any]]:     \"\"\"Run the agent with a user prompt in async streaming mode.      This method builds an internal agent graph (using system prompts, tools and output schemas) and then     runs the graph until the model produces output matching the `output_type`, for example text or structured data.     At this point, a streaming run result object is yielded from which you can stream the output as it comes in,     and -- once this output has completed streaming -- get the complete output, message history, and usage.      As this method will consider the first output matching the `output_type` to be the final output,     it will stop running the agent graph and will not execute any tool calls made by the model after this \"final\" output.     If you want to always run the agent graph to completion and stream events and output at the same time,     use [`agent.run()`][pydantic_ai.agent.AbstractAgent.run] with an `event_stream_handler` or [`agent.iter()`][pydantic_ai.agent.AbstractAgent.iter] instead.      Example:     ```python     from pydantic_ai import Agent      agent = Agent('openai:gpt-4o')      async def main():         async with agent.run_stream('What is the capital of the UK?') as response:             print(await response.get_output())             #> The capital of the UK is London.     ```      Args:         user_prompt: User input to start/continue the conversation.         output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no             output validators since output validators would expect an argument that matches the agent's output type.         message_history: History of the conversation so far.         deferred_tool_results: Optional results for deferred tool calls in the message history.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.         builtin_tools: Optional additional builtin tools for this run.         event_stream_handler: Optional handler for events from the model's streaming response and the agent's execution of tools to use for this run.             It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager.             Note that it does _not_ receive any events after the final result is found.      Returns:         The result of the run.     \"\"\"     if infer_name and self.name is None:         # f_back because `asynccontextmanager` adds one frame         if frame := inspect.currentframe():  # pragma: no branch             self._infer_name(frame.f_back)      event_stream_handler = event_stream_handler or self.event_stream_handler      yielded = False     async with self.iter(         user_prompt,         output_type=output_type,         message_history=message_history,         deferred_tool_results=deferred_tool_results,         model=model,         deps=deps,         model_settings=model_settings,         usage_limits=usage_limits,         usage=usage,         infer_name=False,         toolsets=toolsets,         builtin_tools=builtin_tools,     ) as agent_run:         first_node = agent_run.next_node  # start with the first node         assert isinstance(first_node, _agent_graph.UserPromptNode)  # the first node should be a user prompt node         node = first_node         while True:             graph_ctx = agent_run.ctx             if self.is_model_request_node(node):                 async with node.stream(graph_ctx) as stream:                     final_result_event = None                      async def stream_to_final(                         stream: AgentStream,                     ) -> AsyncIterator[_messages.ModelResponseStreamEvent]:                         nonlocal final_result_event                         async for event in stream:                             yield event                             if isinstance(event, _messages.FinalResultEvent):                                 final_result_event = event                                 break                      if event_stream_handler is not None:                         await event_stream_handler(                             _agent_graph.build_run_context(graph_ctx), stream_to_final(stream)                         )                     else:                         async for _ in stream_to_final(stream):                             pass                      if final_result_event is not None:                         final_result = FinalResult(                             None, final_result_event.tool_name, final_result_event.tool_call_id                         )                         if yielded:                             raise exceptions.AgentRunError('Agent run produced final results')  # pragma: no cover                         yielded = True                          messages = graph_ctx.state.message_history.copy()                          async def on_complete() -> None:                             \"\"\"Called when the stream has completed.                              The model response will have been added to messages by now                             by `StreamedRunResult._marked_completed`.                             \"\"\"                             nonlocal final_result                             final_result = FinalResult(                                 await stream.get_output(), final_result.tool_name, final_result.tool_call_id                             )                              # When we get here, the `ModelRequestNode` has completed streaming after the final result was found.                             # When running an agent with `agent.run`, we'd then move to `CallToolsNode` to execute the tool calls and                             # find the final result.                             # We also want to execute tool calls (in case `agent.end_strategy == 'exhaustive'`) here, but                             # we don't want to use run the `CallToolsNode` logic to determine the final output, as it would be                             # wasteful and could produce a different result (e.g. when text output is followed by tool calls).                             # So we call `process_tool_calls` directly and then end the run with the found final result.                              parts: list[_messages.ModelRequestPart] = []                             async for _event in _agent_graph.process_tool_calls(                                 tool_manager=graph_ctx.deps.tool_manager,                                 tool_calls=stream.response.tool_calls,                                 tool_call_results=None,                                 final_result=final_result,                                 ctx=graph_ctx,                                 output_parts=parts,                             ):                                 pass                              # For backwards compatibility, append a new ModelRequest using the tool returns and retries                             if parts:                                 messages.append(_messages.ModelRequest(parts))                              await agent_run.next(_agent_graph.SetFinalResult(final_result))                          yield StreamedRunResult(                             messages,                             graph_ctx.deps.new_message_index,                             stream,                             on_complete,                         )                         break             elif self.is_call_tools_node(node) and event_stream_handler is not None:                 async with node.stream(agent_run.ctx) as stream:                     await event_stream_handler(_agent_graph.build_run_context(agent_run.ctx), stream)              next_node = await agent_run.next(node)             if isinstance(next_node, End) and agent_run.result is not None:                 # A final output could have been produced by the CallToolsNode rather than the ModelRequestNode,                 # if a tool function raised CallDeferred or ApprovalRequired.                 # In this case there's no response to stream, but we still let the user access the output etc as normal.                 yield StreamedRunResult(                     graph_ctx.state.message_history,                     graph_ctx.deps.new_message_index,                     run_result=agent_run.result,                 )                 yielded = True                 break             if not isinstance(next_node, _agent_graph.AgentNode):                 raise exceptions.AgentRunError(  # pragma: no cover                     'Should have produced a StreamedRunResult before getting here'                 )             node = cast(_agent_graph.AgentNode[Any, Any], next_node)      if not yielded:         raise exceptions.AgentRunError('Agent run finished without producing a final result')  # pragma: no cover ``` |", "url": "https://ai.pydantic.dev/agent/index.html#abstractagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractAgent", "anchor": "abstractagent", "md_text": "#### run\\_stream\\_events\n\n```\nrun_stream_events(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[\n    AgentStreamEvent | AgentRunResultEvent[OutputDataT]\n]\n\nrun_stream_events(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[\n    AgentStreamEvent | AgentRunResultEvent[RunOutputDataT]\n]\n\nrun_stream_events(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[\n    AgentStreamEvent | AgentRunResultEvent[Any]\n]\n```\n\nRun the agent with a user prompt in async mode and stream events from the run.\n\nThis is a convenience method that wraps [`self.run`](index.html#pydantic_ai.agent.AbstractAgent.run) and\nuses the `event_stream_handler` kwarg to get a stream of events from the run.\n\nExample:\n\n```\nfrom pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    events: list[AgentStreamEvent | AgentRunResultEvent] = []\n    async for event in agent.run_stream_events('What is the capital of France?'):\n        events.append(event)\n    print(events)\n    '''\n    [\n        PartStartEvent(index=0, part=TextPart(content='The capital of ')),\n        FinalResultEvent(tool_name=None, tool_call_id=None),\n        PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='France is Paris. ')),\n        AgentRunResultEvent(\n            result=AgentRunResult(output='The capital of France is Paris. ')\n        ),\n    ]\n    '''\n```\n\nArguments are the same as for [`self.run`](index.html#pydantic_ai.agent.AbstractAgent.run),\nexcept that `event_stream_handler` is now allowed.\n\nParameters:", "url": "https://ai.pydantic.dev/agent/index.html#abstractagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractAgent", "anchor": "abstractagent", "md_text": "| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` |\n| `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` |\n| `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AsyncIterator[AgentStreamEvent | AgentRunResultEvent[Any]]` | An async iterable of stream events `AgentStreamEvent` and finally a `AgentRunResultEvent` with the final |\n| `AsyncIterator[AgentStreamEvent | AgentRunResultEvent[Any]]` | run result. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/abstract.py`", "url": "https://ai.pydantic.dev/agent/index.html#abstractagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractAgent", "anchor": "abstractagent", "md_text": "|  |  |\n| --- | --- |\n| ``` 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 ``` | ``` def run_stream_events(     self,     user_prompt: str | Sequence[_messages.UserContent] | None = None,     *,     output_type: OutputSpec[RunOutputDataT] | None = None,     message_history: Sequence[_messages.ModelMessage] | None = None,     deferred_tool_results: DeferredToolResults | None = None,     model: models.Model | models.KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: _usage.UsageLimits | None = None,     usage: _usage.RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     builtin_tools: Sequence[AbstractBuiltinTool] | None = None, ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[Any]]:     \"\"\"Run the agent with a user prompt in async mode and stream events from the run.      This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] and     uses the `event_stream_handler` kwarg to get a stream of events from the run.      Example:     ```python     from pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent      agent = Agent('openai:gpt-4o')      async def main():         events: list[AgentStreamEvent | AgentRunResultEvent] = []         async for event in agent.run_stream_events('What is the capital of France?'):             events.append(event)         print(events)         '''         [             PartStartEvent(index=0, part=TextPart(content='The capital of ')),             FinalResultEvent(tool_name=None, tool_call_id=None),             PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='France is Paris. ')),             AgentRunResultEvent(                 result=AgentRunResult(output='The capital of France is Paris. ')             ),         ]         '''     ```      Arguments are the same as for [`self.run`][pydantic_ai.agent.AbstractAgent.run],     except that `event_stream_handler` is now allowed.      Args:         user_prompt: User input to start/continue the conversation.         output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no             output validators since output validators would expect an argument that matches the agent's output type.         message_history: History of the conversation so far.         deferred_tool_results: Optional results for deferred tool calls in the message history.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.         builtin_tools: Optional additional builtin tools for this run.      Returns:         An async iterable of stream events `AgentStreamEvent` and finally a `AgentRunResultEvent` with the final         run result.     \"\"\"     # unfortunately this hack of returning a generator rather than defining it right here is     # required to allow overloads of this method to work in python's typing system, or at least with pyright     # or at least I couldn't make it work without     return self._run_stream_events(         user_prompt,         output_type=output_type,         message_history=message_history,         deferred_tool_results=deferred_tool_results,         model=model,         deps=deps,         model_settings=model_settings,         usage_limits=usage_limits,         usage=usage,         infer_name=infer_name,         toolsets=toolsets,         builtin_tools=builtin_tools,     ) ``` |\n\n#### iter `abstractmethod` `async`", "url": "https://ai.pydantic.dev/agent/index.html#abstractagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractAgent", "anchor": "abstractagent", "md_text": "```\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AbstractAsyncContextManager[\n    AgentRun[AgentDepsT, OutputDataT]\n]\n\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AbstractAsyncContextManager[\n    AgentRun[AgentDepsT, RunOutputDataT]\n]\n\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[AgentRun[AgentDepsT, Any]]\n```\n\nA contextmanager which can be used to iterate over the agent graph's nodes as they are executed.\n\nThis method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an\n`AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are\nexecuted. This is the API to use if you want to consume the outputs coming from each LLM model response, or the\nstream of events coming from the execution of tools.\n\nThe `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,\nand the final result of the run once it has completed.\n\nFor more details, see the documentation of `AgentRun`.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    nodes = []\n    async with agent.iter('What is the capital of France?') as agent_run:\n        async for node in agent_run:\n            nodes.append(node)\n    print(nodes)\n    '''\n    [\n        UserPromptNode(\n            user_prompt='What is the capital of France?',\n            instructions_functions=[],\n            system_prompts=(),\n            system_prompt_functions=[],\n            system_prompt_dynamic_functions={},\n        ),\n        ModelRequestNode(\n            request=ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='What is the capital of France?',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            )\n        ),\n        CallToolsNode(\n            model_response=ModelResponse(\n                parts=[TextPart(content='The capital of France is Paris.')],\n                usage=RequestUsage(input_tokens=56, output_tokens=7),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            )\n        ),\n        End(data=FinalResult(output='The capital of France is Paris.')),\n    ]\n    '''\n    print(agent_run.result.output)\n    #> The capital of France is Paris.\n```\n\nParameters:", "url": "https://ai.pydantic.dev/agent/index.html#abstractagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractAgent", "anchor": "abstractagent", "md_text": "| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` |\n| `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` |\n| `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AsyncIterator[AgentRun[AgentDepsT, Any]]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/abstract.py`", "url": "https://ai.pydantic.dev/agent/index.html#abstractagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractAgent", "anchor": "abstractagent", "md_text": "|  |  |\n| --- | --- |\n| ``` 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 ``` | ``` @asynccontextmanager @abstractmethod async def iter(     self,     user_prompt: str | Sequence[_messages.UserContent] | None = None,     *,     output_type: OutputSpec[RunOutputDataT] | None = None,     message_history: Sequence[_messages.ModelMessage] | None = None,     deferred_tool_results: DeferredToolResults | None = None,     model: models.Model | models.KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: _usage.UsageLimits | None = None,     usage: _usage.RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     builtin_tools: Sequence[AbstractBuiltinTool] | None = None, ) -> AsyncIterator[AgentRun[AgentDepsT, Any]]:     \"\"\"A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.      This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an     `AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are     executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the     stream of events coming from the execution of tools.      The `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,     and the final result of the run once it has completed.      For more details, see the documentation of `AgentRun`.      Example:     ```python     from pydantic_ai import Agent      agent = Agent('openai:gpt-4o')      async def main():         nodes = []         async with agent.iter('What is the capital of France?') as agent_run:             async for node in agent_run:                 nodes.append(node)         print(nodes)         '''         [             UserPromptNode(                 user_prompt='What is the capital of France?',                 instructions_functions=[],                 system_prompts=(),                 system_prompt_functions=[],                 system_prompt_dynamic_functions={},             ),             ModelRequestNode(                 request=ModelRequest(                     parts=[                         UserPromptPart(                             content='What is the capital of France?',                             timestamp=datetime.datetime(...),                         )                     ]                 )             ),             CallToolsNode(                 model_response=ModelResponse(                     parts=[TextPart(content='The capital of France is Paris.')],                     usage=RequestUsage(input_tokens=56, output_tokens=7),                     model_name='gpt-4o',                     timestamp=datetime.datetime(...),                 )             ),             End(data=FinalResult(output='The capital of France is Paris.')),         ]         '''         print(agent_run.result.output)         #> The capital of France is Paris.     ```      Args:         user_prompt: User input to start/continue the conversation.         output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no             output validators since output validators would expect an argument that matches the agent's output type.         message_history: History of the conversation so far.         deferred_tool_results: Optional results for deferred tool calls in the message history.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.         builtin_tools: Optional additional builtin tools for this run.      Returns:         The result of the run.     \"\"\"     raise NotImplementedError     yield ``` |\n\n#### override `abstractmethod`", "url": "https://ai.pydantic.dev/agent/index.html#abstractagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractAgent", "anchor": "abstractagent", "md_text": "```\noverride(\n    *,\n    name: str | Unset = UNSET,\n    deps: AgentDepsT | Unset = UNSET,\n    model: Model | KnownModelName | str | Unset = UNSET,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | Unset\n    ) = UNSET,\n    tools: (\n        Sequence[\n            Tool[AgentDepsT]\n            | ToolFuncEither[AgentDepsT, ...]\n        ]\n        | Unset\n    ) = UNSET,\n    instructions: Instructions[AgentDepsT] | Unset = UNSET\n) -> Iterator[None]\n```\n\nContext manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.\n\nThis is particularly useful when testing.\nYou can find an example of this [here](https://ai.pydantic.dev/testing/#overriding-model-via-pytest-fixtures).\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `name` | `str | Unset` | The name to use instead of the name passed to the agent constructor and agent run. | `UNSET` |\n| `deps` | `AgentDepsT | Unset` | The dependencies to use instead of the dependencies passed to the agent run. | `UNSET` |\n| `model` | `Model | KnownModelName | str | Unset` | The model to use instead of the model passed to the agent run. | `UNSET` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | Unset` | The toolsets to use instead of the toolsets passed to the agent constructor and agent run. | `UNSET` |\n| `tools` | `Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | Unset` | The tools to use instead of the tools registered with the agent. | `UNSET` |\n| `instructions` | `Instructions[AgentDepsT] | Unset` | The instructions to use instead of the instructions registered with the agent. | `UNSET` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/abstract.py`\n\n|  |  |\n| --- | --- |\n| ``` 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 ``` | ``` @contextmanager @abstractmethod def override(     self,     *,     name: str | _utils.Unset = _utils.UNSET,     deps: AgentDepsT | _utils.Unset = _utils.UNSET,     model: models.Model | models.KnownModelName | str | _utils.Unset = _utils.UNSET,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | _utils.Unset = _utils.UNSET,     tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | _utils.Unset = _utils.UNSET,     instructions: Instructions[AgentDepsT] | _utils.Unset = _utils.UNSET, ) -> Iterator[None]:     \"\"\"Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.      This is particularly useful when testing.     You can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).      Args:         name: The name to use instead of the name passed to the agent constructor and agent run.         deps: The dependencies to use instead of the dependencies passed to the agent run.         model: The model to use instead of the model passed to the agent run.         toolsets: The toolsets to use instead of the toolsets passed to the agent constructor and agent run.         tools: The tools to use instead of the tools registered with the agent.         instructions: The instructions to use instead of the instructions registered with the agent.     \"\"\"     raise NotImplementedError     yield ``` |\n\n#### sequential\\_tool\\_calls `staticmethod`\n\n```\nsequential_tool_calls() -> Iterator[None]\n```\n\nRun tool calls sequentially during the context.\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/abstract.py`\n\n|  |  |\n| --- | --- |\n| ``` 937 938 939 940 941 942 ``` | ``` @staticmethod @contextmanager def sequential_tool_calls() -> Iterator[None]:     \"\"\"Run tool calls sequentially during the context.\"\"\"     with ToolManager.sequential_tool_calls():         yield ``` |\n\n#### is\\_model\\_request\\_node `staticmethod`\n\n```\nis_model_request_node(\n    node: AgentNode[T, S] | End[FinalResult[S]],\n) -> TypeIs[ModelRequestNode[T, S]]\n```\n\nCheck if the node is a `ModelRequestNode`, narrowing the type if it is.\n\nThis method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/abstract.py`", "url": "https://ai.pydantic.dev/agent/index.html#abstractagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractAgent", "anchor": "abstractagent", "md_text": "|  |  |\n| --- | --- |\n| ``` 944 945 946 947 948 949 950 951 952 ``` | ``` @staticmethod def is_model_request_node(     node: _agent_graph.AgentNode[T, S] | End[result.FinalResult[S]], ) -> TypeIs[_agent_graph.ModelRequestNode[T, S]]:     \"\"\"Check if the node is a `ModelRequestNode`, narrowing the type if it is.      This method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.     \"\"\"     return isinstance(node, _agent_graph.ModelRequestNode) ``` |\n\n#### is\\_call\\_tools\\_node `staticmethod`\n\n```\nis_call_tools_node(\n    node: AgentNode[T, S] | End[FinalResult[S]],\n) -> TypeIs[CallToolsNode[T, S]]\n```\n\nCheck if the node is a `CallToolsNode`, narrowing the type if it is.\n\nThis method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/abstract.py`\n\n|  |  |\n| --- | --- |\n| ``` 954 955 956 957 958 959 960 961 962 ``` | ``` @staticmethod def is_call_tools_node(     node: _agent_graph.AgentNode[T, S] | End[result.FinalResult[S]], ) -> TypeIs[_agent_graph.CallToolsNode[T, S]]:     \"\"\"Check if the node is a `CallToolsNode`, narrowing the type if it is.      This method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.     \"\"\"     return isinstance(node, _agent_graph.CallToolsNode) ``` |\n\n#### is\\_user\\_prompt\\_node `staticmethod`\n\n```\nis_user_prompt_node(\n    node: AgentNode[T, S] | End[FinalResult[S]],\n) -> TypeIs[UserPromptNode[T, S]]\n```\n\nCheck if the node is a `UserPromptNode`, narrowing the type if it is.\n\nThis method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/abstract.py`\n\n|  |  |\n| --- | --- |\n| ``` 964 965 966 967 968 969 970 971 972 ``` | ``` @staticmethod def is_user_prompt_node(     node: _agent_graph.AgentNode[T, S] | End[result.FinalResult[S]], ) -> TypeIs[_agent_graph.UserPromptNode[T, S]]:     \"\"\"Check if the node is a `UserPromptNode`, narrowing the type if it is.      This method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.     \"\"\"     return isinstance(node, _agent_graph.UserPromptNode) ``` |\n\n#### is\\_end\\_node `staticmethod`\n\n```\nis_end_node(\n    node: AgentNode[T, S] | End[FinalResult[S]],\n) -> TypeIs[End[FinalResult[S]]]\n```\n\nCheck if the node is a `End`, narrowing the type if it is.\n\nThis method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/abstract.py`\n\n|  |  |\n| --- | --- |\n| ``` 974 975 976 977 978 979 980 981 982 ``` | ``` @staticmethod def is_end_node(     node: _agent_graph.AgentNode[T, S] | End[result.FinalResult[S]], ) -> TypeIs[End[result.FinalResult[S]]]:     \"\"\"Check if the node is a `End`, narrowing the type if it is.      This method preserves the generic parameters while narrowing the type, unlike a direct call to `isinstance`.     \"\"\"     return isinstance(node, End) ``` |\n\n#### to\\_ag\\_ui\n\n```\nto_ag_ui(\n    *,\n    output_type: OutputSpec[OutputDataT] | None = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    debug: bool = False,\n    routes: Sequence[BaseRoute] | None = None,\n    middleware: Sequence[Middleware] | None = None,\n    exception_handlers: (\n        Mapping[Any, ExceptionHandler] | None\n    ) = None,\n    on_startup: Sequence[Callable[[], Any]] | None = None,\n    on_shutdown: Sequence[Callable[[], Any]] | None = None,\n    lifespan: (\n        Lifespan[AGUIApp[AgentDepsT, OutputDataT]] | None\n    ) = None\n) -> AGUIApp[AgentDepsT, OutputDataT]\n```\n\nReturns an ASGI application that handles every AG-UI request by running the agent.", "url": "https://ai.pydantic.dev/agent/index.html#abstractagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractAgent", "anchor": "abstractagent", "md_text": "Note that the `deps` will be the same for each request, with the exception of the AG-UI state that's\ninjected into the `state` field of a `deps` object that implements the [`StateHandler`](../ag_ui/index.html#pydantic_ai.ag_ui.StateHandler) protocol.\nTo provide different `deps` for each request (e.g. based on the authenticated user),\nuse [`pydantic_ai.ag_ui.run_ag_ui`](../ag_ui/index.html#pydantic_ai.ag_ui.run_ag_ui) or\n[`pydantic_ai.ag_ui.handle_ag_ui_request`](../ag_ui/index.html#pydantic_ai.ag_ui.handle_ag_ui_request) instead.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\napp = agent.to_ag_ui()\n```\n\nThe `app` is an ASGI application that can be used with any ASGI server.\n\nTo run the application, you can use the following command:\n\n```\nuvicorn app:app --host 0.0.0.0 --port 8000\n```\n\nSee [AG-UI docs](https://ai.pydantic.dev/ag-ui/) for more information.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `output_type` | `OutputSpec[OutputDataT] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `debug` | `bool` | Boolean indicating if debug tracebacks should be returned on errors. | `False` |\n| `routes` | `Sequence[BaseRoute] | None` | A list of routes to serve incoming HTTP and WebSocket requests. | `None` |\n| `middleware` | `Sequence[Middleware] | None` | A list of middleware to run for every request. A starlette application will always automatically include two middleware classes. `ServerErrorMiddleware` is added as the very outermost middleware, to handle any uncaught errors occurring anywhere in the entire stack. `ExceptionMiddleware` is added as the very innermost middleware, to deal with handled exception cases occurring in the routing or endpoints. | `None` |\n| `exception_handlers` | `Mapping[Any, ExceptionHandler] | None` | A mapping of either integer status codes, or exception class types onto callables which handle the exceptions. Exception handler callables should be of the form `handler(request, exc) -> response` and may be either standard functions, or async functions. | `None` |\n| `on_startup` | `Sequence[Callable[[], Any]] | None` | A list of callables to run on application startup. Startup handler callables do not take any arguments, and may be either standard functions, or async functions. | `None` |\n| `on_shutdown` | `Sequence[Callable[[], Any]] | None` | A list of callables to run on application shutdown. Shutdown handler callables do not take any arguments, and may be either standard functions, or async functions. | `None` |\n| `lifespan` | `Lifespan[AGUIApp[AgentDepsT, OutputDataT]] | None` | A lifespan context function, which can be used to perform startup and shutdown tasks. This is a newer style that replaces the `on_startup` and `on_shutdown` handlers. Use one or the other, not both. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AGUIApp[AgentDepsT, OutputDataT]` | An ASGI application for running Pydantic AI agents with AG-UI protocol support. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/abstract.py`", "url": "https://ai.pydantic.dev/agent/index.html#abstractagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractAgent", "anchor": "abstractagent", "md_text": "|  |  |\n| --- | --- |\n| ```  992  993  994  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 ``` | ``` def to_ag_ui(     self,     *,     # Agent.iter parameters     output_type: OutputSpec[OutputDataT] | None = None,     model: models.Model | models.KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: UsageLimits | None = None,     usage: RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     # Starlette     debug: bool = False,     routes: Sequence[BaseRoute] | None = None,     middleware: Sequence[Middleware] | None = None,     exception_handlers: Mapping[Any, ExceptionHandler] | None = None,     on_startup: Sequence[Callable[[], Any]] | None = None,     on_shutdown: Sequence[Callable[[], Any]] | None = None,     lifespan: Lifespan[AGUIApp[AgentDepsT, OutputDataT]] | None = None, ) -> AGUIApp[AgentDepsT, OutputDataT]:     \"\"\"Returns an ASGI application that handles every AG-UI request by running the agent.      Note that the `deps` will be the same for each request, with the exception of the AG-UI state that's     injected into the `state` field of a `deps` object that implements the [`StateHandler`][pydantic_ai.ag_ui.StateHandler] protocol.     To provide different `deps` for each request (e.g. based on the authenticated user),     use [`pydantic_ai.ag_ui.run_ag_ui`][pydantic_ai.ag_ui.run_ag_ui] or     [`pydantic_ai.ag_ui.handle_ag_ui_request`][pydantic_ai.ag_ui.handle_ag_ui_request] instead.      Example:     ```python     from pydantic_ai import Agent      agent = Agent('openai:gpt-4o')     app = agent.to_ag_ui()     ```      The `app` is an ASGI application that can be used with any ASGI server.      To run the application, you can use the following command:      ```bash     uvicorn app:app --host 0.0.0.0 --port 8000     ```      See [AG-UI docs](../ag-ui.md) for more information.      Args:         output_type: Custom output type to use for this run, `output_type` may only be used if the agent has             no output validators since output validators would expect an argument that matches the agent's             output type.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.          debug: Boolean indicating if debug tracebacks should be returned on errors.         routes: A list of routes to serve incoming HTTP and WebSocket requests.         middleware: A list of middleware to run for every request. A starlette application will always             automatically include two middleware classes. `ServerErrorMiddleware` is added as the very             outermost middleware, to handle any uncaught errors occurring anywhere in the entire stack.             `ExceptionMiddleware` is added as the very innermost middleware, to deal with handled             exception cases occurring in the routing or endpoints.         exception_handlers: A mapping of either integer status codes, or exception class types onto             callables which handle the exceptions. Exception handler callables should be of the form             `handler(request, exc) -> response` and may be either standard functions, or async functions.         on_startup: A list of callables to run on application startup. Startup handler callables do not             take any arguments, and may be either standard functions, or async functions.         on_shutdown: A list of callables to run on application shutdown. Shutdown handler callables do             not take any arguments, and may be either standard functions, or async functions.         lifespan: A lifespan context function, which can be used to perform startup and shutdown tasks.             This is a newer style that replaces the `on_startup` and `on_shutdown` handlers. Use one or             the other, not both.      Returns:         An ASGI application for running Pydantic AI agents with AG-UI protocol support.     \"\"\"     from ..ag_ui import AGUIApp      return AGUIApp(         agent=self,         # Agent.iter parameters         output_type=output_type,         model=model,         deps=deps,         model_settings=model_settings,         usage_limits=usage_limits,         usage=usage,         infer_name=infer_name,         toolsets=toolsets,         # Starlette         debug=debug,         routes=routes,         middleware=middleware,         exception_handlers=exception_handlers,         on_startup=on_startup,         on_shutdown=on_shutdown,         lifespan=lifespan,     ) ``` |", "url": "https://ai.pydantic.dev/agent/index.html#abstractagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractAgent", "anchor": "abstractagent", "md_text": "#### to\\_a2a\n\n```\nto_a2a(\n    *,\n    storage: Storage | None = None,\n    broker: Broker | None = None,\n    name: str | None = None,\n    url: str = \"http://localhost:8000\",\n    version: str = \"1.0.0\",\n    description: str | None = None,\n    provider: AgentProvider | None = None,\n    skills: list[Skill] | None = None,\n    debug: bool = False,\n    routes: Sequence[Route] | None = None,\n    middleware: Sequence[Middleware] | None = None,\n    exception_handlers: (\n        dict[Any, ExceptionHandler] | None\n    ) = None,\n    lifespan: Lifespan[FastA2A] | None = None\n) -> FastA2A\n```\n\nConvert the agent to a FastA2A application.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\napp = agent.to_a2a()\n```\n\nThe `app` is an ASGI application that can be used with any ASGI server.\n\nTo run the application, you can use the following command:\n\n```\nuvicorn app:app --host 0.0.0.0 --port 8000\n```\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/abstract.py`\n\n|  |  |\n| --- | --- |\n| ``` 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 ``` | ``` def to_a2a(     self,     *,     storage: Storage | None = None,     broker: Broker | None = None,     # Agent card     name: str | None = None,     url: str = 'http://localhost:8000',     version: str = '1.0.0',     description: str | None = None,     provider: AgentProvider | None = None,     skills: list[Skill] | None = None,     # Starlette     debug: bool = False,     routes: Sequence[Route] | None = None,     middleware: Sequence[Middleware] | None = None,     exception_handlers: dict[Any, ExceptionHandler] | None = None,     lifespan: Lifespan[FastA2A] | None = None, ) -> FastA2A:     \"\"\"Convert the agent to a FastA2A application.      Example:     ```python     from pydantic_ai import Agent      agent = Agent('openai:gpt-4o')     app = agent.to_a2a()     ```      The `app` is an ASGI application that can be used with any ASGI server.      To run the application, you can use the following command:      ```bash     uvicorn app:app --host 0.0.0.0 --port 8000     ```     \"\"\"     from .._a2a import agent_to_a2a      return agent_to_a2a(         self,         storage=storage,         broker=broker,         name=name,         url=url,         version=version,         description=description,         provider=provider,         skills=skills,         debug=debug,         routes=routes,         middleware=middleware,         exception_handlers=exception_handlers,         lifespan=lifespan,     ) ``` |\n\n#### to\\_cli `async`\n\n```\nto_cli(\n    deps: AgentDepsT = None,\n    prog_name: str = \"pydantic-ai\",\n    message_history: Sequence[ModelMessage] | None = None,\n) -> None\n```\n\nRun the agent in a CLI chat interface.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `deps` | `AgentDepsT` | The dependencies to pass to the agent. | `None` |\n| `prog_name` | `str` | The name of the program to use for the CLI. Defaults to 'pydantic-ai'. | `'pydantic-ai'` |\n| `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` |\n\nExample:\n\nagent\\_to\\_cli.py\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o', instructions='You always respond in Italian.')\n\nasync def main():\n    await agent.to_cli()\n```\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/abstract.py`", "url": "https://ai.pydantic.dev/agent/index.html#abstractagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractAgent", "anchor": "abstractagent", "md_text": "|  |  |\n| --- | --- |\n| ``` 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 ``` | ``` async def to_cli(     self: Self,     deps: AgentDepsT = None,     prog_name: str = 'pydantic-ai',     message_history: Sequence[_messages.ModelMessage] | None = None, ) -> None:     \"\"\"Run the agent in a CLI chat interface.      Args:         deps: The dependencies to pass to the agent.         prog_name: The name of the program to use for the CLI. Defaults to 'pydantic-ai'.         message_history: History of the conversation so far.      Example:     ```python {title=\"agent_to_cli.py\" test=\"skip\"}     from pydantic_ai import Agent      agent = Agent('openai:gpt-4o', instructions='You always respond in Italian.')      async def main():         await agent.to_cli()     ```     \"\"\"     from rich.console import Console      from pydantic_ai._cli import run_chat      await run_chat(         stream=True,         agent=self,         deps=deps,         console=Console(),         code_theme='monokai',         prog_name=prog_name,         message_history=message_history,     ) ``` |\n\n#### to\\_cli\\_sync\n\n```\nto_cli_sync(\n    deps: AgentDepsT = None,\n    prog_name: str = \"pydantic-ai\",\n    message_history: Sequence[ModelMessage] | None = None,\n) -> None\n```\n\nRun the agent in a CLI chat interface with the non-async interface.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `deps` | `AgentDepsT` | The dependencies to pass to the agent. | `None` |\n| `prog_name` | `str` | The name of the program to use for the CLI. Defaults to 'pydantic-ai'. | `'pydantic-ai'` |\n| `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` |\n\nagent\\_to\\_cli\\_sync.py\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o', instructions='You always respond in Italian.')\nagent.to_cli_sync()\nagent.to_cli_sync(prog_name='assistant')\n```\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/abstract.py`\n\n|  |  |\n| --- | --- |\n| ``` 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 ``` | ``` def to_cli_sync(     self: Self,     deps: AgentDepsT = None,     prog_name: str = 'pydantic-ai',     message_history: Sequence[_messages.ModelMessage] | None = None, ) -> None:     \"\"\"Run the agent in a CLI chat interface with the non-async interface.      Args:         deps: The dependencies to pass to the agent.         prog_name: The name of the program to use for the CLI. Defaults to 'pydantic-ai'.         message_history: History of the conversation so far.      ```python {title=\"agent_to_cli_sync.py\" test=\"skip\"}     from pydantic_ai import Agent      agent = Agent('openai:gpt-4o', instructions='You always respond in Italian.')     agent.to_cli_sync()     agent.to_cli_sync(prog_name='assistant')     ```     \"\"\"     return get_event_loop().run_until_complete(         self.to_cli(deps=deps, prog_name=prog_name, message_history=message_history)     ) ``` |", "url": "https://ai.pydantic.dev/agent/index.html#abstractagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "WrapperAgent", "anchor": "wrapperagent", "md_text": "Bases: `AbstractAgent[AgentDepsT, OutputDataT]`\n\nAgent which wraps another agent.\n\nDoes nothing on its own, used as a base class.\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/wrapper.py`", "url": "https://ai.pydantic.dev/agent/index.html#wrapperagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "WrapperAgent", "anchor": "wrapperagent", "md_text": "|  |  |\n| --- | --- |\n| ```  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 ``` | ``` class WrapperAgent(AbstractAgent[AgentDepsT, OutputDataT]):     \"\"\"Agent which wraps another agent.      Does nothing on its own, used as a base class.     \"\"\"      def __init__(self, wrapped: AbstractAgent[AgentDepsT, OutputDataT]):         self.wrapped = wrapped      @property     def model(self) -> models.Model | models.KnownModelName | str | None:         return self.wrapped.model      @property     def name(self) -> str | None:         return self.wrapped.name      @name.setter     def name(self, value: str | None) -> None:         self.wrapped.name = value      @property     def deps_type(self) -> type:         return self.wrapped.deps_type      @property     def output_type(self) -> OutputSpec[OutputDataT]:         return self.wrapped.output_type      @property     def event_stream_handler(self) -> EventStreamHandler[AgentDepsT] | None:         return self.wrapped.event_stream_handler      @property     def toolsets(self) -> Sequence[AbstractToolset[AgentDepsT]]:         return self.wrapped.toolsets      async def __aenter__(self) -> AbstractAgent[AgentDepsT, OutputDataT]:         return await self.wrapped.__aenter__()      async def __aexit__(self, *args: Any) -> bool | None:         return await self.wrapped.__aexit__(*args)      @overload     def iter(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     ) -> AbstractAsyncContextManager[AgentRun[AgentDepsT, OutputDataT]]: ...      @overload     def iter(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT],         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     ) -> AbstractAsyncContextManager[AgentRun[AgentDepsT, RunOutputDataT]]: ...      @asynccontextmanager     async def iter(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT] | None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     ) -> AsyncIterator[AgentRun[AgentDepsT, Any]]:         \"\"\"A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.          This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an         `AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are         executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the         stream of events coming from the execution of tools.          The `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,         and the final result of the run once it has completed.          For more details, see the documentation of `AgentRun`.          Example:         ```python         from pydantic_ai import Agent          agent = Agent('openai:gpt-4o')          async def main():             nodes = []             async with agent.iter('What is the capital of France?') as agent_run:                 async for node in agent_run:                     nodes.append(node)             print(nodes)             '''             [                 UserPromptNode(                     user_prompt='What is the capital of France?',                     instructions_functions=[],                     system_prompts=(),                     system_prompt_functions=[],                     system_prompt_dynamic_functions={},                 ),                 ModelRequestNode(                     request=ModelRequest(                         parts=[                             UserPromptPart(                                 content='What is the capital of France?',                                 timestamp=datetime.datetime(...),                             )                         ]                     )                 ),                 CallToolsNode(                     model_response=ModelResponse(                         parts=[TextPart(content='The capital of France is Paris.')],                         usage=RequestUsage(input_tokens=56, output_tokens=7),                         model_name='gpt-4o',                         timestamp=datetime.datetime(...),                     )                 ),                 End(data=FinalResult(output='The capital of France is Paris.')),             ]             '''             print(agent_run.result.output)             #> The capital of France is Paris.         ```          Args:             user_prompt: User input to start/continue the conversation.             output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no                 output validators since output validators would expect an argument that matches the agent's output type.             message_history: History of the conversation so far.             deferred_tool_results: Optional results for deferred tool calls in the message history.             model: Optional model to use for this run, required if `model` was not set when creating the agent.             deps: Optional dependencies to use for this run.             model_settings: Optional settings to use for this model's request.             usage_limits: Optional limits on model request count or token usage.             usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.             infer_name: Whether to try to infer the agent name from the call frame if it's not set.             toolsets: Optional additional toolsets for this run.             builtin_tools: Optional additional builtin tools for this run.          Returns:             The result of the run.         \"\"\"         async with self.wrapped.iter(             user_prompt=user_prompt,             output_type=output_type,             message_history=message_history,             deferred_tool_results=deferred_tool_results,             model=model,             deps=deps,             model_settings=model_settings,             usage_limits=usage_limits,             usage=usage,             infer_name=infer_name,             toolsets=toolsets,             builtin_tools=builtin_tools,         ) as run:             yield run      @contextmanager     def override(         self,         *,         name: str | _utils.Unset = _utils.UNSET,         deps: AgentDepsT | _utils.Unset = _utils.UNSET,         model: models.Model | models.KnownModelName | str | _utils.Unset = _utils.UNSET,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | _utils.Unset = _utils.UNSET,         tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | _utils.Unset = _utils.UNSET,         instructions: Instructions[AgentDepsT] | _utils.Unset = _utils.UNSET,     ) -> Iterator[None]:         \"\"\"Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.          This is particularly useful when testing.         You can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).          Args:             name: The name to use instead of the name passed to the agent constructor and agent run.             deps: The dependencies to use instead of the dependencies passed to the agent run.             model: The model to use instead of the model passed to the agent run.             toolsets: The toolsets to use instead of the toolsets passed to the agent constructor and agent run.             tools: The tools to use instead of the tools registered with the agent.             instructions: The instructions to use instead of the instructions registered with the agent.         \"\"\"         with self.wrapped.override(             name=name,             deps=deps,             model=model,             toolsets=toolsets,             tools=tools,             instructions=instructions,         ):             yield ``` |", "url": "https://ai.pydantic.dev/agent/index.html#wrapperagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "WrapperAgent", "anchor": "wrapperagent", "md_text": "#### iter `async`\n\n```\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AbstractAsyncContextManager[\n    AgentRun[AgentDepsT, OutputDataT]\n]\n\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AbstractAsyncContextManager[\n    AgentRun[AgentDepsT, RunOutputDataT]\n]\n\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[AgentRun[AgentDepsT, Any]]\n```\n\nA contextmanager which can be used to iterate over the agent graph's nodes as they are executed.\n\nThis method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an\n`AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are\nexecuted. This is the API to use if you want to consume the outputs coming from each LLM model response, or the\nstream of events coming from the execution of tools.\n\nThe `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,\nand the final result of the run once it has completed.\n\nFor more details, see the documentation of `AgentRun`.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    nodes = []\n    async with agent.iter('What is the capital of France?') as agent_run:\n        async for node in agent_run:\n            nodes.append(node)\n    print(nodes)\n    '''\n    [\n        UserPromptNode(\n            user_prompt='What is the capital of France?',\n            instructions_functions=[],\n            system_prompts=(),\n            system_prompt_functions=[],\n            system_prompt_dynamic_functions={},\n        ),\n        ModelRequestNode(\n            request=ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='What is the capital of France?',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            )\n        ),\n        CallToolsNode(\n            model_response=ModelResponse(\n                parts=[TextPart(content='The capital of France is Paris.')],\n                usage=RequestUsage(input_tokens=56, output_tokens=7),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            )\n        ),\n        End(data=FinalResult(output='The capital of France is Paris.')),\n    ]\n    '''\n    print(agent_run.result.output)\n    #> The capital of France is Paris.\n```\n\nParameters:", "url": "https://ai.pydantic.dev/agent/index.html#wrapperagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "WrapperAgent", "anchor": "wrapperagent", "md_text": "| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` |\n| `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` |\n| `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AsyncIterator[AgentRun[AgentDepsT, Any]]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/wrapper.py`", "url": "https://ai.pydantic.dev/agent/index.html#wrapperagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "WrapperAgent", "anchor": "wrapperagent", "md_text": "|  |  |\n| --- | --- |\n| ``` 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 ``` | ``` @asynccontextmanager async def iter(     self,     user_prompt: str | Sequence[_messages.UserContent] | None = None,     *,     output_type: OutputSpec[RunOutputDataT] | None = None,     message_history: Sequence[_messages.ModelMessage] | None = None,     deferred_tool_results: DeferredToolResults | None = None,     model: models.Model | models.KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: _usage.UsageLimits | None = None,     usage: _usage.RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     builtin_tools: Sequence[AbstractBuiltinTool] | None = None, ) -> AsyncIterator[AgentRun[AgentDepsT, Any]]:     \"\"\"A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.      This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an     `AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are     executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the     stream of events coming from the execution of tools.      The `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,     and the final result of the run once it has completed.      For more details, see the documentation of `AgentRun`.      Example:     ```python     from pydantic_ai import Agent      agent = Agent('openai:gpt-4o')      async def main():         nodes = []         async with agent.iter('What is the capital of France?') as agent_run:             async for node in agent_run:                 nodes.append(node)         print(nodes)         '''         [             UserPromptNode(                 user_prompt='What is the capital of France?',                 instructions_functions=[],                 system_prompts=(),                 system_prompt_functions=[],                 system_prompt_dynamic_functions={},             ),             ModelRequestNode(                 request=ModelRequest(                     parts=[                         UserPromptPart(                             content='What is the capital of France?',                             timestamp=datetime.datetime(...),                         )                     ]                 )             ),             CallToolsNode(                 model_response=ModelResponse(                     parts=[TextPart(content='The capital of France is Paris.')],                     usage=RequestUsage(input_tokens=56, output_tokens=7),                     model_name='gpt-4o',                     timestamp=datetime.datetime(...),                 )             ),             End(data=FinalResult(output='The capital of France is Paris.')),         ]         '''         print(agent_run.result.output)         #> The capital of France is Paris.     ```      Args:         user_prompt: User input to start/continue the conversation.         output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no             output validators since output validators would expect an argument that matches the agent's output type.         message_history: History of the conversation so far.         deferred_tool_results: Optional results for deferred tool calls in the message history.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.         builtin_tools: Optional additional builtin tools for this run.      Returns:         The result of the run.     \"\"\"     async with self.wrapped.iter(         user_prompt=user_prompt,         output_type=output_type,         message_history=message_history,         deferred_tool_results=deferred_tool_results,         model=model,         deps=deps,         model_settings=model_settings,         usage_limits=usage_limits,         usage=usage,         infer_name=infer_name,         toolsets=toolsets,         builtin_tools=builtin_tools,     ) as run:         yield run ``` |\n\n#### override", "url": "https://ai.pydantic.dev/agent/index.html#wrapperagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "WrapperAgent", "anchor": "wrapperagent", "md_text": "```\noverride(\n    *,\n    name: str | Unset = UNSET,\n    deps: AgentDepsT | Unset = UNSET,\n    model: Model | KnownModelName | str | Unset = UNSET,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | Unset\n    ) = UNSET,\n    tools: (\n        Sequence[\n            Tool[AgentDepsT]\n            | ToolFuncEither[AgentDepsT, ...]\n        ]\n        | Unset\n    ) = UNSET,\n    instructions: Instructions[AgentDepsT] | Unset = UNSET\n) -> Iterator[None]\n```\n\nContext manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.\n\nThis is particularly useful when testing.\nYou can find an example of this [here](https://ai.pydantic.dev/testing/#overriding-model-via-pytest-fixtures).\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `name` | `str | Unset` | The name to use instead of the name passed to the agent constructor and agent run. | `UNSET` |\n| `deps` | `AgentDepsT | Unset` | The dependencies to use instead of the dependencies passed to the agent run. | `UNSET` |\n| `model` | `Model | KnownModelName | str | Unset` | The model to use instead of the model passed to the agent run. | `UNSET` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | Unset` | The toolsets to use instead of the toolsets passed to the agent constructor and agent run. | `UNSET` |\n| `tools` | `Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | Unset` | The tools to use instead of the tools registered with the agent. | `UNSET` |\n| `instructions` | `Instructions[AgentDepsT] | Unset` | The instructions to use instead of the instructions registered with the agent. | `UNSET` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/agent/wrapper.py`\n\n|  |  |\n| --- | --- |\n| ``` 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 ``` | ``` @contextmanager def override(     self,     *,     name: str | _utils.Unset = _utils.UNSET,     deps: AgentDepsT | _utils.Unset = _utils.UNSET,     model: models.Model | models.KnownModelName | str | _utils.Unset = _utils.UNSET,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | _utils.Unset = _utils.UNSET,     tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | _utils.Unset = _utils.UNSET,     instructions: Instructions[AgentDepsT] | _utils.Unset = _utils.UNSET, ) -> Iterator[None]:     \"\"\"Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.      This is particularly useful when testing.     You can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).      Args:         name: The name to use instead of the name passed to the agent constructor and agent run.         deps: The dependencies to use instead of the dependencies passed to the agent run.         model: The model to use instead of the model passed to the agent run.         toolsets: The toolsets to use instead of the toolsets passed to the agent constructor and agent run.         tools: The tools to use instead of the tools registered with the agent.         instructions: The instructions to use instead of the instructions registered with the agent.     \"\"\"     with self.wrapped.override(         name=name,         deps=deps,         model=model,         toolsets=toolsets,         tools=tools,         instructions=instructions,     ):         yield ``` |", "url": "https://ai.pydantic.dev/agent/index.html#wrapperagent", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AgentRun `dataclass`", "anchor": "agentrun-dataclass", "md_text": "Bases: `Generic[AgentDepsT, OutputDataT]`\n\nA stateful, async-iterable run of an [`Agent`](index.html#pydantic_ai.agent.Agent).\n\nYou generally obtain an `AgentRun` instance by calling `async with my_agent.iter(...) as agent_run:`.\n\nOnce you have an instance, you can use it to iterate through the run's nodes as they execute. When an\n[`End`](../pydantic_graph/nodes/index.html#pydantic_graph.nodes.End) is reached, the run finishes and [`result`](index.html#pydantic_ai.agent.AgentRun.result)\nbecomes available.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    nodes = []\n    # Iterate through the run, recording each node along the way:\n    async with agent.iter('What is the capital of France?') as agent_run:\n        async for node in agent_run:\n            nodes.append(node)\n    print(nodes)\n    '''\n    [\n        UserPromptNode(\n            user_prompt='What is the capital of France?',\n            instructions_functions=[],\n            system_prompts=(),\n            system_prompt_functions=[],\n            system_prompt_dynamic_functions={},\n        ),\n        ModelRequestNode(\n            request=ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='What is the capital of France?',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            )\n        ),\n        CallToolsNode(\n            model_response=ModelResponse(\n                parts=[TextPart(content='The capital of France is Paris.')],\n                usage=RequestUsage(input_tokens=56, output_tokens=7),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            )\n        ),\n        End(data=FinalResult(output='The capital of France is Paris.')),\n    ]\n    '''\n    print(agent_run.result.output)\n    #> The capital of France is Paris.\n```\n\nYou can also manually drive the iteration using the [`next`](index.html#pydantic_ai.agent.AgentRun.next) method for\nmore granular control.\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`", "url": "https://ai.pydantic.dev/agent/index.html#agentrun-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AgentRun `dataclass`", "anchor": "agentrun-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ```  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 ``` | ``` @dataclasses.dataclass(repr=False) class AgentRun(Generic[AgentDepsT, OutputDataT]):     \"\"\"A stateful, async-iterable run of an [`Agent`][pydantic_ai.agent.Agent].      You generally obtain an `AgentRun` instance by calling `async with my_agent.iter(...) as agent_run:`.      Once you have an instance, you can use it to iterate through the run's nodes as they execute. When an     [`End`][pydantic_graph.nodes.End] is reached, the run finishes and [`result`][pydantic_ai.agent.AgentRun.result]     becomes available.      Example:     ```python     from pydantic_ai import Agent      agent = Agent('openai:gpt-4o')      async def main():         nodes = []         # Iterate through the run, recording each node along the way:         async with agent.iter('What is the capital of France?') as agent_run:             async for node in agent_run:                 nodes.append(node)         print(nodes)         '''         [             UserPromptNode(                 user_prompt='What is the capital of France?',                 instructions_functions=[],                 system_prompts=(),                 system_prompt_functions=[],                 system_prompt_dynamic_functions={},             ),             ModelRequestNode(                 request=ModelRequest(                     parts=[                         UserPromptPart(                             content='What is the capital of France?',                             timestamp=datetime.datetime(...),                         )                     ]                 )             ),             CallToolsNode(                 model_response=ModelResponse(                     parts=[TextPart(content='The capital of France is Paris.')],                     usage=RequestUsage(input_tokens=56, output_tokens=7),                     model_name='gpt-4o',                     timestamp=datetime.datetime(...),                 )             ),             End(data=FinalResult(output='The capital of France is Paris.')),         ]         '''         print(agent_run.result.output)         #> The capital of France is Paris.     ```      You can also manually drive the iteration using the [`next`][pydantic_ai.agent.AgentRun.next] method for     more granular control.     \"\"\"      _graph_run: GraphRun[         _agent_graph.GraphAgentState, _agent_graph.GraphAgentDeps[AgentDepsT, Any], FinalResult[OutputDataT]     ]      @overload     def _traceparent(self, *, required: Literal[False]) -> str | None: ...     @overload     def _traceparent(self) -> str: ...     def _traceparent(self, *, required: bool = True) -> str | None:         traceparent = self._graph_run._traceparent(required=False)  # type: ignore[reportPrivateUsage]         if traceparent is None and required:  # pragma: no cover             raise AttributeError('No span was created for this agent run')         return traceparent      @property     def ctx(self) -> GraphRunContext[_agent_graph.GraphAgentState, _agent_graph.GraphAgentDeps[AgentDepsT, Any]]:         \"\"\"The current context of the agent run.\"\"\"         return GraphRunContext[_agent_graph.GraphAgentState, _agent_graph.GraphAgentDeps[AgentDepsT, Any]](             state=self._graph_run.state, deps=self._graph_run.deps         )      @property     def next_node(         self,     ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:         \"\"\"The next node that will be run in the agent graph.          This is the next node that will be used during async iteration, or if a node is not passed to `self.next(...)`.         \"\"\"         task = self._graph_run.next_task         return self._task_to_node(task)      @property     def result(self) -> AgentRunResult[OutputDataT] | None:         \"\"\"The final result of the run if it has ended, otherwise `None`.          Once the run returns an [`End`][pydantic_graph.nodes.End] node, `result` is populated         with an [`AgentRunResult`][pydantic_ai.agent.AgentRunResult].         \"\"\"         graph_run_output = self._graph_run.output         if graph_run_output is None:             return None         return AgentRunResult(             graph_run_output.output,             graph_run_output.tool_name,             self._graph_run.state,             self._graph_run.deps.new_message_index,             self._traceparent(required=False),         )      def __aiter__(         self,     ) -> AsyncIterator[_agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]]:         \"\"\"Provide async-iteration over the nodes in the agent run.\"\"\"         return self      async def __anext__(         self,     ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:         \"\"\"Advance to the next node automatically based on the last returned node.\"\"\"         task = await anext(self._graph_run)         return self._task_to_node(task)      def _task_to_node(         self, task: EndMarker[FinalResult[OutputDataT]] | JoinItem | Sequence[GraphTask]     ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:         if isinstance(task, Sequence) and len(task) == 1:             first_task = task[0]             if isinstance(first_task.inputs, BaseNode):  # pragma: no branch                 base_node: BaseNode[                     _agent_graph.GraphAgentState,                     _agent_graph.GraphAgentDeps[AgentDepsT, OutputDataT],                     FinalResult[OutputDataT],                 ] = first_task.inputs  # type: ignore[reportUnknownMemberType]                 if _agent_graph.is_agent_node(node=base_node):  # pragma: no branch                     return base_node         if isinstance(task, EndMarker):             return End(task.value)         raise exceptions.AgentRunError(f'Unexpected node: {task}')  # pragma: no cover      def _node_to_task(self, node: _agent_graph.AgentNode[AgentDepsT, OutputDataT]) -> GraphTask:         return GraphTask(NodeStep(type(node)).id, inputs=node, fork_stack=())      async def next(         self,         node: _agent_graph.AgentNode[AgentDepsT, OutputDataT],     ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:         \"\"\"Manually drive the agent run by passing in the node you want to run next.          This lets you inspect or mutate the node before continuing execution, or skip certain nodes         under dynamic conditions. The agent run should be stopped when you return an [`End`][pydantic_graph.nodes.End]         node.          Example:         ```python         from pydantic_ai import Agent         from pydantic_graph import End          agent = Agent('openai:gpt-4o')          async def main():             async with agent.iter('What is the capital of France?') as agent_run:                 next_node = agent_run.next_node  # start with the first node                 nodes = [next_node]                 while not isinstance(next_node, End):                     next_node = await agent_run.next(next_node)                     nodes.append(next_node)                 # Once `next_node` is an End, we've finished:                 print(nodes)                 '''                 [                     UserPromptNode(                         user_prompt='What is the capital of France?',                         instructions_functions=[],                         system_prompts=(),                         system_prompt_functions=[],                         system_prompt_dynamic_functions={},                     ),                     ModelRequestNode(                         request=ModelRequest(                             parts=[                                 UserPromptPart(                                     content='What is the capital of France?',                                     timestamp=datetime.datetime(...),                                 )                             ]                         )                     ),                     CallToolsNode(                         model_response=ModelResponse(                             parts=[TextPart(content='The capital of France is Paris.')],                             usage=RequestUsage(input_tokens=56, output_tokens=7),                             model_name='gpt-4o',                             timestamp=datetime.datetime(...),                         )                     ),                     End(data=FinalResult(output='The capital of France is Paris.')),                 ]                 '''                 print('Final result:', agent_run.result.output)                 #> Final result: The capital of France is Paris.         ```          Args:             node: The node to run next in the graph.          Returns:             The next node returned by the graph logic, or an [`End`][pydantic_graph.nodes.End] node if             the run has completed.         \"\"\"         # Note: It might be nice to expose a synchronous interface for iteration, but we shouldn't do it         # on this class, or else IDEs won't warn you if you accidentally use `for` instead of `async for` to iterate.         task = [self._node_to_task(node)]         try:             task = await self._graph_run.next(task)         except StopAsyncIteration:             pass         return self._task_to_node(task)      # TODO (v2): Make this a property     def usage(self) -> _usage.RunUsage:         \"\"\"Get usage statistics for the run so far, including token usage, model requests, and so on.\"\"\"         return self._graph_run.state.usage      def __repr__(self) -> str:  # pragma: no cover         result = self._graph_run.output         result_repr = '<run not finished>' if result is None else repr(result.output)         return f'<{type(self).__name__} result={result_repr} usage={self.usage()}>' ``` |", "url": "https://ai.pydantic.dev/agent/index.html#agentrun-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AgentRun `dataclass`", "anchor": "agentrun-dataclass", "md_text": "#### ctx `property`\n\n```\nctx: GraphRunContext[\n    GraphAgentState, GraphAgentDeps[AgentDepsT, Any]\n]\n```\n\nThe current context of the agent run.\n\n#### next\\_node `property`\n\n```\nnext_node: (\n    AgentNode[AgentDepsT, OutputDataT]\n    | End[FinalResult[OutputDataT]]\n)\n```\n\nThe next node that will be run in the agent graph.\n\nThis is the next node that will be used during async iteration, or if a node is not passed to `self.next(...)`.\n\n#### result `property`\n\n```\nresult: AgentRunResult[OutputDataT] | None\n```\n\nThe final result of the run if it has ended, otherwise `None`.\n\nOnce the run returns an [`End`](../pydantic_graph/nodes/index.html#pydantic_graph.nodes.End) node, `result` is populated\nwith an [`AgentRunResult`](index.html#pydantic_ai.agent.AgentRunResult).\n\n#### \\_\\_aiter\\_\\_\n\n```\n__aiter__() -> (\n    AsyncIterator[\n        AgentNode[AgentDepsT, OutputDataT]\n        | End[FinalResult[OutputDataT]]\n    ]\n)\n```\n\nProvide async-iteration over the nodes in the agent run.\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n|  |  |\n| --- | --- |\n| ``` 138 139 140 141 142 ``` | ``` def __aiter__(     self, ) -> AsyncIterator[_agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]]:     \"\"\"Provide async-iteration over the nodes in the agent run.\"\"\"     return self ``` |\n\n#### \\_\\_anext\\_\\_ `async`\n\n```\n__anext__() -> (\n    AgentNode[AgentDepsT, OutputDataT]\n    | End[FinalResult[OutputDataT]]\n)\n```\n\nAdvance to the next node automatically based on the last returned node.\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n|  |  |\n| --- | --- |\n| ``` 144 145 146 147 148 149 ``` | ``` async def __anext__(     self, ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:     \"\"\"Advance to the next node automatically based on the last returned node.\"\"\"     task = await anext(self._graph_run)     return self._task_to_node(task) ``` |\n\n#### next `async`\n\n```\nnext(\n    node: AgentNode[AgentDepsT, OutputDataT],\n) -> (\n    AgentNode[AgentDepsT, OutputDataT]\n    | End[FinalResult[OutputDataT]]\n)\n```\n\nManually drive the agent run by passing in the node you want to run next.\n\nThis lets you inspect or mutate the node before continuing execution, or skip certain nodes\nunder dynamic conditions. The agent run should be stopped when you return an [`End`](../pydantic_graph/nodes/index.html#pydantic_graph.nodes.End)\nnode.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_graph import End\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    async with agent.iter('What is the capital of France?') as agent_run:\n        next_node = agent_run.next_node  # start with the first node\n        nodes = [next_node]\n        while not isinstance(next_node, End):\n            next_node = await agent_run.next(next_node)\n            nodes.append(next_node)\n        # Once `next_node` is an End, we've finished:\n        print(nodes)\n        '''\n        [\n            UserPromptNode(\n                user_prompt='What is the capital of France?',\n                instructions_functions=[],\n                system_prompts=(),\n                system_prompt_functions=[],\n                system_prompt_dynamic_functions={},\n            ),\n            ModelRequestNode(\n                request=ModelRequest(\n                    parts=[\n                        UserPromptPart(\n                            content='What is the capital of France?',\n                            timestamp=datetime.datetime(...),\n                        )\n                    ]\n                )\n            ),\n            CallToolsNode(\n                model_response=ModelResponse(\n                    parts=[TextPart(content='The capital of France is Paris.')],\n                    usage=RequestUsage(input_tokens=56, output_tokens=7),\n                    model_name='gpt-4o',\n                    timestamp=datetime.datetime(...),\n                )\n            ),\n            End(data=FinalResult(output='The capital of France is Paris.')),\n        ]\n        '''\n        print('Final result:', agent_run.result.output)\n        #> Final result: The capital of France is Paris.\n```\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `node` | `AgentNode[AgentDepsT, OutputDataT]` | The node to run next in the graph. | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]` | The next node returned by the graph logic, or an [`End`](../pydantic_graph/nodes/index.html#pydantic_graph.nodes.End) node if |\n| `AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]` | the run has completed. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`", "url": "https://ai.pydantic.dev/agent/index.html#agentrun-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AgentRun `dataclass`", "anchor": "agentrun-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 ``` | ``` async def next(     self,     node: _agent_graph.AgentNode[AgentDepsT, OutputDataT], ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:     \"\"\"Manually drive the agent run by passing in the node you want to run next.      This lets you inspect or mutate the node before continuing execution, or skip certain nodes     under dynamic conditions. The agent run should be stopped when you return an [`End`][pydantic_graph.nodes.End]     node.      Example:     ```python     from pydantic_ai import Agent     from pydantic_graph import End      agent = Agent('openai:gpt-4o')      async def main():         async with agent.iter('What is the capital of France?') as agent_run:             next_node = agent_run.next_node  # start with the first node             nodes = [next_node]             while not isinstance(next_node, End):                 next_node = await agent_run.next(next_node)                 nodes.append(next_node)             # Once `next_node` is an End, we've finished:             print(nodes)             '''             [                 UserPromptNode(                     user_prompt='What is the capital of France?',                     instructions_functions=[],                     system_prompts=(),                     system_prompt_functions=[],                     system_prompt_dynamic_functions={},                 ),                 ModelRequestNode(                     request=ModelRequest(                         parts=[                             UserPromptPart(                                 content='What is the capital of France?',                                 timestamp=datetime.datetime(...),                             )                         ]                     )                 ),                 CallToolsNode(                     model_response=ModelResponse(                         parts=[TextPart(content='The capital of France is Paris.')],                         usage=RequestUsage(input_tokens=56, output_tokens=7),                         model_name='gpt-4o',                         timestamp=datetime.datetime(...),                     )                 ),                 End(data=FinalResult(output='The capital of France is Paris.')),             ]             '''             print('Final result:', agent_run.result.output)             #> Final result: The capital of France is Paris.     ```      Args:         node: The node to run next in the graph.      Returns:         The next node returned by the graph logic, or an [`End`][pydantic_graph.nodes.End] node if         the run has completed.     \"\"\"     # Note: It might be nice to expose a synchronous interface for iteration, but we shouldn't do it     # on this class, or else IDEs won't warn you if you accidentally use `for` instead of `async for` to iterate.     task = [self._node_to_task(node)]     try:         task = await self._graph_run.next(task)     except StopAsyncIteration:         pass     return self._task_to_node(task) ``` |\n\n#### usage\n\n```\nusage() -> RunUsage\n```\n\nGet usage statistics for the run so far, including token usage, model requests, and so on.\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n|  |  |\n| --- | --- |\n| ``` 248 249 250 ``` | ``` def usage(self) -> _usage.RunUsage:     \"\"\"Get usage statistics for the run so far, including token usage, model requests, and so on.\"\"\"     return self._graph_run.state.usage ``` |", "url": "https://ai.pydantic.dev/agent/index.html#agentrun-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AgentRunResult `dataclass`", "anchor": "agentrunresult-dataclass", "md_text": "Bases: `Generic[OutputDataT]`\n\nThe final result of an agent run.\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`", "url": "https://ai.pydantic.dev/agent/index.html#agentrunresult-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AgentRunResult `dataclass`", "anchor": "agentrunresult-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 ``` | ``` @dataclasses.dataclass class AgentRunResult(Generic[OutputDataT]):     \"\"\"The final result of an agent run.\"\"\"      output: OutputDataT     \"\"\"The output data from the agent run.\"\"\"      _output_tool_name: str | None = dataclasses.field(repr=False, compare=False, default=None)     _state: _agent_graph.GraphAgentState = dataclasses.field(         repr=False, compare=False, default_factory=_agent_graph.GraphAgentState     )     _new_message_index: int = dataclasses.field(repr=False, compare=False, default=0)     _traceparent_value: str | None = dataclasses.field(repr=False, compare=False, default=None)      @overload     def _traceparent(self, *, required: Literal[False]) -> str | None: ...     @overload     def _traceparent(self) -> str: ...     def _traceparent(self, *, required: bool = True) -> str | None:         if self._traceparent_value is None and required:  # pragma: no cover             raise AttributeError('No span was created for this agent run')         return self._traceparent_value      def _set_output_tool_return(self, return_content: str) -> list[_messages.ModelMessage]:         \"\"\"Set return content for the output tool.          Useful if you want to continue the conversation and want to set the response to the output tool call.         \"\"\"         if not self._output_tool_name:             raise ValueError('Cannot set output tool return content when the return type is `str`.')          messages = self._state.message_history         last_message = messages[-1]         for idx, part in enumerate(last_message.parts):             if isinstance(part, _messages.ToolReturnPart) and part.tool_name == self._output_tool_name:                 # Only do deepcopy when we have to modify                 copied_messages = list(messages)                 copied_last = deepcopy(last_message)                 copied_last.parts[idx].content = return_content  # type: ignore[misc]                 copied_messages[-1] = copied_last                 return copied_messages          raise LookupError(f'No tool call found with tool name {self._output_tool_name!r}.')      def all_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:         \"\"\"Return the history of _messages.          Args:             output_tool_return_content: The return content of the tool call to set in the last message.                 This provides a convenient way to modify the content of the output tool call if you want to continue                 the conversation and want to set the response to the output tool call. If `None`, the last message will                 not be modified.          Returns:             List of messages.         \"\"\"         if output_tool_return_content is not None:             return self._set_output_tool_return(output_tool_return_content)         else:             return self._state.message_history      def all_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:         \"\"\"Return all messages from [`all_messages`][pydantic_ai.agent.AgentRunResult.all_messages] as JSON bytes.          Args:             output_tool_return_content: The return content of the tool call to set in the last message.                 This provides a convenient way to modify the content of the output tool call if you want to continue                 the conversation and want to set the response to the output tool call. If `None`, the last message will                 not be modified.          Returns:             JSON bytes representing the messages.         \"\"\"         return _messages.ModelMessagesTypeAdapter.dump_json(             self.all_messages(output_tool_return_content=output_tool_return_content)         )      def new_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:         \"\"\"Return new messages associated with this run.          Messages from older runs are excluded.          Args:             output_tool_return_content: The return content of the tool call to set in the last message.                 This provides a convenient way to modify the content of the output tool call if you want to continue                 the conversation and want to set the response to the output tool call. If `None`, the last message will                 not be modified.          Returns:             List of new messages.         \"\"\"         return self.all_messages(output_tool_return_content=output_tool_return_content)[self._new_message_index :]      def new_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:         \"\"\"Return new messages from [`new_messages`][pydantic_ai.agent.AgentRunResult.new_messages] as JSON bytes.          Args:             output_tool_return_content: The return content of the tool call to set in the last message.                 This provides a convenient way to modify the content of the output tool call if you want to continue                 the conversation and want to set the response to the output tool call. If `None`, the last message will                 not be modified.          Returns:             JSON bytes representing the new messages.         \"\"\"         return _messages.ModelMessagesTypeAdapter.dump_json(             self.new_messages(output_tool_return_content=output_tool_return_content)         )      @property     def response(self) -> _messages.ModelResponse:         \"\"\"Return the last response from the message history.\"\"\"         # The response may not be the very last item if it contained an output tool call. See `CallToolsNode._handle_final_result`.         for message in reversed(self.all_messages()):             if isinstance(message, _messages.ModelResponse):                 return message         raise ValueError('No response found in the message history')  # pragma: no cover      # TODO (v2): Make this a property     def usage(self) -> _usage.RunUsage:         \"\"\"Return the usage of the whole run.\"\"\"         return self._state.usage      # TODO (v2): Make this a property     def timestamp(self) -> datetime:         \"\"\"Return the timestamp of last response.\"\"\"         return self.response.timestamp ``` |", "url": "https://ai.pydantic.dev/agent/index.html#agentrunresult-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AgentRunResult `dataclass`", "anchor": "agentrunresult-dataclass", "md_text": "#### output `instance-attribute`\n\n```\noutput: OutputDataT\n```\n\nThe output data from the agent run.\n\n#### all\\_messages\n\n```\nall_messages(\n    *, output_tool_return_content: str | None = None\n) -> list[ModelMessage]\n```\n\nReturn the history of \\_messages.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `output_tool_return_content` | `str | None` | The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the output tool call if you want to continue the conversation and want to set the response to the output tool call. If `None`, the last message will not be modified. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `list[ModelMessage]` | List of messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n|  |  |\n| --- | --- |\n| ``` 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 ``` | ``` def all_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:     \"\"\"Return the history of _messages.      Args:         output_tool_return_content: The return content of the tool call to set in the last message.             This provides a convenient way to modify the content of the output tool call if you want to continue             the conversation and want to set the response to the output tool call. If `None`, the last message will             not be modified.      Returns:         List of messages.     \"\"\"     if output_tool_return_content is not None:         return self._set_output_tool_return(output_tool_return_content)     else:         return self._state.message_history ``` |\n\n#### all\\_messages\\_json\n\n```\nall_messages_json(\n    *, output_tool_return_content: str | None = None\n) -> bytes\n```\n\nReturn all messages from [`all_messages`](index.html#pydantic_ai.agent.AgentRunResult.all_messages) as JSON bytes.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `output_tool_return_content` | `str | None` | The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the output tool call if you want to continue the conversation and want to set the response to the output tool call. If `None`, the last message will not be modified. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `bytes` | JSON bytes representing the messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n|  |  |\n| --- | --- |\n| ``` 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 ``` | ``` def all_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:     \"\"\"Return all messages from [`all_messages`][pydantic_ai.agent.AgentRunResult.all_messages] as JSON bytes.      Args:         output_tool_return_content: The return content of the tool call to set in the last message.             This provides a convenient way to modify the content of the output tool call if you want to continue             the conversation and want to set the response to the output tool call. If `None`, the last message will             not be modified.      Returns:         JSON bytes representing the messages.     \"\"\"     return _messages.ModelMessagesTypeAdapter.dump_json(         self.all_messages(output_tool_return_content=output_tool_return_content)     ) ``` |\n\n#### new\\_messages\n\n```\nnew_messages(\n    *, output_tool_return_content: str | None = None\n) -> list[ModelMessage]\n```\n\nReturn new messages associated with this run.\n\nMessages from older runs are excluded.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `output_tool_return_content` | `str | None` | The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the output tool call if you want to continue the conversation and want to set the response to the output tool call. If `None`, the last message will not be modified. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `list[ModelMessage]` | List of new messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n|  |  |\n| --- | --- |\n| ``` 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 ``` | ``` def new_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:     \"\"\"Return new messages associated with this run.      Messages from older runs are excluded.      Args:         output_tool_return_content: The return content of the tool call to set in the last message.             This provides a convenient way to modify the content of the output tool call if you want to continue             the conversation and want to set the response to the output tool call. If `None`, the last message will             not be modified.      Returns:         List of new messages.     \"\"\"     return self.all_messages(output_tool_return_content=output_tool_return_content)[self._new_message_index :] ``` |\n\n#### new\\_messages\\_json", "url": "https://ai.pydantic.dev/agent/index.html#agentrunresult-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "AgentRunResult `dataclass`", "anchor": "agentrunresult-dataclass", "md_text": "```\nnew_messages_json(\n    *, output_tool_return_content: str | None = None\n) -> bytes\n```\n\nReturn new messages from [`new_messages`](index.html#pydantic_ai.agent.AgentRunResult.new_messages) as JSON bytes.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `output_tool_return_content` | `str | None` | The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the output tool call if you want to continue the conversation and want to set the response to the output tool call. If `None`, the last message will not be modified. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `bytes` | JSON bytes representing the new messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n|  |  |\n| --- | --- |\n| ``` 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 ``` | ``` def new_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:     \"\"\"Return new messages from [`new_messages`][pydantic_ai.agent.AgentRunResult.new_messages] as JSON bytes.      Args:         output_tool_return_content: The return content of the tool call to set in the last message.             This provides a convenient way to modify the content of the output tool call if you want to continue             the conversation and want to set the response to the output tool call. If `None`, the last message will             not be modified.      Returns:         JSON bytes representing the new messages.     \"\"\"     return _messages.ModelMessagesTypeAdapter.dump_json(         self.new_messages(output_tool_return_content=output_tool_return_content)     ) ``` |\n\n#### response `property`\n\n```\nresponse: ModelResponse\n```\n\nReturn the last response from the message history.\n\n#### usage\n\n```\nusage() -> RunUsage\n```\n\nReturn the usage of the whole run.\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n|  |  |\n| --- | --- |\n| ``` 377 378 379 ``` | ``` def usage(self) -> _usage.RunUsage:     \"\"\"Return the usage of the whole run.\"\"\"     return self._state.usage ``` |\n\n#### timestamp\n\n```\ntimestamp() -> datetime\n```\n\nReturn the timestamp of last response.\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n|  |  |\n| --- | --- |\n| ``` 382 383 384 ``` | ``` def timestamp(self) -> datetime:     \"\"\"Return the timestamp of last response.\"\"\"     return self.response.timestamp ``` |", "url": "https://ai.pydantic.dev/agent/index.html#agentrunresult-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "EndStrategy `module-attribute`", "anchor": "endstrategy-module-attribute", "md_text": "```\nEndStrategy = Literal['early', 'exhaustive']\n```\n\nThe strategy for handling multiple tool calls when a final result is found.\n\n* `'early'`: Stop processing other tool calls once a final result is found\n* `'exhaustive'`: Process all tool calls even after finding a final result", "url": "https://ai.pydantic.dev/agent/index.html#endstrategy-module-attribute", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "RunOutputDataT `module-attribute`", "anchor": "runoutputdatat-module-attribute", "md_text": "```\nRunOutputDataT = TypeVar('RunOutputDataT')\n```\n\nType variable for the result data of a run where `output_type` was customized on the run call.", "url": "https://ai.pydantic.dev/agent/index.html#runoutputdatat-module-attribute", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "capture\\_run\\_messages", "anchor": "capturerunmessages", "md_text": "```\ncapture_run_messages() -> Iterator[list[ModelMessage]]\n```\n\nContext manager to access the messages used in a [`run`](index.html#pydantic_ai.agent.AbstractAgent.run), [`run_sync`](index.html#pydantic_ai.agent.AbstractAgent.run_sync), or [`run_stream`](index.html#pydantic_ai.agent.AbstractAgent.run_stream) call.\n\nUseful when a run may raise an exception, see [model errors](https://ai.pydantic.dev/agents/#model-errors) for more information.\n\nExamples:\n\n```\nfrom pydantic_ai import Agent, capture_run_messages\n\nagent = Agent('test')\n\nwith capture_run_messages() as messages:\n    try:\n        result = agent.run_sync('foobar')\n    except Exception:\n        print(messages)\n        raise\n```\n\nIf you call `run`, `run_sync`, or `run_stream` more than once within a single `capture_run_messages` context,\n`messages` will represent the messages exchanged during the first call only.\n\nSource code in `pydantic_ai_slim/pydantic_ai/_agent_graph.py`\n\n|  |  |\n| --- | --- |\n| ``` 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 ``` | ``` @contextmanager def capture_run_messages() -> Iterator[list[_messages.ModelMessage]]:     \"\"\"Context manager to access the messages used in a [`run`][pydantic_ai.agent.AbstractAgent.run], [`run_sync`][pydantic_ai.agent.AbstractAgent.run_sync], or [`run_stream`][pydantic_ai.agent.AbstractAgent.run_stream] call.      Useful when a run may raise an exception, see [model errors](../agents.md#model-errors) for more information.      Examples:     ```python     from pydantic_ai import Agent, capture_run_messages      agent = Agent('test')      with capture_run_messages() as messages:         try:             result = agent.run_sync('foobar')         except Exception:             print(messages)             raise     ```      !!! note         If you call `run`, `run_sync`, or `run_stream` more than once within a single `capture_run_messages` context,         `messages` will represent the messages exchanged during the first call only.     \"\"\"     token = None     messages: list[_messages.ModelMessage] = []      # Try to reuse existing message context if available     try:         messages = _messages_ctx_var.get().messages     except LookupError:         # No existing context, create a new one         token = _messages_ctx_var.set(_RunMessages(messages))      try:         yield messages     finally:         # Clean up context if we created it         if token is not None:             _messages_ctx_var.reset(token) ``` |", "url": "https://ai.pydantic.dev/agent/index.html#capturerunmessages", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "InstrumentationSettings `dataclass`", "anchor": "instrumentationsettings-dataclass", "md_text": "Options for instrumenting models and agents with OpenTelemetry.\n\nUsed in:\n\n* `Agent(instrument=...)`\n* [`Agent.instrument_all()`](index.html#pydantic_ai.agent.Agent.instrument_all)\n* [`InstrumentedModel`](../models/instrumented/index.html#pydantic_ai.models.instrumented.InstrumentedModel)\n\nSee the [Debugging and Monitoring guide](https://ai.pydantic.dev/logfire/) for more info.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/instrumented.py`", "url": "https://ai.pydantic.dev/agent/index.html#instrumentationsettings-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "InstrumentationSettings `dataclass`", "anchor": "instrumentationsettings-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ```  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 ``` | ``` @dataclass(init=False) class InstrumentationSettings:     \"\"\"Options for instrumenting models and agents with OpenTelemetry.      Used in:      - `Agent(instrument=...)`     - [`Agent.instrument_all()`][pydantic_ai.agent.Agent.instrument_all]     - [`InstrumentedModel`][pydantic_ai.models.instrumented.InstrumentedModel]      See the [Debugging and Monitoring guide](https://ai.pydantic.dev/logfire/) for more info.     \"\"\"      tracer: Tracer = field(repr=False)     event_logger: EventLogger = field(repr=False)     event_mode: Literal['attributes', 'logs'] = 'attributes'     include_binary_content: bool = True     include_content: bool = True     version: Literal[1, 2, 3] = DEFAULT_INSTRUMENTATION_VERSION      def __init__(         self,         *,         tracer_provider: TracerProvider | None = None,         meter_provider: MeterProvider | None = None,         include_binary_content: bool = True,         include_content: bool = True,         version: Literal[1, 2, 3] = DEFAULT_INSTRUMENTATION_VERSION,         event_mode: Literal['attributes', 'logs'] = 'attributes',         event_logger_provider: EventLoggerProvider | None = None,     ):         \"\"\"Create instrumentation options.          Args:             tracer_provider: The OpenTelemetry tracer provider to use.                 If not provided, the global tracer provider is used.                 Calling `logfire.configure()` sets the global tracer provider, so most users don't need this.             meter_provider: The OpenTelemetry meter provider to use.                 If not provided, the global meter provider is used.                 Calling `logfire.configure()` sets the global meter provider, so most users don't need this.             include_binary_content: Whether to include binary content in the instrumentation events.             include_content: Whether to include prompts, completions, and tool call arguments and responses                 in the instrumentation events.             version: Version of the data format. This is unrelated to the Pydantic AI package version.                 Version 1 is based on the legacy event-based OpenTelemetry GenAI spec                     and will be removed in a future release.                     The parameters `event_mode` and `event_logger_provider` are only relevant for version 1.                 Version 2 uses the newer OpenTelemetry GenAI spec and stores messages in the following attributes:                     - `gen_ai.system_instructions` for instructions passed to the agent.                     - `gen_ai.input.messages` and `gen_ai.output.messages` on model request spans.                     - `pydantic_ai.all_messages` on agent run spans.             event_mode: The mode for emitting events in version 1.                 If `'attributes'`, events are attached to the span as attributes.                 If `'logs'`, events are emitted as OpenTelemetry log-based events.             event_logger_provider: The OpenTelemetry event logger provider to use.                 If not provided, the global event logger provider is used.                 Calling `logfire.configure()` sets the global event logger provider, so most users don't need this.                 This is only used if `event_mode='logs'` and `version=1`.         \"\"\"         from pydantic_ai import __version__          tracer_provider = tracer_provider or get_tracer_provider()         meter_provider = meter_provider or get_meter_provider()         event_logger_provider = event_logger_provider or get_event_logger_provider()         scope_name = 'pydantic-ai'         self.tracer = tracer_provider.get_tracer(scope_name, __version__)         self.meter = meter_provider.get_meter(scope_name, __version__)         self.event_logger = event_logger_provider.get_event_logger(scope_name, __version__)         self.event_mode = event_mode         self.include_binary_content = include_binary_content         self.include_content = include_content          if event_mode == 'logs' and version != 1:             warnings.warn(                 'event_mode is only relevant for version=1 which is deprecated and will be removed in a future release.',                 stacklevel=2,             )             version = 1          self.version = version          # As specified in the OpenTelemetry GenAI metrics spec:         # https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-metrics/#metric-gen_aiclienttokenusage         tokens_histogram_kwargs = dict(             name='gen_ai.client.token.usage',             unit='{token}',             description='Measures number of input and output tokens used',         )         try:             self.tokens_histogram = self.meter.create_histogram(                 **tokens_histogram_kwargs,                 explicit_bucket_boundaries_advisory=TOKEN_HISTOGRAM_BOUNDARIES,             )         except TypeError:  # pragma: lax no cover             # Older OTel/logfire versions don't support explicit_bucket_boundaries_advisory             self.tokens_histogram = self.meter.create_histogram(                 **tokens_histogram_kwargs,  # pyright: ignore             )         self.cost_histogram = self.meter.create_histogram(             'operation.cost',             unit='{USD}',             description='Monetary cost',         )      def messages_to_otel_events(self, messages: list[ModelMessage]) -> list[Event]:         \"\"\"Convert a list of model messages to OpenTelemetry events.          Args:             messages: The messages to convert.          Returns:             A list of OpenTelemetry events.         \"\"\"         events: list[Event] = []         instructions = InstrumentedModel._get_instructions(messages)  # pyright: ignore [reportPrivateUsage]         if instructions is not None:             events.append(                 Event(                     'gen_ai.system.message',                     body={**({'content': instructions} if self.include_content else {}), 'role': 'system'},                 )             )          for message_index, message in enumerate(messages):             message_events: list[Event] = []             if isinstance(message, ModelRequest):                 for part in message.parts:                     if hasattr(part, 'otel_event'):                         message_events.append(part.otel_event(self))             elif isinstance(message, ModelResponse):  # pragma: no branch                 message_events = message.otel_events(self)             for event in message_events:                 event.attributes = {                     'gen_ai.message.index': message_index,                     **(event.attributes or {}),                 }             events.extend(message_events)          for event in events:             event.body = InstrumentedModel.serialize_any(event.body)         return events      def messages_to_otel_messages(self, messages: list[ModelMessage]) -> list[_otel_messages.ChatMessage]:         result: list[_otel_messages.ChatMessage] = []         for message in messages:             if isinstance(message, ModelRequest):                 for is_system, group in itertools.groupby(message.parts, key=lambda p: isinstance(p, SystemPromptPart)):                     message_parts: list[_otel_messages.MessagePart] = []                     for part in group:                         if hasattr(part, 'otel_message_parts'):                             message_parts.extend(part.otel_message_parts(self))                     result.append(                         _otel_messages.ChatMessage(role='system' if is_system else 'user', parts=message_parts)                     )             elif isinstance(message, ModelResponse):  # pragma: no branch                 otel_message = _otel_messages.OutputMessage(role='assistant', parts=message.otel_message_parts(self))                 if message.finish_reason is not None:                     otel_message['finish_reason'] = message.finish_reason                 result.append(otel_message)         return result      def handle_messages(self, input_messages: list[ModelMessage], response: ModelResponse, system: str, span: Span):         if self.version == 1:             events = self.messages_to_otel_events(input_messages)             for event in self.messages_to_otel_events([response]):                 events.append(                     Event(                         'gen_ai.choice',                         body={                             'index': 0,                             'message': event.body,                         },                     )                 )             for event in events:                 event.attributes = {                     GEN_AI_SYSTEM_ATTRIBUTE: system,                     **(event.attributes or {}),                 }             self._emit_events(span, events)         else:             output_messages = self.messages_to_otel_messages([response])             assert len(output_messages) == 1             output_message = output_messages[0]             instructions = InstrumentedModel._get_instructions(input_messages)  # pyright: ignore [reportPrivateUsage]             system_instructions_attributes = self.system_instructions_attributes(instructions)             attributes: dict[str, AttributeValue] = {                 'gen_ai.input.messages': json.dumps(self.messages_to_otel_messages(input_messages)),                 'gen_ai.output.messages': json.dumps([output_message]),                 **system_instructions_attributes,                 'logfire.json_schema': json.dumps(                     {                         'type': 'object',                         'properties': {                             'gen_ai.input.messages': {'type': 'array'},                             'gen_ai.output.messages': {'type': 'array'},                             **(                                 {'gen_ai.system_instructions': {'type': 'array'}}                                 if system_instructions_attributes                                 else {}                             ),                             'model_request_parameters': {'type': 'object'},                         },                     }                 ),             }             span.set_attributes(attributes)      def system_instructions_attributes(self, instructions: str | None) -> dict[str, str]:         if instructions and self.include_content:             return {                 'gen_ai.system_instructions': json.dumps([_otel_messages.TextPart(type='text', content=instructions)]),             }         return {}      def _emit_events(self, span: Span, events: list[Event]) -> None:         if self.event_mode == 'logs':             for event in events:                 self.event_logger.emit(event)         else:             attr_name = 'events'             span.set_attributes(                 {                     attr_name: json.dumps([InstrumentedModel.event_to_dict(event) for event in events]),                     'logfire.json_schema': json.dumps(                         {                             'type': 'object',                             'properties': {                                 attr_name: {'type': 'array'},                                 'model_request_parameters': {'type': 'object'},                             },                         }                     ),                 }             )      def record_metrics(         self,         response: ModelResponse,         price_calculation: PriceCalculation | None,         attributes: dict[str, AttributeValue],     ):         for typ in ['input', 'output']:             if not (tokens := getattr(response.usage, f'{typ}_tokens', 0)):  # pragma: no cover                 continue             token_attributes = {**attributes, 'gen_ai.token.type': typ}             self.tokens_histogram.record(tokens, token_attributes)             if price_calculation:                 cost = float(getattr(price_calculation, f'{typ}_price'))                 self.cost_histogram.record(cost, token_attributes) ``` |", "url": "https://ai.pydantic.dev/agent/index.html#instrumentationsettings-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "InstrumentationSettings `dataclass`", "anchor": "instrumentationsettings-dataclass", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    *,\n    tracer_provider: TracerProvider | None = None,\n    meter_provider: MeterProvider | None = None,\n    include_binary_content: bool = True,\n    include_content: bool = True,\n    version: Literal[\n        1, 2, 3\n    ] = DEFAULT_INSTRUMENTATION_VERSION,\n    event_mode: Literal[\n        \"attributes\", \"logs\"\n    ] = \"attributes\",\n    event_logger_provider: EventLoggerProvider | None = None\n)\n```\n\nCreate instrumentation options.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `tracer_provider` | `TracerProvider | None` | The OpenTelemetry tracer provider to use. If not provided, the global tracer provider is used. Calling `logfire.configure()` sets the global tracer provider, so most users don't need this. | `None` |\n| `meter_provider` | `MeterProvider | None` | The OpenTelemetry meter provider to use. If not provided, the global meter provider is used. Calling `logfire.configure()` sets the global meter provider, so most users don't need this. | `None` |\n| `include_binary_content` | `bool` | Whether to include binary content in the instrumentation events. | `True` |\n| `include_content` | `bool` | Whether to include prompts, completions, and tool call arguments and responses in the instrumentation events. | `True` |\n| `version` | `Literal[1, 2, 3]` | Version of the data format. This is unrelated to the Pydantic AI package version. Version 1 is based on the legacy event-based OpenTelemetry GenAI spec and will be removed in a future release. The parameters `event_mode` and `event_logger_provider` are only relevant for version 1. Version 2 uses the newer OpenTelemetry GenAI spec and stores messages in the following attributes: - `gen_ai.system_instructions` for instructions passed to the agent. - `gen_ai.input.messages` and `gen_ai.output.messages` on model request spans. - `pydantic_ai.all_messages` on agent run spans. | `DEFAULT_INSTRUMENTATION_VERSION` |\n| `event_mode` | `Literal['attributes', 'logs']` | The mode for emitting events in version 1. If `'attributes'`, events are attached to the span as attributes. If `'logs'`, events are emitted as OpenTelemetry log-based events. | `'attributes'` |\n| `event_logger_provider` | `EventLoggerProvider | None` | The OpenTelemetry event logger provider to use. If not provided, the global event logger provider is used. Calling `logfire.configure()` sets the global event logger provider, so most users don't need this. This is only used if `event_mode='logs'` and `version=1`. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/instrumented.py`", "url": "https://ai.pydantic.dev/agent/index.html#instrumentationsettings-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "InstrumentationSettings `dataclass`", "anchor": "instrumentationsettings-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ```  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 ``` | ``` def __init__(     self,     *,     tracer_provider: TracerProvider | None = None,     meter_provider: MeterProvider | None = None,     include_binary_content: bool = True,     include_content: bool = True,     version: Literal[1, 2, 3] = DEFAULT_INSTRUMENTATION_VERSION,     event_mode: Literal['attributes', 'logs'] = 'attributes',     event_logger_provider: EventLoggerProvider | None = None, ):     \"\"\"Create instrumentation options.      Args:         tracer_provider: The OpenTelemetry tracer provider to use.             If not provided, the global tracer provider is used.             Calling `logfire.configure()` sets the global tracer provider, so most users don't need this.         meter_provider: The OpenTelemetry meter provider to use.             If not provided, the global meter provider is used.             Calling `logfire.configure()` sets the global meter provider, so most users don't need this.         include_binary_content: Whether to include binary content in the instrumentation events.         include_content: Whether to include prompts, completions, and tool call arguments and responses             in the instrumentation events.         version: Version of the data format. This is unrelated to the Pydantic AI package version.             Version 1 is based on the legacy event-based OpenTelemetry GenAI spec                 and will be removed in a future release.                 The parameters `event_mode` and `event_logger_provider` are only relevant for version 1.             Version 2 uses the newer OpenTelemetry GenAI spec and stores messages in the following attributes:                 - `gen_ai.system_instructions` for instructions passed to the agent.                 - `gen_ai.input.messages` and `gen_ai.output.messages` on model request spans.                 - `pydantic_ai.all_messages` on agent run spans.         event_mode: The mode for emitting events in version 1.             If `'attributes'`, events are attached to the span as attributes.             If `'logs'`, events are emitted as OpenTelemetry log-based events.         event_logger_provider: The OpenTelemetry event logger provider to use.             If not provided, the global event logger provider is used.             Calling `logfire.configure()` sets the global event logger provider, so most users don't need this.             This is only used if `event_mode='logs'` and `version=1`.     \"\"\"     from pydantic_ai import __version__      tracer_provider = tracer_provider or get_tracer_provider()     meter_provider = meter_provider or get_meter_provider()     event_logger_provider = event_logger_provider or get_event_logger_provider()     scope_name = 'pydantic-ai'     self.tracer = tracer_provider.get_tracer(scope_name, __version__)     self.meter = meter_provider.get_meter(scope_name, __version__)     self.event_logger = event_logger_provider.get_event_logger(scope_name, __version__)     self.event_mode = event_mode     self.include_binary_content = include_binary_content     self.include_content = include_content      if event_mode == 'logs' and version != 1:         warnings.warn(             'event_mode is only relevant for version=1 which is deprecated and will be removed in a future release.',             stacklevel=2,         )         version = 1      self.version = version      # As specified in the OpenTelemetry GenAI metrics spec:     # https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-metrics/#metric-gen_aiclienttokenusage     tokens_histogram_kwargs = dict(         name='gen_ai.client.token.usage',         unit='{token}',         description='Measures number of input and output tokens used',     )     try:         self.tokens_histogram = self.meter.create_histogram(             **tokens_histogram_kwargs,             explicit_bucket_boundaries_advisory=TOKEN_HISTOGRAM_BOUNDARIES,         )     except TypeError:  # pragma: lax no cover         # Older OTel/logfire versions don't support explicit_bucket_boundaries_advisory         self.tokens_histogram = self.meter.create_histogram(             **tokens_histogram_kwargs,  # pyright: ignore         )     self.cost_histogram = self.meter.create_histogram(         'operation.cost',         unit='{USD}',         description='Monetary cost',     ) ``` |\n\n#### messages\\_to\\_otel\\_events\n\n```\nmessages_to_otel_events(\n    messages: list[ModelMessage],\n) -> list[Event]\n```\n\nConvert a list of model messages to OpenTelemetry events.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `messages` | `list[ModelMessage]` | The messages to convert. | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `list[Event]` | A list of OpenTelemetry events. |", "url": "https://ai.pydantic.dev/agent/index.html#instrumentationsettings-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "InstrumentationSettings `dataclass`", "anchor": "instrumentationsettings-dataclass", "md_text": "Source code in `pydantic_ai_slim/pydantic_ai/models/instrumented.py`\n\n|  |  |\n| --- | --- |\n| ``` 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 ``` | ``` def messages_to_otel_events(self, messages: list[ModelMessage]) -> list[Event]:     \"\"\"Convert a list of model messages to OpenTelemetry events.      Args:         messages: The messages to convert.      Returns:         A list of OpenTelemetry events.     \"\"\"     events: list[Event] = []     instructions = InstrumentedModel._get_instructions(messages)  # pyright: ignore [reportPrivateUsage]     if instructions is not None:         events.append(             Event(                 'gen_ai.system.message',                 body={**({'content': instructions} if self.include_content else {}), 'role': 'system'},             )         )      for message_index, message in enumerate(messages):         message_events: list[Event] = []         if isinstance(message, ModelRequest):             for part in message.parts:                 if hasattr(part, 'otel_event'):                     message_events.append(part.otel_event(self))         elif isinstance(message, ModelResponse):  # pragma: no branch             message_events = message.otel_events(self)         for event in message_events:             event.attributes = {                 'gen_ai.message.index': message_index,                 **(event.attributes or {}),             }         events.extend(message_events)      for event in events:         event.body = InstrumentedModel.serialize_any(event.body)     return events ``` |", "url": "https://ai.pydantic.dev/agent/index.html#instrumentationsettings-dataclass", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "EventStreamHandler `module-attribute`", "anchor": "eventstreamhandler-module-attribute", "md_text": "```\nEventStreamHandler: TypeAlias = Callable[\n    [\n        RunContext[AgentDepsT],\n        AsyncIterable[AgentStreamEvent],\n    ],\n    Awaitable[None],\n]\n```\n\nA function that receives agent [`RunContext`](../tools/index.html#pydantic_ai.tools.RunContext) and an async iterable of events from the model's streaming response and the agent's execution of tools.", "url": "https://ai.pydantic.dev/agent/index.html#eventstreamhandler-module-attribute", "page": "agent/index.html", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "md_text": "To use the `FastMCPToolset`, you will need to install [`pydantic-ai-slim`](https://ai.pydantic.dev/install/#slim-install) with the `fastmcp` optional group:\n\npipuv\n\n```\npip install \"pydantic-ai-slim[fastmcp]\"\n\nuv add \"pydantic-ai-slim[fastmcp]\"\n```", "url": "https://ai.pydantic.dev/fastmcp-client/index.html#install", "page": "fastmcp-client/index.html", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "md_text": "A `FastMCPToolset` can then be created from:\n\n* A FastMCP Server: `FastMCPToolset(fastmcp.FastMCP('my_server'))`\n* A FastMCP Client: `FastMCPToolset(fastmcp.Client(...))`\n* A FastMCP Transport: `FastMCPToolset(fastmcp.StdioTransport(command='uvx', args=['mcp-run-python', 'stdio']))`\n* A Streamable HTTP URL: `FastMCPToolset('http://localhost:8000/mcp')`\n* An HTTP SSE URL: `FastMCPToolset('http://localhost:8000/sse')`\n* A Python Script: `FastMCPToolset('my_server.py')`\n* A Node.js Script: `FastMCPToolset('my_server.js')`\n* A JSON MCP Configuration: `FastMCPToolset({'mcpServers': {'my_server': {'command': 'uvx', 'args': ['mcp-run-python', 'stdio']}}})`\n\nIf you already have a [FastMCP Server](https://gofastmcp.com/servers) in the same codebase as your Pydantic AI agent, you can create a `FastMCPToolset` directly from it and save agent a network round trip:\n\n```\nfrom fastmcp import FastMCP\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.toolsets.fastmcp import FastMCPToolset\n\nfastmcp_server = FastMCP('my_server')\n@fastmcp_server.tool()\nasync def add(a: int, b: int) -> int:\n    return a + b\n\ntoolset = FastMCPToolset(fastmcp_server)\n\nagent = Agent('openai:gpt-5', toolsets=[toolset])\n\nasync def main():\n    result = await agent.run('What is 7 plus 5?')\n    print(result.output)\n    #> The answer is 12.\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\nConnecting your agent to a Streamable HTTP MCP Server is as simple as:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.toolsets.fastmcp import FastMCPToolset\n\ntoolset = FastMCPToolset('http://localhost:8000/mcp')\n\nagent = Agent('openai:gpt-5', toolsets=[toolset])\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\nYou can also create a `FastMCPToolset` from a JSON MCP Configuration:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.toolsets.fastmcp import FastMCPToolset\n\nmcp_config = {\n    'mcpServers': {\n        'time_mcp_server': {\n            'command': 'uvx',\n            'args': ['mcp-run-python', 'stdio']\n        }\n    }\n}\n\ntoolset = FastMCPToolset(mcp_config)\n\nagent = Agent('openai:gpt-5', toolsets=[toolset])\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/fastmcp-client/index.html#usage", "page": "fastmcp-client/index.html", "source_site": "pydantic_ai"}
{"title": "StreamedRunResult `dataclass`", "anchor": "streamedrunresult-dataclass", "md_text": "Bases: `Generic[AgentDepsT, OutputDataT]`\n\nResult of a streamed run that returns structured data via a tool call.\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`", "url": "https://ai.pydantic.dev/result/index.html#streamedrunresult-dataclass", "page": "result/index.html", "source_site": "pydantic_ai"}
{"title": "StreamedRunResult `dataclass`", "anchor": "streamedrunresult-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 ``` | ``` @dataclass(init=False) class StreamedRunResult(Generic[AgentDepsT, OutputDataT]):     \"\"\"Result of a streamed run that returns structured data via a tool call.\"\"\"      _all_messages: list[_messages.ModelMessage]     _new_message_index: int      _stream_response: AgentStream[AgentDepsT, OutputDataT] | None = None     _on_complete: Callable[[], Awaitable[None]] | None = None      _run_result: AgentRunResult[OutputDataT] | None = None      is_complete: bool = field(default=False, init=False)     \"\"\"Whether the stream has all been received.      This is set to `True` when one of     [`stream_output`][pydantic_ai.result.StreamedRunResult.stream_output],     [`stream_text`][pydantic_ai.result.StreamedRunResult.stream_text],     [`stream_responses`][pydantic_ai.result.StreamedRunResult.stream_responses] or     [`get_output`][pydantic_ai.result.StreamedRunResult.get_output] completes.     \"\"\"      @overload     def __init__(         self,         all_messages: list[_messages.ModelMessage],         new_message_index: int,         stream_response: AgentStream[AgentDepsT, OutputDataT] | None,         on_complete: Callable[[], Awaitable[None]] | None,     ) -> None: ...      @overload     def __init__(         self,         all_messages: list[_messages.ModelMessage],         new_message_index: int,         *,         run_result: AgentRunResult[OutputDataT],     ) -> None: ...      def __init__(         self,         all_messages: list[_messages.ModelMessage],         new_message_index: int,         stream_response: AgentStream[AgentDepsT, OutputDataT] | None = None,         on_complete: Callable[[], Awaitable[None]] | None = None,         run_result: AgentRunResult[OutputDataT] | None = None,     ) -> None:         self._all_messages = all_messages         self._new_message_index = new_message_index          self._stream_response = stream_response         self._on_complete = on_complete         self._run_result = run_result      def all_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:         \"\"\"Return the history of _messages.          Args:             output_tool_return_content: The return content of the tool call to set in the last message.                 This provides a convenient way to modify the content of the output tool call if you want to continue                 the conversation and want to set the response to the output tool call. If `None`, the last message will                 not be modified.          Returns:             List of messages.         \"\"\"         # this is a method to be consistent with the other methods         if output_tool_return_content is not None:             raise NotImplementedError('Setting output tool return content is not supported for this result type.')         return self._all_messages      def all_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:  # pragma: no cover         \"\"\"Return all messages from [`all_messages`][pydantic_ai.result.StreamedRunResult.all_messages] as JSON bytes.          Args:             output_tool_return_content: The return content of the tool call to set in the last message.                 This provides a convenient way to modify the content of the output tool call if you want to continue                 the conversation and want to set the response to the output tool call. If `None`, the last message will                 not be modified.          Returns:             JSON bytes representing the messages.         \"\"\"         return _messages.ModelMessagesTypeAdapter.dump_json(             self.all_messages(output_tool_return_content=output_tool_return_content)         )      def new_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:         \"\"\"Return new messages associated with this run.          Messages from older runs are excluded.          Args:             output_tool_return_content: The return content of the tool call to set in the last message.                 This provides a convenient way to modify the content of the output tool call if you want to continue                 the conversation and want to set the response to the output tool call. If `None`, the last message will                 not be modified.          Returns:             List of new messages.         \"\"\"         return self.all_messages(output_tool_return_content=output_tool_return_content)[self._new_message_index :]      def new_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:  # pragma: no cover         \"\"\"Return new messages from [`new_messages`][pydantic_ai.result.StreamedRunResult.new_messages] as JSON bytes.          Args:             output_tool_return_content: The return content of the tool call to set in the last message.                 This provides a convenient way to modify the content of the output tool call if you want to continue                 the conversation and want to set the response to the output tool call. If `None`, the last message will                 not be modified.          Returns:             JSON bytes representing the new messages.         \"\"\"         return _messages.ModelMessagesTypeAdapter.dump_json(             self.new_messages(output_tool_return_content=output_tool_return_content)         )      @deprecated('`StreamedRunResult.stream` is deprecated, use `stream_output` instead.')     async def stream(self, *, debounce_by: float | None = 0.1) -> AsyncIterator[OutputDataT]:         async for output in self.stream_output(debounce_by=debounce_by):             yield output      async def stream_output(self, *, debounce_by: float | None = 0.1) -> AsyncIterator[OutputDataT]:         \"\"\"Stream the output as an async iterable.          The pydantic validator for structured data will be called in         [partial mode](https://docs.pydantic.dev/dev/concepts/experimental/#partial-validation)         on each iteration.          Args:             debounce_by: by how much (if at all) to debounce/group the output chunks by. `None` means no debouncing.                 Debouncing is particularly important for long structured outputs to reduce the overhead of                 performing validation as each token is received.          Returns:             An async iterable of the response data.         \"\"\"         if self._run_result is not None:             yield self._run_result.output             await self._marked_completed()         elif self._stream_response is not None:             async for output in self._stream_response.stream_output(debounce_by=debounce_by):                 yield output             await self._marked_completed(self.response)         else:             raise ValueError('No stream response or run result provided')  # pragma: no cover      async def stream_text(self, *, delta: bool = False, debounce_by: float | None = 0.1) -> AsyncIterator[str]:         \"\"\"Stream the text result as an async iterable.          !!! note             Result validators will NOT be called on the text result if `delta=True`.          Args:             delta: if `True`, yield each chunk of text as it is received, if `False` (default), yield the full text                 up to the current point.             debounce_by: by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.                 Debouncing is particularly important for long structured responses to reduce the overhead of                 performing validation as each token is received.         \"\"\"         if self._run_result is not None:  # pragma: no cover             # We can't really get here, as `_run_result` is only set in `run_stream` when `CallToolsNode` produces `DeferredToolRequests` output             # as a result of a tool function raising `CallDeferred` or `ApprovalRequired`.             # That'll change if we ever support something like `raise EndRun(output: OutputT)` where `OutputT` could be `str`.             if not isinstance(self._run_result.output, str):                 raise exceptions.UserError('stream_text() can only be used with text responses')             yield self._run_result.output             await self._marked_completed()         elif self._stream_response is not None:             async for text in self._stream_response.stream_text(delta=delta, debounce_by=debounce_by):                 yield text             await self._marked_completed(self.response)         else:             raise ValueError('No stream response or run result provided')  # pragma: no cover      @deprecated('`StreamedRunResult.stream_structured` is deprecated, use `stream_responses` instead.')     async def stream_structured(         self, *, debounce_by: float | None = 0.1     ) -> AsyncIterator[tuple[_messages.ModelResponse, bool]]:         async for msg, last in self.stream_responses(debounce_by=debounce_by):             yield msg, last      async def stream_responses(         self, *, debounce_by: float | None = 0.1     ) -> AsyncIterator[tuple[_messages.ModelResponse, bool]]:         \"\"\"Stream the response as an async iterable of Structured LLM Messages.          Args:             debounce_by: by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.                 Debouncing is particularly important for long structured responses to reduce the overhead of                 performing validation as each token is received.          Returns:             An async iterable of the structured response message and whether that is the last message.         \"\"\"         if self._run_result is not None:             yield self.response, True             await self._marked_completed()         elif self._stream_response is not None:             # if the message currently has any parts with content, yield before streaming             async for msg in self._stream_response.stream_responses(debounce_by=debounce_by):                 yield msg, False              msg = self.response             yield msg, True              await self._marked_completed(msg)         else:             raise ValueError('No stream response or run result provided')  # pragma: no cover      async def get_output(self) -> OutputDataT:         \"\"\"Stream the whole response, validate and return it.\"\"\"         if self._run_result is not None:             output = self._run_result.output             await self._marked_completed()             return output         elif self._stream_response is not None:             output = await self._stream_response.get_output()             await self._marked_completed(self.response)             return output         else:             raise ValueError('No stream response or run result provided')  # pragma: no cover      @property     def response(self) -> _messages.ModelResponse:         \"\"\"Return the current state of the response.\"\"\"         if self._run_result is not None:             return self._run_result.response         elif self._stream_response is not None:             return self._stream_response.get()         else:             raise ValueError('No stream response or run result provided')  # pragma: no cover      # TODO (v2): Make this a property     def usage(self) -> RunUsage:         \"\"\"Return the usage of the whole run.          !!! note             This won't return the full usage until the stream is finished.         \"\"\"         if self._run_result is not None:             return self._run_result.usage()         elif self._stream_response is not None:             return self._stream_response.usage()         else:             raise ValueError('No stream response or run result provided')  # pragma: no cover      # TODO (v2): Make this a property     def timestamp(self) -> datetime:         \"\"\"Get the timestamp of the response.\"\"\"         if self._run_result is not None:             return self._run_result.timestamp()         elif self._stream_response is not None:             return self._stream_response.timestamp()         else:             raise ValueError('No stream response or run result provided')  # pragma: no cover      @deprecated('`validate_structured_output` is deprecated, use `validate_response_output` instead.')     async def validate_structured_output(         self, message: _messages.ModelResponse, *, allow_partial: bool = False     ) -> OutputDataT:         return await self.validate_response_output(message, allow_partial=allow_partial)      async def validate_response_output(         self, message: _messages.ModelResponse, *, allow_partial: bool = False     ) -> OutputDataT:         \"\"\"Validate a structured result message.\"\"\"         if self._run_result is not None:             return self._run_result.output         elif self._stream_response is not None:             return await self._stream_response.validate_response_output(message, allow_partial=allow_partial)         else:             raise ValueError('No stream response or run result provided')  # pragma: no cover      async def _marked_completed(self, message: _messages.ModelResponse | None = None) -> None:         self.is_complete = True         if message is not None:             self._all_messages.append(message)         if self._on_complete is not None:             await self._on_complete() ``` |", "url": "https://ai.pydantic.dev/result/index.html#streamedrunresult-dataclass", "page": "result/index.html", "source_site": "pydantic_ai"}
{"title": "StreamedRunResult `dataclass`", "anchor": "streamedrunresult-dataclass", "md_text": "#### is\\_complete `class-attribute` `instance-attribute`\n\n```\nis_complete: bool = field(default=False, init=False)\n```\n\nWhether the stream has all been received.\n\nThis is set to `True` when one of\n[`stream_output`](index.html#pydantic_ai.result.StreamedRunResult.stream_output),\n[`stream_text`](index.html#pydantic_ai.result.StreamedRunResult.stream_text),\n[`stream_responses`](index.html#pydantic_ai.result.StreamedRunResult.stream_responses) or\n[`get_output`](index.html#pydantic_ai.result.StreamedRunResult.get_output) completes.\n\n#### all\\_messages\n\n```\nall_messages(\n    *, output_tool_return_content: str | None = None\n) -> list[ModelMessage]\n```\n\nReturn the history of \\_messages.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `output_tool_return_content` | `str | None` | The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the output tool call if you want to continue the conversation and want to set the response to the output tool call. If `None`, the last message will not be modified. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `list[ModelMessage]` | List of messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n|  |  |\n| --- | --- |\n| ``` 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 ``` | ``` def all_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:     \"\"\"Return the history of _messages.      Args:         output_tool_return_content: The return content of the tool call to set in the last message.             This provides a convenient way to modify the content of the output tool call if you want to continue             the conversation and want to set the response to the output tool call. If `None`, the last message will             not be modified.      Returns:         List of messages.     \"\"\"     # this is a method to be consistent with the other methods     if output_tool_return_content is not None:         raise NotImplementedError('Setting output tool return content is not supported for this result type.')     return self._all_messages ``` |\n\n#### all\\_messages\\_json\n\n```\nall_messages_json(\n    *, output_tool_return_content: str | None = None\n) -> bytes\n```\n\nReturn all messages from [`all_messages`](index.html#pydantic_ai.result.StreamedRunResult.all_messages) as JSON bytes.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `output_tool_return_content` | `str | None` | The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the output tool call if you want to continue the conversation and want to set the response to the output tool call. If `None`, the last message will not be modified. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `bytes` | JSON bytes representing the messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n|  |  |\n| --- | --- |\n| ``` 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 ``` | ``` def all_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:  # pragma: no cover     \"\"\"Return all messages from [`all_messages`][pydantic_ai.result.StreamedRunResult.all_messages] as JSON bytes.      Args:         output_tool_return_content: The return content of the tool call to set in the last message.             This provides a convenient way to modify the content of the output tool call if you want to continue             the conversation and want to set the response to the output tool call. If `None`, the last message will             not be modified.      Returns:         JSON bytes representing the messages.     \"\"\"     return _messages.ModelMessagesTypeAdapter.dump_json(         self.all_messages(output_tool_return_content=output_tool_return_content)     ) ``` |\n\n#### new\\_messages\n\n```\nnew_messages(\n    *, output_tool_return_content: str | None = None\n) -> list[ModelMessage]\n```\n\nReturn new messages associated with this run.\n\nMessages from older runs are excluded.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `output_tool_return_content` | `str | None` | The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the output tool call if you want to continue the conversation and want to set the response to the output tool call. If `None`, the last message will not be modified. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `list[ModelMessage]` | List of new messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`", "url": "https://ai.pydantic.dev/result/index.html#streamedrunresult-dataclass", "page": "result/index.html", "source_site": "pydantic_ai"}
{"title": "StreamedRunResult `dataclass`", "anchor": "streamedrunresult-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 ``` | ``` def new_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:     \"\"\"Return new messages associated with this run.      Messages from older runs are excluded.      Args:         output_tool_return_content: The return content of the tool call to set in the last message.             This provides a convenient way to modify the content of the output tool call if you want to continue             the conversation and want to set the response to the output tool call. If `None`, the last message will             not be modified.      Returns:         List of new messages.     \"\"\"     return self.all_messages(output_tool_return_content=output_tool_return_content)[self._new_message_index :] ``` |\n\n#### new\\_messages\\_json\n\n```\nnew_messages_json(\n    *, output_tool_return_content: str | None = None\n) -> bytes\n```\n\nReturn new messages from [`new_messages`](index.html#pydantic_ai.result.StreamedRunResult.new_messages) as JSON bytes.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `output_tool_return_content` | `str | None` | The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the output tool call if you want to continue the conversation and want to set the response to the output tool call. If `None`, the last message will not be modified. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `bytes` | JSON bytes representing the new messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n|  |  |\n| --- | --- |\n| ``` 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 ``` | ``` def new_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:  # pragma: no cover     \"\"\"Return new messages from [`new_messages`][pydantic_ai.result.StreamedRunResult.new_messages] as JSON bytes.      Args:         output_tool_return_content: The return content of the tool call to set in the last message.             This provides a convenient way to modify the content of the output tool call if you want to continue             the conversation and want to set the response to the output tool call. If `None`, the last message will             not be modified.      Returns:         JSON bytes representing the new messages.     \"\"\"     return _messages.ModelMessagesTypeAdapter.dump_json(         self.new_messages(output_tool_return_content=output_tool_return_content)     ) ``` |\n\n#### stream `async` `deprecated`\n\n```\nstream(\n    *, debounce_by: float | None = 0.1\n) -> AsyncIterator[OutputDataT]\n```\n\nDeprecated\n\n`StreamedRunResult.stream` is deprecated, use `stream_output` instead.\n\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n|  |  |\n| --- | --- |\n| ``` 381 382 383 384 ``` | ``` @deprecated('`StreamedRunResult.stream` is deprecated, use `stream_output` instead.') async def stream(self, *, debounce_by: float | None = 0.1) -> AsyncIterator[OutputDataT]:     async for output in self.stream_output(debounce_by=debounce_by):         yield output ``` |\n\n#### stream\\_output `async`\n\n```\nstream_output(\n    *, debounce_by: float | None = 0.1\n) -> AsyncIterator[OutputDataT]\n```\n\nStream the output as an async iterable.\n\nThe pydantic validator for structured data will be called in\n[partial mode](https://docs.pydantic.dev/dev/concepts/experimental/#partial-validation)\non each iteration.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `debounce_by` | `float | None` | by how much (if at all) to debounce/group the output chunks by. `None` means no debouncing. Debouncing is particularly important for long structured outputs to reduce the overhead of performing validation as each token is received. | `0.1` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AsyncIterator[OutputDataT]` | An async iterable of the response data. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`", "url": "https://ai.pydantic.dev/result/index.html#streamedrunresult-dataclass", "page": "result/index.html", "source_site": "pydantic_ai"}
{"title": "StreamedRunResult `dataclass`", "anchor": "streamedrunresult-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 ``` | ``` async def stream_output(self, *, debounce_by: float | None = 0.1) -> AsyncIterator[OutputDataT]:     \"\"\"Stream the output as an async iterable.      The pydantic validator for structured data will be called in     [partial mode](https://docs.pydantic.dev/dev/concepts/experimental/#partial-validation)     on each iteration.      Args:         debounce_by: by how much (if at all) to debounce/group the output chunks by. `None` means no debouncing.             Debouncing is particularly important for long structured outputs to reduce the overhead of             performing validation as each token is received.      Returns:         An async iterable of the response data.     \"\"\"     if self._run_result is not None:         yield self._run_result.output         await self._marked_completed()     elif self._stream_response is not None:         async for output in self._stream_response.stream_output(debounce_by=debounce_by):             yield output         await self._marked_completed(self.response)     else:         raise ValueError('No stream response or run result provided')  # pragma: no cover ``` |\n\n#### stream\\_text `async`\n\n```\nstream_text(\n    *, delta: bool = False, debounce_by: float | None = 0.1\n) -> AsyncIterator[str]\n```\n\nStream the text result as an async iterable.\n\nResult validators will NOT be called on the text result if `delta=True`.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `delta` | `bool` | if `True`, yield each chunk of text as it is received, if `False` (default), yield the full text up to the current point. | `False` |\n| `debounce_by` | `float | None` | by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing. Debouncing is particularly important for long structured responses to reduce the overhead of performing validation as each token is received. | `0.1` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n|  |  |\n| --- | --- |\n| ``` 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 ``` | ``` async def stream_text(self, *, delta: bool = False, debounce_by: float | None = 0.1) -> AsyncIterator[str]:     \"\"\"Stream the text result as an async iterable.      !!! note         Result validators will NOT be called on the text result if `delta=True`.      Args:         delta: if `True`, yield each chunk of text as it is received, if `False` (default), yield the full text             up to the current point.         debounce_by: by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.             Debouncing is particularly important for long structured responses to reduce the overhead of             performing validation as each token is received.     \"\"\"     if self._run_result is not None:  # pragma: no cover         # We can't really get here, as `_run_result` is only set in `run_stream` when `CallToolsNode` produces `DeferredToolRequests` output         # as a result of a tool function raising `CallDeferred` or `ApprovalRequired`.         # That'll change if we ever support something like `raise EndRun(output: OutputT)` where `OutputT` could be `str`.         if not isinstance(self._run_result.output, str):             raise exceptions.UserError('stream_text() can only be used with text responses')         yield self._run_result.output         await self._marked_completed()     elif self._stream_response is not None:         async for text in self._stream_response.stream_text(delta=delta, debounce_by=debounce_by):             yield text         await self._marked_completed(self.response)     else:         raise ValueError('No stream response or run result provided')  # pragma: no cover ``` |\n\n#### stream\\_structured `async` `deprecated`\n\n```\nstream_structured(\n    *, debounce_by: float | None = 0.1\n) -> AsyncIterator[tuple[ModelResponse, bool]]\n```\n\nDeprecated\n\n`StreamedRunResult.stream_structured` is deprecated, use `stream_responses` instead.\n\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n|  |  |\n| --- | --- |\n| ``` 439 440 441 442 443 444 ``` | ``` @deprecated('`StreamedRunResult.stream_structured` is deprecated, use `stream_responses` instead.') async def stream_structured(     self, *, debounce_by: float | None = 0.1 ) -> AsyncIterator[tuple[_messages.ModelResponse, bool]]:     async for msg, last in self.stream_responses(debounce_by=debounce_by):         yield msg, last ``` |\n\n#### stream\\_responses `async`\n\n```\nstream_responses(\n    *, debounce_by: float | None = 0.1\n) -> AsyncIterator[tuple[ModelResponse, bool]]\n```\n\nStream the response as an async iterable of Structured LLM Messages.\n\nParameters:", "url": "https://ai.pydantic.dev/result/index.html#streamedrunresult-dataclass", "page": "result/index.html", "source_site": "pydantic_ai"}
{"title": "StreamedRunResult `dataclass`", "anchor": "streamedrunresult-dataclass", "md_text": "| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `debounce_by` | `float | None` | by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing. Debouncing is particularly important for long structured responses to reduce the overhead of performing validation as each token is received. | `0.1` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AsyncIterator[tuple[ModelResponse, bool]]` | An async iterable of the structured response message and whether that is the last message. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n|  |  |\n| --- | --- |\n| ``` 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 ``` | ``` async def stream_responses(     self, *, debounce_by: float | None = 0.1 ) -> AsyncIterator[tuple[_messages.ModelResponse, bool]]:     \"\"\"Stream the response as an async iterable of Structured LLM Messages.      Args:         debounce_by: by how much (if at all) to debounce/group the response chunks by. `None` means no debouncing.             Debouncing is particularly important for long structured responses to reduce the overhead of             performing validation as each token is received.      Returns:         An async iterable of the structured response message and whether that is the last message.     \"\"\"     if self._run_result is not None:         yield self.response, True         await self._marked_completed()     elif self._stream_response is not None:         # if the message currently has any parts with content, yield before streaming         async for msg in self._stream_response.stream_responses(debounce_by=debounce_by):             yield msg, False          msg = self.response         yield msg, True          await self._marked_completed(msg)     else:         raise ValueError('No stream response or run result provided')  # pragma: no cover ``` |\n\n#### get\\_output `async`\n\n```\nget_output() -> OutputDataT\n```\n\nStream the whole response, validate and return it.\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n|  |  |\n| --- | --- |\n| ``` 474 475 476 477 478 479 480 481 482 483 484 485 ``` | ``` async def get_output(self) -> OutputDataT:     \"\"\"Stream the whole response, validate and return it.\"\"\"     if self._run_result is not None:         output = self._run_result.output         await self._marked_completed()         return output     elif self._stream_response is not None:         output = await self._stream_response.get_output()         await self._marked_completed(self.response)         return output     else:         raise ValueError('No stream response or run result provided')  # pragma: no cover ``` |\n\n#### response `property`\n\n```\nresponse: ModelResponse\n```\n\nReturn the current state of the response.\n\n#### usage\n\n```\nusage() -> RunUsage\n```\n\nReturn the usage of the whole run.\n\nThis won't return the full usage until the stream is finished.\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n|  |  |\n| --- | --- |\n| ``` 498 499 500 501 502 503 504 505 506 507 508 509 ``` | ``` def usage(self) -> RunUsage:     \"\"\"Return the usage of the whole run.      !!! note         This won't return the full usage until the stream is finished.     \"\"\"     if self._run_result is not None:         return self._run_result.usage()     elif self._stream_response is not None:         return self._stream_response.usage()     else:         raise ValueError('No stream response or run result provided')  # pragma: no cover ``` |\n\n#### timestamp\n\n```\ntimestamp() -> datetime\n```\n\nGet the timestamp of the response.\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n|  |  |\n| --- | --- |\n| ``` 512 513 514 515 516 517 518 519 ``` | ``` def timestamp(self) -> datetime:     \"\"\"Get the timestamp of the response.\"\"\"     if self._run_result is not None:         return self._run_result.timestamp()     elif self._stream_response is not None:         return self._stream_response.timestamp()     else:         raise ValueError('No stream response or run result provided')  # pragma: no cover ``` |\n\n#### validate\\_structured\\_output `async` `deprecated`\n\n```\nvalidate_structured_output(\n    message: ModelResponse, *, allow_partial: bool = False\n) -> OutputDataT\n```\n\nDeprecated\n\n`validate_structured_output` is deprecated, use `validate_response_output` instead.\n\n\nSource code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n|  |  |\n| --- | --- |\n| ``` 521 522 523 524 525 ``` | ``` @deprecated('`validate_structured_output` is deprecated, use `validate_response_output` instead.') async def validate_structured_output(     self, message: _messages.ModelResponse, *, allow_partial: bool = False ) -> OutputDataT:     return await self.validate_response_output(message, allow_partial=allow_partial) ``` |\n\n#### validate\\_response\\_output `async`\n\n```\nvalidate_response_output(\n    message: ModelResponse, *, allow_partial: bool = False\n) -> OutputDataT\n```\n\nValidate a structured result message.", "url": "https://ai.pydantic.dev/result/index.html#streamedrunresult-dataclass", "page": "result/index.html", "source_site": "pydantic_ai"}
{"title": "StreamedRunResult `dataclass`", "anchor": "streamedrunresult-dataclass", "md_text": "Source code in `pydantic_ai_slim/pydantic_ai/result.py`\n\n|  |  |\n| --- | --- |\n| ``` 527 528 529 530 531 532 533 534 535 536 ``` | ``` async def validate_response_output(     self, message: _messages.ModelResponse, *, allow_partial: bool = False ) -> OutputDataT:     \"\"\"Validate a structured result message.\"\"\"     if self._run_result is not None:         return self._run_result.output     elif self._stream_response is not None:         return await self._stream_response.validate_response_output(message, allow_partial=allow_partial)     else:         raise ValueError('No stream response or run result provided')  # pragma: no cover ``` |", "url": "https://ai.pydantic.dev/result/index.html#streamedrunresult-dataclass", "page": "result/index.html", "source_site": "pydantic_ai"}
{"title": "MCPServer", "anchor": "mcpserver", "md_text": "Bases: `AbstractToolset[Any]`, `ABC`\n\nBase class for attaching agents to MCP servers.\n\nSee <https://modelcontextprotocol.io> for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`", "url": "https://ai.pydantic.dev/mcp/index.html#mcpserver", "page": "mcp/index.html", "source_site": "pydantic_ai"}
{"title": "MCPServer", "anchor": "mcpserver", "md_text": "|  |  |\n| --- | --- |\n| ```  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 ``` | ``` class MCPServer(AbstractToolset[Any], ABC):     \"\"\"Base class for attaching agents to MCP servers.      See <https://modelcontextprotocol.io> for more information.     \"\"\"      tool_prefix: str | None     \"\"\"A prefix to add to all tools that are registered with the server.      If not empty, will include a trailing underscore(`_`).      e.g. if `tool_prefix='foo'`, then a tool named `bar` will be registered as `foo_bar`     \"\"\"      log_level: mcp_types.LoggingLevel | None     \"\"\"The log level to set when connecting to the server, if any.      See <https://modelcontextprotocol.io/specification/2025-03-26/server/utilities/logging#logging> for more details.      If `None`, no log level will be set.     \"\"\"      log_handler: LoggingFnT | None     \"\"\"A handler for logging messages from the server.\"\"\"      timeout: float     \"\"\"The timeout in seconds to wait for the client to initialize.\"\"\"      read_timeout: float     \"\"\"Maximum time in seconds to wait for new messages before timing out.      This timeout applies to the long-lived connection after it's established.     If no new messages are received within this time, the connection will be considered stale     and may be closed. Defaults to 5 minutes (300 seconds).     \"\"\"      process_tool_call: ProcessToolCallback | None     \"\"\"Hook to customize tool calling and optionally pass extra metadata.\"\"\"      allow_sampling: bool     \"\"\"Whether to allow MCP sampling through this client.\"\"\"      sampling_model: models.Model | None     \"\"\"The model to use for sampling.\"\"\"      max_retries: int     \"\"\"The maximum number of times to retry a tool call.\"\"\"      elicitation_callback: ElicitationFnT | None = None     \"\"\"Callback function to handle elicitation requests from the server.\"\"\"      _id: str | None      _enter_lock: Lock = field(compare=False)     _running_count: int     _exit_stack: AsyncExitStack | None      _client: ClientSession     _read_stream: MemoryObjectReceiveStream[SessionMessage | Exception]     _write_stream: MemoryObjectSendStream[SessionMessage]     _server_info: mcp_types.Implementation      def __init__(         self,         tool_prefix: str | None = None,         log_level: mcp_types.LoggingLevel | None = None,         log_handler: LoggingFnT | None = None,         timeout: float = 5,         read_timeout: float = 5 * 60,         process_tool_call: ProcessToolCallback | None = None,         allow_sampling: bool = True,         sampling_model: models.Model | None = None,         max_retries: int = 1,         elicitation_callback: ElicitationFnT | None = None,         *,         id: str | None = None,     ):         self.tool_prefix = tool_prefix         self.log_level = log_level         self.log_handler = log_handler         self.timeout = timeout         self.read_timeout = read_timeout         self.process_tool_call = process_tool_call         self.allow_sampling = allow_sampling         self.sampling_model = sampling_model         self.max_retries = max_retries         self.elicitation_callback = elicitation_callback          self._id = id or tool_prefix          self.__post_init__()      def __post_init__(self):         self._enter_lock = Lock()         self._running_count = 0         self._exit_stack = None      @abstractmethod     @asynccontextmanager     async def client_streams(         self,     ) -> AsyncIterator[         tuple[             MemoryObjectReceiveStream[SessionMessage | Exception],             MemoryObjectSendStream[SessionMessage],         ]     ]:         \"\"\"Create the streams for the MCP server.\"\"\"         raise NotImplementedError('MCP Server subclasses must implement this method.')         yield      @property     def id(self) -> str | None:         return self._id      @id.setter     def id(self, value: str | None):         self._id = value      @property     def label(self) -> str:         if self.id:             return super().label  # pragma: no cover         else:             return repr(self)      @property     def tool_name_conflict_hint(self) -> str:         return 'Set the `tool_prefix` attribute to avoid name conflicts.'      @property     def server_info(self) -> mcp_types.Implementation:         \"\"\"Access the information send by the MCP server during initialization.\"\"\"         if getattr(self, '_server_info', None) is None:             raise AttributeError(                 f'The `{self.__class__.__name__}.server_info` is only instantiated after initialization.'             )         return self._server_info      async def list_tools(self) -> list[mcp_types.Tool]:         \"\"\"Retrieve tools that are currently active on the server.          Note:         - We don't cache tools as they might change.         - We also don't subscribe to the server to avoid complexity.         \"\"\"         async with self:  # Ensure server is running             result = await self._client.list_tools()         return result.tools      async def direct_call_tool(         self,         name: str,         args: dict[str, Any],         metadata: dict[str, Any] | None = None,     ) -> ToolResult:         \"\"\"Call a tool on the server.          Args:             name: The name of the tool to call.             args: The arguments to pass to the tool.             metadata: Request-level metadata (optional)          Returns:             The result of the tool call.          Raises:             ModelRetry: If the tool call fails.         \"\"\"         async with self:  # Ensure server is running             try:                 result = await self._client.send_request(                     mcp_types.ClientRequest(                         mcp_types.CallToolRequest(                             method='tools/call',                             params=mcp_types.CallToolRequestParams(                                 name=name,                                 arguments=args,                                 _meta=mcp_types.RequestParams.Meta(**metadata) if metadata else None,                             ),                         )                     ),                     mcp_types.CallToolResult,                 )             except McpError as e:                 raise exceptions.ModelRetry(e.error.message)          if result.isError:             message: str | None = None             if result.content:  # pragma: no branch                 text_parts = [part.text for part in result.content if isinstance(part, mcp_types.TextContent)]                 message = '\\n'.join(text_parts)              raise exceptions.ModelRetry(message or 'MCP tool call failed')          # Prefer structured content if there are only text parts, which per the docs would contain the JSON-encoded structured content for backward compatibility.         # See https://github.com/modelcontextprotocol/python-sdk#structured-output         if (structured := result.structuredContent) and not any(             not isinstance(part, mcp_types.TextContent) for part in result.content         ):             # The MCP SDK wraps primitives and generic types like list in a `result` key, but we want to use the raw value returned by the tool function.             # See https://github.com/modelcontextprotocol/python-sdk#structured-output             if isinstance(structured, dict) and len(structured) == 1 and 'result' in structured:                 return structured['result']             return structured          mapped = [await self._map_tool_result_part(part) for part in result.content]         return mapped[0] if len(mapped) == 1 else mapped      async def call_tool(         self,         name: str,         tool_args: dict[str, Any],         ctx: RunContext[Any],         tool: ToolsetTool[Any],     ) -> ToolResult:         if self.tool_prefix:             name = name.removeprefix(f'{self.tool_prefix}_')             ctx = replace(ctx, tool_name=name)          if self.process_tool_call is not None:             return await self.process_tool_call(ctx, self.direct_call_tool, name, tool_args)         else:             return await self.direct_call_tool(name, tool_args)      async def get_tools(self, ctx: RunContext[Any]) -> dict[str, ToolsetTool[Any]]:         return {             name: self.tool_for_tool_def(                 ToolDefinition(                     name=name,                     description=mcp_tool.description,                     parameters_json_schema=mcp_tool.inputSchema,                     metadata={                         'meta': mcp_tool.meta,                         'annotations': mcp_tool.annotations.model_dump() if mcp_tool.annotations else None,                         'output_schema': mcp_tool.outputSchema or None,                     },                 ),             )             for mcp_tool in await self.list_tools()             if (name := f'{self.tool_prefix}_{mcp_tool.name}' if self.tool_prefix else mcp_tool.name)         }      def tool_for_tool_def(self, tool_def: ToolDefinition) -> ToolsetTool[Any]:         return ToolsetTool(             toolset=self,             tool_def=tool_def,             max_retries=self.max_retries,             args_validator=TOOL_SCHEMA_VALIDATOR,         )      async def __aenter__(self) -> Self:         \"\"\"Enter the MCP server context.          This will initialize the connection to the server.         If this server is an [`MCPServerStdio`][pydantic_ai.mcp.MCPServerStdio], the server will first be started as a subprocess.          This is a no-op if the MCP server has already been entered.         \"\"\"         async with self._enter_lock:             if self._running_count == 0:                 async with AsyncExitStack() as exit_stack:                     self._read_stream, self._write_stream = await exit_stack.enter_async_context(self.client_streams())                     client = ClientSession(                         read_stream=self._read_stream,                         write_stream=self._write_stream,                         sampling_callback=self._sampling_callback if self.allow_sampling else None,                         elicitation_callback=self.elicitation_callback,                         logging_callback=self.log_handler,                         read_timeout_seconds=timedelta(seconds=self.read_timeout),                     )                     self._client = await exit_stack.enter_async_context(client)                      with anyio.fail_after(self.timeout):                         result = await self._client.initialize()                         self._server_info = result.serverInfo                         if log_level := self.log_level:                             await self._client.set_logging_level(log_level)                      self._exit_stack = exit_stack.pop_all()             self._running_count += 1         return self      async def __aexit__(self, *args: Any) -> bool | None:         if self._running_count == 0:             raise ValueError('MCPServer.__aexit__ called more times than __aenter__')         async with self._enter_lock:             self._running_count -= 1             if self._running_count == 0 and self._exit_stack is not None:                 await self._exit_stack.aclose()                 self._exit_stack = None      @property     def is_running(self) -> bool:         \"\"\"Check if the MCP server is running.\"\"\"         return bool(self._running_count)      async def _sampling_callback(         self, context: RequestContext[ClientSession, Any], params: mcp_types.CreateMessageRequestParams     ) -> mcp_types.CreateMessageResult | mcp_types.ErrorData:         \"\"\"MCP sampling callback.\"\"\"         if self.sampling_model is None:             raise ValueError('Sampling model is not set')  # pragma: no cover          pai_messages = _mcp.map_from_mcp_params(params)         model_settings = models.ModelSettings()         if max_tokens := params.maxTokens:  # pragma: no branch             model_settings['max_tokens'] = max_tokens         if temperature := params.temperature:  # pragma: no branch             model_settings['temperature'] = temperature         if stop_sequences := params.stopSequences:  # pragma: no branch             model_settings['stop_sequences'] = stop_sequences          model_response = await model_request(self.sampling_model, pai_messages, model_settings=model_settings)         return mcp_types.CreateMessageResult(             role='assistant',             content=_mcp.map_from_model_response(model_response),             model=self.sampling_model.model_name,         )      async def _map_tool_result_part(         self, part: mcp_types.ContentBlock     ) -> str | messages.BinaryContent | dict[str, Any] | list[Any]:         # See https://github.com/jlowin/fastmcp/blob/main/docs/servers/tools.mdx#return-values          if isinstance(part, mcp_types.TextContent):             text = part.text             if text.startswith(('[', '{')):                 try:                     return pydantic_core.from_json(text)                 except ValueError:                     pass             return text         elif isinstance(part, mcp_types.ImageContent):             return messages.BinaryContent(data=base64.b64decode(part.data), media_type=part.mimeType)         elif isinstance(part, mcp_types.AudioContent):             # NOTE: The FastMCP server doesn't support audio content.             # See <https://github.com/modelcontextprotocol/python-sdk/issues/952> for more details.             return messages.BinaryContent(                 data=base64.b64decode(part.data), media_type=part.mimeType             )  # pragma: no cover         elif isinstance(part, mcp_types.EmbeddedResource):             resource = part.resource             return self._get_content(resource)         elif isinstance(part, mcp_types.ResourceLink):             resource_result: mcp_types.ReadResourceResult = await self._client.read_resource(part.uri)             return (                 self._get_content(resource_result.contents[0])                 if len(resource_result.contents) == 1                 else [self._get_content(resource) for resource in resource_result.contents]             )         else:             assert_never(part)      def _get_content(         self, resource: mcp_types.TextResourceContents | mcp_types.BlobResourceContents     ) -> str | messages.BinaryContent:         if isinstance(resource, mcp_types.TextResourceContents):             return resource.text         elif isinstance(resource, mcp_types.BlobResourceContents):             return messages.BinaryContent(                 data=base64.b64decode(resource.blob), media_type=resource.mimeType or 'application/octet-stream'             )         else:             assert_never(resource)      def __eq__(self, value: object, /) -> bool:         return isinstance(value, MCPServer) and self.id == value.id and self.tool_prefix == value.tool_prefix ``` |", "url": "https://ai.pydantic.dev/mcp/index.html#mcpserver", "page": "mcp/index.html", "source_site": "pydantic_ai"}
{"title": "MCPServer", "anchor": "mcpserver", "md_text": "#### tool\\_prefix `instance-attribute`\n\n```\ntool_prefix: str | None = tool_prefix\n```\n\nA prefix to add to all tools that are registered with the server.\n\nIf not empty, will include a trailing underscore(`_`).\n\ne.g. if `tool_prefix='foo'`, then a tool named `bar` will be registered as `foo_bar`\n\n#### log\\_level `instance-attribute`\n\n```\nlog_level: LoggingLevel | None = log_level\n```\n\nThe log level to set when connecting to the server, if any.\n\nSee <https://modelcontextprotocol.io/specification/2025-03-26/server/utilities/logging#logging> for more details.\n\nIf `None`, no log level will be set.\n\n#### log\\_handler `instance-attribute`\n\n```\nlog_handler: LoggingFnT | None = log_handler\n```\n\nA handler for logging messages from the server.\n\n#### timeout `instance-attribute`\n\n```\ntimeout: float = timeout\n```\n\nThe timeout in seconds to wait for the client to initialize.\n\n#### read\\_timeout `instance-attribute`\n\n```\nread_timeout: float = read_timeout\n```\n\nMaximum time in seconds to wait for new messages before timing out.\n\nThis timeout applies to the long-lived connection after it's established.\nIf no new messages are received within this time, the connection will be considered stale\nand may be closed. Defaults to 5 minutes (300 seconds).\n\n#### process\\_tool\\_call `instance-attribute`\n\n```\nprocess_tool_call: ProcessToolCallback | None = (\n    process_tool_call\n)\n```\n\nHook to customize tool calling and optionally pass extra metadata.\n\n#### allow\\_sampling `instance-attribute`\n\n```\nallow_sampling: bool = allow_sampling\n```\n\nWhether to allow MCP sampling through this client.\n\n#### sampling\\_model `instance-attribute`\n\n```\nsampling_model: Model | None = sampling_model\n```\n\nThe model to use for sampling.\n\n#### max\\_retries `instance-attribute`\n\n```\nmax_retries: int = max_retries\n```\n\nThe maximum number of times to retry a tool call.\n\n#### elicitation\\_callback `class-attribute` `instance-attribute`\n\n```\nelicitation_callback: ElicitationFnT | None = (\n    elicitation_callback\n)\n```\n\nCallback function to handle elicitation requests from the server.\n\n#### client\\_streams `abstractmethod` `async`\n\n```\nclient_streams() -> AsyncIterator[\n    tuple[\n        MemoryObjectReceiveStream[\n            SessionMessage | Exception\n        ],\n        MemoryObjectSendStream[SessionMessage],\n    ]\n]\n```\n\nCreate the streams for the MCP server.\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n|  |  |\n| --- | --- |\n| ``` 152 153 154 155 156 157 158 159 160 161 162 163 164 ``` | ``` @abstractmethod @asynccontextmanager async def client_streams(     self, ) -> AsyncIterator[     tuple[         MemoryObjectReceiveStream[SessionMessage | Exception],         MemoryObjectSendStream[SessionMessage],     ] ]:     \"\"\"Create the streams for the MCP server.\"\"\"     raise NotImplementedError('MCP Server subclasses must implement this method.')     yield ``` |\n\n#### server\\_info `property`\n\n```\nserver_info: Implementation\n```\n\nAccess the information send by the MCP server during initialization.\n\n#### list\\_tools `async`\n\n```\nlist_tools() -> list[Tool]\n```\n\nRetrieve tools that are currently active on the server.\n\nNote:\n- We don't cache tools as they might change.\n- We also don't subscribe to the server to avoid complexity.\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n|  |  |\n| --- | --- |\n| ``` 194 195 196 197 198 199 200 201 202 203 ``` | ``` async def list_tools(self) -> list[mcp_types.Tool]:     \"\"\"Retrieve tools that are currently active on the server.      Note:     - We don't cache tools as they might change.     - We also don't subscribe to the server to avoid complexity.     \"\"\"     async with self:  # Ensure server is running         result = await self._client.list_tools()     return result.tools ``` |\n\n#### direct\\_call\\_tool `async`\n\n```\ndirect_call_tool(\n    name: str,\n    args: dict[str, Any],\n    metadata: dict[str, Any] | None = None,\n) -> ToolResult\n```\n\nCall a tool on the server.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `name` | `str` | The name of the tool to call. | *required* |\n| `args` | `dict[str, Any]` | The arguments to pass to the tool. | *required* |\n| `metadata` | `dict[str, Any] | None` | Request-level metadata (optional) | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `ToolResult` | The result of the tool call. |\n\nRaises:\n\n| Type | Description |\n| --- | --- |\n| `ModelRetry` | If the tool call fails. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`", "url": "https://ai.pydantic.dev/mcp/index.html#mcpserver", "page": "mcp/index.html", "source_site": "pydantic_ai"}
{"title": "MCPServer", "anchor": "mcpserver", "md_text": "|  |  |\n| --- | --- |\n| ``` 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 ``` | ``` async def direct_call_tool(     self,     name: str,     args: dict[str, Any],     metadata: dict[str, Any] | None = None, ) -> ToolResult:     \"\"\"Call a tool on the server.      Args:         name: The name of the tool to call.         args: The arguments to pass to the tool.         metadata: Request-level metadata (optional)      Returns:         The result of the tool call.      Raises:         ModelRetry: If the tool call fails.     \"\"\"     async with self:  # Ensure server is running         try:             result = await self._client.send_request(                 mcp_types.ClientRequest(                     mcp_types.CallToolRequest(                         method='tools/call',                         params=mcp_types.CallToolRequestParams(                             name=name,                             arguments=args,                             _meta=mcp_types.RequestParams.Meta(**metadata) if metadata else None,                         ),                     )                 ),                 mcp_types.CallToolResult,             )         except McpError as e:             raise exceptions.ModelRetry(e.error.message)      if result.isError:         message: str | None = None         if result.content:  # pragma: no branch             text_parts = [part.text for part in result.content if isinstance(part, mcp_types.TextContent)]             message = '\\n'.join(text_parts)          raise exceptions.ModelRetry(message or 'MCP tool call failed')      # Prefer structured content if there are only text parts, which per the docs would contain the JSON-encoded structured content for backward compatibility.     # See https://github.com/modelcontextprotocol/python-sdk#structured-output     if (structured := result.structuredContent) and not any(         not isinstance(part, mcp_types.TextContent) for part in result.content     ):         # The MCP SDK wraps primitives and generic types like list in a `result` key, but we want to use the raw value returned by the tool function.         # See https://github.com/modelcontextprotocol/python-sdk#structured-output         if isinstance(structured, dict) and len(structured) == 1 and 'result' in structured:             return structured['result']         return structured      mapped = [await self._map_tool_result_part(part) for part in result.content]     return mapped[0] if len(mapped) == 1 else mapped ``` |\n\n#### \\_\\_aenter\\_\\_ `async`\n\n```\n__aenter__() -> Self\n```\n\nEnter the MCP server context.\n\nThis will initialize the connection to the server.\nIf this server is an [`MCPServerStdio`](index.html#pydantic_ai.mcp.MCPServerStdio), the server will first be started as a subprocess.\n\nThis is a no-op if the MCP server has already been entered.\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n|  |  |\n| --- | --- |\n| ``` 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 ``` | ``` async def __aenter__(self) -> Self:     \"\"\"Enter the MCP server context.      This will initialize the connection to the server.     If this server is an [`MCPServerStdio`][pydantic_ai.mcp.MCPServerStdio], the server will first be started as a subprocess.      This is a no-op if the MCP server has already been entered.     \"\"\"     async with self._enter_lock:         if self._running_count == 0:             async with AsyncExitStack() as exit_stack:                 self._read_stream, self._write_stream = await exit_stack.enter_async_context(self.client_streams())                 client = ClientSession(                     read_stream=self._read_stream,                     write_stream=self._write_stream,                     sampling_callback=self._sampling_callback if self.allow_sampling else None,                     elicitation_callback=self.elicitation_callback,                     logging_callback=self.log_handler,                     read_timeout_seconds=timedelta(seconds=self.read_timeout),                 )                 self._client = await exit_stack.enter_async_context(client)                  with anyio.fail_after(self.timeout):                     result = await self._client.initialize()                     self._server_info = result.serverInfo                     if log_level := self.log_level:                         await self._client.set_logging_level(log_level)                  self._exit_stack = exit_stack.pop_all()         self._running_count += 1     return self ``` |\n\n#### is\\_running `property`\n\n```\nis_running: bool\n```\n\nCheck if the MCP server is running.", "url": "https://ai.pydantic.dev/mcp/index.html#mcpserver", "page": "mcp/index.html", "source_site": "pydantic_ai"}
{"title": "MCPServerStdio", "anchor": "mcpserverstdio", "md_text": "Bases: `MCPServer`\n\nRuns an MCP server in a subprocess and communicates with it over stdin/stdout.\n\nThis class implements the stdio transport from the MCP specification.\nSee <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio> for more information.\n\nUsing this class as an async context manager will start the server as a subprocess when entering the context,\nand stop it when exiting the context.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStdio\n\nserver = MCPServerStdio(  # (1)!\n    'uv', args=['run', 'mcp-run-python', 'stdio'], timeout=10\n)\nagent = Agent('openai:gpt-4o', toolsets=[server])\n```\n\n1. See [MCP Run Python](https://github.com/pydantic/mcp-run-python) for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`", "url": "https://ai.pydantic.dev/mcp/index.html#mcpserverstdio", "page": "mcp/index.html", "source_site": "pydantic_ai"}
{"title": "MCPServerStdio", "anchor": "mcpserverstdio", "md_text": "|  |  |\n| --- | --- |\n| ``` 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 ``` | ``` class MCPServerStdio(MCPServer):     \"\"\"Runs an MCP server in a subprocess and communicates with it over stdin/stdout.      This class implements the stdio transport from the MCP specification.     See <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio> for more information.      !!! note         Using this class as an async context manager will start the server as a subprocess when entering the context,         and stop it when exiting the context.      Example:     ```python {py=\"3.10\"}     from pydantic_ai import Agent     from pydantic_ai.mcp import MCPServerStdio      server = MCPServerStdio(  # (1)!         'uv', args=['run', 'mcp-run-python', 'stdio'], timeout=10     )     agent = Agent('openai:gpt-4o', toolsets=[server])     ```      1. See [MCP Run Python](https://github.com/pydantic/mcp-run-python) for more information.     \"\"\"      command: str     \"\"\"The command to run.\"\"\"      args: Sequence[str]     \"\"\"The arguments to pass to the command.\"\"\"      env: dict[str, str] | None     \"\"\"The environment variables the CLI server will have access to.      By default the subprocess will not inherit any environment variables from the parent process.     If you want to inherit the environment variables from the parent process, use `env=os.environ`.     \"\"\"      cwd: str | Path | None     \"\"\"The working directory to use when spawning the process.\"\"\"      # last fields are re-defined from the parent class so they appear as fields     tool_prefix: str | None     log_level: mcp_types.LoggingLevel | None     log_handler: LoggingFnT | None     timeout: float     read_timeout: float     process_tool_call: ProcessToolCallback | None     allow_sampling: bool     sampling_model: models.Model | None     max_retries: int     elicitation_callback: ElicitationFnT | None = None      def __init__(         self,         command: str,         args: Sequence[str],         *,         env: dict[str, str] | None = None,         cwd: str | Path | None = None,         tool_prefix: str | None = None,         log_level: mcp_types.LoggingLevel | None = None,         log_handler: LoggingFnT | None = None,         timeout: float = 5,         read_timeout: float = 5 * 60,         process_tool_call: ProcessToolCallback | None = None,         allow_sampling: bool = True,         sampling_model: models.Model | None = None,         max_retries: int = 1,         elicitation_callback: ElicitationFnT | None = None,         id: str | None = None,     ):         \"\"\"Build a new MCP server.          Args:             command: The command to run.             args: The arguments to pass to the command.             env: The environment variables to set in the subprocess.             cwd: The working directory to use when spawning the process.             tool_prefix: A prefix to add to all tools that are registered with the server.             log_level: The log level to set when connecting to the server, if any.             log_handler: A handler for logging messages from the server.             timeout: The timeout in seconds to wait for the client to initialize.             read_timeout: Maximum time in seconds to wait for new messages before timing out.             process_tool_call: Hook to customize tool calling and optionally pass extra metadata.             allow_sampling: Whether to allow MCP sampling through this client.             sampling_model: The model to use for sampling.             max_retries: The maximum number of times to retry a tool call.             elicitation_callback: Callback function to handle elicitation requests from the server.             id: An optional unique ID for the MCP server. An MCP server needs to have an ID in order to be used in a durable execution environment like Temporal, in which case the ID will be used to identify the server's activities within the workflow.         \"\"\"         self.command = command         self.args = args         self.env = env         self.cwd = cwd          super().__init__(             tool_prefix,             log_level,             log_handler,             timeout,             read_timeout,             process_tool_call,             allow_sampling,             sampling_model,             max_retries,             elicitation_callback,             id=id,         )      @classmethod     def __get_pydantic_core_schema__(cls, _: Any, __: Any) -> CoreSchema:         return core_schema.no_info_after_validator_function(             lambda dct: MCPServerStdio(**dct),             core_schema.typed_dict_schema(                 {                     'command': core_schema.typed_dict_field(core_schema.str_schema()),                     'args': core_schema.typed_dict_field(core_schema.list_schema(core_schema.str_schema())),                     'env': core_schema.typed_dict_field(                         core_schema.dict_schema(core_schema.str_schema(), core_schema.str_schema()),                         required=False,                     ),                 }             ),         )      @asynccontextmanager     async def client_streams(         self,     ) -> AsyncIterator[         tuple[             MemoryObjectReceiveStream[SessionMessage | Exception],             MemoryObjectSendStream[SessionMessage],         ]     ]:         server = StdioServerParameters(command=self.command, args=list(self.args), env=self.env, cwd=self.cwd)         async with stdio_client(server=server) as (read_stream, write_stream):             yield read_stream, write_stream      def __repr__(self) -> str:         repr_args = [             f'command={self.command!r}',             f'args={self.args!r}',         ]         if self.id:             repr_args.append(f'id={self.id!r}')  # pragma: lax no cover         return f'{self.__class__.__name__}({\", \".join(repr_args)})'      def __eq__(self, value: object, /) -> bool:         return (             super().__eq__(value)             and isinstance(value, MCPServerStdio)             and self.command == value.command             and self.args == value.args             and self.env == value.env             and self.cwd == value.cwd         ) ``` |", "url": "https://ai.pydantic.dev/mcp/index.html#mcpserverstdio", "page": "mcp/index.html", "source_site": "pydantic_ai"}
{"title": "MCPServerStdio", "anchor": "mcpserverstdio", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    command: str,\n    args: Sequence[str],\n    *,\n    env: dict[str, str] | None = None,\n    cwd: str | Path | None = None,\n    tool_prefix: str | None = None,\n    log_level: LoggingLevel | None = None,\n    log_handler: LoggingFnT | None = None,\n    timeout: float = 5,\n    read_timeout: float = 5 * 60,\n    process_tool_call: ProcessToolCallback | None = None,\n    allow_sampling: bool = True,\n    sampling_model: Model | None = None,\n    max_retries: int = 1,\n    elicitation_callback: ElicitationFnT | None = None,\n    id: str | None = None\n)\n```\n\nBuild a new MCP server.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `command` | `str` | The command to run. | *required* |\n| `args` | `Sequence[str]` | The arguments to pass to the command. | *required* |\n| `env` | `dict[str, str] | None` | The environment variables to set in the subprocess. | `None` |\n| `cwd` | `str | Path | None` | The working directory to use when spawning the process. | `None` |\n| `tool_prefix` | `str | None` | A prefix to add to all tools that are registered with the server. | `None` |\n| `log_level` | `LoggingLevel | None` | The log level to set when connecting to the server, if any. | `None` |\n| `log_handler` | `LoggingFnT | None` | A handler for logging messages from the server. | `None` |\n| `timeout` | `float` | The timeout in seconds to wait for the client to initialize. | `5` |\n| `read_timeout` | `float` | Maximum time in seconds to wait for new messages before timing out. | `5 * 60` |\n| `process_tool_call` | `ProcessToolCallback | None` | Hook to customize tool calling and optionally pass extra metadata. | `None` |\n| `allow_sampling` | `bool` | Whether to allow MCP sampling through this client. | `True` |\n| `sampling_model` | `Model | None` | The model to use for sampling. | `None` |\n| `max_retries` | `int` | The maximum number of times to retry a tool call. | `1` |\n| `elicitation_callback` | `ElicitationFnT | None` | Callback function to handle elicitation requests from the server. | `None` |\n| `id` | `str | None` | An optional unique ID for the MCP server. An MCP server needs to have an ID in order to be used in a durable execution environment like Temporal, in which case the ID will be used to identify the server's activities within the workflow. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`", "url": "https://ai.pydantic.dev/mcp/index.html#mcpserverstdio", "page": "mcp/index.html", "source_site": "pydantic_ai"}
{"title": "MCPServerStdio", "anchor": "mcpserverstdio", "md_text": "|  |  |\n| --- | --- |\n| ``` 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 ``` | ``` def __init__(     self,     command: str,     args: Sequence[str],     *,     env: dict[str, str] | None = None,     cwd: str | Path | None = None,     tool_prefix: str | None = None,     log_level: mcp_types.LoggingLevel | None = None,     log_handler: LoggingFnT | None = None,     timeout: float = 5,     read_timeout: float = 5 * 60,     process_tool_call: ProcessToolCallback | None = None,     allow_sampling: bool = True,     sampling_model: models.Model | None = None,     max_retries: int = 1,     elicitation_callback: ElicitationFnT | None = None,     id: str | None = None, ):     \"\"\"Build a new MCP server.      Args:         command: The command to run.         args: The arguments to pass to the command.         env: The environment variables to set in the subprocess.         cwd: The working directory to use when spawning the process.         tool_prefix: A prefix to add to all tools that are registered with the server.         log_level: The log level to set when connecting to the server, if any.         log_handler: A handler for logging messages from the server.         timeout: The timeout in seconds to wait for the client to initialize.         read_timeout: Maximum time in seconds to wait for new messages before timing out.         process_tool_call: Hook to customize tool calling and optionally pass extra metadata.         allow_sampling: Whether to allow MCP sampling through this client.         sampling_model: The model to use for sampling.         max_retries: The maximum number of times to retry a tool call.         elicitation_callback: Callback function to handle elicitation requests from the server.         id: An optional unique ID for the MCP server. An MCP server needs to have an ID in order to be used in a durable execution environment like Temporal, in which case the ID will be used to identify the server's activities within the workflow.     \"\"\"     self.command = command     self.args = args     self.env = env     self.cwd = cwd      super().__init__(         tool_prefix,         log_level,         log_handler,         timeout,         read_timeout,         process_tool_call,         allow_sampling,         sampling_model,         max_retries,         elicitation_callback,         id=id,     ) ``` |\n\n#### command `instance-attribute`\n\n```\ncommand: str = command\n```\n\nThe command to run.\n\n#### args `instance-attribute`\n\n```\nargs: Sequence[str] = args\n```\n\nThe arguments to pass to the command.\n\n#### env `instance-attribute`\n\n```\nenv: dict[str, str] | None = env\n```\n\nThe environment variables the CLI server will have access to.\n\nBy default the subprocess will not inherit any environment variables from the parent process.\nIf you want to inherit the environment variables from the parent process, use `env=os.environ`.\n\n#### cwd `instance-attribute`\n\n```\ncwd: str | Path | None = cwd\n```\n\nThe working directory to use when spawning the process.", "url": "https://ai.pydantic.dev/mcp/index.html#mcpserverstdio", "page": "mcp/index.html", "source_site": "pydantic_ai"}
{"title": "MCPServerSSE", "anchor": "mcpserversse", "md_text": "Bases: `_MCPServerHTTP`\n\nAn MCP server that connects over streamable HTTP connections.\n\nThis class implements the SSE transport from the MCP specification.\nSee <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse> for more information.\n\nUsing this class as an async context manager will create a new pool of HTTP connections to connect\nto a server which should already be running.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerSSE\n\nserver = MCPServerSSE('http://localhost:3001/sse')\nagent = Agent('openai:gpt-4o', toolsets=[server])\n```\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n|  |  |\n| --- | --- |\n| ``` 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 ``` | ``` class MCPServerSSE(_MCPServerHTTP):     \"\"\"An MCP server that connects over streamable HTTP connections.      This class implements the SSE transport from the MCP specification.     See <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse> for more information.      !!! note         Using this class as an async context manager will create a new pool of HTTP connections to connect         to a server which should already be running.      Example:     ```python {py=\"3.10\"}     from pydantic_ai import Agent     from pydantic_ai.mcp import MCPServerSSE      server = MCPServerSSE('http://localhost:3001/sse')     agent = Agent('openai:gpt-4o', toolsets=[server])     ```     \"\"\"      @classmethod     def __get_pydantic_core_schema__(cls, _: Any, __: Any) -> CoreSchema:         return core_schema.no_info_after_validator_function(             lambda dct: MCPServerSSE(**dct),             core_schema.typed_dict_schema(                 {                     'url': core_schema.typed_dict_field(core_schema.str_schema()),                     'headers': core_schema.typed_dict_field(                         core_schema.dict_schema(core_schema.str_schema(), core_schema.str_schema()), required=False                     ),                 }             ),         )      @property     def _transport_client(self):         return sse_client  # pragma: no cover      def __eq__(self, value: object, /) -> bool:         return super().__eq__(value) and isinstance(value, MCPServerSSE) and self.url == value.url ``` |", "url": "https://ai.pydantic.dev/mcp/index.html#mcpserversse", "page": "mcp/index.html", "source_site": "pydantic_ai"}
{"title": "MCPServerHTTP `deprecated`", "anchor": "mcpserverhttp-deprecated", "md_text": "Bases: `MCPServerSSE`\n\nDeprecated\n\nThe `MCPServerHTTP` class is deprecated, use `MCPServerSSE` instead.\n\nAn MCP server that connects over HTTP using the old SSE transport.\n\nThis class implements the SSE transport from the MCP specification.\nSee <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse> for more information.\n\nUsing this class as an async context manager will create a new pool of HTTP connections to connect\nto a server which should already be running.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerHTTP\n\nserver = MCPServerHTTP('http://localhost:3001/sse')\nagent = Agent('openai:gpt-4o', toolsets=[server])\n```\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n|  |  |\n| --- | --- |\n| ``` 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 ``` | ``` @deprecated('The `MCPServerHTTP` class is deprecated, use `MCPServerSSE` instead.') class MCPServerHTTP(MCPServerSSE):     \"\"\"An MCP server that connects over HTTP using the old SSE transport.      This class implements the SSE transport from the MCP specification.     See <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse> for more information.      !!! note         Using this class as an async context manager will create a new pool of HTTP connections to connect         to a server which should already be running.      Example:     ```python {py=\"3.10\" test=\"skip\"}     from pydantic_ai import Agent     from pydantic_ai.mcp import MCPServerHTTP      server = MCPServerHTTP('http://localhost:3001/sse')     agent = Agent('openai:gpt-4o', toolsets=[server])     ```     \"\"\" ``` |", "url": "https://ai.pydantic.dev/mcp/index.html#mcpserverhttp-deprecated", "page": "mcp/index.html", "source_site": "pydantic_ai"}
{"title": "MCPServerStreamableHTTP", "anchor": "mcpserverstreamablehttp", "md_text": "Bases: `_MCPServerHTTP`\n\nAn MCP server that connects over HTTP using the Streamable HTTP transport.\n\nThis class implements the Streamable HTTP transport from the MCP specification.\nSee <https://modelcontextprotocol.io/introduction#streamable-http> for more information.\n\nUsing this class as an async context manager will create a new pool of HTTP connections to connect\nto a server which should already be running.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.mcp import MCPServerStreamableHTTP\n\nserver = MCPServerStreamableHTTP('http://localhost:8000/mcp')\nagent = Agent('openai:gpt-4o', toolsets=[server])\n```\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n|  |  |\n| --- | --- |\n| ``` 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 ``` | ``` class MCPServerStreamableHTTP(_MCPServerHTTP):     \"\"\"An MCP server that connects over HTTP using the Streamable HTTP transport.      This class implements the Streamable HTTP transport from the MCP specification.     See <https://modelcontextprotocol.io/introduction#streamable-http> for more information.      !!! note         Using this class as an async context manager will create a new pool of HTTP connections to connect         to a server which should already be running.      Example:     ```python {py=\"3.10\"}     from pydantic_ai import Agent     from pydantic_ai.mcp import MCPServerStreamableHTTP      server = MCPServerStreamableHTTP('http://localhost:8000/mcp')     agent = Agent('openai:gpt-4o', toolsets=[server])     ```     \"\"\"      @classmethod     def __get_pydantic_core_schema__(cls, _: Any, __: Any) -> CoreSchema:         return core_schema.no_info_after_validator_function(             lambda dct: MCPServerStreamableHTTP(**dct),             core_schema.typed_dict_schema(                 {                     'url': core_schema.typed_dict_field(core_schema.str_schema()),                     'headers': core_schema.typed_dict_field(                         core_schema.dict_schema(core_schema.str_schema(), core_schema.str_schema()), required=False                     ),                 }             ),         )      @property     def _transport_client(self):         return streamablehttp_client  # pragma: no cover      def __eq__(self, value: object, /) -> bool:         return super().__eq__(value) and isinstance(value, MCPServerStreamableHTTP) and self.url == value.url ``` |", "url": "https://ai.pydantic.dev/mcp/index.html#mcpserverstreamablehttp", "page": "mcp/index.html", "source_site": "pydantic_ai"}
{"title": "load\\_mcp\\_servers", "anchor": "loadmcpservers", "md_text": "```\nload_mcp_servers(\n    config_path: str | Path,\n) -> list[\n    MCPServerStdio | MCPServerStreamableHTTP | MCPServerSSE\n]\n```\n\nLoad MCP servers from a configuration file.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `config_path` | `str | Path` | The path to the configuration file. | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `list[MCPServerStdio | MCPServerStreamableHTTP | MCPServerSSE]` | A list of MCP servers. |\n\nRaises:\n\n| Type | Description |\n| --- | --- |\n| `FileNotFoundError` | If the configuration file does not exist. |\n| `ValidationError` | If the configuration file does not match the schema. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/mcp.py`\n\n|  |  |\n| --- | --- |\n| ``` 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 950 951 952 953 954 955 956 ``` | ``` def load_mcp_servers(config_path: str | Path) -> list[MCPServerStdio | MCPServerStreamableHTTP | MCPServerSSE]:     \"\"\"Load MCP servers from a configuration file.      Args:         config_path: The path to the configuration file.      Returns:         A list of MCP servers.      Raises:         FileNotFoundError: If the configuration file does not exist.         ValidationError: If the configuration file does not match the schema.     \"\"\"     config_path = Path(config_path)      if not config_path.exists():         raise FileNotFoundError(f'Config file {config_path} not found')      config = MCPServerConfig.model_validate_json(config_path.read_bytes())      servers: list[MCPServerStdio | MCPServerStreamableHTTP | MCPServerSSE] = []     for name, server in config.mcp_servers.items():         server.id = name         server.tool_prefix = name         servers.append(server)      return servers ``` |", "url": "https://ai.pydantic.dev/mcp/index.html#loadmcpservers", "page": "mcp/index.html", "source_site": "pydantic_ai"}
{"title": "ModelSettings", "anchor": "modelsettings", "md_text": "Bases: `TypedDict`\n\nSettings to configure an LLM.\n\nHere we include only settings which apply to multiple models / model providers,\nthough not all of these settings are supported by all models.\n\nSource code in `pydantic_ai_slim/pydantic_ai/settings.py`\n\n|  |  |\n| --- | --- |\n| ```   7   8   9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 ``` | ``` class ModelSettings(TypedDict, total=False):     \"\"\"Settings to configure an LLM.      Here we include only settings which apply to multiple models / model providers,     though not all of these settings are supported by all models.     \"\"\"      max_tokens: int     \"\"\"The maximum number of tokens to generate before stopping.      Supported by:      * Gemini     * Anthropic     * OpenAI     * Groq     * Cohere     * Mistral     * Bedrock     * MCP Sampling     \"\"\"      temperature: float     \"\"\"Amount of randomness injected into the response.      Use `temperature` closer to `0.0` for analytical / multiple choice, and closer to a model's     maximum `temperature` for creative and generative tasks.      Note that even with `temperature` of `0.0`, the results will not be fully deterministic.      Supported by:      * Gemini     * Anthropic     * OpenAI     * Groq     * Cohere     * Mistral     * Bedrock     \"\"\"      top_p: float     \"\"\"An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top_p probability mass.      So 0.1 means only the tokens comprising the top 10% probability mass are considered.      You should either alter `temperature` or `top_p`, but not both.      Supported by:      * Gemini     * Anthropic     * OpenAI     * Groq     * Cohere     * Mistral     * Bedrock     \"\"\"      timeout: float | Timeout     \"\"\"Override the client-level default timeout for a request, in seconds.      Supported by:      * Gemini     * Anthropic     * OpenAI     * Groq     * Mistral     \"\"\"      parallel_tool_calls: bool     \"\"\"Whether to allow parallel tool calls.      Supported by:      * OpenAI (some models, not o1)     * Groq     * Anthropic     \"\"\"      seed: int     \"\"\"The random seed to use for the model, theoretically allowing for deterministic results.      Supported by:      * OpenAI     * Groq     * Cohere     * Mistral     * Gemini     \"\"\"      presence_penalty: float     \"\"\"Penalize new tokens based on whether they have appeared in the text so far.      Supported by:      * OpenAI     * Groq     * Cohere     * Gemini     * Mistral     \"\"\"      frequency_penalty: float     \"\"\"Penalize new tokens based on their existing frequency in the text so far.      Supported by:      * OpenAI     * Groq     * Cohere     * Gemini     * Mistral     \"\"\"      logit_bias: dict[str, int]     \"\"\"Modify the likelihood of specified tokens appearing in the completion.      Supported by:      * OpenAI     * Groq     \"\"\"      stop_sequences: list[str]     \"\"\"Sequences that will cause the model to stop generating.      Supported by:      * OpenAI     * Anthropic     * Bedrock     * Mistral     * Groq     * Cohere     * Google     \"\"\"      extra_headers: dict[str, str]     \"\"\"Extra headers to send to the model.      Supported by:      * OpenAI     * Anthropic     * Groq     \"\"\"      extra_body: object     \"\"\"Extra body to send to the model.      Supported by:      * OpenAI     * Anthropic     * Groq     \"\"\" ``` |\n\n#### max\\_tokens `instance-attribute`", "url": "https://ai.pydantic.dev/settings/index.html#modelsettings", "page": "settings/index.html", "source_site": "pydantic_ai"}
{"title": "ModelSettings", "anchor": "modelsettings", "md_text": "```\nmax_tokens: int\n```\n\nThe maximum number of tokens to generate before stopping.\n\nSupported by:\n\n* Gemini\n* Anthropic\n* OpenAI\n* Groq\n* Cohere\n* Mistral\n* Bedrock\n* MCP Sampling\n\n#### temperature `instance-attribute`\n\n```\ntemperature: float\n```\n\nAmount of randomness injected into the response.\n\nUse `temperature` closer to `0.0` for analytical / multiple choice, and closer to a model's\nmaximum `temperature` for creative and generative tasks.\n\nNote that even with `temperature` of `0.0`, the results will not be fully deterministic.\n\nSupported by:\n\n* Gemini\n* Anthropic\n* OpenAI\n* Groq\n* Cohere\n* Mistral\n* Bedrock\n\n#### top\\_p `instance-attribute`\n\n```\ntop_p: float\n```\n\nAn alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top\\_p probability mass.\n\nSo 0.1 means only the tokens comprising the top 10% probability mass are considered.\n\nYou should either alter `temperature` or `top_p`, but not both.\n\nSupported by:\n\n* Gemini\n* Anthropic\n* OpenAI\n* Groq\n* Cohere\n* Mistral\n* Bedrock\n\n#### timeout `instance-attribute`\n\n```\ntimeout: float | Timeout\n```\n\nOverride the client-level default timeout for a request, in seconds.\n\nSupported by:\n\n* Gemini\n* Anthropic\n* OpenAI\n* Groq\n* Mistral\n\n#### parallel\\_tool\\_calls `instance-attribute`\n\n```\nparallel_tool_calls: bool\n```\n\nWhether to allow parallel tool calls.\n\nSupported by:\n\n* OpenAI (some models, not o1)\n* Groq\n* Anthropic\n\n#### seed `instance-attribute`\n\n```\nseed: int\n```\n\nThe random seed to use for the model, theoretically allowing for deterministic results.\n\nSupported by:\n\n* OpenAI\n* Groq\n* Cohere\n* Mistral\n* Gemini\n\n#### presence\\_penalty `instance-attribute`\n\n```\npresence_penalty: float\n```\n\nPenalize new tokens based on whether they have appeared in the text so far.\n\nSupported by:\n\n* OpenAI\n* Groq\n* Cohere\n* Gemini\n* Mistral\n\n#### frequency\\_penalty `instance-attribute`\n\n```\nfrequency_penalty: float\n```\n\nPenalize new tokens based on their existing frequency in the text so far.\n\nSupported by:\n\n* OpenAI\n* Groq\n* Cohere\n* Gemini\n* Mistral\n\n#### logit\\_bias `instance-attribute`\n\n```\nlogit_bias: dict[str, int]\n```\n\nModify the likelihood of specified tokens appearing in the completion.\n\nSupported by:\n\n* OpenAI\n* Groq\n\n#### stop\\_sequences `instance-attribute`\n\n```\nstop_sequences: list[str]\n```\n\nSequences that will cause the model to stop generating.\n\nSupported by:\n\n* OpenAI\n* Anthropic\n* Bedrock\n* Mistral\n* Groq\n* Cohere\n* Google\n\n#### extra\\_headers `instance-attribute`\n\n```\nextra_headers: dict[str, str]\n```\n\nExtra headers to send to the model.\n\nSupported by:\n\n* OpenAI\n* Anthropic\n* Groq\n\n#### extra\\_body `instance-attribute`\n\n```\nextra_body: object\n```\n\nExtra body to send to the model.\n\nSupported by:\n\n* OpenAI\n* Anthropic\n* Groq", "url": "https://ai.pydantic.dev/settings/index.html#modelsettings", "page": "settings/index.html", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "md_text": "To use `GoogleModel`, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `google` optional group:\n\npipuv\n\n```\npip install \"pydantic-ai-slim[google]\"\n\nuv add \"pydantic-ai-slim[google]\"\n```", "url": "https://ai.pydantic.dev/google/index.html#install", "page": "google/index.html", "source_site": "pydantic_ai"}
{"title": "Configuration", "anchor": "configuration", "md_text": "`GoogleModel` lets you use Google's Gemini models through their [Generative Language API](https://ai.google.dev/api/all-methods) (`generativelanguage.googleapis.com`) or [Vertex AI API](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models) (`*-aiplatform.googleapis.com`).", "url": "https://ai.pydantic.dev/google/index.html#configuration", "page": "google/index.html", "source_site": "pydantic_ai"}
{"title": "API Key (Generative Language API)", "anchor": "api-key-generative-language-api", "md_text": "To use Gemini via the Generative Language API, go to [aistudio.google.com](https://aistudio.google.com/apikey) and create an API key.\n\nOnce you have the API key, set it as an environment variable:\n\n```\nexport GOOGLE_API_KEY=your-api-key\n```\n\nYou can then use `GoogleModel` by name (where GLA stands for Generative Language API):\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('google-gla:gemini-2.5-pro')\n...\n```\n\nOr you can explicitly create the provider:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\nprovider = GoogleProvider(api_key='your-api-key')\nmodel = GoogleModel('gemini-2.5-pro', provider=provider)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/google/index.html#api-key-generative-language-api", "page": "google/index.html", "source_site": "pydantic_ai"}
{"title": "Vertex AI (Enterprise/Cloud)", "anchor": "vertex-ai-enterprisecloud", "md_text": "If you are an enterprise user, you can also use `GoogleModel` to access Gemini via Vertex AI.\n\nThis interface has a number of advantages over the Generative Language API:\n\n1. The VertexAI API comes with more enterprise readiness guarantees.\n2. You can [purchase provisioned throughput](https://cloud.google.com/vertex-ai/generative-ai/docs/provisioned-throughput#purchase-provisioned-throughput) with Vertex AI to guarantee capacity.\n3. If you're running Pydantic AI inside GCP, you don't need to set up authentication, it should \"just work\".\n4. You can decide which region to use, which might be important from a regulatory perspective, and might improve latency.\n\nYou can authenticate using [application default credentials](https://cloud.google.com/docs/authentication/application-default-credentials), a service account, or an [API key](https://cloud.google.com/vertex-ai/generative-ai/docs/start/api-keys?usertype=expressmode).\n\nWhichever way you authenticate, you'll need to have Vertex AI enabled in your GCP account.\n\n#### Application Default Credentials\n\nIf you have the [`gcloud` CLI](https://cloud.google.com/sdk/gcloud) installed and configured, you can use `GoogleProvider` in Vertex AI mode by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('google-vertex:gemini-2.5-pro')\n...\n```\n\nOr you can explicitly create the provider and model:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\nprovider = GoogleProvider(vertexai=True)\nmodel = GoogleModel('gemini-2.5-pro', provider=provider)\nagent = Agent(model)\n...\n```\n\n#### Service Account\n\nTo use a service account JSON file, explicitly create the provider and model:\n\ngoogle\\_model\\_service\\_account.py\n\n```\nfrom google.oauth2 import service_account\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\ncredentials = service_account.Credentials.from_service_account_file(\n    'path/to/service-account.json',\n    scopes=['https://www.googleapis.com/auth/cloud-platform'],\n)\nprovider = GoogleProvider(credentials=credentials, project='your-project-id')\nmodel = GoogleModel('gemini-1.5-flash', provider=provider)\nagent = Agent(model)\n...\n```\n\n#### API Key\n\nTo use Vertex AI with an API key, [create a key](https://cloud.google.com/vertex-ai/generative-ai/docs/start/api-keys?usertype=expressmode) and set it as an environment variable:\n\n```\nexport GOOGLE_API_KEY=your-api-key\n```\n\nYou can then use `GoogleModel` in Vertex AI mode by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('google-vertex:gemini-2.5-pro')\n...\n```\n\nOr you can explicitly create the provider and model:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\nprovider = GoogleProvider(vertexai=True, api_key='your-api-key')\nmodel = GoogleModel('gemini-2.5-pro', provider=provider)\nagent = Agent(model)\n...\n```\n\n#### Customizing Location or Project\n\nYou can specify the location and/or project when using Vertex AI:\n\ngoogle\\_model\\_location.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\nprovider = GoogleProvider(vertexai=True, location='asia-east1', project='your-gcp-project-id')\nmodel = GoogleModel('gemini-2.5-pro', provider=provider)\nagent = Agent(model)\n...\n```\n\n#### Model Garden\n\nYou can access models from the [Model Garden](https://cloud.google.com/model-garden?hl=en) that support the `generateContent` API and are available under your GCP project, including but not limited to Gemini, using one of the following `model_name` patterns:\n\n* `{model_id}` for Gemini models\n* `{publisher}/{model_id}`\n* `publishers/{publisher}/models/{model_id}`\n* `projects/{project}/locations/{location}/publishers/{publisher}/models/{model_id}`\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\nprovider = GoogleProvider(\n    project='your-gcp-project-id',\n    location='us-central1',  # the region where the model is available\n)\nmodel = GoogleModel('meta/llama-3.3-70b-instruct-maas', provider=provider)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/google/index.html#vertex-ai-enterprisecloud", "page": "google/index.html", "source_site": "pydantic_ai"}
{"title": "Custom HTTP Client", "anchor": "custom-http-client", "md_text": "You can customize the `GoogleProvider` with a custom `httpx.AsyncClient`:\n\n```\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel\nfrom pydantic_ai.providers.google import GoogleProvider\n\ncustom_http_client = AsyncClient(timeout=30)\nmodel = GoogleModel(\n    'gemini-2.5-pro',\n    provider=GoogleProvider(api_key='your-api-key', http_client=custom_http_client),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/google/index.html#custom-http-client", "page": "google/index.html", "source_site": "pydantic_ai"}
{"title": "Document, Image, Audio, and Video Input", "anchor": "document-image-audio-and-video-input", "md_text": "`GoogleModel` supports multi-modal input, including documents, images, audio, and video. See the [input documentation](https://ai.pydantic.dev/input/) for details and examples.", "url": "https://ai.pydantic.dev/google/index.html#document-image-audio-and-video-input", "page": "google/index.html", "source_site": "pydantic_ai"}
{"title": "Model settings", "anchor": "model-settings", "md_text": "You can customize model behavior using [`GoogleModelSettings`](../models/google/index.html#pydantic_ai.models.google.GoogleModelSettings):\n\n```\nfrom google.genai.types import HarmBlockThreshold, HarmCategory\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel, GoogleModelSettings\n\nsettings = GoogleModelSettings(\n    temperature=0.2,\n    max_tokens=1024,\n    google_thinking_config={'thinking_budget': 2048},\n    google_safety_settings=[\n        {\n            'category': HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n            'threshold': HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n        }\n    ]\n)\nmodel = GoogleModel('gemini-1.5-flash')\nagent = Agent(model, model_settings=settings)\n...\n```", "url": "https://ai.pydantic.dev/google/index.html#model-settings", "page": "google/index.html", "source_site": "pydantic_ai"}
{"title": "Disable thinking", "anchor": "disable-thinking", "md_text": "You can disable thinking by setting the `thinking_budget` to `0` on the `google_thinking_config`:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel, GoogleModelSettings\n\nmodel_settings = GoogleModelSettings(google_thinking_config={'thinking_budget': 0})\nmodel = GoogleModel('gemini-2.0-flash')\nagent = Agent(model, model_settings=model_settings)\n...\n```\n\nCheck out the [Gemini API docs](https://ai.google.dev/gemini-api/docs/thinking) for more on thinking.", "url": "https://ai.pydantic.dev/google/index.html#disable-thinking", "page": "google/index.html", "source_site": "pydantic_ai"}
{"title": "Safety settings", "anchor": "safety-settings", "md_text": "You can customize the safety settings by setting the `google_safety_settings` field.\n\n```\nfrom google.genai.types import HarmBlockThreshold, HarmCategory\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.google import GoogleModel, GoogleModelSettings\n\nmodel_settings = GoogleModelSettings(\n    google_safety_settings=[\n        {\n            'category': HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n            'threshold': HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n        }\n    ]\n)\nmodel = GoogleModel('gemini-2.0-flash')\nagent = Agent(model, model_settings=model_settings)\n...\n```\n\nSee the [Gemini API docs](https://ai.google.dev/gemini-api/docs/safety-settings) for more on safety settings.", "url": "https://ai.pydantic.dev/google/index.html#safety-settings", "page": "google/index.html", "source_site": "pydantic_ai"}
{"title": "FastA2A", "anchor": "fasta2a", "md_text": "Bases: `Starlette`\n\nThe main class for the FastA2A library.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/applications.py`", "url": "https://ai.pydantic.dev/fasta2a/index.html#fasta2a", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "FastA2A", "anchor": "fasta2a", "md_text": "|  |  |\n| --- | --- |\n| ```  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 ``` | ``` class FastA2A(Starlette):     \"\"\"The main class for the FastA2A library.\"\"\"      def __init__(         self,         *,         storage: Storage,         broker: Broker,         # Agent card         name: str | None = None,         url: str = 'http://localhost:8000',         version: str = '1.0.0',         description: str | None = None,         provider: AgentProvider | None = None,         skills: list[Skill] | None = None,         # Starlette         debug: bool = False,         routes: Sequence[Route] | None = None,         middleware: Sequence[Middleware] | None = None,         exception_handlers: dict[Any, ExceptionHandler] | None = None,         lifespan: Lifespan[FastA2A] | None = None,     ):         if lifespan is None:             lifespan = _default_lifespan          super().__init__(             debug=debug,             routes=routes,             middleware=middleware,             exception_handlers=exception_handlers,             lifespan=lifespan,         )          self.name = name or 'My Agent'         self.url = url         self.version = version         self.description = description         self.provider = provider         self.skills = skills or []         # NOTE: For now, I don't think there's any reason to support any other input/output modes.         self.default_input_modes = ['application/json']         self.default_output_modes = ['application/json']          self.task_manager = TaskManager(broker=broker, storage=storage)          # Setup         self._agent_card_json_schema: bytes | None = None         self.router.add_route('/.well-known/agent.json', self._agent_card_endpoint, methods=['HEAD', 'GET', 'OPTIONS'])         self.router.add_route('/', self._agent_run_endpoint, methods=['POST'])         self.router.add_route('/docs', self._docs_endpoint, methods=['GET'])      async def __call__(self, scope: Scope, receive: Receive, send: Send) -> None:         if scope['type'] == 'http' and not self.task_manager.is_running:             raise RuntimeError('TaskManager was not properly initialized.')         await super().__call__(scope, receive, send)      async def _agent_card_endpoint(self, request: Request) -> Response:         if self._agent_card_json_schema is None:             agent_card = AgentCard(                 name=self.name,                 description=self.description or 'An AI agent exposed as an A2A agent.',                 url=self.url,                 version=self.version,                 protocol_version='0.2.5',                 skills=self.skills,                 default_input_modes=self.default_input_modes,                 default_output_modes=self.default_output_modes,                 capabilities=AgentCapabilities(                     streaming=False, push_notifications=False, state_transition_history=False                 ),             )             if self.provider is not None:                 agent_card['provider'] = self.provider             self._agent_card_json_schema = agent_card_ta.dump_json(agent_card, by_alias=True)         return Response(content=self._agent_card_json_schema, media_type='application/json')      async def _docs_endpoint(self, request: Request) -> Response:         \"\"\"Serve the documentation interface.\"\"\"         docs_path = Path(__file__).parent / 'static' / 'docs.html'         return FileResponse(docs_path, media_type='text/html')      async def _agent_run_endpoint(self, request: Request) -> Response:         \"\"\"This is the main endpoint for the A2A server.          Although the specification allows freedom of choice and implementation, I'm pretty sure about some decisions.          1. The server will always either send a \"submitted\" or a \"failed\" on `tasks/send`.             Never a \"completed\" on the first message.         2. There are three possible ends for the task:             2.1. The task was \"completed\" successfully.             2.2. The task was \"canceled\".             2.3. The task \"failed\".         3. The server will send a \"working\" on the first chunk on `tasks/pushNotification/get`.         \"\"\"         data = await request.body()         a2a_request = a2a_request_ta.validate_json(data)          if a2a_request['method'] == 'message/send':             jsonrpc_response = await self.task_manager.send_message(a2a_request)         elif a2a_request['method'] == 'tasks/get':             jsonrpc_response = await self.task_manager.get_task(a2a_request)         elif a2a_request['method'] == 'tasks/cancel':             jsonrpc_response = await self.task_manager.cancel_task(a2a_request)         else:             raise NotImplementedError(f'Method {a2a_request[\"method\"]} not implemented.')         return Response(             content=a2a_response_ta.dump_json(jsonrpc_response, by_alias=True), media_type='application/json'         ) ``` |", "url": "https://ai.pydantic.dev/fasta2a/index.html#fasta2a", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "Broker `dataclass`", "anchor": "broker-dataclass", "md_text": "Bases: `ABC`\n\nThe broker class is in charge of scheduling the tasks.\n\nThe HTTP server uses the broker to schedule tasks.\n\nThe simple implementation is the `InMemoryBroker`, which is the broker that\nruns the tasks in the same process as the HTTP server. That said, this class can be\nextended to support remote workers.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/broker.py`\n\n|  |  |\n| --- | --- |\n| ``` 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 ``` | ``` @dataclass class Broker(ABC):     \"\"\"The broker class is in charge of scheduling the tasks.      The HTTP server uses the broker to schedule tasks.      The simple implementation is the `InMemoryBroker`, which is the broker that     runs the tasks in the same process as the HTTP server. That said, this class can be     extended to support remote workers.     \"\"\"      @abstractmethod     async def run_task(self, params: TaskSendParams) -> None:         \"\"\"Send a task to be executed by the worker.\"\"\"         raise NotImplementedError('send_run_task is not implemented yet.')      @abstractmethod     async def cancel_task(self, params: TaskIdParams) -> None:         \"\"\"Cancel a task.\"\"\"         raise NotImplementedError('send_cancel_task is not implemented yet.')      @abstractmethod     async def __aenter__(self) -> Self: ...      @abstractmethod     async def __aexit__(self, exc_type: Any, exc_value: Any, traceback: Any): ...      @abstractmethod     def receive_task_operations(self) -> AsyncIterator[TaskOperation]:         \"\"\"Receive task operations from the broker.          On a multi-worker setup, the broker will need to round-robin the task operations         between the workers.         \"\"\" ``` |\n\n#### run\\_task `abstractmethod` `async`\n\n```\nrun_task(params: TaskSendParams) -> None\n```\n\nSend a task to be executed by the worker.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/broker.py`\n\n|  |  |\n| --- | --- |\n| ``` 30 31 32 33 ``` | ``` @abstractmethod async def run_task(self, params: TaskSendParams) -> None:     \"\"\"Send a task to be executed by the worker.\"\"\"     raise NotImplementedError('send_run_task is not implemented yet.') ``` |\n\n#### cancel\\_task `abstractmethod` `async`\n\n```\ncancel_task(params: TaskIdParams) -> None\n```\n\nCancel a task.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/broker.py`\n\n|  |  |\n| --- | --- |\n| ``` 35 36 37 38 ``` | ``` @abstractmethod async def cancel_task(self, params: TaskIdParams) -> None:     \"\"\"Cancel a task.\"\"\"     raise NotImplementedError('send_cancel_task is not implemented yet.') ``` |\n\n#### receive\\_task\\_operations `abstractmethod`\n\n```\nreceive_task_operations() -> AsyncIterator[TaskOperation]\n```\n\nReceive task operations from the broker.\n\nOn a multi-worker setup, the broker will need to round-robin the task operations\nbetween the workers.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/broker.py`\n\n|  |  |\n| --- | --- |\n| ``` 46 47 48 49 50 51 52 ``` | ``` @abstractmethod def receive_task_operations(self) -> AsyncIterator[TaskOperation]:     \"\"\"Receive task operations from the broker.      On a multi-worker setup, the broker will need to round-robin the task operations     between the workers.     \"\"\" ``` |", "url": "https://ai.pydantic.dev/fasta2a/index.html#broker-dataclass", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "Skill", "anchor": "skill", "md_text": "Bases: `TypedDict`\n\nSkills are a unit of capability that an agent can perform.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class Skill(TypedDict):     \"\"\"Skills are a unit of capability that an agent can perform.\"\"\"      id: str     \"\"\"A unique identifier for the skill.\"\"\"      name: str     \"\"\"Human readable name of the skill.\"\"\"      description: str     \"\"\"A human-readable description of the skill.      It will be used by the client or a human as a hint to understand the skill.     \"\"\"      tags: list[str]     \"\"\"Set of tag-words describing classes of capabilities for this specific skill.      Examples: \"cooking\", \"customer support\", \"billing\".     \"\"\"      examples: NotRequired[list[str]]     \"\"\"The set of example scenarios that the skill can perform.      Will be used by the client as a hint to understand how the skill can be used. (e.g. \"I need a recipe for bread\")     \"\"\"      input_modes: list[str]     \"\"\"Supported mime types for input data.\"\"\"      output_modes: list[str]     \"\"\"Supported mime types for output data.\"\"\" ``` |\n\n#### id `instance-attribute`\n\n```\nid: str\n```\n\nA unique identifier for the skill.\n\n#### name `instance-attribute`\n\n```\nname: str\n```\n\nHuman readable name of the skill.\n\n#### description `instance-attribute`\n\n```\ndescription: str\n```\n\nA human-readable description of the skill.\n\nIt will be used by the client or a human as a hint to understand the skill.\n\n#### tags `instance-attribute`\n\n```\ntags: list[str]\n```\n\nSet of tag-words describing classes of capabilities for this specific skill.\n\nExamples: \"cooking\", \"customer support\", \"billing\".\n\n#### examples `instance-attribute`\n\n```\nexamples: NotRequired[list[str]]\n```\n\nThe set of example scenarios that the skill can perform.\n\nWill be used by the client as a hint to understand how the skill can be used. (e.g. \"I need a recipe for bread\")\n\n#### input\\_modes `instance-attribute`\n\n```\ninput_modes: list[str]\n```\n\nSupported mime types for input data.\n\n#### output\\_modes `instance-attribute`\n\n```\noutput_modes: list[str]\n```\n\nSupported mime types for output data.", "url": "https://ai.pydantic.dev/fasta2a/index.html#skill", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "Storage", "anchor": "storage", "md_text": "Bases: `ABC`, `Generic[ContextT]`\n\nA storage to retrieve and save tasks, as well as retrieve and save context.\n\nThe storage serves two purposes:\n1. Task storage: Stores tasks in A2A protocol format with their status, artifacts, and message history\n2. Context storage: Stores conversation context in a format optimized for the specific agent implementation\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/storage.py`\n\n|  |  |\n| --- | --- |\n| ``` 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 ``` | ``` class Storage(ABC, Generic[ContextT]):     \"\"\"A storage to retrieve and save tasks, as well as retrieve and save context.      The storage serves two purposes:     1. Task storage: Stores tasks in A2A protocol format with their status, artifacts, and message history     2. Context storage: Stores conversation context in a format optimized for the specific agent implementation     \"\"\"      @abstractmethod     async def load_task(self, task_id: str, history_length: int | None = None) -> Task | None:         \"\"\"Load a task from storage.          If the task is not found, return None.         \"\"\"      @abstractmethod     async def submit_task(self, context_id: str, message: Message) -> Task:         \"\"\"Submit a task to storage.\"\"\"      @abstractmethod     async def update_task(         self,         task_id: str,         state: TaskState,         new_artifacts: list[Artifact] | None = None,         new_messages: list[Message] | None = None,     ) -> Task:         \"\"\"Update the state of a task. Appends artifacts and messages, if specified.\"\"\"      @abstractmethod     async def load_context(self, context_id: str) -> ContextT | None:         \"\"\"Retrieve the stored context given the `context_id`.\"\"\"      @abstractmethod     async def update_context(self, context_id: str, context: ContextT) -> None:         \"\"\"Updates the context for a `context_id`.          Implementing agent can decide what to store in context.         \"\"\" ``` |\n\n#### load\\_task `abstractmethod` `async`\n\n```\nload_task(\n    task_id: str, history_length: int | None = None\n) -> Task | None\n```\n\nLoad a task from storage.\n\nIf the task is not found, return None.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/storage.py`\n\n|  |  |\n| --- | --- |\n| ``` 25 26 27 28 29 30 ``` | ``` @abstractmethod async def load_task(self, task_id: str, history_length: int | None = None) -> Task | None:     \"\"\"Load a task from storage.      If the task is not found, return None.     \"\"\" ``` |\n\n#### submit\\_task `abstractmethod` `async`\n\n```\nsubmit_task(context_id: str, message: Message) -> Task\n```\n\nSubmit a task to storage.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/storage.py`\n\n|  |  |\n| --- | --- |\n| ``` 32 33 34 ``` | ``` @abstractmethod async def submit_task(self, context_id: str, message: Message) -> Task:     \"\"\"Submit a task to storage.\"\"\" ``` |\n\n#### update\\_task `abstractmethod` `async`\n\n```\nupdate_task(\n    task_id: str,\n    state: TaskState,\n    new_artifacts: list[Artifact] | None = None,\n    new_messages: list[Message] | None = None,\n) -> Task\n```\n\nUpdate the state of a task. Appends artifacts and messages, if specified.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/storage.py`\n\n|  |  |\n| --- | --- |\n| ``` 36 37 38 39 40 41 42 43 44 ``` | ``` @abstractmethod async def update_task(     self,     task_id: str,     state: TaskState,     new_artifacts: list[Artifact] | None = None,     new_messages: list[Message] | None = None, ) -> Task:     \"\"\"Update the state of a task. Appends artifacts and messages, if specified.\"\"\" ``` |\n\n#### load\\_context `abstractmethod` `async`\n\n```\nload_context(context_id: str) -> ContextT | None\n```\n\nRetrieve the stored context given the `context_id`.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/storage.py`\n\n|  |  |\n| --- | --- |\n| ``` 46 47 48 ``` | ``` @abstractmethod async def load_context(self, context_id: str) -> ContextT | None:     \"\"\"Retrieve the stored context given the `context_id`.\"\"\" ``` |\n\n#### update\\_context `abstractmethod` `async`\n\n```\nupdate_context(context_id: str, context: ContextT) -> None\n```\n\nUpdates the context for a `context_id`.\n\nImplementing agent can decide what to store in context.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/storage.py`", "url": "https://ai.pydantic.dev/fasta2a/index.html#storage", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "Storage", "anchor": "storage", "md_text": "|  |  |\n| --- | --- |\n| ``` 50 51 52 53 54 55 ``` | ``` @abstractmethod async def update_context(self, context_id: str, context: ContextT) -> None:     \"\"\"Updates the context for a `context_id`.      Implementing agent can decide what to store in context.     \"\"\" ``` |", "url": "https://ai.pydantic.dev/fasta2a/index.html#storage", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "Worker `dataclass`", "anchor": "worker-dataclass", "md_text": "Bases: `ABC`, `Generic[ContextT]`\n\nA worker is responsible for executing tasks.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/worker.py`\n\n|  |  |\n| --- | --- |\n| ``` 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 ``` | ``` @dataclass class Worker(ABC, Generic[ContextT]):     \"\"\"A worker is responsible for executing tasks.\"\"\"      broker: Broker     storage: Storage[ContextT]      @asynccontextmanager     async def run(self) -> AsyncIterator[None]:         \"\"\"Run the worker.          It connects to the broker, and it makes itself available to receive commands.         \"\"\"         async with anyio.create_task_group() as tg:             tg.start_soon(self._loop)             yield             tg.cancel_scope.cancel()      async def _loop(self) -> None:         async for task_operation in self.broker.receive_task_operations():             await self._handle_task_operation(task_operation)      async def _handle_task_operation(self, task_operation: TaskOperation) -> None:         try:             with use_span(task_operation['_current_span']):                 with tracer.start_as_current_span(                     f'{task_operation[\"operation\"]} task', attributes={'logfire.tags': ['fasta2a']}                 ):                     if task_operation['operation'] == 'run':                         await self.run_task(task_operation['params'])                     elif task_operation['operation'] == 'cancel':                         await self.cancel_task(task_operation['params'])                     else:                         assert_never(task_operation)         except Exception:             await self.storage.update_task(task_operation['params']['id'], state='failed')      @abstractmethod     async def run_task(self, params: TaskSendParams) -> None: ...      @abstractmethod     async def cancel_task(self, params: TaskIdParams) -> None: ...      @abstractmethod     def build_message_history(self, history: list[Message]) -> list[Any]: ...      @abstractmethod     def build_artifacts(self, result: Any) -> list[Artifact]: ... ``` |\n\n#### run `async`\n\n```\nrun() -> AsyncIterator[None]\n```\n\nRun the worker.\n\nIt connects to the broker, and it makes itself available to receive commands.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/worker.py`\n\n|  |  |\n| --- | --- |\n| ``` 29 30 31 32 33 34 35 36 37 38 ``` | ``` @asynccontextmanager async def run(self) -> AsyncIterator[None]:     \"\"\"Run the worker.      It connects to the broker, and it makes itself available to receive commands.     \"\"\"     async with anyio.create_task_group() as tg:         tg.start_soon(self._loop)         yield         tg.cancel_scope.cancel() ``` |\n\nThis module contains the schema for the agent card.", "url": "https://ai.pydantic.dev/fasta2a/index.html#worker-dataclass", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "AgentCard", "anchor": "agentcard", "md_text": "Bases: `TypedDict`\n\nThe card that describes an agent.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class AgentCard(TypedDict):     \"\"\"The card that describes an agent.\"\"\"      name: str     \"\"\"Human readable name of the agent e.g. \"Recipe Agent\".\"\"\"      description: str     \"\"\"A human-readable description of the agent.      Used to assist users and other agents in understanding what the agent can do.     (e.g. \"Agent that helps users with recipes and cooking.\")     \"\"\"      url: str     \"\"\"A URL to the address the agent is hosted at.\"\"\"      version: str     \"\"\"The version of the agent - format is up to the provider. (e.g. \"1.0.0\")\"\"\"      protocol_version: str     \"\"\"The version of the A2A protocol this agent supports.\"\"\"      provider: NotRequired[AgentProvider]     \"\"\"The service provider of the agent.\"\"\"      documentation_url: NotRequired[str]     \"\"\"A URL to documentation for the agent.\"\"\"      icon_url: NotRequired[str]     \"\"\"A URL to an icon for the agent.\"\"\"      preferred_transport: NotRequired[str]     \"\"\"The transport of the preferred endpoint. If empty, defaults to JSONRPC.\"\"\"      additional_interfaces: NotRequired[list[AgentInterface]]     \"\"\"Announcement of additional supported transports.\"\"\"      capabilities: AgentCapabilities     \"\"\"The capabilities of the agent.\"\"\"      security: NotRequired[list[dict[str, list[str]]]]     \"\"\"Security requirements for contacting the agent.\"\"\"      security_schemes: NotRequired[dict[str, SecurityScheme]]     \"\"\"Security scheme definitions.\"\"\"      default_input_modes: list[str]     \"\"\"Supported mime types for input data.\"\"\"      default_output_modes: list[str]     \"\"\"Supported mime types for output data.\"\"\"      skills: list[Skill] ``` |\n\n#### name `instance-attribute`\n\n```\nname: str\n```\n\nHuman readable name of the agent e.g. \"Recipe Agent\".\n\n#### description `instance-attribute`\n\n```\ndescription: str\n```\n\nA human-readable description of the agent.\n\nUsed to assist users and other agents in understanding what the agent can do.\n(e.g. \"Agent that helps users with recipes and cooking.\")\n\n#### url `instance-attribute`\n\n```\nurl: str\n```\n\nA URL to the address the agent is hosted at.\n\n#### version `instance-attribute`\n\n```\nversion: str\n```\n\nThe version of the agent - format is up to the provider. (e.g. \"1.0.0\")\n\n#### protocol\\_version `instance-attribute`\n\n```\nprotocol_version: str\n```\n\nThe version of the A2A protocol this agent supports.\n\n#### provider `instance-attribute`\n\n```\nprovider: NotRequired[AgentProvider]\n```\n\nThe service provider of the agent.\n\n#### documentation\\_url `instance-attribute`\n\n```\ndocumentation_url: NotRequired[str]\n```\n\nA URL to documentation for the agent.\n\n#### icon\\_url `instance-attribute`\n\n```\nicon_url: NotRequired[str]\n```\n\nA URL to an icon for the agent.\n\n#### preferred\\_transport `instance-attribute`\n\n```\npreferred_transport: NotRequired[str]\n```\n\nThe transport of the preferred endpoint. If empty, defaults to JSONRPC.\n\n#### additional\\_interfaces `instance-attribute`\n\n```\nadditional_interfaces: NotRequired[list[AgentInterface]]\n```\n\nAnnouncement of additional supported transports.\n\n#### capabilities `instance-attribute`\n\n```\ncapabilities: AgentCapabilities\n```\n\nThe capabilities of the agent.\n\n#### security `instance-attribute`\n\n```\nsecurity: NotRequired[list[dict[str, list[str]]]]\n```\n\nSecurity requirements for contacting the agent.\n\n#### security\\_schemes `instance-attribute`\n\n```\nsecurity_schemes: NotRequired[dict[str, SecurityScheme]]\n```\n\nSecurity scheme definitions.\n\n#### default\\_input\\_modes `instance-attribute`\n\n```\ndefault_input_modes: list[str]\n```\n\nSupported mime types for input data.\n\n#### default\\_output\\_modes `instance-attribute`\n\n```\ndefault_output_modes: list[str]\n```\n\nSupported mime types for output data.", "url": "https://ai.pydantic.dev/fasta2a/index.html#agentcard", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "AgentProvider", "anchor": "agentprovider", "md_text": "Bases: `TypedDict`\n\nThe service provider of the agent.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 72 73 74 75 76 ``` | ``` class AgentProvider(TypedDict):     \"\"\"The service provider of the agent.\"\"\"      organization: str     url: str ``` |", "url": "https://ai.pydantic.dev/fasta2a/index.html#agentprovider", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "AgentCapabilities", "anchor": "agentcapabilities", "md_text": "Bases: `TypedDict`\n\nThe capabilities of the agent.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 79 80 81 82 83 84 85 86 87 88 89 90 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class AgentCapabilities(TypedDict):     \"\"\"The capabilities of the agent.\"\"\"      streaming: NotRequired[bool]     \"\"\"Whether the agent supports streaming.\"\"\"      push_notifications: NotRequired[bool]     \"\"\"Whether the agent can notify updates to client.\"\"\"      state_transition_history: NotRequired[bool]     \"\"\"Whether the agent exposes status change history for tasks.\"\"\" ``` |\n\n#### streaming `instance-attribute`\n\n```\nstreaming: NotRequired[bool]\n```\n\nWhether the agent supports streaming.\n\n#### push\\_notifications `instance-attribute`\n\n```\npush_notifications: NotRequired[bool]\n```\n\nWhether the agent can notify updates to client.\n\n#### state\\_transition\\_history `instance-attribute`\n\n```\nstate_transition_history: NotRequired[bool]\n```\n\nWhether the agent exposes status change history for tasks.", "url": "https://ai.pydantic.dev/fasta2a/index.html#agentcapabilities", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "HttpSecurityScheme", "anchor": "httpsecurityscheme", "md_text": "Bases: `TypedDict`\n\nHTTP security scheme.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ```  93  94  95  96  97  98  99 100 101 102 103 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class HttpSecurityScheme(TypedDict):     \"\"\"HTTP security scheme.\"\"\"      type: Literal['http']     scheme: str     \"\"\"The name of the HTTP Authorization scheme.\"\"\"     bearer_format: NotRequired[str]     \"\"\"A hint to the client to identify how the bearer token is formatted.\"\"\"     description: NotRequired[str]     \"\"\"Description of this security scheme.\"\"\" ``` |\n\n#### scheme `instance-attribute`\n\n```\nscheme: str\n```\n\nThe name of the HTTP Authorization scheme.\n\n#### bearer\\_format `instance-attribute`\n\n```\nbearer_format: NotRequired[str]\n```\n\nA hint to the client to identify how the bearer token is formatted.\n\n#### description `instance-attribute`\n\n```\ndescription: NotRequired[str]\n```\n\nDescription of this security scheme.", "url": "https://ai.pydantic.dev/fasta2a/index.html#httpsecurityscheme", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "ApiKeySecurityScheme", "anchor": "apikeysecurityscheme", "md_text": "Bases: `TypedDict`\n\nAPI Key security scheme.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 106 107 108 109 110 111 112 113 114 115 116 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class ApiKeySecurityScheme(TypedDict):     \"\"\"API Key security scheme.\"\"\"      type: Literal['apiKey']     name: str     \"\"\"The name of the header, query or cookie parameter to be used.\"\"\"     in_: Literal['query', 'header', 'cookie']     \"\"\"The location of the API key.\"\"\"     description: NotRequired[str]     \"\"\"Description of this security scheme.\"\"\" ``` |\n\n#### name `instance-attribute`\n\n```\nname: str\n```\n\nThe name of the header, query or cookie parameter to be used.\n\n#### in\\_ `instance-attribute`\n\n```\nin_: Literal['query', 'header', 'cookie']\n```\n\nThe location of the API key.\n\n#### description `instance-attribute`\n\n```\ndescription: NotRequired[str]\n```\n\nDescription of this security scheme.", "url": "https://ai.pydantic.dev/fasta2a/index.html#apikeysecurityscheme", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "OAuth2SecurityScheme", "anchor": "oauth2securityscheme", "md_text": "Bases: `TypedDict`\n\nOAuth2 security scheme.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 119 120 121 122 123 124 125 126 127 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class OAuth2SecurityScheme(TypedDict):     \"\"\"OAuth2 security scheme.\"\"\"      type: Literal['oauth2']     flows: dict[str, Any]     \"\"\"An object containing configuration information for the flow types supported.\"\"\"     description: NotRequired[str]     \"\"\"Description of this security scheme.\"\"\" ``` |\n\n#### flows `instance-attribute`\n\n```\nflows: dict[str, Any]\n```\n\nAn object containing configuration information for the flow types supported.\n\n#### description `instance-attribute`\n\n```\ndescription: NotRequired[str]\n```\n\nDescription of this security scheme.", "url": "https://ai.pydantic.dev/fasta2a/index.html#oauth2securityscheme", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "OpenIdConnectSecurityScheme", "anchor": "openidconnectsecurityscheme", "md_text": "Bases: `TypedDict`\n\nOpenID Connect security scheme.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 130 131 132 133 134 135 136 137 138 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class OpenIdConnectSecurityScheme(TypedDict):     \"\"\"OpenID Connect security scheme.\"\"\"      type: Literal['openIdConnect']     open_id_connect_url: str     \"\"\"OpenId Connect URL to discover OAuth2 configuration values.\"\"\"     description: NotRequired[str]     \"\"\"Description of this security scheme.\"\"\" ``` |\n\n#### open\\_id\\_connect\\_url `instance-attribute`\n\n```\nopen_id_connect_url: str\n```\n\nOpenId Connect URL to discover OAuth2 configuration values.\n\n#### description `instance-attribute`\n\n```\ndescription: NotRequired[str]\n```\n\nDescription of this security scheme.", "url": "https://ai.pydantic.dev/fasta2a/index.html#openidconnectsecurityscheme", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "SecurityScheme `module-attribute`", "anchor": "securityscheme-module-attribute", "md_text": "```\nSecurityScheme = Annotated[\n    Union[\n        HttpSecurityScheme,\n        ApiKeySecurityScheme,\n        OAuth2SecurityScheme,\n        OpenIdConnectSecurityScheme,\n    ],\n    Field(discriminator=\"type\"),\n]\n```\n\nA security scheme for authentication.", "url": "https://ai.pydantic.dev/fasta2a/index.html#securityscheme-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "AgentInterface", "anchor": "agentinterface", "md_text": "Bases: `TypedDict`\n\nAn interface that the agent supports.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 148 149 150 151 152 153 154 155 156 157 158 159 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class AgentInterface(TypedDict):     \"\"\"An interface that the agent supports.\"\"\"      transport: str     \"\"\"The transport protocol (e.g., 'jsonrpc', 'websocket').\"\"\"      url: str     \"\"\"The URL endpoint for this transport.\"\"\"      description: NotRequired[str]     \"\"\"Description of this interface.\"\"\" ``` |\n\n#### transport `instance-attribute`\n\n```\ntransport: str\n```\n\nThe transport protocol (e.g., 'jsonrpc', 'websocket').\n\n#### url `instance-attribute`\n\n```\nurl: str\n```\n\nThe URL endpoint for this transport.\n\n#### description `instance-attribute`\n\n```\ndescription: NotRequired[str]\n```\n\nDescription of this interface.", "url": "https://ai.pydantic.dev/fasta2a/index.html#agentinterface", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "AgentExtension", "anchor": "agentextension", "md_text": "Bases: `TypedDict`\n\nA declaration of an extension supported by an Agent.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class AgentExtension(TypedDict):     \"\"\"A declaration of an extension supported by an Agent.\"\"\"      uri: str     \"\"\"The URI of the extension.\"\"\"      description: NotRequired[str]     \"\"\"A description of how this agent uses this extension.\"\"\"      required: NotRequired[bool]     \"\"\"Whether the client must follow specific requirements of the extension.\"\"\"      params: NotRequired[dict[str, Any]]     \"\"\"Optional configuration for the extension.\"\"\" ``` |\n\n#### uri `instance-attribute`\n\n```\nuri: str\n```\n\nThe URI of the extension.\n\n#### description `instance-attribute`\n\n```\ndescription: NotRequired[str]\n```\n\nA description of how this agent uses this extension.\n\n#### required `instance-attribute`\n\n```\nrequired: NotRequired[bool]\n```\n\nWhether the client must follow specific requirements of the extension.\n\n#### params `instance-attribute`\n\n```\nparams: NotRequired[dict[str, Any]]\n```\n\nOptional configuration for the extension.", "url": "https://ai.pydantic.dev/fasta2a/index.html#agentextension", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "Skill", "anchor": "skill", "md_text": "Bases: `TypedDict`\n\nSkills are a unit of capability that an agent can perform.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class Skill(TypedDict):     \"\"\"Skills are a unit of capability that an agent can perform.\"\"\"      id: str     \"\"\"A unique identifier for the skill.\"\"\"      name: str     \"\"\"Human readable name of the skill.\"\"\"      description: str     \"\"\"A human-readable description of the skill.      It will be used by the client or a human as a hint to understand the skill.     \"\"\"      tags: list[str]     \"\"\"Set of tag-words describing classes of capabilities for this specific skill.      Examples: \"cooking\", \"customer support\", \"billing\".     \"\"\"      examples: NotRequired[list[str]]     \"\"\"The set of example scenarios that the skill can perform.      Will be used by the client as a hint to understand how the skill can be used. (e.g. \"I need a recipe for bread\")     \"\"\"      input_modes: list[str]     \"\"\"Supported mime types for input data.\"\"\"      output_modes: list[str]     \"\"\"Supported mime types for output data.\"\"\" ``` |\n\n#### id `instance-attribute`\n\n```\nid: str\n```\n\nA unique identifier for the skill.\n\n#### name `instance-attribute`\n\n```\nname: str\n```\n\nHuman readable name of the skill.\n\n#### description `instance-attribute`\n\n```\ndescription: str\n```\n\nA human-readable description of the skill.\n\nIt will be used by the client or a human as a hint to understand the skill.\n\n#### tags `instance-attribute`\n\n```\ntags: list[str]\n```\n\nSet of tag-words describing classes of capabilities for this specific skill.\n\nExamples: \"cooking\", \"customer support\", \"billing\".\n\n#### examples `instance-attribute`\n\n```\nexamples: NotRequired[list[str]]\n```\n\nThe set of example scenarios that the skill can perform.\n\nWill be used by the client as a hint to understand how the skill can be used. (e.g. \"I need a recipe for bread\")\n\n#### input\\_modes `instance-attribute`\n\n```\ninput_modes: list[str]\n```\n\nSupported mime types for input data.\n\n#### output\\_modes `instance-attribute`\n\n```\noutput_modes: list[str]\n```\n\nSupported mime types for output data.", "url": "https://ai.pydantic.dev/fasta2a/index.html#skill", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "Artifact", "anchor": "artifact", "md_text": "Bases: `TypedDict`\n\nAgents generate Artifacts as an end result of a Task.\n\nArtifacts are immutable, can be named, and can have multiple parts. A streaming response can append parts to\nexisting Artifacts.\n\nA single Task can generate many Artifacts. For example, \"create a webpage\" could create separate HTML and image\nArtifacts.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class Artifact(TypedDict):     \"\"\"Agents generate Artifacts as an end result of a Task.      Artifacts are immutable, can be named, and can have multiple parts. A streaming response can append parts to     existing Artifacts.      A single Task can generate many Artifacts. For example, \"create a webpage\" could create separate HTML and image     Artifacts.     \"\"\"      artifact_id: str     \"\"\"Unique identifier for the artifact.\"\"\"      name: NotRequired[str]     \"\"\"The name of the artifact.\"\"\"      description: NotRequired[str]     \"\"\"A description of the artifact.\"\"\"      parts: list[Part]     \"\"\"The parts that make up the artifact.\"\"\"      metadata: NotRequired[dict[str, Any]]     \"\"\"Metadata about the artifact.\"\"\"      extensions: NotRequired[list[str]]     \"\"\"Array of extensions.\"\"\"      append: NotRequired[bool]     \"\"\"Whether to append this artifact to an existing one.\"\"\"      last_chunk: NotRequired[bool]     \"\"\"Whether this is the last chunk of the artifact.\"\"\" ``` |\n\n#### artifact\\_id `instance-attribute`\n\n```\nartifact_id: str\n```\n\nUnique identifier for the artifact.\n\n#### name `instance-attribute`\n\n```\nname: NotRequired[str]\n```\n\nThe name of the artifact.\n\n#### description `instance-attribute`\n\n```\ndescription: NotRequired[str]\n```\n\nA description of the artifact.\n\n#### parts `instance-attribute`\n\n```\nparts: list[Part]\n```\n\nThe parts that make up the artifact.\n\n#### metadata `instance-attribute`\n\n```\nmetadata: NotRequired[dict[str, Any]]\n```\n\nMetadata about the artifact.\n\n#### extensions `instance-attribute`\n\n```\nextensions: NotRequired[list[str]]\n```\n\nArray of extensions.\n\n#### append `instance-attribute`\n\n```\nappend: NotRequired[bool]\n```\n\nWhether to append this artifact to an existing one.\n\n#### last\\_chunk `instance-attribute`\n\n```\nlast_chunk: NotRequired[bool]\n```\n\nWhether this is the last chunk of the artifact.", "url": "https://ai.pydantic.dev/fasta2a/index.html#artifact", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "PushNotificationConfig", "anchor": "pushnotificationconfig", "md_text": "Bases: `TypedDict`\n\nConfiguration for push notifications.\n\nA2A supports a secure notification mechanism whereby an agent can notify a client of an update\noutside of a connected session via a PushNotificationService. Within and across enterprises,\nit is critical that the agent verifies the identity of the notification service, authenticates\nitself with the service, and presents an identifier that ties the notification to the executing\nTask.\n\nThe target server of the PushNotificationService should be considered a separate service, and\nis not guaranteed (or even expected) to be the client directly. This PushNotificationService is\nresponsible for authenticating and authorizing the agent and for proxying the verified notification\nto the appropriate endpoint (which could be anything from a pub/sub queue, to an email inbox or\nother service, etc).\n\nFor contrived scenarios with isolated client-agent pairs (e.g. local service mesh in a contained\nVPC, etc.) or isolated environments without enterprise security concerns, the client may choose to\nsimply open a port and act as its own PushNotificationService. Any enterprise implementation will\nlikely have a centralized service that authenticates the remote agents with trusted notification\ncredentials and can handle online/offline scenarios. (This should be thought of similarly to a\nmobile Push Notification Service).\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class PushNotificationConfig(TypedDict):     \"\"\"Configuration for push notifications.      A2A supports a secure notification mechanism whereby an agent can notify a client of an update     outside of a connected session via a PushNotificationService. Within and across enterprises,     it is critical that the agent verifies the identity of the notification service, authenticates     itself with the service, and presents an identifier that ties the notification to the executing     Task.      The target server of the PushNotificationService should be considered a separate service, and     is not guaranteed (or even expected) to be the client directly. This PushNotificationService is     responsible for authenticating and authorizing the agent and for proxying the verified notification     to the appropriate endpoint (which could be anything from a pub/sub queue, to an email inbox or     other service, etc).      For contrived scenarios with isolated client-agent pairs (e.g. local service mesh in a contained     VPC, etc.) or isolated environments without enterprise security concerns, the client may choose to     simply open a port and act as its own PushNotificationService. Any enterprise implementation will     likely have a centralized service that authenticates the remote agents with trusted notification     credentials and can handle online/offline scenarios. (This should be thought of similarly to a     mobile Push Notification Service).     \"\"\"      id: NotRequired[str]     \"\"\"Server-assigned identifier.\"\"\"      url: str     \"\"\"The URL to send push notifications to.\"\"\"      token: NotRequired[str]     \"\"\"Token unique to this task/session.\"\"\"      authentication: NotRequired[SecurityScheme]     \"\"\"Authentication details for push notifications.\"\"\" ``` |\n\n#### id `instance-attribute`\n\n```\nid: NotRequired[str]\n```\n\nServer-assigned identifier.\n\n#### url `instance-attribute`\n\n```\nurl: str\n```\n\nThe URL to send push notifications to.\n\n#### token `instance-attribute`\n\n```\ntoken: NotRequired[str]\n```\n\nToken unique to this task/session.\n\n#### authentication `instance-attribute`\n\n```\nauthentication: NotRequired[SecurityScheme]\n```\n\nAuthentication details for push notifications.", "url": "https://ai.pydantic.dev/fasta2a/index.html#pushnotificationconfig", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "TaskPushNotificationConfig", "anchor": "taskpushnotificationconfig", "md_text": "Bases: `TypedDict`\n\nConfiguration for task push notifications.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 287 288 289 290 291 292 293 294 295 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class TaskPushNotificationConfig(TypedDict):     \"\"\"Configuration for task push notifications.\"\"\"      id: str     \"\"\"The task id.\"\"\"      push_notification_config: PushNotificationConfig     \"\"\"The push notification configuration.\"\"\" ``` |\n\n#### id `instance-attribute`\n\n```\nid: str\n```\n\nThe task id.\n\n#### push\\_notification\\_config `instance-attribute`\n\n```\npush_notification_config: PushNotificationConfig\n```\n\nThe push notification configuration.", "url": "https://ai.pydantic.dev/fasta2a/index.html#taskpushnotificationconfig", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "Message", "anchor": "message", "md_text": "Bases: `TypedDict`\n\nA Message contains any content that is not an Artifact.\n\nThis can include things like agent thoughts, user context, instructions, errors, status, or metadata.\n\nAll content from a client comes in the form of a Message. Agents send Messages to communicate status or to provide\ninstructions (whereas generated results are sent as Artifacts).\n\nA Message can have multiple parts to denote different pieces of content. For example, a user request could include\na textual description from a user and then multiple files used as context from the client.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class Message(TypedDict):     \"\"\"A Message contains any content that is not an Artifact.      This can include things like agent thoughts, user context, instructions, errors, status, or metadata.      All content from a client comes in the form of a Message. Agents send Messages to communicate status or to provide     instructions (whereas generated results are sent as Artifacts).      A Message can have multiple parts to denote different pieces of content. For example, a user request could include     a textual description from a user and then multiple files used as context from the client.     \"\"\"      role: Literal['user', 'agent']     \"\"\"The role of the message.\"\"\"      parts: list[Part]     \"\"\"The parts of the message.\"\"\"      kind: Literal['message']     \"\"\"Event type.\"\"\"      metadata: NotRequired[dict[str, Any]]     \"\"\"Metadata about the message.\"\"\"      # Additional fields     message_id: str     \"\"\"Identifier created by the message creator.\"\"\"      context_id: NotRequired[str]     \"\"\"The context the message is associated with.\"\"\"      task_id: NotRequired[str]     \"\"\"Identifier of task the message is related to.\"\"\"      reference_task_ids: NotRequired[list[str]]     \"\"\"Array of task IDs this message references.\"\"\"      extensions: NotRequired[list[str]]     \"\"\"Array of extensions.\"\"\" ``` |\n\n#### role `instance-attribute`\n\n```\nrole: Literal['user', 'agent']\n```\n\nThe role of the message.\n\n#### parts `instance-attribute`\n\n```\nparts: list[Part]\n```\n\nThe parts of the message.\n\n#### kind `instance-attribute`\n\n```\nkind: Literal['message']\n```\n\nEvent type.\n\n#### metadata `instance-attribute`\n\n```\nmetadata: NotRequired[dict[str, Any]]\n```\n\nMetadata about the message.\n\n#### message\\_id `instance-attribute`\n\n```\nmessage_id: str\n```\n\nIdentifier created by the message creator.\n\n#### context\\_id `instance-attribute`\n\n```\ncontext_id: NotRequired[str]\n```\n\nThe context the message is associated with.\n\n#### task\\_id `instance-attribute`\n\n```\ntask_id: NotRequired[str]\n```\n\nIdentifier of task the message is related to.\n\n#### reference\\_task\\_ids `instance-attribute`\n\n```\nreference_task_ids: NotRequired[list[str]]\n```\n\nArray of task IDs this message references.\n\n#### extensions `instance-attribute`\n\n```\nextensions: NotRequired[list[str]]\n```\n\nArray of extensions.", "url": "https://ai.pydantic.dev/fasta2a/index.html#message", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "TextPart", "anchor": "textpart", "md_text": "Bases: `_BasePart`\n\nA part that contains text.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 346 347 348 349 350 351 352 353 354 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class TextPart(_BasePart):     \"\"\"A part that contains text.\"\"\"      kind: Literal['text']     \"\"\"The kind of the part.\"\"\"      text: str     \"\"\"The text of the part.\"\"\" ``` |\n\n#### kind `instance-attribute`\n\n```\nkind: Literal['text']\n```\n\nThe kind of the part.\n\n#### text `instance-attribute`\n\n```\ntext: str\n```\n\nThe text of the part.", "url": "https://ai.pydantic.dev/fasta2a/index.html#textpart", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "FileWithBytes", "anchor": "filewithbytes", "md_text": "Bases: `TypedDict`\n\nFile with base64 encoded data.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 357 358 359 360 361 362 363 364 365 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class FileWithBytes(TypedDict):     \"\"\"File with base64 encoded data.\"\"\"      bytes: str     \"\"\"The base64 encoded content of the file.\"\"\"      mime_type: NotRequired[str]     \"\"\"Optional mime type for the file.\"\"\" ``` |\n\n#### bytes `instance-attribute`\n\n```\nbytes: str\n```\n\nThe base64 encoded content of the file.\n\n#### mime\\_type `instance-attribute`\n\n```\nmime_type: NotRequired[str]\n```\n\nOptional mime type for the file.", "url": "https://ai.pydantic.dev/fasta2a/index.html#filewithbytes", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "FileWithUri", "anchor": "filewithuri", "md_text": "Bases: `TypedDict`\n\nFile with URI reference.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 368 369 370 371 372 373 374 375 376 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class FileWithUri(TypedDict):     \"\"\"File with URI reference.\"\"\"      uri: str     \"\"\"The URI of the file.\"\"\"      mime_type: NotRequired[str]     \"\"\"The mime type of the file.\"\"\" ``` |\n\n#### uri `instance-attribute`\n\n```\nuri: str\n```\n\nThe URI of the file.\n\n#### mime\\_type `instance-attribute`\n\n```\nmime_type: NotRequired[str]\n```\n\nThe mime type of the file.", "url": "https://ai.pydantic.dev/fasta2a/index.html#filewithuri", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "FilePart", "anchor": "filepart", "md_text": "Bases: `_BasePart`\n\nA part that contains a file.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 379 380 381 382 383 384 385 386 387 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class FilePart(_BasePart):     \"\"\"A part that contains a file.\"\"\"      kind: Literal['file']     \"\"\"The kind of the part.\"\"\"      file: FileWithBytes | FileWithUri     \"\"\"The file content - either bytes or URI.\"\"\" ``` |\n\n#### kind `instance-attribute`\n\n```\nkind: Literal['file']\n```\n\nThe kind of the part.\n\n#### file `instance-attribute`\n\n```\nfile: FileWithBytes | FileWithUri\n```\n\nThe file content - either bytes or URI.", "url": "https://ai.pydantic.dev/fasta2a/index.html#filepart", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "DataPart", "anchor": "datapart", "md_text": "Bases: `_BasePart`\n\nA part that contains structured data.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 390 391 392 393 394 395 396 397 398 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class DataPart(_BasePart):     \"\"\"A part that contains structured data.\"\"\"      kind: Literal['data']     \"\"\"The kind of the part.\"\"\"      data: dict[str, Any]     \"\"\"The data of the part.\"\"\" ``` |\n\n#### kind `instance-attribute`\n\n```\nkind: Literal['data']\n```\n\nThe kind of the part.\n\n#### data `instance-attribute`\n\n```\ndata: dict[str, Any]\n```\n\nThe data of the part.", "url": "https://ai.pydantic.dev/fasta2a/index.html#datapart", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "Part `module-attribute`", "anchor": "part-module-attribute", "md_text": "```\nPart = Annotated[\n    Union[TextPart, FilePart, DataPart],\n    Field(discriminator=\"kind\"),\n]\n```\n\nA fully formed piece of content exchanged between a client and a remote agent as part of a Message or an Artifact.\n\nEach Part has its own content type and metadata.", "url": "https://ai.pydantic.dev/fasta2a/index.html#part-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "TaskState `module-attribute`", "anchor": "taskstate-module-attribute", "md_text": "```\nTaskState: TypeAlias = Literal[\n    \"submitted\",\n    \"working\",\n    \"input-required\",\n    \"completed\",\n    \"canceled\",\n    \"failed\",\n    \"rejected\",\n    \"auth-required\",\n    \"unknown\",\n]\n```\n\nThe possible states of a task.", "url": "https://ai.pydantic.dev/fasta2a/index.html#taskstate-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "TaskStatus", "anchor": "taskstatus", "md_text": "Bases: `TypedDict`\n\nStatus and accompanying message for a task.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 413 414 415 416 417 418 419 420 421 422 423 424 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class TaskStatus(TypedDict):     \"\"\"Status and accompanying message for a task.\"\"\"      state: TaskState     \"\"\"The current state of the task.\"\"\"      message: NotRequired[Message]     \"\"\"Additional status updates for client.\"\"\"      timestamp: NotRequired[str]     \"\"\"ISO datetime value of when the status was updated.\"\"\" ``` |\n\n#### state `instance-attribute`\n\n```\nstate: TaskState\n```\n\nThe current state of the task.\n\n#### message `instance-attribute`\n\n```\nmessage: NotRequired[Message]\n```\n\nAdditional status updates for client.\n\n#### timestamp `instance-attribute`\n\n```\ntimestamp: NotRequired[str]\n```\n\nISO datetime value of when the status was updated.", "url": "https://ai.pydantic.dev/fasta2a/index.html#taskstatus", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "Task", "anchor": "task", "md_text": "Bases: `TypedDict`\n\nA Task is a stateful entity that allows Clients and Remote Agents to achieve a specific outcome.\n\nClients and Remote Agents exchange Messages within a Task. Remote Agents generate results as Artifacts.\nA Task is always created by a Client and the status is always determined by the Remote Agent.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class Task(TypedDict):     \"\"\"A Task is a stateful entity that allows Clients and Remote Agents to achieve a specific outcome.      Clients and Remote Agents exchange Messages within a Task. Remote Agents generate results as Artifacts.     A Task is always created by a Client and the status is always determined by the Remote Agent.     \"\"\"      id: str     \"\"\"Unique identifier for the task.\"\"\"      context_id: str     \"\"\"The context the task is associated with.\"\"\"      kind: Literal['task']     \"\"\"Event type.\"\"\"      status: TaskStatus     \"\"\"Current status of the task.\"\"\"      history: NotRequired[list[Message]]     \"\"\"Optional history of messages.\"\"\"      artifacts: NotRequired[list[Artifact]]     \"\"\"Collection of artifacts created by the agent.\"\"\"      metadata: NotRequired[dict[str, Any]]     \"\"\"Extension metadata.\"\"\" ``` |\n\n#### id `instance-attribute`\n\n```\nid: str\n```\n\nUnique identifier for the task.\n\n#### context\\_id `instance-attribute`\n\n```\ncontext_id: str\n```\n\nThe context the task is associated with.\n\n#### kind `instance-attribute`\n\n```\nkind: Literal['task']\n```\n\nEvent type.\n\n#### status `instance-attribute`\n\n```\nstatus: TaskStatus\n```\n\nCurrent status of the task.\n\n#### history `instance-attribute`\n\n```\nhistory: NotRequired[list[Message]]\n```\n\nOptional history of messages.\n\n#### artifacts `instance-attribute`\n\n```\nartifacts: NotRequired[list[Artifact]]\n```\n\nCollection of artifacts created by the agent.\n\n#### metadata `instance-attribute`\n\n```\nmetadata: NotRequired[dict[str, Any]]\n```\n\nExtension metadata.", "url": "https://ai.pydantic.dev/fasta2a/index.html#task", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "TaskStatusUpdateEvent", "anchor": "taskstatusupdateevent", "md_text": "Bases: `TypedDict`\n\nSent by server during message/stream requests.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class TaskStatusUpdateEvent(TypedDict):     \"\"\"Sent by server during message/stream requests.\"\"\"      task_id: str     \"\"\"The id of the task.\"\"\"      context_id: str     \"\"\"The context the task is associated with.\"\"\"      kind: Literal['status-update']     \"\"\"Event type.\"\"\"      status: TaskStatus     \"\"\"The status of the task.\"\"\"      final: bool     \"\"\"Indicates the end of the event stream.\"\"\"      metadata: NotRequired[dict[str, Any]]     \"\"\"Extension metadata.\"\"\" ``` |\n\n#### task\\_id `instance-attribute`\n\n```\ntask_id: str\n```\n\nThe id of the task.\n\n#### context\\_id `instance-attribute`\n\n```\ncontext_id: str\n```\n\nThe context the task is associated with.\n\n#### kind `instance-attribute`\n\n```\nkind: Literal['status-update']\n```\n\nEvent type.\n\n#### status `instance-attribute`\n\n```\nstatus: TaskStatus\n```\n\nThe status of the task.\n\n#### final `instance-attribute`\n\n```\nfinal: bool\n```\n\nIndicates the end of the event stream.\n\n#### metadata `instance-attribute`\n\n```\nmetadata: NotRequired[dict[str, Any]]\n```\n\nExtension metadata.", "url": "https://ai.pydantic.dev/fasta2a/index.html#taskstatusupdateevent", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "TaskArtifactUpdateEvent", "anchor": "taskartifactupdateevent", "md_text": "Bases: `TypedDict`\n\nSent by server during message/stream requests.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class TaskArtifactUpdateEvent(TypedDict):     \"\"\"Sent by server during message/stream requests.\"\"\"      task_id: str     \"\"\"The id of the task.\"\"\"      context_id: str     \"\"\"The context the task is associated with.\"\"\"      kind: Literal['artifact-update']     \"\"\"Event type identification.\"\"\"      artifact: Artifact     \"\"\"The artifact that was updated.\"\"\"      append: NotRequired[bool]     \"\"\"Whether to append to existing artifact (true) or replace (false).\"\"\"      last_chunk: NotRequired[bool]     \"\"\"Indicates this is the final chunk of the artifact.\"\"\"      metadata: NotRequired[dict[str, Any]]     \"\"\"Extension metadata.\"\"\" ``` |\n\n#### task\\_id `instance-attribute`\n\n```\ntask_id: str\n```\n\nThe id of the task.\n\n#### context\\_id `instance-attribute`\n\n```\ncontext_id: str\n```\n\nThe context the task is associated with.\n\n#### kind `instance-attribute`\n\n```\nkind: Literal['artifact-update']\n```\n\nEvent type identification.\n\n#### artifact `instance-attribute`\n\n```\nartifact: Artifact\n```\n\nThe artifact that was updated.\n\n#### append `instance-attribute`\n\n```\nappend: NotRequired[bool]\n```\n\nWhether to append to existing artifact (true) or replace (false).\n\n#### last\\_chunk `instance-attribute`\n\n```\nlast_chunk: NotRequired[bool]\n```\n\nIndicates this is the final chunk of the artifact.\n\n#### metadata `instance-attribute`\n\n```\nmetadata: NotRequired[dict[str, Any]]\n```\n\nExtension metadata.", "url": "https://ai.pydantic.dev/fasta2a/index.html#taskartifactupdateevent", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "TaskIdParams", "anchor": "taskidparams", "md_text": "Bases: `TypedDict`\n\nParameters for a task id.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 506 507 508 509 510 511 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class TaskIdParams(TypedDict):     \"\"\"Parameters for a task id.\"\"\"      id: str     metadata: NotRequired[dict[str, Any]] ``` |", "url": "https://ai.pydantic.dev/fasta2a/index.html#taskidparams", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "TaskQueryParams", "anchor": "taskqueryparams", "md_text": "Bases: `TaskIdParams`\n\nQuery parameters for a task.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 514 515 516 517 518 519 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class TaskQueryParams(TaskIdParams):     \"\"\"Query parameters for a task.\"\"\"      history_length: NotRequired[int]     \"\"\"Number of recent messages to be retrieved.\"\"\" ``` |\n\n#### history\\_length `instance-attribute`\n\n```\nhistory_length: NotRequired[int]\n```\n\nNumber of recent messages to be retrieved.", "url": "https://ai.pydantic.dev/fasta2a/index.html#taskqueryparams", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "MessageSendConfiguration", "anchor": "messagesendconfiguration", "md_text": "Bases: `TypedDict`\n\nConfiguration for the send message request.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class MessageSendConfiguration(TypedDict):     \"\"\"Configuration for the send message request.\"\"\"      accepted_output_modes: list[str]     \"\"\"Accepted output modalities by the client.\"\"\"      blocking: NotRequired[bool]     \"\"\"If the server should treat the client as a blocking request.\"\"\"      history_length: NotRequired[int]     \"\"\"Number of recent messages to be retrieved.\"\"\"      push_notification_config: NotRequired[PushNotificationConfig]     \"\"\"Where the server should send notifications when disconnected.\"\"\" ``` |\n\n#### accepted\\_output\\_modes `instance-attribute`\n\n```\naccepted_output_modes: list[str]\n```\n\nAccepted output modalities by the client.\n\n#### blocking `instance-attribute`\n\n```\nblocking: NotRequired[bool]\n```\n\nIf the server should treat the client as a blocking request.\n\n#### history\\_length `instance-attribute`\n\n```\nhistory_length: NotRequired[int]\n```\n\nNumber of recent messages to be retrieved.\n\n#### push\\_notification\\_config `instance-attribute`\n\n```\npush_notification_config: NotRequired[\n    PushNotificationConfig\n]\n```\n\nWhere the server should send notifications when disconnected.", "url": "https://ai.pydantic.dev/fasta2a/index.html#messagesendconfiguration", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "MessageSendParams", "anchor": "messagesendparams", "md_text": "Bases: `TypedDict`\n\nParameters for message/send method.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 539 540 541 542 543 544 545 546 547 548 549 550 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class MessageSendParams(TypedDict):     \"\"\"Parameters for message/send method.\"\"\"      configuration: NotRequired[MessageSendConfiguration]     \"\"\"Send message configuration.\"\"\"      message: Message     \"\"\"The message being sent to the server.\"\"\"      metadata: NotRequired[dict[str, Any]]     \"\"\"Extension metadata.\"\"\" ``` |\n\n#### configuration `instance-attribute`\n\n```\nconfiguration: NotRequired[MessageSendConfiguration]\n```\n\nSend message configuration.\n\n#### message `instance-attribute`\n\n```\nmessage: Message\n```\n\nThe message being sent to the server.\n\n#### metadata `instance-attribute`\n\n```\nmetadata: NotRequired[dict[str, Any]]\n```\n\nExtension metadata.", "url": "https://ai.pydantic.dev/fasta2a/index.html#messagesendparams", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "TaskSendParams", "anchor": "tasksendparams", "md_text": "Bases: `TypedDict`\n\nInternal parameters for task execution within the framework.\n\nNote: This is not part of the A2A protocol - it's used internally\nfor broker/worker communication.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class TaskSendParams(TypedDict):     \"\"\"Internal parameters for task execution within the framework.      Note: This is not part of the A2A protocol - it's used internally     for broker/worker communication.     \"\"\"      id: str     \"\"\"The id of the task.\"\"\"      context_id: str     \"\"\"The context id for the task.\"\"\"      message: Message     \"\"\"The message to process.\"\"\"      history_length: NotRequired[int]     \"\"\"Number of recent messages to be retrieved.\"\"\"      metadata: NotRequired[dict[str, Any]]     \"\"\"Extension metadata.\"\"\" ``` |\n\n#### id `instance-attribute`\n\n```\nid: str\n```\n\nThe id of the task.\n\n#### context\\_id `instance-attribute`\n\n```\ncontext_id: str\n```\n\nThe context id for the task.\n\n#### message `instance-attribute`\n\n```\nmessage: Message\n```\n\nThe message to process.\n\n#### history\\_length `instance-attribute`\n\n```\nhistory_length: NotRequired[int]\n```\n\nNumber of recent messages to be retrieved.\n\n#### metadata `instance-attribute`\n\n```\nmetadata: NotRequired[dict[str, Any]]\n```\n\nExtension metadata.", "url": "https://ai.pydantic.dev/fasta2a/index.html#tasksendparams", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "ListTaskPushNotificationConfigParams", "anchor": "listtaskpushnotificationconfigparams", "md_text": "Bases: `TypedDict`\n\nParameters for getting list of pushNotificationConfigurations associated with a Task.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 577 578 579 580 581 582 583 584 585 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class ListTaskPushNotificationConfigParams(TypedDict):     \"\"\"Parameters for getting list of pushNotificationConfigurations associated with a Task.\"\"\"      id: str     \"\"\"Task id.\"\"\"      metadata: NotRequired[dict[str, Any]]     \"\"\"Extension metadata.\"\"\" ``` |\n\n#### id `instance-attribute`\n\n```\nid: str\n```\n\nTask id.\n\n#### metadata `instance-attribute`\n\n```\nmetadata: NotRequired[dict[str, Any]]\n```\n\nExtension metadata.", "url": "https://ai.pydantic.dev/fasta2a/index.html#listtaskpushnotificationconfigparams", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "DeleteTaskPushNotificationConfigParams", "anchor": "deletetaskpushnotificationconfigparams", "md_text": "Bases: `TypedDict`\n\nParameters for removing pushNotificationConfiguration associated with a Task.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 588 589 590 591 592 593 594 595 596 597 598 599 ``` | ``` @pydantic.with_config({'alias_generator': to_camel}) class DeleteTaskPushNotificationConfigParams(TypedDict):     \"\"\"Parameters for removing pushNotificationConfiguration associated with a Task.\"\"\"      id: str     \"\"\"Task id.\"\"\"      push_notification_config_id: str     \"\"\"The push notification config id to delete.\"\"\"      metadata: NotRequired[dict[str, Any]]     \"\"\"Extension metadata.\"\"\" ``` |\n\n#### id `instance-attribute`\n\n```\nid: str\n```\n\nTask id.\n\n#### push\\_notification\\_config\\_id `instance-attribute`\n\n```\npush_notification_config_id: str\n```\n\nThe push notification config id to delete.\n\n#### metadata `instance-attribute`\n\n```\nmetadata: NotRequired[dict[str, Any]]\n```\n\nExtension metadata.", "url": "https://ai.pydantic.dev/fasta2a/index.html#deletetaskpushnotificationconfigparams", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "JSONRPCMessage", "anchor": "jsonrpcmessage", "md_text": "Bases: `TypedDict`\n\nA JSON RPC message.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 602 603 604 605 606 607 608 609 ``` | ``` class JSONRPCMessage(TypedDict):     \"\"\"A JSON RPC message.\"\"\"      jsonrpc: Literal['2.0']     \"\"\"The JSON RPC version.\"\"\"      id: int | str | None     \"\"\"The request id.\"\"\" ``` |\n\n#### jsonrpc `instance-attribute`\n\n```\njsonrpc: Literal['2.0']\n```\n\nThe JSON RPC version.\n\n#### id `instance-attribute`\n\n```\nid: int | str | None\n```\n\nThe request id.", "url": "https://ai.pydantic.dev/fasta2a/index.html#jsonrpcmessage", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "JSONRPCRequest", "anchor": "jsonrpcrequest", "md_text": "Bases: `JSONRPCMessage`, `Generic[Method, Params]`\n\nA JSON RPC request.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 616 617 618 619 620 621 622 623 ``` | ``` class JSONRPCRequest(JSONRPCMessage, Generic[Method, Params]):     \"\"\"A JSON RPC request.\"\"\"      method: Method     \"\"\"The method to call.\"\"\"      params: Params     \"\"\"The parameters to pass to the method.\"\"\" ``` |\n\n#### method `instance-attribute`\n\n```\nmethod: Method\n```\n\nThe method to call.\n\n#### params `instance-attribute`\n\n```\nparams: Params\n```\n\nThe parameters to pass to the method.", "url": "https://ai.pydantic.dev/fasta2a/index.html#jsonrpcrequest", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "JSONRPCError", "anchor": "jsonrpcerror", "md_text": "Bases: `TypedDict`, `Generic[CodeT, MessageT]`\n\nA JSON RPC error.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 634 635 636 637 638 639 ``` | ``` class JSONRPCError(TypedDict, Generic[CodeT, MessageT]):     \"\"\"A JSON RPC error.\"\"\"      code: CodeT     message: MessageT     data: NotRequired[Any] ``` |", "url": "https://ai.pydantic.dev/fasta2a/index.html#jsonrpcerror", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "JSONRPCResponse", "anchor": "jsonrpcresponse", "md_text": "Bases: `JSONRPCMessage`, `Generic[ResultT, ErrorT]`\n\nA JSON RPC response.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/schema.py`\n\n|  |  |\n| --- | --- |\n| ``` 646 647 648 649 650 ``` | ``` class JSONRPCResponse(JSONRPCMessage, Generic[ResultT, ErrorT]):     \"\"\"A JSON RPC response.\"\"\"      result: NotRequired[ResultT]     error: NotRequired[ErrorT] ``` |", "url": "https://ai.pydantic.dev/fasta2a/index.html#jsonrpcresponse", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "JSONParseError `module-attribute`", "anchor": "jsonparseerror-module-attribute", "md_text": "```\nJSONParseError = JSONRPCError[\n    Literal[-32700], Literal[\"Invalid JSON payload\"]\n]\n```\n\nA JSON RPC error for a parse error.", "url": "https://ai.pydantic.dev/fasta2a/index.html#jsonparseerror-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "InvalidRequestError `module-attribute`", "anchor": "invalidrequesterror-module-attribute", "md_text": "```\nInvalidRequestError = JSONRPCError[\n    Literal[-32600],\n    Literal[\"Request payload validation error\"],\n]\n```\n\nA JSON RPC error for an invalid request.", "url": "https://ai.pydantic.dev/fasta2a/index.html#invalidrequesterror-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "MethodNotFoundError `module-attribute`", "anchor": "methodnotfounderror-module-attribute", "md_text": "```\nMethodNotFoundError = JSONRPCError[\n    Literal[-32601], Literal[\"Method not found\"]\n]\n```\n\nA JSON RPC error for a method not found.", "url": "https://ai.pydantic.dev/fasta2a/index.html#methodnotfounderror-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "InvalidParamsError `module-attribute`", "anchor": "invalidparamserror-module-attribute", "md_text": "```\nInvalidParamsError = JSONRPCError[\n    Literal[-32602], Literal[\"Invalid parameters\"]\n]\n```\n\nA JSON RPC error for invalid parameters.", "url": "https://ai.pydantic.dev/fasta2a/index.html#invalidparamserror-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "InternalError `module-attribute`", "anchor": "internalerror-module-attribute", "md_text": "```\nInternalError = JSONRPCError[\n    Literal[-32603], Literal[\"Internal error\"]\n]\n```\n\nA JSON RPC error for an internal error.", "url": "https://ai.pydantic.dev/fasta2a/index.html#internalerror-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "TaskNotFoundError `module-attribute`", "anchor": "tasknotfounderror-module-attribute", "md_text": "```\nTaskNotFoundError = JSONRPCError[\n    Literal[-32001], Literal[\"Task not found\"]\n]\n```\n\nA JSON RPC error for a task not found.", "url": "https://ai.pydantic.dev/fasta2a/index.html#tasknotfounderror-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "TaskNotCancelableError `module-attribute`", "anchor": "tasknotcancelableerror-module-attribute", "md_text": "```\nTaskNotCancelableError = JSONRPCError[\n    Literal[-32002], Literal[\"Task not cancelable\"]\n]\n```\n\nA JSON RPC error for a task not cancelable.", "url": "https://ai.pydantic.dev/fasta2a/index.html#tasknotcancelableerror-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "PushNotificationNotSupportedError `module-attribute`", "anchor": "pushnotificationnotsupportederror-module-attribute", "md_text": "```\nPushNotificationNotSupportedError = JSONRPCError[\n    Literal[-32003],\n    Literal[\"Push notification not supported\"],\n]\n```\n\nA JSON RPC error for a push notification not supported.", "url": "https://ai.pydantic.dev/fasta2a/index.html#pushnotificationnotsupportederror-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "UnsupportedOperationError `module-attribute`", "anchor": "unsupportedoperationerror-module-attribute", "md_text": "```\nUnsupportedOperationError = JSONRPCError[\n    Literal[-32004],\n    Literal[\"This operation is not supported\"],\n]\n```\n\nA JSON RPC error for an unsupported operation.", "url": "https://ai.pydantic.dev/fasta2a/index.html#unsupportedoperationerror-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "ContentTypeNotSupportedError `module-attribute`", "anchor": "contenttypenotsupportederror-module-attribute", "md_text": "```\nContentTypeNotSupportedError = JSONRPCError[\n    Literal[-32005], Literal[\"Incompatible content types\"]\n]\n```\n\nA JSON RPC error for incompatible content types.", "url": "https://ai.pydantic.dev/fasta2a/index.html#contenttypenotsupportederror-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "InvalidAgentResponseError `module-attribute`", "anchor": "invalidagentresponseerror-module-attribute", "md_text": "```\nInvalidAgentResponseError = JSONRPCError[\n    Literal[-32006], Literal[\"Invalid agent response\"]\n]\n```\n\nA JSON RPC error for invalid agent response.", "url": "https://ai.pydantic.dev/fasta2a/index.html#invalidagentresponseerror-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "SendMessageRequest `module-attribute`", "anchor": "sendmessagerequest-module-attribute", "md_text": "```\nSendMessageRequest = JSONRPCRequest[\n    Literal[\"message/send\"], MessageSendParams\n]\n```\n\nA JSON RPC request to send a message.", "url": "https://ai.pydantic.dev/fasta2a/index.html#sendmessagerequest-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "SendMessageResponse `module-attribute`", "anchor": "sendmessageresponse-module-attribute", "md_text": "```\nSendMessageResponse = JSONRPCResponse[\n    Union[Task, Message], JSONRPCError[Any, Any]\n]\n```\n\nA JSON RPC response to send a message.", "url": "https://ai.pydantic.dev/fasta2a/index.html#sendmessageresponse-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "StreamMessageRequest `module-attribute`", "anchor": "streammessagerequest-module-attribute", "md_text": "```\nStreamMessageRequest = JSONRPCRequest[\n    Literal[\"message/stream\"], MessageSendParams\n]\n```\n\nA JSON RPC request to stream a message.", "url": "https://ai.pydantic.dev/fasta2a/index.html#streammessagerequest-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "GetTaskRequest `module-attribute`", "anchor": "gettaskrequest-module-attribute", "md_text": "```\nGetTaskRequest = JSONRPCRequest[\n    Literal[\"tasks/get\"], TaskQueryParams\n]\n```\n\nA JSON RPC request to get a task.", "url": "https://ai.pydantic.dev/fasta2a/index.html#gettaskrequest-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "GetTaskResponse `module-attribute`", "anchor": "gettaskresponse-module-attribute", "md_text": "```\nGetTaskResponse = JSONRPCResponse[Task, TaskNotFoundError]\n```\n\nA JSON RPC response to get a task.", "url": "https://ai.pydantic.dev/fasta2a/index.html#gettaskresponse-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "CancelTaskRequest `module-attribute`", "anchor": "canceltaskrequest-module-attribute", "md_text": "```\nCancelTaskRequest = JSONRPCRequest[\n    Literal[\"tasks/cancel\"], TaskIdParams\n]\n```\n\nA JSON RPC request to cancel a task.", "url": "https://ai.pydantic.dev/fasta2a/index.html#canceltaskrequest-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "CancelTaskResponse `module-attribute`", "anchor": "canceltaskresponse-module-attribute", "md_text": "```\nCancelTaskResponse = JSONRPCResponse[\n    Task, Union[TaskNotCancelableError, TaskNotFoundError]\n]\n```\n\nA JSON RPC response to cancel a task.", "url": "https://ai.pydantic.dev/fasta2a/index.html#canceltaskresponse-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "SetTaskPushNotificationRequest `module-attribute`", "anchor": "settaskpushnotificationrequest-module-attribute", "md_text": "```\nSetTaskPushNotificationRequest = JSONRPCRequest[\n    Literal[\"tasks/pushNotification/set\"],\n    TaskPushNotificationConfig,\n]\n```\n\nA JSON RPC request to set a task push notification.", "url": "https://ai.pydantic.dev/fasta2a/index.html#settaskpushnotificationrequest-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "SetTaskPushNotificationResponse `module-attribute`", "anchor": "settaskpushnotificationresponse-module-attribute", "md_text": "```\nSetTaskPushNotificationResponse = JSONRPCResponse[\n    TaskPushNotificationConfig,\n    PushNotificationNotSupportedError,\n]\n```\n\nA JSON RPC response to set a task push notification.", "url": "https://ai.pydantic.dev/fasta2a/index.html#settaskpushnotificationresponse-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "GetTaskPushNotificationRequest `module-attribute`", "anchor": "gettaskpushnotificationrequest-module-attribute", "md_text": "```\nGetTaskPushNotificationRequest = JSONRPCRequest[\n    Literal[\"tasks/pushNotification/get\"], TaskIdParams\n]\n```\n\nA JSON RPC request to get a task push notification.", "url": "https://ai.pydantic.dev/fasta2a/index.html#gettaskpushnotificationrequest-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "GetTaskPushNotificationResponse `module-attribute`", "anchor": "gettaskpushnotificationresponse-module-attribute", "md_text": "```\nGetTaskPushNotificationResponse = JSONRPCResponse[\n    TaskPushNotificationConfig,\n    PushNotificationNotSupportedError,\n]\n```\n\nA JSON RPC response to get a task push notification.", "url": "https://ai.pydantic.dev/fasta2a/index.html#gettaskpushnotificationresponse-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "ResubscribeTaskRequest `module-attribute`", "anchor": "resubscribetaskrequest-module-attribute", "md_text": "```\nResubscribeTaskRequest = JSONRPCRequest[\n    Literal[\"tasks/resubscribe\"], TaskIdParams\n]\n```\n\nA JSON RPC request to resubscribe to a task.", "url": "https://ai.pydantic.dev/fasta2a/index.html#resubscribetaskrequest-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "ListTaskPushNotificationConfigRequest `module-attribute`", "anchor": "listtaskpushnotificationconfigrequest-module-attribute", "md_text": "```\nListTaskPushNotificationConfigRequest = JSONRPCRequest[\n    Literal[\"tasks/pushNotificationConfig/list\"],\n    ListTaskPushNotificationConfigParams,\n]\n```\n\nA JSON RPC request to list task push notification configs.", "url": "https://ai.pydantic.dev/fasta2a/index.html#listtaskpushnotificationconfigrequest-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "DeleteTaskPushNotificationConfigRequest `module-attribute`", "anchor": "deletetaskpushnotificationconfigrequest-module-attribute", "md_text": "```\nDeleteTaskPushNotificationConfigRequest = JSONRPCRequest[\n    Literal[\"tasks/pushNotificationConfig/delete\"],\n    DeleteTaskPushNotificationConfigParams,\n]\n```\n\nA JSON RPC request to delete a task push notification config.", "url": "https://ai.pydantic.dev/fasta2a/index.html#deletetaskpushnotificationconfigrequest-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "A2ARequest `module-attribute`", "anchor": "a2arequest-module-attribute", "md_text": "```\nA2ARequest = Annotated[\n    Union[\n        SendMessageRequest,\n        StreamMessageRequest,\n        GetTaskRequest,\n        CancelTaskRequest,\n        SetTaskPushNotificationRequest,\n        GetTaskPushNotificationRequest,\n        ResubscribeTaskRequest,\n        ListTaskPushNotificationConfigRequest,\n        DeleteTaskPushNotificationConfigRequest,\n    ],\n    Discriminator(\"method\"),\n]\n```\n\nA JSON RPC request to the A2A server.", "url": "https://ai.pydantic.dev/fasta2a/index.html#a2arequest-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "A2AResponse `module-attribute`", "anchor": "a2aresponse-module-attribute", "md_text": "```\nA2AResponse: TypeAlias = Union[\n    SendMessageResponse,\n    GetTaskResponse,\n    CancelTaskResponse,\n    SetTaskPushNotificationResponse,\n    GetTaskPushNotificationResponse,\n]\n```\n\nA JSON RPC response from the A2A server.", "url": "https://ai.pydantic.dev/fasta2a/index.html#a2aresponse-module-attribute", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "A2AClient", "anchor": "a2aclient", "md_text": "A client for the A2A protocol.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/client.py`\n\n|  |  |\n| --- | --- |\n| ``` 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 ``` | ``` class A2AClient:     \"\"\"A client for the A2A protocol.\"\"\"      def __init__(self, base_url: str = 'http://localhost:8000', http_client: httpx.AsyncClient | None = None) -> None:         if http_client is None:             self.http_client = httpx.AsyncClient(base_url=base_url)         else:             self.http_client = http_client             self.http_client.base_url = base_url      async def send_message(         self,         message: Message,         *,         metadata: dict[str, Any] | None = None,         configuration: MessageSendConfiguration | None = None,     ) -> SendMessageResponse:         \"\"\"Send a message using the A2A protocol.          Returns a JSON-RPC response containing either a result (Task) or an error.         \"\"\"         params = MessageSendParams(message=message)         if metadata is not None:             params['metadata'] = metadata         if configuration is not None:             params['configuration'] = configuration          request_id = str(uuid.uuid4())         payload = SendMessageRequest(jsonrpc='2.0', id=request_id, method='message/send', params=params)         content = send_message_request_ta.dump_json(payload, by_alias=True)         response = await self.http_client.post('/', content=content, headers={'Content-Type': 'application/json'})         self._raise_for_status(response)          return send_message_response_ta.validate_json(response.content)      async def get_task(self, task_id: str) -> GetTaskResponse:         payload = GetTaskRequest(jsonrpc='2.0', id=None, method='tasks/get', params={'id': task_id})         content = a2a_request_ta.dump_json(payload, by_alias=True)         response = await self.http_client.post('/', content=content, headers={'Content-Type': 'application/json'})         self._raise_for_status(response)         return get_task_response_ta.validate_json(response.content)      def _raise_for_status(self, response: httpx.Response) -> None:         if response.status_code >= 400:             raise UnexpectedResponseError(response.status_code, response.text) ``` |\n\n#### send\\_message `async`\n\n```\nsend_message(\n    message: Message,\n    *,\n    metadata: dict[str, Any] | None = None,\n    configuration: MessageSendConfiguration | None = None\n) -> SendMessageResponse\n```\n\nSend a message using the A2A protocol.\n\nReturns a JSON-RPC response containing either a result (Task) or an error.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/client.py`\n\n|  |  |\n| --- | --- |\n| ``` 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 ``` | ``` async def send_message(     self,     message: Message,     *,     metadata: dict[str, Any] | None = None,     configuration: MessageSendConfiguration | None = None, ) -> SendMessageResponse:     \"\"\"Send a message using the A2A protocol.      Returns a JSON-RPC response containing either a result (Task) or an error.     \"\"\"     params = MessageSendParams(message=message)     if metadata is not None:         params['metadata'] = metadata     if configuration is not None:         params['configuration'] = configuration      request_id = str(uuid.uuid4())     payload = SendMessageRequest(jsonrpc='2.0', id=request_id, method='message/send', params=params)     content = send_message_request_ta.dump_json(payload, by_alias=True)     response = await self.http_client.post('/', content=content, headers={'Content-Type': 'application/json'})     self._raise_for_status(response)      return send_message_response_ta.validate_json(response.content) ``` |", "url": "https://ai.pydantic.dev/fasta2a/index.html#a2aclient", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "UnexpectedResponseError", "anchor": "unexpectedresponseerror", "md_text": "Bases: `Exception`\n\nAn error raised when an unexpected response is received from the server.\n\nSource code in `.venv/lib/python3.12/site-packages/fasta2a/client.py`\n\n|  |  |\n| --- | --- |\n| ``` 78 79 80 81 82 83 ``` | ``` class UnexpectedResponseError(Exception):     \"\"\"An error raised when an unexpected response is received from the server.\"\"\"      def __init__(self, status_code: int, content: str) -> None:         self.status_code = status_code         self.content = content ``` |", "url": "https://ai.pydantic.dev/fasta2a/index.html#unexpectedresponseerror", "page": "fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "MCP Server", "anchor": "mcp-server", "md_text": "Here's a simple example of a [Python MCP server](https://github.com/modelcontextprotocol/python-sdk) using Pydantic AI within a tool call:\n\nmcp\\_server.py\n\n```\nfrom mcp.server.fastmcp import FastMCP\n\nfrom pydantic_ai import Agent\n\nserver = FastMCP('Pydantic AI Server')\nserver_agent = Agent(\n    'anthropic:claude-3-5-haiku-latest', system_prompt='always reply in rhyme'\n)\n\n\n@server.tool()\nasync def poet(theme: str) -> str:\n    \"\"\"Poem generator\"\"\"\n    r = await server_agent.run(f'write a poem about {theme}')\n    return r.output\n\n\nif __name__ == '__main__':\n    server.run()\n```", "url": "https://ai.pydantic.dev/server/index.html#mcp-server", "page": "server/index.html", "source_site": "pydantic_ai"}
{"title": "Simple client", "anchor": "simple-client", "md_text": "This server can be queried with any MCP client. Here is an example using the Python SDK directly:\n\nmcp\\_client.py\n\n```\nimport asyncio\nimport os\n\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\n\n\nasync def client():\n    server_params = StdioServerParameters(\n        command='python', args=['mcp_server.py'], env=os.environ\n    )\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write) as session:\n            await session.initialize()\n            result = await session.call_tool('poet', {'theme': 'socks'})\n            print(result.content[0].text)\n            \"\"\"\n            Oh, socks, those garments soft and sweet,\n            That nestle softly 'round our feet,\n            From cotton, wool, or blended thread,\n            They keep our toes from feeling dread.\n            \"\"\"\n\n\nif __name__ == '__main__':\n    asyncio.run(client())\n```", "url": "https://ai.pydantic.dev/server/index.html#simple-client", "page": "server/index.html", "source_site": "pydantic_ai"}
{"title": "MCP Sampling", "anchor": "mcp-sampling", "md_text": "See the [MCP client docs](../client/index.html#mcp-sampling) for details of what MCP sampling is, and how you can support it when using Pydantic AI as an MCP client.\n\nWhen Pydantic AI agents are used within MCP servers, they can use sampling via [`MCPSamplingModel`](../models/mcp-sampling/index.html#pydantic_ai.models.mcp_sampling.MCPSamplingModel).\n\nWe can extend the above example to use sampling so instead of connecting directly to the LLM, the agent calls back through the MCP client to make LLM calls.\n\nmcp\\_server\\_sampling.py\n\n```\nfrom mcp.server.fastmcp import Context, FastMCP\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.mcp_sampling import MCPSamplingModel\n\nserver = FastMCP('Pydantic AI Server with sampling')\nserver_agent = Agent(system_prompt='always reply in rhyme')\n\n\n@server.tool()\nasync def poet(ctx: Context, theme: str) -> str:\n    \"\"\"Poem generator\"\"\"\n    r = await server_agent.run(f'write a poem about {theme}', model=MCPSamplingModel(session=ctx.session))\n    return r.output\n\n\nif __name__ == '__main__':\n    server.run()  # run the server over stdio\n```\n\nThe [above](index.html#simple-client) client does not support sampling, so if you tried to use it with this server you'd get an error.\n\nThe simplest way to support sampling in an MCP client is to [use](../client/index.html#mcp-sampling) a Pydantic AI agent as the client, but if you wanted to support sampling with the vanilla MCP SDK, you could do so like this:\n\nmcp\\_client\\_sampling.py\n\n```\nimport asyncio\nfrom typing import Any\n\nfrom mcp import ClientSession, StdioServerParameters\nfrom mcp.client.stdio import stdio_client\nfrom mcp.shared.context import RequestContext\nfrom mcp.types import (\n    CreateMessageRequestParams,\n    CreateMessageResult,\n    ErrorData,\n    TextContent,\n)\n\n\nasync def sampling_callback(\n    context: RequestContext[ClientSession, Any], params: CreateMessageRequestParams\n) -> CreateMessageResult | ErrorData:\n    print('sampling system prompt:', params.systemPrompt)\n    #> sampling system prompt: always reply in rhyme\n    print('sampling messages:', params.messages)\n    \"\"\"\n    sampling messages:\n    [\n        SamplingMessage(\n            role='user',\n            content=TextContent(\n                type='text',\n                text='write a poem about socks',\n                annotations=None,\n                meta=None,\n            ),\n        )\n    ]\n    \"\"\"\n\n    # TODO get the response content by calling an LLM...\n    response_content = 'Socks for a fox.'\n\n    return CreateMessageResult(\n        role='assistant',\n        content=TextContent(type='text', text=response_content),\n        model='fictional-llm',\n    )\n\n\nasync def client():\n    server_params = StdioServerParameters(command='python', args=['mcp_server_sampling.py'])\n    async with stdio_client(server_params) as (read, write):\n        async with ClientSession(read, write, sampling_callback=sampling_callback) as session:\n            await session.initialize()\n            result = await session.call_tool('poet', {'theme': 'socks'})\n            print(result.content[0].text)\n            #> Socks for a fox.\n\n\nif __name__ == '__main__':\n    asyncio.run(client())\n```\n\n*(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/server/index.html#mcp-sampling", "page": "server/index.html", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "md_text": "With [dependencies installed and environment variables set](../setup/index.html#usage), run:\n\npipuv\n\n```\npython -m pydantic_ai_examples.question_graph\n\nuv run -m pydantic_ai_examples.question_graph\n```", "url": "https://ai.pydantic.dev/question-graph/index.html#running-the-example", "page": "question-graph/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "md_text": "[question\\_graph.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/question_graph.py)\n\n```\n\"\"\"Example of a graph for asking and evaluating questions.\n\nRun with:\n\n    uv run -m pydantic_ai_examples.question_graph\n\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nfrom dataclasses import dataclass, field\nfrom pathlib import Path\n\nimport logfire\nfrom groq import BaseModel\n\nfrom pydantic_ai import Agent, ModelMessage, format_as_xml\nfrom pydantic_graph import (\n    BaseNode,\n    End,\n    Graph,\n    GraphRunContext,\n)\nfrom pydantic_graph.persistence.file import FileStatePersistence\n\n# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\nlogfire.configure(send_to_logfire='if-token-present')\nlogfire.instrument_pydantic_ai()\n\nask_agent = Agent('openai:gpt-4o', output_type=str)\n\n\n@dataclass\nclass QuestionState:\n    question: str | None = None\n    ask_agent_messages: list[ModelMessage] = field(default_factory=list)\n    evaluate_agent_messages: list[ModelMessage] = field(default_factory=list)\n\n\n@dataclass\nclass Ask(BaseNode[QuestionState]):\n    async def run(self, ctx: GraphRunContext[QuestionState]) -> Answer:\n        result = await ask_agent.run(\n            'Ask a simple question with a single correct answer.',\n            message_history=ctx.state.ask_agent_messages,\n        )\n        ctx.state.ask_agent_messages += result.all_messages()\n        ctx.state.question = result.output\n        return Answer(result.output)\n\n\n@dataclass\nclass Answer(BaseNode[QuestionState]):\n    question: str\n\n    async def run(self, ctx: GraphRunContext[QuestionState]) -> Evaluate:\n        answer = input(f'{self.question}: ')\n        return Evaluate(answer)\n\n\nclass EvaluationOutput(BaseModel, use_attribute_docstrings=True):\n    correct: bool\n    \"\"\"Whether the answer is correct.\"\"\"\n    comment: str\n    \"\"\"Comment on the answer, reprimand the user if the answer is wrong.\"\"\"\n\n\nevaluate_agent = Agent(\n    'openai:gpt-4o',\n    output_type=EvaluationOutput,\n    system_prompt='Given a question and answer, evaluate if the answer is correct.',\n)\n\n\n@dataclass\nclass Evaluate(BaseNode[QuestionState, None, str]):\n    answer: str\n\n    async def run(\n        self,\n        ctx: GraphRunContext[QuestionState],\n    ) -> End[str] | Reprimand:\n        assert ctx.state.question is not None\n        result = await evaluate_agent.run(\n            format_as_xml({'question': ctx.state.question, 'answer': self.answer}),\n            message_history=ctx.state.evaluate_agent_messages,\n        )\n        ctx.state.evaluate_agent_messages += result.all_messages()\n        if result.output.correct:\n            return End(result.output.comment)\n        else:\n            return Reprimand(result.output.comment)\n\n\n@dataclass\nclass Reprimand(BaseNode[QuestionState]):\n    comment: str\n\n    async def run(self, ctx: GraphRunContext[QuestionState]) -> Ask:\n        print(f'Comment: {self.comment}')\n        ctx.state.question = None\n        return Ask()\n\n\nquestion_graph = Graph(\n    nodes=(Ask, Answer, Evaluate, Reprimand), state_type=QuestionState\n)\n\n\nasync def run_as_continuous():\n    state = QuestionState()\n    node = Ask()\n    end = await question_graph.run(node, state=state)\n    print('END:', end.output)\n\n\nasync def run_as_cli(answer: str | None):\n    persistence = FileStatePersistence(Path('question_graph.json'))\n    persistence.set_graph_types(question_graph)\n\n    if snapshot := await persistence.load_next():\n        state = snapshot.state\n        assert answer is not None, (\n            'answer required, usage \"uv run -m pydantic_ai_examples.question_graph cli <answer>\"'\n        )\n        node = Evaluate(answer)\n    else:\n        state = QuestionState()\n        node = Ask()\n    # debug(state, node)\n\n    async with question_graph.iter(node, state=state, persistence=persistence) as run:\n        while True:\n            node = await run.next()\n            if isinstance(node, End):\n                print('END:', node.data)\n                history = await persistence.load_all()\n                print('history:', '\\n'.join(str(e.node) for e in history), sep='\\n')\n                print('Finished!')\n                break\n            elif isinstance(node, Answer):\n                print(node.question)\n                break\n            # otherwise just continue\n\n\nif __name__ == '__main__':\n    import asyncio\n    import sys\n\n    try:\n        sub_command = sys.argv[1]\n        assert sub_command in ('continuous', 'cli', 'mermaid')\n    except (IndexError, AssertionError):\n        print(\n            'Usage:\\n'\n            '  uv run -m pydantic_ai_examples.question_graph mermaid\\n'\n            'or:\\n'\n            '  uv run -m pydantic_ai_examples.question_graph continuous\\n'\n            'or:\\n'\n            '  uv run -m pydantic_ai_examples.question_graph cli [answer]',\n            file=sys.stderr,\n        )\n        sys.exit(1)\n\n    if sub_command == 'mermaid':\n        print(question_graph.mermaid_code(start_node=Ask))\n    elif sub_command == 'continuous':\n        asyncio.run(run_as_continuous())\n    else:\n        a = sys.argv[2] if len(sys.argv) > 2 else None\n        asyncio.run(run_as_cli(a))\n```\n\nThe mermaid diagram generated in this example looks like this:", "url": "https://ai.pydantic.dev/question-graph/index.html#example-code", "page": "question-graph/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "md_text": "```\n---\ntitle: question_graph\n---\nstateDiagram-v2\n  [*] --> Ask\n  Ask --> Answer: ask the question\n  Answer --> Evaluate: answer the question\n  Evaluate --> Congratulate\n  Evaluate --> Castigate\n  Congratulate --> [*]: success\n  Castigate --> Ask: try again\n```", "url": "https://ai.pydantic.dev/question-graph/index.html#example-code", "page": "question-graph/index.html", "source_site": "pydantic_ai"}
{"title": "ModelRetry", "anchor": "modelretry", "md_text": "Bases: `Exception`\n\nException to raise when a tool function should be retried.\n\nThe agent will return the message to the model and ask it to try calling the function/tool again.\n\nSource code in `pydantic_ai_slim/pydantic_ai/exceptions.py`\n\n|  |  |\n| --- | --- |\n| ``` 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 ``` | ``` class ModelRetry(Exception):     \"\"\"Exception to raise when a tool function should be retried.      The agent will return the message to the model and ask it to try calling the function/tool again.     \"\"\"      message: str     \"\"\"The message to return to the model.\"\"\"      def __init__(self, message: str):         self.message = message         super().__init__(message)      def __eq__(self, other: Any) -> bool:         return isinstance(other, self.__class__) and other.message == self.message      @classmethod     def __get_pydantic_core_schema__(cls, _: Any, __: Any) -> core_schema.CoreSchema:         \"\"\"Pydantic core schema to allow `ModelRetry` to be (de)serialized.\"\"\"         schema = core_schema.typed_dict_schema(             {                 'message': core_schema.typed_dict_field(core_schema.str_schema()),                 'kind': core_schema.typed_dict_field(core_schema.literal_schema(['model-retry'])),             }         )         return core_schema.no_info_after_validator_function(             lambda dct: ModelRetry(dct['message']),             schema,             serialization=core_schema.plain_serializer_function_ser_schema(                 lambda x: {'message': x.message, 'kind': 'model-retry'},                 return_schema=schema,             ),         ) ``` |\n\n#### message `instance-attribute`\n\n```\nmessage: str = message\n```\n\nThe message to return to the model.\n\n#### \\_\\_get\\_pydantic\\_core\\_schema\\_\\_ `classmethod`\n\n```\n__get_pydantic_core_schema__(_: Any, __: Any) -> CoreSchema\n```\n\nPydantic core schema to allow `ModelRetry` to be (de)serialized.\n\nSource code in `pydantic_ai_slim/pydantic_ai/exceptions.py`\n\n|  |  |\n| --- | --- |\n| ``` 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 ``` | ``` @classmethod def __get_pydantic_core_schema__(cls, _: Any, __: Any) -> core_schema.CoreSchema:     \"\"\"Pydantic core schema to allow `ModelRetry` to be (de)serialized.\"\"\"     schema = core_schema.typed_dict_schema(         {             'message': core_schema.typed_dict_field(core_schema.str_schema()),             'kind': core_schema.typed_dict_field(core_schema.literal_schema(['model-retry'])),         }     )     return core_schema.no_info_after_validator_function(         lambda dct: ModelRetry(dct['message']),         schema,         serialization=core_schema.plain_serializer_function_ser_schema(             lambda x: {'message': x.message, 'kind': 'model-retry'},             return_schema=schema,         ),     ) ``` |", "url": "https://ai.pydantic.dev/exceptions/index.html#modelretry", "page": "exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "CallDeferred", "anchor": "calldeferred", "md_text": "Bases: `Exception`\n\nException to raise when a tool call should be deferred.\n\nSee [tools docs](https://ai.pydantic.dev/deferred-tools/#deferred-tools) for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/exceptions.py`\n\n|  |  |\n| --- | --- |\n| ``` 66 67 68 69 70 71 72 ``` | ``` class CallDeferred(Exception):     \"\"\"Exception to raise when a tool call should be deferred.      See [tools docs](../deferred-tools.md#deferred-tools) for more information.     \"\"\"      pass ``` |", "url": "https://ai.pydantic.dev/exceptions/index.html#calldeferred", "page": "exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "ApprovalRequired", "anchor": "approvalrequired", "md_text": "Bases: `Exception`\n\nException to raise when a tool call requires human-in-the-loop approval.\n\nSee [tools docs](https://ai.pydantic.dev/deferred-tools/#human-in-the-loop-tool-approval) for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/exceptions.py`\n\n|  |  |\n| --- | --- |\n| ``` 75 76 77 78 79 80 81 ``` | ``` class ApprovalRequired(Exception):     \"\"\"Exception to raise when a tool call requires human-in-the-loop approval.      See [tools docs](../deferred-tools.md#human-in-the-loop-tool-approval) for more information.     \"\"\"      pass ``` |", "url": "https://ai.pydantic.dev/exceptions/index.html#approvalrequired", "page": "exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "UserError", "anchor": "usererror", "md_text": "Bases: `RuntimeError`\n\nError caused by a usage mistake by the application developer — You!\n\nSource code in `pydantic_ai_slim/pydantic_ai/exceptions.py`\n\n|  |  |\n| --- | --- |\n| ``` 84 85 86 87 88 89 90 91 92 ``` | ``` class UserError(RuntimeError):     \"\"\"Error caused by a usage mistake by the application developer — You!\"\"\"      message: str     \"\"\"Description of the mistake.\"\"\"      def __init__(self, message: str):         self.message = message         super().__init__(message) ``` |\n\n#### message `instance-attribute`\n\n```\nmessage: str = message\n```\n\nDescription of the mistake.", "url": "https://ai.pydantic.dev/exceptions/index.html#usererror", "page": "exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "AgentRunError", "anchor": "agentrunerror", "md_text": "Bases: `RuntimeError`\n\nBase class for errors occurring during an agent run.\n\nSource code in `pydantic_ai_slim/pydantic_ai/exceptions.py`\n\n|  |  |\n| --- | --- |\n| ```  95  96  97  98  99 100 101 102 103 104 105 106 ``` | ``` class AgentRunError(RuntimeError):     \"\"\"Base class for errors occurring during an agent run.\"\"\"      message: str     \"\"\"The error message.\"\"\"      def __init__(self, message: str):         self.message = message         super().__init__(message)      def __str__(self) -> str:         return self.message ``` |\n\n#### message `instance-attribute`\n\n```\nmessage: str = message\n```\n\nThe error message.", "url": "https://ai.pydantic.dev/exceptions/index.html#agentrunerror", "page": "exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "UsageLimitExceeded", "anchor": "usagelimitexceeded", "md_text": "Bases: `AgentRunError`\n\nError raised when a Model's usage exceeds the specified limits.\n\nSource code in `pydantic_ai_slim/pydantic_ai/exceptions.py`\n\n|  |  |\n| --- | --- |\n| ``` 109 110 ``` | ``` class UsageLimitExceeded(AgentRunError):     \"\"\"Error raised when a Model's usage exceeds the specified limits.\"\"\" ``` |", "url": "https://ai.pydantic.dev/exceptions/index.html#usagelimitexceeded", "page": "exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "UnexpectedModelBehavior", "anchor": "unexpectedmodelbehavior", "md_text": "Bases: `AgentRunError`\n\nError caused by unexpected Model behavior, e.g. an unexpected response code.\n\nSource code in `pydantic_ai_slim/pydantic_ai/exceptions.py`\n\n|  |  |\n| --- | --- |\n| ``` 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 ``` | ``` class UnexpectedModelBehavior(AgentRunError):     \"\"\"Error caused by unexpected Model behavior, e.g. an unexpected response code.\"\"\"      message: str     \"\"\"Description of the unexpected behavior.\"\"\"     body: str | None     \"\"\"The body of the response, if available.\"\"\"      def __init__(self, message: str, body: str | None = None):         self.message = message         if body is None:             self.body: str | None = None         else:             try:                 self.body = json.dumps(json.loads(body), indent=2)             except ValueError:                 self.body = body         super().__init__(message)      def __str__(self) -> str:         if self.body:             return f'{self.message}, body:\\n{self.body}'         else:             return self.message ``` |\n\n#### message `instance-attribute`\n\n```\nmessage: str = message\n```\n\nDescription of the unexpected behavior.\n\n#### body `instance-attribute`\n\n```\nbody: str | None = dumps(loads(body), indent=2)\n```\n\nThe body of the response, if available.", "url": "https://ai.pydantic.dev/exceptions/index.html#unexpectedmodelbehavior", "page": "exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "ModelHTTPError", "anchor": "modelhttperror", "md_text": "Bases: `AgentRunError`\n\nRaised when an model provider response has a status code of 4xx or 5xx.\n\nSource code in `pydantic_ai_slim/pydantic_ai/exceptions.py`\n\n|  |  |\n| --- | --- |\n| ``` 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 ``` | ``` class ModelHTTPError(AgentRunError):     \"\"\"Raised when an model provider response has a status code of 4xx or 5xx.\"\"\"      status_code: int     \"\"\"The HTTP status code returned by the API.\"\"\"      model_name: str     \"\"\"The name of the model associated with the error.\"\"\"      body: object | None     \"\"\"The body of the response, if available.\"\"\"      message: str     \"\"\"The error message with the status code and response body, if available.\"\"\"      def __init__(self, status_code: int, model_name: str, body: object | None = None):         self.status_code = status_code         self.model_name = model_name         self.body = body         message = f'status_code: {status_code}, model_name: {model_name}, body: {body}'         super().__init__(message) ``` |\n\n#### message `instance-attribute`\n\n```\nmessage: str\n```\n\nThe error message with the status code and response body, if available.\n\n#### status\\_code `instance-attribute`\n\n```\nstatus_code: int = status_code\n```\n\nThe HTTP status code returned by the API.\n\n#### model\\_name `instance-attribute`\n\n```\nmodel_name: str = model_name\n```\n\nThe name of the model associated with the error.\n\n#### body `instance-attribute`\n\n```\nbody: object | None = body\n```\n\nThe body of the response, if available.", "url": "https://ai.pydantic.dev/exceptions/index.html#modelhttperror", "page": "exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "FallbackExceptionGroup", "anchor": "fallbackexceptiongroup", "md_text": "Bases: `ExceptionGroup[Any]`\n\nA group of exceptions that can be raised when all fallback models fail.\n\nSource code in `pydantic_ai_slim/pydantic_ai/exceptions.py`\n\n|  |  |\n| --- | --- |\n| ``` 162 163 ``` | ``` class FallbackExceptionGroup(ExceptionGroup[Any]):     \"\"\"A group of exceptions that can be raised when all fallback models fail.\"\"\" ``` |", "url": "https://ai.pydantic.dev/exceptions/index.html#fallbackexceptiongroup", "page": "exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "IncompleteToolCall", "anchor": "incompletetoolcall", "md_text": "Bases: `UnexpectedModelBehavior`\n\nError raised when a model stops due to token limit while emitting a tool call.\n\nSource code in `pydantic_ai_slim/pydantic_ai/exceptions.py`\n\n|  |  |\n| --- | --- |\n| ``` 174 175 ``` | ``` class IncompleteToolCall(UnexpectedModelBehavior):     \"\"\"Error raised when a model stops due to token limit while emitting a tool call.\"\"\" ``` |", "url": "https://ai.pydantic.dev/exceptions/index.html#incompletetoolcall", "page": "exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "md_text": "With [dependencies installed and environment variables set](../setup/index.html#usage), run:\n\npipuv\n\n```\npython -m pydantic_ai_examples.stream_markdown\n\nuv run -m pydantic_ai_examples.stream_markdown\n```", "url": "https://ai.pydantic.dev/stream-markdown/index.html#running-the-example", "page": "stream-markdown/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "md_text": "[stream\\_markdown.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/stream_markdown.py)\n\n```\n\"\"\"This example shows how to stream markdown from an agent, using the `rich` library to display the markdown.\n\nRun with:\n\n    uv run -m pydantic_ai_examples.stream_markdown\n\"\"\"\n\nimport asyncio\nimport os\n\nimport logfire\nfrom rich.console import Console, ConsoleOptions, RenderResult\nfrom rich.live import Live\nfrom rich.markdown import CodeBlock, Markdown\nfrom rich.syntax import Syntax\nfrom rich.text import Text\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models import KnownModelName\n\n# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\nlogfire.configure(send_to_logfire='if-token-present')\nlogfire.instrument_pydantic_ai()\n\nagent = Agent()\n\n# models to try, and the appropriate env var\nmodels: list[tuple[KnownModelName, str]] = [\n    ('google-gla:gemini-2.0-flash', 'GEMINI_API_KEY'),\n    ('openai:gpt-4o-mini', 'OPENAI_API_KEY'),\n    ('groq:llama-3.3-70b-versatile', 'GROQ_API_KEY'),\n]\n\n\nasync def main():\n    prettier_code_blocks()\n    console = Console()\n    prompt = 'Show me a short example of using Pydantic.'\n    console.log(f'Asking: {prompt}...', style='cyan')\n    for model, env_var in models:\n        if env_var in os.environ:\n            console.log(f'Using model: {model}')\n            with Live('', console=console, vertical_overflow='visible') as live:\n                async with agent.run_stream(prompt, model=model) as result:\n                    async for message in result.stream_output():\n                        live.update(Markdown(message))\n            console.log(result.usage())\n        else:\n            console.log(f'{model} requires {env_var} to be set.')\n\n\ndef prettier_code_blocks():\n    \"\"\"Make rich code blocks prettier and easier to copy.\n\n    From https://github.com/samuelcolvin/aicli/blob/v0.8.0/samuelcolvin_aicli.py#L22\n    \"\"\"\n\n    class SimpleCodeBlock(CodeBlock):\n        def __rich_console__(\n            self, console: Console, options: ConsoleOptions\n        ) -> RenderResult:\n            code = str(self.text).rstrip()\n            yield Text(self.lexer_name, style='dim')\n            yield Syntax(\n                code,\n                self.lexer_name,\n                theme=self.theme,\n                background_color='default',\n                word_wrap=True,\n            )\n            yield Text(f'/{self.lexer_name}', style='dim')\n\n    Markdown.elements['fence'] = SimpleCodeBlock\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```", "url": "https://ai.pydantic.dev/stream-markdown/index.html#example-code", "page": "stream-markdown/index.html", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "md_text": "To run this example properly, you might want to add two extra API keys **(Note if either key is missing, the code will fall back to dummy data, so they're not required)**:\n\n* A weather API key from [tomorrow.io](https://www.tomorrow.io/weather-api/) set via `WEATHER_API_KEY`\n* A geocoding API key from [geocode.maps.co](https://geocode.maps.co/) set via `GEO_API_KEY`\n\nWith [dependencies installed and environment variables set](../setup/index.html#usage), run:\n\npipuv\n\n```\npython -m pydantic_ai_examples.weather_agent\n\nuv run -m pydantic_ai_examples.weather_agent\n```", "url": "https://ai.pydantic.dev/weather-agent/index.html#running-the-example", "page": "weather-agent/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "md_text": "[weather\\_agent.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/weather_agent.py)\n\n```\n\"\"\"Example of Pydantic AI with multiple tools which the LLM needs to call in turn to answer a question.\n\nIn this case the idea is a \"weather\" agent — the user can ask for the weather in multiple cities,\nthe agent will use the `get_lat_lng` tool to get the latitude and longitude of the locations, then use\nthe `get_weather` tool to get the weather.\n\nRun with:\n\n    uv run -m pydantic_ai_examples.weather_agent\n\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport asyncio\nfrom dataclasses import dataclass\nfrom typing import Any\n\nimport logfire\nfrom httpx import AsyncClient\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, RunContext\n\n# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\nlogfire.configure(send_to_logfire='if-token-present')\nlogfire.instrument_pydantic_ai()\n\n\n@dataclass\nclass Deps:\n    client: AsyncClient\n\n\nweather_agent = Agent(\n    'openai:gpt-4.1-mini',\n    # 'Be concise, reply with one sentence.' is enough for some models (like openai) to use\n    # the below tools appropriately, but others like anthropic and gemini require a bit more direction.\n    instructions='Be concise, reply with one sentence.',\n    deps_type=Deps,\n    retries=2,\n)\n\n\nclass LatLng(BaseModel):\n    lat: float\n    lng: float\n\n\n@weather_agent.tool\nasync def get_lat_lng(ctx: RunContext[Deps], location_description: str) -> LatLng:\n    \"\"\"Get the latitude and longitude of a location.\n\n    Args:\n        ctx: The context.\n        location_description: A description of a location.\n    \"\"\"\n    # NOTE: the response here will be random, and is not related to the location description.\n    r = await ctx.deps.client.get(\n        'https://demo-endpoints.pydantic.workers.dev/latlng',\n        params={'location': location_description},\n    )\n    r.raise_for_status()\n    return LatLng.model_validate_json(r.content)\n\n\n@weather_agent.tool\nasync def get_weather(ctx: RunContext[Deps], lat: float, lng: float) -> dict[str, Any]:\n    \"\"\"Get the weather at a location.\n\n    Args:\n        ctx: The context.\n        lat: Latitude of the location.\n        lng: Longitude of the location.\n    \"\"\"\n    # NOTE: the responses here will be random, and are not related to the lat and lng.\n    temp_response, descr_response = await asyncio.gather(\n        ctx.deps.client.get(\n            'https://demo-endpoints.pydantic.workers.dev/number',\n            params={'min': 10, 'max': 30},\n        ),\n        ctx.deps.client.get(\n            'https://demo-endpoints.pydantic.workers.dev/weather',\n            params={'lat': lat, 'lng': lng},\n        ),\n    )\n    temp_response.raise_for_status()\n    descr_response.raise_for_status()\n    return {\n        'temperature': f'{temp_response.text} °C',\n        'description': descr_response.text,\n    }\n\n\nasync def main():\n    async with AsyncClient() as client:\n        logfire.instrument_httpx(client, capture_all=True)\n        deps = Deps(client=client)\n        result = await weather_agent.run(\n            'What is the weather like in London and in Wiltshire?', deps=deps\n        )\n        print('Response:', result.output)\n\n\nif __name__ == '__main__':\n    asyncio.run(main())\n```", "url": "https://ai.pydantic.dev/weather-agent/index.html#example-code", "page": "weather-agent/index.html", "source_site": "pydantic_ai"}
{"title": "Running the UI", "anchor": "running-the-ui", "md_text": "You can build multi-turn chat applications for your agent with [Gradio](https://www.gradio.app/), a framework for building AI web applications entirely in python. Gradio comes with built-in chat components and agent support so the entire UI will be implemented in a single python file!\n\nHere's what the UI looks like for the weather agent:\n\n```\npip install gradio>=5.9.0\npython/uv-run -m pydantic_ai_examples.weather_agent_gradio\n```", "url": "https://ai.pydantic.dev/weather-agent/index.html#running-the-ui", "page": "weather-agent/index.html", "source_site": "pydantic_ai"}
{"title": "UI Code", "anchor": "ui-code", "md_text": "[weather\\_agent\\_gradio.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/weather_agent_gradio.py)\n\n```\nfrom __future__ import annotations as _annotations\n\nimport json\n\nfrom httpx import AsyncClient\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import ToolCallPart, ToolReturnPart\nfrom pydantic_ai_examples.weather_agent import Deps, weather_agent\n\ntry:\n    import gradio as gr\nexcept ImportError as e:\n    raise ImportError(\n        'Please install gradio with `pip install gradio`. You must use python>=3.10.'\n    ) from e\n\nTOOL_TO_DISPLAY_NAME = {'get_lat_lng': 'Geocoding API', 'get_weather': 'Weather API'}\n\nclient = AsyncClient()\ndeps = Deps(client=client)\n\n\nasync def stream_from_agent(prompt: str, chatbot: list[dict], past_messages: list):\n    chatbot.append({'role': 'user', 'content': prompt})\n    yield gr.Textbox(interactive=False, value=''), chatbot, gr.skip()\n    async with weather_agent.run_stream(\n        prompt, deps=deps, message_history=past_messages\n    ) as result:\n        for message in result.new_messages():\n            for call in message.parts:\n                if isinstance(call, ToolCallPart):\n                    call_args = call.args_as_json_str()\n                    metadata = {\n                        'title': f'🛠️ Using {TOOL_TO_DISPLAY_NAME[call.tool_name]}',\n                    }\n                    if call.tool_call_id is not None:\n                        metadata['id'] = call.tool_call_id\n\n                    gr_message = {\n                        'role': 'assistant',\n                        'content': 'Parameters: ' + call_args,\n                        'metadata': metadata,\n                    }\n                    chatbot.append(gr_message)\n                if isinstance(call, ToolReturnPart):\n                    for gr_message in chatbot:\n                        if (\n                            gr_message.get('metadata', {}).get('id', '')\n                            == call.tool_call_id\n                        ):\n                            if isinstance(call.content, BaseModel):\n                                json_content = call.content.model_dump_json()\n                            else:\n                                json_content = json.dumps(call.content)\n                            gr_message['content'] += f'\\nOutput: {json_content}'\n                yield gr.skip(), chatbot, gr.skip()\n        chatbot.append({'role': 'assistant', 'content': ''})\n        async for message in result.stream_text():\n            chatbot[-1]['content'] = message\n            yield gr.skip(), chatbot, gr.skip()\n        past_messages = result.all_messages()\n\n        yield gr.Textbox(interactive=True), gr.skip(), past_messages\n\n\nasync def handle_retry(chatbot, past_messages: list, retry_data: gr.RetryData):\n    new_history = chatbot[: retry_data.index]\n    previous_prompt = chatbot[retry_data.index]['content']\n    past_messages = past_messages[: retry_data.index]\n    async for update in stream_from_agent(previous_prompt, new_history, past_messages):\n        yield update\n\n\ndef undo(chatbot, past_messages: list, undo_data: gr.UndoData):\n    new_history = chatbot[: undo_data.index]\n    past_messages = past_messages[: undo_data.index]\n    return chatbot[undo_data.index]['content'], new_history, past_messages\n\n\ndef select_data(message: gr.SelectData) -> str:\n    return message.value['text']\n\n\nwith gr.Blocks() as demo:\n    gr.HTML(\n        \"\"\"\n<div style=\"display: flex; justify-content: center; align-items: center; gap: 2rem; padding: 1rem; width: 100%\">\n    <img src=\"https://ai.pydantic.dev/img/logo-white.svg\" style=\"max-width: 200px; height: auto\">\n    <div>\n        <h1 style=\"margin: 0 0 1rem 0\">Weather Assistant</h1>\n        <h3 style=\"margin: 0 0 0.5rem 0\">\n            This assistant answer your weather questions.\n        </h3>\n    </div>\n</div>\n\"\"\"\n    )\n    past_messages = gr.State([])\n    chatbot = gr.Chatbot(\n        label='Packing Assistant',\n        type='messages',\n        avatar_images=(None, 'https://ai.pydantic.dev/img/logo-white.svg'),\n        examples=[\n            {'text': 'What is the weather like in Miami?'},\n            {'text': 'What is the weather like in London?'},\n        ],\n    )\n    with gr.Row():\n        prompt = gr.Textbox(\n            lines=1,\n            show_label=False,\n            placeholder='What is the weather like in New York City?',\n        )\n    generation = prompt.submit(\n        stream_from_agent,\n        inputs=[prompt, chatbot, past_messages],\n        outputs=[prompt, chatbot, past_messages],\n    )\n    chatbot.example_select(select_data, None, [prompt])\n    chatbot.retry(\n        handle_retry, [chatbot, past_messages], [prompt, chatbot, past_messages]\n    )\n    chatbot.undo(undo, [chatbot, past_messages], [prompt, chatbot, past_messages])\n\n\nif __name__ == '__main__':\n    demo.launch()\n```", "url": "https://ai.pydantic.dev/weather-agent/index.html#ui-code", "page": "weather-agent/index.html", "source_site": "pydantic_ai"}
{"title": "Durable Execution", "anchor": "durable-execution", "md_text": "In Temporal's durable execution implementation, a program that crashes or encounters an exception while interacting with a model or API will retry until it can successfully complete.\n\nTemporal relies primarily on a replay mechanism to recover from failures.\nAs the program makes progress, Temporal saves key inputs and decisions, allowing a re-started program to pick up right where it left off.\n\nThe key to making this work is to separate the application's repeatable (deterministic) and non-repeatable (non-deterministic) parts:\n\n1. Deterministic pieces, termed [**workflows**](https://docs.temporal.io/workflow-definition), execute the same way when re-run with the same inputs.\n2. Non-deterministic pieces, termed [**activities**](https://docs.temporal.io/activities), can run arbitrary code, performing I/O and any other operations.\n\nWorkflow code can run for extended periods and, if interrupted, resume exactly where it left off.\nCritically, workflow code generally *cannot* include any kind of I/O, over the network, disk, etc.\nActivity code faces no restrictions on I/O or external interactions, but if an activity fails part-way through it is restarted from the beginning.\n\nIf you are familiar with celery, it may be helpful to think of Temporal activities as similar to celery tasks, but where you wait for the task to complete and obtain its result before proceeding to the next step in the workflow.\nHowever, Temporal workflows and activities offer a great deal more flexibility and functionality than celery tasks.\n\nSee the [Temporal documentation](https://docs.temporal.io/evaluate/understanding-temporal#temporal-application-the-building-blocks) for more information\n\nIn the case of Pydantic AI agents, integration with Temporal means that [model requests](https://ai.pydantic.dev/models/overview/), [tool calls](https://ai.pydantic.dev/tools/) that may require I/O, and [MCP server communication](../client/index.html) all need to be offloaded to Temporal activities due to their I/O requirements, while the logic that coordinates them (i.e. the agent run) lives in the workflow. Code that handles a scheduled job or web request can then execute the workflow, which will in turn execute the activities as needed.\n\nThe diagram below shows the overall architecture of an agentic application in Temporal.\nThe Temporal Server is responsible for tracking program execution and making sure the associated state is preserved reliably (i.e., stored to an internal database, and possibly replicated across cloud regions).\nTemporal Server manages data in encrypted form, so all data processing occurs on the Worker, which runs the workflow and activities.\n\n```\n            +---------------------+\n            |   Temporal Server   |      (Stores workflow state,\n            +---------------------+       schedules activities,\n                     ^                    persists progress)\n                     |\n        Save state,  |   Schedule Tasks,\n        progress,    |   load state on resume\n        timeouts     |\n                     |\n+------------------------------------------------------+\n|                      Worker                          |\n|   +----------------------------------------------+   |\n|   |              Workflow Code                   |   |\n|   |       (Agent Run Loop)                       |   |\n|   +----------------------------------------------+   |\n|          |          |                |               |\n|          v          v                v               |\n|   +-----------+ +------------+ +-------------+       |\n|   | Activity  | | Activity   | |  Activity   |       |\n|   | (Tool)    | | (MCP Tool) | | (Model API) |       |\n|   +-----------+ +------------+ +-------------+       |\n|         |           |                |               |\n+------------------------------------------------------+\n          |           |                |\n          v           v                v\n      [External APIs, services, databases, etc.]\n```\n\nSee the [Temporal documentation](https://docs.temporal.io/evaluate/understanding-temporal#temporal-application-the-building-blocks) for more information.", "url": "https://ai.pydantic.dev/temporal/index.html#durable-execution", "page": "temporal/index.html", "source_site": "pydantic_ai"}
{"title": "Durable Agent", "anchor": "durable-agent", "md_text": "Any agent can be wrapped in a [`TemporalAgent`](../durable_exec/index.html#pydantic_ai.durable_exec.temporal.TemporalAgent) to get a durable agent that can be used inside a deterministic Temporal workflow, by automatically offloading all work that requires I/O (namely model requests, tool calls, and MCP server communication) to non-deterministic activities.\n\nAt the time of wrapping, the agent's [model](https://ai.pydantic.dev/models/overview/) and [toolsets](https://ai.pydantic.dev/toolsets/) (including function tools registered on the agent and MCP servers) are frozen, activities are dynamically created for each, and the original model and toolsets are wrapped to call on the worker to execute the corresponding activities instead of directly performing the actions inside the workflow. The original agent can still be used as normal outside the Temporal workflow, but any changes to its model or toolsets after wrapping will not be reflected in the durable agent.\n\nHere is a simple but complete example of wrapping an agent for durable execution, creating a Temporal workflow with durable execution logic, connecting to a Temporal server, and running the workflow from non-durable code. All it requires is a Temporal server to be [running locally](https://github.com/temporalio/temporal#download-and-start-temporal-server-locally):\n\n```\nbrew install temporal\ntemporal server start-dev\n```\n\ntemporal\\_agent.py\n\n```\nimport uuid\n\nfrom temporalio import workflow\nfrom temporalio.client import Client\nfrom temporalio.worker import Worker\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.durable_exec.temporal import (\n    AgentPlugin,\n    PydanticAIPlugin,\n    TemporalAgent,\n)\n\nagent = Agent(\n    'gpt-5',\n    instructions=\"You're an expert in geography.\",\n    name='geography',  # (10)!\n)\n\ntemporal_agent = TemporalAgent(agent)  # (1)!\n\n\n@workflow.defn\nclass GeographyWorkflow:  # (2)!\n    @workflow.run\n    async def run(self, prompt: str) -> str:\n        result = await temporal_agent.run(prompt)  # (3)!\n        return result.output\n\n\nasync def main():\n    client = await Client.connect(  # (4)!\n        'localhost:7233',  # (5)!\n        plugins=[PydanticAIPlugin()],  # (6)!\n    )\n\n    async with Worker(  # (7)!\n        client,\n        task_queue='geography',\n        workflows=[GeographyWorkflow],\n        plugins=[AgentPlugin(temporal_agent)],  # (8)!\n    ):\n        output = await client.execute_workflow(  # (9)!\n            GeographyWorkflow.run,\n            args=['What is the capital of Mexico?'],\n            id=f'geography-{uuid.uuid4()}',\n            task_queue='geography',\n        )\n        print(output)\n        #> Mexico City (Ciudad de México, CDMX)\n```\n\n1. The original `Agent` cannot be used inside a deterministic Temporal workflow, but the `TemporalAgent` can.\n2. As explained above, the workflow represents a deterministic piece of code that can use non-deterministic activities for operations that require I/O.\n3. [`TemporalAgent.run()`](../durable_exec/index.html#pydantic_ai.durable_exec.temporal.TemporalAgent.run) works just like [`Agent.run()`](../agent/index.html#pydantic_ai.agent.AbstractAgent.run), but it will automatically offload model requests, tool calls, and MCP server communication to Temporal activities.\n4. We connect to the Temporal server which keeps track of workflow and activity execution.\n5. This assumes the Temporal server is [running locally](https://github.com/temporalio/temporal#download-and-start-temporal-server-locally).\n6. The [`PydanticAIPlugin`](../durable_exec/index.html#pydantic_ai.durable_exec.temporal.PydanticAIPlugin) tells Temporal to use Pydantic for serialization and deserialization, and to treat [`UserError`](../exceptions/index.html#pydantic_ai.exceptions.UserError) exceptions as non-retryable.\n7. We start the worker that will listen on the specified task queue and run workflows and activities. In a real world application, this might be run in a separate service.\n8. The [`AgentPlugin`](../durable_exec/index.html#pydantic_ai.durable_exec.temporal.AgentPlugin) registers the `TemporalAgent`'s activities with the worker.\n9. We call on the server to execute the workflow on a worker that's listening on the specified task queue.\n10. The agent's `name` is used to uniquely identify its activities.\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\nIn a real world application, the agent, workflow, and worker are typically defined separately from the code that calls for a workflow to be executed.\nBecause Temporal workflows need to be defined at the top level of the file and the `TemporalAgent` instance is needed inside the workflow and when starting the worker (to register the activities), it needs to be defined at the top level of the file as well.\n\nFor more information on how to use Temporal in Python applications, see their [Python SDK guide](https://docs.temporal.io/develop/python).", "url": "https://ai.pydantic.dev/temporal/index.html#durable-agent", "page": "temporal/index.html", "source_site": "pydantic_ai"}
{"title": "Temporal Integration Considerations", "anchor": "temporal-integration-considerations", "md_text": "There are a few considerations specific to agents and toolsets when using Temporal for durable execution. These are important to understand to ensure that your agents and toolsets work correctly with Temporal's workflow and activity model.", "url": "https://ai.pydantic.dev/temporal/index.html#temporal-integration-considerations", "page": "temporal/index.html", "source_site": "pydantic_ai"}
{"title": "Agent Names and Toolset IDs", "anchor": "agent-names-and-toolset-ids", "md_text": "To ensure that Temporal knows what code to run when an activity fails or is interrupted and then restarted, even if your code is changed in between, each activity needs to have a name that's stable and unique.\n\nWhen `TemporalAgent` dynamically creates activities for the wrapped agent's model requests and toolsets (specifically those that implement their own tool listing and calling, i.e. [`FunctionToolset`](../toolsets/index.html#pydantic_ai.toolsets.FunctionToolset) and [`MCPServer`](../mcp/index.html#pydantic_ai.mcp.MCPServer)), their names are derived from the agent's [`name`](../agent/index.html#pydantic_ai.agent.AbstractAgent.name) and the toolsets' [`id`s](../toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.id). These fields are normally optional, but are required to be set when using Temporal. They should not be changed once the durable agent has been deployed to production as this would break active workflows.\n\nOther than that, any agent and toolset will just work!", "url": "https://ai.pydantic.dev/temporal/index.html#agent-names-and-toolset-ids", "page": "temporal/index.html", "source_site": "pydantic_ai"}
{"title": "Instructions Functions, Output Functions, and History Processors", "anchor": "instructions-functions-output-functions-and-history-processors", "md_text": "Pydantic AI runs non-async [instructions](https://ai.pydantic.dev/agents/#instructions) and [system prompt](https://ai.pydantic.dev/agents/#system-prompts) functions, [history processors](https://ai.pydantic.dev/message-history/#processing-message-history), [output functions](https://ai.pydantic.dev/output/#output-functions), and [output validators](https://ai.pydantic.dev/output/#output-validator-functions) in threads, which are not supported inside Temporal workflows and require an activity. Ensure that these functions are async instead.\n\nSynchronous tool functions are supported, as tools are automatically run in activities unless this is [explicitly disabled](index.html#activity-configuration). Still, it's recommended to make tool functions async as well to improve performance.", "url": "https://ai.pydantic.dev/temporal/index.html#instructions-functions-output-functions-and-history-processors", "page": "temporal/index.html", "source_site": "pydantic_ai"}
{"title": "Agent Run Context and Dependencies", "anchor": "agent-run-context-and-dependencies", "md_text": "As workflows and activities run in separate processes, any values passed between them need to be serializable. As these payloads are stored in the workflow execution event history, Temporal limits their size to 2MB.\n\nTo account for these limitations, tool functions and the [event stream handler](index.html#streaming) running inside activities receive a limited version of the agent's [`RunContext`](../tools/index.html#pydantic_ai.tools.RunContext), and it's your responsibility to make sure that the [dependencies](https://ai.pydantic.dev/dependencies/) object provided to [`TemporalAgent.run()`](../durable_exec/index.html#pydantic_ai.durable_exec.temporal.TemporalAgent.run) can be serialized using Pydantic.\n\nSpecifically, only the `deps`, `retries`, `tool_call_id`, `tool_name`, `tool_call_approved`, `retry`, `max_retries` and `run_step` fields are available by default, and trying to access `model`, `usage`, `prompt`, `messages`, or `tracer` will raise an error.\nIf you need one or more of these attributes to be available inside activities, you can create a [`TemporalRunContext`](../durable_exec/index.html#pydantic_ai.durable_exec.temporal.TemporalRunContext) subclass with custom `serialize_run_context` and `deserialize_run_context` class methods and pass it to [`TemporalAgent`](../durable_exec/index.html#pydantic_ai.durable_exec.temporal.TemporalAgent) as `run_context_type`.", "url": "https://ai.pydantic.dev/temporal/index.html#agent-run-context-and-dependencies", "page": "temporal/index.html", "source_site": "pydantic_ai"}
{"title": "Streaming", "anchor": "streaming", "md_text": "Because Temporal activities cannot stream output directly to the activity call site, [`Agent.run_stream()`](../agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream), [`Agent.run_stream_events()`](../agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream_events), and [`Agent.iter()`](../agent/index.html#pydantic_ai.agent.Agent.iter) are not supported.\n\nInstead, you can implement streaming by setting an [`event_stream_handler`](../agent/index.html#pydantic_ai.agent.EventStreamHandler) on the `Agent` or `TemporalAgent` instance and using [`TemporalAgent.run()`](../durable_exec/index.html#pydantic_ai.durable_exec.temporal.TemporalAgent.run) inside the workflow.\nThe event stream handler function will receive the agent [run context](../tools/index.html#pydantic_ai.tools.RunContext) and an async iterable of events from the model's streaming response and the agent's execution of tools. For examples, see the [streaming docs](https://ai.pydantic.dev/agents/#streaming-all-events).\n\nAs the streaming model request activity, workflow, and workflow execution call all take place in separate processes, passing data between them requires some care:\n\n* To get data from the workflow call site or workflow to the event stream handler, you can use a [dependencies object](index.html#agent-run-context-and-dependencies).\n* To get data from the event stream handler to the workflow, workflow call site, or a frontend, you need to use an external system that the event stream handler can write to and the event consumer can read from, like a message queue. You can use the dependency object to make sure the same connection string or other unique ID is available in all the places that need it.", "url": "https://ai.pydantic.dev/temporal/index.html#streaming", "page": "temporal/index.html", "source_site": "pydantic_ai"}
{"title": "Activity Configuration", "anchor": "activity-configuration", "md_text": "Temporal activity configuration, like timeouts and retry policies, can be customized by passing [`temporalio.workflow.ActivityConfig`](https://python.temporal.io/temporalio.workflow.ActivityConfig.html) objects to the `TemporalAgent` constructor:\n\n* `activity_config`: The base Temporal activity config to use for all activities. If no config is provided, a `start_to_close_timeout` of 60 seconds is used.\n* `model_activity_config`: The Temporal activity config to use for model request activities. This is merged with the base activity config.\n* `toolset_activity_config`: The Temporal activity config to use for get-tools and call-tool activities for specific toolsets identified by ID. This is merged with the base activity config.\n* `tool_activity_config`: The Temporal activity config to use for specific tool call activities identified by toolset ID and tool name.\n  This is merged with the base and toolset-specific activity configs.\n\n  If a tool does not use I/O, you can specify `False` to disable using an activity. Note that the tool is required to be defined as an `async` function as non-async tools are run in threads which are non-deterministic and thus not supported outside of activities.", "url": "https://ai.pydantic.dev/temporal/index.html#activity-configuration", "page": "temporal/index.html", "source_site": "pydantic_ai"}
{"title": "Activity Retries", "anchor": "activity-retries", "md_text": "On top of the automatic retries for request failures that Temporal will perform, Pydantic AI and various provider API clients also have their own request retry logic. Enabling these at the same time may cause the request to be retried more often than expected, with improper `Retry-After` handling.\n\nWhen using Temporal, it's recommended to not use [HTTP Request Retries](https://ai.pydantic.dev/retries/) and to turn off your provider API client's own retry logic, for example by setting `max_retries=0` on a [custom `OpenAIProvider` API client](../openai/index.html#custom-openai-client).\n\nYou can customize Temporal's retry policy using [activity configuration](index.html#activity-configuration).", "url": "https://ai.pydantic.dev/temporal/index.html#activity-retries", "page": "temporal/index.html", "source_site": "pydantic_ai"}
{"title": "Observability with Logfire", "anchor": "observability-with-logfire", "md_text": "Temporal generates telemetry events and metrics for each workflow and activity execution, and Pydantic AI generates events for each agent run, model request and tool call. These can be sent to [Pydantic Logfire](https://ai.pydantic.dev/logfire/) to get a complete picture of what's happening in your application.\n\nTo use Logfire with Temporal, you need to pass a [`LogfirePlugin`](../durable_exec/index.html#pydantic_ai.durable_exec.temporal.LogfirePlugin) object to Temporal's `Client.connect()`:\n\nlogfire\\_plugin.py\n\n```\nfrom temporalio.client import Client\n\nfrom pydantic_ai.durable_exec.temporal import LogfirePlugin, PydanticAIPlugin\n\n\nasync def main():\n    client = await Client.connect(\n        'localhost:7233',\n        plugins=[PydanticAIPlugin(), LogfirePlugin()],\n    )\n```\n\nBy default, the `LogfirePlugin` will instrument Temporal (including metrics) and Pydantic AI and send all data to Logfire. To customize Logfire configuration and instrumentation, you can pass a `logfire_setup` function to the `LogfirePlugin` constructor and return a custom `Logfire` instance (i.e. the result of `logfire.configure()`). To disable sending Temporal metrics to Logfire, you can pass `metrics=False` to the `LogfirePlugin` constructor.", "url": "https://ai.pydantic.dev/temporal/index.html#observability-with-logfire", "page": "temporal/index.html", "source_site": "pydantic_ai"}
{"title": "Known Issues", "anchor": "known-issues", "md_text": "", "url": "https://ai.pydantic.dev/temporal/index.html#known-issues", "page": "temporal/index.html", "source_site": "pydantic_ai"}
{"title": "Pandas", "anchor": "pandas", "md_text": "When `logfire.info` is used inside an activity and the `pandas` package is among your project's dependencies, you may encounter the following error which seems to be the result of an import race condition:\n\n```\nAttributeError: partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)\n```\n\nTo fix this, you can use the [`temporalio.workflow.unsafe.imports_passed_through()`](https://python.temporal.io/temporalio.workflow.unsafe.html#imports_passed_through) context manager to proactively import the package and not have it be reloaded in the workflow sandbox:\n\ntemporal\\_activity.py\n\n```\nfrom temporalio import workflow\n\nwith workflow.unsafe.imports_passed_through():\n    import pandas\n```", "url": "https://ai.pydantic.dev/temporal/index.html#pandas", "page": "temporal/index.html", "source_site": "pydantic_ai"}
{"title": "AgentRun `dataclass`", "anchor": "agentrun-dataclass", "md_text": "Bases: `Generic[AgentDepsT, OutputDataT]`\n\nA stateful, async-iterable run of an [`Agent`](../agent/index.html#pydantic_ai.agent.Agent).\n\nYou generally obtain an `AgentRun` instance by calling `async with my_agent.iter(...) as agent_run:`.\n\nOnce you have an instance, you can use it to iterate through the run's nodes as they execute. When an\n[`End`](../pydantic_graph/nodes/index.html#pydantic_graph.nodes.End) is reached, the run finishes and [`result`](../agent/index.html#pydantic_ai.agent.AgentRun.result)\nbecomes available.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    nodes = []\n    # Iterate through the run, recording each node along the way:\n    async with agent.iter('What is the capital of France?') as agent_run:\n        async for node in agent_run:\n            nodes.append(node)\n    print(nodes)\n    '''\n    [\n        UserPromptNode(\n            user_prompt='What is the capital of France?',\n            instructions_functions=[],\n            system_prompts=(),\n            system_prompt_functions=[],\n            system_prompt_dynamic_functions={},\n        ),\n        ModelRequestNode(\n            request=ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='What is the capital of France?',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            )\n        ),\n        CallToolsNode(\n            model_response=ModelResponse(\n                parts=[TextPart(content='The capital of France is Paris.')],\n                usage=RequestUsage(input_tokens=56, output_tokens=7),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            )\n        ),\n        End(data=FinalResult(output='The capital of France is Paris.')),\n    ]\n    '''\n    print(agent_run.result.output)\n    #> The capital of France is Paris.\n```\n\nYou can also manually drive the iteration using the [`next`](../agent/index.html#pydantic_ai.agent.AgentRun.next) method for\nmore granular control.\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`", "url": "https://ai.pydantic.dev/run/index.html#agentrun-dataclass", "page": "run/index.html", "source_site": "pydantic_ai"}
{"title": "AgentRun `dataclass`", "anchor": "agentrun-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ```  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 ``` | ``` @dataclasses.dataclass(repr=False) class AgentRun(Generic[AgentDepsT, OutputDataT]):     \"\"\"A stateful, async-iterable run of an [`Agent`][pydantic_ai.agent.Agent].      You generally obtain an `AgentRun` instance by calling `async with my_agent.iter(...) as agent_run:`.      Once you have an instance, you can use it to iterate through the run's nodes as they execute. When an     [`End`][pydantic_graph.nodes.End] is reached, the run finishes and [`result`][pydantic_ai.agent.AgentRun.result]     becomes available.      Example:     ```python     from pydantic_ai import Agent      agent = Agent('openai:gpt-4o')      async def main():         nodes = []         # Iterate through the run, recording each node along the way:         async with agent.iter('What is the capital of France?') as agent_run:             async for node in agent_run:                 nodes.append(node)         print(nodes)         '''         [             UserPromptNode(                 user_prompt='What is the capital of France?',                 instructions_functions=[],                 system_prompts=(),                 system_prompt_functions=[],                 system_prompt_dynamic_functions={},             ),             ModelRequestNode(                 request=ModelRequest(                     parts=[                         UserPromptPart(                             content='What is the capital of France?',                             timestamp=datetime.datetime(...),                         )                     ]                 )             ),             CallToolsNode(                 model_response=ModelResponse(                     parts=[TextPart(content='The capital of France is Paris.')],                     usage=RequestUsage(input_tokens=56, output_tokens=7),                     model_name='gpt-4o',                     timestamp=datetime.datetime(...),                 )             ),             End(data=FinalResult(output='The capital of France is Paris.')),         ]         '''         print(agent_run.result.output)         #> The capital of France is Paris.     ```      You can also manually drive the iteration using the [`next`][pydantic_ai.agent.AgentRun.next] method for     more granular control.     \"\"\"      _graph_run: GraphRun[         _agent_graph.GraphAgentState, _agent_graph.GraphAgentDeps[AgentDepsT, Any], FinalResult[OutputDataT]     ]      @overload     def _traceparent(self, *, required: Literal[False]) -> str | None: ...     @overload     def _traceparent(self) -> str: ...     def _traceparent(self, *, required: bool = True) -> str | None:         traceparent = self._graph_run._traceparent(required=False)  # type: ignore[reportPrivateUsage]         if traceparent is None and required:  # pragma: no cover             raise AttributeError('No span was created for this agent run')         return traceparent      @property     def ctx(self) -> GraphRunContext[_agent_graph.GraphAgentState, _agent_graph.GraphAgentDeps[AgentDepsT, Any]]:         \"\"\"The current context of the agent run.\"\"\"         return GraphRunContext[_agent_graph.GraphAgentState, _agent_graph.GraphAgentDeps[AgentDepsT, Any]](             state=self._graph_run.state, deps=self._graph_run.deps         )      @property     def next_node(         self,     ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:         \"\"\"The next node that will be run in the agent graph.          This is the next node that will be used during async iteration, or if a node is not passed to `self.next(...)`.         \"\"\"         task = self._graph_run.next_task         return self._task_to_node(task)      @property     def result(self) -> AgentRunResult[OutputDataT] | None:         \"\"\"The final result of the run if it has ended, otherwise `None`.          Once the run returns an [`End`][pydantic_graph.nodes.End] node, `result` is populated         with an [`AgentRunResult`][pydantic_ai.agent.AgentRunResult].         \"\"\"         graph_run_output = self._graph_run.output         if graph_run_output is None:             return None         return AgentRunResult(             graph_run_output.output,             graph_run_output.tool_name,             self._graph_run.state,             self._graph_run.deps.new_message_index,             self._traceparent(required=False),         )      def __aiter__(         self,     ) -> AsyncIterator[_agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]]:         \"\"\"Provide async-iteration over the nodes in the agent run.\"\"\"         return self      async def __anext__(         self,     ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:         \"\"\"Advance to the next node automatically based on the last returned node.\"\"\"         task = await anext(self._graph_run)         return self._task_to_node(task)      def _task_to_node(         self, task: EndMarker[FinalResult[OutputDataT]] | JoinItem | Sequence[GraphTask]     ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:         if isinstance(task, Sequence) and len(task) == 1:             first_task = task[0]             if isinstance(first_task.inputs, BaseNode):  # pragma: no branch                 base_node: BaseNode[                     _agent_graph.GraphAgentState,                     _agent_graph.GraphAgentDeps[AgentDepsT, OutputDataT],                     FinalResult[OutputDataT],                 ] = first_task.inputs  # type: ignore[reportUnknownMemberType]                 if _agent_graph.is_agent_node(node=base_node):  # pragma: no branch                     return base_node         if isinstance(task, EndMarker):             return End(task.value)         raise exceptions.AgentRunError(f'Unexpected node: {task}')  # pragma: no cover      def _node_to_task(self, node: _agent_graph.AgentNode[AgentDepsT, OutputDataT]) -> GraphTask:         return GraphTask(NodeStep(type(node)).id, inputs=node, fork_stack=())      async def next(         self,         node: _agent_graph.AgentNode[AgentDepsT, OutputDataT],     ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:         \"\"\"Manually drive the agent run by passing in the node you want to run next.          This lets you inspect or mutate the node before continuing execution, or skip certain nodes         under dynamic conditions. The agent run should be stopped when you return an [`End`][pydantic_graph.nodes.End]         node.          Example:         ```python         from pydantic_ai import Agent         from pydantic_graph import End          agent = Agent('openai:gpt-4o')          async def main():             async with agent.iter('What is the capital of France?') as agent_run:                 next_node = agent_run.next_node  # start with the first node                 nodes = [next_node]                 while not isinstance(next_node, End):                     next_node = await agent_run.next(next_node)                     nodes.append(next_node)                 # Once `next_node` is an End, we've finished:                 print(nodes)                 '''                 [                     UserPromptNode(                         user_prompt='What is the capital of France?',                         instructions_functions=[],                         system_prompts=(),                         system_prompt_functions=[],                         system_prompt_dynamic_functions={},                     ),                     ModelRequestNode(                         request=ModelRequest(                             parts=[                                 UserPromptPart(                                     content='What is the capital of France?',                                     timestamp=datetime.datetime(...),                                 )                             ]                         )                     ),                     CallToolsNode(                         model_response=ModelResponse(                             parts=[TextPart(content='The capital of France is Paris.')],                             usage=RequestUsage(input_tokens=56, output_tokens=7),                             model_name='gpt-4o',                             timestamp=datetime.datetime(...),                         )                     ),                     End(data=FinalResult(output='The capital of France is Paris.')),                 ]                 '''                 print('Final result:', agent_run.result.output)                 #> Final result: The capital of France is Paris.         ```          Args:             node: The node to run next in the graph.          Returns:             The next node returned by the graph logic, or an [`End`][pydantic_graph.nodes.End] node if             the run has completed.         \"\"\"         # Note: It might be nice to expose a synchronous interface for iteration, but we shouldn't do it         # on this class, or else IDEs won't warn you if you accidentally use `for` instead of `async for` to iterate.         task = [self._node_to_task(node)]         try:             task = await self._graph_run.next(task)         except StopAsyncIteration:             pass         return self._task_to_node(task)      # TODO (v2): Make this a property     def usage(self) -> _usage.RunUsage:         \"\"\"Get usage statistics for the run so far, including token usage, model requests, and so on.\"\"\"         return self._graph_run.state.usage      def __repr__(self) -> str:  # pragma: no cover         result = self._graph_run.output         result_repr = '<run not finished>' if result is None else repr(result.output)         return f'<{type(self).__name__} result={result_repr} usage={self.usage()}>' ``` |", "url": "https://ai.pydantic.dev/run/index.html#agentrun-dataclass", "page": "run/index.html", "source_site": "pydantic_ai"}
{"title": "AgentRun `dataclass`", "anchor": "agentrun-dataclass", "md_text": "#### ctx `property`\n\n```\nctx: GraphRunContext[\n    GraphAgentState, GraphAgentDeps[AgentDepsT, Any]\n]\n```\n\nThe current context of the agent run.\n\n#### next\\_node `property`\n\n```\nnext_node: (\n    AgentNode[AgentDepsT, OutputDataT]\n    | End[FinalResult[OutputDataT]]\n)\n```\n\nThe next node that will be run in the agent graph.\n\nThis is the next node that will be used during async iteration, or if a node is not passed to `self.next(...)`.\n\n#### result `property`\n\n```\nresult: AgentRunResult[OutputDataT] | None\n```\n\nThe final result of the run if it has ended, otherwise `None`.\n\nOnce the run returns an [`End`](../pydantic_graph/nodes/index.html#pydantic_graph.nodes.End) node, `result` is populated\nwith an [`AgentRunResult`](../agent/index.html#pydantic_ai.agent.AgentRunResult).\n\n#### \\_\\_aiter\\_\\_\n\n```\n__aiter__() -> (\n    AsyncIterator[\n        AgentNode[AgentDepsT, OutputDataT]\n        | End[FinalResult[OutputDataT]]\n    ]\n)\n```\n\nProvide async-iteration over the nodes in the agent run.\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n|  |  |\n| --- | --- |\n| ``` 138 139 140 141 142 ``` | ``` def __aiter__(     self, ) -> AsyncIterator[_agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]]:     \"\"\"Provide async-iteration over the nodes in the agent run.\"\"\"     return self ``` |\n\n#### \\_\\_anext\\_\\_ `async`\n\n```\n__anext__() -> (\n    AgentNode[AgentDepsT, OutputDataT]\n    | End[FinalResult[OutputDataT]]\n)\n```\n\nAdvance to the next node automatically based on the last returned node.\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n|  |  |\n| --- | --- |\n| ``` 144 145 146 147 148 149 ``` | ``` async def __anext__(     self, ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:     \"\"\"Advance to the next node automatically based on the last returned node.\"\"\"     task = await anext(self._graph_run)     return self._task_to_node(task) ``` |\n\n#### next `async`\n\n```\nnext(\n    node: AgentNode[AgentDepsT, OutputDataT],\n) -> (\n    AgentNode[AgentDepsT, OutputDataT]\n    | End[FinalResult[OutputDataT]]\n)\n```\n\nManually drive the agent run by passing in the node you want to run next.\n\nThis lets you inspect or mutate the node before continuing execution, or skip certain nodes\nunder dynamic conditions. The agent run should be stopped when you return an [`End`](../pydantic_graph/nodes/index.html#pydantic_graph.nodes.End)\nnode.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_graph import End\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    async with agent.iter('What is the capital of France?') as agent_run:\n        next_node = agent_run.next_node  # start with the first node\n        nodes = [next_node]\n        while not isinstance(next_node, End):\n            next_node = await agent_run.next(next_node)\n            nodes.append(next_node)\n        # Once `next_node` is an End, we've finished:\n        print(nodes)\n        '''\n        [\n            UserPromptNode(\n                user_prompt='What is the capital of France?',\n                instructions_functions=[],\n                system_prompts=(),\n                system_prompt_functions=[],\n                system_prompt_dynamic_functions={},\n            ),\n            ModelRequestNode(\n                request=ModelRequest(\n                    parts=[\n                        UserPromptPart(\n                            content='What is the capital of France?',\n                            timestamp=datetime.datetime(...),\n                        )\n                    ]\n                )\n            ),\n            CallToolsNode(\n                model_response=ModelResponse(\n                    parts=[TextPart(content='The capital of France is Paris.')],\n                    usage=RequestUsage(input_tokens=56, output_tokens=7),\n                    model_name='gpt-4o',\n                    timestamp=datetime.datetime(...),\n                )\n            ),\n            End(data=FinalResult(output='The capital of France is Paris.')),\n        ]\n        '''\n        print('Final result:', agent_run.result.output)\n        #> Final result: The capital of France is Paris.\n```\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `node` | `AgentNode[AgentDepsT, OutputDataT]` | The node to run next in the graph. | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]` | The next node returned by the graph logic, or an [`End`](../pydantic_graph/nodes/index.html#pydantic_graph.nodes.End) node if |\n| `AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]` | the run has completed. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`", "url": "https://ai.pydantic.dev/run/index.html#agentrun-dataclass", "page": "run/index.html", "source_site": "pydantic_ai"}
{"title": "AgentRun `dataclass`", "anchor": "agentrun-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 ``` | ``` async def next(     self,     node: _agent_graph.AgentNode[AgentDepsT, OutputDataT], ) -> _agent_graph.AgentNode[AgentDepsT, OutputDataT] | End[FinalResult[OutputDataT]]:     \"\"\"Manually drive the agent run by passing in the node you want to run next.      This lets you inspect or mutate the node before continuing execution, or skip certain nodes     under dynamic conditions. The agent run should be stopped when you return an [`End`][pydantic_graph.nodes.End]     node.      Example:     ```python     from pydantic_ai import Agent     from pydantic_graph import End      agent = Agent('openai:gpt-4o')      async def main():         async with agent.iter('What is the capital of France?') as agent_run:             next_node = agent_run.next_node  # start with the first node             nodes = [next_node]             while not isinstance(next_node, End):                 next_node = await agent_run.next(next_node)                 nodes.append(next_node)             # Once `next_node` is an End, we've finished:             print(nodes)             '''             [                 UserPromptNode(                     user_prompt='What is the capital of France?',                     instructions_functions=[],                     system_prompts=(),                     system_prompt_functions=[],                     system_prompt_dynamic_functions={},                 ),                 ModelRequestNode(                     request=ModelRequest(                         parts=[                             UserPromptPart(                                 content='What is the capital of France?',                                 timestamp=datetime.datetime(...),                             )                         ]                     )                 ),                 CallToolsNode(                     model_response=ModelResponse(                         parts=[TextPart(content='The capital of France is Paris.')],                         usage=RequestUsage(input_tokens=56, output_tokens=7),                         model_name='gpt-4o',                         timestamp=datetime.datetime(...),                     )                 ),                 End(data=FinalResult(output='The capital of France is Paris.')),             ]             '''             print('Final result:', agent_run.result.output)             #> Final result: The capital of France is Paris.     ```      Args:         node: The node to run next in the graph.      Returns:         The next node returned by the graph logic, or an [`End`][pydantic_graph.nodes.End] node if         the run has completed.     \"\"\"     # Note: It might be nice to expose a synchronous interface for iteration, but we shouldn't do it     # on this class, or else IDEs won't warn you if you accidentally use `for` instead of `async for` to iterate.     task = [self._node_to_task(node)]     try:         task = await self._graph_run.next(task)     except StopAsyncIteration:         pass     return self._task_to_node(task) ``` |\n\n#### usage\n\n```\nusage() -> RunUsage\n```\n\nGet usage statistics for the run so far, including token usage, model requests, and so on.\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n|  |  |\n| --- | --- |\n| ``` 248 249 250 ``` | ``` def usage(self) -> _usage.RunUsage:     \"\"\"Get usage statistics for the run so far, including token usage, model requests, and so on.\"\"\"     return self._graph_run.state.usage ``` |", "url": "https://ai.pydantic.dev/run/index.html#agentrun-dataclass", "page": "run/index.html", "source_site": "pydantic_ai"}
{"title": "AgentRunResult `dataclass`", "anchor": "agentrunresult-dataclass", "md_text": "Bases: `Generic[OutputDataT]`\n\nThe final result of an agent run.\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`", "url": "https://ai.pydantic.dev/run/index.html#agentrunresult-dataclass", "page": "run/index.html", "source_site": "pydantic_ai"}
{"title": "AgentRunResult `dataclass`", "anchor": "agentrunresult-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 ``` | ``` @dataclasses.dataclass class AgentRunResult(Generic[OutputDataT]):     \"\"\"The final result of an agent run.\"\"\"      output: OutputDataT     \"\"\"The output data from the agent run.\"\"\"      _output_tool_name: str | None = dataclasses.field(repr=False, compare=False, default=None)     _state: _agent_graph.GraphAgentState = dataclasses.field(         repr=False, compare=False, default_factory=_agent_graph.GraphAgentState     )     _new_message_index: int = dataclasses.field(repr=False, compare=False, default=0)     _traceparent_value: str | None = dataclasses.field(repr=False, compare=False, default=None)      @overload     def _traceparent(self, *, required: Literal[False]) -> str | None: ...     @overload     def _traceparent(self) -> str: ...     def _traceparent(self, *, required: bool = True) -> str | None:         if self._traceparent_value is None and required:  # pragma: no cover             raise AttributeError('No span was created for this agent run')         return self._traceparent_value      def _set_output_tool_return(self, return_content: str) -> list[_messages.ModelMessage]:         \"\"\"Set return content for the output tool.          Useful if you want to continue the conversation and want to set the response to the output tool call.         \"\"\"         if not self._output_tool_name:             raise ValueError('Cannot set output tool return content when the return type is `str`.')          messages = self._state.message_history         last_message = messages[-1]         for idx, part in enumerate(last_message.parts):             if isinstance(part, _messages.ToolReturnPart) and part.tool_name == self._output_tool_name:                 # Only do deepcopy when we have to modify                 copied_messages = list(messages)                 copied_last = deepcopy(last_message)                 copied_last.parts[idx].content = return_content  # type: ignore[misc]                 copied_messages[-1] = copied_last                 return copied_messages          raise LookupError(f'No tool call found with tool name {self._output_tool_name!r}.')      def all_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:         \"\"\"Return the history of _messages.          Args:             output_tool_return_content: The return content of the tool call to set in the last message.                 This provides a convenient way to modify the content of the output tool call if you want to continue                 the conversation and want to set the response to the output tool call. If `None`, the last message will                 not be modified.          Returns:             List of messages.         \"\"\"         if output_tool_return_content is not None:             return self._set_output_tool_return(output_tool_return_content)         else:             return self._state.message_history      def all_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:         \"\"\"Return all messages from [`all_messages`][pydantic_ai.agent.AgentRunResult.all_messages] as JSON bytes.          Args:             output_tool_return_content: The return content of the tool call to set in the last message.                 This provides a convenient way to modify the content of the output tool call if you want to continue                 the conversation and want to set the response to the output tool call. If `None`, the last message will                 not be modified.          Returns:             JSON bytes representing the messages.         \"\"\"         return _messages.ModelMessagesTypeAdapter.dump_json(             self.all_messages(output_tool_return_content=output_tool_return_content)         )      def new_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:         \"\"\"Return new messages associated with this run.          Messages from older runs are excluded.          Args:             output_tool_return_content: The return content of the tool call to set in the last message.                 This provides a convenient way to modify the content of the output tool call if you want to continue                 the conversation and want to set the response to the output tool call. If `None`, the last message will                 not be modified.          Returns:             List of new messages.         \"\"\"         return self.all_messages(output_tool_return_content=output_tool_return_content)[self._new_message_index :]      def new_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:         \"\"\"Return new messages from [`new_messages`][pydantic_ai.agent.AgentRunResult.new_messages] as JSON bytes.          Args:             output_tool_return_content: The return content of the tool call to set in the last message.                 This provides a convenient way to modify the content of the output tool call if you want to continue                 the conversation and want to set the response to the output tool call. If `None`, the last message will                 not be modified.          Returns:             JSON bytes representing the new messages.         \"\"\"         return _messages.ModelMessagesTypeAdapter.dump_json(             self.new_messages(output_tool_return_content=output_tool_return_content)         )      @property     def response(self) -> _messages.ModelResponse:         \"\"\"Return the last response from the message history.\"\"\"         # The response may not be the very last item if it contained an output tool call. See `CallToolsNode._handle_final_result`.         for message in reversed(self.all_messages()):             if isinstance(message, _messages.ModelResponse):                 return message         raise ValueError('No response found in the message history')  # pragma: no cover      # TODO (v2): Make this a property     def usage(self) -> _usage.RunUsage:         \"\"\"Return the usage of the whole run.\"\"\"         return self._state.usage      # TODO (v2): Make this a property     def timestamp(self) -> datetime:         \"\"\"Return the timestamp of last response.\"\"\"         return self.response.timestamp ``` |", "url": "https://ai.pydantic.dev/run/index.html#agentrunresult-dataclass", "page": "run/index.html", "source_site": "pydantic_ai"}
{"title": "AgentRunResult `dataclass`", "anchor": "agentrunresult-dataclass", "md_text": "#### output `instance-attribute`\n\n```\noutput: OutputDataT\n```\n\nThe output data from the agent run.\n\n#### all\\_messages\n\n```\nall_messages(\n    *, output_tool_return_content: str | None = None\n) -> list[ModelMessage]\n```\n\nReturn the history of \\_messages.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `output_tool_return_content` | `str | None` | The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the output tool call if you want to continue the conversation and want to set the response to the output tool call. If `None`, the last message will not be modified. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `list[ModelMessage]` | List of messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n|  |  |\n| --- | --- |\n| ``` 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 ``` | ``` def all_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:     \"\"\"Return the history of _messages.      Args:         output_tool_return_content: The return content of the tool call to set in the last message.             This provides a convenient way to modify the content of the output tool call if you want to continue             the conversation and want to set the response to the output tool call. If `None`, the last message will             not be modified.      Returns:         List of messages.     \"\"\"     if output_tool_return_content is not None:         return self._set_output_tool_return(output_tool_return_content)     else:         return self._state.message_history ``` |\n\n#### all\\_messages\\_json\n\n```\nall_messages_json(\n    *, output_tool_return_content: str | None = None\n) -> bytes\n```\n\nReturn all messages from [`all_messages`](../agent/index.html#pydantic_ai.agent.AgentRunResult.all_messages) as JSON bytes.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `output_tool_return_content` | `str | None` | The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the output tool call if you want to continue the conversation and want to set the response to the output tool call. If `None`, the last message will not be modified. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `bytes` | JSON bytes representing the messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n|  |  |\n| --- | --- |\n| ``` 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 ``` | ``` def all_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:     \"\"\"Return all messages from [`all_messages`][pydantic_ai.agent.AgentRunResult.all_messages] as JSON bytes.      Args:         output_tool_return_content: The return content of the tool call to set in the last message.             This provides a convenient way to modify the content of the output tool call if you want to continue             the conversation and want to set the response to the output tool call. If `None`, the last message will             not be modified.      Returns:         JSON bytes representing the messages.     \"\"\"     return _messages.ModelMessagesTypeAdapter.dump_json(         self.all_messages(output_tool_return_content=output_tool_return_content)     ) ``` |\n\n#### new\\_messages\n\n```\nnew_messages(\n    *, output_tool_return_content: str | None = None\n) -> list[ModelMessage]\n```\n\nReturn new messages associated with this run.\n\nMessages from older runs are excluded.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `output_tool_return_content` | `str | None` | The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the output tool call if you want to continue the conversation and want to set the response to the output tool call. If `None`, the last message will not be modified. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `list[ModelMessage]` | List of new messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n|  |  |\n| --- | --- |\n| ``` 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 ``` | ``` def new_messages(self, *, output_tool_return_content: str | None = None) -> list[_messages.ModelMessage]:     \"\"\"Return new messages associated with this run.      Messages from older runs are excluded.      Args:         output_tool_return_content: The return content of the tool call to set in the last message.             This provides a convenient way to modify the content of the output tool call if you want to continue             the conversation and want to set the response to the output tool call. If `None`, the last message will             not be modified.      Returns:         List of new messages.     \"\"\"     return self.all_messages(output_tool_return_content=output_tool_return_content)[self._new_message_index :] ``` |\n\n#### new\\_messages\\_json", "url": "https://ai.pydantic.dev/run/index.html#agentrunresult-dataclass", "page": "run/index.html", "source_site": "pydantic_ai"}
{"title": "AgentRunResult `dataclass`", "anchor": "agentrunresult-dataclass", "md_text": "```\nnew_messages_json(\n    *, output_tool_return_content: str | None = None\n) -> bytes\n```\n\nReturn new messages from [`new_messages`](../agent/index.html#pydantic_ai.agent.AgentRunResult.new_messages) as JSON bytes.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `output_tool_return_content` | `str | None` | The return content of the tool call to set in the last message. This provides a convenient way to modify the content of the output tool call if you want to continue the conversation and want to set the response to the output tool call. If `None`, the last message will not be modified. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `bytes` | JSON bytes representing the new messages. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n|  |  |\n| --- | --- |\n| ``` 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 ``` | ``` def new_messages_json(self, *, output_tool_return_content: str | None = None) -> bytes:     \"\"\"Return new messages from [`new_messages`][pydantic_ai.agent.AgentRunResult.new_messages] as JSON bytes.      Args:         output_tool_return_content: The return content of the tool call to set in the last message.             This provides a convenient way to modify the content of the output tool call if you want to continue             the conversation and want to set the response to the output tool call. If `None`, the last message will             not be modified.      Returns:         JSON bytes representing the new messages.     \"\"\"     return _messages.ModelMessagesTypeAdapter.dump_json(         self.new_messages(output_tool_return_content=output_tool_return_content)     ) ``` |\n\n#### response `property`\n\n```\nresponse: ModelResponse\n```\n\nReturn the last response from the message history.\n\n#### usage\n\n```\nusage() -> RunUsage\n```\n\nReturn the usage of the whole run.\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n|  |  |\n| --- | --- |\n| ``` 377 378 379 ``` | ``` def usage(self) -> _usage.RunUsage:     \"\"\"Return the usage of the whole run.\"\"\"     return self._state.usage ``` |\n\n#### timestamp\n\n```\ntimestamp() -> datetime\n```\n\nReturn the timestamp of last response.\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n|  |  |\n| --- | --- |\n| ``` 382 383 384 ``` | ``` def timestamp(self) -> datetime:     \"\"\"Return the timestamp of last response.\"\"\"     return self.response.timestamp ``` |", "url": "https://ai.pydantic.dev/run/index.html#agentrunresult-dataclass", "page": "run/index.html", "source_site": "pydantic_ai"}
{"title": "AgentRunResultEvent `dataclass`", "anchor": "agentrunresultevent-dataclass", "md_text": "Bases: `Generic[OutputDataT]`\n\nAn event indicating the agent run ended and containing the final result of the agent run.\n\nSource code in `pydantic_ai_slim/pydantic_ai/run.py`\n\n|  |  |\n| --- | --- |\n| ``` 387 388 389 390 391 392 393 394 395 396 397 398 399 ``` | ``` @dataclasses.dataclass(repr=False) class AgentRunResultEvent(Generic[OutputDataT]):     \"\"\"An event indicating the agent run ended and containing the final result of the agent run.\"\"\"      result: AgentRunResult[OutputDataT]     \"\"\"The result of the run.\"\"\"      _: dataclasses.KW_ONLY      event_kind: Literal['agent_run_result'] = 'agent_run_result'     \"\"\"Event type identifier, used as a discriminator.\"\"\"      __repr__ = _utils.dataclasses_no_defaults_repr ``` |\n\n#### result `instance-attribute`\n\n```\nresult: AgentRunResult[OutputDataT]\n```\n\nThe result of the run.\n\n#### event\\_kind `class-attribute` `instance-attribute`\n\n```\nevent_kind: Literal[\"agent_run_result\"] = \"agent_run_result\"\n```\n\nEvent type identifier, used as a discriminator.", "url": "https://ai.pydantic.dev/run/index.html#agentrunresultevent-dataclass", "page": "run/index.html", "source_site": "pydantic_ai"}
{"title": "Durable Execution", "anchor": "durable-execution", "md_text": "Prefect 3.0 brings [transactional semantics](https://www.prefect.io/blog/transactional-ml-pipelines-with-prefect-3-0) to your Python workflows, allowing you to group tasks into atomic units and define failure modes. If any part of a transaction fails, the entire transaction can be rolled back to a clean state.\n\n* **Flows** are the top-level entry points for your workflow. They can contain tasks and other flows.\n* **Tasks** are individual units of work that can be retried, cached, and monitored independently.\n\nPrefect 3.0's approach to transactional orchestration makes your workflows automatically **idempotent**: rerunnable without duplication or inconsistency across any environment. Every task is executed within a transaction that governs when and where the task's result record is persisted. If the task runs again under an identical context, it will not re-execute but instead load its previous result.\n\nThe diagram below shows the overall architecture of an agentic application with Prefect.\nPrefect uses client-side task orchestration by default, with optional server connectivity for advanced features like scheduling and monitoring.\n\n```\n            +---------------------+\n            |   Prefect Server    |      (Monitoring,\n            |      or Cloud       |       scheduling, UI,\n            +---------------------+       orchestration)\n                     ^\n                     |\n        Flow state,  |   Schedule flows,\n        metadata,    |   track execution\n        logs         |\n                     |\n+------------------------------------------------------+\n|               Application Process                    |\n|   +----------------------------------------------+   |\n|   |              Flow (Agent.run)                |   |\n|   +----------------------------------------------+   |\n|          |          |                |               |\n|          v          v                v               |\n|   +-----------+ +------------+ +-------------+       |\n|   |   Task    | |    Task    | |    Task     |       |\n|   |  (Tool)   | | (MCP Tool) | | (Model API) |       |\n|   +-----------+ +------------+ +-------------+       |\n|         |           |                |               |\n|       Cache &     Cache &          Cache &           |\n|       persist     persist          persist           |\n|         to           to               to             |\n|         v            v                v              |\n|   +----------------------------------------------+   |\n|   |     Result Storage (Local FS, S3, etc.)     |    |\n|   +----------------------------------------------+   |\n+------------------------------------------------------+\n          |           |                |\n          v           v                v\n      [External APIs, services, databases, etc.]\n```\n\nSee the [Prefect documentation](https://docs.prefect.io/) for more information.", "url": "https://ai.pydantic.dev/prefect/index.html#durable-execution", "page": "prefect/index.html", "source_site": "pydantic_ai"}
{"title": "Durable Agent", "anchor": "durable-agent", "md_text": "Any agent can be wrapped in a [`PrefectAgent`](../durable_exec/index.html#pydantic_ai.durable_exec.prefect.PrefectAgent) to get durable execution. `PrefectAgent` automatically:\n\n* Wraps [`Agent.run`](../agent/index.html#pydantic_ai.agent.AbstractAgent.run) and [`Agent.run_sync`](../agent/index.html#pydantic_ai.agent.AbstractAgent.run_sync) as Prefect flows.\n* Wraps [model requests](https://ai.pydantic.dev/models/overview/) as Prefect tasks.\n* Wraps [tool calls](https://ai.pydantic.dev/tools/) as Prefect tasks (configurable per-tool).\n* Wraps [MCP communication](../client/index.html) as Prefect tasks.\n\nEvent stream handlers are **automatically wrapped** by Prefect when running inside a Prefect flow. Each event from the stream is processed in a separate Prefect task for durability. You can customize the task behavior using the `event_stream_handler_task_config` parameter when creating the `PrefectAgent`. Do **not** manually decorate event stream handlers with `@task`. For examples, see the [streaming docs](https://ai.pydantic.dev/agents/#streaming-all-events)\n\nThe original agent, model, and MCP server can still be used as normal outside the Prefect flow.\n\nHere is a simple but complete example of wrapping an agent for durable execution. All it requires is to install Pydantic AI with Prefect:\n\npipuv\n\n```\npip install pydantic-ai[prefect]\n\nuv add pydantic-ai[prefect]\n```\n\nOr if you're using the slim package, you can install it with the `prefect` optional group:\n\npipuv\n\n```\npip install pydantic-ai-slim[prefect]\n\nuv add pydantic-ai-slim[prefect]\n```\n\nprefect\\_agent.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.durable_exec.prefect import PrefectAgent\n\nagent = Agent(\n    'gpt-4o',\n    instructions=\"You're an expert in geography.\",\n    name='geography',  # (1)!\n)\n\nprefect_agent = PrefectAgent(agent)  # (2)!\n\nasync def main():\n    result = await prefect_agent.run('What is the capital of Mexico?')  # (3)!\n    print(result.output)\n    #> Mexico City (Ciudad de México, CDMX)\n```\n\n1. The agent's `name` is used to uniquely identify its flows and tasks.\n2. Wrapping the agent with `PrefectAgent` enables durable execution for all agent runs.\n3. [`PrefectAgent.run()`](../durable_exec/index.html#pydantic_ai.durable_exec.prefect.PrefectAgent.run) works like [`Agent.run()`](../agent/index.html#pydantic_ai.agent.AbstractAgent.run), but runs as a Prefect flow and executes model requests, decorated tool calls, and MCP communication as Prefect tasks.\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*\n\nFor more information on how to use Prefect in Python applications, see their [Python documentation](https://docs.prefect.io/v3/how-to-guides/workflows/write-and-run).", "url": "https://ai.pydantic.dev/prefect/index.html#durable-agent", "page": "prefect/index.html", "source_site": "pydantic_ai"}
{"title": "Prefect Integration Considerations", "anchor": "prefect-integration-considerations", "md_text": "When using Prefect with Pydantic AI agents, there are a few important considerations to ensure workflows behave correctly.", "url": "https://ai.pydantic.dev/prefect/index.html#prefect-integration-considerations", "page": "prefect/index.html", "source_site": "pydantic_ai"}
{"title": "Agent Requirements", "anchor": "agent-requirements", "md_text": "Each agent instance must have a unique `name` so Prefect can correctly identify and track its flows and tasks.", "url": "https://ai.pydantic.dev/prefect/index.html#agent-requirements", "page": "prefect/index.html", "source_site": "pydantic_ai"}
{"title": "Tool Wrapping", "anchor": "tool-wrapping", "md_text": "Agent tools are automatically wrapped as Prefect tasks, which means they benefit from:\n\n* **Retry logic**: Failed tool calls can be retried automatically\n* **Caching**: Tool results are cached based on their inputs\n* **Observability**: Tool execution is tracked in the Prefect UI\n\nYou can customize tool task behavior using `tool_task_config` (applies to all tools) or `tool_task_config_by_name` (per-tool configuration):\n\nprefect\\_agent\\_config.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.durable_exec.prefect import PrefectAgent, TaskConfig\n\nagent = Agent('gpt-4o', name='my_agent')\n\n@agent.tool_plain\ndef fetch_data(url: str) -> str:\n    # This tool will be wrapped as a Prefect task\n    ...\n\nprefect_agent = PrefectAgent(\n    agent,\n    tool_task_config=TaskConfig(retries=3),  # Default for all tools\n    tool_task_config_by_name={\n        'fetch_data': TaskConfig(timeout_seconds=10.0),  # Specific to fetch_data\n        'simple_tool': None,  # Disable task wrapping for simple_tool\n    },\n)\n```\n\nSet a tool's config to `None` in `tool_task_config_by_name` to disable task wrapping for that specific tool.", "url": "https://ai.pydantic.dev/prefect/index.html#tool-wrapping", "page": "prefect/index.html", "source_site": "pydantic_ai"}
{"title": "Streaming", "anchor": "streaming", "md_text": "When running inside a Prefect flow, [`Agent.run_stream()`](../agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream) works but doesn't provide real-time streaming because Prefect tasks consume their entire execution before returning results. The method will execute fully and return the complete result at once.\n\nFor real-time streaming behavior inside Prefect flows, you can set an [`event_stream_handler`](../agent/index.html#pydantic_ai.agent.EventStreamHandler) on the `Agent` or `PrefectAgent` instance and use [`PrefectAgent.run()`](../durable_exec/index.html#pydantic_ai.durable_exec.prefect.PrefectAgent.run).\n\n**Note**: Event stream handlers behave differently when running inside a Prefect flow versus outside:\n- **Outside a flow**: The handler receives events as they stream from the model\n- **Inside a flow**: Each event is wrapped as a Prefect task for durability, which may affect timing but ensures reliability\n\nThe event stream handler function will receive the agent [run context](../tools/index.html#pydantic_ai.tools.RunContext) and an async iterable of events from the model's streaming response and the agent's execution of tools. For examples, see the [streaming docs](https://ai.pydantic.dev/agents/#streaming-all-events).", "url": "https://ai.pydantic.dev/prefect/index.html#streaming", "page": "prefect/index.html", "source_site": "pydantic_ai"}
{"title": "Task Configuration", "anchor": "task-configuration", "md_text": "You can customize Prefect task behavior, such as retries and timeouts, by passing [`TaskConfig`](../durable_exec/index.html#pydantic_ai.durable_exec.prefect.TaskConfig) objects to the `PrefectAgent` constructor:\n\n* `mcp_task_config`: Configuration for MCP server communication tasks\n* `model_task_config`: Configuration for model request tasks\n* `tool_task_config`: Default configuration for all tool calls\n* `tool_task_config_by_name`: Per-tool task configuration (overrides `tool_task_config`)\n* `event_stream_handler_task_config`: Configuration for event stream handler tasks (applies when running inside a Prefect flow)\n\nAvailable `TaskConfig` options:\n\n* `retries`: Maximum number of retries for the task (default: `0`)\n* `retry_delay_seconds`: Delay between retries in seconds (can be a single value or list for exponential backoff, default: `1.0`)\n* `timeout_seconds`: Maximum time in seconds for the task to complete\n* `cache_policy`: Custom Prefect cache policy for the task\n* `persist_result`: Whether to persist the task result\n* `result_storage`: Prefect result storage for the task (e.g., `'s3-bucket/my-storage'` or a `WritableFileSystem` block)\n* `log_prints`: Whether to log print statements from the task (default: `False`)\n\nExample:\n\nprefect\\_agent\\_config.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.durable_exec.prefect import PrefectAgent, TaskConfig\n\nagent = Agent(\n    'gpt-4o',\n    instructions=\"You're an expert in geography.\",\n    name='geography',\n)\n\nprefect_agent = PrefectAgent(\n    agent,\n    model_task_config=TaskConfig(\n        retries=3,\n        retry_delay_seconds=[1.0, 2.0, 4.0],  # Exponential backoff\n        timeout_seconds=30.0,\n    ),\n)\n\nasync def main():\n    result = await prefect_agent.run('What is the capital of France?')\n    print(result.output)\n    #> Paris\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/prefect/index.html#task-configuration", "page": "prefect/index.html", "source_site": "pydantic_ai"}
{"title": "Retry Considerations", "anchor": "retry-considerations", "md_text": "Pydantic AI and provider API clients have their own retry logic. When using Prefect, you may want to:\n\n* Disable [HTTP Request Retries](https://ai.pydantic.dev/retries/) in Pydantic AI\n* Turn off your provider API client's retry logic (e.g., `max_retries=0` on a [custom OpenAI client](../openai/index.html#custom-openai-client))\n* Rely on Prefect's task-level retry configuration for consistency\n\nThis prevents requests from being retried multiple times at different layers.", "url": "https://ai.pydantic.dev/prefect/index.html#retry-considerations", "page": "prefect/index.html", "source_site": "pydantic_ai"}
{"title": "Caching and Idempotency", "anchor": "caching-and-idempotency", "md_text": "Prefect 3.0 provides built-in caching and transactional semantics. Tasks with identical inputs will not re-execute if their results are already cached, making workflows naturally idempotent and resilient to failures.\n\n* **Task inputs**: Messages, settings, parameters, tool arguments, and serializable dependencies\n\n**Note**: For user dependencies to be included in cache keys, they must be serializable (e.g., Pydantic models or basic Python types). Non-serializable dependencies are automatically excluded from cache computation.", "url": "https://ai.pydantic.dev/prefect/index.html#caching-and-idempotency", "page": "prefect/index.html", "source_site": "pydantic_ai"}
{"title": "Observability with Prefect and Logfire", "anchor": "observability-with-prefect-and-logfire", "md_text": "Prefect provides a built-in UI for monitoring flow runs, task executions, and failures. You can:\n\n* View real-time flow run status\n* Debug failures with full stack traces\n* Set up alerts and notifications\n\nTo access the Prefect UI, you can either:\n\n1. Use [Prefect Cloud](https://www.prefect.io/cloud) (managed service)\n2. Run a local [Prefect server](https://docs.prefect.io/v3/how-to-guides/self-hosted/server-cli) with `prefect server start`\n\nYou can also use [Pydantic Logfire](https://ai.pydantic.dev/logfire/) for detailed observability. When using both Prefect and Logfire, you'll get complementary views:\n\n* **Prefect**: Workflow-level orchestration, task status, and retry history\n* **Logfire**: Fine-grained tracing of agent runs, model requests, and tool invocations\n\nWhen using Logfire with Prefect, you can enable distributed tracing to see spans for your Prefect runs included with your agent runs, model requests, and tool invocations.\n\nFor more information about Prefect monitoring, see the [Prefect documentation](https://docs.prefect.io/).", "url": "https://ai.pydantic.dev/prefect/index.html#observability-with-prefect-and-logfire", "page": "prefect/index.html", "source_site": "pydantic_ai"}
{"title": "Deployments and Scheduling", "anchor": "deployments-and-scheduling", "md_text": "To deploy and schedule a `PrefectAgent`, wrap it in a Prefect flow and use the flow's [`serve()`](https://docs.prefect.io/v3/how-to-guides/deployments/create-deployments#create-a-deployment-with-serve) or [`deploy()`](https://docs.prefect.io/v3/how-to-guides/deployments/deploy-via-python) methods:\n\nserve\\_agent.py\n\n```\nfrom prefect import flow\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.durable_exec.prefect import PrefectAgent\n\n\n@flow\nasync def daily_report_flow(user_prompt: str):\n    \"\"\"Generate a daily report using the agent.\"\"\"\n    agent = Agent(  # (1)!\n        'openai:gpt-4o',\n        name='daily_report_agent',\n        instructions='Generate a daily summary report.',\n    )\n\n    prefect_agent = PrefectAgent(agent)\n\n    result = await prefect_agent.run(user_prompt)\n    return result.output\n\n\n\n# Serve the flow with a daily schedule\nif __name__ == '__main__':\n    daily_report_flow.serve(\n        name='daily-report-deployment',\n        cron='0 9 * * *',  # Run daily at 9am\n        parameters={'user_prompt': \"Generate today's report\"},\n        tags=['production', 'reports'],\n    )\n```\n\n1. Each flow run executes in an isolated process, and all inputs and dependencies must be serializable. Because Agent instances cannot be serialized, instantiate the agent inside the flow rather than at the module level.\n\nThe `serve()` method accepts scheduling options:\n\n* **`cron`**: Cron schedule string (e.g., `'0 9 * * *'` for daily at 9am)\n* **`interval`**: Schedule interval in seconds or as a timedelta\n* **`rrule`**: iCalendar RRule schedule string\n\nFor production deployments with Docker, Kubernetes, or other infrastructure, use the flow's [`deploy()`](https://docs.prefect.io/v3/how-to-guides/deployments/deploy-via-python) method. See the [Prefect deployment documentation](https://docs.prefect.io/v3/how-to-guides/deployments/create-deploymentsy) for more information.", "url": "https://ai.pydantic.dev/prefect/index.html#deployments-and-scheduling", "page": "prefect/index.html", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "md_text": "To use `AnthropicModel` models, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `anthropic` optional group:\n\npipuv\n\n```\npip install \"pydantic-ai-slim[anthropic]\"\n\nuv add \"pydantic-ai-slim[anthropic]\"\n```", "url": "https://ai.pydantic.dev/anthropic/index.html#install", "page": "anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "Configuration", "anchor": "configuration", "md_text": "To use [Anthropic](https://anthropic.com) through their API, go to [console.anthropic.com/settings/keys](https://console.anthropic.com/settings/keys) to generate an API key.\n\n`AnthropicModelName` contains a list of available Anthropic models.", "url": "https://ai.pydantic.dev/anthropic/index.html#configuration", "page": "anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "Environment variable", "anchor": "environment-variable", "md_text": "Once you have the API key, you can set it as an environment variable:\n\n```\nexport ANTHROPIC_API_KEY='your-api-key'\n```\n\nYou can then use `AnthropicModel` by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('anthropic:claude-3-5-sonnet-latest')\n...\n```\n\nOr initialise the model directly with just the model name:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel\n\nmodel = AnthropicModel('claude-3-5-sonnet-latest')\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/anthropic/index.html#environment-variable", "page": "anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "`provider` argument", "anchor": "provider-argument", "md_text": "You can provide a custom `Provider` via the `provider` argument:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.providers.anthropic import AnthropicProvider\n\nmodel = AnthropicModel(\n    'claude-3-5-sonnet-latest', provider=AnthropicProvider(api_key='your-api-key')\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/anthropic/index.html#provider-argument", "page": "anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "Custom HTTP Client", "anchor": "custom-http-client", "md_text": "You can customize the `AnthropicProvider` with a custom `httpx.AsyncClient`:\n\n```\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.anthropic import AnthropicModel\nfrom pydantic_ai.providers.anthropic import AnthropicProvider\n\ncustom_http_client = AsyncClient(timeout=30)\nmodel = AnthropicModel(\n    'claude-3-5-sonnet-latest',\n    provider=AnthropicProvider(api_key='your-api-key', http_client=custom_http_client),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/anthropic/index.html#custom-http-client", "page": "anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "FinishReason `module-attribute`", "anchor": "finishreason-module-attribute", "md_text": "```\nFinishReason: TypeAlias = Literal[\n    \"stop\", \"length\", \"content_filter\", \"tool_call\", \"error\"\n]\n```\n\nReason the model finished generating the response, normalized to OpenTelemetry values.", "url": "https://ai.pydantic.dev/messages/index.html#finishreason-module-attribute", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "SystemPromptPart `dataclass`", "anchor": "systempromptpart-dataclass", "md_text": "A system prompt, generally written by the application developer.\n\nThis gives the model context and guidance on how to respond.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 ``` | ``` @dataclass(repr=False) class SystemPromptPart:     \"\"\"A system prompt, generally written by the application developer.      This gives the model context and guidance on how to respond.     \"\"\"      content: str     \"\"\"The content of the prompt.\"\"\"      _: KW_ONLY      timestamp: datetime = field(default_factory=_now_utc)     \"\"\"The timestamp of the prompt.\"\"\"      dynamic_ref: str | None = None     \"\"\"The ref of the dynamic system prompt function that generated this part.      Only set if system prompt is dynamic, see [`system_prompt`][pydantic_ai.Agent.system_prompt] for more information.     \"\"\"      part_kind: Literal['system-prompt'] = 'system-prompt'     \"\"\"Part type identifier, this is available on all parts as a discriminator.\"\"\"      def otel_event(self, settings: InstrumentationSettings) -> Event:         return Event(             'gen_ai.system.message',             body={'role': 'system', **({'content': self.content} if settings.include_content else {})},         )      def otel_message_parts(self, settings: InstrumentationSettings) -> list[_otel_messages.MessagePart]:         return [_otel_messages.TextPart(type='text', **{'content': self.content} if settings.include_content else {})]      __repr__ = _utils.dataclasses_no_defaults_repr ``` |\n\n#### content `instance-attribute`\n\n```\ncontent: str\n```\n\nThe content of the prompt.\n\n#### timestamp `class-attribute` `instance-attribute`\n\n```\ntimestamp: datetime = field(default_factory=now_utc)\n```\n\nThe timestamp of the prompt.\n\n#### dynamic\\_ref `class-attribute` `instance-attribute`\n\n```\ndynamic_ref: str | None = None\n```\n\nThe ref of the dynamic system prompt function that generated this part.\n\nOnly set if system prompt is dynamic, see [`system_prompt`](../agent/index.html#pydantic_ai.agent.Agent.system_prompt) for more information.\n\n#### part\\_kind `class-attribute` `instance-attribute`\n\n```\npart_kind: Literal['system-prompt'] = 'system-prompt'\n```\n\nPart type identifier, this is available on all parts as a discriminator.", "url": "https://ai.pydantic.dev/messages/index.html#systempromptpart-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "FileUrl `dataclass`", "anchor": "fileurl-dataclass", "md_text": "Bases: `ABC`\n\nAbstract base class for any URL-based file.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 ``` | ``` @dataclass(init=False, repr=False) class FileUrl(ABC):     \"\"\"Abstract base class for any URL-based file.\"\"\"      url: str     \"\"\"The URL of the file.\"\"\"      _: KW_ONLY      force_download: bool = False     \"\"\"For OpenAI and Google APIs it:      * If True, the file is downloaded and the data is sent to the model as bytes.     * If False, the URL is sent directly to the model and no download is performed.     \"\"\"      vendor_metadata: dict[str, Any] | None = None     \"\"\"Vendor-specific metadata for the file.      Supported by:     - `GoogleModel`: `VideoUrl.vendor_metadata` is used as `video_metadata`: https://ai.google.dev/gemini-api/docs/video-understanding#customize-video-processing     - `OpenAIChatModel`, `OpenAIResponsesModel`: `ImageUrl.vendor_metadata['detail']` is used as `detail` setting for images     \"\"\"      _media_type: Annotated[str | None, pydantic.Field(alias='media_type', default=None, exclude=True)] = field(         compare=False, default=None     )      _identifier: Annotated[str | None, pydantic.Field(alias='identifier', default=None, exclude=True)] = field(         compare=False, default=None     )      def __init__(         self,         url: str,         *,         media_type: str | None = None,         identifier: str | None = None,         force_download: bool = False,         vendor_metadata: dict[str, Any] | None = None,     ) -> None:         self.url = url         self._media_type = media_type         self._identifier = identifier         self.force_download = force_download         self.vendor_metadata = vendor_metadata      @pydantic.computed_field     @property     def media_type(self) -> str:         \"\"\"Return the media type of the file, based on the URL or the provided `media_type`.\"\"\"         return self._media_type or self._infer_media_type()      @pydantic.computed_field     @property     def identifier(self) -> str:         \"\"\"The identifier of the file, such as a unique ID.          This identifier can be provided to the model in a message to allow it to refer to this file in a tool call argument,         and the tool can look up the file in question by iterating over the message history and finding the matching `FileUrl`.          This identifier is only automatically passed to the model when the `FileUrl` is returned by a tool.         If you're passing the `FileUrl` as a user message, it's up to you to include a separate text part with the identifier,         e.g. \"This is file <identifier>:\" preceding the `FileUrl`.          It's also included in inline-text delimiters for providers that require inlining text documents, so the model can         distinguish multiple files.         \"\"\"         return self._identifier or _multi_modal_content_identifier(self.url)      @abstractmethod     def _infer_media_type(self) -> str:         \"\"\"Infer the media type of the file based on the URL.\"\"\"         raise NotImplementedError      @property     @abstractmethod     def format(self) -> str:         \"\"\"The file format.\"\"\"         raise NotImplementedError      __repr__ = _utils.dataclasses_no_defaults_repr ``` |\n\n#### url `instance-attribute`\n\n```\nurl: str = url\n```\n\nThe URL of the file.\n\n#### force\\_download `class-attribute` `instance-attribute`\n\n```\nforce_download: bool = force_download\n```\n\nFor OpenAI and Google APIs it:\n\n* If True, the file is downloaded and the data is sent to the model as bytes.\n* If False, the URL is sent directly to the model and no download is performed.\n\n#### vendor\\_metadata `class-attribute` `instance-attribute`\n\n```\nvendor_metadata: dict[str, Any] | None = vendor_metadata\n```\n\nVendor-specific metadata for the file.\n\nSupported by:\n- `GoogleModel`: `VideoUrl.vendor_metadata` is used as `video_metadata`: https://ai.google.dev/gemini-api/docs/video-understanding#customize-video-processing\n- `OpenAIChatModel`, `OpenAIResponsesModel`: `ImageUrl.vendor_metadata['detail']` is used as `detail` setting for images\n\n#### media\\_type `property`\n\n```\nmedia_type: str\n```\n\nReturn the media type of the file, based on the URL or the provided `media_type`.\n\n#### identifier `property`\n\n```\nidentifier: str\n```\n\nThe identifier of the file, such as a unique ID.\n\nThis identifier can be provided to the model in a message to allow it to refer to this file in a tool call argument,\nand the tool can look up the file in question by iterating over the message history and finding the matching `FileUrl`.", "url": "https://ai.pydantic.dev/messages/index.html#fileurl-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "FileUrl `dataclass`", "anchor": "fileurl-dataclass", "md_text": "This identifier is only automatically passed to the model when the `FileUrl` is returned by a tool.\nIf you're passing the `FileUrl` as a user message, it's up to you to include a separate text part with the identifier,\ne.g. \"This is file :\" preceding the `FileUrl`.\n\nIt's also included in inline-text delimiters for providers that require inlining text documents, so the model can\ndistinguish multiple files.\n\n#### format `abstractmethod` `property`\n\n```\nformat: str\n```\n\nThe file format.", "url": "https://ai.pydantic.dev/messages/index.html#fileurl-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "VideoUrl `dataclass`", "anchor": "videourl-dataclass", "md_text": "Bases: `FileUrl`\n\nA URL to a video.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 ``` | ``` @dataclass(init=False, repr=False) class VideoUrl(FileUrl):     \"\"\"A URL to a video.\"\"\"      url: str     \"\"\"The URL of the video.\"\"\"      _: KW_ONLY      kind: Literal['video-url'] = 'video-url'     \"\"\"Type identifier, this is available on all parts as a discriminator.\"\"\"      def __init__(         self,         url: str,         *,         media_type: str | None = None,         identifier: str | None = None,         force_download: bool = False,         vendor_metadata: dict[str, Any] | None = None,         kind: Literal['video-url'] = 'video-url',         # Required for inline-snapshot which expects all dataclass `__init__` methods to take all field names as kwargs.         _media_type: str | None = None,         _identifier: str | None = None,     ) -> None:         super().__init__(             url=url,             force_download=force_download,             vendor_metadata=vendor_metadata,             media_type=media_type or _media_type,             identifier=identifier or _identifier,         )         self.kind = kind      def _infer_media_type(self) -> VideoMediaType:         \"\"\"Return the media type of the video, based on the url.\"\"\"         if self.url.endswith('.mkv'):             return 'video/x-matroska'         elif self.url.endswith('.mov'):             return 'video/quicktime'         elif self.url.endswith('.mp4'):             return 'video/mp4'         elif self.url.endswith('.webm'):             return 'video/webm'         elif self.url.endswith('.flv'):             return 'video/x-flv'         elif self.url.endswith(('.mpeg', '.mpg')):             return 'video/mpeg'         elif self.url.endswith('.wmv'):             return 'video/x-ms-wmv'         elif self.url.endswith('.three_gp'):             return 'video/3gpp'         # Assume that YouTube videos are mp4 because there would be no extension         # to infer from. This should not be a problem, as Gemini disregards media         # type for YouTube URLs.         elif self.is_youtube:             return 'video/mp4'         else:             raise ValueError(                 f'Could not infer media type from video URL: {self.url}. Explicitly provide a `media_type` instead.'             )      @property     def is_youtube(self) -> bool:         \"\"\"True if the URL has a YouTube domain.\"\"\"         return self.url.startswith(('https://youtu.be/', 'https://youtube.com/', 'https://www.youtube.com/'))      @property     def format(self) -> VideoFormat:         \"\"\"The file format of the video.          The choice of supported formats were based on the Bedrock Converse API. Other APIs don't require to use a format.         \"\"\"         return _video_format_lookup[self.media_type] ``` |\n\n#### url `instance-attribute`\n\n```\nurl: str\n```\n\nThe URL of the video.\n\n#### kind `class-attribute` `instance-attribute`\n\n```\nkind: Literal['video-url'] = kind\n```\n\nType identifier, this is available on all parts as a discriminator.\n\n#### is\\_youtube `property`\n\n```\nis_youtube: bool\n```\n\nTrue if the URL has a YouTube domain.\n\n#### format `property`\n\n```\nformat: VideoFormat\n```\n\nThe file format of the video.\n\nThe choice of supported formats were based on the Bedrock Converse API. Other APIs don't require to use a format.", "url": "https://ai.pydantic.dev/messages/index.html#videourl-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "AudioUrl `dataclass`", "anchor": "audiourl-dataclass", "md_text": "Bases: `FileUrl`\n\nA URL to an audio file.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 ``` | ``` @dataclass(init=False, repr=False) class AudioUrl(FileUrl):     \"\"\"A URL to an audio file.\"\"\"      url: str     \"\"\"The URL of the audio file.\"\"\"      _: KW_ONLY      kind: Literal['audio-url'] = 'audio-url'     \"\"\"Type identifier, this is available on all parts as a discriminator.\"\"\"      def __init__(         self,         url: str,         *,         media_type: str | None = None,         identifier: str | None = None,         force_download: bool = False,         vendor_metadata: dict[str, Any] | None = None,         kind: Literal['audio-url'] = 'audio-url',         # Required for inline-snapshot which expects all dataclass `__init__` methods to take all field names as kwargs.         _media_type: str | None = None,         _identifier: str | None = None,     ) -> None:         super().__init__(             url=url,             force_download=force_download,             vendor_metadata=vendor_metadata,             media_type=media_type or _media_type,             identifier=identifier or _identifier,         )         self.kind = kind      def _infer_media_type(self) -> AudioMediaType:         \"\"\"Return the media type of the audio file, based on the url.          References:         - Gemini: https://ai.google.dev/gemini-api/docs/audio#supported-formats         \"\"\"         if self.url.endswith('.mp3'):             return 'audio/mpeg'         if self.url.endswith('.wav'):             return 'audio/wav'         if self.url.endswith('.flac'):             return 'audio/flac'         if self.url.endswith('.oga'):             return 'audio/ogg'         if self.url.endswith('.aiff'):             return 'audio/aiff'         if self.url.endswith('.aac'):             return 'audio/aac'          raise ValueError(             f'Could not infer media type from audio URL: {self.url}. Explicitly provide a `media_type` instead.'         )      @property     def format(self) -> AudioFormat:         \"\"\"The file format of the audio file.\"\"\"         return _audio_format_lookup[self.media_type] ``` |\n\n#### url `instance-attribute`\n\n```\nurl: str\n```\n\nThe URL of the audio file.\n\n#### kind `class-attribute` `instance-attribute`\n\n```\nkind: Literal['audio-url'] = kind\n```\n\nType identifier, this is available on all parts as a discriminator.\n\n#### format `property`\n\n```\nformat: AudioFormat\n```\n\nThe file format of the audio file.", "url": "https://ai.pydantic.dev/messages/index.html#audiourl-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "ImageUrl `dataclass`", "anchor": "imageurl-dataclass", "md_text": "Bases: `FileUrl`\n\nA URL to an image.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 ``` | ``` @dataclass(init=False, repr=False) class ImageUrl(FileUrl):     \"\"\"A URL to an image.\"\"\"      url: str     \"\"\"The URL of the image.\"\"\"      _: KW_ONLY      kind: Literal['image-url'] = 'image-url'     \"\"\"Type identifier, this is available on all parts as a discriminator.\"\"\"      def __init__(         self,         url: str,         *,         media_type: str | None = None,         identifier: str | None = None,         force_download: bool = False,         vendor_metadata: dict[str, Any] | None = None,         kind: Literal['image-url'] = 'image-url',         # Required for inline-snapshot which expects all dataclass `__init__` methods to take all field names as kwargs.         _media_type: str | None = None,         _identifier: str | None = None,     ) -> None:         super().__init__(             url=url,             force_download=force_download,             vendor_metadata=vendor_metadata,             media_type=media_type or _media_type,             identifier=identifier or _identifier,         )         self.kind = kind      def _infer_media_type(self) -> ImageMediaType:         \"\"\"Return the media type of the image, based on the url.\"\"\"         if self.url.endswith(('.jpg', '.jpeg')):             return 'image/jpeg'         elif self.url.endswith('.png'):             return 'image/png'         elif self.url.endswith('.gif'):             return 'image/gif'         elif self.url.endswith('.webp'):             return 'image/webp'         else:             raise ValueError(                 f'Could not infer media type from image URL: {self.url}. Explicitly provide a `media_type` instead.'             )      @property     def format(self) -> ImageFormat:         \"\"\"The file format of the image.          The choice of supported formats were based on the Bedrock Converse API. Other APIs don't require to use a format.         \"\"\"         return _image_format_lookup[self.media_type] ``` |\n\n#### url `instance-attribute`\n\n```\nurl: str\n```\n\nThe URL of the image.\n\n#### kind `class-attribute` `instance-attribute`\n\n```\nkind: Literal['image-url'] = kind\n```\n\nType identifier, this is available on all parts as a discriminator.\n\n#### format `property`\n\n```\nformat: ImageFormat\n```\n\nThe file format of the image.\n\nThe choice of supported formats were based on the Bedrock Converse API. Other APIs don't require to use a format.", "url": "https://ai.pydantic.dev/messages/index.html#imageurl-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "DocumentUrl `dataclass`", "anchor": "documenturl-dataclass", "md_text": "Bases: `FileUrl`\n\nThe URL of the document.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 ``` | ``` @dataclass(init=False, repr=False) class DocumentUrl(FileUrl):     \"\"\"The URL of the document.\"\"\"      url: str     \"\"\"The URL of the document.\"\"\"      _: KW_ONLY      kind: Literal['document-url'] = 'document-url'     \"\"\"Type identifier, this is available on all parts as a discriminator.\"\"\"      def __init__(         self,         url: str,         *,         media_type: str | None = None,         identifier: str | None = None,         force_download: bool = False,         vendor_metadata: dict[str, Any] | None = None,         kind: Literal['document-url'] = 'document-url',         # Required for inline-snapshot which expects all dataclass `__init__` methods to take all field names as kwargs.         _media_type: str | None = None,         _identifier: str | None = None,     ) -> None:         super().__init__(             url=url,             force_download=force_download,             vendor_metadata=vendor_metadata,             media_type=media_type or _media_type,             identifier=identifier or _identifier,         )         self.kind = kind      def _infer_media_type(self) -> str:         \"\"\"Return the media type of the document, based on the url.\"\"\"         # Common document types are hardcoded here as mime-type support for these         # extensions varies across operating systems.         if self.url.endswith(('.md', '.mdx', '.markdown')):             return 'text/markdown'         elif self.url.endswith('.asciidoc'):             return 'text/x-asciidoc'         elif self.url.endswith('.txt'):             return 'text/plain'         elif self.url.endswith('.pdf'):             return 'application/pdf'         elif self.url.endswith('.rtf'):             return 'application/rtf'         elif self.url.endswith('.docx'):             return 'application/vnd.openxmlformats-officedocument.wordprocessingml.document'         elif self.url.endswith('.xlsx'):             return 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'          type_, _ = guess_type(self.url)         if type_ is None:             raise ValueError(                 f'Could not infer media type from document URL: {self.url}. Explicitly provide a `media_type` instead.'             )         return type_      @property     def format(self) -> DocumentFormat:         \"\"\"The file format of the document.          The choice of supported formats were based on the Bedrock Converse API. Other APIs don't require to use a format.         \"\"\"         media_type = self.media_type         try:             return _document_format_lookup[media_type]         except KeyError as e:             raise ValueError(f'Unknown document media type: {media_type}') from e ``` |\n\n#### url `instance-attribute`\n\n```\nurl: str\n```\n\nThe URL of the document.\n\n#### kind `class-attribute` `instance-attribute`\n\n```\nkind: Literal['document-url'] = kind\n```\n\nType identifier, this is available on all parts as a discriminator.\n\n#### format `property`\n\n```\nformat: DocumentFormat\n```\n\nThe file format of the document.\n\nThe choice of supported formats were based on the Bedrock Converse API. Other APIs don't require to use a format.", "url": "https://ai.pydantic.dev/messages/index.html#documenturl-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "BinaryContent `dataclass`", "anchor": "binarycontent-dataclass", "md_text": "Binary content, e.g. an audio or image file.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`", "url": "https://ai.pydantic.dev/messages/index.html#binarycontent-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "BinaryContent `dataclass`", "anchor": "binarycontent-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 ``` | ``` @dataclass(init=False, repr=False) class BinaryContent:     \"\"\"Binary content, e.g. an audio or image file.\"\"\"      data: bytes     \"\"\"The binary data.\"\"\"      _: KW_ONLY      media_type: AudioMediaType | ImageMediaType | DocumentMediaType | str     \"\"\"The media type of the binary data.\"\"\"      vendor_metadata: dict[str, Any] | None = None     \"\"\"Vendor-specific metadata for the file.      Supported by:     - `GoogleModel`: `BinaryContent.vendor_metadata` is used as `video_metadata`: https://ai.google.dev/gemini-api/docs/video-understanding#customize-video-processing     - `OpenAIChatModel`, `OpenAIResponsesModel`: `BinaryContent.vendor_metadata['detail']` is used as `detail` setting for images     \"\"\"      _identifier: Annotated[str | None, pydantic.Field(alias='identifier', default=None, exclude=True)] = field(         compare=False, default=None, repr=False     )      kind: Literal['binary'] = 'binary'     \"\"\"Type identifier, this is available on all parts as a discriminator.\"\"\"      def __init__(         self,         data: bytes,         *,         media_type: AudioMediaType | ImageMediaType | DocumentMediaType | str,         identifier: str | None = None,         vendor_metadata: dict[str, Any] | None = None,         kind: Literal['binary'] = 'binary',         # Required for inline-snapshot which expects all dataclass `__init__` methods to take all field names as kwargs.         _identifier: str | None = None,     ) -> None:         self.data = data         self.media_type = media_type         self._identifier = identifier or _identifier         self.vendor_metadata = vendor_metadata         self.kind = kind      @staticmethod     def narrow_type(bc: BinaryContent) -> BinaryContent | BinaryImage:         \"\"\"Narrow the type of the `BinaryContent` to `BinaryImage` if it's an image.\"\"\"         if bc.is_image:             return BinaryImage(                 data=bc.data,                 media_type=bc.media_type,                 identifier=bc.identifier,                 vendor_metadata=bc.vendor_metadata,             )         else:             return bc  # pragma: no cover      @classmethod     def from_data_uri(cls, data_uri: str) -> Self:         \"\"\"Create a `BinaryContent` from a data URI.\"\"\"         prefix = 'data:'         if not data_uri.startswith(prefix):             raise ValueError('Data URI must start with \"data:\"')  # pragma: no cover         media_type, data = data_uri[len(prefix) :].split(';base64,', 1)         return cls(data=base64.b64decode(data), media_type=media_type)      @pydantic.computed_field     @property     def identifier(self) -> str:         \"\"\"Identifier for the binary content, such as a unique ID.          This identifier can be provided to the model in a message to allow it to refer to this file in a tool call argument,         and the tool can look up the file in question by iterating over the message history and finding the matching `BinaryContent`.          This identifier is only automatically passed to the model when the `BinaryContent` is returned by a tool.         If you're passing the `BinaryContent` as a user message, it's up to you to include a separate text part with the identifier,         e.g. \"This is file <identifier>:\" preceding the `BinaryContent`.          It's also included in inline-text delimiters for providers that require inlining text documents, so the model can         distinguish multiple files.         \"\"\"         return self._identifier or _multi_modal_content_identifier(self.data)      @property     def data_uri(self) -> str:         \"\"\"Convert the `BinaryContent` to a data URI.\"\"\"         return f'data:{self.media_type};base64,{base64.b64encode(self.data).decode()}'      @property     def is_audio(self) -> bool:         \"\"\"Return `True` if the media type is an audio type.\"\"\"         return self.media_type.startswith('audio/')      @property     def is_image(self) -> bool:         \"\"\"Return `True` if the media type is an image type.\"\"\"         return self.media_type.startswith('image/')      @property     def is_video(self) -> bool:         \"\"\"Return `True` if the media type is a video type.\"\"\"         return self.media_type.startswith('video/')      @property     def is_document(self) -> bool:         \"\"\"Return `True` if the media type is a document type.\"\"\"         return self.media_type in _document_format_lookup      @property     def format(self) -> str:         \"\"\"The file format of the binary content.\"\"\"         try:             if self.is_audio:                 return _audio_format_lookup[self.media_type]             elif self.is_image:                 return _image_format_lookup[self.media_type]             elif self.is_video:                 return _video_format_lookup[self.media_type]             else:                 return _document_format_lookup[self.media_type]         except KeyError as e:             raise ValueError(f'Unknown media type: {self.media_type}') from e      __repr__ = _utils.dataclasses_no_defaults_repr ``` |", "url": "https://ai.pydantic.dev/messages/index.html#binarycontent-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "BinaryContent `dataclass`", "anchor": "binarycontent-dataclass", "md_text": "#### data `instance-attribute`\n\n```\ndata: bytes = data\n```\n\nThe binary data.\n\n#### media\\_type `instance-attribute`\n\n```\nmedia_type: (\n    AudioMediaType\n    | ImageMediaType\n    | DocumentMediaType\n    | str\n) = media_type\n```\n\nThe media type of the binary data.\n\n#### vendor\\_metadata `class-attribute` `instance-attribute`\n\n```\nvendor_metadata: dict[str, Any] | None = vendor_metadata\n```\n\nVendor-specific metadata for the file.\n\nSupported by:\n- `GoogleModel`: `BinaryContent.vendor_metadata` is used as `video_metadata`: https://ai.google.dev/gemini-api/docs/video-understanding#customize-video-processing\n- `OpenAIChatModel`, `OpenAIResponsesModel`: `BinaryContent.vendor_metadata['detail']` is used as `detail` setting for images\n\n#### kind `class-attribute` `instance-attribute`\n\n```\nkind: Literal['binary'] = kind\n```\n\nType identifier, this is available on all parts as a discriminator.\n\n#### narrow\\_type `staticmethod`\n\n```\nnarrow_type(\n    bc: BinaryContent,\n) -> BinaryContent | BinaryImage\n```\n\nNarrow the type of the `BinaryContent` to `BinaryImage` if it's an image.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 506 507 508 509 510 511 512 513 514 515 516 517 ``` | ``` @staticmethod def narrow_type(bc: BinaryContent) -> BinaryContent | BinaryImage:     \"\"\"Narrow the type of the `BinaryContent` to `BinaryImage` if it's an image.\"\"\"     if bc.is_image:         return BinaryImage(             data=bc.data,             media_type=bc.media_type,             identifier=bc.identifier,             vendor_metadata=bc.vendor_metadata,         )     else:         return bc  # pragma: no cover ``` |\n\n#### from\\_data\\_uri `classmethod`\n\n```\nfrom_data_uri(data_uri: str) -> Self\n```\n\nCreate a `BinaryContent` from a data URI.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 519 520 521 522 523 524 525 526 ``` | ``` @classmethod def from_data_uri(cls, data_uri: str) -> Self:     \"\"\"Create a `BinaryContent` from a data URI.\"\"\"     prefix = 'data:'     if not data_uri.startswith(prefix):         raise ValueError('Data URI must start with \"data:\"')  # pragma: no cover     media_type, data = data_uri[len(prefix) :].split(';base64,', 1)     return cls(data=base64.b64decode(data), media_type=media_type) ``` |\n\n#### identifier `property`\n\n```\nidentifier: str\n```\n\nIdentifier for the binary content, such as a unique ID.\n\nThis identifier can be provided to the model in a message to allow it to refer to this file in a tool call argument,\nand the tool can look up the file in question by iterating over the message history and finding the matching `BinaryContent`.\n\nThis identifier is only automatically passed to the model when the `BinaryContent` is returned by a tool.\nIf you're passing the `BinaryContent` as a user message, it's up to you to include a separate text part with the identifier,\ne.g. \"This is file :\" preceding the `BinaryContent`.\n\nIt's also included in inline-text delimiters for providers that require inlining text documents, so the model can\ndistinguish multiple files.\n\n#### data\\_uri `property`\n\n```\ndata_uri: str\n```\n\nConvert the `BinaryContent` to a data URI.\n\n#### is\\_audio `property`\n\n```\nis_audio: bool\n```\n\nReturn `True` if the media type is an audio type.\n\n#### is\\_image `property`\n\n```\nis_image: bool\n```\n\nReturn `True` if the media type is an image type.\n\n#### is\\_video `property`\n\n```\nis_video: bool\n```\n\nReturn `True` if the media type is a video type.\n\n#### is\\_document `property`\n\n```\nis_document: bool\n```\n\nReturn `True` if the media type is a document type.\n\n#### format `property`\n\n```\nformat: str\n```\n\nThe file format of the binary content.", "url": "https://ai.pydantic.dev/messages/index.html#binarycontent-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "BinaryImage", "anchor": "binaryimage", "md_text": "Bases: `BinaryContent`\n\nBinary content that's guaranteed to be an image.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 ``` | ``` class BinaryImage(BinaryContent):     \"\"\"Binary content that's guaranteed to be an image.\"\"\"      def __init__(         self,         data: bytes,         *,         media_type: str,         identifier: str | None = None,         vendor_metadata: dict[str, Any] | None = None,         # Required for inline-snapshot which expects all dataclass `__init__` methods to take all field names as kwargs.         kind: Literal['binary'] = 'binary',         _identifier: str | None = None,     ):         super().__init__(             data=data, media_type=media_type, identifier=identifier or _identifier, vendor_metadata=vendor_metadata         )          if not self.is_image:             raise ValueError('`BinaryImage` must be have a media type that starts with \"image/\"')  # pragma: no cover ``` |", "url": "https://ai.pydantic.dev/messages/index.html#binaryimage", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "ToolReturn `dataclass`", "anchor": "toolreturn-dataclass", "md_text": "A structured return value for tools that need to provide both a return value and custom content to the model.\n\nThis class allows tools to return complex responses that include:\n- A return value for actual tool return\n- Custom content (including multi-modal content) to be sent to the model as a UserPromptPart\n- Optional metadata for application use\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 ``` | ``` @dataclass(repr=False) class ToolReturn:     \"\"\"A structured return value for tools that need to provide both a return value and custom content to the model.      This class allows tools to return complex responses that include:     - A return value for actual tool return     - Custom content (including multi-modal content) to be sent to the model as a UserPromptPart     - Optional metadata for application use     \"\"\"      return_value: Any     \"\"\"The return value to be used in the tool response.\"\"\"      _: KW_ONLY      content: str | Sequence[UserContent] | None = None     \"\"\"The content to be sent to the model as a UserPromptPart.\"\"\"      metadata: Any = None     \"\"\"Additional data that can be accessed programmatically by the application but is not sent to the LLM.\"\"\"      kind: Literal['tool-return'] = 'tool-return'      __repr__ = _utils.dataclasses_no_defaults_repr ``` |\n\n#### return\\_value `instance-attribute`\n\n```\nreturn_value: Any\n```\n\nThe return value to be used in the tool response.\n\n#### content `class-attribute` `instance-attribute`\n\n```\ncontent: str | Sequence[UserContent] | None = None\n```\n\nThe content to be sent to the model as a UserPromptPart.\n\n#### metadata `class-attribute` `instance-attribute`\n\n```\nmetadata: Any = None\n```\n\nAdditional data that can be accessed programmatically by the application but is not sent to the LLM.", "url": "https://ai.pydantic.dev/messages/index.html#toolreturn-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "UserPromptPart `dataclass`", "anchor": "userpromptpart-dataclass", "md_text": "A user prompt, generally written by the end user.\n\nContent comes from the `user_prompt` parameter of [`Agent.run`](../agent/index.html#pydantic_ai.agent.AbstractAgent.run),\n[`Agent.run_sync`](../agent/index.html#pydantic_ai.agent.AbstractAgent.run_sync), and [`Agent.run_stream`](../agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream).\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 ``` | ``` @dataclass(repr=False) class UserPromptPart:     \"\"\"A user prompt, generally written by the end user.      Content comes from the `user_prompt` parameter of [`Agent.run`][pydantic_ai.agent.AbstractAgent.run],     [`Agent.run_sync`][pydantic_ai.agent.AbstractAgent.run_sync], and [`Agent.run_stream`][pydantic_ai.agent.AbstractAgent.run_stream].     \"\"\"      content: str | Sequence[UserContent]     \"\"\"The content of the prompt.\"\"\"      _: KW_ONLY      timestamp: datetime = field(default_factory=_now_utc)     \"\"\"The timestamp of the prompt.\"\"\"      part_kind: Literal['user-prompt'] = 'user-prompt'     \"\"\"Part type identifier, this is available on all parts as a discriminator.\"\"\"      def otel_event(self, settings: InstrumentationSettings) -> Event:         content = [{'kind': part.pop('type'), **part} for part in self.otel_message_parts(settings)]         for part in content:             if part['kind'] == 'binary' and 'content' in part:                 part['binary_content'] = part.pop('content')         content = [             part['content'] if part == {'kind': 'text', 'content': part.get('content')} else part for part in content         ]         if content in ([{'kind': 'text'}], [self.content]):             content = content[0]         return Event('gen_ai.user.message', body={'content': content, 'role': 'user'})      def otel_message_parts(self, settings: InstrumentationSettings) -> list[_otel_messages.MessagePart]:         parts: list[_otel_messages.MessagePart] = []         content: Sequence[UserContent] = [self.content] if isinstance(self.content, str) else self.content         for part in content:             if isinstance(part, str):                 parts.append(                     _otel_messages.TextPart(type='text', **({'content': part} if settings.include_content else {}))                 )             elif isinstance(part, ImageUrl | AudioUrl | DocumentUrl | VideoUrl):                 parts.append(                     _otel_messages.MediaUrlPart(                         type=part.kind,                         **{'url': part.url} if settings.include_content else {},                     )                 )             elif isinstance(part, BinaryContent):                 converted_part = _otel_messages.BinaryDataPart(type='binary', media_type=part.media_type)                 if settings.include_content and settings.include_binary_content:                     converted_part['content'] = base64.b64encode(part.data).decode()                 parts.append(converted_part)             else:                 parts.append({'type': part.kind})  # pragma: no cover         return parts      __repr__ = _utils.dataclasses_no_defaults_repr ``` |\n\n#### content `instance-attribute`\n\n```\ncontent: str | Sequence[UserContent]\n```\n\nThe content of the prompt.\n\n#### timestamp `class-attribute` `instance-attribute`\n\n```\ntimestamp: datetime = field(default_factory=now_utc)\n```\n\nThe timestamp of the prompt.\n\n#### part\\_kind `class-attribute` `instance-attribute`\n\n```\npart_kind: Literal['user-prompt'] = 'user-prompt'\n```\n\nPart type identifier, this is available on all parts as a discriminator.", "url": "https://ai.pydantic.dev/messages/index.html#userpromptpart-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "BaseToolReturnPart `dataclass`", "anchor": "basetoolreturnpart-dataclass", "md_text": "Base class for tool return parts.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 ``` | ``` @dataclass(repr=False) class BaseToolReturnPart:     \"\"\"Base class for tool return parts.\"\"\"      tool_name: str     \"\"\"The name of the \"tool\" was called.\"\"\"      content: Any     \"\"\"The return value.\"\"\"      tool_call_id: str = field(default_factory=_generate_tool_call_id)     \"\"\"The tool call identifier, this is used by some models including OpenAI.      In case the tool call id is not provided by the model, Pydantic AI will generate a random one.     \"\"\"      _: KW_ONLY      metadata: Any = None     \"\"\"Additional data that can be accessed programmatically by the application but is not sent to the LLM.\"\"\"      timestamp: datetime = field(default_factory=_now_utc)     \"\"\"The timestamp, when the tool returned.\"\"\"      def model_response_str(self) -> str:         \"\"\"Return a string representation of the content for the model.\"\"\"         if isinstance(self.content, str):             return self.content         else:             return tool_return_ta.dump_json(self.content).decode()      def model_response_object(self) -> dict[str, Any]:         \"\"\"Return a dictionary representation of the content, wrapping non-dict types appropriately.\"\"\"         # gemini supports JSON dict return values, but no other JSON types, hence we wrap anything else in a dict         if isinstance(self.content, dict):             return tool_return_ta.dump_python(self.content, mode='json')  # pyright: ignore[reportUnknownMemberType]         else:             return {'return_value': tool_return_ta.dump_python(self.content, mode='json')}      def otel_event(self, settings: InstrumentationSettings) -> Event:         return Event(             'gen_ai.tool.message',             body={                 **({'content': self.content} if settings.include_content else {}),                 'role': 'tool',                 'id': self.tool_call_id,                 'name': self.tool_name,             },         )      def otel_message_parts(self, settings: InstrumentationSettings) -> list[_otel_messages.MessagePart]:         from .models.instrumented import InstrumentedModel          part = _otel_messages.ToolCallResponsePart(             type='tool_call_response',             id=self.tool_call_id,             name=self.tool_name,         )          if settings.include_content and self.content is not None:             part['result'] = InstrumentedModel.serialize_any(self.content)          return [part]      def has_content(self) -> bool:         \"\"\"Return `True` if the tool return has content.\"\"\"         return self.content is not None  # pragma: no cover      __repr__ = _utils.dataclasses_no_defaults_repr ``` |\n\n#### tool\\_name `instance-attribute`\n\n```\ntool_name: str\n```\n\nThe name of the \"tool\" was called.\n\n#### content `instance-attribute`\n\n```\ncontent: Any\n```\n\nThe return value.\n\n#### tool\\_call\\_id `class-attribute` `instance-attribute`\n\n```\ntool_call_id: str = field(\n    default_factory=generate_tool_call_id\n)\n```\n\nThe tool call identifier, this is used by some models including OpenAI.\n\nIn case the tool call id is not provided by the model, Pydantic AI will generate a random one.\n\n#### metadata `class-attribute` `instance-attribute`\n\n```\nmetadata: Any = None\n```\n\nAdditional data that can be accessed programmatically by the application but is not sent to the LLM.\n\n#### timestamp `class-attribute` `instance-attribute`\n\n```\ntimestamp: datetime = field(default_factory=now_utc)\n```\n\nThe timestamp, when the tool returned.\n\n#### model\\_response\\_str\n\n```\nmodel_response_str() -> str\n```\n\nReturn a string representation of the content for the model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 763 764 765 766 767 768 ``` | ``` def model_response_str(self) -> str:     \"\"\"Return a string representation of the content for the model.\"\"\"     if isinstance(self.content, str):         return self.content     else:         return tool_return_ta.dump_json(self.content).decode() ``` |\n\n#### model\\_response\\_object\n\n```\nmodel_response_object() -> dict[str, Any]\n```\n\nReturn a dictionary representation of the content, wrapping non-dict types appropriately.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`", "url": "https://ai.pydantic.dev/messages/index.html#basetoolreturnpart-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "BaseToolReturnPart `dataclass`", "anchor": "basetoolreturnpart-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 770 771 772 773 774 775 776 ``` | ``` def model_response_object(self) -> dict[str, Any]:     \"\"\"Return a dictionary representation of the content, wrapping non-dict types appropriately.\"\"\"     # gemini supports JSON dict return values, but no other JSON types, hence we wrap anything else in a dict     if isinstance(self.content, dict):         return tool_return_ta.dump_python(self.content, mode='json')  # pyright: ignore[reportUnknownMemberType]     else:         return {'return_value': tool_return_ta.dump_python(self.content, mode='json')} ``` |\n\n#### has\\_content\n\n```\nhas_content() -> bool\n```\n\nReturn `True` if the tool return has content.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 803 804 805 ``` | ``` def has_content(self) -> bool:     \"\"\"Return `True` if the tool return has content.\"\"\"     return self.content is not None  # pragma: no cover ``` |", "url": "https://ai.pydantic.dev/messages/index.html#basetoolreturnpart-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "ToolReturnPart `dataclass`", "anchor": "toolreturnpart-dataclass", "md_text": "Bases: `BaseToolReturnPart`\n\nA tool return message, this encodes the result of running a tool.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 810 811 812 813 814 815 816 817 ``` | ``` @dataclass(repr=False) class ToolReturnPart(BaseToolReturnPart):     \"\"\"A tool return message, this encodes the result of running a tool.\"\"\"      _: KW_ONLY      part_kind: Literal['tool-return'] = 'tool-return'     \"\"\"Part type identifier, this is available on all parts as a discriminator.\"\"\" ``` |\n\n#### part\\_kind `class-attribute` `instance-attribute`\n\n```\npart_kind: Literal['tool-return'] = 'tool-return'\n```\n\nPart type identifier, this is available on all parts as a discriminator.", "url": "https://ai.pydantic.dev/messages/index.html#toolreturnpart-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "BuiltinToolReturnPart `dataclass`", "anchor": "builtintoolreturnpart-dataclass", "md_text": "Bases: `BaseToolReturnPart`\n\nA tool return message from a built-in tool.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 820 821 822 823 824 825 826 827 828 829 830 ``` | ``` @dataclass(repr=False) class BuiltinToolReturnPart(BaseToolReturnPart):     \"\"\"A tool return message from a built-in tool.\"\"\"      _: KW_ONLY      provider_name: str | None = None     \"\"\"The name of the provider that generated the response.\"\"\"      part_kind: Literal['builtin-tool-return'] = 'builtin-tool-return'     \"\"\"Part type identifier, this is available on all parts as a discriminator.\"\"\" ``` |\n\n#### provider\\_name `class-attribute` `instance-attribute`\n\n```\nprovider_name: str | None = None\n```\n\nThe name of the provider that generated the response.\n\n#### part\\_kind `class-attribute` `instance-attribute`\n\n```\npart_kind: Literal[\"builtin-tool-return\"] = (\n    \"builtin-tool-return\"\n)\n```\n\nPart type identifier, this is available on all parts as a discriminator.", "url": "https://ai.pydantic.dev/messages/index.html#builtintoolreturnpart-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "RetryPromptPart `dataclass`", "anchor": "retrypromptpart-dataclass", "md_text": "A message back to a model asking it to try again.\n\nThis can be sent for a number of reasons:\n\n* Pydantic validation of tool arguments failed, here content is derived from a Pydantic\n  [`ValidationError`](https://docs.pydantic.dev/latest/api/pydantic_core/#pydantic_core.ValidationError)\n* a tool raised a [`ModelRetry`](../exceptions/index.html#pydantic_ai.exceptions.ModelRetry) exception\n* no tool was found for the tool name\n* the model returned plain text when a structured response was expected\n* Pydantic validation of a structured response failed, here content is derived from a Pydantic\n  [`ValidationError`](https://docs.pydantic.dev/latest/api/pydantic_core/#pydantic_core.ValidationError)\n* an output validator raised a [`ModelRetry`](../exceptions/index.html#pydantic_ai.exceptions.ModelRetry) exception\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 ``` | ``` @dataclass(repr=False) class RetryPromptPart:     \"\"\"A message back to a model asking it to try again.      This can be sent for a number of reasons:      * Pydantic validation of tool arguments failed, here content is derived from a Pydantic       [`ValidationError`][pydantic_core.ValidationError]     * a tool raised a [`ModelRetry`][pydantic_ai.exceptions.ModelRetry] exception     * no tool was found for the tool name     * the model returned plain text when a structured response was expected     * Pydantic validation of a structured response failed, here content is derived from a Pydantic       [`ValidationError`][pydantic_core.ValidationError]     * an output validator raised a [`ModelRetry`][pydantic_ai.exceptions.ModelRetry] exception     \"\"\"      content: list[pydantic_core.ErrorDetails] | str     \"\"\"Details of why and how the model should retry.      If the retry was triggered by a [`ValidationError`][pydantic_core.ValidationError], this will be a list of     error details.     \"\"\"      _: KW_ONLY      tool_name: str | None = None     \"\"\"The name of the tool that was called, if any.\"\"\"      tool_call_id: str = field(default_factory=_generate_tool_call_id)     \"\"\"The tool call identifier, this is used by some models including OpenAI.      In case the tool call id is not provided by the model, Pydantic AI will generate a random one.     \"\"\"      timestamp: datetime = field(default_factory=_now_utc)     \"\"\"The timestamp, when the retry was triggered.\"\"\"      part_kind: Literal['retry-prompt'] = 'retry-prompt'     \"\"\"Part type identifier, this is available on all parts as a discriminator.\"\"\"      def model_response(self) -> str:         \"\"\"Return a string message describing why the retry is requested.\"\"\"         if isinstance(self.content, str):             if self.tool_name is None:                 description = f'Validation feedback:\\n{self.content}'             else:                 description = self.content         else:             json_errors = error_details_ta.dump_json(self.content, exclude={'__all__': {'ctx'}}, indent=2)             description = f'{len(self.content)} validation errors: {json_errors.decode()}'         return f'{description}\\n\\nFix the errors and try again.'      def otel_event(self, settings: InstrumentationSettings) -> Event:         if self.tool_name is None:             return Event('gen_ai.user.message', body={'content': self.model_response(), 'role': 'user'})         else:             return Event(                 'gen_ai.tool.message',                 body={                     **({'content': self.model_response()} if settings.include_content else {}),                     'role': 'tool',                     'id': self.tool_call_id,                     'name': self.tool_name,                 },             )      def otel_message_parts(self, settings: InstrumentationSettings) -> list[_otel_messages.MessagePart]:         if self.tool_name is None:             return [_otel_messages.TextPart(type='text', content=self.model_response())]         else:             part = _otel_messages.ToolCallResponsePart(                 type='tool_call_response',                 id=self.tool_call_id,                 name=self.tool_name,             )              if settings.include_content:                 part['result'] = self.model_response()              return [part]      __repr__ = _utils.dataclasses_no_defaults_repr ``` |\n\n#### content `instance-attribute`\n\n```\ncontent: list[ErrorDetails] | str\n```\n\nDetails of why and how the model should retry.\n\nIf the retry was triggered by a [`ValidationError`](https://docs.pydantic.dev/latest/api/pydantic_core/#pydantic_core.ValidationError), this will be a list of\nerror details.\n\n#### tool\\_name `class-attribute` `instance-attribute`\n\n```\ntool_name: str | None = None\n```\n\nThe name of the tool that was called, if any.", "url": "https://ai.pydantic.dev/messages/index.html#retrypromptpart-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "RetryPromptPart `dataclass`", "anchor": "retrypromptpart-dataclass", "md_text": "#### tool\\_call\\_id `class-attribute` `instance-attribute`\n\n```\ntool_call_id: str = field(\n    default_factory=generate_tool_call_id\n)\n```\n\nThe tool call identifier, this is used by some models including OpenAI.\n\nIn case the tool call id is not provided by the model, Pydantic AI will generate a random one.\n\n#### timestamp `class-attribute` `instance-attribute`\n\n```\ntimestamp: datetime = field(default_factory=now_utc)\n```\n\nThe timestamp, when the retry was triggered.\n\n#### part\\_kind `class-attribute` `instance-attribute`\n\n```\npart_kind: Literal['retry-prompt'] = 'retry-prompt'\n```\n\nPart type identifier, this is available on all parts as a discriminator.\n\n#### model\\_response\n\n```\nmodel_response() -> str\n```\n\nReturn a string message describing why the retry is requested.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 876 877 878 879 880 881 882 883 884 885 886 ``` | ``` def model_response(self) -> str:     \"\"\"Return a string message describing why the retry is requested.\"\"\"     if isinstance(self.content, str):         if self.tool_name is None:             description = f'Validation feedback:\\n{self.content}'         else:             description = self.content     else:         json_errors = error_details_ta.dump_json(self.content, exclude={'__all__': {'ctx'}}, indent=2)         description = f'{len(self.content)} validation errors: {json_errors.decode()}'     return f'{description}\\n\\nFix the errors and try again.' ``` |", "url": "https://ai.pydantic.dev/messages/index.html#retrypromptpart-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "ModelRequestPart `module-attribute`", "anchor": "modelrequestpart-module-attribute", "md_text": "```\nModelRequestPart = Annotated[\n    SystemPromptPart\n    | UserPromptPart\n    | ToolReturnPart\n    | RetryPromptPart,\n    Discriminator(\"part_kind\"),\n]\n```\n\nA message part sent by Pydantic AI to a model.", "url": "https://ai.pydantic.dev/messages/index.html#modelrequestpart-module-attribute", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "ModelRequest `dataclass`", "anchor": "modelrequest-dataclass", "md_text": "A request generated by Pydantic AI and sent to a model, e.g. a message from the Pydantic AI app to the model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 ``` | ``` @dataclass(repr=False) class ModelRequest:     \"\"\"A request generated by Pydantic AI and sent to a model, e.g. a message from the Pydantic AI app to the model.\"\"\"      parts: Sequence[ModelRequestPart]     \"\"\"The parts of the user message.\"\"\"      _: KW_ONLY      instructions: str | None = None     \"\"\"The instructions for the model.\"\"\"      kind: Literal['request'] = 'request'     \"\"\"Message type identifier, this is available on all parts as a discriminator.\"\"\"      @classmethod     def user_text_prompt(cls, user_prompt: str, *, instructions: str | None = None) -> ModelRequest:         \"\"\"Create a `ModelRequest` with a single user prompt as text.\"\"\"         return cls(parts=[UserPromptPart(user_prompt)], instructions=instructions)      __repr__ = _utils.dataclasses_no_defaults_repr ``` |\n\n#### parts `instance-attribute`\n\n```\nparts: Sequence[ModelRequestPart]\n```\n\nThe parts of the user message.\n\n#### instructions `class-attribute` `instance-attribute`\n\n```\ninstructions: str | None = None\n```\n\nThe instructions for the model.\n\n#### kind `class-attribute` `instance-attribute`\n\n```\nkind: Literal['request'] = 'request'\n```\n\nMessage type identifier, this is available on all parts as a discriminator.\n\n#### user\\_text\\_prompt `classmethod`\n\n```\nuser_text_prompt(\n    user_prompt: str, *, instructions: str | None = None\n) -> ModelRequest\n```\n\nCreate a `ModelRequest` with a single user prompt as text.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 941 942 943 944 ``` | ``` @classmethod def user_text_prompt(cls, user_prompt: str, *, instructions: str | None = None) -> ModelRequest:     \"\"\"Create a `ModelRequest` with a single user prompt as text.\"\"\"     return cls(parts=[UserPromptPart(user_prompt)], instructions=instructions) ``` |", "url": "https://ai.pydantic.dev/messages/index.html#modelrequest-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "TextPart `dataclass`", "anchor": "textpart-dataclass", "md_text": "A plain text response from a model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 949 950 951 952 953 954 955 956 957 958 959 960 961 962 963 964 965 966 967 968 ``` | ``` @dataclass(repr=False) class TextPart:     \"\"\"A plain text response from a model.\"\"\"      content: str     \"\"\"The text content of the response.\"\"\"      _: KW_ONLY      id: str | None = None     \"\"\"An optional identifier of the text part.\"\"\"      part_kind: Literal['text'] = 'text'     \"\"\"Part type identifier, this is available on all parts as a discriminator.\"\"\"      def has_content(self) -> bool:         \"\"\"Return `True` if the text content is non-empty.\"\"\"         return bool(self.content)      __repr__ = _utils.dataclasses_no_defaults_repr ``` |\n\n#### content `instance-attribute`\n\n```\ncontent: str\n```\n\nThe text content of the response.\n\n#### id `class-attribute` `instance-attribute`\n\n```\nid: str | None = None\n```\n\nAn optional identifier of the text part.\n\n#### part\\_kind `class-attribute` `instance-attribute`\n\n```\npart_kind: Literal['text'] = 'text'\n```\n\nPart type identifier, this is available on all parts as a discriminator.\n\n#### has\\_content\n\n```\nhas_content() -> bool\n```\n\nReturn `True` if the text content is non-empty.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 964 965 966 ``` | ``` def has_content(self) -> bool:     \"\"\"Return `True` if the text content is non-empty.\"\"\"     return bool(self.content) ``` |", "url": "https://ai.pydantic.dev/messages/index.html#textpart-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "ThinkingPart `dataclass`", "anchor": "thinkingpart-dataclass", "md_text": "A thinking response from a model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ```  971  972  973  974  975  976  977  978  979  980  981  982  983  984  985  986  987  988  989  990  991  992  993  994  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007 ``` | ``` @dataclass(repr=False) class ThinkingPart:     \"\"\"A thinking response from a model.\"\"\"      content: str     \"\"\"The thinking content of the response.\"\"\"      _: KW_ONLY      id: str | None = None     \"\"\"The identifier of the thinking part.\"\"\"      signature: str | None = None     \"\"\"The signature of the thinking.      Supported by:      * Anthropic (corresponds to the `signature` field)     * Bedrock (corresponds to the `signature` field)     * Google (corresponds to the `thought_signature` field)     * OpenAI (corresponds to the `encrypted_content` field)     \"\"\"      provider_name: str | None = None     \"\"\"The name of the provider that generated the response.      Signatures are only sent back to the same provider.     \"\"\"      part_kind: Literal['thinking'] = 'thinking'     \"\"\"Part type identifier, this is available on all parts as a discriminator.\"\"\"      def has_content(self) -> bool:         \"\"\"Return `True` if the thinking content is non-empty.\"\"\"         return bool(self.content)      __repr__ = _utils.dataclasses_no_defaults_repr ``` |\n\n#### content `instance-attribute`\n\n```\ncontent: str\n```\n\nThe thinking content of the response.\n\n#### id `class-attribute` `instance-attribute`\n\n```\nid: str | None = None\n```\n\nThe identifier of the thinking part.\n\n#### signature `class-attribute` `instance-attribute`\n\n```\nsignature: str | None = None\n```\n\nThe signature of the thinking.\n\nSupported by:\n\n* Anthropic (corresponds to the `signature` field)\n* Bedrock (corresponds to the `signature` field)\n* Google (corresponds to the `thought_signature` field)\n* OpenAI (corresponds to the `encrypted_content` field)\n\n#### provider\\_name `class-attribute` `instance-attribute`\n\n```\nprovider_name: str | None = None\n```\n\nThe name of the provider that generated the response.\n\nSignatures are only sent back to the same provider.\n\n#### part\\_kind `class-attribute` `instance-attribute`\n\n```\npart_kind: Literal['thinking'] = 'thinking'\n```\n\nPart type identifier, this is available on all parts as a discriminator.\n\n#### has\\_content\n\n```\nhas_content() -> bool\n```\n\nReturn `True` if the thinking content is non-empty.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 1003 1004 1005 ``` | ``` def has_content(self) -> bool:     \"\"\"Return `True` if the thinking content is non-empty.\"\"\"     return bool(self.content) ``` |", "url": "https://ai.pydantic.dev/messages/index.html#thinkingpart-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "FilePart `dataclass`", "anchor": "filepart-dataclass", "md_text": "A file response from a model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 ``` | ``` @dataclass(repr=False) class FilePart:     \"\"\"A file response from a model.\"\"\"      content: Annotated[BinaryContent, pydantic.AfterValidator(BinaryImage.narrow_type)]     \"\"\"The file content of the response.\"\"\"      _: KW_ONLY      id: str | None = None     \"\"\"The identifier of the file part.\"\"\"      provider_name: str | None = None     \"\"\"The name of the provider that generated the response.     \"\"\"      part_kind: Literal['file'] = 'file'     \"\"\"Part type identifier, this is available on all parts as a discriminator.\"\"\"      def has_content(self) -> bool:         \"\"\"Return `True` if the file content is non-empty.\"\"\"         return bool(self.content)  # pragma: no cover      __repr__ = _utils.dataclasses_no_defaults_repr ``` |\n\n#### content `instance-attribute`\n\n```\ncontent: Annotated[\n    BinaryContent, AfterValidator(narrow_type)\n]\n```\n\nThe file content of the response.\n\n#### id `class-attribute` `instance-attribute`\n\n```\nid: str | None = None\n```\n\nThe identifier of the file part.\n\n#### provider\\_name `class-attribute` `instance-attribute`\n\n```\nprovider_name: str | None = None\n```\n\nThe name of the provider that generated the response.\n\n#### part\\_kind `class-attribute` `instance-attribute`\n\n```\npart_kind: Literal['file'] = 'file'\n```\n\nPart type identifier, this is available on all parts as a discriminator.\n\n#### has\\_content\n\n```\nhas_content() -> bool\n```\n\nReturn `True` if the file content is non-empty.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 1029 1030 1031 ``` | ``` def has_content(self) -> bool:     \"\"\"Return `True` if the file content is non-empty.\"\"\"     return bool(self.content)  # pragma: no cover ``` |", "url": "https://ai.pydantic.dev/messages/index.html#filepart-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "BaseToolCallPart `dataclass`", "anchor": "basetoolcallpart-dataclass", "md_text": "A tool call from a model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 ``` | ``` @dataclass(repr=False) class BaseToolCallPart:     \"\"\"A tool call from a model.\"\"\"      tool_name: str     \"\"\"The name of the tool to call.\"\"\"      args: str | dict[str, Any] | None = None     \"\"\"The arguments to pass to the tool.      This is stored either as a JSON string or a Python dictionary depending on how data was received.     \"\"\"      tool_call_id: str = field(default_factory=_generate_tool_call_id)     \"\"\"The tool call identifier, this is used by some models including OpenAI.      In case the tool call id is not provided by the model, Pydantic AI will generate a random one.     \"\"\"      _: KW_ONLY      id: str | None = None     \"\"\"An optional identifier of the tool call part, separate from the tool call ID.      This is used by some APIs like OpenAI Responses.\"\"\"      def args_as_dict(self) -> dict[str, Any]:         \"\"\"Return the arguments as a Python dictionary.          This is just for convenience with models that require dicts as input.         \"\"\"         if not self.args:             return {}         if isinstance(self.args, dict):             return self.args         args = pydantic_core.from_json(self.args)         assert isinstance(args, dict), 'args should be a dict'         return cast(dict[str, Any], args)      def args_as_json_str(self) -> str:         \"\"\"Return the arguments as a JSON string.          This is just for convenience with models that require JSON strings as input.         \"\"\"         if not self.args:             return '{}'         if isinstance(self.args, str):             return self.args         return pydantic_core.to_json(self.args).decode()      def has_content(self) -> bool:         \"\"\"Return `True` if the arguments contain any data.\"\"\"         if isinstance(self.args, dict):             # TODO: This should probably return True if you have the value False, or 0, etc.             #   It makes sense to me to ignore empty strings, but not sure about empty lists or dicts             return any(self.args.values())         else:             return bool(self.args)      __repr__ = _utils.dataclasses_no_defaults_repr ``` |\n\n#### tool\\_name `instance-attribute`\n\n```\ntool_name: str\n```\n\nThe name of the tool to call.\n\n#### args `class-attribute` `instance-attribute`\n\n```\nargs: str | dict[str, Any] | None = None\n```\n\nThe arguments to pass to the tool.\n\nThis is stored either as a JSON string or a Python dictionary depending on how data was received.\n\n#### tool\\_call\\_id `class-attribute` `instance-attribute`\n\n```\ntool_call_id: str = field(\n    default_factory=generate_tool_call_id\n)\n```\n\nThe tool call identifier, this is used by some models including OpenAI.\n\nIn case the tool call id is not provided by the model, Pydantic AI will generate a random one.\n\n#### id `class-attribute` `instance-attribute`\n\n```\nid: str | None = None\n```\n\nAn optional identifier of the tool call part, separate from the tool call ID.\n\nThis is used by some APIs like OpenAI Responses.\n\n#### args\\_as\\_dict\n\n```\nargs_as_dict() -> dict[str, Any]\n```\n\nReturn the arguments as a Python dictionary.\n\nThis is just for convenience with models that require dicts as input.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 ``` | ``` def args_as_dict(self) -> dict[str, Any]:     \"\"\"Return the arguments as a Python dictionary.      This is just for convenience with models that require dicts as input.     \"\"\"     if not self.args:         return {}     if isinstance(self.args, dict):         return self.args     args = pydantic_core.from_json(self.args)     assert isinstance(args, dict), 'args should be a dict'     return cast(dict[str, Any], args) ``` |\n\n#### args\\_as\\_json\\_str\n\n```\nargs_as_json_str() -> str\n```\n\nReturn the arguments as a JSON string.\n\nThis is just for convenience with models that require JSON strings as input.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`", "url": "https://ai.pydantic.dev/messages/index.html#basetoolcallpart-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "BaseToolCallPart `dataclass`", "anchor": "basetoolcallpart-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 ``` | ``` def args_as_json_str(self) -> str:     \"\"\"Return the arguments as a JSON string.      This is just for convenience with models that require JSON strings as input.     \"\"\"     if not self.args:         return '{}'     if isinstance(self.args, str):         return self.args     return pydantic_core.to_json(self.args).decode() ``` |\n\n#### has\\_content\n\n```\nhas_content() -> bool\n```\n\nReturn `True` if the arguments contain any data.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 1086 1087 1088 1089 1090 1091 1092 1093 ``` | ``` def has_content(self) -> bool:     \"\"\"Return `True` if the arguments contain any data.\"\"\"     if isinstance(self.args, dict):         # TODO: This should probably return True if you have the value False, or 0, etc.         #   It makes sense to me to ignore empty strings, but not sure about empty lists or dicts         return any(self.args.values())     else:         return bool(self.args) ``` |", "url": "https://ai.pydantic.dev/messages/index.html#basetoolcallpart-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "ToolCallPart `dataclass`", "anchor": "toolcallpart-dataclass", "md_text": "Bases: `BaseToolCallPart`\n\nA tool call from a model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 1098 1099 1100 1101 1102 1103 1104 1105 ``` | ``` @dataclass(repr=False) class ToolCallPart(BaseToolCallPart):     \"\"\"A tool call from a model.\"\"\"      _: KW_ONLY      part_kind: Literal['tool-call'] = 'tool-call'     \"\"\"Part type identifier, this is available on all parts as a discriminator.\"\"\" ``` |\n\n#### part\\_kind `class-attribute` `instance-attribute`\n\n```\npart_kind: Literal['tool-call'] = 'tool-call'\n```\n\nPart type identifier, this is available on all parts as a discriminator.", "url": "https://ai.pydantic.dev/messages/index.html#toolcallpart-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "BuiltinToolCallPart `dataclass`", "anchor": "builtintoolcallpart-dataclass", "md_text": "Bases: `BaseToolCallPart`\n\nA tool call to a built-in tool.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 ``` | ``` @dataclass(repr=False) class BuiltinToolCallPart(BaseToolCallPart):     \"\"\"A tool call to a built-in tool.\"\"\"      _: KW_ONLY      provider_name: str | None = None     \"\"\"The name of the provider that generated the response.      Built-in tool calls are only sent back to the same provider.     \"\"\"      part_kind: Literal['builtin-tool-call'] = 'builtin-tool-call'     \"\"\"Part type identifier, this is available on all parts as a discriminator.\"\"\" ``` |\n\n#### provider\\_name `class-attribute` `instance-attribute`\n\n```\nprovider_name: str | None = None\n```\n\nThe name of the provider that generated the response.\n\nBuilt-in tool calls are only sent back to the same provider.\n\n#### part\\_kind `class-attribute` `instance-attribute`\n\n```\npart_kind: Literal[\"builtin-tool-call\"] = (\n    \"builtin-tool-call\"\n)\n```\n\nPart type identifier, this is available on all parts as a discriminator.", "url": "https://ai.pydantic.dev/messages/index.html#builtintoolcallpart-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "ModelResponsePart `module-attribute`", "anchor": "modelresponsepart-module-attribute", "md_text": "```\nModelResponsePart = Annotated[\n    TextPart\n    | ToolCallPart\n    | BuiltinToolCallPart\n    | BuiltinToolReturnPart\n    | ThinkingPart\n    | FilePart,\n    Discriminator(\"part_kind\"),\n]\n```\n\nA message part returned by a model.", "url": "https://ai.pydantic.dev/messages/index.html#modelresponsepart-module-attribute", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "ModelResponse `dataclass`", "anchor": "modelresponse-dataclass", "md_text": "A response from a model, e.g. a message from the model to the Pydantic AI app.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`", "url": "https://ai.pydantic.dev/messages/index.html#modelresponse-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "ModelResponse `dataclass`", "anchor": "modelresponse-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 ``` | ``` @dataclass(repr=False) class ModelResponse:     \"\"\"A response from a model, e.g. a message from the model to the Pydantic AI app.\"\"\"      parts: Sequence[ModelResponsePart]     \"\"\"The parts of the model message.\"\"\"      _: KW_ONLY      usage: RequestUsage = field(default_factory=RequestUsage)     \"\"\"Usage information for the request.      This has a default to make tests easier, and to support loading old messages where usage will be missing.     \"\"\"      model_name: str | None = None     \"\"\"The name of the model that generated the response.\"\"\"      timestamp: datetime = field(default_factory=_now_utc)     \"\"\"The timestamp of the response.      If the model provides a timestamp in the response (as OpenAI does) that will be used.     \"\"\"      kind: Literal['response'] = 'response'     \"\"\"Message type identifier, this is available on all parts as a discriminator.\"\"\"      provider_name: str | None = None     \"\"\"The name of the LLM provider that generated the response.\"\"\"      provider_details: Annotated[         dict[str, Any] | None,         # `vendor_details` is deprecated, but we still want to support deserializing model responses stored in a DB before the name was changed         pydantic.Field(validation_alias=pydantic.AliasChoices('provider_details', 'vendor_details')),     ] = None     \"\"\"Additional provider-specific details in a serializable format.      This allows storing selected vendor-specific data that isn't mapped to standard ModelResponse fields.     For OpenAI models, this may include 'logprobs', 'finish_reason', etc.     \"\"\"      provider_response_id: Annotated[         str | None,         # `vendor_id` is deprecated, but we still want to support deserializing model responses stored in a DB before the name was changed         pydantic.Field(validation_alias=pydantic.AliasChoices('provider_response_id', 'vendor_id')),     ] = None     \"\"\"request ID as specified by the model provider. This can be used to track the specific request to the model.\"\"\"      finish_reason: FinishReason | None = None     \"\"\"Reason the model finished generating the response, normalized to OpenTelemetry values.\"\"\"      @property     def text(self) -> str | None:         \"\"\"Get the text in the response.\"\"\"         texts: list[str] = []         last_part: ModelResponsePart | None = None         for part in self.parts:             if isinstance(part, TextPart):                 # Adjacent text parts should be joined together, but if there are parts in between                 # (like built-in tool calls) they should have newlines between them                 if isinstance(last_part, TextPart):                     texts[-1] += part.content                 else:                     texts.append(part.content)             last_part = part         if not texts:             return None          return '\\n\\n'.join(texts)      @property     def thinking(self) -> str | None:         \"\"\"Get the thinking in the response.\"\"\"         thinking_parts = [part.content for part in self.parts if isinstance(part, ThinkingPart)]         if not thinking_parts:             return None         return '\\n\\n'.join(thinking_parts)      @property     def files(self) -> list[BinaryContent]:         \"\"\"Get the files in the response.\"\"\"         return [part.content for part in self.parts if isinstance(part, FilePart)]      @property     def images(self) -> list[BinaryImage]:         \"\"\"Get the images in the response.\"\"\"         return [file for file in self.files if isinstance(file, BinaryImage)]      @property     def tool_calls(self) -> list[ToolCallPart]:         \"\"\"Get the tool calls in the response.\"\"\"         return [part for part in self.parts if isinstance(part, ToolCallPart)]      @property     def builtin_tool_calls(self) -> list[tuple[BuiltinToolCallPart, BuiltinToolReturnPart]]:         \"\"\"Get the builtin tool calls and results in the response.\"\"\"         calls = [part for part in self.parts if isinstance(part, BuiltinToolCallPart)]         if not calls:             return []         returns_by_id = {part.tool_call_id: part for part in self.parts if isinstance(part, BuiltinToolReturnPart)}         return [             (call_part, returns_by_id[call_part.tool_call_id])             for call_part in calls             if call_part.tool_call_id in returns_by_id         ]      @deprecated('`price` is deprecated, use `cost` instead')     def price(self) -> genai_types.PriceCalculation:  # pragma: no cover         return self.cost()      def cost(self) -> genai_types.PriceCalculation:         \"\"\"Calculate the cost of the usage.          Uses [`genai-prices`](https://github.com/pydantic/genai-prices).         \"\"\"         assert self.model_name, 'Model name is required to calculate price'         return calc_price(             self.usage,             self.model_name,             provider_id=self.provider_name,             genai_request_timestamp=self.timestamp,         )      def otel_events(self, settings: InstrumentationSettings) -> list[Event]:         \"\"\"Return OpenTelemetry events for the response.\"\"\"         result: list[Event] = []          def new_event_body():             new_body: dict[str, Any] = {'role': 'assistant'}             ev = Event('gen_ai.assistant.message', body=new_body)             result.append(ev)             return new_body          body = new_event_body()         for part in self.parts:             if isinstance(part, ToolCallPart):                 body.setdefault('tool_calls', []).append(                     {                         'id': part.tool_call_id,                         'type': 'function',                         'function': {                             'name': part.tool_name,                             **({'arguments': part.args} if settings.include_content else {}),                         },                     }                 )             elif isinstance(part, TextPart | ThinkingPart):                 kind = part.part_kind                 body.setdefault('content', []).append(                     {'kind': kind, **({'text': part.content} if settings.include_content else {})}                 )             elif isinstance(part, FilePart):                 body.setdefault('content', []).append(                     {                         'kind': 'binary',                         'media_type': part.content.media_type,                         **(                             {'binary_content': base64.b64encode(part.content.data).decode()}                             if settings.include_content and settings.include_binary_content                             else {}                         ),                     }                 )          if content := body.get('content'):             text_content = content[0].get('text')             if content == [{'kind': 'text', 'text': text_content}]:                 body['content'] = text_content          return result      def otel_message_parts(self, settings: InstrumentationSettings) -> list[_otel_messages.MessagePart]:         parts: list[_otel_messages.MessagePart] = []         for part in self.parts:             if isinstance(part, TextPart):                 parts.append(                     _otel_messages.TextPart(                         type='text',                         **({'content': part.content} if settings.include_content else {}),                     )                 )             elif isinstance(part, ThinkingPart):                 parts.append(                     _otel_messages.ThinkingPart(                         type='thinking',                         **({'content': part.content} if settings.include_content else {}),                     )                 )             elif isinstance(part, FilePart):                 converted_part = _otel_messages.BinaryDataPart(type='binary', media_type=part.content.media_type)                 if settings.include_content and settings.include_binary_content:                     converted_part['content'] = base64.b64encode(part.content.data).decode()                 parts.append(converted_part)             elif isinstance(part, BaseToolCallPart):                 call_part = _otel_messages.ToolCallPart(type='tool_call', id=part.tool_call_id, name=part.tool_name)                 if isinstance(part, BuiltinToolCallPart):                     call_part['builtin'] = True                 if settings.include_content and part.args is not None:                     from .models.instrumented import InstrumentedModel                      if isinstance(part.args, str):                         call_part['arguments'] = part.args                     else:                         call_part['arguments'] = {k: InstrumentedModel.serialize_any(v) for k, v in part.args.items()}                  parts.append(call_part)             elif isinstance(part, BuiltinToolReturnPart):                 return_part = _otel_messages.ToolCallResponsePart(                     type='tool_call_response',                     id=part.tool_call_id,                     name=part.tool_name,                     builtin=True,                 )                 if settings.include_content and part.content is not None:  # pragma: no branch                     from .models.instrumented import InstrumentedModel                      return_part['result'] = InstrumentedModel.serialize_any(part.content)                  parts.append(return_part)         return parts      @property     @deprecated('`vendor_details` is deprecated, use `provider_details` instead')     def vendor_details(self) -> dict[str, Any] | None:         return self.provider_details      @property     @deprecated('`vendor_id` is deprecated, use `provider_response_id` instead')     def vendor_id(self) -> str | None:         return self.provider_response_id      @property     @deprecated('`provider_request_id` is deprecated, use `provider_response_id` instead')     def provider_request_id(self) -> str | None:         return self.provider_response_id      __repr__ = _utils.dataclasses_no_defaults_repr ``` |", "url": "https://ai.pydantic.dev/messages/index.html#modelresponse-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "ModelResponse `dataclass`", "anchor": "modelresponse-dataclass", "md_text": "#### parts `instance-attribute`\n\n```\nparts: Sequence[ModelResponsePart]\n```\n\nThe parts of the model message.\n\n#### usage `class-attribute` `instance-attribute`\n\n```\nusage: RequestUsage = field(default_factory=RequestUsage)\n```\n\nUsage information for the request.\n\nThis has a default to make tests easier, and to support loading old messages where usage will be missing.\n\n#### model\\_name `class-attribute` `instance-attribute`\n\n```\nmodel_name: str | None = None\n```\n\nThe name of the model that generated the response.\n\n#### timestamp `class-attribute` `instance-attribute`\n\n```\ntimestamp: datetime = field(default_factory=now_utc)\n```\n\nThe timestamp of the response.\n\nIf the model provides a timestamp in the response (as OpenAI does) that will be used.\n\n#### kind `class-attribute` `instance-attribute`\n\n```\nkind: Literal['response'] = 'response'\n```\n\nMessage type identifier, this is available on all parts as a discriminator.\n\n#### provider\\_name `class-attribute` `instance-attribute`\n\n```\nprovider_name: str | None = None\n```\n\nThe name of the LLM provider that generated the response.\n\n#### provider\\_details `class-attribute` `instance-attribute`\n\n```\nprovider_details: Annotated[\n    dict[str, Any] | None,\n    Field(\n        validation_alias=AliasChoices(\n            provider_details, vendor_details\n        )\n    ),\n] = None\n```\n\nAdditional provider-specific details in a serializable format.\n\nThis allows storing selected vendor-specific data that isn't mapped to standard ModelResponse fields.\nFor OpenAI models, this may include 'logprobs', 'finish\\_reason', etc.\n\n#### provider\\_response\\_id `class-attribute` `instance-attribute`\n\n```\nprovider_response_id: Annotated[\n    str | None,\n    Field(\n        validation_alias=AliasChoices(\n            provider_response_id, vendor_id\n        )\n    ),\n] = None\n```\n\nrequest ID as specified by the model provider. This can be used to track the specific request to the model.\n\n#### finish\\_reason `class-attribute` `instance-attribute`\n\n```\nfinish_reason: FinishReason | None = None\n```\n\nReason the model finished generating the response, normalized to OpenTelemetry values.\n\n#### text `property`\n\n```\ntext: str | None\n```\n\nGet the text in the response.\n\n#### thinking `property`\n\n```\nthinking: str | None\n```\n\nGet the thinking in the response.\n\n#### files `property`\n\n```\nfiles: list[BinaryContent]\n```\n\nGet the files in the response.\n\n#### images `property`\n\n```\nimages: list[BinaryImage]\n```\n\nGet the images in the response.\n\n#### tool\\_calls `property`\n\n```\ntool_calls: list[ToolCallPart]\n```\n\nGet the tool calls in the response.\n\n#### builtin\\_tool\\_calls `property`\n\n```\nbuiltin_tool_calls: list[\n    tuple[BuiltinToolCallPart, BuiltinToolReturnPart]\n]\n```\n\nGet the builtin tool calls and results in the response.\n\n#### price `deprecated`\n\n```\nprice() -> PriceCalculation\n```\n\nDeprecated\n\n`price` is deprecated, use `cost` instead\n\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 1237 1238 1239 ``` | ``` @deprecated('`price` is deprecated, use `cost` instead') def price(self) -> genai_types.PriceCalculation:  # pragma: no cover     return self.cost() ``` |\n\n#### cost\n\n```\ncost() -> PriceCalculation\n```\n\nCalculate the cost of the usage.\n\nUses [`genai-prices`](https://github.com/pydantic/genai-prices).\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 ``` | ``` def cost(self) -> genai_types.PriceCalculation:     \"\"\"Calculate the cost of the usage.      Uses [`genai-prices`](https://github.com/pydantic/genai-prices).     \"\"\"     assert self.model_name, 'Model name is required to calculate price'     return calc_price(         self.usage,         self.model_name,         provider_id=self.provider_name,         genai_request_timestamp=self.timestamp,     ) ``` |\n\n#### otel\\_events\n\n```\notel_events(\n    settings: InstrumentationSettings,\n) -> list[Event]\n```\n\nReturn OpenTelemetry events for the response.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`", "url": "https://ai.pydantic.dev/messages/index.html#modelresponse-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "ModelResponse `dataclass`", "anchor": "modelresponse-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 ``` | ``` def otel_events(self, settings: InstrumentationSettings) -> list[Event]:     \"\"\"Return OpenTelemetry events for the response.\"\"\"     result: list[Event] = []      def new_event_body():         new_body: dict[str, Any] = {'role': 'assistant'}         ev = Event('gen_ai.assistant.message', body=new_body)         result.append(ev)         return new_body      body = new_event_body()     for part in self.parts:         if isinstance(part, ToolCallPart):             body.setdefault('tool_calls', []).append(                 {                     'id': part.tool_call_id,                     'type': 'function',                     'function': {                         'name': part.tool_name,                         **({'arguments': part.args} if settings.include_content else {}),                     },                 }             )         elif isinstance(part, TextPart | ThinkingPart):             kind = part.part_kind             body.setdefault('content', []).append(                 {'kind': kind, **({'text': part.content} if settings.include_content else {})}             )         elif isinstance(part, FilePart):             body.setdefault('content', []).append(                 {                     'kind': 'binary',                     'media_type': part.content.media_type,                     **(                         {'binary_content': base64.b64encode(part.content.data).decode()}                         if settings.include_content and settings.include_binary_content                         else {}                     ),                 }             )      if content := body.get('content'):         text_content = content[0].get('text')         if content == [{'kind': 'text', 'text': text_content}]:             body['content'] = text_content      return result ``` |", "url": "https://ai.pydantic.dev/messages/index.html#modelresponse-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "ModelMessage `module-attribute`", "anchor": "modelmessage-module-attribute", "md_text": "```\nModelMessage = Annotated[\n    ModelRequest | ModelResponse, Discriminator(\"kind\")\n]\n```\n\nAny message sent to or returned by a model.", "url": "https://ai.pydantic.dev/messages/index.html#modelmessage-module-attribute", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "ModelMessagesTypeAdapter `module-attribute`", "anchor": "modelmessagestypeadapter-module-attribute", "md_text": "```\nModelMessagesTypeAdapter = TypeAdapter(\n    list[ModelMessage],\n    config=ConfigDict(\n        defer_build=True,\n        ser_json_bytes=\"base64\",\n        val_json_bytes=\"base64\",\n    ),\n)\n```\n\nPydantic [`TypeAdapter`](https://docs.pydantic.dev/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter) for (de)serializing messages.", "url": "https://ai.pydantic.dev/messages/index.html#modelmessagestypeadapter-module-attribute", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "TextPartDelta `dataclass`", "anchor": "textpartdelta-dataclass", "md_text": "A partial update (delta) for a `TextPart` to append new text content.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 ``` | ``` @dataclass(repr=False) class TextPartDelta:     \"\"\"A partial update (delta) for a `TextPart` to append new text content.\"\"\"      content_delta: str     \"\"\"The incremental text content to add to the existing `TextPart` content.\"\"\"      _: KW_ONLY      part_delta_kind: Literal['text'] = 'text'     \"\"\"Part delta type identifier, used as a discriminator.\"\"\"      def apply(self, part: ModelResponsePart) -> TextPart:         \"\"\"Apply this text delta to an existing `TextPart`.          Args:             part: The existing model response part, which must be a `TextPart`.          Returns:             A new `TextPart` with updated text content.          Raises:             ValueError: If `part` is not a `TextPart`.         \"\"\"         if not isinstance(part, TextPart):             raise ValueError('Cannot apply TextPartDeltas to non-TextParts')  # pragma: no cover         return replace(part, content=part.content + self.content_delta)      __repr__ = _utils.dataclasses_no_defaults_repr ``` |\n\n#### content\\_delta `instance-attribute`\n\n```\ncontent_delta: str\n```\n\nThe incremental text content to add to the existing `TextPart` content.\n\n#### part\\_delta\\_kind `class-attribute` `instance-attribute`\n\n```\npart_delta_kind: Literal['text'] = 'text'\n```\n\nPart delta type identifier, used as a discriminator.\n\n#### apply\n\n```\napply(part: ModelResponsePart) -> TextPart\n```\n\nApply this text delta to an existing `TextPart`.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `part` | `ModelResponsePart` | The existing model response part, which must be a `TextPart`. | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `TextPart` | A new `TextPart` with updated text content. |\n\nRaises:\n\n| Type | Description |\n| --- | --- |\n| `ValueError` | If `part` is not a `TextPart`. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 ``` | ``` def apply(self, part: ModelResponsePart) -> TextPart:     \"\"\"Apply this text delta to an existing `TextPart`.      Args:         part: The existing model response part, which must be a `TextPart`.      Returns:         A new `TextPart` with updated text content.      Raises:         ValueError: If `part` is not a `TextPart`.     \"\"\"     if not isinstance(part, TextPart):         raise ValueError('Cannot apply TextPartDeltas to non-TextParts')  # pragma: no cover     return replace(part, content=part.content + self.content_delta) ``` |", "url": "https://ai.pydantic.dev/messages/index.html#textpartdelta-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "ThinkingPartDelta `dataclass`", "anchor": "thinkingpartdelta-dataclass", "md_text": "A partial update (delta) for a `ThinkingPart` to append new thinking content.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 ``` | ``` @dataclass(repr=False, kw_only=True) class ThinkingPartDelta:     \"\"\"A partial update (delta) for a `ThinkingPart` to append new thinking content.\"\"\"      content_delta: str | None = None     \"\"\"The incremental thinking content to add to the existing `ThinkingPart` content.\"\"\"      signature_delta: str | None = None     \"\"\"Optional signature delta.      Note this is never treated as a delta — it can replace None.     \"\"\"      provider_name: str | None = None     \"\"\"Optional provider name for the thinking part.      Signatures are only sent back to the same provider.     \"\"\"      part_delta_kind: Literal['thinking'] = 'thinking'     \"\"\"Part delta type identifier, used as a discriminator.\"\"\"      @overload     def apply(self, part: ModelResponsePart) -> ThinkingPart: ...      @overload     def apply(self, part: ModelResponsePart | ThinkingPartDelta) -> ThinkingPart | ThinkingPartDelta: ...      def apply(self, part: ModelResponsePart | ThinkingPartDelta) -> ThinkingPart | ThinkingPartDelta:         \"\"\"Apply this thinking delta to an existing `ThinkingPart`.          Args:             part: The existing model response part, which must be a `ThinkingPart`.          Returns:             A new `ThinkingPart` with updated thinking content.          Raises:             ValueError: If `part` is not a `ThinkingPart`.         \"\"\"         if isinstance(part, ThinkingPart):             new_content = part.content + self.content_delta if self.content_delta else part.content             new_signature = self.signature_delta if self.signature_delta is not None else part.signature             new_provider_name = self.provider_name if self.provider_name is not None else part.provider_name             return replace(part, content=new_content, signature=new_signature, provider_name=new_provider_name)         elif isinstance(part, ThinkingPartDelta):             if self.content_delta is None and self.signature_delta is None:                 raise ValueError('Cannot apply ThinkingPartDelta with no content or signature')             if self.content_delta is not None:                 part = replace(part, content_delta=(part.content_delta or '') + self.content_delta)             if self.signature_delta is not None:                 part = replace(part, signature_delta=self.signature_delta)             if self.provider_name is not None:                 part = replace(part, provider_name=self.provider_name)             return part         raise ValueError(  # pragma: no cover             f'Cannot apply ThinkingPartDeltas to non-ThinkingParts or non-ThinkingPartDeltas ({part=}, {self=})'         )      __repr__ = _utils.dataclasses_no_defaults_repr ``` |\n\n#### content\\_delta `class-attribute` `instance-attribute`\n\n```\ncontent_delta: str | None = None\n```\n\nThe incremental thinking content to add to the existing `ThinkingPart` content.\n\n#### signature\\_delta `class-attribute` `instance-attribute`\n\n```\nsignature_delta: str | None = None\n```\n\nOptional signature delta.\n\nNote this is never treated as a delta — it can replace None.\n\n#### provider\\_name `class-attribute` `instance-attribute`\n\n```\nprovider_name: str | None = None\n```\n\nOptional provider name for the thinking part.\n\nSignatures are only sent back to the same provider.\n\n#### part\\_delta\\_kind `class-attribute` `instance-attribute`\n\n```\npart_delta_kind: Literal['thinking'] = 'thinking'\n```\n\nPart delta type identifier, used as a discriminator.\n\n#### apply\n\n```\napply(part: ModelResponsePart) -> ThinkingPart\n\napply(\n    part: ModelResponsePart | ThinkingPartDelta,\n) -> ThinkingPart | ThinkingPartDelta\n\napply(\n    part: ModelResponsePart | ThinkingPartDelta,\n) -> ThinkingPart | ThinkingPartDelta\n```\n\nApply this thinking delta to an existing `ThinkingPart`.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `part` | `ModelResponsePart | ThinkingPartDelta` | The existing model response part, which must be a `ThinkingPart`. | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `ThinkingPart | ThinkingPartDelta` | A new `ThinkingPart` with updated thinking content. |\n\nRaises:\n\n| Type | Description |\n| --- | --- |\n| `ValueError` | If `part` is not a `ThinkingPart`. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`", "url": "https://ai.pydantic.dev/messages/index.html#thinkingpartdelta-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "ThinkingPartDelta `dataclass`", "anchor": "thinkingpartdelta-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 ``` | ``` def apply(self, part: ModelResponsePart | ThinkingPartDelta) -> ThinkingPart | ThinkingPartDelta:     \"\"\"Apply this thinking delta to an existing `ThinkingPart`.      Args:         part: The existing model response part, which must be a `ThinkingPart`.      Returns:         A new `ThinkingPart` with updated thinking content.      Raises:         ValueError: If `part` is not a `ThinkingPart`.     \"\"\"     if isinstance(part, ThinkingPart):         new_content = part.content + self.content_delta if self.content_delta else part.content         new_signature = self.signature_delta if self.signature_delta is not None else part.signature         new_provider_name = self.provider_name if self.provider_name is not None else part.provider_name         return replace(part, content=new_content, signature=new_signature, provider_name=new_provider_name)     elif isinstance(part, ThinkingPartDelta):         if self.content_delta is None and self.signature_delta is None:             raise ValueError('Cannot apply ThinkingPartDelta with no content or signature')         if self.content_delta is not None:             part = replace(part, content_delta=(part.content_delta or '') + self.content_delta)         if self.signature_delta is not None:             part = replace(part, signature_delta=self.signature_delta)         if self.provider_name is not None:             part = replace(part, provider_name=self.provider_name)         return part     raise ValueError(  # pragma: no cover         f'Cannot apply ThinkingPartDeltas to non-ThinkingParts or non-ThinkingPartDeltas ({part=}, {self=})'     ) ``` |", "url": "https://ai.pydantic.dev/messages/index.html#thinkingpartdelta-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "ToolCallPartDelta `dataclass`", "anchor": "toolcallpartdelta-dataclass", "md_text": "A partial update (delta) for a `ToolCallPart` to modify tool name, arguments, or tool call ID.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`", "url": "https://ai.pydantic.dev/messages/index.html#toolcallpartdelta-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "ToolCallPartDelta `dataclass`", "anchor": "toolcallpartdelta-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 ``` | ``` @dataclass(repr=False, kw_only=True) class ToolCallPartDelta:     \"\"\"A partial update (delta) for a `ToolCallPart` to modify tool name, arguments, or tool call ID.\"\"\"      tool_name_delta: str | None = None     \"\"\"Incremental text to add to the existing tool name, if any.\"\"\"      args_delta: str | dict[str, Any] | None = None     \"\"\"Incremental data to add to the tool arguments.      If this is a string, it will be appended to existing JSON arguments.     If this is a dict, it will be merged with existing dict arguments.     \"\"\"      tool_call_id: str | None = None     \"\"\"Optional tool call identifier, this is used by some models including OpenAI.      Note this is never treated as a delta — it can replace None, but otherwise if a     non-matching value is provided an error will be raised.\"\"\"      part_delta_kind: Literal['tool_call'] = 'tool_call'     \"\"\"Part delta type identifier, used as a discriminator.\"\"\"      def as_part(self) -> ToolCallPart | None:         \"\"\"Convert this delta to a fully formed `ToolCallPart` if possible, otherwise return `None`.          Returns:             A `ToolCallPart` if `tool_name_delta` is set, otherwise `None`.         \"\"\"         if self.tool_name_delta is None:             return None          return ToolCallPart(self.tool_name_delta, self.args_delta, self.tool_call_id or _generate_tool_call_id())      @overload     def apply(self, part: ModelResponsePart) -> ToolCallPart | BuiltinToolCallPart: ...      @overload     def apply(         self, part: ModelResponsePart | ToolCallPartDelta     ) -> ToolCallPart | BuiltinToolCallPart | ToolCallPartDelta: ...      def apply(         self, part: ModelResponsePart | ToolCallPartDelta     ) -> ToolCallPart | BuiltinToolCallPart | ToolCallPartDelta:         \"\"\"Apply this delta to a part or delta, returning a new part or delta with the changes applied.          Args:             part: The existing model response part or delta to update.          Returns:             Either a new `ToolCallPart` or `BuiltinToolCallPart`, or an updated `ToolCallPartDelta`.          Raises:             ValueError: If `part` is neither a `ToolCallPart`, `BuiltinToolCallPart`, nor a `ToolCallPartDelta`.             UnexpectedModelBehavior: If applying JSON deltas to dict arguments or vice versa.         \"\"\"         if isinstance(part, ToolCallPart | BuiltinToolCallPart):             return self._apply_to_part(part)          if isinstance(part, ToolCallPartDelta):             return self._apply_to_delta(part)          raise ValueError(  # pragma: no cover             f'Can only apply ToolCallPartDeltas to ToolCallParts, BuiltinToolCallParts, or ToolCallPartDeltas, not {part}'         )      def _apply_to_delta(self, delta: ToolCallPartDelta) -> ToolCallPart | BuiltinToolCallPart | ToolCallPartDelta:         \"\"\"Internal helper to apply this delta to another delta.\"\"\"         if self.tool_name_delta:             # Append incremental text to the existing tool_name_delta             updated_tool_name_delta = (delta.tool_name_delta or '') + self.tool_name_delta             delta = replace(delta, tool_name_delta=updated_tool_name_delta)          if isinstance(self.args_delta, str):             if isinstance(delta.args_delta, dict):                 raise UnexpectedModelBehavior(                     f'Cannot apply JSON deltas to non-JSON tool arguments ({delta=}, {self=})'                 )             updated_args_delta = (delta.args_delta or '') + self.args_delta             delta = replace(delta, args_delta=updated_args_delta)         elif isinstance(self.args_delta, dict):             if isinstance(delta.args_delta, str):                 raise UnexpectedModelBehavior(                     f'Cannot apply dict deltas to non-dict tool arguments ({delta=}, {self=})'                 )             updated_args_delta = {**(delta.args_delta or {}), **self.args_delta}             delta = replace(delta, args_delta=updated_args_delta)          if self.tool_call_id:             delta = replace(delta, tool_call_id=self.tool_call_id)          # If we now have enough data to create a full ToolCallPart, do so         if delta.tool_name_delta is not None:             return ToolCallPart(delta.tool_name_delta, delta.args_delta, delta.tool_call_id or _generate_tool_call_id())          return delta      def _apply_to_part(self, part: ToolCallPart | BuiltinToolCallPart) -> ToolCallPart | BuiltinToolCallPart:         \"\"\"Internal helper to apply this delta directly to a `ToolCallPart` or `BuiltinToolCallPart`.\"\"\"         if self.tool_name_delta:             # Append incremental text to the existing tool_name             tool_name = part.tool_name + self.tool_name_delta             part = replace(part, tool_name=tool_name)          if isinstance(self.args_delta, str):             if isinstance(part.args, dict):                 raise UnexpectedModelBehavior(f'Cannot apply JSON deltas to non-JSON tool arguments ({part=}, {self=})')             updated_json = (part.args or '') + self.args_delta             part = replace(part, args=updated_json)         elif isinstance(self.args_delta, dict):             if isinstance(part.args, str):                 raise UnexpectedModelBehavior(f'Cannot apply dict deltas to non-dict tool arguments ({part=}, {self=})')             updated_dict = {**(part.args or {}), **self.args_delta}             part = replace(part, args=updated_dict)          if self.tool_call_id:             part = replace(part, tool_call_id=self.tool_call_id)         return part      __repr__ = _utils.dataclasses_no_defaults_repr ``` |", "url": "https://ai.pydantic.dev/messages/index.html#toolcallpartdelta-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "ToolCallPartDelta `dataclass`", "anchor": "toolcallpartdelta-dataclass", "md_text": "#### tool\\_name\\_delta `class-attribute` `instance-attribute`\n\n```\ntool_name_delta: str | None = None\n```\n\nIncremental text to add to the existing tool name, if any.\n\n#### args\\_delta `class-attribute` `instance-attribute`\n\n```\nargs_delta: str | dict[str, Any] | None = None\n```\n\nIncremental data to add to the tool arguments.\n\nIf this is a string, it will be appended to existing JSON arguments.\nIf this is a dict, it will be merged with existing dict arguments.\n\n#### tool\\_call\\_id `class-attribute` `instance-attribute`\n\n```\ntool_call_id: str | None = None\n```\n\nOptional tool call identifier, this is used by some models including OpenAI.\n\nNote this is never treated as a delta — it can replace None, but otherwise if a\nnon-matching value is provided an error will be raised.\n\n#### part\\_delta\\_kind `class-attribute` `instance-attribute`\n\n```\npart_delta_kind: Literal['tool_call'] = 'tool_call'\n```\n\nPart delta type identifier, used as a discriminator.\n\n#### as\\_part\n\n```\nas_part() -> ToolCallPart | None\n```\n\nConvert this delta to a fully formed `ToolCallPart` if possible, otherwise return `None`.\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `ToolCallPart | None` | A `ToolCallPart` if `tool_name_delta` is set, otherwise `None`. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 ``` | ``` def as_part(self) -> ToolCallPart | None:     \"\"\"Convert this delta to a fully formed `ToolCallPart` if possible, otherwise return `None`.      Returns:         A `ToolCallPart` if `tool_name_delta` is set, otherwise `None`.     \"\"\"     if self.tool_name_delta is None:         return None      return ToolCallPart(self.tool_name_delta, self.args_delta, self.tool_call_id or _generate_tool_call_id()) ``` |\n\n#### apply\n\n```\napply(\n    part: ModelResponsePart,\n) -> ToolCallPart | BuiltinToolCallPart\n\napply(\n    part: ModelResponsePart | ToolCallPartDelta,\n) -> ToolCallPart | BuiltinToolCallPart | ToolCallPartDelta\n\napply(\n    part: ModelResponsePart | ToolCallPartDelta,\n) -> ToolCallPart | BuiltinToolCallPart | ToolCallPartDelta\n```\n\nApply this delta to a part or delta, returning a new part or delta with the changes applied.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `part` | `ModelResponsePart | ToolCallPartDelta` | The existing model response part or delta to update. | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `ToolCallPart | BuiltinToolCallPart | ToolCallPartDelta` | Either a new `ToolCallPart` or `BuiltinToolCallPart`, or an updated `ToolCallPartDelta`. |\n\nRaises:\n\n| Type | Description |\n| --- | --- |\n| `ValueError` | If `part` is neither a `ToolCallPart`, `BuiltinToolCallPart`, nor a `ToolCallPartDelta`. |\n| `UnexpectedModelBehavior` | If applying JSON deltas to dict arguments or vice versa. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 ``` | ``` def apply(     self, part: ModelResponsePart | ToolCallPartDelta ) -> ToolCallPart | BuiltinToolCallPart | ToolCallPartDelta:     \"\"\"Apply this delta to a part or delta, returning a new part or delta with the changes applied.      Args:         part: The existing model response part or delta to update.      Returns:         Either a new `ToolCallPart` or `BuiltinToolCallPart`, or an updated `ToolCallPartDelta`.      Raises:         ValueError: If `part` is neither a `ToolCallPart`, `BuiltinToolCallPart`, nor a `ToolCallPartDelta`.         UnexpectedModelBehavior: If applying JSON deltas to dict arguments or vice versa.     \"\"\"     if isinstance(part, ToolCallPart | BuiltinToolCallPart):         return self._apply_to_part(part)      if isinstance(part, ToolCallPartDelta):         return self._apply_to_delta(part)      raise ValueError(  # pragma: no cover         f'Can only apply ToolCallPartDeltas to ToolCallParts, BuiltinToolCallParts, or ToolCallPartDeltas, not {part}'     ) ``` |", "url": "https://ai.pydantic.dev/messages/index.html#toolcallpartdelta-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "ModelResponsePartDelta `module-attribute`", "anchor": "modelresponsepartdelta-module-attribute", "md_text": "```\nModelResponsePartDelta = Annotated[\n    TextPartDelta | ThinkingPartDelta | ToolCallPartDelta,\n    Discriminator(\"part_delta_kind\"),\n]\n```\n\nA partial update (delta) for any model response part.", "url": "https://ai.pydantic.dev/messages/index.html#modelresponsepartdelta-module-attribute", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "PartStartEvent `dataclass`", "anchor": "partstartevent-dataclass", "md_text": "An event indicating that a new part has started.\n\nIf multiple `PartStartEvent`s are received with the same index,\nthe new one should fully replace the old one.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 ``` | ``` @dataclass(repr=False, kw_only=True) class PartStartEvent:     \"\"\"An event indicating that a new part has started.      If multiple `PartStartEvent`s are received with the same index,     the new one should fully replace the old one.     \"\"\"      index: int     \"\"\"The index of the part within the overall response parts list.\"\"\"      part: ModelResponsePart     \"\"\"The newly started `ModelResponsePart`.\"\"\"      event_kind: Literal['part_start'] = 'part_start'     \"\"\"Event type identifier, used as a discriminator.\"\"\"      __repr__ = _utils.dataclasses_no_defaults_repr ``` |\n\n#### index `instance-attribute`\n\n```\nindex: int\n```\n\nThe index of the part within the overall response parts list.\n\n#### part `instance-attribute`\n\n```\npart: ModelResponsePart\n```\n\nThe newly started `ModelResponsePart`.\n\n#### event\\_kind `class-attribute` `instance-attribute`\n\n```\nevent_kind: Literal['part_start'] = 'part_start'\n```\n\nEvent type identifier, used as a discriminator.", "url": "https://ai.pydantic.dev/messages/index.html#partstartevent-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "PartDeltaEvent `dataclass`", "anchor": "partdeltaevent-dataclass", "md_text": "An event indicating a delta update for an existing part.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 ``` | ``` @dataclass(repr=False, kw_only=True) class PartDeltaEvent:     \"\"\"An event indicating a delta update for an existing part.\"\"\"      index: int     \"\"\"The index of the part within the overall response parts list.\"\"\"      delta: ModelResponsePartDelta     \"\"\"The delta to apply to the specified part.\"\"\"      event_kind: Literal['part_delta'] = 'part_delta'     \"\"\"Event type identifier, used as a discriminator.\"\"\"      __repr__ = _utils.dataclasses_no_defaults_repr ``` |\n\n#### index `instance-attribute`\n\n```\nindex: int\n```\n\nThe index of the part within the overall response parts list.\n\n#### delta `instance-attribute`\n\n```\ndelta: ModelResponsePartDelta\n```\n\nThe delta to apply to the specified part.\n\n#### event\\_kind `class-attribute` `instance-attribute`\n\n```\nevent_kind: Literal['part_delta'] = 'part_delta'\n```\n\nEvent type identifier, used as a discriminator.", "url": "https://ai.pydantic.dev/messages/index.html#partdeltaevent-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "FinalResultEvent `dataclass`", "anchor": "finalresultevent-dataclass", "md_text": "An event indicating the response to the current model request matches the output schema and will produce a result.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 ``` | ``` @dataclass(repr=False, kw_only=True) class FinalResultEvent:     \"\"\"An event indicating the response to the current model request matches the output schema and will produce a result.\"\"\"      tool_name: str | None     \"\"\"The name of the output tool that was called. `None` if the result is from text content and not from a tool.\"\"\"     tool_call_id: str | None     \"\"\"The tool call ID, if any, that this result is associated with.\"\"\"     event_kind: Literal['final_result'] = 'final_result'     \"\"\"Event type identifier, used as a discriminator.\"\"\"      __repr__ = _utils.dataclasses_no_defaults_repr ``` |\n\n#### tool\\_name `instance-attribute`\n\n```\ntool_name: str | None\n```\n\nThe name of the output tool that was called. `None` if the result is from text content and not from a tool.\n\n#### tool\\_call\\_id `instance-attribute`\n\n```\ntool_call_id: str | None\n```\n\nThe tool call ID, if any, that this result is associated with.\n\n#### event\\_kind `class-attribute` `instance-attribute`\n\n```\nevent_kind: Literal['final_result'] = 'final_result'\n```\n\nEvent type identifier, used as a discriminator.", "url": "https://ai.pydantic.dev/messages/index.html#finalresultevent-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "ModelResponseStreamEvent `module-attribute`", "anchor": "modelresponsestreamevent-module-attribute", "md_text": "```\nModelResponseStreamEvent = Annotated[\n    PartStartEvent | PartDeltaEvent | FinalResultEvent,\n    Discriminator(\"event_kind\"),\n]\n```\n\nAn event in the model response stream, starting a new part, applying a delta to an existing one, or indicating the final result.", "url": "https://ai.pydantic.dev/messages/index.html#modelresponsestreamevent-module-attribute", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "FunctionToolCallEvent `dataclass`", "anchor": "functiontoolcallevent-dataclass", "md_text": "An event indicating the start to a call to a function tool.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 1671 1672 1673 1674 1675 1676 1677 1678 1679 1680 ``` | ``` @dataclass(repr=False) class FunctionToolCallEvent:     \"\"\"An event indicating the start to a call to a function tool.\"\"\"      part: ToolCallPart     \"\"\"The (function) tool call to make.\"\"\"      _: KW_ONLY      event_kind: Literal['function_tool_call'] = 'function_tool_call'     \"\"\"Event type identifier, used as a discriminator.\"\"\"      @property     def tool_call_id(self) -> str:         \"\"\"An ID used for matching details about the call to its result.\"\"\"         return self.part.tool_call_id      @property     @deprecated('`call_id` is deprecated, use `tool_call_id` instead.')     def call_id(self) -> str:         \"\"\"An ID used for matching details about the call to its result.\"\"\"         return self.part.tool_call_id  # pragma: no cover      __repr__ = _utils.dataclasses_no_defaults_repr ``` |\n\n#### part `instance-attribute`\n\n```\npart: ToolCallPart\n```\n\nThe (function) tool call to make.\n\n#### event\\_kind `class-attribute` `instance-attribute`\n\n```\nevent_kind: Literal[\"function_tool_call\"] = (\n    \"function_tool_call\"\n)\n```\n\nEvent type identifier, used as a discriminator.\n\n#### tool\\_call\\_id `property`\n\n```\ntool_call_id: str\n```\n\nAn ID used for matching details about the call to its result.\n\n#### call\\_id `property`\n\n```\ncall_id: str\n```\n\nAn ID used for matching details about the call to its result.", "url": "https://ai.pydantic.dev/messages/index.html#functiontoolcallevent-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "FunctionToolResultEvent `dataclass`", "anchor": "functiontoolresultevent-dataclass", "md_text": "An event indicating the result of a function tool call.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 1683 1684 1685 1686 1687 1688 1689 1690 1691 1692 1693 1694 1695 1696 1697 1698 1699 1700 1701 1702 1703 ``` | ``` @dataclass(repr=False) class FunctionToolResultEvent:     \"\"\"An event indicating the result of a function tool call.\"\"\"      result: ToolReturnPart | RetryPromptPart     \"\"\"The result of the call to the function tool.\"\"\"      _: KW_ONLY      content: str | Sequence[UserContent] | None = None     \"\"\"The content that will be sent to the model as a UserPromptPart following the result.\"\"\"      event_kind: Literal['function_tool_result'] = 'function_tool_result'     \"\"\"Event type identifier, used as a discriminator.\"\"\"      @property     def tool_call_id(self) -> str:         \"\"\"An ID used to match the result to its original call.\"\"\"         return self.result.tool_call_id      __repr__ = _utils.dataclasses_no_defaults_repr ``` |\n\n#### result `instance-attribute`\n\n```\nresult: ToolReturnPart | RetryPromptPart\n```\n\nThe result of the call to the function tool.\n\n#### content `class-attribute` `instance-attribute`\n\n```\ncontent: str | Sequence[UserContent] | None = None\n```\n\nThe content that will be sent to the model as a UserPromptPart following the result.\n\n#### event\\_kind `class-attribute` `instance-attribute`\n\n```\nevent_kind: Literal[\"function_tool_result\"] = (\n    \"function_tool_result\"\n)\n```\n\nEvent type identifier, used as a discriminator.\n\n#### tool\\_call\\_id `property`\n\n```\ntool_call_id: str\n```\n\nAn ID used to match the result to its original call.", "url": "https://ai.pydantic.dev/messages/index.html#functiontoolresultevent-dataclass", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "BuiltinToolCallEvent `dataclass` `deprecated`", "anchor": "builtintoolcallevent-dataclass-deprecated", "md_text": "Deprecated\n\n`BuiltinToolCallEvent` is deprecated, look for `PartStartEvent` and `PartDeltaEvent` with `BuiltinToolCallPart` instead.\n\nAn event indicating the start to a call to a built-in tool.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 1706 1707 1708 1709 1710 1711 1712 1713 1714 1715 1716 1717 1718 1719 ``` | ``` @deprecated(     '`BuiltinToolCallEvent` is deprecated, look for `PartStartEvent` and `PartDeltaEvent` with `BuiltinToolCallPart` instead.' ) @dataclass(repr=False) class BuiltinToolCallEvent:     \"\"\"An event indicating the start to a call to a built-in tool.\"\"\"      part: BuiltinToolCallPart     \"\"\"The built-in tool call to make.\"\"\"      _: KW_ONLY      event_kind: Literal['builtin_tool_call'] = 'builtin_tool_call'     \"\"\"Event type identifier, used as a discriminator.\"\"\" ``` |\n\n#### part `instance-attribute`\n\n```\npart: BuiltinToolCallPart\n```\n\nThe built-in tool call to make.\n\n#### event\\_kind `class-attribute` `instance-attribute`\n\n```\nevent_kind: Literal[\"builtin_tool_call\"] = (\n    \"builtin_tool_call\"\n)\n```\n\nEvent type identifier, used as a discriminator.", "url": "https://ai.pydantic.dev/messages/index.html#builtintoolcallevent-dataclass-deprecated", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "BuiltinToolResultEvent `dataclass` `deprecated`", "anchor": "builtintoolresultevent-dataclass-deprecated", "md_text": "Deprecated\n\n`BuiltinToolResultEvent` is deprecated, look for `PartStartEvent` and `PartDeltaEvent` with `BuiltinToolReturnPart` instead.\n\nAn event indicating the result of a built-in tool call.\n\nSource code in `pydantic_ai_slim/pydantic_ai/messages.py`\n\n|  |  |\n| --- | --- |\n| ``` 1722 1723 1724 1725 1726 1727 1728 1729 1730 1731 1732 1733 1734 1735 ``` | ``` @deprecated(     '`BuiltinToolResultEvent` is deprecated, look for `PartStartEvent` and `PartDeltaEvent` with `BuiltinToolReturnPart` instead.' ) @dataclass(repr=False) class BuiltinToolResultEvent:     \"\"\"An event indicating the result of a built-in tool call.\"\"\"      result: BuiltinToolReturnPart     \"\"\"The result of the call to the built-in tool.\"\"\"      _: KW_ONLY      event_kind: Literal['builtin_tool_result'] = 'builtin_tool_result'     \"\"\"Event type identifier, used as a discriminator.\"\"\" ``` |\n\n#### result `instance-attribute`\n\n```\nresult: BuiltinToolReturnPart\n```\n\nThe result of the call to the built-in tool.\n\n#### event\\_kind `class-attribute` `instance-attribute`\n\n```\nevent_kind: Literal[\"builtin_tool_result\"] = (\n    \"builtin_tool_result\"\n)\n```\n\nEvent type identifier, used as a discriminator.", "url": "https://ai.pydantic.dev/messages/index.html#builtintoolresultevent-dataclass-deprecated", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "HandleResponseEvent `module-attribute`", "anchor": "handleresponseevent-module-attribute", "md_text": "```\nHandleResponseEvent = Annotated[\n    FunctionToolCallEvent\n    | FunctionToolResultEvent\n    | BuiltinToolCallEvent\n    | BuiltinToolResultEvent,\n    Discriminator(\"event_kind\"),\n]\n```\n\nAn event yielded when handling a model response, indicating tool calls and results.", "url": "https://ai.pydantic.dev/messages/index.html#handleresponseevent-module-attribute", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "AgentStreamEvent `module-attribute`", "anchor": "agentstreamevent-module-attribute", "md_text": "```\nAgentStreamEvent = Annotated[\n    ModelResponseStreamEvent | HandleResponseEvent,\n    Discriminator(\"event_kind\"),\n]\n```\n\nAn event in the agent stream: model response stream events and response-handling events.", "url": "https://ai.pydantic.dev/messages/index.html#agentstreamevent-module-attribute", "page": "messages/index.html", "source_site": "pydantic_ai"}
{"title": "OutputDataT `module-attribute`", "anchor": "outputdatat-module-attribute", "md_text": "```\nOutputDataT = TypeVar(\n    \"OutputDataT\", default=str, covariant=True\n)\n```\n\nCovariant type variable for the output data type of a run.", "url": "https://ai.pydantic.dev/output/index.html#outputdatat-module-attribute", "page": "output/index.html", "source_site": "pydantic_ai"}
{"title": "ToolOutput `dataclass`", "anchor": "tooloutput-dataclass", "md_text": "Bases: `Generic[OutputDataT]`\n\nMarker class to use a tool for output and optionally customize the tool.\n\nExample:\n\ntool\\_output.py\n\n```\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, ToolOutput\n\n\nclass Fruit(BaseModel):\n    name: str\n    color: str\n\n\nclass Vehicle(BaseModel):\n    name: str\n    wheels: int\n\n\nagent = Agent(\n    'openai:gpt-4o',\n    output_type=[\n        ToolOutput(Fruit, name='return_fruit'),\n        ToolOutput(Vehicle, name='return_vehicle'),\n    ],\n)\nresult = agent.run_sync('What is a banana?')\nprint(repr(result.output))\n#> Fruit(name='banana', color='yellow')\n```\n\nSource code in `pydantic_ai_slim/pydantic_ai/output.py`\n\n|  |  |\n| --- | --- |\n| ```  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 ``` | ``` @dataclass(init=False) class ToolOutput(Generic[OutputDataT]):     \"\"\"Marker class to use a tool for output and optionally customize the tool.      Example:     ```python {title=\"tool_output.py\"}     from pydantic import BaseModel      from pydantic_ai import Agent, ToolOutput       class Fruit(BaseModel):         name: str         color: str       class Vehicle(BaseModel):         name: str         wheels: int       agent = Agent(         'openai:gpt-4o',         output_type=[             ToolOutput(Fruit, name='return_fruit'),             ToolOutput(Vehicle, name='return_vehicle'),         ],     )     result = agent.run_sync('What is a banana?')     print(repr(result.output))     #> Fruit(name='banana', color='yellow')     ```     \"\"\"      output: OutputTypeOrFunction[OutputDataT]     \"\"\"An output type or function.\"\"\"     name: str | None     \"\"\"The name of the tool that will be passed to the model. If not specified and only one output is provided, `final_result` will be used. If multiple outputs are provided, the name of the output type or function will be added to the tool name.\"\"\"     description: str | None     \"\"\"The description of the tool that will be passed to the model. If not specified, the docstring of the output type or function will be used.\"\"\"     max_retries: int | None     \"\"\"The maximum number of retries for the tool.\"\"\"     strict: bool | None     \"\"\"Whether to use strict mode for the tool.\"\"\"      def __init__(         self,         type_: OutputTypeOrFunction[OutputDataT],         *,         name: str | None = None,         description: str | None = None,         max_retries: int | None = None,         strict: bool | None = None,     ):         self.output = type_         self.name = name         self.description = description         self.max_retries = max_retries         self.strict = strict ``` |\n\n#### output `instance-attribute`\n\n```\noutput: OutputTypeOrFunction[OutputDataT] = type_\n```\n\nAn output type or function.\n\n#### name `instance-attribute`\n\n```\nname: str | None = name\n```\n\nThe name of the tool that will be passed to the model. If not specified and only one output is provided, `final_result` will be used. If multiple outputs are provided, the name of the output type or function will be added to the tool name.\n\n#### description `instance-attribute`\n\n```\ndescription: str | None = description\n```\n\nThe description of the tool that will be passed to the model. If not specified, the docstring of the output type or function will be used.\n\n#### max\\_retries `instance-attribute`\n\n```\nmax_retries: int | None = max_retries\n```\n\nThe maximum number of retries for the tool.\n\n#### strict `instance-attribute`\n\n```\nstrict: bool | None = strict\n```\n\nWhether to use strict mode for the tool.", "url": "https://ai.pydantic.dev/output/index.html#tooloutput-dataclass", "page": "output/index.html", "source_site": "pydantic_ai"}
{"title": "NativeOutput `dataclass`", "anchor": "nativeoutput-dataclass", "md_text": "Bases: `Generic[OutputDataT]`\n\nMarker class to use the model's native structured outputs functionality for outputs and optionally customize the name and description.\n\nExample:\n\nnative\\_output.py\n\n```\nfrom pydantic_ai import Agent, NativeOutput\n\nfrom tool_output import Fruit, Vehicle\n\nagent = Agent(\n    'openai:gpt-4o',\n    output_type=NativeOutput(\n        [Fruit, Vehicle],\n        name='Fruit or vehicle',\n        description='Return a fruit or vehicle.'\n    ),\n)\nresult = agent.run_sync('What is a Ford Explorer?')\nprint(repr(result.output))\n#> Vehicle(name='Ford Explorer', wheels=4)\n```\n\nSource code in `pydantic_ai_slim/pydantic_ai/output.py`\n\n|  |  |\n| --- | --- |\n| ``` 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 ``` | ``` @dataclass(init=False) class NativeOutput(Generic[OutputDataT]):     \"\"\"Marker class to use the model's native structured outputs functionality for outputs and optionally customize the name and description.      Example:     ```python {title=\"native_output.py\" requires=\"tool_output.py\"}     from pydantic_ai import Agent, NativeOutput      from tool_output import Fruit, Vehicle      agent = Agent(         'openai:gpt-4o',         output_type=NativeOutput(             [Fruit, Vehicle],             name='Fruit or vehicle',             description='Return a fruit or vehicle.'         ),     )     result = agent.run_sync('What is a Ford Explorer?')     print(repr(result.output))     #> Vehicle(name='Ford Explorer', wheels=4)     ```     \"\"\"      outputs: OutputTypeOrFunction[OutputDataT] | Sequence[OutputTypeOrFunction[OutputDataT]]     \"\"\"The output types or functions.\"\"\"     name: str | None     \"\"\"The name of the structured output that will be passed to the model. If not specified and only one output is provided, the name of the output type or function will be used.\"\"\"     description: str | None     \"\"\"The description of the structured output that will be passed to the model. If not specified and only one output is provided, the docstring of the output type or function will be used.\"\"\"     strict: bool | None     \"\"\"Whether to use strict mode for the output, if the model supports it.\"\"\"      def __init__(         self,         outputs: OutputTypeOrFunction[OutputDataT] | Sequence[OutputTypeOrFunction[OutputDataT]],         *,         name: str | None = None,         description: str | None = None,         strict: bool | None = None,     ):         self.outputs = outputs         self.name = name         self.description = description         self.strict = strict ``` |\n\n#### outputs `instance-attribute`\n\n```\noutputs: (\n    OutputTypeOrFunction[OutputDataT]\n    | Sequence[OutputTypeOrFunction[OutputDataT]]\n) = outputs\n```\n\nThe output types or functions.\n\n#### name `instance-attribute`\n\n```\nname: str | None = name\n```\n\nThe name of the structured output that will be passed to the model. If not specified and only one output is provided, the name of the output type or function will be used.\n\n#### description `instance-attribute`\n\n```\ndescription: str | None = description\n```\n\nThe description of the structured output that will be passed to the model. If not specified and only one output is provided, the docstring of the output type or function will be used.\n\n#### strict `instance-attribute`\n\n```\nstrict: bool | None = strict\n```\n\nWhether to use strict mode for the output, if the model supports it.", "url": "https://ai.pydantic.dev/output/index.html#nativeoutput-dataclass", "page": "output/index.html", "source_site": "pydantic_ai"}
{"title": "PromptedOutput `dataclass`", "anchor": "promptedoutput-dataclass", "md_text": "Bases: `Generic[OutputDataT]`\n\nMarker class to use a prompt to tell the model what to output and optionally customize the prompt.\n\nExample:\n\nprompted\\_output.py\n\n```\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, PromptedOutput\n\nfrom tool_output import Vehicle\n\n\nclass Device(BaseModel):\n    name: str\n    kind: str\n\n\nagent = Agent(\n    'openai:gpt-4o',\n    output_type=PromptedOutput(\n        [Vehicle, Device],\n        name='Vehicle or device',\n        description='Return a vehicle or device.'\n    ),\n)\nresult = agent.run_sync('What is a MacBook?')\nprint(repr(result.output))\n#> Device(name='MacBook', kind='laptop')\n\nagent = Agent(\n    'openai:gpt-4o',\n    output_type=PromptedOutput(\n        [Vehicle, Device],\n        template='Gimme some JSON: {schema}'\n    ),\n)\nresult = agent.run_sync('What is a Ford Explorer?')\nprint(repr(result.output))\n#> Vehicle(name='Ford Explorer', wheels=4)\n```\n\nSource code in `pydantic_ai_slim/pydantic_ai/output.py`\n\n|  |  |\n| --- | --- |\n| ``` 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 ``` | ``` @dataclass(init=False) class PromptedOutput(Generic[OutputDataT]):     \"\"\"Marker class to use a prompt to tell the model what to output and optionally customize the prompt.      Example:     ```python {title=\"prompted_output.py\" requires=\"tool_output.py\"}     from pydantic import BaseModel      from pydantic_ai import Agent, PromptedOutput      from tool_output import Vehicle       class Device(BaseModel):         name: str         kind: str       agent = Agent(         'openai:gpt-4o',         output_type=PromptedOutput(             [Vehicle, Device],             name='Vehicle or device',             description='Return a vehicle or device.'         ),     )     result = agent.run_sync('What is a MacBook?')     print(repr(result.output))     #> Device(name='MacBook', kind='laptop')      agent = Agent(         'openai:gpt-4o',         output_type=PromptedOutput(             [Vehicle, Device],             template='Gimme some JSON: {schema}'         ),     )     result = agent.run_sync('What is a Ford Explorer?')     print(repr(result.output))     #> Vehicle(name='Ford Explorer', wheels=4)     ```     \"\"\"      outputs: OutputTypeOrFunction[OutputDataT] | Sequence[OutputTypeOrFunction[OutputDataT]]     \"\"\"The output types or functions.\"\"\"     name: str | None     \"\"\"The name of the structured output that will be passed to the model. If not specified and only one output is provided, the name of the output type or function will be used.\"\"\"     description: str | None     \"\"\"The description that will be passed to the model. If not specified and only one output is provided, the docstring of the output type or function will be used.\"\"\"     template: str | None     \"\"\"Template for the prompt passed to the model.     The '{schema}' placeholder will be replaced with the output JSON schema.     If not specified, the default template specified on the model's profile will be used.     \"\"\"      def __init__(         self,         outputs: OutputTypeOrFunction[OutputDataT] | Sequence[OutputTypeOrFunction[OutputDataT]],         *,         name: str | None = None,         description: str | None = None,         template: str | None = None,     ):         self.outputs = outputs         self.name = name         self.description = description         self.template = template ``` |\n\n#### outputs `instance-attribute`\n\n```\noutputs: (\n    OutputTypeOrFunction[OutputDataT]\n    | Sequence[OutputTypeOrFunction[OutputDataT]]\n) = outputs\n```\n\nThe output types or functions.\n\n#### name `instance-attribute`\n\n```\nname: str | None = name\n```\n\nThe name of the structured output that will be passed to the model. If not specified and only one output is provided, the name of the output type or function will be used.\n\n#### description `instance-attribute`\n\n```\ndescription: str | None = description\n```\n\nThe description that will be passed to the model. If not specified and only one output is provided, the docstring of the output type or function will be used.\n\n#### template `instance-attribute`\n\n```\ntemplate: str | None = template\n```\n\nTemplate for the prompt passed to the model.\nThe '{schema}' placeholder will be replaced with the output JSON schema.\nIf not specified, the default template specified on the model's profile will be used.", "url": "https://ai.pydantic.dev/output/index.html#promptedoutput-dataclass", "page": "output/index.html", "source_site": "pydantic_ai"}
{"title": "TextOutput `dataclass`", "anchor": "textoutput-dataclass", "md_text": "Bases: `Generic[OutputDataT]`\n\nMarker class to use text output for an output function taking a string argument.\n\nExample:\n\n```\nfrom pydantic_ai import Agent, TextOutput\n\n\ndef split_into_words(text: str) -> list[str]:\n    return text.split()\n\n\nagent = Agent(\n    'openai:gpt-4o',\n    output_type=TextOutput(split_into_words),\n)\nresult = agent.run_sync('Who was Albert Einstein?')\nprint(result.output)\n#> ['Albert', 'Einstein', 'was', 'a', 'German-born', 'theoretical', 'physicist.']\n```\n\nSource code in `pydantic_ai_slim/pydantic_ai/output.py`\n\n|  |  |\n| --- | --- |\n| ``` 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 ``` | ``` @dataclass class TextOutput(Generic[OutputDataT]):     \"\"\"Marker class to use text output for an output function taking a string argument.      Example:     ```python     from pydantic_ai import Agent, TextOutput       def split_into_words(text: str) -> list[str]:         return text.split()       agent = Agent(         'openai:gpt-4o',         output_type=TextOutput(split_into_words),     )     result = agent.run_sync('Who was Albert Einstein?')     print(result.output)     #> ['Albert', 'Einstein', 'was', 'a', 'German-born', 'theoretical', 'physicist.']     ```     \"\"\"      output_function: TextOutputFunc[OutputDataT]     \"\"\"The function that will be called to process the model's plain text output. The function must take a single string argument.\"\"\" ``` |\n\n#### output\\_function `instance-attribute`\n\n```\noutput_function: TextOutputFunc[OutputDataT]\n```\n\nThe function that will be called to process the model's plain text output. The function must take a single string argument.", "url": "https://ai.pydantic.dev/output/index.html#textoutput-dataclass", "page": "output/index.html", "source_site": "pydantic_ai"}
{"title": "StructuredDict", "anchor": "structureddict", "md_text": "```\nStructuredDict(\n    json_schema: JsonSchemaValue,\n    name: str | None = None,\n    description: str | None = None,\n) -> type[JsonSchemaValue]\n```\n\nReturns a `dict[str, Any]` subclass with a JSON schema attached that will be used for structured output.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `json_schema` | `JsonSchemaValue` | A JSON schema of type `object` defining the structure of the dictionary content. | *required* |\n| `name` | `str | None` | Optional name of the structured output. If not provided, the `title` field of the JSON schema will be used if it's present. | `None` |\n| `description` | `str | None` | Optional description of the structured output. If not provided, the `description` field of the JSON schema will be used if it's present. | `None` |\n\nExample:\n\nstructured\\_dict.py\n\n```\nfrom pydantic_ai import Agent, StructuredDict\n\nschema = {\n    'type': 'object',\n    'properties': {\n        'name': {'type': 'string'},\n        'age': {'type': 'integer'}\n    },\n    'required': ['name', 'age']\n}\n\nagent = Agent('openai:gpt-4o', output_type=StructuredDict(schema))\nresult = agent.run_sync('Create a person')\nprint(result.output)\n#> {'name': 'John Doe', 'age': 30}\n```\n\nSource code in `pydantic_ai_slim/pydantic_ai/output.py`\n\n|  |  |\n| --- | --- |\n| ``` 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 ``` | ``` def StructuredDict(     json_schema: JsonSchemaValue, name: str | None = None, description: str | None = None ) -> type[JsonSchemaValue]:     \"\"\"Returns a `dict[str, Any]` subclass with a JSON schema attached that will be used for structured output.      Args:         json_schema: A JSON schema of type `object` defining the structure of the dictionary content.         name: Optional name of the structured output. If not provided, the `title` field of the JSON schema will be used if it's present.         description: Optional description of the structured output. If not provided, the `description` field of the JSON schema will be used if it's present.      Example:     ```python {title=\"structured_dict.py\"}     from pydantic_ai import Agent, StructuredDict      schema = {         'type': 'object',         'properties': {             'name': {'type': 'string'},             'age': {'type': 'integer'}         },         'required': ['name', 'age']     }      agent = Agent('openai:gpt-4o', output_type=StructuredDict(schema))     result = agent.run_sync('Create a person')     print(result.output)     #> {'name': 'John Doe', 'age': 30}     ```     \"\"\"     json_schema = _utils.check_object_json_schema(json_schema)      # Pydantic `TypeAdapter` fails when `object.__get_pydantic_json_schema__` has `$defs`, so we inline them     # See https://github.com/pydantic/pydantic/issues/12145     if '$defs' in json_schema:         json_schema = InlineDefsJsonSchemaTransformer(json_schema).walk()         if '$defs' in json_schema:             raise exceptions.UserError(                 '`StructuredDict` does not currently support recursive `$ref`s and `$defs`. See https://github.com/pydantic/pydantic/issues/12145 for more information.'             )      if name:         json_schema['title'] = name      if description:         json_schema['description'] = description      class _StructuredDict(JsonSchemaValue):         __is_model_like__ = True          @classmethod         def __get_pydantic_core_schema__(             cls, source_type: Any, handler: GetCoreSchemaHandler         ) -> core_schema.CoreSchema:             return core_schema.dict_schema(                 keys_schema=core_schema.str_schema(),                 values_schema=core_schema.any_schema(),             )          @classmethod         def __get_pydantic_json_schema__(             cls, core_schema: core_schema.CoreSchema, handler: GetJsonSchemaHandler         ) -> JsonSchemaValue:             return json_schema      return _StructuredDict ``` |", "url": "https://ai.pydantic.dev/output/index.html#structureddict", "page": "output/index.html", "source_site": "pydantic_ai"}
{"title": "DeferredToolRequests `dataclass`", "anchor": "deferredtoolrequests-dataclass", "md_text": "Tool calls that require approval or external execution.\n\nThis can be used as an agent's `output_type` and will be used as the output of the agent run if the model called any deferred tools.\n\nResults can be passed to the next agent run using a [`DeferredToolResults`](../tools/index.html#pydantic_ai.tools.DeferredToolResults) object with the same tool call IDs.\n\nSee [deferred tools docs](https://ai.pydantic.dev/deferred-tools/#deferred-tools) for more information.\n\nSource code in `pydantic_ai_slim/pydantic_ai/tools.py`\n\n|  |  |\n| --- | --- |\n| ``` 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 ``` | ``` @dataclass(kw_only=True) class DeferredToolRequests:     \"\"\"Tool calls that require approval or external execution.      This can be used as an agent's `output_type` and will be used as the output of the agent run if the model called any deferred tools.      Results can be passed to the next agent run using a [`DeferredToolResults`][pydantic_ai.tools.DeferredToolResults] object with the same tool call IDs.      See [deferred tools docs](../deferred-tools.md#deferred-tools) for more information.     \"\"\"      calls: list[ToolCallPart] = field(default_factory=list)     \"\"\"Tool calls that require external execution.\"\"\"     approvals: list[ToolCallPart] = field(default_factory=list)     \"\"\"Tool calls that require human-in-the-loop approval.\"\"\" ``` |\n\n#### calls `class-attribute` `instance-attribute`\n\n```\ncalls: list[ToolCallPart] = field(default_factory=list)\n```\n\nTool calls that require external execution.\n\n#### approvals `class-attribute` `instance-attribute`\n\n```\napprovals: list[ToolCallPart] = field(default_factory=list)\n```\n\nTool calls that require human-in-the-loop approval.", "url": "https://ai.pydantic.dev/output/index.html#deferredtoolrequests-dataclass", "page": "output/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractBuiltinTool `dataclass`", "anchor": "abstractbuiltintool-dataclass", "md_text": "Bases: `ABC`\n\nA builtin tool that can be used by an agent.\n\nThis class is abstract and cannot be instantiated directly.\n\nThe builtin tools are passed to the model as part of the `ModelRequestParameters`.\n\nSource code in `pydantic_ai_slim/pydantic_ai/builtin_tools.py`\n\n|  |  |\n| --- | --- |\n| ``` 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 ``` | ``` @dataclass(kw_only=True) class AbstractBuiltinTool(ABC):     \"\"\"A builtin tool that can be used by an agent.      This class is abstract and cannot be instantiated directly.      The builtin tools are passed to the model as part of the `ModelRequestParameters`.     \"\"\"      kind: str = 'unknown_builtin_tool'     \"\"\"Built-in tool identifier, this should be available on all built-in tools as a discriminator.\"\"\"      @property     def unique_id(self) -> str:         \"\"\"A unique identifier for the builtin tool.          If multiple instances of the same builtin tool can be passed to the model, subclasses should override this property to allow them to be distinguished.         \"\"\"         return self.kind      def __init_subclass__(cls, **kwargs: Any) -> None:         super().__init_subclass__(**kwargs)         _BUILTIN_TOOL_TYPES[cls.kind] = cls      @classmethod     def __get_pydantic_core_schema__(         cls, _source_type: Any, handler: pydantic.GetCoreSchemaHandler     ) -> core_schema.CoreSchema:         if cls is not AbstractBuiltinTool:             return handler(cls)          tools = _BUILTIN_TOOL_TYPES.values()         if len(tools) == 1:  # pragma: no cover             tools_type = next(iter(tools))         else:             tools_annotated = [Annotated[tool, pydantic.Tag(tool.kind)] for tool in tools]             tools_type = Annotated[Union[tuple(tools_annotated)], pydantic.Discriminator(_tool_discriminator)]  # noqa: UP007          return handler(tools_type) ``` |\n\n#### kind `class-attribute` `instance-attribute`\n\n```\nkind: str = 'unknown_builtin_tool'\n```\n\nBuilt-in tool identifier, this should be available on all built-in tools as a discriminator.\n\n#### unique\\_id `property`\n\n```\nunique_id: str\n```\n\nA unique identifier for the builtin tool.\n\nIf multiple instances of the same builtin tool can be passed to the model, subclasses should override this property to allow them to be distinguished.", "url": "https://ai.pydantic.dev/builtin_tools/index.html#abstractbuiltintool-dataclass", "page": "builtin_tools/index.html", "source_site": "pydantic_ai"}
{"title": "WebSearchTool `dataclass`", "anchor": "websearchtool-dataclass", "md_text": "Bases: `AbstractBuiltinTool`\n\nA builtin tool that allows your agent to search the web for information.\n\nThe parameters that PydanticAI passes depend on the model, as some parameters may not be supported by certain models.\n\nSupported by:\n\n* Anthropic\n* OpenAI Responses\n* Groq\n* Google\n\nSource code in `pydantic_ai_slim/pydantic_ai/builtin_tools.py`\n\n|  |  |\n| --- | --- |\n| ```  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 ``` | ``` @dataclass(kw_only=True) class WebSearchTool(AbstractBuiltinTool):     \"\"\"A builtin tool that allows your agent to search the web for information.      The parameters that PydanticAI passes depend on the model, as some parameters may not be supported by certain models.      Supported by:      * Anthropic     * OpenAI Responses     * Groq     * Google     \"\"\"      search_context_size: Literal['low', 'medium', 'high'] = 'medium'     \"\"\"The `search_context_size` parameter controls how much context is retrieved from the web to help the tool formulate a response.      Supported by:      * OpenAI Responses     \"\"\"      user_location: WebSearchUserLocation | None = None     \"\"\"The `user_location` parameter allows you to localize search results based on a user's location.      Supported by:      * Anthropic     * OpenAI Responses     \"\"\"      blocked_domains: list[str] | None = None     \"\"\"If provided, these domains will never appear in results.      With Anthropic, you can only use one of `blocked_domains` or `allowed_domains`, not both.      Supported by:      * Anthropic, see <https://docs.anthropic.com/en/docs/build-with-claude/tool-use/web-search-tool#domain-filtering>     * Groq, see <https://console.groq.com/docs/agentic-tooling#search-settings>     \"\"\"      allowed_domains: list[str] | None = None     \"\"\"If provided, only these domains will be included in results.      With Anthropic, you can only use one of `blocked_domains` or `allowed_domains`, not both.      Supported by:      * Anthropic, see <https://docs.anthropic.com/en/docs/build-with-claude/tool-use/web-search-tool#domain-filtering>     * Groq, see <https://console.groq.com/docs/agentic-tooling#search-settings>     \"\"\"      max_uses: int | None = None     \"\"\"If provided, the tool will stop searching the web after the given number of uses.      Supported by:      * Anthropic     \"\"\"      kind: str = 'web_search'     \"\"\"The kind of tool.\"\"\" ``` |\n\n#### search\\_context\\_size `class-attribute` `instance-attribute`\n\n```\nsearch_context_size: Literal[\"low\", \"medium\", \"high\"] = (\n    \"medium\"\n)\n```\n\nThe `search_context_size` parameter controls how much context is retrieved from the web to help the tool formulate a response.\n\nSupported by:\n\n* OpenAI Responses\n\n#### user\\_location `class-attribute` `instance-attribute`\n\n```\nuser_location: WebSearchUserLocation | None = None\n```\n\nThe `user_location` parameter allows you to localize search results based on a user's location.\n\nSupported by:\n\n* Anthropic\n* OpenAI Responses\n\n#### blocked\\_domains `class-attribute` `instance-attribute`\n\n```\nblocked_domains: list[str] | None = None\n```\n\nIf provided, these domains will never appear in results.\n\nWith Anthropic, you can only use one of `blocked_domains` or `allowed_domains`, not both.\n\nSupported by:\n\n* Anthropic, see <https://docs.anthropic.com/en/docs/build-with-claude/tool-use/web-search-tool#domain-filtering>\n* Groq, see <https://console.groq.com/docs/agentic-tooling#search-settings>\n\n#### allowed\\_domains `class-attribute` `instance-attribute`\n\n```\nallowed_domains: list[str] | None = None\n```\n\nIf provided, only these domains will be included in results.\n\nWith Anthropic, you can only use one of `blocked_domains` or `allowed_domains`, not both.\n\nSupported by:\n\n* Anthropic, see <https://docs.anthropic.com/en/docs/build-with-claude/tool-use/web-search-tool#domain-filtering>\n* Groq, see <https://console.groq.com/docs/agentic-tooling#search-settings>\n\n#### max\\_uses `class-attribute` `instance-attribute`\n\n```\nmax_uses: int | None = None\n```\n\nIf provided, the tool will stop searching the web after the given number of uses.\n\nSupported by:\n\n* Anthropic\n\n#### kind `class-attribute` `instance-attribute`\n\n```\nkind: str = 'web_search'\n```\n\nThe kind of tool.", "url": "https://ai.pydantic.dev/builtin_tools/index.html#websearchtool-dataclass", "page": "builtin_tools/index.html", "source_site": "pydantic_ai"}
{"title": "WebSearchUserLocation", "anchor": "websearchuserlocation", "md_text": "Bases: `TypedDict`\n\nAllows you to localize search results based on a user's location.\n\nSupported by:\n\n* Anthropic\n* OpenAI Responses\n\nSource code in `pydantic_ai_slim/pydantic_ai/builtin_tools.py`\n\n|  |  |\n| --- | --- |\n| ``` 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 ``` | ``` class WebSearchUserLocation(TypedDict, total=False):     \"\"\"Allows you to localize search results based on a user's location.      Supported by:      * Anthropic     * OpenAI Responses     \"\"\"      city: str     \"\"\"The city where the user is located.\"\"\"      country: str     \"\"\"The country where the user is located. For OpenAI, this must be a 2-letter country code (e.g., 'US', 'GB').\"\"\"      region: str     \"\"\"The region or state where the user is located.\"\"\"      timezone: str     \"\"\"The timezone of the user's location.\"\"\" ``` |\n\n#### city `instance-attribute`\n\n```\ncity: str\n```\n\nThe city where the user is located.\n\n#### country `instance-attribute`\n\n```\ncountry: str\n```\n\nThe country where the user is located. For OpenAI, this must be a 2-letter country code (e.g., 'US', 'GB').\n\n#### region `instance-attribute`\n\n```\nregion: str\n```\n\nThe region or state where the user is located.\n\n#### timezone `instance-attribute`\n\n```\ntimezone: str\n```\n\nThe timezone of the user's location.", "url": "https://ai.pydantic.dev/builtin_tools/index.html#websearchuserlocation", "page": "builtin_tools/index.html", "source_site": "pydantic_ai"}
{"title": "CodeExecutionTool `dataclass`", "anchor": "codeexecutiontool-dataclass", "md_text": "Bases: `AbstractBuiltinTool`\n\nA builtin tool that allows your agent to execute code.\n\nSupported by:\n\n* Anthropic\n* OpenAI Responses\n* Google\n\nSource code in `pydantic_ai_slim/pydantic_ai/builtin_tools.py`\n\n|  |  |\n| --- | --- |\n| ``` 153 154 155 156 157 158 159 160 161 162 163 164 165 ``` | ``` @dataclass(kw_only=True) class CodeExecutionTool(AbstractBuiltinTool):     \"\"\"A builtin tool that allows your agent to execute code.      Supported by:      * Anthropic     * OpenAI Responses     * Google     \"\"\"      kind: str = 'code_execution'     \"\"\"The kind of tool.\"\"\" ``` |\n\n#### kind `class-attribute` `instance-attribute`\n\n```\nkind: str = 'code_execution'\n```\n\nThe kind of tool.", "url": "https://ai.pydantic.dev/builtin_tools/index.html#codeexecutiontool-dataclass", "page": "builtin_tools/index.html", "source_site": "pydantic_ai"}
{"title": "UrlContextTool `dataclass`", "anchor": "urlcontexttool-dataclass", "md_text": "Bases: `AbstractBuiltinTool`\n\nAllows your agent to access contents from URLs.\n\nSupported by:\n\n* Google\n\nSource code in `pydantic_ai_slim/pydantic_ai/builtin_tools.py`\n\n|  |  |\n| --- | --- |\n| ``` 168 169 170 171 172 173 174 175 176 177 178 ``` | ``` @dataclass(kw_only=True) class UrlContextTool(AbstractBuiltinTool):     \"\"\"Allows your agent to access contents from URLs.      Supported by:      * Google     \"\"\"      kind: str = 'url_context'     \"\"\"The kind of tool.\"\"\" ``` |\n\n#### kind `class-attribute` `instance-attribute`\n\n```\nkind: str = 'url_context'\n```\n\nThe kind of tool.", "url": "https://ai.pydantic.dev/builtin_tools/index.html#urlcontexttool-dataclass", "page": "builtin_tools/index.html", "source_site": "pydantic_ai"}
{"title": "ImageGenerationTool `dataclass`", "anchor": "imagegenerationtool-dataclass", "md_text": "Bases: `AbstractBuiltinTool`\n\nA builtin tool that allows your agent to generate images.\n\nSupported by:\n\n* OpenAI Responses\n* Google\n\nSource code in `pydantic_ai_slim/pydantic_ai/builtin_tools.py`\n\n|  |  |\n| --- | --- |\n| ``` 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 ``` | ``` @dataclass(kw_only=True) class ImageGenerationTool(AbstractBuiltinTool):     \"\"\"A builtin tool that allows your agent to generate images.      Supported by:      * OpenAI Responses     * Google     \"\"\"      background: Literal['transparent', 'opaque', 'auto'] = 'auto'     \"\"\"Background type for the generated image.      Supported by:      * OpenAI Responses. 'transparent' is only supported for 'png' and 'webp' output formats.     \"\"\"      input_fidelity: Literal['high', 'low'] | None = None     \"\"\"     Control how much effort the model will exert to match the style and features,     especially facial features, of input images.      Supported by:      * OpenAI Responses. Default: 'low'.     \"\"\"      moderation: Literal['auto', 'low'] = 'auto'     \"\"\"Moderation level for the generated image.      Supported by:      * OpenAI Responses     \"\"\"      output_compression: int = 100     \"\"\"Compression level for the output image.      Supported by:      * OpenAI Responses. Only supported for 'png' and 'webp' output formats.     \"\"\"      output_format: Literal['png', 'webp', 'jpeg'] | None = None     \"\"\"The output format of the generated image.      Supported by:      * OpenAI Responses. Default: 'png'.     \"\"\"      partial_images: int = 0     \"\"\"     Number of partial images to generate in streaming mode.      Supported by:      * OpenAI Responses. Supports 0 to 3.     \"\"\"      quality: Literal['low', 'medium', 'high', 'auto'] = 'auto'     \"\"\"The quality of the generated image.      Supported by:      * OpenAI Responses     \"\"\"      size: Literal['1024x1024', '1024x1536', '1536x1024', 'auto'] = 'auto'     \"\"\"The size of the generated image.      Supported by:      * OpenAI Responses     \"\"\"      kind: str = 'image_generation'     \"\"\"The kind of tool.\"\"\" ``` |\n\n#### background `class-attribute` `instance-attribute`\n\n```\nbackground: Literal[\"transparent\", \"opaque\", \"auto\"] = (\n    \"auto\"\n)\n```\n\nBackground type for the generated image.\n\nSupported by:\n\n* OpenAI Responses. 'transparent' is only supported for 'png' and 'webp' output formats.\n\n#### input\\_fidelity `class-attribute` `instance-attribute`\n\n```\ninput_fidelity: Literal['high', 'low'] | None = None\n```\n\nControl how much effort the model will exert to match the style and features,\nespecially facial features, of input images.\n\nSupported by:\n\n* OpenAI Responses. Default: 'low'.\n\n#### moderation `class-attribute` `instance-attribute`\n\n```\nmoderation: Literal['auto', 'low'] = 'auto'\n```\n\nModeration level for the generated image.\n\nSupported by:\n\n* OpenAI Responses\n\n#### output\\_compression `class-attribute` `instance-attribute`\n\n```\noutput_compression: int = 100\n```\n\nCompression level for the output image.\n\nSupported by:\n\n* OpenAI Responses. Only supported for 'png' and 'webp' output formats.\n\n#### output\\_format `class-attribute` `instance-attribute`\n\n```\noutput_format: Literal['png', 'webp', 'jpeg'] | None = None\n```\n\nThe output format of the generated image.\n\nSupported by:\n\n* OpenAI Responses. Default: 'png'.\n\n#### partial\\_images `class-attribute` `instance-attribute`\n\n```\npartial_images: int = 0\n```\n\nNumber of partial images to generate in streaming mode.\n\nSupported by:\n\n* OpenAI Responses. Supports 0 to 3.\n\n#### quality `class-attribute` `instance-attribute`\n\n```\nquality: Literal['low', 'medium', 'high', 'auto'] = 'auto'\n```\n\nThe quality of the generated image.\n\nSupported by:\n\n* OpenAI Responses\n\n#### size `class-attribute` `instance-attribute`\n\n```\nsize: Literal[\n    \"1024x1024\", \"1024x1536\", \"1536x1024\", \"auto\"\n] = \"auto\"\n```\n\nThe size of the generated image.\n\nSupported by:\n\n* OpenAI Responses\n\n#### kind `class-attribute` `instance-attribute`\n\n```\nkind: str = 'image_generation'\n```\n\nThe kind of tool.", "url": "https://ai.pydantic.dev/builtin_tools/index.html#imagegenerationtool-dataclass", "page": "builtin_tools/index.html", "source_site": "pydantic_ai"}
{"title": "MemoryTool `dataclass`", "anchor": "memorytool-dataclass", "md_text": "Bases: `AbstractBuiltinTool`\n\nA builtin tool that allows your agent to use memory.\n\nSupported by:\n\n* Anthropic\n\nSource code in `pydantic_ai_slim/pydantic_ai/builtin_tools.py`\n\n|  |  |\n| --- | --- |\n| ``` 262 263 264 265 266 267 268 269 270 271 272 ``` | ``` @dataclass(kw_only=True) class MemoryTool(AbstractBuiltinTool):     \"\"\"A builtin tool that allows your agent to use memory.      Supported by:      * Anthropic     \"\"\"      kind: str = 'memory'     \"\"\"The kind of tool.\"\"\" ``` |\n\n#### kind `class-attribute` `instance-attribute`\n\n```\nkind: str = 'memory'\n```\n\nThe kind of tool.", "url": "https://ai.pydantic.dev/builtin_tools/index.html#memorytool-dataclass", "page": "builtin_tools/index.html", "source_site": "pydantic_ai"}
{"title": "MCPServerTool `dataclass`", "anchor": "mcpservertool-dataclass", "md_text": "Bases: `AbstractBuiltinTool`\n\nA builtin tool that allows your agent to use MCP servers.\n\nSupported by:\n\n* OpenAI Responses\n* Anthropic\n\nSource code in `pydantic_ai_slim/pydantic_ai/builtin_tools.py`\n\n|  |  |\n| --- | --- |\n| ``` 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 ``` | ``` @dataclass(kw_only=True) class MCPServerTool(AbstractBuiltinTool):     \"\"\"A builtin tool that allows your agent to use MCP servers.      Supported by:      * OpenAI Responses     * Anthropic     \"\"\"      id: str     \"\"\"A unique identifier for the MCP server.\"\"\"      url: str     \"\"\"The URL of the MCP server to use.      For OpenAI Responses, it is possible to use `connector_id` by providing it as `x-openai-connector:<connector_id>`.     \"\"\"      authorization_token: str | None = None     \"\"\"Authorization header to use when making requests to the MCP server.      Supported by:      * OpenAI Responses     * Anthropic     \"\"\"      description: str | None = None     \"\"\"A description of the MCP server.      Supported by:      * OpenAI Responses     \"\"\"      allowed_tools: list[str] | None = None     \"\"\"A list of tools that the MCP server can use.      Supported by:      * OpenAI Responses     * Anthropic     \"\"\"      headers: dict[str, str] | None = None     \"\"\"Optional HTTP headers to send to the MCP server.      Use for authentication or other purposes.      Supported by:      * OpenAI Responses     \"\"\"      kind: str = 'mcp_server'      @property     def unique_id(self) -> str:         return ':'.join([self.kind, self.id]) ``` |\n\n#### id `instance-attribute`\n\n```\nid: str\n```\n\nA unique identifier for the MCP server.\n\n#### url `instance-attribute`\n\n```\nurl: str\n```\n\nThe URL of the MCP server to use.\n\nFor OpenAI Responses, it is possible to use `connector_id` by providing it as `x-openai-connector:<connector_id>`.\n\n#### authorization\\_token `class-attribute` `instance-attribute`\n\n```\nauthorization_token: str | None = None\n```\n\nAuthorization header to use when making requests to the MCP server.\n\nSupported by:\n\n* OpenAI Responses\n* Anthropic\n\n#### description `class-attribute` `instance-attribute`\n\n```\ndescription: str | None = None\n```\n\nA description of the MCP server.\n\nSupported by:\n\n* OpenAI Responses\n\n#### allowed\\_tools `class-attribute` `instance-attribute`\n\n```\nallowed_tools: list[str] | None = None\n```\n\nA list of tools that the MCP server can use.\n\nSupported by:\n\n* OpenAI Responses\n* Anthropic\n\n#### headers `class-attribute` `instance-attribute`\n\n```\nheaders: dict[str, str] | None = None\n```\n\nOptional HTTP headers to send to the MCP server.\n\nUse for authentication or other purposes.\n\nSupported by:\n\n* OpenAI Responses", "url": "https://ai.pydantic.dev/builtin_tools/index.html#mcpservertool-dataclass", "page": "builtin_tools/index.html", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "md_text": "To use `GroqModel`, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `groq` optional group:\n\npipuv\n\n```\npip install \"pydantic-ai-slim[groq]\"\n\nuv add \"pydantic-ai-slim[groq]\"\n```", "url": "https://ai.pydantic.dev/groq/index.html#install", "page": "groq/index.html", "source_site": "pydantic_ai"}
{"title": "Configuration", "anchor": "configuration", "md_text": "To use [Groq](https://groq.com/) through their API, go to [console.groq.com/keys](https://console.groq.com/keys) and follow your nose until you find the place to generate an API key.\n\n`GroqModelName` contains a list of available Groq models.", "url": "https://ai.pydantic.dev/groq/index.html#configuration", "page": "groq/index.html", "source_site": "pydantic_ai"}
{"title": "Environment variable", "anchor": "environment-variable", "md_text": "Once you have the API key, you can set it as an environment variable:\n\n```\nexport GROQ_API_KEY='your-api-key'\n```\n\nYou can then use `GroqModel` by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('groq:llama-3.3-70b-versatile')\n...\n```\n\nOr initialise the model directly with just the model name:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.groq import GroqModel\n\nmodel = GroqModel('llama-3.3-70b-versatile')\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/groq/index.html#environment-variable", "page": "groq/index.html", "source_site": "pydantic_ai"}
{"title": "`provider` argument", "anchor": "provider-argument", "md_text": "You can provide a custom `Provider` via the `provider` argument:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.groq import GroqModel\nfrom pydantic_ai.providers.groq import GroqProvider\n\nmodel = GroqModel(\n    'llama-3.3-70b-versatile', provider=GroqProvider(api_key='your-api-key')\n)\nagent = Agent(model)\n...\n```\n\nYou can also customize the `GroqProvider` with a custom `httpx.AsyncHTTPClient`:\n\n```\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.groq import GroqModel\nfrom pydantic_ai.providers.groq import GroqProvider\n\ncustom_http_client = AsyncClient(timeout=30)\nmodel = GroqModel(\n    'llama-3.3-70b-versatile',\n    provider=GroqProvider(api_key='your-api-key', http_client=custom_http_client),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/groq/index.html#provider-argument", "page": "groq/index.html", "source_site": "pydantic_ai"}
{"title": "RetryConfig", "anchor": "retryconfig", "md_text": "Bases: `TypedDict`\n\nThe configuration for tenacity-based retrying.\n\nThese are precisely the arguments to the tenacity `retry` decorator, and they are generally\nused internally by passing them to that decorator via `@retry(**config)` or similar.\n\nAll fields are optional, and if not provided, the default values from the `tenacity.retry` decorator will be used.\n\nSource code in `pydantic_ai_slim/pydantic_ai/retries.py`\n\n|  |  |\n| --- | --- |\n| ```  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 ``` | ``` class RetryConfig(TypedDict, total=False):     \"\"\"The configuration for tenacity-based retrying.      These are precisely the arguments to the tenacity `retry` decorator, and they are generally     used internally by passing them to that decorator via `@retry(**config)` or similar.      All fields are optional, and if not provided, the default values from the `tenacity.retry` decorator will be used.     \"\"\"      sleep: Callable[[int | float], None | Awaitable[None]]     \"\"\"A sleep strategy to use for sleeping between retries.      Tenacity's default for this argument is `tenacity.nap.sleep`.\"\"\"      stop: StopBaseT     \"\"\"     A stop strategy to determine when to stop retrying.      Tenacity's default for this argument is `tenacity.stop.stop_never`.\"\"\"      wait: WaitBaseT     \"\"\"     A wait strategy to determine how long to wait between retries.      Tenacity's default for this argument is `tenacity.wait.wait_none`.\"\"\"      retry: SyncRetryBaseT | RetryBaseT     \"\"\"A retry strategy to determine which exceptions should trigger a retry.      Tenacity's default for this argument is `tenacity.retry.retry_if_exception_type()`.\"\"\"      before: Callable[[RetryCallState], None | Awaitable[None]]     \"\"\"     A callable that is called before each retry attempt.      Tenacity's default for this argument is `tenacity.before.before_nothing`.\"\"\"      after: Callable[[RetryCallState], None | Awaitable[None]]     \"\"\"     A callable that is called after each retry attempt.      Tenacity's default for this argument is `tenacity.after.after_nothing`.\"\"\"      before_sleep: Callable[[RetryCallState], None | Awaitable[None]] | None     \"\"\"     An optional callable that is called before sleeping between retries.      Tenacity's default for this argument is `None`.\"\"\"      reraise: bool     \"\"\"Whether to reraise the last exception if the retry attempts are exhausted, or raise a RetryError instead.      Tenacity's default for this argument is `False`.\"\"\"      retry_error_cls: type[RetryError]     \"\"\"The exception class to raise when the retry attempts are exhausted and `reraise` is False.      Tenacity's default for this argument is `tenacity.RetryError`.\"\"\"      retry_error_callback: Callable[[RetryCallState], Any | Awaitable[Any]] | None     \"\"\"An optional callable that is called when the retry attempts are exhausted and `reraise` is False.      Tenacity's default for this argument is `None`.\"\"\" ``` |\n\n#### sleep `instance-attribute`\n\n```\nsleep: Callable[[int | float], None | Awaitable[None]]\n```\n\nA sleep strategy to use for sleeping between retries.\n\nTenacity's default for this argument is `tenacity.nap.sleep`.\n\n#### stop `instance-attribute`\n\n```\nstop: StopBaseT\n```\n\nA stop strategy to determine when to stop retrying.\n\nTenacity's default for this argument is `tenacity.stop.stop_never`.\n\n#### wait `instance-attribute`\n\n```\nwait: WaitBaseT\n```\n\nA wait strategy to determine how long to wait between retries.\n\nTenacity's default for this argument is `tenacity.wait.wait_none`.\n\n#### retry `instance-attribute`\n\n```\nretry: RetryBaseT | RetryBaseT\n```\n\nA retry strategy to determine which exceptions should trigger a retry.\n\nTenacity's default for this argument is `tenacity.retry.retry_if_exception_type()`.\n\n#### before `instance-attribute`\n\n```\nbefore: Callable[[RetryCallState], None | Awaitable[None]]\n```\n\nA callable that is called before each retry attempt.\n\nTenacity's default for this argument is `tenacity.before.before_nothing`.\n\n#### after `instance-attribute`\n\n```\nafter: Callable[[RetryCallState], None | Awaitable[None]]\n```\n\nA callable that is called after each retry attempt.\n\nTenacity's default for this argument is `tenacity.after.after_nothing`.\n\n#### before\\_sleep `instance-attribute`\n\n```\nbefore_sleep: (\n    Callable[[RetryCallState], None | Awaitable[None]]\n    | None\n)\n```\n\nAn optional callable that is called before sleeping between retries.\n\nTenacity's default for this argument is `None`.\n\n#### reraise `instance-attribute`\n\n```\nreraise: bool\n```", "url": "https://ai.pydantic.dev/retries/index.html#retryconfig", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "RetryConfig", "anchor": "retryconfig", "md_text": "Whether to reraise the last exception if the retry attempts are exhausted, or raise a RetryError instead.\n\nTenacity's default for this argument is `False`.\n\n#### retry\\_error\\_cls `instance-attribute`\n\n```\nretry_error_cls: type[RetryError]\n```\n\nThe exception class to raise when the retry attempts are exhausted and `reraise` is False.\n\nTenacity's default for this argument is `tenacity.RetryError`.\n\n#### retry\\_error\\_callback `instance-attribute`\n\n```\nretry_error_callback: (\n    Callable[[RetryCallState], Any | Awaitable[Any]] | None\n)\n```\n\nAn optional callable that is called when the retry attempts are exhausted and `reraise` is False.\n\nTenacity's default for this argument is `None`.", "url": "https://ai.pydantic.dev/retries/index.html#retryconfig", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "TenacityTransport", "anchor": "tenacitytransport", "md_text": "Bases: `BaseTransport`\n\nSynchronous HTTP transport with tenacity-based retry functionality.\n\nThis transport wraps another BaseTransport and adds retry capabilities using the tenacity library.\nIt can be configured to retry requests based on various conditions such as specific exception types,\nresponse status codes, or custom validation logic.\n\nThe transport works by intercepting HTTP requests and responses, allowing the tenacity controller\nto determine when and how to retry failed requests. The validate\\_response function can be used\nto convert HTTP responses into exceptions that trigger retries.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `wrapped` | `BaseTransport | None` | The underlying transport to wrap and add retry functionality to. | `None` |\n| `config` | `RetryConfig` | The arguments to use for the tenacity `retry` decorator, including retry conditions, wait strategy, stop conditions, etc. See the tenacity docs for more info. | *required* |\n| `validate_response` | `Callable[[Response], Any] | None` | Optional callable that takes a Response and can raise an exception to be handled by the controller if the response should trigger a retry. Common use case is to raise exceptions for certain HTTP status codes. If None, no response validation is performed. | `None` |\n\nExample\n\n```\nfrom httpx import Client, HTTPStatusError, HTTPTransport\nfrom tenacity import retry_if_exception_type, stop_after_attempt\n\nfrom pydantic_ai.retries import RetryConfig, TenacityTransport, wait_retry_after\n\ntransport = TenacityTransport(\n    RetryConfig(\n        retry=retry_if_exception_type(HTTPStatusError),\n        wait=wait_retry_after(max_wait=300),\n        stop=stop_after_attempt(5),\n        reraise=True\n    ),\n    HTTPTransport(),\n    validate_response=lambda r: r.raise_for_status()\n)\nclient = Client(transport=transport)\n```\n\n\nSource code in `pydantic_ai_slim/pydantic_ai/retries.py`", "url": "https://ai.pydantic.dev/retries/index.html#tenacitytransport", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "TenacityTransport", "anchor": "tenacitytransport", "md_text": "|  |  |\n| --- | --- |\n| ``` 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 ``` | ``` class TenacityTransport(BaseTransport):     \"\"\"Synchronous HTTP transport with tenacity-based retry functionality.      This transport wraps another BaseTransport and adds retry capabilities using the tenacity library.     It can be configured to retry requests based on various conditions such as specific exception types,     response status codes, or custom validation logic.      The transport works by intercepting HTTP requests and responses, allowing the tenacity controller     to determine when and how to retry failed requests. The validate_response function can be used     to convert HTTP responses into exceptions that trigger retries.      Args:         wrapped: The underlying transport to wrap and add retry functionality to.         config: The arguments to use for the tenacity `retry` decorator, including retry conditions,             wait strategy, stop conditions, etc. See the tenacity docs for more info.         validate_response: Optional callable that takes a Response and can raise an exception             to be handled by the controller if the response should trigger a retry.             Common use case is to raise exceptions for certain HTTP status codes.             If None, no response validation is performed.      Example:         ```python         from httpx import Client, HTTPStatusError, HTTPTransport         from tenacity import retry_if_exception_type, stop_after_attempt          from pydantic_ai.retries import RetryConfig, TenacityTransport, wait_retry_after          transport = TenacityTransport(             RetryConfig(                 retry=retry_if_exception_type(HTTPStatusError),                 wait=wait_retry_after(max_wait=300),                 stop=stop_after_attempt(5),                 reraise=True             ),             HTTPTransport(),             validate_response=lambda r: r.raise_for_status()         )         client = Client(transport=transport)         ```     \"\"\"      def __init__(         self,         config: RetryConfig,         wrapped: BaseTransport | None = None,         validate_response: Callable[[Response], Any] | None = None,     ):         self.config = config         self.wrapped = wrapped or HTTPTransport()         self.validate_response = validate_response      def handle_request(self, request: Request) -> Response:         \"\"\"Handle an HTTP request with retry logic.          Args:             request: The HTTP request to handle.          Returns:             The HTTP response.          Raises:             RuntimeError: If the retry controller did not make any attempts.             Exception: Any exception raised by the wrapped transport or validation function.         \"\"\"          @retry(**self.config)         def handle_request(req: Request) -> Response:             response = self.wrapped.handle_request(req)              # this is normally set by httpx _after_ calling this function, but we want the request in the validator:             response.request = req              if self.validate_response:                 try:                     self.validate_response(response)                 except Exception:                     response.close()                     raise             return response          return handle_request(request)      def __enter__(self) -> TenacityTransport:         self.wrapped.__enter__()         return self      def __exit__(         self,         exc_type: type[BaseException] | None = None,         exc_value: BaseException | None = None,         traceback: TracebackType | None = None,     ) -> None:         self.wrapped.__exit__(exc_type, exc_value, traceback)      def close(self) -> None:         self.wrapped.close()  # pragma: no cover ``` |\n\n#### handle\\_request\n\n```\nhandle_request(request: Request) -> Response\n```\n\nHandle an HTTP request with retry logic.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `request` | `Request` | The HTTP request to handle. | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `Response` | The HTTP response. |\n\nRaises:\n\n| Type | Description |\n| --- | --- |\n| `RuntimeError` | If the retry controller did not make any attempts. |\n| `Exception` | Any exception raised by the wrapped transport or validation function. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/retries.py`", "url": "https://ai.pydantic.dev/retries/index.html#tenacitytransport", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "TenacityTransport", "anchor": "tenacitytransport", "md_text": "|  |  |\n| --- | --- |\n| ``` 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 ``` | ``` def handle_request(self, request: Request) -> Response:     \"\"\"Handle an HTTP request with retry logic.      Args:         request: The HTTP request to handle.      Returns:         The HTTP response.      Raises:         RuntimeError: If the retry controller did not make any attempts.         Exception: Any exception raised by the wrapped transport or validation function.     \"\"\"      @retry(**self.config)     def handle_request(req: Request) -> Response:         response = self.wrapped.handle_request(req)          # this is normally set by httpx _after_ calling this function, but we want the request in the validator:         response.request = req          if self.validate_response:             try:                 self.validate_response(response)             except Exception:                 response.close()                 raise         return response      return handle_request(request) ``` |", "url": "https://ai.pydantic.dev/retries/index.html#tenacitytransport", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "AsyncTenacityTransport", "anchor": "asynctenacitytransport", "md_text": "Bases: `AsyncBaseTransport`\n\nAsynchronous HTTP transport with tenacity-based retry functionality.\n\nThis transport wraps another AsyncBaseTransport and adds retry capabilities using the tenacity library.\nIt can be configured to retry requests based on various conditions such as specific exception types,\nresponse status codes, or custom validation logic.\n\nThe transport works by intercepting HTTP requests and responses, allowing the tenacity controller\nto determine when and how to retry failed requests. The validate\\_response function can be used\nto convert HTTP responses into exceptions that trigger retries.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `wrapped` | `AsyncBaseTransport | None` | The underlying async transport to wrap and add retry functionality to. | `None` |\n| `config` | `RetryConfig` | The arguments to use for the tenacity `retry` decorator, including retry conditions, wait strategy, stop conditions, etc. See the tenacity docs for more info. | *required* |\n| `validate_response` | `Callable[[Response], Any] | None` | Optional callable that takes a Response and can raise an exception to be handled by the controller if the response should trigger a retry. Common use case is to raise exceptions for certain HTTP status codes. If None, no response validation is performed. | `None` |\n\nExample\n\n```\nfrom httpx import AsyncClient, HTTPStatusError\nfrom tenacity import retry_if_exception_type, stop_after_attempt\n\nfrom pydantic_ai.retries import AsyncTenacityTransport, RetryConfig, wait_retry_after\n\ntransport = AsyncTenacityTransport(\n    RetryConfig(\n        retry=retry_if_exception_type(HTTPStatusError),\n        wait=wait_retry_after(max_wait=300),\n        stop=stop_after_attempt(5),\n        reraise=True\n    ),\n    validate_response=lambda r: r.raise_for_status()\n)\nclient = AsyncClient(transport=transport)\n```\n\n\nSource code in `pydantic_ai_slim/pydantic_ai/retries.py`", "url": "https://ai.pydantic.dev/retries/index.html#asynctenacitytransport", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "AsyncTenacityTransport", "anchor": "asynctenacitytransport", "md_text": "|  |  |\n| --- | --- |\n| ``` 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 ``` | ``` class AsyncTenacityTransport(AsyncBaseTransport):     \"\"\"Asynchronous HTTP transport with tenacity-based retry functionality.      This transport wraps another AsyncBaseTransport and adds retry capabilities using the tenacity library.     It can be configured to retry requests based on various conditions such as specific exception types,     response status codes, or custom validation logic.      The transport works by intercepting HTTP requests and responses, allowing the tenacity controller     to determine when and how to retry failed requests. The validate_response function can be used     to convert HTTP responses into exceptions that trigger retries.      Args:         wrapped: The underlying async transport to wrap and add retry functionality to.         config: The arguments to use for the tenacity `retry` decorator, including retry conditions,             wait strategy, stop conditions, etc. See the tenacity docs for more info.         validate_response: Optional callable that takes a Response and can raise an exception             to be handled by the controller if the response should trigger a retry.             Common use case is to raise exceptions for certain HTTP status codes.             If None, no response validation is performed.      Example:         ```python         from httpx import AsyncClient, HTTPStatusError         from tenacity import retry_if_exception_type, stop_after_attempt          from pydantic_ai.retries import AsyncTenacityTransport, RetryConfig, wait_retry_after          transport = AsyncTenacityTransport(             RetryConfig(                 retry=retry_if_exception_type(HTTPStatusError),                 wait=wait_retry_after(max_wait=300),                 stop=stop_after_attempt(5),                 reraise=True             ),             validate_response=lambda r: r.raise_for_status()         )         client = AsyncClient(transport=transport)         ```     \"\"\"      def __init__(         self,         config: RetryConfig,         wrapped: AsyncBaseTransport | None = None,         validate_response: Callable[[Response], Any] | None = None,     ):         self.config = config         self.wrapped = wrapped or AsyncHTTPTransport()         self.validate_response = validate_response      async def handle_async_request(self, request: Request) -> Response:         \"\"\"Handle an async HTTP request with retry logic.          Args:             request: The HTTP request to handle.          Returns:             The HTTP response.          Raises:             RuntimeError: If the retry controller did not make any attempts.             Exception: Any exception raised by the wrapped transport or validation function.         \"\"\"          @retry(**self.config)         async def handle_async_request(req: Request) -> Response:             response = await self.wrapped.handle_async_request(req)              # this is normally set by httpx _after_ calling this function, but we want the request in the validator:             response.request = req              if self.validate_response:                 try:                     self.validate_response(response)                 except Exception:                     await response.aclose()                     raise             return response          return await handle_async_request(request)      async def __aenter__(self) -> AsyncTenacityTransport:         await self.wrapped.__aenter__()         return self      async def __aexit__(         self,         exc_type: type[BaseException] | None = None,         exc_value: BaseException | None = None,         traceback: TracebackType | None = None,     ) -> None:         await self.wrapped.__aexit__(exc_type, exc_value, traceback)      async def aclose(self) -> None:         await self.wrapped.aclose() ``` |\n\n#### handle\\_async\\_request `async`\n\n```\nhandle_async_request(request: Request) -> Response\n```\n\nHandle an async HTTP request with retry logic.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `request` | `Request` | The HTTP request to handle. | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `Response` | The HTTP response. |\n\nRaises:\n\n| Type | Description |\n| --- | --- |\n| `RuntimeError` | If the retry controller did not make any attempts. |\n| `Exception` | Any exception raised by the wrapped transport or validation function. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/retries.py`", "url": "https://ai.pydantic.dev/retries/index.html#asynctenacitytransport", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "AsyncTenacityTransport", "anchor": "asynctenacitytransport", "md_text": "|  |  |\n| --- | --- |\n| ``` 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 ``` | ``` async def handle_async_request(self, request: Request) -> Response:     \"\"\"Handle an async HTTP request with retry logic.      Args:         request: The HTTP request to handle.      Returns:         The HTTP response.      Raises:         RuntimeError: If the retry controller did not make any attempts.         Exception: Any exception raised by the wrapped transport or validation function.     \"\"\"      @retry(**self.config)     async def handle_async_request(req: Request) -> Response:         response = await self.wrapped.handle_async_request(req)          # this is normally set by httpx _after_ calling this function, but we want the request in the validator:         response.request = req          if self.validate_response:             try:                 self.validate_response(response)             except Exception:                 await response.aclose()                 raise         return response      return await handle_async_request(request) ``` |", "url": "https://ai.pydantic.dev/retries/index.html#asynctenacitytransport", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "wait\\_retry\\_after", "anchor": "waitretryafter", "md_text": "```\nwait_retry_after(\n    fallback_strategy: (\n        Callable[[RetryCallState], float] | None\n    ) = None,\n    max_wait: float = 300,\n) -> Callable[[RetryCallState], float]\n```\n\nCreate a tenacity-compatible wait strategy that respects HTTP Retry-After headers.\n\nThis wait strategy checks if the exception contains an HTTPStatusError with a\nRetry-After header, and if so, waits for the time specified in the header.\nIf no header is present or parsing fails, it falls back to the provided strategy.\n\nThe Retry-After header can be in two formats:\n- An integer representing seconds to wait\n- An HTTP date string representing when to retry\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `fallback_strategy` | `Callable[[RetryCallState], float] | None` | Wait strategy to use when no Retry-After header is present or parsing fails. Defaults to exponential backoff with max 60s. | `None` |\n| `max_wait` | `float` | Maximum time to wait in seconds, regardless of header value. Defaults to 300 (5 minutes). | `300` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `Callable[[RetryCallState], float]` | A wait function that can be used with tenacity retry decorators. |\n\nExample\n\n```\nfrom httpx import AsyncClient, HTTPStatusError\nfrom tenacity import retry_if_exception_type, stop_after_attempt\n\nfrom pydantic_ai.retries import AsyncTenacityTransport, RetryConfig, wait_retry_after\n\ntransport = AsyncTenacityTransport(\n    RetryConfig(\n        retry=retry_if_exception_type(HTTPStatusError),\n        wait=wait_retry_after(max_wait=120),\n        stop=stop_after_attempt(5),\n        reraise=True\n    ),\n    validate_response=lambda r: r.raise_for_status()\n)\nclient = AsyncClient(transport=transport)\n```\n\n\nSource code in `pydantic_ai_slim/pydantic_ai/retries.py`\n\n|  |  |\n| --- | --- |\n| ``` 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 ``` | ``` def wait_retry_after(     fallback_strategy: Callable[[RetryCallState], float] | None = None, max_wait: float = 300 ) -> Callable[[RetryCallState], float]:     \"\"\"Create a tenacity-compatible wait strategy that respects HTTP Retry-After headers.      This wait strategy checks if the exception contains an HTTPStatusError with a     Retry-After header, and if so, waits for the time specified in the header.     If no header is present or parsing fails, it falls back to the provided strategy.      The Retry-After header can be in two formats:     - An integer representing seconds to wait     - An HTTP date string representing when to retry      Args:         fallback_strategy: Wait strategy to use when no Retry-After header is present                           or parsing fails. Defaults to exponential backoff with max 60s.         max_wait: Maximum time to wait in seconds, regardless of header value.                  Defaults to 300 (5 minutes).      Returns:         A wait function that can be used with tenacity retry decorators.      Example:         ```python         from httpx import AsyncClient, HTTPStatusError         from tenacity import retry_if_exception_type, stop_after_attempt          from pydantic_ai.retries import AsyncTenacityTransport, RetryConfig, wait_retry_after          transport = AsyncTenacityTransport(             RetryConfig(                 retry=retry_if_exception_type(HTTPStatusError),                 wait=wait_retry_after(max_wait=120),                 stop=stop_after_attempt(5),                 reraise=True             ),             validate_response=lambda r: r.raise_for_status()         )         client = AsyncClient(transport=transport)         ```     \"\"\"     if fallback_strategy is None:         fallback_strategy = wait_exponential(multiplier=1, max=60)      def wait_func(state: RetryCallState) -> float:         exc = state.outcome.exception() if state.outcome else None         if isinstance(exc, HTTPStatusError):             retry_after = exc.response.headers.get('retry-after')             if retry_after:                 try:                     # Try parsing as seconds first                     wait_seconds = int(retry_after)                     return min(float(wait_seconds), max_wait)                 except ValueError:                     # Try parsing as HTTP date                     try:                         retry_time = cast(datetime, parsedate_to_datetime(retry_after))                         assert isinstance(retry_time, datetime)                         now = datetime.now(timezone.utc)                         wait_seconds = (retry_time - now).total_seconds()                          if wait_seconds > 0:                             return min(wait_seconds, max_wait)                     except (ValueError, TypeError, AssertionError):                         # If date parsing fails, fall back to fallback strategy                         pass          # Use fallback strategy         return fallback_strategy(state)      return wait_func ``` |", "url": "https://ai.pydantic.dev/retries/index.html#waitretryafter", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "md_text": "With [dependencies installed and environment variables set](../setup/index.html#usage), run:\n\npipuv\n\n```\npython -m pydantic_ai_examples.pydantic_model\n\nuv run -m pydantic_ai_examples.pydantic_model\n```\n\nThis examples uses `openai:gpt-4o` by default, but it works well with other models, e.g. you can run it\nwith Gemini using:\n\npipuv\n\n```\nPYDANTIC_AI_MODEL=gemini-1.5-pro python -m pydantic_ai_examples.pydantic_model\n\nPYDANTIC_AI_MODEL=gemini-1.5-pro uv run -m pydantic_ai_examples.pydantic_model\n```\n\n(or `PYDANTIC_AI_MODEL=gemini-1.5-flash ...`)", "url": "https://ai.pydantic.dev/pydantic-model/index.html#running-the-example", "page": "pydantic-model/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "md_text": "[pydantic\\_model.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/pydantic_model.py)\n\n```\n\"\"\"Simple example of using Pydantic AI to construct a Pydantic model from a text input.\n\nRun with:\n\n    uv run -m pydantic_ai_examples.pydantic_model\n\"\"\"\n\nimport os\n\nimport logfire\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent\n\n# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\nlogfire.configure(send_to_logfire='if-token-present')\nlogfire.instrument_pydantic_ai()\n\n\nclass MyModel(BaseModel):\n    city: str\n    country: str\n\n\nmodel = os.getenv('PYDANTIC_AI_MODEL', 'openai:gpt-4o')\nprint(f'Using model: {model}')\nagent = Agent(model, output_type=MyModel)\n\nif __name__ == '__main__':\n    result = agent.run_sync('The windy city in the US of A.')\n    print(result.output)\n    print(result.usage())\n```", "url": "https://ai.pydantic.dev/pydantic-model/index.html#example-code", "page": "pydantic-model/index.html", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "md_text": "To use `BedrockConverseModel`, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `bedrock` optional group:\n\npipuv\n\n```\npip install \"pydantic-ai-slim[bedrock]\"\n\nuv add \"pydantic-ai-slim[bedrock]\"\n```", "url": "https://ai.pydantic.dev/bedrock/index.html#install", "page": "bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "Configuration", "anchor": "configuration", "md_text": "To use [AWS Bedrock](https://aws.amazon.com/bedrock/), you'll need an AWS account with Bedrock enabled and appropriate credentials. You can use either AWS credentials directly or a pre-configured boto3 client.\n\n`BedrockModelName` contains a list of available Bedrock models, including models from Anthropic, Amazon, Cohere, Meta, and Mistral.", "url": "https://ai.pydantic.dev/bedrock/index.html#configuration", "page": "bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "Environment variables", "anchor": "environment-variables", "md_text": "You can set your AWS credentials as environment variables ([among other options](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#using-environment-variables)):\n\n```\nexport AWS_BEARER_TOKEN_BEDROCK='your-api-key'\n# or:\nexport AWS_ACCESS_KEY_ID='your-access-key'\nexport AWS_SECRET_ACCESS_KEY='your-secret-key'\nexport AWS_DEFAULT_REGION='us-east-1'  # or your preferred region\n```\n\nYou can then use `BedrockConverseModel` by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('bedrock:anthropic.claude-3-sonnet-20240229-v1:0')\n...\n```\n\nOr initialize the model directly with just the model name:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.bedrock import BedrockConverseModel\n\nmodel = BedrockConverseModel('anthropic.claude-3-sonnet-20240229-v1:0')\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/bedrock/index.html#environment-variables", "page": "bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "Customizing Bedrock Runtime API", "anchor": "customizing-bedrock-runtime-api", "md_text": "You can customize the Bedrock Runtime API calls by adding additional parameters, such as [guardrail\nconfigurations](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html) and [performance settings](https://docs.aws.amazon.com/bedrock/latest/userguide/latency-optimized-inference.html). For a complete list of configurable parameters, refer to the\ndocumentation for [`BedrockModelSettings`](../models/bedrock/index.html#pydantic_ai.models.bedrock.BedrockModelSettings).\n\ncustomize\\_bedrock\\_model\\_settings.py\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.bedrock import BedrockConverseModel, BedrockModelSettings\n\n# Define Bedrock model settings with guardrail and performance configurations\nbedrock_model_settings = BedrockModelSettings(\n    bedrock_guardrail_config={\n        'guardrailIdentifier': 'v1',\n        'guardrailVersion': 'v1',\n        'trace': 'enabled'\n    },\n    bedrock_performance_configuration={\n        'latency': 'optimized'\n    }\n)\n\n\nmodel = BedrockConverseModel(model_name='us.amazon.nova-pro-v1:0')\n\nagent = Agent(model=model, model_settings=bedrock_model_settings)\n```", "url": "https://ai.pydantic.dev/bedrock/index.html#customizing-bedrock-runtime-api", "page": "bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "`provider` argument", "anchor": "provider-argument", "md_text": "You can provide a custom `BedrockProvider` via the `provider` argument. This is useful when you want to specify credentials directly or use a custom boto3 client:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.bedrock import BedrockConverseModel\nfrom pydantic_ai.providers.bedrock import BedrockProvider\n\n# Using AWS credentials directly\nmodel = BedrockConverseModel(\n    'anthropic.claude-3-sonnet-20240229-v1:0',\n    provider=BedrockProvider(\n        region_name='us-east-1',\n        aws_access_key_id='your-access-key',\n        aws_secret_access_key='your-secret-key',\n    ),\n)\nagent = Agent(model)\n...\n```\n\nYou can also pass a pre-configured boto3 client:\n\n```\nimport boto3\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.bedrock import BedrockConverseModel\nfrom pydantic_ai.providers.bedrock import BedrockProvider\n\n# Using a pre-configured boto3 client\nbedrock_client = boto3.client('bedrock-runtime', region_name='us-east-1')\nmodel = BedrockConverseModel(\n    'anthropic.claude-3-sonnet-20240229-v1:0',\n    provider=BedrockProvider(bedrock_client=bedrock_client),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/bedrock/index.html#provider-argument", "page": "bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "md_text": "With [dependencies installed and environment variables set](../setup/index.html#usage), run:\n\npipuv\n\n```\npython -m pydantic_ai_examples.data_analyst\n\nuv run -m pydantic_ai_examples.data_analyst\n```\n\nOutput (debug):\n\n> Based on my analysis of the Cornell Movie Review dataset (rotten\\_tomatoes), there are **4,265 negative comments** in the training split. These are the reviews labeled as 'neg' (represented by 0 in the dataset).", "url": "https://ai.pydantic.dev/data-analyst/index.html#running-the-example", "page": "data-analyst/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "md_text": "[data\\_analyst.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/data_analyst.py)\n\n```\nfrom dataclasses import dataclass, field\n\nimport datasets\nimport duckdb\nimport pandas as pd\n\nfrom pydantic_ai import Agent, ModelRetry, RunContext\n\n\n@dataclass\nclass AnalystAgentDeps:\n    output: dict[str, pd.DataFrame] = field(default_factory=dict)\n\n    def store(self, value: pd.DataFrame) -> str:\n        \"\"\"Store the output in deps and return the reference such as Out[1] to be used by the LLM.\"\"\"\n        ref = f'Out[{len(self.output) + 1}]'\n        self.output[ref] = value\n        return ref\n\n    def get(self, ref: str) -> pd.DataFrame:\n        if ref not in self.output:\n            raise ModelRetry(\n                f'Error: {ref} is not a valid variable reference. Check the previous messages and try again.'\n            )\n        return self.output[ref]\n\n\nanalyst_agent = Agent(\n    'openai:gpt-4o',\n    deps_type=AnalystAgentDeps,\n    instructions='You are a data analyst and your job is to analyze the data according to the user request.',\n)\n\n\n@analyst_agent.tool\ndef load_dataset(\n    ctx: RunContext[AnalystAgentDeps],\n    path: str,\n    split: str = 'train',\n) -> str:\n    \"\"\"Load the `split` of dataset `dataset_name` from huggingface.\n\n    Args:\n        ctx: Pydantic AI agent RunContext\n        path: name of the dataset in the form of `<user_name>/<dataset_name>`\n        split: load the split of the dataset (default: \"train\")\n    \"\"\"\n    # begin load data from hf\n    builder = datasets.load_dataset_builder(path)  # pyright: ignore[reportUnknownMemberType]\n    splits: dict[str, datasets.SplitInfo] = builder.info.splits or {}  # pyright: ignore[reportUnknownMemberType]\n    if split not in splits:\n        raise ModelRetry(\n            f'{split} is not valid for dataset {path}. Valid splits are {\",\".join(splits.keys())}'\n        )\n\n    builder.download_and_prepare()  # pyright: ignore[reportUnknownMemberType]\n    dataset = builder.as_dataset(split=split)\n    assert isinstance(dataset, datasets.Dataset)\n    dataframe = dataset.to_pandas()\n    assert isinstance(dataframe, pd.DataFrame)\n    # end load data from hf\n\n    # store the dataframe in the deps and get a ref like \"Out[1]\"\n    ref = ctx.deps.store(dataframe)\n    # construct a summary of the loaded dataset\n    output = [\n        f'Loaded the dataset as `{ref}`.',\n        f'Description: {dataset.info.description}'\n        if dataset.info.description\n        else None,\n        f'Features: {dataset.info.features!r}' if dataset.info.features else None,\n    ]\n    return '\\n'.join(filter(None, output))\n\n\n@analyst_agent.tool\ndef run_duckdb(ctx: RunContext[AnalystAgentDeps], dataset: str, sql: str) -> str:\n    \"\"\"Run DuckDB SQL query on the DataFrame.\n\n    Note that the virtual table name used in DuckDB SQL must be `dataset`.\n\n    Args:\n        ctx: Pydantic AI agent RunContext\n        dataset: reference string to the DataFrame\n        sql: the query to be executed using DuckDB\n    \"\"\"\n    data = ctx.deps.get(dataset)\n    result = duckdb.query_df(df=data, virtual_table_name='dataset', sql_query=sql)\n    # pass the result as ref (because DuckDB SQL can select many rows, creating another huge dataframe)\n    ref = ctx.deps.store(result.df())  # pyright: ignore[reportUnknownMemberType]\n    return f'Executed SQL, result is `{ref}`'\n\n\n@analyst_agent.tool\ndef display(ctx: RunContext[AnalystAgentDeps], name: str) -> str:\n    \"\"\"Display at most 5 rows of the dataframe.\"\"\"\n    dataset = ctx.deps.get(name)\n    return dataset.head().to_string()  # pyright: ignore[reportUnknownMemberType]\n\n\nif __name__ == '__main__':\n    deps = AnalystAgentDeps()\n    result = analyst_agent.run_sync(\n        user_prompt='Count how many negative comments are there in the dataset `cornell-movie-review-data/rotten_tomatoes`',\n        deps=deps,\n    )\n    print(result.output)\n```", "url": "https://ai.pydantic.dev/data-analyst/index.html#example-code", "page": "data-analyst/index.html", "source_site": "pydantic_ai"}
{"title": "Appendix", "anchor": "appendix", "md_text": "", "url": "https://ai.pydantic.dev/data-analyst/index.html#appendix", "page": "data-analyst/index.html", "source_site": "pydantic_ai"}
{"title": "Choosing a Model", "anchor": "choosing-a-model", "md_text": "This example requires using a model that understands DuckDB SQL. You can check with `clai`:\n\n```\n> clai -m bedrock:us.anthropic.claude-3-7-sonnet-20250219-v1:0\nclai - Pydantic AI CLI v0.0.1.dev920+41dd069 with bedrock:us.anthropic.claude-3-7-sonnet-20250219-v1:0\nclai ➤ do you understand duckdb sql?\n# DuckDB SQL\n\nYes, I understand DuckDB SQL. DuckDB is an in-process analytical SQL database\nthat uses syntax similar to PostgreSQL. It specializes in analytical queries\nand is designed for high-performance analysis of structured data.\n\nSome key features of DuckDB SQL include:\n\n • OLAP (Online Analytical Processing) optimized\n • Columnar-vectorized query execution\n • Standard SQL support with PostgreSQL compatibility\n • Support for complex analytical queries\n • Efficient handling of CSV/Parquet/JSON files\n\nI can help you with DuckDB SQL queries, schema design, optimization, or other\nDuckDB-related questions.\n```", "url": "https://ai.pydantic.dev/data-analyst/index.html#choosing-a-model", "page": "data-analyst/index.html", "source_site": "pydantic_ai"}
{"title": "What is Pydantic Evals?", "anchor": "what-is-pydantic-evals", "md_text": "Pydantic Evals helps you:\n\n* **Create test datasets** with type-safe structured inputs and expected outputs\n* **Run evaluations** against your AI systems with automatic concurrency\n* **Score results** using deterministic checks, LLM judges, or custom evaluators\n* **Generate reports** with detailed metrics, assertions, and performance data\n* **Track changes** by comparing evaluation runs over time\n* **Integrate with Logfire** for visualization and collaborative analysis", "url": "https://ai.pydantic.dev/quick-start/index.html#what-is-pydantic-evals", "page": "quick-start/index.html", "source_site": "pydantic_ai"}
{"title": "Installation", "anchor": "installation", "md_text": "```\npip install pydantic-evals\n```\n\nFor OpenTelemetry tracing and Logfire integration:\n\n```\npip install 'pydantic-evals[logfire]'\n```", "url": "https://ai.pydantic.dev/quick-start/index.html#installation", "page": "quick-start/index.html", "source_site": "pydantic_ai"}
{"title": "Quick Start", "anchor": "quick-start", "md_text": "While evaluations are typically used to test AI systems, the Pydantic Evals framework works with any function call. To demonstrate the core functionality, we'll start with a simple, deterministic example.\n\nHere's a complete example of evaluating a simple text transformation function:\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Contains, EqualsExpected\n\n# Create a dataset with test cases\ndataset = Dataset(\n    cases=[\n        Case(\n            name='uppercase_basic',\n            inputs='hello world',\n            expected_output='HELLO WORLD',\n        ),\n        Case(\n            name='uppercase_with_numbers',\n            inputs='hello 123',\n            expected_output='HELLO 123',\n        ),\n    ],\n    evaluators=[\n        EqualsExpected(),  # Check exact match with expected_output\n        Contains(value='HELLO', case_sensitive=True),  # Check contains \"HELLO\"\n    ],\n)\n\n\n# Define the function to evaluate\ndef uppercase_text(text: str) -> str:\n    return text.upper()\n\n\n# Run the evaluation\nreport = dataset.evaluate_sync(uppercase_text)\n\n# Print the results\nreport.print()\n\"\"\"\n        Evaluation Summary: uppercase_text\n┏━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ Case ID                ┃ Assertions ┃ Duration ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n│ uppercase_basic        │ ✔✔         │     10ms │\n├────────────────────────┼────────────┼──────────┤\n│ uppercase_with_numbers │ ✔✔         │     10ms │\n├────────────────────────┼────────────┼──────────┤\n│ Averages               │ 100.0% ✔   │     10ms │\n└────────────────────────┴────────────┴──────────┘\n\"\"\"\n```\n\nOutput:\n\n```\n                  Evaluation Summary: uppercase_text\n┏━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ Case ID                 ┃ Assertions ┃ Duration ┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n│ uppercase_basic         │ ✔✔         │     10ms │\n├─────────────────────────┼────────────┼──────────┤\n│ uppercase_with_numbers  │ ✔✔         │     10ms │\n├─────────────────────────┼────────────┼──────────┤\n│ Averages                │ 100.0% ✔   │     10ms │\n└─────────────────────────┴────────────┴──────────┘\n```", "url": "https://ai.pydantic.dev/quick-start/index.html#quick-start", "page": "quick-start/index.html", "source_site": "pydantic_ai"}
{"title": "Key Concepts", "anchor": "key-concepts", "md_text": "Understanding a few core concepts will help you get the most out of Pydantic Evals:\n\n* **[`Dataset`](../pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset)** - A collection of test cases and (optional) evaluators\n* **[`Case`](../pydantic_evals/dataset/index.html#pydantic_evals.dataset.Case)** - A single test scenario with inputs and optional expected outputs and case-specific evaluators\n* **[`Evaluator`](../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.Evaluator)** - A function that scores or validates task outputs\n* **[`EvaluationReport`](../pydantic_evals/reporting/index.html#pydantic_evals.reporting.EvaluationReport)** - Results from running an evaluation\n\nFor a deeper dive, see [Core Concepts](../core-concepts/index.html).", "url": "https://ai.pydantic.dev/quick-start/index.html#key-concepts", "page": "quick-start/index.html", "source_site": "pydantic_ai"}
{"title": "Common Use Cases", "anchor": "common-use-cases", "md_text": "", "url": "https://ai.pydantic.dev/quick-start/index.html#common-use-cases", "page": "quick-start/index.html", "source_site": "pydantic_ai"}
{"title": "Deterministic Validation", "anchor": "deterministic-validation", "md_text": "Test that your AI system produces correctly-structured outputs:\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Contains, IsInstance\n\ndataset = Dataset(\n    cases=[\n        Case(inputs={'data': 'required_key present'}, expected_output={'result': 'success'}),\n    ],\n    evaluators=[\n        IsInstance(type_name='dict'),\n        Contains(value='required_key'),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/quick-start/index.html#deterministic-validation", "page": "quick-start/index.html", "source_site": "pydantic_ai"}
{"title": "LLM-as-a-Judge Evaluation", "anchor": "llm-as-a-judge-evaluation", "md_text": "Use an LLM to evaluate subjective qualities like accuracy or helpfulness:\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\ndataset = Dataset(\n    cases=[\n        Case(inputs='What is the capital of France?', expected_output='Paris'),\n    ],\n    evaluators=[\n        LLMJudge(\n            rubric='Response is accurate and helpful',\n            include_input=True,\n            model='anthropic:claude-3-7-sonnet-latest',\n        )\n    ],\n)\n```", "url": "https://ai.pydantic.dev/quick-start/index.html#llm-as-a-judge-evaluation", "page": "quick-start/index.html", "source_site": "pydantic_ai"}
{"title": "Performance Testing", "anchor": "performance-testing", "md_text": "Ensure your system meets performance requirements:\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import MaxDuration\n\ndataset = Dataset(\n    cases=[\n        Case(inputs='test input', expected_output='test output'),\n    ],\n    evaluators=[\n        MaxDuration(seconds=2.0),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/quick-start/index.html#performance-testing", "page": "quick-start/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "md_text": "Explore the documentation to learn more:\n\n* **[Core Concepts](../core-concepts/index.html)** - Understand the data model and evaluation flow\n* **[Built-in Evaluators](../evaluators/built-in/index.html)** - Learn about all available evaluators\n* **[Custom Evaluators](../evaluators/custom/index.html)** - Write your own evaluation logic\n* **[Dataset Management](../how-to/dataset-management/index.html)** - Save, load, and generate datasets\n* **[Examples](../examples/simple-validation/index.html)** - Practical examples for common scenarios", "url": "https://ai.pydantic.dev/quick-start/index.html#next-steps", "page": "quick-start/index.html", "source_site": "pydantic_ai"}
{"title": "Screenshots", "anchor": "screenshots", "md_text": "This is what the analysis sent into Slack will look like:\n\n[![Slack message](../slack-lead-qualifier-slack.png)](../slack-lead-qualifier-slack.png)\n\nThis is what the corresponding trace in [Logfire](https://pydantic.dev/logfire) will look like:\n\n[![Logfire trace](../slack-lead-qualifier-logfire.png)](../slack-lead-qualifier-logfire.png)\n\nAll of these entries can be clicked on to get more details about what happened at that step, including the full conversation with the LLM and HTTP requests and responses.", "url": "https://ai.pydantic.dev/slack-lead-qualifier/index.html#screenshots", "page": "slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Prerequisites", "anchor": "prerequisites", "md_text": "If you just want to see the code without actually going through the effort of setting up the bits necessary to run it, feel free to [jump ahead](index.html#the-code).", "url": "https://ai.pydantic.dev/slack-lead-qualifier/index.html#prerequisites", "page": "slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Slack app", "anchor": "slack-app", "md_text": "You need to have a Slack workspace and the necessary permissions to create apps.\n\n2. Create a new Slack app using the instructions at <https://docs.slack.dev/quickstart>.\n   1. In step 2, \"Requesting scopes\", request the following scopes:\n      * [`users.read`](https://docs.slack.dev/reference/scopes/users.read)\n      * [`users.read.email`](https://docs.slack.dev/reference/scopes/users.read.email)\n      * [`users.profile.read`](https://docs.slack.dev/reference/scopes/users.profile.read)\n   2. In step 3, \"Installing and authorizing the app\", note down the Access Token as we're going to need to store it as a Secret in Modal.\n   3. You can skip steps 4 and 5. We're going to need to subscribe to the `team_join` event, but at this point you don't have a webhook URL yet.\n3. Create the channels the app will post into, and add the Slack app to them:\n\n   * `#new-slack-leads`\n   * `#daily-slack-leads-summary`\n\n   These names are hard-coded in the example. If you want to use different channels, you can clone the repo and change them in `examples/pydantic_examples/slack_lead_qualifier/functions.py`.", "url": "https://ai.pydantic.dev/slack-lead-qualifier/index.html#slack-app", "page": "slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Logfire Write Token", "anchor": "logfire-write-token", "md_text": "1. If you don't have a Logfire account yet, create one on <https://logfire-us.pydantic.dev/>.\n2. Create a new project named, for example, `slack-lead-qualifier`.\n3. Generate a new Write Token and note it down, as we're going to need to store it as a Secret in Modal.", "url": "https://ai.pydantic.dev/slack-lead-qualifier/index.html#logfire-write-token", "page": "slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAI API Key", "anchor": "openai-api-key", "md_text": "1. If you don't have an OpenAI account yet, create one on <https://platform.openai.com/>.\n2. Create a new API Key in Settings and note it down, as we're going to need to store it as a Secret in Modal.", "url": "https://ai.pydantic.dev/slack-lead-qualifier/index.html#openai-api-key", "page": "slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Modal account", "anchor": "modal-account", "md_text": "1. If you don't have a Modal account yet, create one on <https://modal.com/signup>.\n2. Create 3 Secrets of type \"Custom\" on <https://modal.com/secrets>:\n   * Name: `slack`, key: `SLACK_API_KEY`, value: the Slack Access Token you generated earlier\n   * Name: `logfire`, key: `LOGFIRE_TOKEN`, value: the Logfire Write Token you generated earlier\n   * Name: `openai`, key: `OPENAI_API_KEY`, value: the OpenAI API Key you generated earlier", "url": "https://ai.pydantic.dev/slack-lead-qualifier/index.html#modal-account", "page": "slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "md_text": "1. Make sure you have the [dependencies installed](../setup/index.html#usage).\n2. Authenticate with Modal:\n\n   ```\n   python/uv-run -m modal setup\n   ```\n3. Run the example as an [ephemeral Modal app](https://modal.com/docs/guide/apps#ephemeral-apps), meaning it will only run until you quit it using Ctrl+C:\n\n   ```\n   python/uv-run -m modal serve -m pydantic_ai_examples.slack_lead_qualifier.modal\n   ```\n4. Note down the URL after `Created web function web_app =>`, this is your webhook endpoint URL.\n5. Go back to <https://docs.slack.dev/quickstart> and follow step 4, \"Configuring the app for event listening\", to subscribe to the `team_join` event with the webhook endpoint URL you noted down as the Request URL.\n\nNow when someone new (possibly you with a throwaway email) joins the Slack workspace, you'll see the webhook event being processed in the terminal where you ran `modal serve` and in the Logfire Live view, and after waiting a few seconds you should see the result appear in the `#new-slack-leads` Slack channel!\n\nYou can also fake a Slack signup event and try out the agent like this, with any name or email you please:\n\n```\ncurl -X POST <webhook endpoint URL> \\\n-H \"Content-Type: application/json\" \\\n-d '{\n    \"type\": \"event_callback\",\n    \"event\": {\n        \"type\": \"team_join\",\n        \"user\": {\n            \"profile\": {\n                \"email\": \"samuel@pydantic.dev\",\n                \"first_name\": \"Samuel\",\n                \"last_name\": \"Colvin\",\n                \"display_name\": \"Samuel Colvin\"\n            }\n        }\n    }\n}'\n```\n\nIf you'd like to deploy this app into your Modal workspace in a persistent fashion, you can use this command:\n\n```\npython/uv-run -m modal deploy -m pydantic_ai_examples.slack_lead_qualifier.modal\n```\n\nYou'll likely want to [download the code](https://github.com/pydantic/pydantic-ai/tree/main/examples/pydantic_ai_examples/slack_lead_qualifier) first, put it in a new repo, and then do [continuous deployment](https://modal.com/docs/guide/continuous-deployment#github-actions) using GitHub Actions.\n\nDon't forget to update the Slack event request URL to the new persistent URL! You'll also want to modify the [instructions for the agent](index.html#agent) to your own situation.", "url": "https://ai.pydantic.dev/slack-lead-qualifier/index.html#usage", "page": "slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "The code", "anchor": "the-code", "md_text": "We're going to start with the basics, and then gradually build up into the full app.", "url": "https://ai.pydantic.dev/slack-lead-qualifier/index.html#the-code", "page": "slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Models", "anchor": "models", "md_text": "#### `Profile`\n\nFirst, we define a [Pydantic](https://docs.pydantic.dev) model that represents a Slack user profile. These are the fields we get from the [`team_join`](https://docs.slack.dev/reference/events/team_join) event that's sent to the webhook endpoint that we'll define in a bit.\n\n[slack\\_lead\\_qualifier/models.py (L11-L15)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/models.py#L11-L15)\n\n```\n...\n\nclass Profile(BaseModel):\n    first_name: str | None = None\n    last_name: str | None = None\n    display_name: str | None = None\n    email: str\n\n...\n```\n\nWe also define a `Profile.as_prompt()` helper method that uses [`format_as_xml`](../format_prompt/index.html#pydantic_ai.format_prompt.format_as_xml) to turn the profile into a string that can be sent to the model.\n\n[slack\\_lead\\_qualifier/models.py (L7-L19)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/models.py#L7-L19)\n\n```\n...\n\nfrom pydantic_ai import format_as_xml\n\n...\n\nclass Profile(BaseModel):\n\n...\n\n    def as_prompt(self) -> str:\n        return format_as_xml(self, root_tag='profile')\n\n...\n```\n\n#### `Analysis`\n\nThe second model we'll need represents the result of the analysis that the agent will perform. We include docstrings to provide additional context to the model on what these fields should contain.\n\n[slack\\_lead\\_qualifier/models.py (L23-L31)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/models.py#L23-L31)\n\n```\n...\n\nclass Analysis(BaseModel):\n    profile: Profile\n    organization_name: str\n    organization_domain: str\n    job_title: str\n    relevance: Annotated[int, Ge(1), Le(5)]\n    \"\"\"Estimated fit for Pydantic Logfire: 1 = low, 5 = high\"\"\"\n    summary: str\n    \"\"\"One-sentence welcome note summarising who they are and how we might help\"\"\"\n\n...\n```\n\nWe also define a `Analysis.as_slack_blocks()` helper method that turns the analysis into some [Slack blocks](https://api.slack.com/reference/block-kit/blocks) that can be sent to the Slack API to post a new message.\n\n[slack\\_lead\\_qualifier/models.py (L23-L46)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/models.py#L23-L46)\n\n```\n...\n\nclass Analysis(BaseModel):\n\n...\n\n    def as_slack_blocks(self, include_relevance: bool = False) -> list[dict[str, Any]]:\n        profile = self.profile\n        relevance = f'({self.relevance}/5)' if include_relevance else ''\n        return [\n            {\n                'type': 'markdown',\n                'text': f'[{profile.display_name}](mailto:{profile.email}), {self.job_title} at [**{self.organization_name}**](https://{self.organization_domain}) {relevance}',\n            },\n            {\n                'type': 'markdown',\n                'text': self.summary,\n            },\n        ]\n```", "url": "https://ai.pydantic.dev/slack-lead-qualifier/index.html#models", "page": "slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Agent", "anchor": "agent", "md_text": "Now it's time to get into Pydantic AI and define the agent that will do the actual analysis!\n\nWe specify the model we'll use (`openai:gpt-4o`), provide [instructions](https://ai.pydantic.dev/agents/#instructions), give the agent access to the [DuckDuckGo search tool](https://ai.pydantic.dev/common-tools/#duckduckgo-search-tool), and tell it to output either an `Analysis` or `None` using the [Native Output](https://ai.pydantic.dev/output/#native-output) structured output mode.\n\nThe real meat of the app is in the instructions that tell the agent how to evaluate each new Slack member. If you plan to use this app yourself, you'll of course want to modify them to your own situation.\n\n[slack\\_lead\\_qualifier/agent.py (L7-L40)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/agent.py#L7-L40)\n\n```\n...\n\nfrom pydantic_ai import Agent, NativeOutput\nfrom pydantic_ai.common_tools.duckduckgo import duckduckgo_search_tool\n\n...\n\nagent = Agent(\n    'openai:gpt-4o',\n    instructions=dedent(\n        \"\"\"\n        When a new person joins our public Slack, please put together a brief snapshot so we can be most useful to them.\n\n        **What to include**\n\n        1. **Who they are:**  Any details about their professional role or projects (e.g. LinkedIn, GitHub, company bio).\n        2. **Where they work:**  Name of the organisation and its domain.\n        3. **How we can help:**  On a scale of 1–5, estimate how likely they are to benefit from **Pydantic Logfire**\n           (our paid observability tool) based on factors such as company size, product maturity, or AI usage.\n           *1 = probably not relevant, 5 = very strong fit.*\n\n        **Our products (for context only)**\n        • **Pydantic Validation** – Python data-validation (open source)\n        • **Pydantic AI** – Python agent framework (open source)\n        • **Pydantic Logfire** – Observability for traces, logs & metrics with first-class AI support (commercial)\n\n        **How to research**\n\n        • Use the provided DuckDuckGo search tool to research the person and the organization they work for, based on the email domain or what you find on e.g. LinkedIn and GitHub.\n        • If you can't find enough to form a reasonable view, return **None**.\n        \"\"\"\n    ),\n    tools=[duckduckgo_search_tool()],\n    output_type=NativeOutput([Analysis, NoneType]),\n)\n\n...\n```\n\n#### `analyze_profile`\n\nWe also define a `analyze_profile` helper function that takes a `Profile`, runs the agent, and returns an `Analysis` (or `None`), and instrument it using [Logfire](https://ai.pydantic.dev/logfire/).\n\n[slack\\_lead\\_qualifier/agent.py (L44-L47)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/agent.py#L44-L47)\n\n```\n...\n\n@logfire.instrument('Analyze profile')\nasync def analyze_profile(profile: Profile) -> Analysis | None:\n    result = await agent.run(profile.as_prompt())\n    return result.output\n```", "url": "https://ai.pydantic.dev/slack-lead-qualifier/index.html#agent", "page": "slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Analysis store", "anchor": "analysis-store", "md_text": "The next building block we'll need is a place to store all the analyses that have been done so that we can look them up when we send the daily summary.\n\nFortunately, Modal provides us with a convenient way to store some data that can be read back in a subsequent Modal run (webhook or scheduled): [`modal.Dict`](https://modal.com/docs/reference/modal.Dict).\n\nWe define some convenience methods to easily add, list, and clear analyses.\n\n[slack\\_lead\\_qualifier/store.py (L4-L31)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/store.py#L4-L31)\n\n```\n...\n\nimport modal\n\n...\n\nclass AnalysisStore:\n    @classmethod\n    @logfire.instrument('Add analysis to store')\n    async def add(cls, analysis: Analysis):\n        await cls._get_store().put.aio(analysis.profile.email, analysis.model_dump())\n\n    @classmethod\n    @logfire.instrument('List analyses from store')\n    async def list(cls) -> list[Analysis]:\n        return [\n            Analysis.model_validate(analysis)\n            async for analysis in cls._get_store().values.aio()\n        ]\n\n    @classmethod\n    @logfire.instrument('Clear analyses from store')\n    async def clear(cls):\n        await cls._get_store().clear.aio()\n\n    @classmethod\n    def _get_store(cls) -> modal.Dict:\n        return modal.Dict.from_name('analyses', create_if_missing=True)  # type: ignore\n```\n\nNote that `# type: ignore` on the last line -- unfortunately `modal` does not fully define its types, so we need this to stop our static type checker `pyright`, which we run over all Pydantic AI code including examples, from complaining.", "url": "https://ai.pydantic.dev/slack-lead-qualifier/index.html#analysis-store", "page": "slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Send Slack message", "anchor": "send-slack-message", "md_text": "Next, we'll need a way to actually send a Slack message, so we define a simple function that uses Slack's [`chat.postMessage`](https://api.slack.com/methods/chat.postMessage) API.\n\n[slack\\_lead\\_qualifier/slack.py (L8-L30)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/slack.py#L8-L30)\n\n```\n...\n\nAPI_KEY = os.getenv('SLACK_API_KEY')\nassert API_KEY, 'SLACK_API_KEY is not set'\n\n\n@logfire.instrument('Send Slack message')\nasync def send_slack_message(channel: str, blocks: list[dict[str, Any]]):\n    client = httpx.AsyncClient()\n    response = await client.post(\n        'https://slack.com/api/chat.postMessage',\n        json={\n            'channel': channel,\n            'blocks': blocks,\n        },\n        headers={\n            'Authorization': f'Bearer {API_KEY}',\n        },\n        timeout=5,\n    )\n    response.raise_for_status()\n    result = response.json()\n    if not result.get('ok', False):\n        error = result.get('error', 'Unknown error')\n        raise Exception(f'Failed to send to Slack: {error}')\n```", "url": "https://ai.pydantic.dev/slack-lead-qualifier/index.html#send-slack-message", "page": "slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Features", "anchor": "features", "md_text": "Now we can start putting these building blocks together to implement the actual features we want!\n\n#### `process_slack_member`\n\nThis function takes a [`Profile`](index.html#profile), [analyzes](index.html#analyze_profile) it using the agent, adds it to the [`AnalysisStore`](index.html#analysis-store), and [sends](index.html#send-slack-message) the analysis into the `#new-slack-leads` channel.\n\n[slack\\_lead\\_qualifier/functions.py (L4-L45)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/functions.py#L4-L45)\n\n```\n...\n\nfrom .agent import analyze_profile\nfrom .models import Profile\n\nfrom .slack import send_slack_message\nfrom .store import AnalysisStore\n\n...\n\nNEW_LEAD_CHANNEL = '#new-slack-leads'\n\n...\n\n@logfire.instrument('Process Slack member')\nasync def process_slack_member(profile: Profile):\n    analysis = await analyze_profile(profile)\n    logfire.info('Analysis', analysis=analysis)\n\n    if analysis is None:\n        return\n\n    await AnalysisStore().add(analysis)\n\n    await send_slack_message(\n        NEW_LEAD_CHANNEL,\n        [\n            {\n                'type': 'header',\n                'text': {\n                    'type': 'plain_text',\n                    'text': f'New Slack member with score {analysis.relevance}/5',\n                },\n            },\n            {\n                'type': 'divider',\n            },\n            *analysis.as_slack_blocks(),\n        ],\n    )\n\n...\n```\n\n#### `send_daily_summary`\n\nThis function list all of the analyses in the [`AnalysisStore`](index.html#analysis-store), takes the top 5 by relevance, [sends](index.html#send-slack-message) them into the `#daily-slack-leads-summary` channel, and clears the `AnalysisStore` so that the next daily run won't process these analyses again.\n\n[slack\\_lead\\_qualifier/functions.py (L8-L85)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/functions.py#L8-L85)\n\n```\n...\n\nfrom .slack import send_slack_message\nfrom .store import AnalysisStore\n\n...\n\nDAILY_SUMMARY_CHANNEL = '#daily-slack-leads-summary'\n\n...\n\n@logfire.instrument('Send daily summary')\nasync def send_daily_summary():\n    analyses = await AnalysisStore().list()\n    logfire.info('Analyses', analyses=analyses)\n\n    if len(analyses) == 0:\n        return\n\n    sorted_analyses = sorted(analyses, key=lambda x: x.relevance, reverse=True)\n    top_analyses = sorted_analyses[:5]\n\n    blocks = [\n        {\n            'type': 'header',\n            'text': {\n                'type': 'plain_text',\n                'text': f'Top {len(top_analyses)} new Slack members from the last 24 hours',\n            },\n        },\n    ]\n\n    for analysis in top_analyses:\n        blocks.extend(\n            [\n                {\n                    'type': 'divider',\n                },\n                *analysis.as_slack_blocks(include_relevance=True),\n            ]\n        )\n\n    await send_slack_message(\n        DAILY_SUMMARY_CHANNEL,\n        blocks,\n    )\n\n    await AnalysisStore().clear()\n```", "url": "https://ai.pydantic.dev/slack-lead-qualifier/index.html#features", "page": "slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Web app", "anchor": "web-app", "md_text": "As it stands, neither of these functions are actually being called from anywhere.\n\nLet's implement a [FastAPI](https://fastapi.tiangolo.com/) endpoint to handle the `team_join` Slack webhook (also known as the [Slack Events API](https://docs.slack.dev/apis/events-api)) and call the [`process_slack_member`](index.html#process_slack_member) function we just defined. We also instrument FastAPI using Logfire for good measure.\n\n[slack\\_lead\\_qualifier/app.py (L20-L36)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/app.py#L20-L36)\n\n```\n...\n\napp = FastAPI()\nlogfire.instrument_fastapi(app, capture_headers=True)\n\n\n@app.post('/')\nasync def process_webhook(payload: dict[str, Any]) -> dict[str, Any]:\n    if payload['type'] == 'url_verification':\n        return {'challenge': payload['challenge']}\n    elif (\n        payload['type'] == 'event_callback' and payload['event']['type'] == 'team_join'\n    ):\n        profile = Profile.model_validate(payload['event']['user']['profile'])\n\n        process_slack_member(profile)\n        return {'status': 'OK'}\n\n    raise HTTPException(status_code=status.HTTP_422_UNPROCESSABLE_ENTITY)\n```\n\n#### `process_slack_member` with Modal\n\nI was a little sneaky there -- we're not actually calling the [`process_slack_member`](index.html#process_slack_member) function we defined in `functions.py` directly, as Slack requires webhooks to respond within 3 seconds, and we need a bit more time than that to talk to the LLM, do some web searches, and send the Slack message.\n\nInstead, we're calling the following function defined alongside the app, which uses Modal's [`modal.Function.spawn`](https://modal.com/docs/reference/modal.Function#spawn) feature to run a function in the background. (If you're curious what the Modal side of this function looks like, you can [jump ahead](index.html#backgrounded-process_slack_member).)\n\nBecause `modal.py` (which we'll see in the next section) imports `app.py`, we import from `modal.py` inside the function definition because doing so at the top level would have resulted in a circular import error.\n\nWe also pass along the current Logfire context to get [Distributed Tracing](https://logfire.pydantic.dev/docs/how-to-guides/distributed-tracing/), meaning that the background function execution will show up nested under the webhook request trace, so that we have everything related to that request in one place.\n\n[slack\\_lead\\_qualifier/app.py (L11-L16)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/app.py#L11-L16)\n\n```\n...\n\ndef process_slack_member(profile: Profile):\n    from .modal import process_slack_member as _process_slack_member\n\n    _process_slack_member.spawn(\n        profile.model_dump(), logfire_ctx=get_context()\n    )\n\n...\n```", "url": "https://ai.pydantic.dev/slack-lead-qualifier/index.html#web-app", "page": "slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Modal app", "anchor": "modal-app", "md_text": "Now let's see how easy Modal makes it to deploy all of this.\n\n#### Set up Modal\n\nThe first thing we do is define the Modal app, by specifying the base image to use (Debian with Python 3.13), all the Python packages it needs, and all of the secrets defined in the Modal interface that need to be made available during runtime.\n\n[slack\\_lead\\_qualifier/modal.py (L4-L21)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/modal.py#L4-L21)\n\n```\n...\n\nimport modal\n\nimage = modal.Image.debian_slim(python_version='3.13').pip_install(\n    'pydantic',\n    'pydantic_ai_slim[openai,duckduckgo]',\n    'logfire[httpx,fastapi]',\n    'fastapi[standard]',\n    'httpx',\n)\napp = modal.App(\n    name='slack-lead-qualifier',\n    image=image,\n    secrets=[\n        modal.Secret.from_name('logfire'),\n        modal.Secret.from_name('openai'),\n        modal.Secret.from_name('slack'),\n    ],\n)\n\n...\n```\n\n#### Set up Logfire\n\nNext, we define a function to set up Logfire instrumentation for Pydantic AI and HTTPX.\n\nWe cannot do this at the top level of the file, as the requested packages (like `logfire`) will only be available within functions running on Modal (like the ones we'll define next). This file, `modal.py`, runs on your local machine and only has access to the `modal` package.\n\n[slack\\_lead\\_qualifier/modal.py (L25-L30)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/modal.py#L25-L30)\n\n```\n...\n\ndef setup_logfire():\n    import logfire\n\n    logfire.configure(service_name=app.name)\n    logfire.instrument_pydantic_ai()\n    logfire.instrument_httpx(capture_all=True)\n\n...\n```\n\n#### Web app\n\nTo deploy a [web endpoint](https://modal.com/docs/guide/webhooks) on Modal, we simply define a function that returns an ASGI app (like FastAPI) and decorate it with `@app.function()` and `@modal.asgi_app()`.\n\nThis `web_app` function will be run on Modal, so inside the function we can call the `setup_logfire` function that requires the `logfire` package, and import `app.py` which uses the other requested packages.\n\nBy default, Modal spins up a container to handle a function call (like a web request) on-demand, meaning there's a little bit of startup time to each request. However, Slack requires webhooks to respond within 3 seconds, so we specify `min_containers=1` to keep the web endpoint running and ready to answer requests at all times. This is a bit annoying and wasteful, but fortunately [Modal's pricing](https://modal.com/pricing) is pretty reasonable, you get $30 free monthly compute, and they offer up to $50k in free credits for startup and academic researchers.\n\n[slack\\_lead\\_qualifier/modal.py (L34-L41)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/modal.py#L34-L41)\n\n```\n...\n\n@app.function(min_containers=1)\n@modal.asgi_app()  # type: ignore\ndef web_app():\n    setup_logfire()\n\n    from .app import app as _app\n\n    return _app\n\n...\n```\n\nNote that `# type: ignore` on the `@modal.asgi_app()` line -- unfortunately `modal` does not fully define its types, so we need this to stop our static type checker `pyright`, which we run over all Pydantic AI code including examples, from complaining.\n\n#### Scheduled `send_daily_summary`\n\nTo define a [scheduled function](https://modal.com/docs/guide/cron), we can use the `@app.function()` decorator with a `schedule` argument. This Modal function will call our imported [`send_daily_summary`](index.html#send_daily_summary) function every day at 8 am UTC.\n\n[slack\\_lead\\_qualifier/modal.py (L60-L66)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/modal.py#L60-L66)\n\n```\n...\n\n@app.function(schedule=modal.Cron('0 8 * * *'))  # Every day at 8am UTC\nasync def send_daily_summary():\n    setup_logfire()\n\n    from .functions import send_daily_summary as _send_daily_summary\n\n    await _send_daily_summary()\n```\n\n#### Backgrounded `process_slack_member`\n\nFinally, we define a Modal function that wraps our [`process_slack_member`](index.html#process_slack_member) function, so that it can run in the background.\n\nAs you'll remember from when we [spawned this function from the web app](index.html#process_slack_member-with-modal), we passed along the Logfire context to get [Distributed Tracing](https://logfire.pydantic.dev/docs/how-to-guides/distributed-tracing/), so we need to attach it here.\n\n[slack\\_lead\\_qualifier/modal.py (L45-L56)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/modal.py#L45-L56)\n\n```\n...\n\n@app.function()\nasync def process_slack_member(profile_raw: dict[str, Any], logfire_ctx: Any):\n    setup_logfire()\n\n    from logfire.propagate import attach_context", "url": "https://ai.pydantic.dev/slack-lead-qualifier/index.html#modal-app", "page": "slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Modal app", "anchor": "modal-app", "md_text": "    from .functions import process_slack_member as _process_slack_member\n    from .models import Profile\n\n    with attach_context(logfire_ctx):\n        profile = Profile.model_validate(profile_raw)\n        await _process_slack_member(profile)\n\n...\n```", "url": "https://ai.pydantic.dev/slack-lead-qualifier/index.html#modal-app", "page": "slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Conclusion", "anchor": "conclusion", "md_text": "And that's it! Now, assuming you've met the [prerequisites](index.html#prerequisites), you can run or deploy the app using the commands under [usage](index.html#usage).", "url": "https://ai.pydantic.dev/slack-lead-qualifier/index.html#conclusion", "page": "slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "md_text": "With [dependencies installed and environment variables set](../setup/index.html#usage), run:\n\npipuv\n\n```\npython -m pydantic_ai_examples.stream_whales\n\nuv run -m pydantic_ai_examples.stream_whales\n```\n\nShould give an output like this:", "url": "https://ai.pydantic.dev/stream-whales/index.html#running-the-example", "page": "stream-whales/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "md_text": "[stream\\_whales.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/stream_whales.py)\n\n```\n\"\"\"Information about whales — an example of streamed structured response validation.\n\nThis script streams structured responses from GPT-4 about whales, validates the data\nand displays it as a dynamic table using Rich as the data is received.\n\nRun with:\n\n    uv run -m pydantic_ai_examples.stream_whales\n\"\"\"\n\nfrom typing import Annotated\n\nimport logfire\nfrom pydantic import Field\nfrom rich.console import Console\nfrom rich.live import Live\nfrom rich.table import Table\nfrom typing_extensions import NotRequired, TypedDict\n\nfrom pydantic_ai import Agent\n\n# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\nlogfire.configure(send_to_logfire='if-token-present')\nlogfire.instrument_pydantic_ai()\n\n\nclass Whale(TypedDict):\n    name: str\n    length: Annotated[\n        float, Field(description='Average length of an adult whale in meters.')\n    ]\n    weight: NotRequired[\n        Annotated[\n            float,\n            Field(description='Average weight of an adult whale in kilograms.', ge=50),\n        ]\n    ]\n    ocean: NotRequired[str]\n    description: NotRequired[Annotated[str, Field(description='Short Description')]]\n\n\nagent = Agent('openai:gpt-4', output_type=list[Whale])\n\n\nasync def main():\n    console = Console()\n    with Live('\\n' * 36, console=console) as live:\n        console.print('Requesting data...', style='cyan')\n        async with agent.run_stream(\n            'Generate me details of 5 species of Whale.'\n        ) as result:\n            console.print('Response:', style='green')\n\n            async for whales in result.stream_output(debounce_by=0.01):\n                table = Table(\n                    title='Species of Whale',\n                    caption='Streaming Structured responses from GPT-4',\n                    width=120,\n                )\n                table.add_column('ID', justify='right')\n                table.add_column('Name')\n                table.add_column('Avg. Length (m)', justify='right')\n                table.add_column('Avg. Weight (kg)', justify='right')\n                table.add_column('Ocean')\n                table.add_column('Description', justify='right')\n\n                for wid, whale in enumerate(whales, start=1):\n                    table.add_row(\n                        str(wid),\n                        whale['name'],\n                        f'{whale[\"length\"]:0.0f}',\n                        f'{w:0.0f}' if (w := whale.get('weight')) else '…',\n                        whale.get('ocean') or '…',\n                        whale.get('description') or '…',\n                    )\n                live.update(table)\n\n\nif __name__ == '__main__':\n    import asyncio\n\n    asyncio.run(main())\n```", "url": "https://ai.pydantic.dev/stream-whales/index.html#example-code", "page": "stream-whales/index.html", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "md_text": "With [dependencies installed and environment variables set](../setup/index.html#usage), run:\n\npipuv\n\n```\npython -m pydantic_ai_examples.chat_app\n\nuv run -m pydantic_ai_examples.chat_app\n```\n\nThen open the app at [localhost:8000](http://localhost:8000).\n\n[![Example conversation](../chat-app-example.png)](../chat-app-example.png)", "url": "https://ai.pydantic.dev/chat-app/index.html#running-the-example", "page": "chat-app/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "md_text": "Python code that runs the chat app:\n\n[chat\\_app.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/chat_app.py)\n\n```\n\"\"\"Simple chat app example build with FastAPI.\n\nRun with:\n\n    uv run -m pydantic_ai_examples.chat_app\n\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport asyncio\nimport json\nimport sqlite3\nfrom collections.abc import AsyncIterator, Callable\nfrom concurrent.futures.thread import ThreadPoolExecutor\nfrom contextlib import asynccontextmanager\nfrom dataclasses import dataclass\nfrom datetime import datetime, timezone\nfrom functools import partial\nfrom pathlib import Path\nfrom typing import Annotated, Any, Literal, TypeVar\n\nimport fastapi\nimport logfire\nfrom fastapi import Depends, Request\nfrom fastapi.responses import FileResponse, Response, StreamingResponse\nfrom typing_extensions import LiteralString, ParamSpec, TypedDict\n\nfrom pydantic_ai import (\n    Agent,\n    ModelMessage,\n    ModelMessagesTypeAdapter,\n    ModelRequest,\n    ModelResponse,\n    TextPart,\n    UnexpectedModelBehavior,\n    UserPromptPart,\n)\n\n# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\nlogfire.configure(send_to_logfire='if-token-present')\nlogfire.instrument_pydantic_ai()\n\nagent = Agent('openai:gpt-4o')\nTHIS_DIR = Path(__file__).parent\n\n\n@asynccontextmanager\nasync def lifespan(_app: fastapi.FastAPI):\n    async with Database.connect() as db:\n        yield {'db': db}\n\n\napp = fastapi.FastAPI(lifespan=lifespan)\nlogfire.instrument_fastapi(app)\n\n\n@app.get('/')\nasync def index() -> FileResponse:\n    return FileResponse((THIS_DIR / 'chat_app.html'), media_type='text/html')\n\n\n@app.get('/chat_app.ts')\nasync def main_ts() -> FileResponse:\n    \"\"\"Get the raw typescript code, it's compiled in the browser, forgive me.\"\"\"\n    return FileResponse((THIS_DIR / 'chat_app.ts'), media_type='text/plain')\n\n\nasync def get_db(request: Request) -> Database:\n    return request.state.db\n\n\n@app.get('/chat/')\nasync def get_chat(database: Database = Depends(get_db)) -> Response:\n    msgs = await database.get_messages()\n    return Response(\n        b'\\n'.join(json.dumps(to_chat_message(m)).encode('utf-8') for m in msgs),\n        media_type='text/plain',\n    )\n\n\nclass ChatMessage(TypedDict):\n    \"\"\"Format of messages sent to the browser.\"\"\"\n\n    role: Literal['user', 'model']\n    timestamp: str\n    content: str\n\n\ndef to_chat_message(m: ModelMessage) -> ChatMessage:\n    first_part = m.parts[0]\n    if isinstance(m, ModelRequest):\n        if isinstance(first_part, UserPromptPart):\n            assert isinstance(first_part.content, str)\n            return {\n                'role': 'user',\n                'timestamp': first_part.timestamp.isoformat(),\n                'content': first_part.content,\n            }\n    elif isinstance(m, ModelResponse):\n        if isinstance(first_part, TextPart):\n            return {\n                'role': 'model',\n                'timestamp': m.timestamp.isoformat(),\n                'content': first_part.content,\n            }\n    raise UnexpectedModelBehavior(f'Unexpected message type for chat app: {m}')\n\n\n@app.post('/chat/')\nasync def post_chat(\n    prompt: Annotated[str, fastapi.Form()], database: Database = Depends(get_db)\n) -> StreamingResponse:\n    async def stream_messages():\n        \"\"\"Streams new line delimited JSON `Message`s to the client.\"\"\"\n        # stream the user prompt so that can be displayed straight away\n        yield (\n            json.dumps(\n                {\n                    'role': 'user',\n                    'timestamp': datetime.now(tz=timezone.utc).isoformat(),\n                    'content': prompt,\n                }\n            ).encode('utf-8')\n            + b'\\n'\n        )\n        # get the chat history so far to pass as context to the agent\n        messages = await database.get_messages()\n        # run the agent with the user prompt and the chat history\n        async with agent.run_stream(prompt, message_history=messages) as result:\n            async for text in result.stream_output(debounce_by=0.01):\n                # text here is a `str` and the frontend wants\n                # JSON encoded ModelResponse, so we create one\n                m = ModelResponse(parts=[TextPart(text)], timestamp=result.timestamp())\n                yield json.dumps(to_chat_message(m)).encode('utf-8') + b'\\n'\n\n        # add new messages (e.g. the user prompt and the agent response in this case) to the database\n        await database.add_messages(result.new_messages_json())\n\n    return StreamingResponse(stream_messages(), media_type='text/plain')\n\n\nP = ParamSpec('P')\nR = TypeVar('R')\n\n\n@dataclass\nclass Database:\n    \"\"\"Rudimentary database to store chat messages in SQLite.\n\n    The SQLite standard library package is synchronous, so we\n    use a thread pool executor to run queries asynchronously.\n    \"\"\"\n\n    con: sqlite3.Connection\n    _loop: asyncio.AbstractEventLoop\n    _executor: ThreadPoolExecutor", "url": "https://ai.pydantic.dev/chat-app/index.html#example-code", "page": "chat-app/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "md_text": "    @classmethod\n    @asynccontextmanager\n    async def connect(\n        cls, file: Path = THIS_DIR / '.chat_app_messages.sqlite'\n    ) -> AsyncIterator[Database]:\n        with logfire.span('connect to DB'):\n            loop = asyncio.get_event_loop()\n            executor = ThreadPoolExecutor(max_workers=1)\n            con = await loop.run_in_executor(executor, cls._connect, file)\n            slf = cls(con, loop, executor)\n        try:\n            yield slf\n        finally:\n            await slf._asyncify(con.close)\n\n    @staticmethod\n    def _connect(file: Path) -> sqlite3.Connection:\n        con = sqlite3.connect(str(file))\n        con = logfire.instrument_sqlite3(con)\n        cur = con.cursor()\n        cur.execute(\n            'CREATE TABLE IF NOT EXISTS messages (id INT PRIMARY KEY, message_list TEXT);'\n        )\n        con.commit()\n        return con\n\n    async def add_messages(self, messages: bytes):\n        await self._asyncify(\n            self._execute,\n            'INSERT INTO messages (message_list) VALUES (?);',\n            messages,\n            commit=True,\n        )\n        await self._asyncify(self.con.commit)\n\n    async def get_messages(self) -> list[ModelMessage]:\n        c = await self._asyncify(\n            self._execute, 'SELECT message_list FROM messages order by id'\n        )\n        rows = await self._asyncify(c.fetchall)\n        messages: list[ModelMessage] = []\n        for row in rows:\n            messages.extend(ModelMessagesTypeAdapter.validate_json(row[0]))\n        return messages\n\n    def _execute(\n        self, sql: LiteralString, *args: Any, commit: bool = False\n    ) -> sqlite3.Cursor:\n        cur = self.con.cursor()\n        cur.execute(sql, args)\n        if commit:\n            self.con.commit()\n        return cur\n\n    async def _asyncify(\n        self, func: Callable[P, R], *args: P.args, **kwargs: P.kwargs\n    ) -> R:\n        return await self._loop.run_in_executor(  # type: ignore\n            self._executor,\n            partial(func, **kwargs),\n            *args,  # type: ignore\n        )\n\n\nif __name__ == '__main__':\n    import uvicorn\n\n    uvicorn.run(\n        'pydantic_ai_examples.chat_app:app', reload=True, reload_dirs=[str(THIS_DIR)]\n    )\n```\n\nSimple HTML page to render the app:\n\n[chat\\_app.html](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/chat_app.html)\n\n```\n<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n  <meta charset=\"UTF-8\">\n  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n  <title>Chat App</title>\n  <link href=\"https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css\" rel=\"stylesheet\">\n  <style>\n    main {\n      max-width: 700px;\n    }\n    #conversation .user::before {\n      content: 'You asked: ';\n      font-weight: bold;\n      display: block;\n    }\n    #conversation .model::before {\n      content: 'AI Response: ';\n      font-weight: bold;\n      display: block;\n    }\n    #spinner {\n      opacity: 0;\n      transition: opacity 500ms ease-in;\n      width: 30px;\n      height: 30px;\n      border: 3px solid #222;\n      border-bottom-color: transparent;\n      border-radius: 50%;\n      animation: rotation 1s linear infinite;\n    }\n    @keyframes rotation {\n      0% { transform: rotate(0deg); }\n      100% { transform: rotate(360deg); }\n    }\n    #spinner.active {\n      opacity: 1;\n    }\n  </style>\n</head>\n<body>\n  <main class=\"border rounded mx-auto my-5 p-4\">\n    <h1>Chat App</h1>\n    <p>Ask me anything...</p>\n    <div id=\"conversation\" class=\"px-2\"></div>\n    <div class=\"d-flex justify-content-center mb-3\">\n      <div id=\"spinner\"></div>\n    </div>\n    <form method=\"post\">\n      <input id=\"prompt-input\" name=\"prompt\" class=\"form-control\"/>\n      <div class=\"d-flex justify-content-end\">\n        <button class=\"btn btn-primary mt-2\">Send</button>\n      </div>\n    </form>\n    <div id=\"error\" class=\"d-none text-danger\">\n      Error occurred, check the browser developer console for more information.\n    </div>\n  </main>\n</body>\n</html>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/typescript/5.6.3/typescript.min.js\" crossorigin=\"anonymous\" referrerpolicy=\"no-referrer\"></script>\n<script type=\"module\">\n  // to let me write TypeScript, without adding the burden of npm we do a dirty, non-production-ready hack\n  // and transpile the TypeScript code in the browser\n  // this is (arguably) A neat demo trick, but not suitable for production!\n  async function loadTs() {\n    const response = await fetch('/chat_app.ts');\n    const tsCode = await response.text();\n    const jsCode = window.ts.transpile(tsCode, { target: \"es2015\" });\n    let script = document.createElement('script');\n    script.type = 'module';\n    script.text = jsCode;\n    document.body.appendChild(script);\n  }", "url": "https://ai.pydantic.dev/chat-app/index.html#example-code", "page": "chat-app/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "md_text": "  loadTs().catch((e) => {\n    console.error(e);\n    document.getElementById('error').classList.remove('d-none');\n    document.getElementById('spinner').classList.remove('active');\n  });\n</script>\n```\n\nTypeScript to handle rendering the messages, to keep this simple (and at the risk of offending frontend developers) the typescript code is passed to the browser as plain text and transpiled in the browser.\n\n[chat\\_app.ts](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/chat_app.ts)\n\n```\n// BIG FAT WARNING: to avoid the complexity of npm, this typescript is compiled in the browser\n// there's currently no static type checking\n\nimport { marked } from 'https://cdnjs.cloudflare.com/ajax/libs/marked/15.0.0/lib/marked.esm.js'\nconst convElement = document.getElementById('conversation')\n\nconst promptInput = document.getElementById('prompt-input') as HTMLInputElement\nconst spinner = document.getElementById('spinner')\n\n// stream the response and render messages as each chunk is received\n// data is sent as newline-delimited JSON\nasync function onFetchResponse(response: Response): Promise<void> {\n  let text = ''\n  let decoder = new TextDecoder()\n  if (response.ok) {\n    const reader = response.body.getReader()\n    while (true) {\n      const {done, value} = await reader.read()\n      if (done) {\n        break\n      }\n      text += decoder.decode(value)\n      addMessages(text)\n      spinner.classList.remove('active')\n    }\n    addMessages(text)\n    promptInput.disabled = false\n    promptInput.focus()\n  } else {\n    const text = await response.text()\n    console.error(`Unexpected response: ${response.status}`, {response, text})\n    throw new Error(`Unexpected response: ${response.status}`)\n  }\n}\n\n// The format of messages, this matches pydantic-ai both for brevity and understanding\n// in production, you might not want to keep this format all the way to the frontend\ninterface Message {\n  role: string\n  content: string\n  timestamp: string\n}\n\n// take raw response text and render messages into the `#conversation` element\n// Message timestamp is assumed to be a unique identifier of a message, and is used to deduplicate\n// hence you can send data about the same message multiple times, and it will be updated\n// instead of creating a new message elements\nfunction addMessages(responseText: string) {\n  const lines = responseText.split('\\n')\n  const messages: Message[] = lines.filter(line => line.length > 1).map(j => JSON.parse(j))\n  for (const message of messages) {\n    // we use the timestamp as a crude element id\n    const {timestamp, role, content} = message\n    const id = `msg-${timestamp}`\n    let msgDiv = document.getElementById(id)\n    if (!msgDiv) {\n      msgDiv = document.createElement('div')\n      msgDiv.id = id\n      msgDiv.title = `${role} at ${timestamp}`\n      msgDiv.classList.add('border-top', 'pt-2', role)\n      convElement.appendChild(msgDiv)\n    }\n    msgDiv.innerHTML = marked.parse(content)\n  }\n  window.scrollTo({ top: document.body.scrollHeight, behavior: 'smooth' })\n}\n\nfunction onError(error: any) {\n  console.error(error)\n  document.getElementById('error').classList.remove('d-none')\n  document.getElementById('spinner').classList.remove('active')\n}\n\nasync function onSubmit(e: SubmitEvent): Promise<void> {\n  e.preventDefault()\n  spinner.classList.add('active')\n  const body = new FormData(e.target as HTMLFormElement)\n\n  promptInput.value = ''\n  promptInput.disabled = true\n\n  const response = await fetch('/chat/', {method: 'POST', body})\n  await onFetchResponse(response)\n}\n\n// call onSubmit when the form is submitted (e.g. user clicks the send button or hits Enter)\ndocument.querySelector('form').addEventListener('submit', (e) => onSubmit(e).catch(onError))\n\n// load messages on page load\nfetch('/chat/').then(onFetchResponse).catch(onError)\n```", "url": "https://ai.pydantic.dev/chat-app/index.html#example-code", "page": "chat-app/index.html", "source_site": "pydantic_ai"}
{"title": "tool\\_from\\_langchain", "anchor": "toolfromlangchain", "md_text": "```\ntool_from_langchain(langchain_tool: LangChainTool) -> Tool\n```\n\nCreates a Pydantic AI tool proxy from a LangChain tool.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `langchain_tool` | `LangChainTool` | The LangChain tool to wrap. | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `Tool` | A Pydantic AI tool that corresponds to the LangChain tool. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ext/langchain.py`\n\n|  |  |\n| --- | --- |\n| ``` 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 ``` | ``` def tool_from_langchain(langchain_tool: LangChainTool) -> Tool:     \"\"\"Creates a Pydantic AI tool proxy from a LangChain tool.      Args:         langchain_tool: The LangChain tool to wrap.      Returns:         A Pydantic AI tool that corresponds to the LangChain tool.     \"\"\"     function_name = langchain_tool.name     function_description = langchain_tool.description     inputs = langchain_tool.args.copy()     required = sorted({name for name, detail in inputs.items() if 'default' not in detail})     schema: JsonSchemaValue = langchain_tool.get_input_jsonschema()     if 'additionalProperties' not in schema:         schema['additionalProperties'] = False     if required:         schema['required'] = required      defaults = {name: detail['default'] for name, detail in inputs.items() if 'default' in detail}      # restructures the arguments to match langchain tool run     def proxy(*args: Any, **kwargs: Any) -> str:         assert not args, 'This should always be called with kwargs'         kwargs = defaults | kwargs         return langchain_tool.run(kwargs)      return Tool.from_schema(         function=proxy,         name=function_name,         description=function_description,         json_schema=schema,     ) ``` |", "url": "https://ai.pydantic.dev/ext/index.html#toolfromlangchain", "page": "ext/index.html", "source_site": "pydantic_ai"}
{"title": "LangChainToolset", "anchor": "langchaintoolset", "md_text": "Bases: `FunctionToolset`\n\nA toolset that wraps LangChain tools.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ext/langchain.py`\n\n|  |  |\n| --- | --- |\n| ``` 67 68 69 70 71 ``` | ``` class LangChainToolset(FunctionToolset):     \"\"\"A toolset that wraps LangChain tools.\"\"\"      def __init__(self, tools: list[LangChainTool], *, id: str | None = None):         super().__init__([tool_from_langchain(tool) for tool in tools], id=id) ``` |", "url": "https://ai.pydantic.dev/ext/index.html#langchaintoolset", "page": "ext/index.html", "source_site": "pydantic_ai"}
{"title": "tool\\_from\\_aci", "anchor": "toolfromaci", "md_text": "```\ntool_from_aci(\n    aci_function: str, linked_account_owner_id: str\n) -> Tool\n```\n\nCreates a Pydantic AI tool proxy from an ACI.dev function.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `aci_function` | `str` | The ACI.dev function to wrap. | *required* |\n| `linked_account_owner_id` | `str` | The ACI user ID to execute the function on behalf of. | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `Tool` | A Pydantic AI tool that corresponds to the ACI.dev tool. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/ext/aci.py`\n\n|  |  |\n| --- | --- |\n| ``` 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 ``` | ``` def tool_from_aci(aci_function: str, linked_account_owner_id: str) -> Tool:     \"\"\"Creates a Pydantic AI tool proxy from an ACI.dev function.      Args:         aci_function: The ACI.dev function to wrap.         linked_account_owner_id: The ACI user ID to execute the function on behalf of.      Returns:         A Pydantic AI tool that corresponds to the ACI.dev tool.     \"\"\"     aci = ACI()     function_definition = aci.functions.get_definition(aci_function)     function_name = function_definition['function']['name']     function_description = function_definition['function']['description']     inputs = function_definition['function']['parameters']      json_schema = {         'additionalProperties': inputs.get('additionalProperties', False),         'properties': inputs.get('properties', {}),         'required': inputs.get('required', []),         # Default to 'object' if not specified         'type': inputs.get('type', 'object'),     }      # Clean the schema     json_schema = _clean_schema(json_schema)      def implementation(*args: Any, **kwargs: Any) -> str:         if args:             raise TypeError('Positional arguments are not allowed')         return aci.handle_function_call(             function_name,             kwargs,             linked_account_owner_id=linked_account_owner_id,             allowed_apps_only=True,         )      return Tool.from_schema(         function=implementation,         name=function_name,         description=function_description,         json_schema=json_schema,     ) ``` |", "url": "https://ai.pydantic.dev/ext/index.html#toolfromaci", "page": "ext/index.html", "source_site": "pydantic_ai"}
{"title": "ACIToolset", "anchor": "acitoolset", "md_text": "Bases: `FunctionToolset`\n\nA toolset that wraps ACI.dev tools.\n\nSource code in `pydantic_ai_slim/pydantic_ai/ext/aci.py`\n\n|  |  |\n| --- | --- |\n| ``` 70 71 72 73 74 75 76 ``` | ``` class ACIToolset(FunctionToolset):     \"\"\"A toolset that wraps ACI.dev tools.\"\"\"      def __init__(self, aci_functions: Sequence[str], linked_account_owner_id: str, *, id: str | None = None):         super().__init__(             [tool_from_aci(aci_function, linked_account_owner_id) for aci_function in aci_functions], id=id         ) ``` |", "url": "https://ai.pydantic.dev/ext/index.html#acitoolset", "page": "ext/index.html", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "md_text": "To use `MistralModel`, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `mistral` optional group:\n\npipuv\n\n```\npip install \"pydantic-ai-slim[mistral]\"\n\nuv add \"pydantic-ai-slim[mistral]\"\n```", "url": "https://ai.pydantic.dev/mistral/index.html#install", "page": "mistral/index.html", "source_site": "pydantic_ai"}
{"title": "Configuration", "anchor": "configuration", "md_text": "To use [Mistral](https://mistral.ai) through their API, go to [console.mistral.ai/api-keys/](https://console.mistral.ai/api-keys/) and follow your nose until you find the place to generate an API key.\n\n`LatestMistralModelNames` contains a list of the most popular Mistral models.", "url": "https://ai.pydantic.dev/mistral/index.html#configuration", "page": "mistral/index.html", "source_site": "pydantic_ai"}
{"title": "Environment variable", "anchor": "environment-variable", "md_text": "Once you have the API key, you can set it as an environment variable:\n\n```\nexport MISTRAL_API_KEY='your-api-key'\n```\n\nYou can then use `MistralModel` by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('mistral:mistral-large-latest')\n...\n```\n\nOr initialise the model directly with just the model name:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.mistral import MistralModel\n\nmodel = MistralModel('mistral-small-latest')\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/mistral/index.html#environment-variable", "page": "mistral/index.html", "source_site": "pydantic_ai"}
{"title": "`provider` argument", "anchor": "provider-argument", "md_text": "You can provide a custom `Provider` via the `provider` argument:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.mistral import MistralModel\nfrom pydantic_ai.providers.mistral import MistralProvider\n\nmodel = MistralModel(\n    'mistral-large-latest', provider=MistralProvider(api_key='your-api-key', base_url='https://<mistral-provider-endpoint>')\n)\nagent = Agent(model)\n...\n```\n\nYou can also customize the provider with a custom `httpx.AsyncHTTPClient`:\n\n```\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.mistral import MistralModel\nfrom pydantic_ai.providers.mistral import MistralProvider\n\ncustom_http_client = AsyncClient(timeout=30)\nmodel = MistralModel(\n    'mistral-large-latest',\n    provider=MistralProvider(api_key='your-api-key', http_client=custom_http_client),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/mistral/index.html#provider-argument", "page": "mistral/index.html", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "md_text": "To use OpenAI models or OpenAI-compatible APIs, you need to either install `pydantic-ai`, or install `pydantic-ai-slim` with the `openai` optional group:\n\npipuv\n\n```\npip install \"pydantic-ai-slim[openai]\"\n\nuv add \"pydantic-ai-slim[openai]\"\n```", "url": "https://ai.pydantic.dev/openai/index.html#install", "page": "openai/index.html", "source_site": "pydantic_ai"}
{"title": "Configuration", "anchor": "configuration", "md_text": "To use `OpenAIChatModel` with the OpenAI API, go to [platform.openai.com](https://platform.openai.com/) and follow your nose until you find the place to generate an API key.", "url": "https://ai.pydantic.dev/openai/index.html#configuration", "page": "openai/index.html", "source_site": "pydantic_ai"}
{"title": "Environment variable", "anchor": "environment-variable", "md_text": "Once you have the API key, you can set it as an environment variable:\n\n```\nexport OPENAI_API_KEY='your-api-key'\n```\n\nYou can then use `OpenAIChatModel` by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-5')\n...\n```\n\nOr initialise the model directly with just the model name:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\n\nmodel = OpenAIChatModel('gpt-5')\nagent = Agent(model)\n...\n```\n\nBy default, the `OpenAIChatModel` uses the `OpenAIProvider` with the `base_url` set to `https://api.openai.com/v1`.", "url": "https://ai.pydantic.dev/openai/index.html#environment-variable", "page": "openai/index.html", "source_site": "pydantic_ai"}
{"title": "Configure the provider", "anchor": "configure-the-provider", "md_text": "If you want to pass parameters in code to the provider, you can programmatically instantiate the\n[OpenAIProvider](../providers/index.html#pydantic_ai.providers.openai.OpenAIProvider) and pass it to the model:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nmodel = OpenAIChatModel('gpt-5', provider=OpenAIProvider(api_key='your-api-key'))\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/openai/index.html#configure-the-provider", "page": "openai/index.html", "source_site": "pydantic_ai"}
{"title": "Custom OpenAI Client", "anchor": "custom-openai-client", "md_text": "`OpenAIProvider` also accepts a custom `AsyncOpenAI` client via the `openai_client` parameter, so you can customise the `organization`, `project`, `base_url` etc. as defined in the [OpenAI API docs](https://platform.openai.com/docs/api-reference).\n\ncustom\\_openai\\_client.py\n\n```\nfrom openai import AsyncOpenAI\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nclient = AsyncOpenAI(max_retries=3)\nmodel = OpenAIChatModel('gpt-5', provider=OpenAIProvider(openai_client=client))\nagent = Agent(model)\n...\n```\n\nYou could also use the [`AsyncAzureOpenAI`](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/switching-endpoints) client\nto use the Azure OpenAI API. Note that the `AsyncAzureOpenAI` is a subclass of `AsyncOpenAI`.\n\n```\nfrom openai import AsyncAzureOpenAI\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nclient = AsyncAzureOpenAI(\n    azure_endpoint='...',\n    api_version='2024-07-01-preview',\n    api_key='your-api-key',\n)\n\nmodel = OpenAIChatModel(\n    'gpt-5',\n    provider=OpenAIProvider(openai_client=client),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/openai/index.html#custom-openai-client", "page": "openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAI Responses API", "anchor": "openai-responses-api", "md_text": "Pydantic AI also supports OpenAI's [Responses API](https://platform.openai.com/docs/api-reference/responses) through the\n\nYou can use [`OpenAIResponsesModel`](../models/openai/index.html#pydantic_ai.models.openai.OpenAIResponsesModel) by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai-responses:gpt-5')\n...\n```\n\nOr initialise the model directly with just the model name:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIResponsesModel\n\nmodel = OpenAIResponsesModel('gpt-5')\nagent = Agent(model)\n...\n```\n\nYou can learn more about the differences between the Responses API and Chat Completions API in the [OpenAI API docs](https://platform.openai.com/docs/guides/migrate-to-responses).", "url": "https://ai.pydantic.dev/openai/index.html#openai-responses-api", "page": "openai/index.html", "source_site": "pydantic_ai"}
{"title": "Built-in tools", "anchor": "built-in-tools", "md_text": "The Responses API has built-in tools that you can use instead of building your own:\n\n* [Web search](https://platform.openai.com/docs/guides/tools-web-search): allow models to search the web for the latest information before generating a response.\n* [Code interpreter](https://platform.openai.com/docs/guides/tools-code-interpreter): allow models to write and run Python code in a sandboxed environment before generating a response.\n* [Image generation](https://platform.openai.com/docs/guides/tools-image-generation): allow models to generate images based on a text prompt.\n* [File search](https://platform.openai.com/docs/guides/tools-file-search): allow models to search your files for relevant information before generating a response.\n* [Computer use](https://platform.openai.com/docs/guides/tools-computer-use): allow models to use a computer to perform tasks on your behalf.\n\nWeb search, Code interpreter, and Image generation are natively supported through the [Built-in tools](https://ai.pydantic.dev/builtin-tools/) feature.\n\nFile search and Computer use can be enabled by passing an [`openai.types.responses.FileSearchToolParam`](https://github.com/openai/openai-python/blob/main/src/openai/types/responses/file_search_tool_param.py) or [`openai.types.responses.ComputerToolParam`](https://github.com/openai/openai-python/blob/main/src/openai/types/responses/computer_tool_param.py) in the `openai_builtin_tools` setting on [`OpenAIResponsesModelSettings`](../models/openai/index.html#pydantic_ai.models.openai.OpenAIResponsesModelSettings). They don't currently generate [`BuiltinToolCallPart`](../messages/index.html#pydantic_ai.messages.BuiltinToolCallPart) or [`BuiltinToolReturnPart`](../messages/index.html#pydantic_ai.messages.BuiltinToolReturnPart) parts in the message history, or streamed events; please submit an issue if you need native support for these built-in tools.\n\nfile\\_search\\_tool.py\n\n```\nfrom openai.types.responses import FileSearchToolParam\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIResponsesModel, OpenAIResponsesModelSettings\n\nmodel_settings = OpenAIResponsesModelSettings(\n    openai_builtin_tools=[\n        FileSearchToolParam(\n            type='file_search',\n            vector_store_ids=['your-history-book-vector-store-id']\n        )\n    ],\n)\nmodel = OpenAIResponsesModel('gpt-5')\nagent = Agent(model=model, model_settings=model_settings)\n\nresult = agent.run_sync('Who was Albert Einstein?')\nprint(result.output)\n#> Albert Einstein was a German-born theoretical physicist.\n```\n\n#### Referencing earlier responses\n\nThe Responses API supports referencing earlier model responses in a new request using a `previous_response_id` parameter, to ensure the full [conversation state](https://platform.openai.com/docs/guides/conversation-state?api-mode=responses#passing-context-from-the-previous-response) including [reasoning items](https://platform.openai.com/docs/guides/reasoning#keeping-reasoning-items-in-context) are kept in context. This is available through the `openai_previous_response_id` field in\n[`OpenAIResponsesModelSettings`](../models/openai/index.html#pydantic_ai.models.openai.OpenAIResponsesModelSettings).\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIResponsesModel, OpenAIResponsesModelSettings\n\nmodel = OpenAIResponsesModel('gpt-5')\nagent = Agent(model=model)\n\nresult = agent.run_sync('The secret is 1234')\nmodel_settings = OpenAIResponsesModelSettings(\n    openai_previous_response_id=result.all_messages()[-1].provider_response_id\n)\nresult = agent.run_sync('What is the secret code?', model_settings=model_settings)\nprint(result.output)\n#> 1234\n```\n\nBy passing the `provider_response_id` from an earlier run, you can allow the model to build on its own prior reasoning without needing to resend the full message history.\n\n##### Automatically referencing earlier responses\n\nWhen the `openai_previous_response_id` field is set to `'auto'`, Pydantic AI will automatically select the most recent `provider_response_id` from message history and omit messages that came before it, letting the OpenAI API leverage server-side history instead for improved efficiency.\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIResponsesModel, OpenAIResponsesModelSettings\n\nmodel = OpenAIResponsesModel('gpt-5')\nagent = Agent(model=model)\n\nresult1 = agent.run_sync('Tell me a joke.')\nprint(result1.output)\n#> Did you hear about the toothpaste scandal? They called it Colgate.\n\n# When set to 'auto', the most recent provider_response_id\n# and messages after it are sent as request.\nmodel_settings = OpenAIResponsesModelSettings(openai_previous_response_id='auto')\nresult2 = agent.run_sync(\n    'Explain?',\n    message_history=result1.new_messages(),\n    model_settings=model_settings\n)\nprint(result2.output)\n#> This is an excellent joke invented by Samuel Colvin, it needs no explanation.\n```", "url": "https://ai.pydantic.dev/openai/index.html#built-in-tools", "page": "openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAI-compatible Models", "anchor": "openai-compatible-models", "md_text": "Many providers and models are compatible with the OpenAI API, and can be used with `OpenAIChatModel` in Pydantic AI.\nBefore getting started, check the [installation and configuration](index.html#install) instructions above.\n\nTo use another OpenAI-compatible API, you can make use of the `base_url` and `api_key` arguments from `OpenAIProvider`:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nmodel = OpenAIChatModel(\n    'model_name',\n    provider=OpenAIProvider(\n        base_url='https://<openai-compatible-api-endpoint>', api_key='your-api-key'\n    ),\n)\nagent = Agent(model)\n...\n```\n\nVarious providers also have their own provider classes so that you don't need to specify the base URL yourself and you can use the standard `<PROVIDER>_API_KEY` environment variable to set the API key.\nWhen a provider has its own provider class, you can use the `Agent(\"<provider>:<model>\")` shorthand, e.g. `Agent(\"deepseek:deepseek-chat\")` or `Agent(\"openrouter:google/gemini-2.5-pro-preview\")`, instead of building the `OpenAIChatModel` explicitly. Similarly, you can pass the provider name as a string to the `provider` argument on `OpenAIChatModel` instead of building instantiating the provider class explicitly.\n\n#### Model Profile\n\nSometimes, the provider or model you're using will have slightly different requirements than OpenAI's API or models, like having different restrictions on JSON schemas for tool definitions, or not supporting tool definitions to be marked as strict.\n\nWhen using an alternative provider class provided by Pydantic AI, an appropriate model profile is typically selected automatically based on the model name.\nIf the model you're using is not working correctly out of the box, you can tweak various aspects of how model requests are constructed by providing your own [`ModelProfile`](../profiles/index.html#pydantic_ai.profiles.ModelProfile) (for behaviors shared among all model classes) or [`OpenAIModelProfile`](../profiles/index.html#pydantic_ai.profiles.openai.OpenAIModelProfile) (for behaviors specific to `OpenAIChatModel`):\n\n```\nfrom pydantic_ai import Agent, InlineDefsJsonSchemaTransformer\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.profiles.openai import OpenAIModelProfile\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nmodel = OpenAIChatModel(\n    'model_name',\n    provider=OpenAIProvider(\n        base_url='https://<openai-compatible-api-endpoint>.com', api_key='your-api-key'\n    ),\n    profile=OpenAIModelProfile(\n        json_schema_transformer=InlineDefsJsonSchemaTransformer,  # Supported by any model class on a plain ModelProfile\n        openai_supports_strict_tool_definition=False  # Supported by OpenAIModel only, requires OpenAIModelProfile\n    )\n)\nagent = Agent(model)\n```", "url": "https://ai.pydantic.dev/openai/index.html#openai-compatible-models", "page": "openai/index.html", "source_site": "pydantic_ai"}
{"title": "DeepSeek", "anchor": "deepseek", "md_text": "To use the [DeepSeek](https://deepseek.com) provider, first create an API key by following the [Quick Start guide](https://api-docs.deepseek.com/).\n\nYou can then set the `DEEPSEEK_API_KEY` environment variable and use [`DeepSeekProvider`](../providers/index.html#pydantic_ai.providers.deepseek.DeepSeekProvider) by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('deepseek:deepseek-chat')\n...\n```\n\nOr initialise the model and provider directly:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.deepseek import DeepSeekProvider\n\nmodel = OpenAIChatModel(\n    'deepseek-chat',\n    provider=DeepSeekProvider(api_key='your-deepseek-api-key'),\n)\nagent = Agent(model)\n...\n```\n\nYou can also customize any provider with a custom `http_client`:\n\n```\nfrom httpx import AsyncClient\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.deepseek import DeepSeekProvider\n\ncustom_http_client = AsyncClient(timeout=30)\nmodel = OpenAIChatModel(\n    'deepseek-chat',\n    provider=DeepSeekProvider(\n        api_key='your-deepseek-api-key', http_client=custom_http_client\n    ),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/openai/index.html#deepseek", "page": "openai/index.html", "source_site": "pydantic_ai"}
{"title": "Ollama", "anchor": "ollama", "md_text": "Pydantic AI supports both self-hosted [Ollama](https://ollama.com/) servers (running locally or remotely) and [Ollama Cloud](https://ollama.com/cloud).\n\nFor servers running locally, use the `http://localhost:11434/v1` base URL. For Ollama Cloud, use `https://ollama.com/v1` and ensure an API key is set.\n\nYou can set the `OLLAMA_BASE_URL` and (optionally) `OLLAMA_API_KEY` environment variables and use [`OllamaProvider`](../providers/index.html#pydantic_ai.providers.ollama.OllamaProvider) by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('ollama:gpt-oss:20b')\n...\n```\n\nOr initialise the model and provider directly:\n\n```\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.ollama import OllamaProvider\n\n\nclass CityLocation(BaseModel):\n    city: str\n    country: str\n\n\nollama_model = OpenAIChatModel(\n    model_name='gpt-oss:20b',\n    provider=OllamaProvider(base_url='http://localhost:11434/v1'),  # (1)!\n)\nagent = Agent(ollama_model, output_type=CityLocation)\n\nresult = agent.run_sync('Where were the olympics held in 2012?')\nprint(result.output)\n#> city='London' country='United Kingdom'\nprint(result.usage())\n#> RunUsage(input_tokens=57, output_tokens=8, requests=1)\n```\n\n1. For Ollama Cloud, use the `base_url='https://ollama.com/v1'` and set the `OLLAMA_API_KEY` environment variable.", "url": "https://ai.pydantic.dev/openai/index.html#ollama", "page": "openai/index.html", "source_site": "pydantic_ai"}
{"title": "Azure AI Foundry", "anchor": "azure-ai-foundry", "md_text": "To use [Azure AI Foundry](https://ai.azure.com/) as your provider, you can set the `AZURE_OPENAI_ENDPOINT`, `AZURE_OPENAI_API_KEY`, and `OPENAI_API_VERSION` environment variables and use [`AzureProvider`](../providers/index.html#pydantic_ai.providers.azure.AzureProvider) by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('azure:gpt-5')\n...\n```\n\nOr initialise the model and provider directly:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.azure import AzureProvider\n\nmodel = OpenAIChatModel(\n    'gpt-5',\n    provider=AzureProvider(\n        azure_endpoint='your-azure-endpoint',\n        api_version='your-api-version',\n        api_key='your-api-key',\n    ),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/openai/index.html#azure-ai-foundry", "page": "openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenRouter", "anchor": "openrouter", "md_text": "To use [OpenRouter](https://openrouter.ai), first create an API key at [openrouter.ai/keys](https://openrouter.ai/keys).\n\nYou can set the `OPENROUTER_API_KEY` environment variable and use [`OpenRouterProvider`](../providers/index.html#pydantic_ai.providers.openrouter.OpenRouterProvider) by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openrouter:anthropic/claude-3.5-sonnet')\n...\n```\n\nOr initialise the model and provider directly:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openrouter import OpenRouterProvider\n\nmodel = OpenAIChatModel(\n    'anthropic/claude-3.5-sonnet',\n    provider=OpenRouterProvider(api_key='your-openrouter-api-key'),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/openai/index.html#openrouter", "page": "openai/index.html", "source_site": "pydantic_ai"}
{"title": "Vercel AI Gateway", "anchor": "vercel-ai-gateway", "md_text": "To use [Vercel's AI Gateway](https://vercel.com/docs/ai-gateway), first follow the [documentation](https://vercel.com/docs/ai-gateway) instructions on obtaining an API key or OIDC token.\n\nYou can set the `VERCEL_AI_GATEWAY_API_KEY` and `VERCEL_OIDC_TOKEN` environment variables and use [`VercelProvider`](../providers/index.html#pydantic_ai.providers.vercel.VercelProvider) by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('vercel:anthropic/claude-4-sonnet')\n...\n```\n\nOr initialise the model and provider directly:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.vercel import VercelProvider\n\nmodel = OpenAIChatModel(\n    'anthropic/claude-4-sonnet',\n    provider=VercelProvider(api_key='your-vercel-ai-gateway-api-key'),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/openai/index.html#vercel-ai-gateway", "page": "openai/index.html", "source_site": "pydantic_ai"}
{"title": "Grok (xAI)", "anchor": "grok-xai", "md_text": "Go to [xAI API Console](https://console.x.ai/) and create an API key.\n\nYou can set the `GROK_API_KEY` environment variable and use [`GrokProvider`](../providers/index.html#pydantic_ai.providers.grok.GrokProvider) by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('grok:grok-2-1212')\n...\n```\n\nOr initialise the model and provider directly:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.grok import GrokProvider\n\nmodel = OpenAIChatModel(\n    'grok-2-1212',\n    provider=GrokProvider(api_key='your-xai-api-key'),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/openai/index.html#grok-xai", "page": "openai/index.html", "source_site": "pydantic_ai"}
{"title": "MoonshotAI", "anchor": "moonshotai", "md_text": "Create an API key in the [Moonshot Console](https://platform.moonshot.ai/console).\n\nYou can set the `MOONSHOTAI_API_KEY` environment variable and use [`MoonshotAIProvider`](../providers/index.html#pydantic_ai.providers.moonshotai.MoonshotAIProvider) by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('moonshotai:kimi-k2-0711-preview')\n...\n```\n\nOr initialise the model and provider directly:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.moonshotai import MoonshotAIProvider\n\nmodel = OpenAIChatModel(\n    'kimi-k2-0711-preview',\n    provider=MoonshotAIProvider(api_key='your-moonshot-api-key'),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/openai/index.html#moonshotai", "page": "openai/index.html", "source_site": "pydantic_ai"}
{"title": "GitHub Models", "anchor": "github-models", "md_text": "To use [GitHub Models](https://docs.github.com/en/github-models), you'll need a GitHub personal access token with the `models: read` permission.\n\nYou can set the `GITHUB_API_KEY` environment variable and use [`GitHubProvider`](../providers/index.html#pydantic_ai.providers.github.GitHubProvider) by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('github:xai/grok-3-mini')\n...\n```\n\nOr initialise the model and provider directly:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.github import GitHubProvider\n\nmodel = OpenAIChatModel(\n    'xai/grok-3-mini',  # GitHub Models uses prefixed model names\n    provider=GitHubProvider(api_key='your-github-token'),\n)\nagent = Agent(model)\n...\n```\n\nGitHub Models supports various model families with different prefixes. You can see the full list on the [GitHub Marketplace](https://github.com/marketplace?type=models) or the public [catalog endpoint](https://models.github.ai/catalog/models).", "url": "https://ai.pydantic.dev/openai/index.html#github-models", "page": "openai/index.html", "source_site": "pydantic_ai"}
{"title": "Perplexity", "anchor": "perplexity", "md_text": "Follow the Perplexity [getting started](https://docs.perplexity.ai/guides/getting-started)\nguide to create an API key.\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.openai import OpenAIProvider\n\nmodel = OpenAIChatModel(\n    'sonar-pro',\n    provider=OpenAIProvider(\n        base_url='https://api.perplexity.ai',\n        api_key='your-perplexity-api-key',\n    ),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/openai/index.html#perplexity", "page": "openai/index.html", "source_site": "pydantic_ai"}
{"title": "Fireworks AI", "anchor": "fireworks-ai", "md_text": "Go to [Fireworks.AI](https://fireworks.ai/) and create an API key in your account settings.\n\nYou can set the `FIREWORKS_API_KEY` environment variable and use [`FireworksProvider`](../providers/index.html#pydantic_ai.providers.fireworks.FireworksProvider) by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('fireworks:accounts/fireworks/models/qwq-32b')\n...\n```\n\nOr initialise the model and provider directly:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.fireworks import FireworksProvider\n\nmodel = OpenAIChatModel(\n    'accounts/fireworks/models/qwq-32b',  # model library available at https://fireworks.ai/models\n    provider=FireworksProvider(api_key='your-fireworks-api-key'),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/openai/index.html#fireworks-ai", "page": "openai/index.html", "source_site": "pydantic_ai"}
{"title": "Together AI", "anchor": "together-ai", "md_text": "Go to [Together.ai](https://www.together.ai/) and create an API key in your account settings.\n\nYou can set the `TOGETHER_API_KEY` environment variable and use [`TogetherProvider`](../providers/index.html#pydantic_ai.providers.together.TogetherProvider) by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('together:meta-llama/Llama-3.3-70B-Instruct-Turbo-Free')\n...\n```\n\nOr initialise the model and provider directly:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.together import TogetherProvider\n\nmodel = OpenAIChatModel(\n    'meta-llama/Llama-3.3-70B-Instruct-Turbo-Free',  # model library available at https://www.together.ai/models\n    provider=TogetherProvider(api_key='your-together-api-key'),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/openai/index.html#together-ai", "page": "openai/index.html", "source_site": "pydantic_ai"}
{"title": "Heroku AI", "anchor": "heroku-ai", "md_text": "To use [Heroku AI](https://www.heroku.com/ai), first create an API key.\n\nYou can set the `HEROKU_INFERENCE_KEY` and (optionally )`HEROKU_INFERENCE_URL` environment variables and use [`HerokuProvider`](../providers/index.html#pydantic_ai.providers.heroku.HerokuProvider) by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('heroku:claude-3-7-sonnet')\n...\n```\n\nOr initialise the model and provider directly:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.heroku import HerokuProvider\n\nmodel = OpenAIChatModel(\n    'claude-3-7-sonnet',\n    provider=HerokuProvider(api_key='your-heroku-inference-key'),\n)\nagent = Agent(model)\n...\n```", "url": "https://ai.pydantic.dev/openai/index.html#heroku-ai", "page": "openai/index.html", "source_site": "pydantic_ai"}
{"title": "Cerebras", "anchor": "cerebras", "md_text": "To use [Cerebras](https://cerebras.ai/), you need to create an API key in the [Cerebras Console](https://cloud.cerebras.ai/).\n\nYou can set the `CEREBRAS_API_KEY` environment variable and use [`CerebrasProvider`](../providers/index.html#pydantic_ai.providers.cerebras.CerebrasProvider) by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('cerebras:llama3.3-70b')\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n```\n\nOr initialise the model and provider directly:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.cerebras import CerebrasProvider\n\nmodel = OpenAIChatModel(\n    'llama3.3-70b',\n    provider=CerebrasProvider(api_key='your-cerebras-api-key'),\n)\nagent = Agent(model)\n\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n```", "url": "https://ai.pydantic.dev/openai/index.html#cerebras", "page": "openai/index.html", "source_site": "pydantic_ai"}
{"title": "LiteLLM", "anchor": "litellm", "md_text": "To use [LiteLLM](https://www.litellm.ai/), set the configs as outlined in the [doc](https://docs.litellm.ai/docs/set_keys). In `LiteLLMProvider`, you can pass `api_base` and `api_key`. The value of these configs will depend on your setup. For example, if you are using OpenAI models, then you need to pass `https://api.openai.com/v1` as the `api_base` and your OpenAI API key as the `api_key`. If you are using a LiteLLM proxy server running on your local machine, then you need to pass `http://localhost:<port>` as the `api_base` and your LiteLLM API key (or a placeholder) as the `api_key`.\n\nTo use custom LLMs, use `custom/` prefix in the model name.\n\nOnce you have the configs, use the [`LiteLLMProvider`](../providers/index.html#pydantic_ai.providers.litellm.LiteLLMProvider) as follows:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.litellm import LiteLLMProvider\n\nmodel = OpenAIChatModel(\n    'openai/gpt-3.5-turbo',\n    provider=LiteLLMProvider(\n        api_base='<api-base-url>',\n        api_key='<api-key>'\n    )\n)\nagent = Agent(model)\n\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n...\n```", "url": "https://ai.pydantic.dev/openai/index.html#litellm", "page": "openai/index.html", "source_site": "pydantic_ai"}
{"title": "Nebius AI Studio", "anchor": "nebius-ai-studio", "md_text": "Go to [Nebius AI Studio](https://studio.nebius.com/) and create an API key.\n\nYou can set the `NEBIUS_API_KEY` environment variable and use [`NebiusProvider`](../providers/index.html#pydantic_ai.providers.nebius.NebiusProvider) by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('nebius:Qwen/Qwen3-32B-fast')\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n```\n\nOr initialise the model and provider directly:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.nebius import NebiusProvider\n\nmodel = OpenAIChatModel(\n    'Qwen/Qwen3-32B-fast',\n    provider=NebiusProvider(api_key='your-nebius-api-key'),\n)\nagent = Agent(model)\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n```", "url": "https://ai.pydantic.dev/openai/index.html#nebius-ai-studio", "page": "openai/index.html", "source_site": "pydantic_ai"}
{"title": "OVHcloud AI Endpoints", "anchor": "ovhcloud-ai-endpoints", "md_text": "To use OVHcloud AI Endpoints, you need to create a new API key. To do so, go to the [OVHcloud manager](https://ovh.com/manager), then in Public Cloud > AI Endpoints > API keys. Click on `Create a new API key` and copy your new key.\n\nYou can explore the [catalog](https://endpoints.ai.cloud.ovh.net/catalog) to find which models are available.\n\nYou can set the `OVHCLOUD_API_KEY` environment variable and use [`OVHcloudProvider`](../providers/index.html#pydantic_ai.providers.ovhcloud.OVHcloudProvider) by name:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('ovhcloud:gpt-oss-120b')\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n```\n\nIf you need to configure the provider, you can use the [`OVHcloudProvider`](../providers/index.html#pydantic_ai.providers.ovhcloud.OVHcloudProvider) class:\n\n```\nfrom pydantic_ai import Agent\nfrom pydantic_ai.models.openai import OpenAIChatModel\nfrom pydantic_ai.providers.ovhcloud import OVHcloudProvider\n\nmodel = OpenAIChatModel(\n    'gpt-oss-120b',\n    provider=OVHcloudProvider(api_key='your-api-key'),\n)\nagent = Agent(model)\nresult = agent.run_sync('What is the capital of France?')\nprint(result.output)\n#> The capital of France is Paris.\n```", "url": "https://ai.pydantic.dev/openai/index.html#ovhcloud-ai-endpoints", "page": "openai/index.html", "source_site": "pydantic_ai"}
{"title": "TemporalAgent", "anchor": "temporalagent", "md_text": "Bases: `WrapperAgent[AgentDepsT, OutputDataT]`\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py`", "url": "https://ai.pydantic.dev/durable_exec/index.html#temporalagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "TemporalAgent", "anchor": "temporalagent", "md_text": "|  |  |\n| --- | --- |\n| ```  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 886 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 ``` | ``` class TemporalAgent(WrapperAgent[AgentDepsT, OutputDataT]):     def __init__(         self,         wrapped: AbstractAgent[AgentDepsT, OutputDataT],         *,         name: str | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,         activity_config: ActivityConfig | None = None,         model_activity_config: ActivityConfig | None = None,         toolset_activity_config: dict[str, ActivityConfig] | None = None,         tool_activity_config: dict[str, dict[str, ActivityConfig | Literal[False]]] | None = None,         run_context_type: type[TemporalRunContext[AgentDepsT]] = TemporalRunContext[AgentDepsT],         temporalize_toolset_func: Callable[             [                 AbstractToolset[AgentDepsT],                 str,                 ActivityConfig,                 dict[str, ActivityConfig | Literal[False]],                 type[AgentDepsT],                 type[TemporalRunContext[AgentDepsT]],             ],             AbstractToolset[AgentDepsT],         ] = temporalize_toolset,     ):         \"\"\"Wrap an agent to enable it to be used inside a Temporal workflow, by automatically offloading model requests, tool calls, and MCP server communication to Temporal activities.          After wrapping, the original agent can still be used as normal outside of the Temporal workflow, but any changes to its model or toolsets after wrapping will not be reflected in the durable agent.          Args:             wrapped: The agent to wrap.             name: Optional unique agent name to use in the Temporal activities' names. If not provided, the agent's `name` will be used.             event_stream_handler: Optional event stream handler to use instead of the one set on the wrapped agent.             activity_config: The base Temporal activity config to use for all activities. If no config is provided, a `start_to_close_timeout` of 60 seconds is used.             model_activity_config: The Temporal activity config to use for model request activities. This is merged with the base activity config.             toolset_activity_config: The Temporal activity config to use for get-tools and call-tool activities for specific toolsets identified by ID. This is merged with the base activity config.             tool_activity_config: The Temporal activity config to use for specific tool call activities identified by toolset ID and tool name.                 This is merged with the base and toolset-specific activity configs.                 If a tool does not use IO, you can specify `False` to disable using an activity.                 Note that the tool is required to be defined as an `async` function as non-async tools are run in threads which are non-deterministic and thus not supported outside of activities.             run_context_type: The `TemporalRunContext` subclass to use to serialize and deserialize the run context for use inside a Temporal activity.                 By default, only the `deps`, `retries`, `tool_call_id`, `tool_name`, `retry` and `run_step` attributes will be available.                 To make another attribute available, create a `TemporalRunContext` subclass with a custom `serialize_run_context` class method that returns a dictionary that includes the attribute.             temporalize_toolset_func: Optional function to use to prepare \"leaf\" toolsets (i.e. those that implement their own tool listing and calling) for Temporal by wrapping them in a `TemporalWrapperToolset` that moves methods that require IO to Temporal activities.                 If not provided, only `FunctionToolset` and `MCPServer` will be prepared for Temporal.                 The function takes the toolset, the activity name prefix, the toolset-specific activity config, the tool-specific activity configs and the run context type.         \"\"\"         super().__init__(wrapped)          self._name = name         self._event_stream_handler = event_stream_handler         self.run_context_type = run_context_type          # start_to_close_timeout is required         activity_config = activity_config or ActivityConfig(start_to_close_timeout=timedelta(seconds=60))          # `pydantic_ai.exceptions.UserError` and `pydantic.errors.PydanticUserError` are not retryable         retry_policy = activity_config.get('retry_policy') or RetryPolicy()         retry_policy.non_retryable_error_types = [             *(retry_policy.non_retryable_error_types or []),             UserError.__name__,             PydanticUserError.__name__,         ]         activity_config['retry_policy'] = retry_policy         self.activity_config = activity_config          model_activity_config = model_activity_config or {}         toolset_activity_config = toolset_activity_config or {}         tool_activity_config = tool_activity_config or {}          if self.name is None:             raise UserError(                 \"An agent needs to have a unique `name` in order to be used with Temporal. The name will be used to identify the agent's activities within the workflow.\"             )          activity_name_prefix = f'agent__{self.name}'          activities: list[Callable[..., Any]] = []         if not isinstance(wrapped.model, Model):             raise UserError(                 'An agent needs to have a `model` in order to be used with Temporal, it cannot be set at agent run time.'             )          async def event_stream_handler_activity(params: _EventStreamHandlerParams, deps: AgentDepsT) -> None:             # We can never get here without an `event_stream_handler`, as `TemporalAgent.run_stream` and `TemporalAgent.iter` raise an error saying to use `TemporalAgent.run` instead,             # and that only ends up calling `event_stream_handler` if it is set.             assert self.event_stream_handler is not None              run_context = self.run_context_type.deserialize_run_context(params.serialized_run_context, deps=deps)              async def streamed_response():                 yield params.event              await self.event_stream_handler(run_context, streamed_response())          # Set type hint explicitly so that Temporal can take care of serialization and deserialization         event_stream_handler_activity.__annotations__['deps'] = self.deps_type          self.event_stream_handler_activity = activity.defn(name=f'{activity_name_prefix}__event_stream_handler')(             event_stream_handler_activity         )         activities.append(self.event_stream_handler_activity)          temporal_model = TemporalModel(             wrapped.model,             activity_name_prefix=activity_name_prefix,             activity_config=activity_config | model_activity_config,             deps_type=self.deps_type,             run_context_type=self.run_context_type,             event_stream_handler=self.event_stream_handler,         )         activities.extend(temporal_model.temporal_activities)          def temporalize_toolset(toolset: AbstractToolset[AgentDepsT]) -> AbstractToolset[AgentDepsT]:             id = toolset.id             if id is None:                 raise UserError(                     \"Toolsets that are 'leaves' (i.e. those that implement their own tool listing and calling) need to have a unique `id` in order to be used with Temporal. The ID will be used to identify the toolset's activities within the workflow.\"                 )              toolset = temporalize_toolset_func(                 toolset,                 activity_name_prefix,                 activity_config | toolset_activity_config.get(id, {}),                 tool_activity_config.get(id, {}),                 self.deps_type,                 self.run_context_type,             )             if isinstance(toolset, TemporalWrapperToolset):                 activities.extend(toolset.temporal_activities)             return toolset          temporal_toolsets = [toolset.visit_and_replace(temporalize_toolset) for toolset in wrapped.toolsets]          self._model = temporal_model         self._toolsets = temporal_toolsets         self._temporal_activities = activities          self._temporal_overrides_active: ContextVar[bool] = ContextVar('_temporal_overrides_active', default=False)      @property     def name(self) -> str | None:         return self._name or super().name      @name.setter     def name(self, value: str | None) -> None:  # pragma: no cover         raise UserError(             'The agent name cannot be changed after creation. If you need to change the name, create a new agent.'         )      @property     def model(self) -> Model:         return self._model      @property     def event_stream_handler(self) -> EventStreamHandler[AgentDepsT] | None:         handler = self._event_stream_handler or super().event_stream_handler         if handler is None:             return None         elif workflow.in_workflow():             return self._call_event_stream_handler_activity         else:             return handler      async def _call_event_stream_handler_activity(         self, ctx: RunContext[AgentDepsT], stream: AsyncIterable[_messages.AgentStreamEvent]     ) -> None:         serialized_run_context = self.run_context_type.serialize_run_context(ctx)         async for event in stream:             await workflow.execute_activity(  # pyright: ignore[reportUnknownMemberType]                 activity=self.event_stream_handler_activity,                 args=[                     _EventStreamHandlerParams(                         event=event,                         serialized_run_context=serialized_run_context,                     ),                     ctx.deps,                 ],                 **self.activity_config,             )      @property     def toolsets(self) -> Sequence[AbstractToolset[AgentDepsT]]:         with self._temporal_overrides():             return super().toolsets      @property     def temporal_activities(self) -> list[Callable[..., Any]]:         return self._temporal_activities      @contextmanager     def _temporal_overrides(self) -> Iterator[None]:         # We reset tools here as the temporalized function toolset is already in self._toolsets.         with super().override(model=self._model, toolsets=self._toolsets, tools=[]):             token = self._temporal_overrides_active.set(True)             try:                 yield             except PydanticSerializationError as e:                 raise UserError(                     \"The `deps` object failed to be serialized. Temporal requires all objects that are passed to activities to be serializable using Pydantic's `TypeAdapter`.\"                 ) from e             finally:                 self._temporal_overrides_active.reset(token)      @overload     async def run(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AgentRunResult[OutputDataT]: ...      @overload     async def run(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT],         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AgentRunResult[RunOutputDataT]: ...      async def run(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT] | None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,         **_deprecated_kwargs: Never,     ) -> AgentRunResult[Any]:         \"\"\"Run the agent with a user prompt in async mode.          This method builds an internal agent graph (using system prompts, tools and result schemas) and then         runs the graph to completion. The result of the run is returned.          Example:         ```python         from pydantic_ai import Agent          agent = Agent('openai:gpt-4o')          async def main():             agent_run = await agent.run('What is the capital of France?')             print(agent_run.output)             #> The capital of France is Paris.         ```          Args:             user_prompt: User input to start/continue the conversation.             output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no                 output validators since output validators would expect an argument that matches the agent's output type.             message_history: History of the conversation so far.             deferred_tool_results: Optional results for deferred tool calls in the message history.             model: Optional model to use for this run, required if `model` was not set when creating the agent.             deps: Optional dependencies to use for this run.             model_settings: Optional settings to use for this model's request.             usage_limits: Optional limits on model request count or token usage.             usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.             infer_name: Whether to try to infer the agent name from the call frame if it's not set.             toolsets: Optional additional toolsets for this run.             event_stream_handler: Optional event stream handler to use for this run.             builtin_tools: Optional additional builtin tools for this run.          Returns:             The result of the run.         \"\"\"         if workflow.in_workflow() and event_stream_handler is not None:             raise UserError(                 'Event stream handler cannot be set at agent run time inside a Temporal workflow, it must be set at agent creation time.'             )          with self._temporal_overrides():             return await super().run(                 user_prompt,                 output_type=output_type,                 message_history=message_history,                 deferred_tool_results=deferred_tool_results,                 model=model,                 deps=deps,                 model_settings=model_settings,                 usage_limits=usage_limits,                 usage=usage,                 infer_name=infer_name,                 toolsets=toolsets,                 builtin_tools=builtin_tools,                 event_stream_handler=event_stream_handler or self.event_stream_handler,                 **_deprecated_kwargs,             )      @overload     def run_sync(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AgentRunResult[OutputDataT]: ...      @overload     def run_sync(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT],         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AgentRunResult[RunOutputDataT]: ...      def run_sync(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT] | None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,         **_deprecated_kwargs: Never,     ) -> AgentRunResult[Any]:         \"\"\"Synchronously run the agent with a user prompt.          This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] with `loop.run_until_complete(...)`.         You therefore can't use this method inside async code or if there's an active event loop.          Example:         ```python         from pydantic_ai import Agent          agent = Agent('openai:gpt-4o')          result_sync = agent.run_sync('What is the capital of Italy?')         print(result_sync.output)         #> The capital of Italy is Rome.         ```          Args:             user_prompt: User input to start/continue the conversation.             output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no                 output validators since output validators would expect an argument that matches the agent's output type.             message_history: History of the conversation so far.             deferred_tool_results: Optional results for deferred tool calls in the message history.             model: Optional model to use for this run, required if `model` was not set when creating the agent.             deps: Optional dependencies to use for this run.             model_settings: Optional settings to use for this model's request.             usage_limits: Optional limits on model request count or token usage.             usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.             infer_name: Whether to try to infer the agent name from the call frame if it's not set.             toolsets: Optional additional toolsets for this run.             event_stream_handler: Optional event stream handler to use for this run.             builtin_tools: Optional additional builtin tools for this run.          Returns:             The result of the run.         \"\"\"         if workflow.in_workflow():             raise UserError(                 '`agent.run_sync()` cannot be used inside a Temporal workflow. Use `await agent.run()` instead.'             )          return super().run_sync(             user_prompt,             output_type=output_type,             message_history=message_history,             deferred_tool_results=deferred_tool_results,             model=model,             deps=deps,             model_settings=model_settings,             usage_limits=usage_limits,             usage=usage,             infer_name=infer_name,             toolsets=toolsets,             builtin_tools=builtin_tools,             event_stream_handler=event_stream_handler,             **_deprecated_kwargs,         )      @overload     def run_stream(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AbstractAsyncContextManager[StreamedRunResult[AgentDepsT, OutputDataT]]: ...      @overload     def run_stream(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT],         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AbstractAsyncContextManager[StreamedRunResult[AgentDepsT, RunOutputDataT]]: ...      @asynccontextmanager     async def run_stream(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT] | None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,         **_deprecated_kwargs: Never,     ) -> AsyncIterator[StreamedRunResult[AgentDepsT, Any]]:         \"\"\"Run the agent with a user prompt in async mode, returning a streamed response.          Example:         ```python         from pydantic_ai import Agent          agent = Agent('openai:gpt-4o')          async def main():             async with agent.run_stream('What is the capital of the UK?') as response:                 print(await response.get_output())                 #> The capital of the UK is London.         ```          Args:             user_prompt: User input to start/continue the conversation.             output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no                 output validators since output validators would expect an argument that matches the agent's output type.             message_history: History of the conversation so far.             deferred_tool_results: Optional results for deferred tool calls in the message history.             model: Optional model to use for this run, required if `model` was not set when creating the agent.             deps: Optional dependencies to use for this run.             model_settings: Optional settings to use for this model's request.             usage_limits: Optional limits on model request count or token usage.             usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.             infer_name: Whether to try to infer the agent name from the call frame if it's not set.             toolsets: Optional additional toolsets for this run.             builtin_tools: Optional additional builtin tools for this run.             event_stream_handler: Optional event stream handler to use for this run. It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager.          Returns:             The result of the run.         \"\"\"         if workflow.in_workflow():             raise UserError(                 '`agent.run_stream()` cannot be used inside a Temporal workflow. '                 'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'             )          async with super().run_stream(             user_prompt,             output_type=output_type,             message_history=message_history,             deferred_tool_results=deferred_tool_results,             model=model,             deps=deps,             model_settings=model_settings,             usage_limits=usage_limits,             usage=usage,             infer_name=infer_name,             toolsets=toolsets,             event_stream_handler=event_stream_handler,             builtin_tools=builtin_tools,             **_deprecated_kwargs,         ) as result:             yield result      @overload     def run_stream_events(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[OutputDataT]]: ...      @overload     def run_stream_events(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT],         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[RunOutputDataT]]: ...      def run_stream_events(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT] | None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[Any]]:         \"\"\"Run the agent with a user prompt in async mode and stream events from the run.          This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] and         uses the `event_stream_handler` kwarg to get a stream of events from the run.          Example:         ```python         from pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent          agent = Agent('openai:gpt-4o')          async def main():             events: list[AgentStreamEvent | AgentRunResultEvent] = []             async for event in agent.run_stream_events('What is the capital of France?'):                 events.append(event)             print(events)             '''             [                 PartStartEvent(index=0, part=TextPart(content='The capital of ')),                 FinalResultEvent(tool_name=None, tool_call_id=None),                 PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='France is Paris. ')),                 AgentRunResultEvent(                     result=AgentRunResult(output='The capital of France is Paris. ')                 ),             ]             '''         ```          Arguments are the same as for [`self.run`][pydantic_ai.agent.AbstractAgent.run],         except that `event_stream_handler` is now allowed.          Args:             user_prompt: User input to start/continue the conversation.             output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no                 output validators since output validators would expect an argument that matches the agent's output type.             message_history: History of the conversation so far.             deferred_tool_results: Optional results for deferred tool calls in the message history.             model: Optional model to use for this run, required if `model` was not set when creating the agent.             deps: Optional dependencies to use for this run.             model_settings: Optional settings to use for this model's request.             usage_limits: Optional limits on model request count or token usage.             usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.             infer_name: Whether to try to infer the agent name from the call frame if it's not set.             toolsets: Optional additional toolsets for this run.             builtin_tools: Optional additional builtin tools for this run.          Returns:             An async iterable of stream events `AgentStreamEvent` and finally a `AgentRunResultEvent` with the final             run result.         \"\"\"         if workflow.in_workflow():             raise UserError(                 '`agent.run_stream_events()` cannot be used inside a Temporal workflow. '                 'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'             )          return super().run_stream_events(             user_prompt,             output_type=output_type,             message_history=message_history,             deferred_tool_results=deferred_tool_results,             model=model,             deps=deps,             model_settings=model_settings,             usage_limits=usage_limits,             usage=usage,             infer_name=infer_name,             toolsets=toolsets,             builtin_tools=builtin_tools,         )      @overload     def iter(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         **_deprecated_kwargs: Never,     ) -> AbstractAsyncContextManager[AgentRun[AgentDepsT, OutputDataT]]: ...      @overload     def iter(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT],         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         **_deprecated_kwargs: Never,     ) -> AbstractAsyncContextManager[AgentRun[AgentDepsT, RunOutputDataT]]: ...      @asynccontextmanager     async def iter(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT] | None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         **_deprecated_kwargs: Never,     ) -> AsyncIterator[AgentRun[AgentDepsT, Any]]:         \"\"\"A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.          This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an         `AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are         executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the         stream of events coming from the execution of tools.          The `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,         and the final result of the run once it has completed.          For more details, see the documentation of `AgentRun`.          Example:         ```python         from pydantic_ai import Agent          agent = Agent('openai:gpt-4o')          async def main():             nodes = []             async with agent.iter('What is the capital of France?') as agent_run:                 async for node in agent_run:                     nodes.append(node)             print(nodes)             '''             [                 UserPromptNode(                     user_prompt='What is the capital of France?',                     instructions_functions=[],                     system_prompts=(),                     system_prompt_functions=[],                     system_prompt_dynamic_functions={},                 ),                 ModelRequestNode(                     request=ModelRequest(                         parts=[                             UserPromptPart(                                 content='What is the capital of France?',                                 timestamp=datetime.datetime(...),                             )                         ]                     )                 ),                 CallToolsNode(                     model_response=ModelResponse(                         parts=[TextPart(content='The capital of France is Paris.')],                         usage=RequestUsage(input_tokens=56, output_tokens=7),                         model_name='gpt-4o',                         timestamp=datetime.datetime(...),                     )                 ),                 End(data=FinalResult(output='The capital of France is Paris.')),             ]             '''             print(agent_run.result.output)             #> The capital of France is Paris.         ```          Args:             user_prompt: User input to start/continue the conversation.             output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no                 output validators since output validators would expect an argument that matches the agent's output type.             message_history: History of the conversation so far.             deferred_tool_results: Optional results for deferred tool calls in the message history.             model: Optional model to use for this run, required if `model` was not set when creating the agent.             deps: Optional dependencies to use for this run.             model_settings: Optional settings to use for this model's request.             usage_limits: Optional limits on model request count or token usage.             usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.             infer_name: Whether to try to infer the agent name from the call frame if it's not set.             toolsets: Optional additional toolsets for this run.             builtin_tools: Optional additional builtin tools for this run.          Returns:             The result of the run.         \"\"\"         if workflow.in_workflow():             if not self._temporal_overrides_active.get():                 raise UserError(                     '`agent.iter()` cannot be used inside a Temporal workflow. '                     'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'                 )              if model is not None:                 raise UserError(                     'Model cannot be set at agent run time inside a Temporal workflow, it must be set at agent creation time.'                 )             if toolsets is not None:                 raise UserError(                     'Toolsets cannot be set at agent run time inside a Temporal workflow, it must be set at agent creation time.'                 )          async with super().iter(             user_prompt=user_prompt,             output_type=output_type,             message_history=message_history,             deferred_tool_results=deferred_tool_results,             model=model,             deps=deps,             model_settings=model_settings,             usage_limits=usage_limits,             usage=usage,             infer_name=infer_name,             toolsets=toolsets,             builtin_tools=builtin_tools,             **_deprecated_kwargs,         ) as run:             yield run      @contextmanager     def override(         self,         *,         name: str | _utils.Unset = _utils.UNSET,         deps: AgentDepsT | _utils.Unset = _utils.UNSET,         model: models.Model | models.KnownModelName | str | _utils.Unset = _utils.UNSET,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | _utils.Unset = _utils.UNSET,         tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | _utils.Unset = _utils.UNSET,         instructions: Instructions[AgentDepsT] | _utils.Unset = _utils.UNSET,     ) -> Iterator[None]:         \"\"\"Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.          This is particularly useful when testing.         You can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).          Args:             name: The name to use instead of the name passed to the agent constructor and agent run.             deps: The dependencies to use instead of the dependencies passed to the agent run.             model: The model to use instead of the model passed to the agent run.             toolsets: The toolsets to use instead of the toolsets passed to the agent constructor and agent run.             tools: The tools to use instead of the tools registered with the agent.             instructions: The instructions to use instead of the instructions registered with the agent.         \"\"\"         if workflow.in_workflow():             if _utils.is_set(model):                 raise UserError(                     'Model cannot be contextually overridden inside a Temporal workflow, it must be set at agent creation time.'                 )             if _utils.is_set(toolsets):                 raise UserError(                     'Toolsets cannot be contextually overridden inside a Temporal workflow, they must be set at agent creation time.'                 )             if _utils.is_set(tools):                 raise UserError(                     'Tools cannot be contextually overridden inside a Temporal workflow, they must be set at agent creation time.'                 )          with super().override(             name=name,             deps=deps,             model=model,             toolsets=toolsets,             tools=tools,             instructions=instructions,         ):             yield ``` |", "url": "https://ai.pydantic.dev/durable_exec/index.html#temporalagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "TemporalAgent", "anchor": "temporalagent", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    wrapped: AbstractAgent[AgentDepsT, OutputDataT],\n    *,\n    name: str | None = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None,\n    activity_config: ActivityConfig | None = None,\n    model_activity_config: ActivityConfig | None = None,\n    toolset_activity_config: (\n        dict[str, ActivityConfig] | None\n    ) = None,\n    tool_activity_config: (\n        dict[\n            str, dict[str, ActivityConfig | Literal[False]]\n        ]\n        | None\n    ) = None,\n    run_context_type: type[\n        TemporalRunContext[AgentDepsT]\n    ] = TemporalRunContext[AgentDepsT],\n    temporalize_toolset_func: Callable[\n        [\n            AbstractToolset[AgentDepsT],\n            str,\n            ActivityConfig,\n            dict[str, ActivityConfig | Literal[False]],\n            type[AgentDepsT],\n            type[TemporalRunContext[AgentDepsT]],\n        ],\n        AbstractToolset[AgentDepsT],\n    ] = temporalize_toolset\n)\n```\n\nWrap an agent to enable it to be used inside a Temporal workflow, by automatically offloading model requests, tool calls, and MCP server communication to Temporal activities.\n\nAfter wrapping, the original agent can still be used as normal outside of the Temporal workflow, but any changes to its model or toolsets after wrapping will not be reflected in the durable agent.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `wrapped` | `AbstractAgent[AgentDepsT, OutputDataT]` | The agent to wrap. | *required* |\n| `name` | `str | None` | Optional unique agent name to use in the Temporal activities' names. If not provided, the agent's `name` will be used. | `None` |\n| `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional event stream handler to use instead of the one set on the wrapped agent. | `None` |\n| `activity_config` | `ActivityConfig | None` | The base Temporal activity config to use for all activities. If no config is provided, a `start_to_close_timeout` of 60 seconds is used. | `None` |\n| `model_activity_config` | `ActivityConfig | None` | The Temporal activity config to use for model request activities. This is merged with the base activity config. | `None` |\n| `toolset_activity_config` | `dict[str, ActivityConfig] | None` | The Temporal activity config to use for get-tools and call-tool activities for specific toolsets identified by ID. This is merged with the base activity config. | `None` |\n| `tool_activity_config` | `dict[str, dict[str, ActivityConfig | Literal[False]]] | None` | The Temporal activity config to use for specific tool call activities identified by toolset ID and tool name. This is merged with the base and toolset-specific activity configs. If a tool does not use IO, you can specify `False` to disable using an activity. Note that the tool is required to be defined as an `async` function as non-async tools are run in threads which are non-deterministic and thus not supported outside of activities. | `None` |\n| `run_context_type` | `type[TemporalRunContext[AgentDepsT]]` | The `TemporalRunContext` subclass to use to serialize and deserialize the run context for use inside a Temporal activity. By default, only the `deps`, `retries`, `tool_call_id`, `tool_name`, `retry` and `run_step` attributes will be available. To make another attribute available, create a `TemporalRunContext` subclass with a custom `serialize_run_context` class method that returns a dictionary that includes the attribute. | `TemporalRunContext[AgentDepsT]` |\n| `temporalize_toolset_func` | `Callable[[AbstractToolset[AgentDepsT], str, ActivityConfig, dict[str, ActivityConfig | Literal[False]], type[AgentDepsT], type[TemporalRunContext[AgentDepsT]]], AbstractToolset[AgentDepsT]]` | Optional function to use to prepare \"leaf\" toolsets (i.e. those that implement their own tool listing and calling) for Temporal by wrapping them in a `TemporalWrapperToolset` that moves methods that require IO to Temporal activities. If not provided, only `FunctionToolset` and `MCPServer` will be prepared for Temporal. The function takes the toolset, the activity name prefix, the toolset-specific activity config, the tool-specific activity configs and the run context type. | `temporalize_toolset` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py`", "url": "https://ai.pydantic.dev/durable_exec/index.html#temporalagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "TemporalAgent", "anchor": "temporalagent", "md_text": "|  |  |\n| --- | --- |\n| ```  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 ``` | ``` def __init__(     self,     wrapped: AbstractAgent[AgentDepsT, OutputDataT],     *,     name: str | None = None,     event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     activity_config: ActivityConfig | None = None,     model_activity_config: ActivityConfig | None = None,     toolset_activity_config: dict[str, ActivityConfig] | None = None,     tool_activity_config: dict[str, dict[str, ActivityConfig | Literal[False]]] | None = None,     run_context_type: type[TemporalRunContext[AgentDepsT]] = TemporalRunContext[AgentDepsT],     temporalize_toolset_func: Callable[         [             AbstractToolset[AgentDepsT],             str,             ActivityConfig,             dict[str, ActivityConfig | Literal[False]],             type[AgentDepsT],             type[TemporalRunContext[AgentDepsT]],         ],         AbstractToolset[AgentDepsT],     ] = temporalize_toolset, ):     \"\"\"Wrap an agent to enable it to be used inside a Temporal workflow, by automatically offloading model requests, tool calls, and MCP server communication to Temporal activities.      After wrapping, the original agent can still be used as normal outside of the Temporal workflow, but any changes to its model or toolsets after wrapping will not be reflected in the durable agent.      Args:         wrapped: The agent to wrap.         name: Optional unique agent name to use in the Temporal activities' names. If not provided, the agent's `name` will be used.         event_stream_handler: Optional event stream handler to use instead of the one set on the wrapped agent.         activity_config: The base Temporal activity config to use for all activities. If no config is provided, a `start_to_close_timeout` of 60 seconds is used.         model_activity_config: The Temporal activity config to use for model request activities. This is merged with the base activity config.         toolset_activity_config: The Temporal activity config to use for get-tools and call-tool activities for specific toolsets identified by ID. This is merged with the base activity config.         tool_activity_config: The Temporal activity config to use for specific tool call activities identified by toolset ID and tool name.             This is merged with the base and toolset-specific activity configs.             If a tool does not use IO, you can specify `False` to disable using an activity.             Note that the tool is required to be defined as an `async` function as non-async tools are run in threads which are non-deterministic and thus not supported outside of activities.         run_context_type: The `TemporalRunContext` subclass to use to serialize and deserialize the run context for use inside a Temporal activity.             By default, only the `deps`, `retries`, `tool_call_id`, `tool_name`, `retry` and `run_step` attributes will be available.             To make another attribute available, create a `TemporalRunContext` subclass with a custom `serialize_run_context` class method that returns a dictionary that includes the attribute.         temporalize_toolset_func: Optional function to use to prepare \"leaf\" toolsets (i.e. those that implement their own tool listing and calling) for Temporal by wrapping them in a `TemporalWrapperToolset` that moves methods that require IO to Temporal activities.             If not provided, only `FunctionToolset` and `MCPServer` will be prepared for Temporal.             The function takes the toolset, the activity name prefix, the toolset-specific activity config, the tool-specific activity configs and the run context type.     \"\"\"     super().__init__(wrapped)      self._name = name     self._event_stream_handler = event_stream_handler     self.run_context_type = run_context_type      # start_to_close_timeout is required     activity_config = activity_config or ActivityConfig(start_to_close_timeout=timedelta(seconds=60))      # `pydantic_ai.exceptions.UserError` and `pydantic.errors.PydanticUserError` are not retryable     retry_policy = activity_config.get('retry_policy') or RetryPolicy()     retry_policy.non_retryable_error_types = [         *(retry_policy.non_retryable_error_types or []),         UserError.__name__,         PydanticUserError.__name__,     ]     activity_config['retry_policy'] = retry_policy     self.activity_config = activity_config      model_activity_config = model_activity_config or {}     toolset_activity_config = toolset_activity_config or {}     tool_activity_config = tool_activity_config or {}      if self.name is None:         raise UserError(             \"An agent needs to have a unique `name` in order to be used with Temporal. The name will be used to identify the agent's activities within the workflow.\"         )      activity_name_prefix = f'agent__{self.name}'      activities: list[Callable[..., Any]] = []     if not isinstance(wrapped.model, Model):         raise UserError(             'An agent needs to have a `model` in order to be used with Temporal, it cannot be set at agent run time.'         )      async def event_stream_handler_activity(params: _EventStreamHandlerParams, deps: AgentDepsT) -> None:         # We can never get here without an `event_stream_handler`, as `TemporalAgent.run_stream` and `TemporalAgent.iter` raise an error saying to use `TemporalAgent.run` instead,         # and that only ends up calling `event_stream_handler` if it is set.         assert self.event_stream_handler is not None          run_context = self.run_context_type.deserialize_run_context(params.serialized_run_context, deps=deps)          async def streamed_response():             yield params.event          await self.event_stream_handler(run_context, streamed_response())      # Set type hint explicitly so that Temporal can take care of serialization and deserialization     event_stream_handler_activity.__annotations__['deps'] = self.deps_type      self.event_stream_handler_activity = activity.defn(name=f'{activity_name_prefix}__event_stream_handler')(         event_stream_handler_activity     )     activities.append(self.event_stream_handler_activity)      temporal_model = TemporalModel(         wrapped.model,         activity_name_prefix=activity_name_prefix,         activity_config=activity_config | model_activity_config,         deps_type=self.deps_type,         run_context_type=self.run_context_type,         event_stream_handler=self.event_stream_handler,     )     activities.extend(temporal_model.temporal_activities)      def temporalize_toolset(toolset: AbstractToolset[AgentDepsT]) -> AbstractToolset[AgentDepsT]:         id = toolset.id         if id is None:             raise UserError(                 \"Toolsets that are 'leaves' (i.e. those that implement their own tool listing and calling) need to have a unique `id` in order to be used with Temporal. The ID will be used to identify the toolset's activities within the workflow.\"             )          toolset = temporalize_toolset_func(             toolset,             activity_name_prefix,             activity_config | toolset_activity_config.get(id, {}),             tool_activity_config.get(id, {}),             self.deps_type,             self.run_context_type,         )         if isinstance(toolset, TemporalWrapperToolset):             activities.extend(toolset.temporal_activities)         return toolset      temporal_toolsets = [toolset.visit_and_replace(temporalize_toolset) for toolset in wrapped.toolsets]      self._model = temporal_model     self._toolsets = temporal_toolsets     self._temporal_activities = activities      self._temporal_overrides_active: ContextVar[bool] = ContextVar('_temporal_overrides_active', default=False) ``` |", "url": "https://ai.pydantic.dev/durable_exec/index.html#temporalagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "TemporalAgent", "anchor": "temporalagent", "md_text": "#### run `async`\n\n```\nrun(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[OutputDataT]\n\nrun(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[RunOutputDataT]\n\nrun(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AgentRunResult[Any]\n```\n\nRun the agent with a user prompt in async mode.\n\nThis method builds an internal agent graph (using system prompts, tools and result schemas) and then\nruns the graph to completion. The result of the run is returned.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    agent_run = await agent.run('What is the capital of France?')\n    print(agent_run.output)\n    #> The capital of France is Paris.\n```\n\nParameters:", "url": "https://ai.pydantic.dev/durable_exec/index.html#temporalagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "TemporalAgent", "anchor": "temporalagent", "md_text": "| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` |\n| `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` |\n| `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional event stream handler to use for this run. | `None` |\n| `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AgentRunResult[Any]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py`", "url": "https://ai.pydantic.dev/durable_exec/index.html#temporalagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "TemporalAgent", "anchor": "temporalagent", "md_text": "|  |  |\n| --- | --- |\n| ``` 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 ``` | ``` async def run(     self,     user_prompt: str | Sequence[_messages.UserContent] | None = None,     *,     output_type: OutputSpec[RunOutputDataT] | None = None,     message_history: Sequence[_messages.ModelMessage] | None = None,     deferred_tool_results: DeferredToolResults | None = None,     model: models.Model | models.KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: _usage.UsageLimits | None = None,     usage: _usage.RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     **_deprecated_kwargs: Never, ) -> AgentRunResult[Any]:     \"\"\"Run the agent with a user prompt in async mode.      This method builds an internal agent graph (using system prompts, tools and result schemas) and then     runs the graph to completion. The result of the run is returned.      Example:     ```python     from pydantic_ai import Agent      agent = Agent('openai:gpt-4o')      async def main():         agent_run = await agent.run('What is the capital of France?')         print(agent_run.output)         #> The capital of France is Paris.     ```      Args:         user_prompt: User input to start/continue the conversation.         output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no             output validators since output validators would expect an argument that matches the agent's output type.         message_history: History of the conversation so far.         deferred_tool_results: Optional results for deferred tool calls in the message history.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.         event_stream_handler: Optional event stream handler to use for this run.         builtin_tools: Optional additional builtin tools for this run.      Returns:         The result of the run.     \"\"\"     if workflow.in_workflow() and event_stream_handler is not None:         raise UserError(             'Event stream handler cannot be set at agent run time inside a Temporal workflow, it must be set at agent creation time.'         )      with self._temporal_overrides():         return await super().run(             user_prompt,             output_type=output_type,             message_history=message_history,             deferred_tool_results=deferred_tool_results,             model=model,             deps=deps,             model_settings=model_settings,             usage_limits=usage_limits,             usage=usage,             infer_name=infer_name,             toolsets=toolsets,             builtin_tools=builtin_tools,             event_stream_handler=event_stream_handler or self.event_stream_handler,             **_deprecated_kwargs,         ) ``` |\n\n#### run\\_sync\n\n```\nrun_sync(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[OutputDataT]", "url": "https://ai.pydantic.dev/durable_exec/index.html#temporalagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "TemporalAgent", "anchor": "temporalagent", "md_text": "run_sync(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[RunOutputDataT]\n\nrun_sync(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AgentRunResult[Any]\n```\n\nSynchronously run the agent with a user prompt.\n\nThis is a convenience method that wraps [`self.run`](../agent/index.html#pydantic_ai.agent.AbstractAgent.run) with `loop.run_until_complete(...)`.\nYou therefore can't use this method inside async code or if there's an active event loop.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nresult_sync = agent.run_sync('What is the capital of Italy?')\nprint(result_sync.output)\n#> The capital of Italy is Rome.\n```\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` |\n| `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` |\n| `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional event stream handler to use for this run. | `None` |\n| `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AgentRunResult[Any]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py`", "url": "https://ai.pydantic.dev/durable_exec/index.html#temporalagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "TemporalAgent", "anchor": "temporalagent", "md_text": "|  |  |\n| --- | --- |\n| ``` 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 ``` | ``` def run_sync(     self,     user_prompt: str | Sequence[_messages.UserContent] | None = None,     *,     output_type: OutputSpec[RunOutputDataT] | None = None,     message_history: Sequence[_messages.ModelMessage] | None = None,     deferred_tool_results: DeferredToolResults | None = None,     model: models.Model | models.KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: _usage.UsageLimits | None = None,     usage: _usage.RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     **_deprecated_kwargs: Never, ) -> AgentRunResult[Any]:     \"\"\"Synchronously run the agent with a user prompt.      This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] with `loop.run_until_complete(...)`.     You therefore can't use this method inside async code or if there's an active event loop.      Example:     ```python     from pydantic_ai import Agent      agent = Agent('openai:gpt-4o')      result_sync = agent.run_sync('What is the capital of Italy?')     print(result_sync.output)     #> The capital of Italy is Rome.     ```      Args:         user_prompt: User input to start/continue the conversation.         output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no             output validators since output validators would expect an argument that matches the agent's output type.         message_history: History of the conversation so far.         deferred_tool_results: Optional results for deferred tool calls in the message history.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.         event_stream_handler: Optional event stream handler to use for this run.         builtin_tools: Optional additional builtin tools for this run.      Returns:         The result of the run.     \"\"\"     if workflow.in_workflow():         raise UserError(             '`agent.run_sync()` cannot be used inside a Temporal workflow. Use `await agent.run()` instead.'         )      return super().run_sync(         user_prompt,         output_type=output_type,         message_history=message_history,         deferred_tool_results=deferred_tool_results,         model=model,         deps=deps,         model_settings=model_settings,         usage_limits=usage_limits,         usage=usage,         infer_name=infer_name,         toolsets=toolsets,         builtin_tools=builtin_tools,         event_stream_handler=event_stream_handler,         **_deprecated_kwargs,     ) ``` |\n\n#### run\\_stream `async`\n\n```\nrun_stream(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AbstractAsyncContextManager[\n    StreamedRunResult[AgentDepsT, OutputDataT]\n]", "url": "https://ai.pydantic.dev/durable_exec/index.html#temporalagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "TemporalAgent", "anchor": "temporalagent", "md_text": "run_stream(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AbstractAsyncContextManager[\n    StreamedRunResult[AgentDepsT, RunOutputDataT]\n]\n\nrun_stream(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AsyncIterator[StreamedRunResult[AgentDepsT, Any]]\n```\n\nRun the agent with a user prompt in async mode, returning a streamed response.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    async with agent.run_stream('What is the capital of the UK?') as response:\n        print(await response.get_output())\n        #> The capital of the UK is London.\n```\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` |\n| `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` |\n| `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n| `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional event stream handler to use for this run. It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AsyncIterator[StreamedRunResult[AgentDepsT, Any]]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py`", "url": "https://ai.pydantic.dev/durable_exec/index.html#temporalagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "TemporalAgent", "anchor": "temporalagent", "md_text": "|  |  |\n| --- | --- |\n| ``` 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 ``` | ``` @asynccontextmanager async def run_stream(     self,     user_prompt: str | Sequence[_messages.UserContent] | None = None,     *,     output_type: OutputSpec[RunOutputDataT] | None = None,     message_history: Sequence[_messages.ModelMessage] | None = None,     deferred_tool_results: DeferredToolResults | None = None,     model: models.Model | models.KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: _usage.UsageLimits | None = None,     usage: _usage.RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     **_deprecated_kwargs: Never, ) -> AsyncIterator[StreamedRunResult[AgentDepsT, Any]]:     \"\"\"Run the agent with a user prompt in async mode, returning a streamed response.      Example:     ```python     from pydantic_ai import Agent      agent = Agent('openai:gpt-4o')      async def main():         async with agent.run_stream('What is the capital of the UK?') as response:             print(await response.get_output())             #> The capital of the UK is London.     ```      Args:         user_prompt: User input to start/continue the conversation.         output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no             output validators since output validators would expect an argument that matches the agent's output type.         message_history: History of the conversation so far.         deferred_tool_results: Optional results for deferred tool calls in the message history.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.         builtin_tools: Optional additional builtin tools for this run.         event_stream_handler: Optional event stream handler to use for this run. It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager.      Returns:         The result of the run.     \"\"\"     if workflow.in_workflow():         raise UserError(             '`agent.run_stream()` cannot be used inside a Temporal workflow. '             'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'         )      async with super().run_stream(         user_prompt,         output_type=output_type,         message_history=message_history,         deferred_tool_results=deferred_tool_results,         model=model,         deps=deps,         model_settings=model_settings,         usage_limits=usage_limits,         usage=usage,         infer_name=infer_name,         toolsets=toolsets,         event_stream_handler=event_stream_handler,         builtin_tools=builtin_tools,         **_deprecated_kwargs,     ) as result:         yield result ``` |\n\n#### run\\_stream\\_events\n\n```\nrun_stream_events(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[\n    AgentStreamEvent | AgentRunResultEvent[OutputDataT]\n]", "url": "https://ai.pydantic.dev/durable_exec/index.html#temporalagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "TemporalAgent", "anchor": "temporalagent", "md_text": "run_stream_events(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[\n    AgentStreamEvent | AgentRunResultEvent[RunOutputDataT]\n]\n\nrun_stream_events(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[\n    AgentStreamEvent | AgentRunResultEvent[Any]\n]\n```\n\nRun the agent with a user prompt in async mode and stream events from the run.\n\nThis is a convenience method that wraps [`self.run`](../agent/index.html#pydantic_ai.agent.AbstractAgent.run) and\nuses the `event_stream_handler` kwarg to get a stream of events from the run.\n\nExample:\n\n```\nfrom pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    events: list[AgentStreamEvent | AgentRunResultEvent] = []\n    async for event in agent.run_stream_events('What is the capital of France?'):\n        events.append(event)\n    print(events)\n    '''\n    [\n        PartStartEvent(index=0, part=TextPart(content='The capital of ')),\n        FinalResultEvent(tool_name=None, tool_call_id=None),\n        PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='France is Paris. ')),\n        AgentRunResultEvent(\n            result=AgentRunResult(output='The capital of France is Paris. ')\n        ),\n    ]\n    '''\n```\n\nArguments are the same as for [`self.run`](../agent/index.html#pydantic_ai.agent.AbstractAgent.run),\nexcept that `event_stream_handler` is now allowed.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` |\n| `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` |\n| `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AsyncIterator[AgentStreamEvent | AgentRunResultEvent[Any]]` | An async iterable of stream events `AgentStreamEvent` and finally a `AgentRunResultEvent` with the final |\n| `AsyncIterator[AgentStreamEvent | AgentRunResultEvent[Any]]` | run result. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py`", "url": "https://ai.pydantic.dev/durable_exec/index.html#temporalagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "TemporalAgent", "anchor": "temporalagent", "md_text": "|  |  |\n| --- | --- |\n| ``` 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 ``` | ``` def run_stream_events(     self,     user_prompt: str | Sequence[_messages.UserContent] | None = None,     *,     output_type: OutputSpec[RunOutputDataT] | None = None,     message_history: Sequence[_messages.ModelMessage] | None = None,     deferred_tool_results: DeferredToolResults | None = None,     model: models.Model | models.KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: _usage.UsageLimits | None = None,     usage: _usage.RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     builtin_tools: Sequence[AbstractBuiltinTool] | None = None, ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[Any]]:     \"\"\"Run the agent with a user prompt in async mode and stream events from the run.      This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] and     uses the `event_stream_handler` kwarg to get a stream of events from the run.      Example:     ```python     from pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent      agent = Agent('openai:gpt-4o')      async def main():         events: list[AgentStreamEvent | AgentRunResultEvent] = []         async for event in agent.run_stream_events('What is the capital of France?'):             events.append(event)         print(events)         '''         [             PartStartEvent(index=0, part=TextPart(content='The capital of ')),             FinalResultEvent(tool_name=None, tool_call_id=None),             PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='France is Paris. ')),             AgentRunResultEvent(                 result=AgentRunResult(output='The capital of France is Paris. ')             ),         ]         '''     ```      Arguments are the same as for [`self.run`][pydantic_ai.agent.AbstractAgent.run],     except that `event_stream_handler` is now allowed.      Args:         user_prompt: User input to start/continue the conversation.         output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no             output validators since output validators would expect an argument that matches the agent's output type.         message_history: History of the conversation so far.         deferred_tool_results: Optional results for deferred tool calls in the message history.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.         builtin_tools: Optional additional builtin tools for this run.      Returns:         An async iterable of stream events `AgentStreamEvent` and finally a `AgentRunResultEvent` with the final         run result.     \"\"\"     if workflow.in_workflow():         raise UserError(             '`agent.run_stream_events()` cannot be used inside a Temporal workflow. '             'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'         )      return super().run_stream_events(         user_prompt,         output_type=output_type,         message_history=message_history,         deferred_tool_results=deferred_tool_results,         model=model,         deps=deps,         model_settings=model_settings,         usage_limits=usage_limits,         usage=usage,         infer_name=infer_name,         toolsets=toolsets,         builtin_tools=builtin_tools,     ) ``` |\n\n#### iter `async`", "url": "https://ai.pydantic.dev/durable_exec/index.html#temporalagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "TemporalAgent", "anchor": "temporalagent", "md_text": "```\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AbstractAsyncContextManager[\n    AgentRun[AgentDepsT, OutputDataT]\n]\n\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AbstractAsyncContextManager[\n    AgentRun[AgentDepsT, RunOutputDataT]\n]\n\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AsyncIterator[AgentRun[AgentDepsT, Any]]\n```\n\nA contextmanager which can be used to iterate over the agent graph's nodes as they are executed.\n\nThis method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an\n`AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are\nexecuted. This is the API to use if you want to consume the outputs coming from each LLM model response, or the\nstream of events coming from the execution of tools.\n\nThe `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,\nand the final result of the run once it has completed.\n\nFor more details, see the documentation of `AgentRun`.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    nodes = []\n    async with agent.iter('What is the capital of France?') as agent_run:\n        async for node in agent_run:\n            nodes.append(node)\n    print(nodes)\n    '''\n    [\n        UserPromptNode(\n            user_prompt='What is the capital of France?',\n            instructions_functions=[],\n            system_prompts=(),\n            system_prompt_functions=[],\n            system_prompt_dynamic_functions={},\n        ),\n        ModelRequestNode(\n            request=ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='What is the capital of France?',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            )\n        ),\n        CallToolsNode(\n            model_response=ModelResponse(\n                parts=[TextPart(content='The capital of France is Paris.')],\n                usage=RequestUsage(input_tokens=56, output_tokens=7),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            )\n        ),\n        End(data=FinalResult(output='The capital of France is Paris.')),\n    ]\n    '''\n    print(agent_run.result.output)\n    #> The capital of France is Paris.\n```\n\nParameters:", "url": "https://ai.pydantic.dev/durable_exec/index.html#temporalagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "TemporalAgent", "anchor": "temporalagent", "md_text": "| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` |\n| `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` |\n| `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AsyncIterator[AgentRun[AgentDepsT, Any]]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py`", "url": "https://ai.pydantic.dev/durable_exec/index.html#temporalagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "TemporalAgent", "anchor": "temporalagent", "md_text": "|  |  |\n| --- | --- |\n| ``` 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 ``` | ``` @asynccontextmanager async def iter(     self,     user_prompt: str | Sequence[_messages.UserContent] | None = None,     *,     output_type: OutputSpec[RunOutputDataT] | None = None,     message_history: Sequence[_messages.ModelMessage] | None = None,     deferred_tool_results: DeferredToolResults | None = None,     model: models.Model | models.KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: _usage.UsageLimits | None = None,     usage: _usage.RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     **_deprecated_kwargs: Never, ) -> AsyncIterator[AgentRun[AgentDepsT, Any]]:     \"\"\"A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.      This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an     `AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are     executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the     stream of events coming from the execution of tools.      The `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,     and the final result of the run once it has completed.      For more details, see the documentation of `AgentRun`.      Example:     ```python     from pydantic_ai import Agent      agent = Agent('openai:gpt-4o')      async def main():         nodes = []         async with agent.iter('What is the capital of France?') as agent_run:             async for node in agent_run:                 nodes.append(node)         print(nodes)         '''         [             UserPromptNode(                 user_prompt='What is the capital of France?',                 instructions_functions=[],                 system_prompts=(),                 system_prompt_functions=[],                 system_prompt_dynamic_functions={},             ),             ModelRequestNode(                 request=ModelRequest(                     parts=[                         UserPromptPart(                             content='What is the capital of France?',                             timestamp=datetime.datetime(...),                         )                     ]                 )             ),             CallToolsNode(                 model_response=ModelResponse(                     parts=[TextPart(content='The capital of France is Paris.')],                     usage=RequestUsage(input_tokens=56, output_tokens=7),                     model_name='gpt-4o',                     timestamp=datetime.datetime(...),                 )             ),             End(data=FinalResult(output='The capital of France is Paris.')),         ]         '''         print(agent_run.result.output)         #> The capital of France is Paris.     ```      Args:         user_prompt: User input to start/continue the conversation.         output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no             output validators since output validators would expect an argument that matches the agent's output type.         message_history: History of the conversation so far.         deferred_tool_results: Optional results for deferred tool calls in the message history.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.         builtin_tools: Optional additional builtin tools for this run.      Returns:         The result of the run.     \"\"\"     if workflow.in_workflow():         if not self._temporal_overrides_active.get():             raise UserError(                 '`agent.iter()` cannot be used inside a Temporal workflow. '                 'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'             )          if model is not None:             raise UserError(                 'Model cannot be set at agent run time inside a Temporal workflow, it must be set at agent creation time.'             )         if toolsets is not None:             raise UserError(                 'Toolsets cannot be set at agent run time inside a Temporal workflow, it must be set at agent creation time.'             )      async with super().iter(         user_prompt=user_prompt,         output_type=output_type,         message_history=message_history,         deferred_tool_results=deferred_tool_results,         model=model,         deps=deps,         model_settings=model_settings,         usage_limits=usage_limits,         usage=usage,         infer_name=infer_name,         toolsets=toolsets,         builtin_tools=builtin_tools,         **_deprecated_kwargs,     ) as run:         yield run ``` |", "url": "https://ai.pydantic.dev/durable_exec/index.html#temporalagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "TemporalAgent", "anchor": "temporalagent", "md_text": "#### override\n\n```\noverride(\n    *,\n    name: str | Unset = UNSET,\n    deps: AgentDepsT | Unset = UNSET,\n    model: Model | KnownModelName | str | Unset = UNSET,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | Unset\n    ) = UNSET,\n    tools: (\n        Sequence[\n            Tool[AgentDepsT]\n            | ToolFuncEither[AgentDepsT, ...]\n        ]\n        | Unset\n    ) = UNSET,\n    instructions: Instructions[AgentDepsT] | Unset = UNSET\n) -> Iterator[None]\n```\n\nContext manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.\n\nThis is particularly useful when testing.\nYou can find an example of this [here](https://ai.pydantic.dev/testing/#overriding-model-via-pytest-fixtures).\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `name` | `str | Unset` | The name to use instead of the name passed to the agent constructor and agent run. | `UNSET` |\n| `deps` | `AgentDepsT | Unset` | The dependencies to use instead of the dependencies passed to the agent run. | `UNSET` |\n| `model` | `Model | KnownModelName | str | Unset` | The model to use instead of the model passed to the agent run. | `UNSET` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | Unset` | The toolsets to use instead of the toolsets passed to the agent constructor and agent run. | `UNSET` |\n| `tools` | `Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | Unset` | The tools to use instead of the tools registered with the agent. | `UNSET` |\n| `instructions` | `Instructions[AgentDepsT] | Unset` | The instructions to use instead of the instructions registered with the agent. | `UNSET` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py`\n\n|  |  |\n| --- | --- |\n| ``` 887 888 889 890 891 892 893 894 895 896 897 898 899 900 901 902 903 904 905 906 907 908 909 910 911 912 913 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 ``` | ``` @contextmanager def override(     self,     *,     name: str | _utils.Unset = _utils.UNSET,     deps: AgentDepsT | _utils.Unset = _utils.UNSET,     model: models.Model | models.KnownModelName | str | _utils.Unset = _utils.UNSET,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | _utils.Unset = _utils.UNSET,     tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | _utils.Unset = _utils.UNSET,     instructions: Instructions[AgentDepsT] | _utils.Unset = _utils.UNSET, ) -> Iterator[None]:     \"\"\"Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.      This is particularly useful when testing.     You can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).      Args:         name: The name to use instead of the name passed to the agent constructor and agent run.         deps: The dependencies to use instead of the dependencies passed to the agent run.         model: The model to use instead of the model passed to the agent run.         toolsets: The toolsets to use instead of the toolsets passed to the agent constructor and agent run.         tools: The tools to use instead of the tools registered with the agent.         instructions: The instructions to use instead of the instructions registered with the agent.     \"\"\"     if workflow.in_workflow():         if _utils.is_set(model):             raise UserError(                 'Model cannot be contextually overridden inside a Temporal workflow, it must be set at agent creation time.'             )         if _utils.is_set(toolsets):             raise UserError(                 'Toolsets cannot be contextually overridden inside a Temporal workflow, they must be set at agent creation time.'             )         if _utils.is_set(tools):             raise UserError(                 'Tools cannot be contextually overridden inside a Temporal workflow, they must be set at agent creation time.'             )      with super().override(         name=name,         deps=deps,         model=model,         toolsets=toolsets,         tools=tools,         instructions=instructions,     ):         yield ``` |", "url": "https://ai.pydantic.dev/durable_exec/index.html#temporalagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "LogfirePlugin", "anchor": "logfireplugin", "md_text": "Bases: `Plugin`\n\nTemporal client plugin for Logfire.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_logfire.py`\n\n|  |  |\n| --- | --- |\n| ``` 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 ``` | ``` class LogfirePlugin(ClientPlugin):     \"\"\"Temporal client plugin for Logfire.\"\"\"      def __init__(self, setup_logfire: Callable[[], Logfire] = _default_setup_logfire, *, metrics: bool = True):         try:             import logfire  # noqa: F401 # pyright: ignore[reportUnusedImport]         except ImportError as _import_error:             raise ImportError(                 'Please install the `logfire` package to use the Logfire plugin, '                 'you can use the `logfire` optional group — `pip install \"pydantic-ai-slim[logfire]\"`'             ) from _import_error          self.setup_logfire = setup_logfire         self.metrics = metrics      def init_client_plugin(self, next: ClientPlugin) -> None:         self.next_client_plugin = next      def configure_client(self, config: ClientConfig) -> ClientConfig:         from opentelemetry.trace import get_tracer         from temporalio.contrib.opentelemetry import TracingInterceptor          interceptors = config.get('interceptors', [])         config['interceptors'] = [*interceptors, TracingInterceptor(get_tracer('temporalio'))]         return self.next_client_plugin.configure_client(config)      async def connect_service_client(self, config: ConnectConfig) -> ServiceClient:         logfire = self.setup_logfire()          if self.metrics:             logfire_config = logfire.config             token = logfire_config.token             if logfire_config.send_to_logfire and token is not None and logfire_config.metrics is not False:                 base_url = logfire_config.advanced.generate_base_url(token)                 metrics_url = base_url + '/v1/metrics'                 headers = {'Authorization': f'Bearer {token}'}                  config.runtime = Runtime(                     telemetry=TelemetryConfig(metrics=OpenTelemetryConfig(url=metrics_url, headers=headers))                 )          return await self.next_client_plugin.connect_service_client(config) ``` |", "url": "https://ai.pydantic.dev/durable_exec/index.html#logfireplugin", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "TemporalRunContext", "anchor": "temporalruncontext", "md_text": "Bases: `RunContext[AgentDepsT]`\n\nThe [`RunContext`](../tools/index.html#pydantic_ai.tools.RunContext) subclass to use to serialize and deserialize the run context for use inside a Temporal activity.\n\nBy default, only the `deps`, `retries`, `tool_call_id`, `tool_name`, `tool_call_approved`, `retry`, `max_retries` and `run_step` attributes will be available.\nTo make another attribute available, create a `TemporalRunContext` subclass with a custom `serialize_run_context` class method that returns a dictionary that includes the attribute and pass it to [`TemporalAgent`](index.html#pydantic_ai.durable_exec.temporal.TemporalAgent).\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_run_context.py`\n\n|  |  |\n| --- | --- |\n| ```  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 ``` | ``` class TemporalRunContext(RunContext[AgentDepsT]):     \"\"\"The [`RunContext`][pydantic_ai.tools.RunContext] subclass to use to serialize and deserialize the run context for use inside a Temporal activity.      By default, only the `deps`, `retries`, `tool_call_id`, `tool_name`, `tool_call_approved`, `retry`, `max_retries` and `run_step` attributes will be available.     To make another attribute available, create a `TemporalRunContext` subclass with a custom `serialize_run_context` class method that returns a dictionary that includes the attribute and pass it to [`TemporalAgent`][pydantic_ai.durable_exec.temporal.TemporalAgent].     \"\"\"      def __init__(self, deps: AgentDepsT, **kwargs: Any):         self.__dict__ = {**kwargs, 'deps': deps}         setattr(             self,             '__dataclass_fields__',             {name: field for name, field in RunContext.__dataclass_fields__.items() if name in self.__dict__},         )      def __getattribute__(self, name: str) -> Any:         try:             return super().__getattribute__(name)         except AttributeError as e:  # pragma: no cover             if name in RunContext.__dataclass_fields__:                 raise UserError(                     f'{self.__class__.__name__!r} object has no attribute {name!r}. '                     'To make the attribute available, create a `TemporalRunContext` subclass with a custom `serialize_run_context` class method that returns a dictionary that includes the attribute and pass it to `TemporalAgent`.'                 )             else:                 raise e      @classmethod     def serialize_run_context(cls, ctx: RunContext[Any]) -> dict[str, Any]:         \"\"\"Serialize the run context to a `dict[str, Any]`.\"\"\"         return {             'retries': ctx.retries,             'tool_call_id': ctx.tool_call_id,             'tool_name': ctx.tool_name,             'tool_call_approved': ctx.tool_call_approved,             'retry': ctx.retry,             'max_retries': ctx.max_retries,             'run_step': ctx.run_step,         }      @classmethod     def deserialize_run_context(cls, ctx: dict[str, Any], deps: AgentDepsT) -> TemporalRunContext[AgentDepsT]:         \"\"\"Deserialize the run context from a `dict[str, Any]`.\"\"\"         return cls(**ctx, deps=deps) ``` |\n\n#### serialize\\_run\\_context `classmethod`\n\n```\nserialize_run_context(\n    ctx: RunContext[Any],\n) -> dict[str, Any]\n```\n\nSerialize the run context to a `dict[str, Any]`.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_run_context.py`\n\n|  |  |\n| --- | --- |\n| ``` 36 37 38 39 40 41 42 43 44 45 46 47 ``` | ``` @classmethod def serialize_run_context(cls, ctx: RunContext[Any]) -> dict[str, Any]:     \"\"\"Serialize the run context to a `dict[str, Any]`.\"\"\"     return {         'retries': ctx.retries,         'tool_call_id': ctx.tool_call_id,         'tool_name': ctx.tool_name,         'tool_call_approved': ctx.tool_call_approved,         'retry': ctx.retry,         'max_retries': ctx.max_retries,         'run_step': ctx.run_step,     } ``` |\n\n#### deserialize\\_run\\_context `classmethod`\n\n```\ndeserialize_run_context(\n    ctx: dict[str, Any], deps: AgentDepsT\n) -> TemporalRunContext[AgentDepsT]\n```\n\nDeserialize the run context from a `dict[str, Any]`.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_run_context.py`\n\n|  |  |\n| --- | --- |\n| ``` 49 50 51 52 ``` | ``` @classmethod def deserialize_run_context(cls, ctx: dict[str, Any], deps: AgentDepsT) -> TemporalRunContext[AgentDepsT]:     \"\"\"Deserialize the run context from a `dict[str, Any]`.\"\"\"     return cls(**ctx, deps=deps) ``` |", "url": "https://ai.pydantic.dev/durable_exec/index.html#temporalruncontext", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PydanticAIPlugin", "anchor": "pydanticaiplugin", "md_text": "Bases: `Plugin`, `Plugin`\n\nTemporal client and worker plugin for Pydantic AI.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/__init__.py`\n\n|  |  |\n| --- | --- |\n| ```  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 ``` | ``` class PydanticAIPlugin(ClientPlugin, WorkerPlugin):     \"\"\"Temporal client and worker plugin for Pydantic AI.\"\"\"      def init_client_plugin(self, next: ClientPlugin) -> None:         self.next_client_plugin = next      def init_worker_plugin(self, next: WorkerPlugin) -> None:         self.next_worker_plugin = next      def configure_client(self, config: ClientConfig) -> ClientConfig:         config['data_converter'] = self._get_new_data_converter(config.get('data_converter'))         return self.next_client_plugin.configure_client(config)      def configure_worker(self, config: WorkerConfig) -> WorkerConfig:         runner = config.get('workflow_runner')  # pyright: ignore[reportUnknownMemberType]         if isinstance(runner, SandboxedWorkflowRunner):  # pragma: no branch             config['workflow_runner'] = replace(                 runner,                 restrictions=runner.restrictions.with_passthrough_modules(                     'pydantic_ai',                     'pydantic',                     'pydantic_core',                     'logfire',                     'rich',                     'httpx',                     'anyio',                     'httpcore',                     # Imported inside `logfire._internal.json_encoder` when running `logfire.info` inside an activity with attributes to serialize                     'attrs',                     # Imported inside `logfire._internal.json_schema` when running `logfire.info` inside an activity with attributes to serialize                     'numpy',                     'pandas',                 ),             )          config['workflow_failure_exception_types'] = [             *config.get('workflow_failure_exception_types', []),  # pyright: ignore[reportUnknownMemberType]             UserError,             PydanticUserError,         ]          return self.next_worker_plugin.configure_worker(config)      async def connect_service_client(self, config: ConnectConfig) -> ServiceClient:         return await self.next_client_plugin.connect_service_client(config)      async def run_worker(self, worker: Worker) -> None:         await self.next_worker_plugin.run_worker(worker)      def configure_replayer(self, config: ReplayerConfig) -> ReplayerConfig:  # pragma: no cover         config['data_converter'] = self._get_new_data_converter(config.get('data_converter'))  # pyright: ignore[reportUnknownMemberType]         return self.next_worker_plugin.configure_replayer(config)      def run_replayer(         self,         replayer: Replayer,         histories: AsyncIterator[WorkflowHistory],     ) -> AbstractAsyncContextManager[AsyncIterator[WorkflowReplayResult]]:  # pragma: no cover         return self.next_worker_plugin.run_replayer(replayer, histories)      def _get_new_data_converter(self, converter: DataConverter | None) -> DataConverter:         if converter and converter.payload_converter_class not in (             DefaultPayloadConverter,             PydanticPayloadConverter,         ):             warnings.warn(  # pragma: no cover                 'A non-default Temporal data converter was used which has been replaced with the Pydantic data converter.'             )          return pydantic_data_converter ``` |", "url": "https://ai.pydantic.dev/durable_exec/index.html#pydanticaiplugin", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "AgentPlugin", "anchor": "agentplugin", "md_text": "Bases: `Plugin`\n\nTemporal worker plugin for a specific Pydantic AI agent.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/temporal/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 ``` | ``` class AgentPlugin(WorkerPlugin):     \"\"\"Temporal worker plugin for a specific Pydantic AI agent.\"\"\"      def __init__(self, agent: TemporalAgent[Any, Any]):         self.agent = agent      def init_worker_plugin(self, next: WorkerPlugin) -> None:         self.next_worker_plugin = next      def configure_worker(self, config: WorkerConfig) -> WorkerConfig:         activities: Sequence[Callable[..., Any]] = config.get('activities', [])  # pyright: ignore[reportUnknownMemberType]         # Activities are checked for name conflicts by Temporal.         config['activities'] = [*activities, *self.agent.temporal_activities]         return self.next_worker_plugin.configure_worker(config)      async def run_worker(self, worker: Worker) -> None:         await self.next_worker_plugin.run_worker(worker)      def configure_replayer(self, config: ReplayerConfig) -> ReplayerConfig:  # pragma: no cover         return self.next_worker_plugin.configure_replayer(config)      def run_replayer(         self,         replayer: Replayer,         histories: AsyncIterator[WorkflowHistory],     ) -> AbstractAsyncContextManager[AsyncIterator[WorkflowReplayResult]]:  # pragma: no cover         return self.next_worker_plugin.run_replayer(replayer, histories) ``` |", "url": "https://ai.pydantic.dev/durable_exec/index.html#agentplugin", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "DBOSAgent", "anchor": "dbosagent", "md_text": "Bases: `WrapperAgent[AgentDepsT, OutputDataT]`, `DBOSConfiguredInstance`\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py`", "url": "https://ai.pydantic.dev/durable_exec/index.html#dbosagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "DBOSAgent", "anchor": "dbosagent", "md_text": "|  |  |\n| --- | --- |\n| ```  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 ``` | ``` @DBOS.dbos_class() class DBOSAgent(WrapperAgent[AgentDepsT, OutputDataT], DBOSConfiguredInstance):     def __init__(         self,         wrapped: AbstractAgent[AgentDepsT, OutputDataT],         *,         name: str | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,         mcp_step_config: StepConfig | None = None,         model_step_config: StepConfig | None = None,     ):         \"\"\"Wrap an agent to enable it with DBOS durable workflows, by automatically offloading model requests, tool calls, and MCP server communication to DBOS steps.          After wrapping, the original agent can still be used as normal outside of the DBOS workflow.          Args:             wrapped: The agent to wrap.             name: Optional unique agent name to use as the DBOS configured instance name. If not provided, the agent's `name` will be used.             event_stream_handler: Optional event stream handler to use instead of the one set on the wrapped agent.             mcp_step_config: The base DBOS step config to use for MCP server steps. If no config is provided, use the default settings of DBOS.             model_step_config: The DBOS step config to use for model request steps. If no config is provided, use the default settings of DBOS.         \"\"\"         super().__init__(wrapped)          self._name = name or wrapped.name         self._event_stream_handler = event_stream_handler         if self._name is None:             raise UserError(                 \"An agent needs to have a unique `name` in order to be used with DBOS. The name will be used to identify the agent's workflows and steps.\"             )          # Merge the config with the default DBOS config         self._mcp_step_config = mcp_step_config or {}         self._model_step_config = model_step_config or {}          if not isinstance(wrapped.model, Model):             raise UserError(                 'An agent needs to have a `model` in order to be used with DBOS, it cannot be set at agent run time.'             )          dbos_model = DBOSModel(             wrapped.model,             step_name_prefix=self._name,             step_config=self._model_step_config,             event_stream_handler=self.event_stream_handler,         )         self._model = dbos_model          dbosagent_name = self._name          def dbosify_toolset(toolset: AbstractToolset[AgentDepsT]) -> AbstractToolset[AgentDepsT]:             # Replace MCPServer with DBOSMCPServer             try:                 from pydantic_ai.mcp import MCPServer                  from ._mcp_server import DBOSMCPServer             except ImportError:                 pass             else:                 if isinstance(toolset, MCPServer):                     return DBOSMCPServer(                         wrapped=toolset,                         step_name_prefix=dbosagent_name,                         step_config=self._mcp_step_config,                     )              return toolset          dbos_toolsets = [toolset.visit_and_replace(dbosify_toolset) for toolset in wrapped.toolsets]         self._toolsets = dbos_toolsets         DBOSConfiguredInstance.__init__(self, self._name)          # Wrap the `run` method in a DBOS workflow         @DBOS.workflow(name=f'{self._name}.run')         async def wrapped_run_workflow(             user_prompt: str | Sequence[_messages.UserContent] | None = None,             *,             output_type: OutputSpec[RunOutputDataT] | None = None,             message_history: Sequence[_messages.ModelMessage] | None = None,             deferred_tool_results: DeferredToolResults | None = None,             model: models.Model | models.KnownModelName | str | None = None,             deps: AgentDepsT,             model_settings: ModelSettings | None = None,             usage_limits: _usage.UsageLimits | None = None,             usage: _usage.RunUsage | None = None,             infer_name: bool = True,             toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,             builtin_tools: Sequence[AbstractBuiltinTool] | None = None,             event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,             **_deprecated_kwargs: Never,         ) -> AgentRunResult[Any]:             with self._dbos_overrides():                 return await super(WrapperAgent, self).run(                     user_prompt,                     output_type=output_type,                     message_history=message_history,                     deferred_tool_results=deferred_tool_results,                     model=model,                     deps=deps,                     model_settings=model_settings,                     usage_limits=usage_limits,                     usage=usage,                     infer_name=infer_name,                     toolsets=toolsets,                     builtin_tools=builtin_tools,                     event_stream_handler=event_stream_handler,                     **_deprecated_kwargs,                 )          self.dbos_wrapped_run_workflow = wrapped_run_workflow          # Wrap the `run_sync` method in a DBOS workflow         @DBOS.workflow(name=f'{self._name}.run_sync')         def wrapped_run_sync_workflow(             user_prompt: str | Sequence[_messages.UserContent] | None = None,             *,             output_type: OutputSpec[RunOutputDataT] | None = None,             message_history: Sequence[_messages.ModelMessage] | None = None,             deferred_tool_results: DeferredToolResults | None = None,             model: models.Model | models.KnownModelName | str | None = None,             deps: AgentDepsT,             model_settings: ModelSettings | None = None,             usage_limits: _usage.UsageLimits | None = None,             usage: _usage.RunUsage | None = None,             infer_name: bool = True,             toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,             builtin_tools: Sequence[AbstractBuiltinTool] | None = None,             event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,             **_deprecated_kwargs: Never,         ) -> AgentRunResult[Any]:             with self._dbos_overrides():                 return super(DBOSAgent, self).run_sync(                     user_prompt,                     output_type=output_type,                     message_history=message_history,                     deferred_tool_results=deferred_tool_results,                     model=model,                     deps=deps,                     model_settings=model_settings,                     usage_limits=usage_limits,                     usage=usage,                     infer_name=infer_name,                     toolsets=toolsets,                     builtin_tools=builtin_tools,                     event_stream_handler=event_stream_handler,                     **_deprecated_kwargs,                 )          self.dbos_wrapped_run_sync_workflow = wrapped_run_sync_workflow      @property     def name(self) -> str | None:         return self._name      @name.setter     def name(self, value: str | None) -> None:  # pragma: no cover         raise UserError(             'The agent name cannot be changed after creation. If you need to change the name, create a new agent.'         )      @property     def model(self) -> Model:         return self._model      @property     def event_stream_handler(self) -> EventStreamHandler[AgentDepsT] | None:         handler = self._event_stream_handler or super().event_stream_handler         if handler is None:             return None         elif DBOS.workflow_id is not None and DBOS.step_id is None:             # Special case if it's in a DBOS workflow but not a step, we need to iterate through all events and call the handler.             return self._call_event_stream_handler_in_workflow         else:             return handler      async def _call_event_stream_handler_in_workflow(         self, ctx: RunContext[AgentDepsT], stream: AsyncIterable[_messages.AgentStreamEvent]     ) -> None:         handler = self._event_stream_handler or super().event_stream_handler         assert handler is not None          async def streamed_response(event: _messages.AgentStreamEvent):             yield event          async for event in stream:             await handler(ctx, streamed_response(event))      @property     def toolsets(self) -> Sequence[AbstractToolset[AgentDepsT]]:         with self._dbos_overrides():             return super().toolsets      @contextmanager     def _dbos_overrides(self) -> Iterator[None]:         # Override with DBOSModel and DBOSMCPServer in the toolsets.         with (             super().override(model=self._model, toolsets=self._toolsets, tools=[]),             self.sequential_tool_calls(),         ):             yield      @overload     async def run(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AgentRunResult[OutputDataT]: ...      @overload     async def run(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT],         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AgentRunResult[RunOutputDataT]: ...      async def run(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT] | None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,         **_deprecated_kwargs: Never,     ) -> AgentRunResult[Any]:         \"\"\"Run the agent with a user prompt in async mode.          This method builds an internal agent graph (using system prompts, tools and result schemas) and then         runs the graph to completion. The result of the run is returned.          Example:         ```python         from pydantic_ai import Agent          agent = Agent('openai:gpt-4o')          async def main():             agent_run = await agent.run('What is the capital of France?')             print(agent_run.output)             #> The capital of France is Paris.         ```          Args:             user_prompt: User input to start/continue the conversation.             output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no                 output validators since output validators would expect an argument that matches the agent's output type.             message_history: History of the conversation so far.             deferred_tool_results: Optional results for deferred tool calls in the message history.             model: Optional model to use for this run, required if `model` was not set when creating the agent.             deps: Optional dependencies to use for this run.             model_settings: Optional settings to use for this model's request.             usage_limits: Optional limits on model request count or token usage.             usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.             infer_name: Whether to try to infer the agent name from the call frame if it's not set.             toolsets: Optional additional toolsets for this run.             builtin_tools: Optional additional builtin tools for this run.             event_stream_handler: Optional event stream handler to use for this run.          Returns:             The result of the run.         \"\"\"         return await self.dbos_wrapped_run_workflow(             user_prompt,             output_type=output_type,             message_history=message_history,             deferred_tool_results=deferred_tool_results,             model=model,             deps=deps,             model_settings=model_settings,             usage_limits=usage_limits,             usage=usage,             infer_name=infer_name,             toolsets=toolsets,             builtin_tools=builtin_tools,             event_stream_handler=event_stream_handler,             **_deprecated_kwargs,         )      @overload     def run_sync(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AgentRunResult[OutputDataT]: ...      @overload     def run_sync(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT],         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AgentRunResult[RunOutputDataT]: ...      def run_sync(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT] | None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,         **_deprecated_kwargs: Never,     ) -> AgentRunResult[Any]:         \"\"\"Synchronously run the agent with a user prompt.          This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] with `loop.run_until_complete(...)`.         You therefore can't use this method inside async code or if there's an active event loop.          Example:         ```python         from pydantic_ai import Agent          agent = Agent('openai:gpt-4o')          result_sync = agent.run_sync('What is the capital of Italy?')         print(result_sync.output)         #> The capital of Italy is Rome.         ```          Args:             user_prompt: User input to start/continue the conversation.             output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no                 output validators since output validators would expect an argument that matches the agent's output type.             message_history: History of the conversation so far.             deferred_tool_results: Optional results for deferred tool calls in the message history.             model: Optional model to use for this run, required if `model` was not set when creating the agent.             deps: Optional dependencies to use for this run.             model_settings: Optional settings to use for this model's request.             usage_limits: Optional limits on model request count or token usage.             usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.             infer_name: Whether to try to infer the agent name from the call frame if it's not set.             toolsets: Optional additional toolsets for this run.             builtin_tools: Optional additional builtin tools for this run.             event_stream_handler: Optional event stream handler to use for this run.          Returns:             The result of the run.         \"\"\"         return self.dbos_wrapped_run_sync_workflow(             user_prompt,             output_type=output_type,             message_history=message_history,             deferred_tool_results=deferred_tool_results,             model=model,             deps=deps,             model_settings=model_settings,             usage_limits=usage_limits,             usage=usage,             infer_name=infer_name,             toolsets=toolsets,             builtin_tools=builtin_tools,             event_stream_handler=event_stream_handler,             **_deprecated_kwargs,         )      @overload     def run_stream(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AbstractAsyncContextManager[StreamedRunResult[AgentDepsT, OutputDataT]]: ...      @overload     def run_stream(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT],         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AbstractAsyncContextManager[StreamedRunResult[AgentDepsT, RunOutputDataT]]: ...      @asynccontextmanager     async def run_stream(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT] | None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,         **_deprecated_kwargs: Never,     ) -> AsyncIterator[StreamedRunResult[AgentDepsT, Any]]:         \"\"\"Run the agent with a user prompt in async mode, returning a streamed response.          Example:         ```python         from pydantic_ai import Agent          agent = Agent('openai:gpt-4o')          async def main():             async with agent.run_stream('What is the capital of the UK?') as response:                 print(await response.get_output())                 #> The capital of the UK is London.         ```          Args:             user_prompt: User input to start/continue the conversation.             output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no                 output validators since output validators would expect an argument that matches the agent's output type.             message_history: History of the conversation so far.             deferred_tool_results: Optional results for deferred tool calls in the message history.             model: Optional model to use for this run, required if `model` was not set when creating the agent.             deps: Optional dependencies to use for this run.             model_settings: Optional settings to use for this model's request.             usage_limits: Optional limits on model request count or token usage.             usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.             infer_name: Whether to try to infer the agent name from the call frame if it's not set.             toolsets: Optional additional toolsets for this run.             builtin_tools: Optional additional builtin tools for this run.             event_stream_handler: Optional event stream handler to use for this run. It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager.          Returns:             The result of the run.         \"\"\"         if DBOS.workflow_id is not None and DBOS.step_id is None:             raise UserError(                 '`agent.run_stream()` cannot be used inside a DBOS workflow. '                 'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'             )          async with super().run_stream(             user_prompt,             output_type=output_type,             message_history=message_history,             deferred_tool_results=deferred_tool_results,             model=model,             deps=deps,             model_settings=model_settings,             usage_limits=usage_limits,             usage=usage,             infer_name=infer_name,             toolsets=toolsets,             builtin_tools=builtin_tools,             event_stream_handler=event_stream_handler,             **_deprecated_kwargs,         ) as result:             yield result      @overload     def run_stream_events(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[OutputDataT]]: ...      @overload     def run_stream_events(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT],         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[RunOutputDataT]]: ...      def run_stream_events(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT] | None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[Any]]:         \"\"\"Run the agent with a user prompt in async mode and stream events from the run.          This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] and         uses the `event_stream_handler` kwarg to get a stream of events from the run.          Example:         ```python         from pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent          agent = Agent('openai:gpt-4o')          async def main():             events: list[AgentStreamEvent | AgentRunResultEvent] = []             async for event in agent.run_stream_events('What is the capital of France?'):                 events.append(event)             print(events)             '''             [                 PartStartEvent(index=0, part=TextPart(content='The capital of ')),                 FinalResultEvent(tool_name=None, tool_call_id=None),                 PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='France is Paris. ')),                 AgentRunResultEvent(                     result=AgentRunResult(output='The capital of France is Paris. ')                 ),             ]             '''         ```          Arguments are the same as for [`self.run`][pydantic_ai.agent.AbstractAgent.run],         except that `event_stream_handler` is now allowed.          Args:             user_prompt: User input to start/continue the conversation.             output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no                 output validators since output validators would expect an argument that matches the agent's output type.             message_history: History of the conversation so far.             deferred_tool_results: Optional results for deferred tool calls in the message history.             model: Optional model to use for this run, required if `model` was not set when creating the agent.             deps: Optional dependencies to use for this run.             model_settings: Optional settings to use for this model's request.             usage_limits: Optional limits on model request count or token usage.             usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.             infer_name: Whether to try to infer the agent name from the call frame if it's not set.             toolsets: Optional additional toolsets for this run.             builtin_tools: Optional additional builtin tools for this run.          Returns:             An async iterable of stream events `AgentStreamEvent` and finally a `AgentRunResultEvent` with the final             run result.         \"\"\"         raise UserError(             '`agent.run_stream_events()` cannot be used with DBOS. '             'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'         )      @overload     def iter(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         **_deprecated_kwargs: Never,     ) -> AbstractAsyncContextManager[AgentRun[AgentDepsT, OutputDataT]]: ...      @overload     def iter(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT],         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         **_deprecated_kwargs: Never,     ) -> AbstractAsyncContextManager[AgentRun[AgentDepsT, RunOutputDataT]]: ...      @asynccontextmanager     async def iter(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT] | None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         **_deprecated_kwargs: Never,     ) -> AsyncIterator[AgentRun[AgentDepsT, Any]]:         \"\"\"A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.          This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an         `AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are         executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the         stream of events coming from the execution of tools.          The `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,         and the final result of the run once it has completed.          For more details, see the documentation of `AgentRun`.          Example:         ```python         from pydantic_ai import Agent          agent = Agent('openai:gpt-4o')          async def main():             nodes = []             async with agent.iter('What is the capital of France?') as agent_run:                 async for node in agent_run:                     nodes.append(node)             print(nodes)             '''             [                 UserPromptNode(                     user_prompt='What is the capital of France?',                     instructions_functions=[],                     system_prompts=(),                     system_prompt_functions=[],                     system_prompt_dynamic_functions={},                 ),                 ModelRequestNode(                     request=ModelRequest(                         parts=[                             UserPromptPart(                                 content='What is the capital of France?',                                 timestamp=datetime.datetime(...),                             )                         ]                     )                 ),                 CallToolsNode(                     model_response=ModelResponse(                         parts=[TextPart(content='The capital of France is Paris.')],                         usage=RequestUsage(input_tokens=56, output_tokens=7),                         model_name='gpt-4o',                         timestamp=datetime.datetime(...),                     )                 ),                 End(data=FinalResult(output='The capital of France is Paris.')),             ]             '''             print(agent_run.result.output)             #> The capital of France is Paris.         ```          Args:             user_prompt: User input to start/continue the conversation.             output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no                 output validators since output validators would expect an argument that matches the agent's output type.             message_history: History of the conversation so far.             deferred_tool_results: Optional results for deferred tool calls in the message history.             model: Optional model to use for this run, required if `model` was not set when creating the agent.             deps: Optional dependencies to use for this run.             model_settings: Optional settings to use for this model's request.             usage_limits: Optional limits on model request count or token usage.             usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.             infer_name: Whether to try to infer the agent name from the call frame if it's not set.             toolsets: Optional additional toolsets for this run.             builtin_tools: Optional additional builtin tools for this run.          Returns:             The result of the run.         \"\"\"         if model is not None and not isinstance(model, DBOSModel):             raise UserError(                 'Non-DBOS model cannot be set at agent run time inside a DBOS workflow, it must be set at agent creation time.'             )          with self._dbos_overrides():             async with super().iter(                 user_prompt=user_prompt,                 output_type=output_type,                 message_history=message_history,                 deferred_tool_results=deferred_tool_results,                 model=model,                 deps=deps,                 model_settings=model_settings,                 usage_limits=usage_limits,                 usage=usage,                 infer_name=infer_name,                 toolsets=toolsets,                 builtin_tools=builtin_tools,                 **_deprecated_kwargs,             ) as run:                 yield run      @contextmanager     def override(         self,         *,         name: str | _utils.Unset = _utils.UNSET,         deps: AgentDepsT | _utils.Unset = _utils.UNSET,         model: models.Model | models.KnownModelName | str | _utils.Unset = _utils.UNSET,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | _utils.Unset = _utils.UNSET,         tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | _utils.Unset = _utils.UNSET,         instructions: Instructions[AgentDepsT] | _utils.Unset = _utils.UNSET,     ) -> Iterator[None]:         \"\"\"Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.          This is particularly useful when testing.         You can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).          Args:             name: The name to use instead of the name passed to the agent constructor and agent run.             deps: The dependencies to use instead of the dependencies passed to the agent run.             model: The model to use instead of the model passed to the agent run.             toolsets: The toolsets to use instead of the toolsets passed to the agent constructor and agent run.             tools: The tools to use instead of the tools registered with the agent.             instructions: The instructions to use instead of the instructions registered with the agent.         \"\"\"         if _utils.is_set(model) and not isinstance(model, (DBOSModel)):             raise UserError(                 'Non-DBOS model cannot be contextually overridden inside a DBOS workflow, it must be set at agent creation time.'             )          with super().override(             name=name,             deps=deps,             model=model,             toolsets=toolsets,             tools=tools,             instructions=instructions,         ):             yield ``` |", "url": "https://ai.pydantic.dev/durable_exec/index.html#dbosagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "DBOSAgent", "anchor": "dbosagent", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    wrapped: AbstractAgent[AgentDepsT, OutputDataT],\n    *,\n    name: str | None = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None,\n    mcp_step_config: StepConfig | None = None,\n    model_step_config: StepConfig | None = None\n)\n```\n\nWrap an agent to enable it with DBOS durable workflows, by automatically offloading model requests, tool calls, and MCP server communication to DBOS steps.\n\nAfter wrapping, the original agent can still be used as normal outside of the DBOS workflow.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `wrapped` | `AbstractAgent[AgentDepsT, OutputDataT]` | The agent to wrap. | *required* |\n| `name` | `str | None` | Optional unique agent name to use as the DBOS configured instance name. If not provided, the agent's `name` will be used. | `None` |\n| `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional event stream handler to use instead of the one set on the wrapped agent. | `None` |\n| `mcp_step_config` | `StepConfig | None` | The base DBOS step config to use for MCP server steps. If no config is provided, use the default settings of DBOS. | `None` |\n| `model_step_config` | `StepConfig | None` | The DBOS step config to use for model request steps. If no config is provided, use the default settings of DBOS. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py`", "url": "https://ai.pydantic.dev/durable_exec/index.html#dbosagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "DBOSAgent", "anchor": "dbosagent", "md_text": "|  |  |\n| --- | --- |\n| ```  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 ``` | ``` def __init__(     self,     wrapped: AbstractAgent[AgentDepsT, OutputDataT],     *,     name: str | None = None,     event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     mcp_step_config: StepConfig | None = None,     model_step_config: StepConfig | None = None, ):     \"\"\"Wrap an agent to enable it with DBOS durable workflows, by automatically offloading model requests, tool calls, and MCP server communication to DBOS steps.      After wrapping, the original agent can still be used as normal outside of the DBOS workflow.      Args:         wrapped: The agent to wrap.         name: Optional unique agent name to use as the DBOS configured instance name. If not provided, the agent's `name` will be used.         event_stream_handler: Optional event stream handler to use instead of the one set on the wrapped agent.         mcp_step_config: The base DBOS step config to use for MCP server steps. If no config is provided, use the default settings of DBOS.         model_step_config: The DBOS step config to use for model request steps. If no config is provided, use the default settings of DBOS.     \"\"\"     super().__init__(wrapped)      self._name = name or wrapped.name     self._event_stream_handler = event_stream_handler     if self._name is None:         raise UserError(             \"An agent needs to have a unique `name` in order to be used with DBOS. The name will be used to identify the agent's workflows and steps.\"         )      # Merge the config with the default DBOS config     self._mcp_step_config = mcp_step_config or {}     self._model_step_config = model_step_config or {}      if not isinstance(wrapped.model, Model):         raise UserError(             'An agent needs to have a `model` in order to be used with DBOS, it cannot be set at agent run time.'         )      dbos_model = DBOSModel(         wrapped.model,         step_name_prefix=self._name,         step_config=self._model_step_config,         event_stream_handler=self.event_stream_handler,     )     self._model = dbos_model      dbosagent_name = self._name      def dbosify_toolset(toolset: AbstractToolset[AgentDepsT]) -> AbstractToolset[AgentDepsT]:         # Replace MCPServer with DBOSMCPServer         try:             from pydantic_ai.mcp import MCPServer              from ._mcp_server import DBOSMCPServer         except ImportError:             pass         else:             if isinstance(toolset, MCPServer):                 return DBOSMCPServer(                     wrapped=toolset,                     step_name_prefix=dbosagent_name,                     step_config=self._mcp_step_config,                 )          return toolset      dbos_toolsets = [toolset.visit_and_replace(dbosify_toolset) for toolset in wrapped.toolsets]     self._toolsets = dbos_toolsets     DBOSConfiguredInstance.__init__(self, self._name)      # Wrap the `run` method in a DBOS workflow     @DBOS.workflow(name=f'{self._name}.run')     async def wrapped_run_workflow(         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT] | None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,         **_deprecated_kwargs: Never,     ) -> AgentRunResult[Any]:         with self._dbos_overrides():             return await super(WrapperAgent, self).run(                 user_prompt,                 output_type=output_type,                 message_history=message_history,                 deferred_tool_results=deferred_tool_results,                 model=model,                 deps=deps,                 model_settings=model_settings,                 usage_limits=usage_limits,                 usage=usage,                 infer_name=infer_name,                 toolsets=toolsets,                 builtin_tools=builtin_tools,                 event_stream_handler=event_stream_handler,                 **_deprecated_kwargs,             )      self.dbos_wrapped_run_workflow = wrapped_run_workflow      # Wrap the `run_sync` method in a DBOS workflow     @DBOS.workflow(name=f'{self._name}.run_sync')     def wrapped_run_sync_workflow(         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT] | None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,         **_deprecated_kwargs: Never,     ) -> AgentRunResult[Any]:         with self._dbos_overrides():             return super(DBOSAgent, self).run_sync(                 user_prompt,                 output_type=output_type,                 message_history=message_history,                 deferred_tool_results=deferred_tool_results,                 model=model,                 deps=deps,                 model_settings=model_settings,                 usage_limits=usage_limits,                 usage=usage,                 infer_name=infer_name,                 toolsets=toolsets,                 builtin_tools=builtin_tools,                 event_stream_handler=event_stream_handler,                 **_deprecated_kwargs,             )      self.dbos_wrapped_run_sync_workflow = wrapped_run_sync_workflow ``` |", "url": "https://ai.pydantic.dev/durable_exec/index.html#dbosagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "DBOSAgent", "anchor": "dbosagent", "md_text": "#### run `async`\n\n```\nrun(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[OutputDataT]\n\nrun(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[RunOutputDataT]\n\nrun(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AgentRunResult[Any]\n```\n\nRun the agent with a user prompt in async mode.\n\nThis method builds an internal agent graph (using system prompts, tools and result schemas) and then\nruns the graph to completion. The result of the run is returned.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    agent_run = await agent.run('What is the capital of France?')\n    print(agent_run.output)\n    #> The capital of France is Paris.\n```\n\nParameters:", "url": "https://ai.pydantic.dev/durable_exec/index.html#dbosagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "DBOSAgent", "anchor": "dbosagent", "md_text": "| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` |\n| `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` |\n| `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n| `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional event stream handler to use for this run. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AgentRunResult[Any]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py`", "url": "https://ai.pydantic.dev/durable_exec/index.html#dbosagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "DBOSAgent", "anchor": "dbosagent", "md_text": "|  |  |\n| --- | --- |\n| ``` 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 ``` | ``` async def run(     self,     user_prompt: str | Sequence[_messages.UserContent] | None = None,     *,     output_type: OutputSpec[RunOutputDataT] | None = None,     message_history: Sequence[_messages.ModelMessage] | None = None,     deferred_tool_results: DeferredToolResults | None = None,     model: models.Model | models.KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: _usage.UsageLimits | None = None,     usage: _usage.RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     **_deprecated_kwargs: Never, ) -> AgentRunResult[Any]:     \"\"\"Run the agent with a user prompt in async mode.      This method builds an internal agent graph (using system prompts, tools and result schemas) and then     runs the graph to completion. The result of the run is returned.      Example:     ```python     from pydantic_ai import Agent      agent = Agent('openai:gpt-4o')      async def main():         agent_run = await agent.run('What is the capital of France?')         print(agent_run.output)         #> The capital of France is Paris.     ```      Args:         user_prompt: User input to start/continue the conversation.         output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no             output validators since output validators would expect an argument that matches the agent's output type.         message_history: History of the conversation so far.         deferred_tool_results: Optional results for deferred tool calls in the message history.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.         builtin_tools: Optional additional builtin tools for this run.         event_stream_handler: Optional event stream handler to use for this run.      Returns:         The result of the run.     \"\"\"     return await self.dbos_wrapped_run_workflow(         user_prompt,         output_type=output_type,         message_history=message_history,         deferred_tool_results=deferred_tool_results,         model=model,         deps=deps,         model_settings=model_settings,         usage_limits=usage_limits,         usage=usage,         infer_name=infer_name,         toolsets=toolsets,         builtin_tools=builtin_tools,         event_stream_handler=event_stream_handler,         **_deprecated_kwargs,     ) ``` |\n\n#### run\\_sync\n\n```\nrun_sync(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[OutputDataT]", "url": "https://ai.pydantic.dev/durable_exec/index.html#dbosagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "DBOSAgent", "anchor": "dbosagent", "md_text": "run_sync(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[RunOutputDataT]\n\nrun_sync(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AgentRunResult[Any]\n```\n\nSynchronously run the agent with a user prompt.\n\nThis is a convenience method that wraps [`self.run`](../agent/index.html#pydantic_ai.agent.AbstractAgent.run) with `loop.run_until_complete(...)`.\nYou therefore can't use this method inside async code or if there's an active event loop.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nresult_sync = agent.run_sync('What is the capital of Italy?')\nprint(result_sync.output)\n#> The capital of Italy is Rome.\n```\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` |\n| `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` |\n| `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n| `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional event stream handler to use for this run. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AgentRunResult[Any]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py`", "url": "https://ai.pydantic.dev/durable_exec/index.html#dbosagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "DBOSAgent", "anchor": "dbosagent", "md_text": "|  |  |\n| --- | --- |\n| ``` 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 ``` | ``` def run_sync(     self,     user_prompt: str | Sequence[_messages.UserContent] | None = None,     *,     output_type: OutputSpec[RunOutputDataT] | None = None,     message_history: Sequence[_messages.ModelMessage] | None = None,     deferred_tool_results: DeferredToolResults | None = None,     model: models.Model | models.KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: _usage.UsageLimits | None = None,     usage: _usage.RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     **_deprecated_kwargs: Never, ) -> AgentRunResult[Any]:     \"\"\"Synchronously run the agent with a user prompt.      This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] with `loop.run_until_complete(...)`.     You therefore can't use this method inside async code or if there's an active event loop.      Example:     ```python     from pydantic_ai import Agent      agent = Agent('openai:gpt-4o')      result_sync = agent.run_sync('What is the capital of Italy?')     print(result_sync.output)     #> The capital of Italy is Rome.     ```      Args:         user_prompt: User input to start/continue the conversation.         output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no             output validators since output validators would expect an argument that matches the agent's output type.         message_history: History of the conversation so far.         deferred_tool_results: Optional results for deferred tool calls in the message history.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.         builtin_tools: Optional additional builtin tools for this run.         event_stream_handler: Optional event stream handler to use for this run.      Returns:         The result of the run.     \"\"\"     return self.dbos_wrapped_run_sync_workflow(         user_prompt,         output_type=output_type,         message_history=message_history,         deferred_tool_results=deferred_tool_results,         model=model,         deps=deps,         model_settings=model_settings,         usage_limits=usage_limits,         usage=usage,         infer_name=infer_name,         toolsets=toolsets,         builtin_tools=builtin_tools,         event_stream_handler=event_stream_handler,         **_deprecated_kwargs,     ) ``` |\n\n#### run\\_stream `async`\n\n```\nrun_stream(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AbstractAsyncContextManager[\n    StreamedRunResult[AgentDepsT, OutputDataT]\n]", "url": "https://ai.pydantic.dev/durable_exec/index.html#dbosagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "DBOSAgent", "anchor": "dbosagent", "md_text": "run_stream(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AbstractAsyncContextManager[\n    StreamedRunResult[AgentDepsT, RunOutputDataT]\n]\n\nrun_stream(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AsyncIterator[StreamedRunResult[AgentDepsT, Any]]\n```\n\nRun the agent with a user prompt in async mode, returning a streamed response.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    async with agent.run_stream('What is the capital of the UK?') as response:\n        print(await response.get_output())\n        #> The capital of the UK is London.\n```\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` |\n| `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` |\n| `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n| `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional event stream handler to use for this run. It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AsyncIterator[StreamedRunResult[AgentDepsT, Any]]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py`", "url": "https://ai.pydantic.dev/durable_exec/index.html#dbosagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "DBOSAgent", "anchor": "dbosagent", "md_text": "|  |  |\n| --- | --- |\n| ``` 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 ``` | ``` @asynccontextmanager async def run_stream(     self,     user_prompt: str | Sequence[_messages.UserContent] | None = None,     *,     output_type: OutputSpec[RunOutputDataT] | None = None,     message_history: Sequence[_messages.ModelMessage] | None = None,     deferred_tool_results: DeferredToolResults | None = None,     model: models.Model | models.KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: _usage.UsageLimits | None = None,     usage: _usage.RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     **_deprecated_kwargs: Never, ) -> AsyncIterator[StreamedRunResult[AgentDepsT, Any]]:     \"\"\"Run the agent with a user prompt in async mode, returning a streamed response.      Example:     ```python     from pydantic_ai import Agent      agent = Agent('openai:gpt-4o')      async def main():         async with agent.run_stream('What is the capital of the UK?') as response:             print(await response.get_output())             #> The capital of the UK is London.     ```      Args:         user_prompt: User input to start/continue the conversation.         output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no             output validators since output validators would expect an argument that matches the agent's output type.         message_history: History of the conversation so far.         deferred_tool_results: Optional results for deferred tool calls in the message history.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.         builtin_tools: Optional additional builtin tools for this run.         event_stream_handler: Optional event stream handler to use for this run. It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager.      Returns:         The result of the run.     \"\"\"     if DBOS.workflow_id is not None and DBOS.step_id is None:         raise UserError(             '`agent.run_stream()` cannot be used inside a DBOS workflow. '             'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'         )      async with super().run_stream(         user_prompt,         output_type=output_type,         message_history=message_history,         deferred_tool_results=deferred_tool_results,         model=model,         deps=deps,         model_settings=model_settings,         usage_limits=usage_limits,         usage=usage,         infer_name=infer_name,         toolsets=toolsets,         builtin_tools=builtin_tools,         event_stream_handler=event_stream_handler,         **_deprecated_kwargs,     ) as result:         yield result ``` |\n\n#### run\\_stream\\_events\n\n```\nrun_stream_events(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[\n    AgentStreamEvent | AgentRunResultEvent[OutputDataT]\n]", "url": "https://ai.pydantic.dev/durable_exec/index.html#dbosagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "DBOSAgent", "anchor": "dbosagent", "md_text": "run_stream_events(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[\n    AgentStreamEvent | AgentRunResultEvent[RunOutputDataT]\n]\n\nrun_stream_events(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[\n    AgentStreamEvent | AgentRunResultEvent[Any]\n]\n```\n\nRun the agent with a user prompt in async mode and stream events from the run.\n\nThis is a convenience method that wraps [`self.run`](../agent/index.html#pydantic_ai.agent.AbstractAgent.run) and\nuses the `event_stream_handler` kwarg to get a stream of events from the run.\n\nExample:\n\n```\nfrom pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    events: list[AgentStreamEvent | AgentRunResultEvent] = []\n    async for event in agent.run_stream_events('What is the capital of France?'):\n        events.append(event)\n    print(events)\n    '''\n    [\n        PartStartEvent(index=0, part=TextPart(content='The capital of ')),\n        FinalResultEvent(tool_name=None, tool_call_id=None),\n        PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='France is Paris. ')),\n        AgentRunResultEvent(\n            result=AgentRunResult(output='The capital of France is Paris. ')\n        ),\n    ]\n    '''\n```\n\nArguments are the same as for [`self.run`](../agent/index.html#pydantic_ai.agent.AbstractAgent.run),\nexcept that `event_stream_handler` is now allowed.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` |\n| `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` |\n| `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AsyncIterator[AgentStreamEvent | AgentRunResultEvent[Any]]` | An async iterable of stream events `AgentStreamEvent` and finally a `AgentRunResultEvent` with the final |\n| `AsyncIterator[AgentStreamEvent | AgentRunResultEvent[Any]]` | run result. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py`", "url": "https://ai.pydantic.dev/durable_exec/index.html#dbosagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "DBOSAgent", "anchor": "dbosagent", "md_text": "|  |  |\n| --- | --- |\n| ``` 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 ``` | ``` def run_stream_events(     self,     user_prompt: str | Sequence[_messages.UserContent] | None = None,     *,     output_type: OutputSpec[RunOutputDataT] | None = None,     message_history: Sequence[_messages.ModelMessage] | None = None,     deferred_tool_results: DeferredToolResults | None = None,     model: models.Model | models.KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: _usage.UsageLimits | None = None,     usage: _usage.RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     builtin_tools: Sequence[AbstractBuiltinTool] | None = None, ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[Any]]:     \"\"\"Run the agent with a user prompt in async mode and stream events from the run.      This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] and     uses the `event_stream_handler` kwarg to get a stream of events from the run.      Example:     ```python     from pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent      agent = Agent('openai:gpt-4o')      async def main():         events: list[AgentStreamEvent | AgentRunResultEvent] = []         async for event in agent.run_stream_events('What is the capital of France?'):             events.append(event)         print(events)         '''         [             PartStartEvent(index=0, part=TextPart(content='The capital of ')),             FinalResultEvent(tool_name=None, tool_call_id=None),             PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='France is Paris. ')),             AgentRunResultEvent(                 result=AgentRunResult(output='The capital of France is Paris. ')             ),         ]         '''     ```      Arguments are the same as for [`self.run`][pydantic_ai.agent.AbstractAgent.run],     except that `event_stream_handler` is now allowed.      Args:         user_prompt: User input to start/continue the conversation.         output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no             output validators since output validators would expect an argument that matches the agent's output type.         message_history: History of the conversation so far.         deferred_tool_results: Optional results for deferred tool calls in the message history.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.         builtin_tools: Optional additional builtin tools for this run.      Returns:         An async iterable of stream events `AgentStreamEvent` and finally a `AgentRunResultEvent` with the final         run result.     \"\"\"     raise UserError(         '`agent.run_stream_events()` cannot be used with DBOS. '         'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'     ) ``` |\n\n#### iter `async`\n\n```\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AbstractAsyncContextManager[\n    AgentRun[AgentDepsT, OutputDataT]\n]", "url": "https://ai.pydantic.dev/durable_exec/index.html#dbosagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "DBOSAgent", "anchor": "dbosagent", "md_text": "iter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AbstractAsyncContextManager[\n    AgentRun[AgentDepsT, RunOutputDataT]\n]\n\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AsyncIterator[AgentRun[AgentDepsT, Any]]\n```\n\nA contextmanager which can be used to iterate over the agent graph's nodes as they are executed.\n\nThis method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an\n`AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are\nexecuted. This is the API to use if you want to consume the outputs coming from each LLM model response, or the\nstream of events coming from the execution of tools.\n\nThe `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,\nand the final result of the run once it has completed.\n\nFor more details, see the documentation of `AgentRun`.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    nodes = []\n    async with agent.iter('What is the capital of France?') as agent_run:\n        async for node in agent_run:\n            nodes.append(node)\n    print(nodes)\n    '''\n    [\n        UserPromptNode(\n            user_prompt='What is the capital of France?',\n            instructions_functions=[],\n            system_prompts=(),\n            system_prompt_functions=[],\n            system_prompt_dynamic_functions={},\n        ),\n        ModelRequestNode(\n            request=ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='What is the capital of France?',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            )\n        ),\n        CallToolsNode(\n            model_response=ModelResponse(\n                parts=[TextPart(content='The capital of France is Paris.')],\n                usage=RequestUsage(input_tokens=56, output_tokens=7),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            )\n        ),\n        End(data=FinalResult(output='The capital of France is Paris.')),\n    ]\n    '''\n    print(agent_run.result.output)\n    #> The capital of France is Paris.\n```\n\nParameters:", "url": "https://ai.pydantic.dev/durable_exec/index.html#dbosagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "DBOSAgent", "anchor": "dbosagent", "md_text": "| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` |\n| `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` |\n| `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AsyncIterator[AgentRun[AgentDepsT, Any]]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py`", "url": "https://ai.pydantic.dev/durable_exec/index.html#dbosagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "DBOSAgent", "anchor": "dbosagent", "md_text": "|  |  |\n| --- | --- |\n| ``` 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 ``` | ``` @asynccontextmanager async def iter(     self,     user_prompt: str | Sequence[_messages.UserContent] | None = None,     *,     output_type: OutputSpec[RunOutputDataT] | None = None,     message_history: Sequence[_messages.ModelMessage] | None = None,     deferred_tool_results: DeferredToolResults | None = None,     model: models.Model | models.KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: _usage.UsageLimits | None = None,     usage: _usage.RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     **_deprecated_kwargs: Never, ) -> AsyncIterator[AgentRun[AgentDepsT, Any]]:     \"\"\"A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.      This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an     `AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are     executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the     stream of events coming from the execution of tools.      The `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,     and the final result of the run once it has completed.      For more details, see the documentation of `AgentRun`.      Example:     ```python     from pydantic_ai import Agent      agent = Agent('openai:gpt-4o')      async def main():         nodes = []         async with agent.iter('What is the capital of France?') as agent_run:             async for node in agent_run:                 nodes.append(node)         print(nodes)         '''         [             UserPromptNode(                 user_prompt='What is the capital of France?',                 instructions_functions=[],                 system_prompts=(),                 system_prompt_functions=[],                 system_prompt_dynamic_functions={},             ),             ModelRequestNode(                 request=ModelRequest(                     parts=[                         UserPromptPart(                             content='What is the capital of France?',                             timestamp=datetime.datetime(...),                         )                     ]                 )             ),             CallToolsNode(                 model_response=ModelResponse(                     parts=[TextPart(content='The capital of France is Paris.')],                     usage=RequestUsage(input_tokens=56, output_tokens=7),                     model_name='gpt-4o',                     timestamp=datetime.datetime(...),                 )             ),             End(data=FinalResult(output='The capital of France is Paris.')),         ]         '''         print(agent_run.result.output)         #> The capital of France is Paris.     ```      Args:         user_prompt: User input to start/continue the conversation.         output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no             output validators since output validators would expect an argument that matches the agent's output type.         message_history: History of the conversation so far.         deferred_tool_results: Optional results for deferred tool calls in the message history.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.         builtin_tools: Optional additional builtin tools for this run.      Returns:         The result of the run.     \"\"\"     if model is not None and not isinstance(model, DBOSModel):         raise UserError(             'Non-DBOS model cannot be set at agent run time inside a DBOS workflow, it must be set at agent creation time.'         )      with self._dbos_overrides():         async with super().iter(             user_prompt=user_prompt,             output_type=output_type,             message_history=message_history,             deferred_tool_results=deferred_tool_results,             model=model,             deps=deps,             model_settings=model_settings,             usage_limits=usage_limits,             usage=usage,             infer_name=infer_name,             toolsets=toolsets,             builtin_tools=builtin_tools,             **_deprecated_kwargs,         ) as run:             yield run ``` |", "url": "https://ai.pydantic.dev/durable_exec/index.html#dbosagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "DBOSAgent", "anchor": "dbosagent", "md_text": "#### override\n\n```\noverride(\n    *,\n    name: str | Unset = UNSET,\n    deps: AgentDepsT | Unset = UNSET,\n    model: Model | KnownModelName | str | Unset = UNSET,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | Unset\n    ) = UNSET,\n    tools: (\n        Sequence[\n            Tool[AgentDepsT]\n            | ToolFuncEither[AgentDepsT, ...]\n        ]\n        | Unset\n    ) = UNSET,\n    instructions: Instructions[AgentDepsT] | Unset = UNSET\n) -> Iterator[None]\n```\n\nContext manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.\n\nThis is particularly useful when testing.\nYou can find an example of this [here](https://ai.pydantic.dev/testing/#overriding-model-via-pytest-fixtures).\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `name` | `str | Unset` | The name to use instead of the name passed to the agent constructor and agent run. | `UNSET` |\n| `deps` | `AgentDepsT | Unset` | The dependencies to use instead of the dependencies passed to the agent run. | `UNSET` |\n| `model` | `Model | KnownModelName | str | Unset` | The model to use instead of the model passed to the agent run. | `UNSET` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | Unset` | The toolsets to use instead of the toolsets passed to the agent constructor and agent run. | `UNSET` |\n| `tools` | `Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | Unset` | The tools to use instead of the tools registered with the agent. | `UNSET` |\n| `instructions` | `Instructions[AgentDepsT] | Unset` | The instructions to use instead of the instructions registered with the agent. | `UNSET` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py`\n\n|  |  |\n| --- | --- |\n| ``` 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 ``` | ``` @contextmanager def override(     self,     *,     name: str | _utils.Unset = _utils.UNSET,     deps: AgentDepsT | _utils.Unset = _utils.UNSET,     model: models.Model | models.KnownModelName | str | _utils.Unset = _utils.UNSET,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | _utils.Unset = _utils.UNSET,     tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | _utils.Unset = _utils.UNSET,     instructions: Instructions[AgentDepsT] | _utils.Unset = _utils.UNSET, ) -> Iterator[None]:     \"\"\"Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions.      This is particularly useful when testing.     You can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).      Args:         name: The name to use instead of the name passed to the agent constructor and agent run.         deps: The dependencies to use instead of the dependencies passed to the agent run.         model: The model to use instead of the model passed to the agent run.         toolsets: The toolsets to use instead of the toolsets passed to the agent constructor and agent run.         tools: The tools to use instead of the tools registered with the agent.         instructions: The instructions to use instead of the instructions registered with the agent.     \"\"\"     if _utils.is_set(model) and not isinstance(model, (DBOSModel)):         raise UserError(             'Non-DBOS model cannot be contextually overridden inside a DBOS workflow, it must be set at agent creation time.'         )      with super().override(         name=name,         deps=deps,         model=model,         toolsets=toolsets,         tools=tools,         instructions=instructions,     ):         yield ``` |", "url": "https://ai.pydantic.dev/durable_exec/index.html#dbosagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "DBOSMCPServer", "anchor": "dbosmcpserver", "md_text": "Bases: `WrapperToolset[AgentDepsT]`, `ABC`\n\nA wrapper for MCPServer that integrates with DBOS, turning call\\_tool and get\\_tools to DBOS steps.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_mcp_server.py`\n\n|  |  |\n| --- | --- |\n| ``` 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 ``` | ``` class DBOSMCPServer(WrapperToolset[AgentDepsT], ABC):     \"\"\"A wrapper for MCPServer that integrates with DBOS, turning call_tool and get_tools to DBOS steps.\"\"\"      def __init__(         self,         wrapped: MCPServer,         *,         step_name_prefix: str,         step_config: StepConfig,     ):         super().__init__(wrapped)         self._step_config = step_config or {}         self._step_name_prefix = step_name_prefix         id_suffix = f'__{wrapped.id}' if wrapped.id else ''         self._name = f'{step_name_prefix}__mcp_server{id_suffix}'          # Wrap get_tools in a DBOS step.         @DBOS.step(             name=f'{self._name}.get_tools',             **self._step_config,         )         async def wrapped_get_tools_step(             ctx: RunContext[AgentDepsT],         ) -> dict[str, ToolsetTool[AgentDepsT]]:             return await super(DBOSMCPServer, self).get_tools(ctx)          self._dbos_wrapped_get_tools_step = wrapped_get_tools_step          # Wrap call_tool in a DBOS step.         @DBOS.step(             name=f'{self._name}.call_tool',             **self._step_config,         )         async def wrapped_call_tool_step(             name: str,             tool_args: dict[str, Any],             ctx: RunContext[AgentDepsT],             tool: ToolsetTool[AgentDepsT],         ) -> ToolResult:             return await super(DBOSMCPServer, self).call_tool(name, tool_args, ctx, tool)          self._dbos_wrapped_call_tool_step = wrapped_call_tool_step      @property     def id(self) -> str | None:         return self.wrapped.id      async def __aenter__(self) -> Self:         # The wrapped MCPServer enters itself around listing and calling tools         # so we don't need to enter it here (nor could we because we're not inside a DBOS step).         return self      async def __aexit__(self, *args: Any) -> bool | None:         return None      def visit_and_replace(         self, visitor: Callable[[AbstractToolset[AgentDepsT]], AbstractToolset[AgentDepsT]]     ) -> AbstractToolset[AgentDepsT]:         # DBOS-ified toolsets cannot be swapped out after the fact.         return self      async def get_tools(self, ctx: RunContext[AgentDepsT]) -> dict[str, ToolsetTool[AgentDepsT]]:         return await self._dbos_wrapped_get_tools_step(ctx)      async def call_tool(         self,         name: str,         tool_args: dict[str, Any],         ctx: RunContext[AgentDepsT],         tool: ToolsetTool[AgentDepsT],     ) -> ToolResult:         return await self._dbos_wrapped_call_tool_step(name, tool_args, ctx, tool) ``` |", "url": "https://ai.pydantic.dev/durable_exec/index.html#dbosmcpserver", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "DBOSModel", "anchor": "dbosmodel", "md_text": "Bases: `WrapperModel`\n\nA wrapper for Model that integrates with DBOS, turning request and request\\_stream to DBOS steps.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_model.py`\n\n|  |  |\n| --- | --- |\n| ```  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 ``` | ``` class DBOSModel(WrapperModel):     \"\"\"A wrapper for Model that integrates with DBOS, turning request and request_stream to DBOS steps.\"\"\"      def __init__(         self,         model: Model,         *,         step_name_prefix: str,         step_config: StepConfig,         event_stream_handler: EventStreamHandler[Any] | None = None,     ):         super().__init__(model)         self.step_config = step_config         self.event_stream_handler = event_stream_handler         self._step_name_prefix = step_name_prefix          # Wrap the request in a DBOS step.         @DBOS.step(             name=f'{self._step_name_prefix}__model.request',             **self.step_config,         )         async def wrapped_request_step(             messages: list[ModelMessage],             model_settings: ModelSettings | None,             model_request_parameters: ModelRequestParameters,         ) -> ModelResponse:             return await super(DBOSModel, self).request(messages, model_settings, model_request_parameters)          self._dbos_wrapped_request_step = wrapped_request_step          # Wrap the request_stream in a DBOS step.         @DBOS.step(             name=f'{self._step_name_prefix}__model.request_stream',             **self.step_config,         )         async def wrapped_request_stream_step(             messages: list[ModelMessage],             model_settings: ModelSettings | None,             model_request_parameters: ModelRequestParameters,             run_context: RunContext[Any] | None = None,         ) -> ModelResponse:             async with super(DBOSModel, self).request_stream(                 messages, model_settings, model_request_parameters, run_context             ) as streamed_response:                 if self.event_stream_handler is not None:                     assert run_context is not None, (                         'A DBOS model cannot be used with `pydantic_ai.direct.model_request_stream()` as it requires a `run_context`. Set an `event_stream_handler` on the agent and use `agent.run()` instead.'                     )                     await self.event_stream_handler(run_context, streamed_response)                  async for _ in streamed_response:                     pass             return streamed_response.get()          self._dbos_wrapped_request_stream_step = wrapped_request_stream_step      async def request(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> ModelResponse:         return await self._dbos_wrapped_request_step(messages, model_settings, model_request_parameters)      @asynccontextmanager     async def request_stream(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,         run_context: RunContext[Any] | None = None,     ) -> AsyncIterator[StreamedResponse]:         # If not in a workflow (could be in a step), just call the wrapped request_stream method.         if DBOS.workflow_id is None or DBOS.step_id is not None:             async with super().request_stream(                 messages, model_settings, model_request_parameters, run_context             ) as streamed_response:                 yield streamed_response                 return          response = await self._dbos_wrapped_request_stream_step(             messages, model_settings, model_request_parameters, run_context         )         yield DBOSStreamedResponse(model_request_parameters, response) ``` |", "url": "https://ai.pydantic.dev/durable_exec/index.html#dbosmodel", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "StepConfig", "anchor": "stepconfig", "md_text": "Bases: `TypedDict`\n\nConfiguration for a step in the DBOS workflow.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_utils.py`\n\n|  |  |\n| --- | --- |\n| ```  4  5  6  7  8  9 10 ``` | ``` class StepConfig(TypedDict, total=False):     \"\"\"Configuration for a step in the DBOS workflow.\"\"\"      retries_allowed: bool     interval_seconds: float     max_attempts: int     backoff_rate: float ``` |", "url": "https://ai.pydantic.dev/durable_exec/index.html#stepconfig", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PrefectAgent", "anchor": "prefectagent", "md_text": "Bases: `WrapperAgent[AgentDepsT, OutputDataT]`\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py`", "url": "https://ai.pydantic.dev/durable_exec/index.html#prefectagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PrefectAgent", "anchor": "prefectagent", "md_text": "|  |  |\n| --- | --- |\n| ```  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 ``` | ``` class PrefectAgent(WrapperAgent[AgentDepsT, OutputDataT]):     def __init__(         self,         wrapped: AbstractAgent[AgentDepsT, OutputDataT],         *,         name: str | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,         mcp_task_config: TaskConfig | None = None,         model_task_config: TaskConfig | None = None,         tool_task_config: TaskConfig | None = None,         tool_task_config_by_name: dict[str, TaskConfig | None] | None = None,         event_stream_handler_task_config: TaskConfig | None = None,         prefectify_toolset_func: Callable[             [AbstractToolset[AgentDepsT], TaskConfig, TaskConfig, dict[str, TaskConfig | None]],             AbstractToolset[AgentDepsT],         ] = prefectify_toolset,     ):         \"\"\"Wrap an agent to enable it with Prefect durable flows, by automatically offloading model requests, tool calls, and MCP server communication to Prefect tasks.          After wrapping, the original agent can still be used as normal outside of the Prefect flow.          Args:             wrapped: The agent to wrap.             name: Optional unique agent name to use as the Prefect flow name prefix. If not provided, the agent's `name` will be used.             event_stream_handler: Optional event stream handler to use instead of the one set on the wrapped agent.             mcp_task_config: The base Prefect task config to use for MCP server tasks. If no config is provided, use the default settings of Prefect.             model_task_config: The Prefect task config to use for model request tasks. If no config is provided, use the default settings of Prefect.             tool_task_config: The default Prefect task config to use for tool calls. If no config is provided, use the default settings of Prefect.             tool_task_config_by_name: Per-tool task configuration. Keys are tool names, values are TaskConfig or None (None disables task wrapping for that tool).             event_stream_handler_task_config: The Prefect task config to use for the event stream handler task. If no config is provided, use the default settings of Prefect.             prefectify_toolset_func: Optional function to use to prepare toolsets for Prefect by wrapping them in a `PrefectWrapperToolset` that moves methods that require IO to Prefect tasks.                 If not provided, only `FunctionToolset` and `MCPServer` will be prepared for Prefect.                 The function takes the toolset, the task config, the tool-specific task config, and the tool-specific task config by name.         \"\"\"         super().__init__(wrapped)          self._name = name or wrapped.name         self._event_stream_handler = event_stream_handler         if self._name is None:             raise UserError(                 \"An agent needs to have a unique `name` in order to be used with Prefect. The name will be used to identify the agent's flows and tasks.\"             )          # Merge the config with the default Prefect config         self._mcp_task_config = default_task_config | (mcp_task_config or {})         self._model_task_config = default_task_config | (model_task_config or {})         self._tool_task_config = default_task_config | (tool_task_config or {})         self._tool_task_config_by_name = tool_task_config_by_name or {}         self._event_stream_handler_task_config = default_task_config | (event_stream_handler_task_config or {})          if not isinstance(wrapped.model, Model):             raise UserError(                 'An agent needs to have a `model` in order to be used with Prefect, it cannot be set at agent run time.'             )          prefect_model = PrefectModel(             wrapped.model,             task_config=self._model_task_config,             event_stream_handler=self.event_stream_handler,         )         self._model = prefect_model          def _prefectify_toolset(toolset: AbstractToolset[AgentDepsT]) -> AbstractToolset[AgentDepsT]:             \"\"\"Convert a toolset to its Prefect equivalent.\"\"\"             return prefectify_toolset_func(                 toolset,                 self._mcp_task_config,                 self._tool_task_config,                 self._tool_task_config_by_name,             )          prefect_toolsets = [toolset.visit_and_replace(_prefectify_toolset) for toolset in wrapped.toolsets]         self._toolsets = prefect_toolsets          # Context variable to track when we're inside this agent's Prefect flow         self._in_prefect_agent_flow: ContextVar[bool] = ContextVar(             f'_in_prefect_agent_flow_{self._name}', default=False         )      @property     def name(self) -> str | None:         return self._name      @name.setter     def name(self, value: str | None) -> None:  # pragma: no cover         raise UserError(             'The agent name cannot be changed after creation. If you need to change the name, create a new agent.'         )      @property     def model(self) -> Model:         return self._model      @property     def event_stream_handler(self) -> EventStreamHandler[AgentDepsT] | None:         handler = self._event_stream_handler or super().event_stream_handler         if handler is None:             return None         elif FlowRunContext.get() is not None:             # Special case if it's in a Prefect flow, we need to iterate through all events and call the handler.             return self._call_event_stream_handler_in_flow         else:             return handler      async def _call_event_stream_handler_in_flow(         self, ctx: RunContext[AgentDepsT], stream: AsyncIterable[_messages.AgentStreamEvent]     ) -> None:         handler = self._event_stream_handler or super().event_stream_handler         assert handler is not None          # Create a task to handle each event         @task(name='Handle Stream Event', **self._event_stream_handler_task_config)         async def event_stream_handler_task(event: _messages.AgentStreamEvent) -> None:             async def streamed_response():                 yield event              await handler(ctx, streamed_response())          async for event in stream:             await event_stream_handler_task(event)      @property     def toolsets(self) -> Sequence[AbstractToolset[AgentDepsT]]:         with self._prefect_overrides():             return super().toolsets      @contextmanager     def _prefect_overrides(self) -> Iterator[None]:         # Override with PrefectModel and PrefectMCPServer in the toolsets.         with super().override(model=self._model, toolsets=self._toolsets, tools=[]):             yield      @overload     async def run(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AgentRunResult[OutputDataT]: ...      @overload     async def run(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT],         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AgentRunResult[RunOutputDataT]: ...      async def run(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT] | None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,         **_deprecated_kwargs: Never,     ) -> AgentRunResult[Any]:         \"\"\"Run the agent with a user prompt in async mode.          This method builds an internal agent graph (using system prompts, tools and result schemas) and then         runs the graph to completion. The result of the run is returned.          Example:         ```python         from pydantic_ai import Agent          agent = Agent('openai:gpt-4o')          async def main():             agent_run = await agent.run('What is the capital of France?')             print(agent_run.output)             #> The capital of France is Paris.         ```          Args:             user_prompt: User input to start/continue the conversation.             output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no                 output validators since output validators would expect an argument that matches the agent's output type.             message_history: History of the conversation so far.             deferred_tool_results: Optional results for deferred tool calls in the message history.             model: Optional model to use for this run, required if `model` was not set when creating the agent.             deps: Optional dependencies to use for this run.             model_settings: Optional settings to use for this model's request.             usage_limits: Optional limits on model request count or token usage.             usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.             infer_name: Whether to try to infer the agent name from the call frame if it's not set.             toolsets: Optional additional toolsets for this run.             event_stream_handler: Optional event stream handler to use for this run.             builtin_tools: Optional additional builtin tools for this run.          Returns:             The result of the run.         \"\"\"          @flow(name=f'{self._name} Run')         async def wrapped_run_flow() -> AgentRunResult[Any]:             # Mark that we're inside a PrefectAgent flow             token = self._in_prefect_agent_flow.set(True)             try:                 with self._prefect_overrides():                     result = await super(WrapperAgent, self).run(                         user_prompt,                         output_type=output_type,                         message_history=message_history,                         deferred_tool_results=deferred_tool_results,                         model=model,                         deps=deps,                         model_settings=model_settings,                         usage_limits=usage_limits,                         usage=usage,                         infer_name=infer_name,                         toolsets=toolsets,                         event_stream_handler=event_stream_handler,                     )                     return result             finally:                 self._in_prefect_agent_flow.reset(token)          return await wrapped_run_flow()      @overload     def run_sync(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AgentRunResult[OutputDataT]: ...      @overload     def run_sync(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT],         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AgentRunResult[RunOutputDataT]: ...      def run_sync(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT] | None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,         **_deprecated_kwargs: Never,     ) -> AgentRunResult[Any]:         \"\"\"Synchronously run the agent with a user prompt.          This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] with `loop.run_until_complete(...)`.         You therefore can't use this method inside async code or if there's an active event loop.          Example:         ```python         from pydantic_ai import Agent          agent = Agent('openai:gpt-4o')          result_sync = agent.run_sync('What is the capital of Italy?')         print(result_sync.output)         #> The capital of Italy is Rome.         ```          Args:             user_prompt: User input to start/continue the conversation.             output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no                 output validators since output validators would expect an argument that matches the agent's output type.             message_history: History of the conversation so far.             deferred_tool_results: Optional results for deferred tool calls in the message history.             model: Optional model to use for this run, required if `model` was not set when creating the agent.             deps: Optional dependencies to use for this run.             model_settings: Optional settings to use for this model's request.             usage_limits: Optional limits on model request count or token usage.             usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.             infer_name: Whether to try to infer the agent name from the call frame if it's not set.             toolsets: Optional additional toolsets for this run.             event_stream_handler: Optional event stream handler to use for this run.             builtin_tools: Optional additional builtin tools for this run.          Returns:             The result of the run.         \"\"\"          @flow(name=f'{self._name} Sync Run')         def wrapped_run_sync_flow() -> AgentRunResult[Any]:             # Mark that we're inside a PrefectAgent flow             token = self._in_prefect_agent_flow.set(True)             try:                 with self._prefect_overrides():                     # Using `run_coro_as_sync` from Prefect with async `run` to avoid event loop conflicts.                     result = run_coro_as_sync(                         super(PrefectAgent, self).run(                             user_prompt,                             output_type=output_type,                             message_history=message_history,                             deferred_tool_results=deferred_tool_results,                             model=model,                             deps=deps,                             model_settings=model_settings,                             usage_limits=usage_limits,                             usage=usage,                             infer_name=infer_name,                             toolsets=toolsets,                             event_stream_handler=event_stream_handler,                         )                     )                     return result             finally:                 self._in_prefect_agent_flow.reset(token)          return wrapped_run_sync_flow()      @overload     def run_stream(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AbstractAsyncContextManager[StreamedRunResult[AgentDepsT, OutputDataT]]: ...      @overload     def run_stream(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT],         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     ) -> AbstractAsyncContextManager[StreamedRunResult[AgentDepsT, RunOutputDataT]]: ...      @asynccontextmanager     async def run_stream(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT] | None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,         event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,         **_deprecated_kwargs: Never,     ) -> AsyncIterator[StreamedRunResult[AgentDepsT, Any]]:         \"\"\"Run the agent with a user prompt in async mode, returning a streamed response.          Example:         ```python         from pydantic_ai import Agent          agent = Agent('openai:gpt-4o')          async def main():             async with agent.run_stream('What is the capital of the UK?') as response:                 print(await response.get_output())                 #> The capital of the UK is London.         ```          Args:             user_prompt: User input to start/continue the conversation.             output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no                 output validators since output validators would expect an argument that matches the agent's output type.             message_history: History of the conversation so far.             deferred_tool_results: Optional results for deferred tool calls in the message history.             model: Optional model to use for this run, required if `model` was not set when creating the agent.             deps: Optional dependencies to use for this run.             model_settings: Optional settings to use for this model's request.             usage_limits: Optional limits on model request count or token usage.             usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.             infer_name: Whether to try to infer the agent name from the call frame if it's not set.             toolsets: Optional additional toolsets for this run.             builtin_tools: Optional additional builtin tools for this run.             event_stream_handler: Optional event stream handler to use for this run. It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager.          Returns:             The result of the run.         \"\"\"         if FlowRunContext.get() is not None:             raise UserError(                 '`agent.run_stream()` cannot be used inside a Prefect flow. '                 'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'             )          async with super().run_stream(             user_prompt,             output_type=output_type,             message_history=message_history,             deferred_tool_results=deferred_tool_results,             model=model,             deps=deps,             model_settings=model_settings,             usage_limits=usage_limits,             usage=usage,             infer_name=infer_name,             toolsets=toolsets,             event_stream_handler=event_stream_handler,             builtin_tools=builtin_tools,             **_deprecated_kwargs,         ) as result:             yield result      @overload     def run_stream_events(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[OutputDataT]]: ...      @overload     def run_stream_events(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT],         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[RunOutputDataT]]: ...      def run_stream_events(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT] | None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[Any]]:         \"\"\"Run the agent with a user prompt in async mode and stream events from the run.          This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] and         uses the `event_stream_handler` kwarg to get a stream of events from the run.          Example:         ```python         from pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent          agent = Agent('openai:gpt-4o')          async def main():             events: list[AgentStreamEvent | AgentRunResultEvent] = []             async for event in agent.run_stream_events('What is the capital of France?'):                 events.append(event)             print(events)             '''             [                 PartStartEvent(index=0, part=TextPart(content='The capital of ')),                 FinalResultEvent(tool_name=None, tool_call_id=None),                 PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='France is Paris. ')),                 AgentRunResultEvent(                     result=AgentRunResult(output='The capital of France is Paris. ')                 ),             ]             '''         ```          Arguments are the same as for [`self.run`][pydantic_ai.agent.AbstractAgent.run],         except that `event_stream_handler` is now allowed.          Args:             user_prompt: User input to start/continue the conversation.             output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no                 output validators since output validators would expect an argument that matches the agent's output type.             message_history: History of the conversation so far.             deferred_tool_results: Optional results for deferred tool calls in the message history.             model: Optional model to use for this run, required if `model` was not set when creating the agent.             deps: Optional dependencies to use for this run.             model_settings: Optional settings to use for this model's request.             usage_limits: Optional limits on model request count or token usage.             usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.             infer_name: Whether to try to infer the agent name from the call frame if it's not set.             toolsets: Optional additional toolsets for this run.             builtin_tools: Optional additional builtin tools for this run.          Returns:             An async iterable of stream events `AgentStreamEvent` and finally a `AgentRunResultEvent` with the final             run result.         \"\"\"         if FlowRunContext.get() is not None:             raise UserError(                 '`agent.run_stream_events()` cannot be used inside a Prefect flow. '                 'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'             )          return super().run_stream_events(             user_prompt,             output_type=output_type,             message_history=message_history,             deferred_tool_results=deferred_tool_results,             model=model,             deps=deps,             model_settings=model_settings,             usage_limits=usage_limits,             usage=usage,             infer_name=infer_name,             toolsets=toolsets,             builtin_tools=builtin_tools,         )      @overload     def iter(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     ) -> AbstractAsyncContextManager[AgentRun[AgentDepsT, OutputDataT]]: ...      @overload     def iter(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT],         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     ) -> AbstractAsyncContextManager[AgentRun[AgentDepsT, RunOutputDataT]]: ...      @asynccontextmanager     async def iter(         self,         user_prompt: str | Sequence[_messages.UserContent] | None = None,         *,         output_type: OutputSpec[RunOutputDataT] | None = None,         message_history: Sequence[_messages.ModelMessage] | None = None,         deferred_tool_results: DeferredToolResults | None = None,         model: models.Model | models.KnownModelName | str | None = None,         deps: AgentDepsT = None,         model_settings: ModelSettings | None = None,         usage_limits: _usage.UsageLimits | None = None,         usage: _usage.RunUsage | None = None,         infer_name: bool = True,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,         builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     ) -> AsyncIterator[AgentRun[AgentDepsT, Any]]:         \"\"\"A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.          This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an         `AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are         executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the         stream of events coming from the execution of tools.          The `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,         and the final result of the run once it has completed.          For more details, see the documentation of `AgentRun`.          Example:         ```python         from pydantic_ai import Agent          agent = Agent('openai:gpt-4o')          async def main():             nodes = []             async with agent.iter('What is the capital of France?') as agent_run:                 async for node in agent_run:                     nodes.append(node)             print(nodes)             '''             [                 UserPromptNode(                     user_prompt='What is the capital of France?',                     instructions_functions=[],                     system_prompts=(),                     system_prompt_functions=[],                     system_prompt_dynamic_functions={},                 ),                 ModelRequestNode(                     request=ModelRequest(                         parts=[                             UserPromptPart(                                 content='What is the capital of France?',                                 timestamp=datetime.datetime(...),                             )                         ]                     )                 ),                 CallToolsNode(                     model_response=ModelResponse(                         parts=[TextPart(content='The capital of France is Paris.')],                         usage=RequestUsage(input_tokens=56, output_tokens=7),                         model_name='gpt-4o',                         timestamp=datetime.datetime(...),                     )                 ),                 End(data=FinalResult(output='The capital of France is Paris.')),             ]             '''             print(agent_run.result.output)             #> The capital of France is Paris.         ```          Args:             user_prompt: User input to start/continue the conversation.             output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no                 output validators since output validators would expect an argument that matches the agent's output type.             message_history: History of the conversation so far.             deferred_tool_results: Optional results for deferred tool calls in the message history.             model: Optional model to use for this run, required if `model` was not set when creating the agent.             deps: Optional dependencies to use for this run.             model_settings: Optional settings to use for this model's request.             usage_limits: Optional limits on model request count or token usage.             usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.             infer_name: Whether to try to infer the agent name from the call frame if it's not set.             toolsets: Optional additional toolsets for this run.             builtin_tools: Optional additional builtin tools for this run.          Returns:             The result of the run.         \"\"\"         if model is not None and not isinstance(model, PrefectModel):             raise UserError(                 'Non-Prefect model cannot be set at agent run time inside a Prefect flow, it must be set at agent creation time.'             )          with self._prefect_overrides():             async with super().iter(                 user_prompt=user_prompt,                 output_type=output_type,                 message_history=message_history,                 deferred_tool_results=deferred_tool_results,                 model=model,                 deps=deps,                 model_settings=model_settings,                 usage_limits=usage_limits,                 usage=usage,                 infer_name=infer_name,                 toolsets=toolsets,             ) as run:                 yield run      @contextmanager     def override(         self,         *,         name: str | _utils.Unset = _utils.UNSET,         deps: AgentDepsT | _utils.Unset = _utils.UNSET,         model: models.Model | models.KnownModelName | str | _utils.Unset = _utils.UNSET,         toolsets: Sequence[AbstractToolset[AgentDepsT]] | _utils.Unset = _utils.UNSET,         tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | _utils.Unset = _utils.UNSET,         instructions: Instructions[AgentDepsT] | _utils.Unset = _utils.UNSET,     ) -> Iterator[None]:         \"\"\"Context manager to temporarily override agent dependencies, model, toolsets, tools, or instructions.          This is particularly useful when testing.         You can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).          Args:             name: The name to use instead of the name passed to the agent constructor and agent run.             deps: The dependencies to use instead of the dependencies passed to the agent run.             model: The model to use instead of the model passed to the agent run.             toolsets: The toolsets to use instead of the toolsets passed to the agent constructor and agent run.             tools: The tools to use instead of the tools registered with the agent.             instructions: The instructions to use instead of the instructions registered with the agent.         \"\"\"         if _utils.is_set(model) and not isinstance(model, PrefectModel):             raise UserError(                 'Non-Prefect model cannot be contextually overridden inside a Prefect flow, it must be set at agent creation time.'             )          with super().override(             name=name, deps=deps, model=model, toolsets=toolsets, tools=tools, instructions=instructions         ):             yield ``` |", "url": "https://ai.pydantic.dev/durable_exec/index.html#prefectagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PrefectAgent", "anchor": "prefectagent", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    wrapped: AbstractAgent[AgentDepsT, OutputDataT],\n    *,\n    name: str | None = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None,\n    mcp_task_config: TaskConfig | None = None,\n    model_task_config: TaskConfig | None = None,\n    tool_task_config: TaskConfig | None = None,\n    tool_task_config_by_name: (\n        dict[str, TaskConfig | None] | None\n    ) = None,\n    event_stream_handler_task_config: (\n        TaskConfig | None\n    ) = None,\n    prefectify_toolset_func: Callable[\n        [\n            AbstractToolset[AgentDepsT],\n            TaskConfig,\n            TaskConfig,\n            dict[str, TaskConfig | None],\n        ],\n        AbstractToolset[AgentDepsT],\n    ] = prefectify_toolset\n)\n```\n\nWrap an agent to enable it with Prefect durable flows, by automatically offloading model requests, tool calls, and MCP server communication to Prefect tasks.\n\nAfter wrapping, the original agent can still be used as normal outside of the Prefect flow.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `wrapped` | `AbstractAgent[AgentDepsT, OutputDataT]` | The agent to wrap. | *required* |\n| `name` | `str | None` | Optional unique agent name to use as the Prefect flow name prefix. If not provided, the agent's `name` will be used. | `None` |\n| `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional event stream handler to use instead of the one set on the wrapped agent. | `None` |\n| `mcp_task_config` | `TaskConfig | None` | The base Prefect task config to use for MCP server tasks. If no config is provided, use the default settings of Prefect. | `None` |\n| `model_task_config` | `TaskConfig | None` | The Prefect task config to use for model request tasks. If no config is provided, use the default settings of Prefect. | `None` |\n| `tool_task_config` | `TaskConfig | None` | The default Prefect task config to use for tool calls. If no config is provided, use the default settings of Prefect. | `None` |\n| `tool_task_config_by_name` | `dict[str, TaskConfig | None] | None` | Per-tool task configuration. Keys are tool names, values are TaskConfig or None (None disables task wrapping for that tool). | `None` |\n| `event_stream_handler_task_config` | `TaskConfig | None` | The Prefect task config to use for the event stream handler task. If no config is provided, use the default settings of Prefect. | `None` |\n| `prefectify_toolset_func` | `Callable[[AbstractToolset[AgentDepsT], TaskConfig, TaskConfig, dict[str, TaskConfig | None]], AbstractToolset[AgentDepsT]]` | Optional function to use to prepare toolsets for Prefect by wrapping them in a `PrefectWrapperToolset` that moves methods that require IO to Prefect tasks. If not provided, only `FunctionToolset` and `MCPServer` will be prepared for Prefect. The function takes the toolset, the task config, the tool-specific task config, and the tool-specific task config by name. | `prefectify_toolset` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py`", "url": "https://ai.pydantic.dev/durable_exec/index.html#prefectagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PrefectAgent", "anchor": "prefectagent", "md_text": "|  |  |\n| --- | --- |\n| ```  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 ``` | ``` def __init__(     self,     wrapped: AbstractAgent[AgentDepsT, OutputDataT],     *,     name: str | None = None,     event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     mcp_task_config: TaskConfig | None = None,     model_task_config: TaskConfig | None = None,     tool_task_config: TaskConfig | None = None,     tool_task_config_by_name: dict[str, TaskConfig | None] | None = None,     event_stream_handler_task_config: TaskConfig | None = None,     prefectify_toolset_func: Callable[         [AbstractToolset[AgentDepsT], TaskConfig, TaskConfig, dict[str, TaskConfig | None]],         AbstractToolset[AgentDepsT],     ] = prefectify_toolset, ):     \"\"\"Wrap an agent to enable it with Prefect durable flows, by automatically offloading model requests, tool calls, and MCP server communication to Prefect tasks.      After wrapping, the original agent can still be used as normal outside of the Prefect flow.      Args:         wrapped: The agent to wrap.         name: Optional unique agent name to use as the Prefect flow name prefix. If not provided, the agent's `name` will be used.         event_stream_handler: Optional event stream handler to use instead of the one set on the wrapped agent.         mcp_task_config: The base Prefect task config to use for MCP server tasks. If no config is provided, use the default settings of Prefect.         model_task_config: The Prefect task config to use for model request tasks. If no config is provided, use the default settings of Prefect.         tool_task_config: The default Prefect task config to use for tool calls. If no config is provided, use the default settings of Prefect.         tool_task_config_by_name: Per-tool task configuration. Keys are tool names, values are TaskConfig or None (None disables task wrapping for that tool).         event_stream_handler_task_config: The Prefect task config to use for the event stream handler task. If no config is provided, use the default settings of Prefect.         prefectify_toolset_func: Optional function to use to prepare toolsets for Prefect by wrapping them in a `PrefectWrapperToolset` that moves methods that require IO to Prefect tasks.             If not provided, only `FunctionToolset` and `MCPServer` will be prepared for Prefect.             The function takes the toolset, the task config, the tool-specific task config, and the tool-specific task config by name.     \"\"\"     super().__init__(wrapped)      self._name = name or wrapped.name     self._event_stream_handler = event_stream_handler     if self._name is None:         raise UserError(             \"An agent needs to have a unique `name` in order to be used with Prefect. The name will be used to identify the agent's flows and tasks.\"         )      # Merge the config with the default Prefect config     self._mcp_task_config = default_task_config | (mcp_task_config or {})     self._model_task_config = default_task_config | (model_task_config or {})     self._tool_task_config = default_task_config | (tool_task_config or {})     self._tool_task_config_by_name = tool_task_config_by_name or {}     self._event_stream_handler_task_config = default_task_config | (event_stream_handler_task_config or {})      if not isinstance(wrapped.model, Model):         raise UserError(             'An agent needs to have a `model` in order to be used with Prefect, it cannot be set at agent run time.'         )      prefect_model = PrefectModel(         wrapped.model,         task_config=self._model_task_config,         event_stream_handler=self.event_stream_handler,     )     self._model = prefect_model      def _prefectify_toolset(toolset: AbstractToolset[AgentDepsT]) -> AbstractToolset[AgentDepsT]:         \"\"\"Convert a toolset to its Prefect equivalent.\"\"\"         return prefectify_toolset_func(             toolset,             self._mcp_task_config,             self._tool_task_config,             self._tool_task_config_by_name,         )      prefect_toolsets = [toolset.visit_and_replace(_prefectify_toolset) for toolset in wrapped.toolsets]     self._toolsets = prefect_toolsets      # Context variable to track when we're inside this agent's Prefect flow     self._in_prefect_agent_flow: ContextVar[bool] = ContextVar(         f'_in_prefect_agent_flow_{self._name}', default=False     ) ``` |", "url": "https://ai.pydantic.dev/durable_exec/index.html#prefectagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PrefectAgent", "anchor": "prefectagent", "md_text": "#### run `async`\n\n```\nrun(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[OutputDataT]\n\nrun(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[RunOutputDataT]\n\nrun(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AgentRunResult[Any]\n```\n\nRun the agent with a user prompt in async mode.\n\nThis method builds an internal agent graph (using system prompts, tools and result schemas) and then\nruns the graph to completion. The result of the run is returned.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    agent_run = await agent.run('What is the capital of France?')\n    print(agent_run.output)\n    #> The capital of France is Paris.\n```\n\nParameters:", "url": "https://ai.pydantic.dev/durable_exec/index.html#prefectagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PrefectAgent", "anchor": "prefectagent", "md_text": "| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` |\n| `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` |\n| `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional event stream handler to use for this run. | `None` |\n| `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AgentRunResult[Any]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py`", "url": "https://ai.pydantic.dev/durable_exec/index.html#prefectagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PrefectAgent", "anchor": "prefectagent", "md_text": "|  |  |\n| --- | --- |\n| ``` 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 ``` | ``` async def run(     self,     user_prompt: str | Sequence[_messages.UserContent] | None = None,     *,     output_type: OutputSpec[RunOutputDataT] | None = None,     message_history: Sequence[_messages.ModelMessage] | None = None,     deferred_tool_results: DeferredToolResults | None = None,     model: models.Model | models.KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: _usage.UsageLimits | None = None,     usage: _usage.RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     **_deprecated_kwargs: Never, ) -> AgentRunResult[Any]:     \"\"\"Run the agent with a user prompt in async mode.      This method builds an internal agent graph (using system prompts, tools and result schemas) and then     runs the graph to completion. The result of the run is returned.      Example:     ```python     from pydantic_ai import Agent      agent = Agent('openai:gpt-4o')      async def main():         agent_run = await agent.run('What is the capital of France?')         print(agent_run.output)         #> The capital of France is Paris.     ```      Args:         user_prompt: User input to start/continue the conversation.         output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no             output validators since output validators would expect an argument that matches the agent's output type.         message_history: History of the conversation so far.         deferred_tool_results: Optional results for deferred tool calls in the message history.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.         event_stream_handler: Optional event stream handler to use for this run.         builtin_tools: Optional additional builtin tools for this run.      Returns:         The result of the run.     \"\"\"      @flow(name=f'{self._name} Run')     async def wrapped_run_flow() -> AgentRunResult[Any]:         # Mark that we're inside a PrefectAgent flow         token = self._in_prefect_agent_flow.set(True)         try:             with self._prefect_overrides():                 result = await super(WrapperAgent, self).run(                     user_prompt,                     output_type=output_type,                     message_history=message_history,                     deferred_tool_results=deferred_tool_results,                     model=model,                     deps=deps,                     model_settings=model_settings,                     usage_limits=usage_limits,                     usage=usage,                     infer_name=infer_name,                     toolsets=toolsets,                     event_stream_handler=event_stream_handler,                 )                 return result         finally:             self._in_prefect_agent_flow.reset(token)      return await wrapped_run_flow() ``` |\n\n#### run\\_sync\n\n```\nrun_sync(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[OutputDataT]", "url": "https://ai.pydantic.dev/durable_exec/index.html#prefectagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PrefectAgent", "anchor": "prefectagent", "md_text": "run_sync(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AgentRunResult[RunOutputDataT]\n\nrun_sync(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AgentRunResult[Any]\n```\n\nSynchronously run the agent with a user prompt.\n\nThis is a convenience method that wraps [`self.run`](../agent/index.html#pydantic_ai.agent.AbstractAgent.run) with `loop.run_until_complete(...)`.\nYou therefore can't use this method inside async code or if there's an active event loop.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nresult_sync = agent.run_sync('What is the capital of Italy?')\nprint(result_sync.output)\n#> The capital of Italy is Rome.\n```\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` |\n| `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` |\n| `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional event stream handler to use for this run. | `None` |\n| `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AgentRunResult[Any]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py`", "url": "https://ai.pydantic.dev/durable_exec/index.html#prefectagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PrefectAgent", "anchor": "prefectagent", "md_text": "|  |  |\n| --- | --- |\n| ``` 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 ``` | ``` def run_sync(     self,     user_prompt: str | Sequence[_messages.UserContent] | None = None,     *,     output_type: OutputSpec[RunOutputDataT] | None = None,     message_history: Sequence[_messages.ModelMessage] | None = None,     deferred_tool_results: DeferredToolResults | None = None,     model: models.Model | models.KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: _usage.UsageLimits | None = None,     usage: _usage.RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     **_deprecated_kwargs: Never, ) -> AgentRunResult[Any]:     \"\"\"Synchronously run the agent with a user prompt.      This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] with `loop.run_until_complete(...)`.     You therefore can't use this method inside async code or if there's an active event loop.      Example:     ```python     from pydantic_ai import Agent      agent = Agent('openai:gpt-4o')      result_sync = agent.run_sync('What is the capital of Italy?')     print(result_sync.output)     #> The capital of Italy is Rome.     ```      Args:         user_prompt: User input to start/continue the conversation.         output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no             output validators since output validators would expect an argument that matches the agent's output type.         message_history: History of the conversation so far.         deferred_tool_results: Optional results for deferred tool calls in the message history.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.         event_stream_handler: Optional event stream handler to use for this run.         builtin_tools: Optional additional builtin tools for this run.      Returns:         The result of the run.     \"\"\"      @flow(name=f'{self._name} Sync Run')     def wrapped_run_sync_flow() -> AgentRunResult[Any]:         # Mark that we're inside a PrefectAgent flow         token = self._in_prefect_agent_flow.set(True)         try:             with self._prefect_overrides():                 # Using `run_coro_as_sync` from Prefect with async `run` to avoid event loop conflicts.                 result = run_coro_as_sync(                     super(PrefectAgent, self).run(                         user_prompt,                         output_type=output_type,                         message_history=message_history,                         deferred_tool_results=deferred_tool_results,                         model=model,                         deps=deps,                         model_settings=model_settings,                         usage_limits=usage_limits,                         usage=usage,                         infer_name=infer_name,                         toolsets=toolsets,                         event_stream_handler=event_stream_handler,                     )                 )                 return result         finally:             self._in_prefect_agent_flow.reset(token)      return wrapped_run_sync_flow() ``` |\n\n#### run\\_stream `async`\n\n```\nrun_stream(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AbstractAsyncContextManager[\n    StreamedRunResult[AgentDepsT, OutputDataT]\n]", "url": "https://ai.pydantic.dev/durable_exec/index.html#prefectagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PrefectAgent", "anchor": "prefectagent", "md_text": "run_stream(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None\n) -> AbstractAsyncContextManager[\n    StreamedRunResult[AgentDepsT, RunOutputDataT]\n]\n\nrun_stream(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None,\n    event_stream_handler: (\n        EventStreamHandler[AgentDepsT] | None\n    ) = None,\n    **_deprecated_kwargs: Never\n) -> AsyncIterator[StreamedRunResult[AgentDepsT, Any]]\n```\n\nRun the agent with a user prompt in async mode, returning a streamed response.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    async with agent.run_stream('What is the capital of the UK?') as response:\n        print(await response.get_output())\n        #> The capital of the UK is London.\n```\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` |\n| `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` |\n| `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n| `event_stream_handler` | `EventStreamHandler[AgentDepsT] | None` | Optional event stream handler to use for this run. It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AsyncIterator[StreamedRunResult[AgentDepsT, Any]]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py`", "url": "https://ai.pydantic.dev/durable_exec/index.html#prefectagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PrefectAgent", "anchor": "prefectagent", "md_text": "|  |  |\n| --- | --- |\n| ``` 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 ``` | ``` @asynccontextmanager async def run_stream(     self,     user_prompt: str | Sequence[_messages.UserContent] | None = None,     *,     output_type: OutputSpec[RunOutputDataT] | None = None,     message_history: Sequence[_messages.ModelMessage] | None = None,     deferred_tool_results: DeferredToolResults | None = None,     model: models.Model | models.KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: _usage.UsageLimits | None = None,     usage: _usage.RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     builtin_tools: Sequence[AbstractBuiltinTool] | None = None,     event_stream_handler: EventStreamHandler[AgentDepsT] | None = None,     **_deprecated_kwargs: Never, ) -> AsyncIterator[StreamedRunResult[AgentDepsT, Any]]:     \"\"\"Run the agent with a user prompt in async mode, returning a streamed response.      Example:     ```python     from pydantic_ai import Agent      agent = Agent('openai:gpt-4o')      async def main():         async with agent.run_stream('What is the capital of the UK?') as response:             print(await response.get_output())             #> The capital of the UK is London.     ```      Args:         user_prompt: User input to start/continue the conversation.         output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no             output validators since output validators would expect an argument that matches the agent's output type.         message_history: History of the conversation so far.         deferred_tool_results: Optional results for deferred tool calls in the message history.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.         builtin_tools: Optional additional builtin tools for this run.         event_stream_handler: Optional event stream handler to use for this run. It will receive all the events up until the final result is found, which you can then read or stream from inside the context manager.      Returns:         The result of the run.     \"\"\"     if FlowRunContext.get() is not None:         raise UserError(             '`agent.run_stream()` cannot be used inside a Prefect flow. '             'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'         )      async with super().run_stream(         user_prompt,         output_type=output_type,         message_history=message_history,         deferred_tool_results=deferred_tool_results,         model=model,         deps=deps,         model_settings=model_settings,         usage_limits=usage_limits,         usage=usage,         infer_name=infer_name,         toolsets=toolsets,         event_stream_handler=event_stream_handler,         builtin_tools=builtin_tools,         **_deprecated_kwargs,     ) as result:         yield result ``` |\n\n#### run\\_stream\\_events\n\n```\nrun_stream_events(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[\n    AgentStreamEvent | AgentRunResultEvent[OutputDataT]\n]", "url": "https://ai.pydantic.dev/durable_exec/index.html#prefectagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PrefectAgent", "anchor": "prefectagent", "md_text": "run_stream_events(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[\n    AgentStreamEvent | AgentRunResultEvent[RunOutputDataT]\n]\n\nrun_stream_events(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[\n    AgentStreamEvent | AgentRunResultEvent[Any]\n]\n```\n\nRun the agent with a user prompt in async mode and stream events from the run.\n\nThis is a convenience method that wraps [`self.run`](../agent/index.html#pydantic_ai.agent.AbstractAgent.run) and\nuses the `event_stream_handler` kwarg to get a stream of events from the run.\n\nExample:\n\n```\nfrom pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    events: list[AgentStreamEvent | AgentRunResultEvent] = []\n    async for event in agent.run_stream_events('What is the capital of France?'):\n        events.append(event)\n    print(events)\n    '''\n    [\n        PartStartEvent(index=0, part=TextPart(content='The capital of ')),\n        FinalResultEvent(tool_name=None, tool_call_id=None),\n        PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='France is Paris. ')),\n        AgentRunResultEvent(\n            result=AgentRunResult(output='The capital of France is Paris. ')\n        ),\n    ]\n    '''\n```\n\nArguments are the same as for [`self.run`](../agent/index.html#pydantic_ai.agent.AbstractAgent.run),\nexcept that `event_stream_handler` is now allowed.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` |\n| `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` |\n| `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AsyncIterator[AgentStreamEvent | AgentRunResultEvent[Any]]` | An async iterable of stream events `AgentStreamEvent` and finally a `AgentRunResultEvent` with the final |\n| `AsyncIterator[AgentStreamEvent | AgentRunResultEvent[Any]]` | run result. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py`", "url": "https://ai.pydantic.dev/durable_exec/index.html#prefectagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PrefectAgent", "anchor": "prefectagent", "md_text": "|  |  |\n| --- | --- |\n| ``` 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 ``` | ``` def run_stream_events(     self,     user_prompt: str | Sequence[_messages.UserContent] | None = None,     *,     output_type: OutputSpec[RunOutputDataT] | None = None,     message_history: Sequence[_messages.ModelMessage] | None = None,     deferred_tool_results: DeferredToolResults | None = None,     model: models.Model | models.KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: _usage.UsageLimits | None = None,     usage: _usage.RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     builtin_tools: Sequence[AbstractBuiltinTool] | None = None, ) -> AsyncIterator[_messages.AgentStreamEvent | AgentRunResultEvent[Any]]:     \"\"\"Run the agent with a user prompt in async mode and stream events from the run.      This is a convenience method that wraps [`self.run`][pydantic_ai.agent.AbstractAgent.run] and     uses the `event_stream_handler` kwarg to get a stream of events from the run.      Example:     ```python     from pydantic_ai import Agent, AgentRunResultEvent, AgentStreamEvent      agent = Agent('openai:gpt-4o')      async def main():         events: list[AgentStreamEvent | AgentRunResultEvent] = []         async for event in agent.run_stream_events('What is the capital of France?'):             events.append(event)         print(events)         '''         [             PartStartEvent(index=0, part=TextPart(content='The capital of ')),             FinalResultEvent(tool_name=None, tool_call_id=None),             PartDeltaEvent(index=0, delta=TextPartDelta(content_delta='France is Paris. ')),             AgentRunResultEvent(                 result=AgentRunResult(output='The capital of France is Paris. ')             ),         ]         '''     ```      Arguments are the same as for [`self.run`][pydantic_ai.agent.AbstractAgent.run],     except that `event_stream_handler` is now allowed.      Args:         user_prompt: User input to start/continue the conversation.         output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no             output validators since output validators would expect an argument that matches the agent's output type.         message_history: History of the conversation so far.         deferred_tool_results: Optional results for deferred tool calls in the message history.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.         builtin_tools: Optional additional builtin tools for this run.      Returns:         An async iterable of stream events `AgentStreamEvent` and finally a `AgentRunResultEvent` with the final         run result.     \"\"\"     if FlowRunContext.get() is not None:         raise UserError(             '`agent.run_stream_events()` cannot be used inside a Prefect flow. '             'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'         )      return super().run_stream_events(         user_prompt,         output_type=output_type,         message_history=message_history,         deferred_tool_results=deferred_tool_results,         model=model,         deps=deps,         model_settings=model_settings,         usage_limits=usage_limits,         usage=usage,         infer_name=infer_name,         toolsets=toolsets,         builtin_tools=builtin_tools,     ) ``` |\n\n#### iter `async`", "url": "https://ai.pydantic.dev/durable_exec/index.html#prefectagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PrefectAgent", "anchor": "prefectagent", "md_text": "```\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AbstractAsyncContextManager[\n    AgentRun[AgentDepsT, OutputDataT]\n]\n\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT],\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AbstractAsyncContextManager[\n    AgentRun[AgentDepsT, RunOutputDataT]\n]\n\niter(\n    user_prompt: str | Sequence[UserContent] | None = None,\n    *,\n    output_type: OutputSpec[RunOutputDataT] | None = None,\n    message_history: Sequence[ModelMessage] | None = None,\n    deferred_tool_results: (\n        DeferredToolResults | None\n    ) = None,\n    model: Model | KnownModelName | str | None = None,\n    deps: AgentDepsT = None,\n    model_settings: ModelSettings | None = None,\n    usage_limits: UsageLimits | None = None,\n    usage: RunUsage | None = None,\n    infer_name: bool = True,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | None\n    ) = None,\n    builtin_tools: (\n        Sequence[AbstractBuiltinTool] | None\n    ) = None\n) -> AsyncIterator[AgentRun[AgentDepsT, Any]]\n```\n\nA contextmanager which can be used to iterate over the agent graph's nodes as they are executed.\n\nThis method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an\n`AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are\nexecuted. This is the API to use if you want to consume the outputs coming from each LLM model response, or the\nstream of events coming from the execution of tools.\n\nThe `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,\nand the final result of the run once it has completed.\n\nFor more details, see the documentation of `AgentRun`.\n\nExample:\n\n```\nfrom pydantic_ai import Agent\n\nagent = Agent('openai:gpt-4o')\n\nasync def main():\n    nodes = []\n    async with agent.iter('What is the capital of France?') as agent_run:\n        async for node in agent_run:\n            nodes.append(node)\n    print(nodes)\n    '''\n    [\n        UserPromptNode(\n            user_prompt='What is the capital of France?',\n            instructions_functions=[],\n            system_prompts=(),\n            system_prompt_functions=[],\n            system_prompt_dynamic_functions={},\n        ),\n        ModelRequestNode(\n            request=ModelRequest(\n                parts=[\n                    UserPromptPart(\n                        content='What is the capital of France?',\n                        timestamp=datetime.datetime(...),\n                    )\n                ]\n            )\n        ),\n        CallToolsNode(\n            model_response=ModelResponse(\n                parts=[TextPart(content='The capital of France is Paris.')],\n                usage=RequestUsage(input_tokens=56, output_tokens=7),\n                model_name='gpt-4o',\n                timestamp=datetime.datetime(...),\n            )\n        ),\n        End(data=FinalResult(output='The capital of France is Paris.')),\n    ]\n    '''\n    print(agent_run.result.output)\n    #> The capital of France is Paris.\n```\n\nParameters:", "url": "https://ai.pydantic.dev/durable_exec/index.html#prefectagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PrefectAgent", "anchor": "prefectagent", "md_text": "| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `user_prompt` | `str | Sequence[UserContent] | None` | User input to start/continue the conversation. | `None` |\n| `output_type` | `OutputSpec[RunOutputDataT] | None` | Custom output type to use for this run, `output_type` may only be used if the agent has no output validators since output validators would expect an argument that matches the agent's output type. | `None` |\n| `message_history` | `Sequence[ModelMessage] | None` | History of the conversation so far. | `None` |\n| `deferred_tool_results` | `DeferredToolResults | None` | Optional results for deferred tool calls in the message history. | `None` |\n| `model` | `Model | KnownModelName | str | None` | Optional model to use for this run, required if `model` was not set when creating the agent. | `None` |\n| `deps` | `AgentDepsT` | Optional dependencies to use for this run. | `None` |\n| `model_settings` | `ModelSettings | None` | Optional settings to use for this model's request. | `None` |\n| `usage_limits` | `UsageLimits | None` | Optional limits on model request count or token usage. | `None` |\n| `usage` | `RunUsage | None` | Optional usage to start with, useful for resuming a conversation or agents used in tools. | `None` |\n| `infer_name` | `bool` | Whether to try to infer the agent name from the call frame if it's not set. | `True` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | None` | Optional additional toolsets for this run. | `None` |\n| `builtin_tools` | `Sequence[AbstractBuiltinTool] | None` | Optional additional builtin tools for this run. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AsyncIterator[AgentRun[AgentDepsT, Any]]` | The result of the run. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py`", "url": "https://ai.pydantic.dev/durable_exec/index.html#prefectagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PrefectAgent", "anchor": "prefectagent", "md_text": "|  |  |\n| --- | --- |\n| ``` 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 ``` | ``` @asynccontextmanager async def iter(     self,     user_prompt: str | Sequence[_messages.UserContent] | None = None,     *,     output_type: OutputSpec[RunOutputDataT] | None = None,     message_history: Sequence[_messages.ModelMessage] | None = None,     deferred_tool_results: DeferredToolResults | None = None,     model: models.Model | models.KnownModelName | str | None = None,     deps: AgentDepsT = None,     model_settings: ModelSettings | None = None,     usage_limits: _usage.UsageLimits | None = None,     usage: _usage.RunUsage | None = None,     infer_name: bool = True,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | None = None,     builtin_tools: Sequence[AbstractBuiltinTool] | None = None, ) -> AsyncIterator[AgentRun[AgentDepsT, Any]]:     \"\"\"A contextmanager which can be used to iterate over the agent graph's nodes as they are executed.      This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an     `AgentRun` object. The `AgentRun` can be used to async-iterate over the nodes of the graph as they are     executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the     stream of events coming from the execution of tools.      The `AgentRun` also provides methods to access the full message history, new messages, and usage statistics,     and the final result of the run once it has completed.      For more details, see the documentation of `AgentRun`.      Example:     ```python     from pydantic_ai import Agent      agent = Agent('openai:gpt-4o')      async def main():         nodes = []         async with agent.iter('What is the capital of France?') as agent_run:             async for node in agent_run:                 nodes.append(node)         print(nodes)         '''         [             UserPromptNode(                 user_prompt='What is the capital of France?',                 instructions_functions=[],                 system_prompts=(),                 system_prompt_functions=[],                 system_prompt_dynamic_functions={},             ),             ModelRequestNode(                 request=ModelRequest(                     parts=[                         UserPromptPart(                             content='What is the capital of France?',                             timestamp=datetime.datetime(...),                         )                     ]                 )             ),             CallToolsNode(                 model_response=ModelResponse(                     parts=[TextPart(content='The capital of France is Paris.')],                     usage=RequestUsage(input_tokens=56, output_tokens=7),                     model_name='gpt-4o',                     timestamp=datetime.datetime(...),                 )             ),             End(data=FinalResult(output='The capital of France is Paris.')),         ]         '''         print(agent_run.result.output)         #> The capital of France is Paris.     ```      Args:         user_prompt: User input to start/continue the conversation.         output_type: Custom output type to use for this run, `output_type` may only be used if the agent has no             output validators since output validators would expect an argument that matches the agent's output type.         message_history: History of the conversation so far.         deferred_tool_results: Optional results for deferred tool calls in the message history.         model: Optional model to use for this run, required if `model` was not set when creating the agent.         deps: Optional dependencies to use for this run.         model_settings: Optional settings to use for this model's request.         usage_limits: Optional limits on model request count or token usage.         usage: Optional usage to start with, useful for resuming a conversation or agents used in tools.         infer_name: Whether to try to infer the agent name from the call frame if it's not set.         toolsets: Optional additional toolsets for this run.         builtin_tools: Optional additional builtin tools for this run.      Returns:         The result of the run.     \"\"\"     if model is not None and not isinstance(model, PrefectModel):         raise UserError(             'Non-Prefect model cannot be set at agent run time inside a Prefect flow, it must be set at agent creation time.'         )      with self._prefect_overrides():         async with super().iter(             user_prompt=user_prompt,             output_type=output_type,             message_history=message_history,             deferred_tool_results=deferred_tool_results,             model=model,             deps=deps,             model_settings=model_settings,             usage_limits=usage_limits,             usage=usage,             infer_name=infer_name,             toolsets=toolsets,         ) as run:             yield run ``` |", "url": "https://ai.pydantic.dev/durable_exec/index.html#prefectagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PrefectAgent", "anchor": "prefectagent", "md_text": "#### override\n\n```\noverride(\n    *,\n    name: str | Unset = UNSET,\n    deps: AgentDepsT | Unset = UNSET,\n    model: Model | KnownModelName | str | Unset = UNSET,\n    toolsets: (\n        Sequence[AbstractToolset[AgentDepsT]] | Unset\n    ) = UNSET,\n    tools: (\n        Sequence[\n            Tool[AgentDepsT]\n            | ToolFuncEither[AgentDepsT, ...]\n        ]\n        | Unset\n    ) = UNSET,\n    instructions: Instructions[AgentDepsT] | Unset = UNSET\n) -> Iterator[None]\n```\n\nContext manager to temporarily override agent dependencies, model, toolsets, tools, or instructions.\n\nThis is particularly useful when testing.\nYou can find an example of this [here](https://ai.pydantic.dev/testing/#overriding-model-via-pytest-fixtures).\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `name` | `str | Unset` | The name to use instead of the name passed to the agent constructor and agent run. | `UNSET` |\n| `deps` | `AgentDepsT | Unset` | The dependencies to use instead of the dependencies passed to the agent run. | `UNSET` |\n| `model` | `Model | KnownModelName | str | Unset` | The model to use instead of the model passed to the agent run. | `UNSET` |\n| `toolsets` | `Sequence[AbstractToolset[AgentDepsT]] | Unset` | The toolsets to use instead of the toolsets passed to the agent constructor and agent run. | `UNSET` |\n| `tools` | `Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | Unset` | The tools to use instead of the tools registered with the agent. | `UNSET` |\n| `instructions` | `Instructions[AgentDepsT] | Unset` | The instructions to use instead of the instructions registered with the agent. | `UNSET` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py`\n\n|  |  |\n| --- | --- |\n| ``` 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 ``` | ``` @contextmanager def override(     self,     *,     name: str | _utils.Unset = _utils.UNSET,     deps: AgentDepsT | _utils.Unset = _utils.UNSET,     model: models.Model | models.KnownModelName | str | _utils.Unset = _utils.UNSET,     toolsets: Sequence[AbstractToolset[AgentDepsT]] | _utils.Unset = _utils.UNSET,     tools: Sequence[Tool[AgentDepsT] | ToolFuncEither[AgentDepsT, ...]] | _utils.Unset = _utils.UNSET,     instructions: Instructions[AgentDepsT] | _utils.Unset = _utils.UNSET, ) -> Iterator[None]:     \"\"\"Context manager to temporarily override agent dependencies, model, toolsets, tools, or instructions.      This is particularly useful when testing.     You can find an example of this [here](../testing.md#overriding-model-via-pytest-fixtures).      Args:         name: The name to use instead of the name passed to the agent constructor and agent run.         deps: The dependencies to use instead of the dependencies passed to the agent run.         model: The model to use instead of the model passed to the agent run.         toolsets: The toolsets to use instead of the toolsets passed to the agent constructor and agent run.         tools: The tools to use instead of the tools registered with the agent.         instructions: The instructions to use instead of the instructions registered with the agent.     \"\"\"     if _utils.is_set(model) and not isinstance(model, PrefectModel):         raise UserError(             'Non-Prefect model cannot be contextually overridden inside a Prefect flow, it must be set at agent creation time.'         )      with super().override(         name=name, deps=deps, model=model, toolsets=toolsets, tools=tools, instructions=instructions     ):         yield ``` |", "url": "https://ai.pydantic.dev/durable_exec/index.html#prefectagent", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PrefectFunctionToolset", "anchor": "prefectfunctiontoolset", "md_text": "Bases: `PrefectWrapperToolset[AgentDepsT]`\n\nA wrapper for FunctionToolset that integrates with Prefect, turning tool calls into Prefect tasks.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_function_toolset.py`\n\n|  |  |\n| --- | --- |\n| ``` 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 ``` | ``` class PrefectFunctionToolset(PrefectWrapperToolset[AgentDepsT]):     \"\"\"A wrapper for FunctionToolset that integrates with Prefect, turning tool calls into Prefect tasks.\"\"\"      def __init__(         self,         wrapped: FunctionToolset[AgentDepsT],         *,         task_config: TaskConfig,         tool_task_config: dict[str, TaskConfig | None],     ):         super().__init__(wrapped)         self._task_config = default_task_config | (task_config or {})         self._tool_task_config = tool_task_config or {}          @task         async def _call_tool_task(             tool_name: str,             tool_args: dict[str, Any],             ctx: RunContext[AgentDepsT],             tool: ToolsetTool[AgentDepsT],         ) -> Any:             return await super(PrefectFunctionToolset, self).call_tool(tool_name, tool_args, ctx, tool)          self._call_tool_task = _call_tool_task      async def call_tool(         self,         name: str,         tool_args: dict[str, Any],         ctx: RunContext[AgentDepsT],         tool: ToolsetTool[AgentDepsT],     ) -> Any:         \"\"\"Call a tool, wrapped as a Prefect task with a descriptive name.\"\"\"         # Check if this specific tool has custom config or is disabled         tool_specific_config = self._tool_task_config.get(name, default_task_config)         if tool_specific_config is None:             # None means this tool should not be wrapped as a task             return await super().call_tool(name, tool_args, ctx, tool)          # Merge tool-specific config with default config         merged_config = self._task_config | tool_specific_config          return await self._call_tool_task.with_options(name=f'Call Tool: {name}', **merged_config)(             name, tool_args, ctx, tool         ) ``` |\n\n#### call\\_tool `async`\n\n```\ncall_tool(\n    name: str,\n    tool_args: dict[str, Any],\n    ctx: RunContext[AgentDepsT],\n    tool: ToolsetTool[AgentDepsT],\n) -> Any\n```\n\nCall a tool, wrapped as a Prefect task with a descriptive name.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_function_toolset.py`\n\n|  |  |\n| --- | --- |\n| ``` 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 ``` | ``` async def call_tool(     self,     name: str,     tool_args: dict[str, Any],     ctx: RunContext[AgentDepsT],     tool: ToolsetTool[AgentDepsT], ) -> Any:     \"\"\"Call a tool, wrapped as a Prefect task with a descriptive name.\"\"\"     # Check if this specific tool has custom config or is disabled     tool_specific_config = self._tool_task_config.get(name, default_task_config)     if tool_specific_config is None:         # None means this tool should not be wrapped as a task         return await super().call_tool(name, tool_args, ctx, tool)      # Merge tool-specific config with default config     merged_config = self._task_config | tool_specific_config      return await self._call_tool_task.with_options(name=f'Call Tool: {name}', **merged_config)(         name, tool_args, ctx, tool     ) ``` |", "url": "https://ai.pydantic.dev/durable_exec/index.html#prefectfunctiontoolset", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PrefectMCPServer", "anchor": "prefectmcpserver", "md_text": "Bases: `PrefectWrapperToolset[AgentDepsT]`, `ABC`\n\nA wrapper for MCPServer that integrates with Prefect, turning call\\_tool and get\\_tools into Prefect tasks.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_mcp_server.py`\n\n|  |  |\n| --- | --- |\n| ``` 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 ``` | ``` class PrefectMCPServer(PrefectWrapperToolset[AgentDepsT], ABC):     \"\"\"A wrapper for MCPServer that integrates with Prefect, turning call_tool and get_tools into Prefect tasks.\"\"\"      def __init__(         self,         wrapped: MCPServer,         *,         task_config: TaskConfig,     ):         super().__init__(wrapped)         self._task_config = default_task_config | (task_config or {})         self._mcp_id = wrapped.id          @task         async def _call_tool_task(             tool_name: str,             tool_args: dict[str, Any],             ctx: RunContext[AgentDepsT],             tool: ToolsetTool[AgentDepsT],         ) -> ToolResult:             return await super(PrefectMCPServer, self).call_tool(tool_name, tool_args, ctx, tool)          self._call_tool_task = _call_tool_task      async def __aenter__(self) -> Self:         await self.wrapped.__aenter__()         return self      async def __aexit__(self, *args: Any) -> bool | None:         return await self.wrapped.__aexit__(*args)      async def call_tool(         self,         name: str,         tool_args: dict[str, Any],         ctx: RunContext[AgentDepsT],         tool: ToolsetTool[AgentDepsT],     ) -> ToolResult:         \"\"\"Call an MCP tool, wrapped as a Prefect task with a descriptive name.\"\"\"         return await self._call_tool_task.with_options(name=f'Call MCP Tool: {name}', **self._task_config)(             name, tool_args, ctx, tool         ) ``` |\n\n#### call\\_tool `async`\n\n```\ncall_tool(\n    name: str,\n    tool_args: dict[str, Any],\n    ctx: RunContext[AgentDepsT],\n    tool: ToolsetTool[AgentDepsT],\n) -> ToolResult\n```\n\nCall an MCP tool, wrapped as a Prefect task with a descriptive name.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_mcp_server.py`\n\n|  |  |\n| --- | --- |\n| ``` 50 51 52 53 54 55 56 57 58 59 60 ``` | ``` async def call_tool(     self,     name: str,     tool_args: dict[str, Any],     ctx: RunContext[AgentDepsT],     tool: ToolsetTool[AgentDepsT], ) -> ToolResult:     \"\"\"Call an MCP tool, wrapped as a Prefect task with a descriptive name.\"\"\"     return await self._call_tool_task.with_options(name=f'Call MCP Tool: {name}', **self._task_config)(         name, tool_args, ctx, tool     ) ``` |", "url": "https://ai.pydantic.dev/durable_exec/index.html#prefectmcpserver", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PrefectModel", "anchor": "prefectmodel", "md_text": "Bases: `WrapperModel`\n\nA wrapper for Model that integrates with Prefect, turning request and request\\_stream into Prefect tasks.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model.py`\n\n|  |  |\n| --- | --- |\n| ```  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 ``` | ``` class PrefectModel(WrapperModel):     \"\"\"A wrapper for Model that integrates with Prefect, turning request and request_stream into Prefect tasks.\"\"\"      def __init__(         self,         model: Any,         *,         task_config: TaskConfig,         event_stream_handler: EventStreamHandler[Any] | None = None,     ):         super().__init__(model)         self.task_config = default_task_config | (task_config or {})         self.event_stream_handler = event_stream_handler          @task         async def wrapped_request(             messages: list[ModelMessage],             model_settings: ModelSettings | None,             model_request_parameters: ModelRequestParameters,         ) -> ModelResponse:             response = await super(PrefectModel, self).request(messages, model_settings, model_request_parameters)             return response          self._wrapped_request = wrapped_request          @task         async def request_stream_task(             messages: list[ModelMessage],             model_settings: ModelSettings | None,             model_request_parameters: ModelRequestParameters,             ctx: RunContext[Any] | None,         ) -> ModelResponse:             async with super(PrefectModel, self).request_stream(                 messages, model_settings, model_request_parameters, ctx             ) as streamed_response:                 if self.event_stream_handler is not None:                     assert ctx is not None, (                         'A Prefect model cannot be used with `pydantic_ai.direct.model_request_stream()` as it requires a `run_context`. '                         'Set an `event_stream_handler` on the agent and use `agent.run()` instead.'                     )                     await self.event_stream_handler(ctx, streamed_response)                  # Consume the entire stream                 async for _ in streamed_response:                     pass             response = streamed_response.get()             return response          self._wrapped_request_stream = request_stream_task      async def request(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> ModelResponse:         \"\"\"Make a model request, wrapped as a Prefect task when in a flow.\"\"\"         return await self._wrapped_request.with_options(             name=f'Model Request: {self.wrapped.model_name}', **self.task_config         )(messages, model_settings, model_request_parameters)      @asynccontextmanager     async def request_stream(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,         run_context: RunContext[Any] | None = None,     ) -> AsyncIterator[StreamedResponse]:         \"\"\"Make a streaming model request.          When inside a Prefect flow, the stream is consumed within a task and         a non-streaming response is returned. When not in a flow, behaves normally.         \"\"\"         # Check if we're in a flow context         flow_run_context = FlowRunContext.get()          # If not in a flow, just call the wrapped request_stream method         if flow_run_context is None:             async with super().request_stream(                 messages, model_settings, model_request_parameters, run_context             ) as streamed_response:                 yield streamed_response                 return          # If in a flow, consume the stream in a task and return the final response         response = await self._wrapped_request_stream.with_options(             name=f'Model Request (Streaming): {self.wrapped.model_name}', **self.task_config         )(messages, model_settings, model_request_parameters, run_context)         yield PrefectStreamedResponse(model_request_parameters, response) ``` |\n\n#### request `async`\n\n```\nrequest(\n    messages: list[ModelMessage],\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n) -> ModelResponse\n```\n\nMake a model request, wrapped as a Prefect task when in a flow.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model.py`", "url": "https://ai.pydantic.dev/durable_exec/index.html#prefectmodel", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PrefectModel", "anchor": "prefectmodel", "md_text": "|  |  |\n| --- | --- |\n| ``` 113 114 115 116 117 118 119 120 121 122 ``` | ``` async def request(     self,     messages: list[ModelMessage],     model_settings: ModelSettings | None,     model_request_parameters: ModelRequestParameters, ) -> ModelResponse:     \"\"\"Make a model request, wrapped as a Prefect task when in a flow.\"\"\"     return await self._wrapped_request.with_options(         name=f'Model Request: {self.wrapped.model_name}', **self.task_config     )(messages, model_settings, model_request_parameters) ``` |\n\n#### request\\_stream `async`\n\n```\nrequest_stream(\n    messages: list[ModelMessage],\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n    run_context: RunContext[Any] | None = None,\n) -> AsyncIterator[StreamedResponse]\n```\n\nMake a streaming model request.\n\nWhen inside a Prefect flow, the stream is consumed within a task and\na non-streaming response is returned. When not in a flow, behaves normally.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model.py`\n\n|  |  |\n| --- | --- |\n| ``` 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 ``` | ``` @asynccontextmanager async def request_stream(     self,     messages: list[ModelMessage],     model_settings: ModelSettings | None,     model_request_parameters: ModelRequestParameters,     run_context: RunContext[Any] | None = None, ) -> AsyncIterator[StreamedResponse]:     \"\"\"Make a streaming model request.      When inside a Prefect flow, the stream is consumed within a task and     a non-streaming response is returned. When not in a flow, behaves normally.     \"\"\"     # Check if we're in a flow context     flow_run_context = FlowRunContext.get()      # If not in a flow, just call the wrapped request_stream method     if flow_run_context is None:         async with super().request_stream(             messages, model_settings, model_request_parameters, run_context         ) as streamed_response:             yield streamed_response             return      # If in a flow, consume the stream in a task and return the final response     response = await self._wrapped_request_stream.with_options(         name=f'Model Request (Streaming): {self.wrapped.model_name}', **self.task_config     )(messages, model_settings, model_request_parameters, run_context)     yield PrefectStreamedResponse(model_request_parameters, response) ``` |", "url": "https://ai.pydantic.dev/durable_exec/index.html#prefectmodel", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "TaskConfig", "anchor": "taskconfig", "md_text": "Bases: `TypedDict`\n\nConfiguration for a task in Prefect.\n\nThese options are passed to the `@task` decorator.\n\nSource code in `pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_types.py`\n\n|  |  |\n| --- | --- |\n| ``` 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 ``` | ``` class TaskConfig(TypedDict, total=False):     \"\"\"Configuration for a task in Prefect.      These options are passed to the `@task` decorator.     \"\"\"      retries: int     \"\"\"Maximum number of retries for the task.\"\"\"      retry_delay_seconds: float | list[float]     \"\"\"Delay between retries in seconds. Can be a single value or a list for custom backoff.\"\"\"      timeout_seconds: float     \"\"\"Maximum time in seconds for the task to complete.\"\"\"      cache_policy: CachePolicy     \"\"\"Prefect cache policy for the task.\"\"\"      persist_result: bool     \"\"\"Whether to persist the task result.\"\"\"      result_storage: ResultStorage     \"\"\"Prefect result storage for the task. Should be a storage block or a block slug like `s3-bucket/my-storage`.\"\"\"      log_prints: bool     \"\"\"Whether to log print statements from the task.\"\"\" ``` |\n\n#### retries `instance-attribute`\n\n```\nretries: int\n```\n\nMaximum number of retries for the task.\n\n#### retry\\_delay\\_seconds `instance-attribute`\n\n```\nretry_delay_seconds: float | list[float]\n```\n\nDelay between retries in seconds. Can be a single value or a list for custom backoff.\n\n#### timeout\\_seconds `instance-attribute`\n\n```\ntimeout_seconds: float\n```\n\nMaximum time in seconds for the task to complete.\n\n#### cache\\_policy `instance-attribute`\n\n```\ncache_policy: CachePolicy\n```\n\nPrefect cache policy for the task.\n\n#### persist\\_result `instance-attribute`\n\n```\npersist_result: bool\n```\n\nWhether to persist the task result.\n\n#### result\\_storage `instance-attribute`\n\n```\nresult_storage: ResultStorage\n```\n\nPrefect result storage for the task. Should be a storage block or a block slug like `s3-bucket/my-storage`.\n\n#### log\\_prints `instance-attribute`\n\n```\nlog_prints: bool\n```\n\nWhether to log print statements from the task.", "url": "https://ai.pydantic.dev/durable_exec/index.html#taskconfig", "page": "durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "md_text": "Pydantic Evals is built around these core concepts:\n\n* **[`Dataset`](../pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset)** - A static definition containing test cases and evaluators\n* **[`Case`](../pydantic_evals/dataset/index.html#pydantic_evals.dataset.Case)** - A single test scenario with inputs and optional expected outputs\n* **[`Evaluator`](../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.Evaluator)** - Logic for scoring or validating outputs\n* **Experiment** - The act of running a task function against all cases in a dataset. (This corresponds to a call to `Dataset.evaluate`.)\n* **[`EvaluationReport`](../pydantic_evals/reporting/index.html#pydantic_evals.reporting.EvaluationReport)** - The results from running an experiment\n\nThe key distinction is between:\n\n* **Definition** (`Dataset` with `Case`s and `Evaluator`s) - what you want to test\n* **Execution** (Experiment) - running your task against those tests\n* **Results** (`EvaluationReport`) - what happened during the experiment", "url": "https://ai.pydantic.dev/core-concepts/index.html#overview", "page": "core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Unit Testing Analogy", "anchor": "unit-testing-analogy", "md_text": "A helpful way to think about Pydantic Evals:\n\n| Unit Testing | Pydantic Evals |\n| --- | --- |\n| Test function | [`Case`](../pydantic_evals/dataset/index.html#pydantic_evals.dataset.Case) + [`Evaluator`](../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.Evaluator) |\n| Test suite | [`Dataset`](../pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset) |\n| Running tests (`pytest`) | **Experiment** (`dataset.evaluate(task)`) |\n| Test report | [`EvaluationReport`](../pydantic_evals/reporting/index.html#pydantic_evals.reporting.EvaluationReport) |\n| `assert` | Evaluator returning `bool` |\n\n**Key Difference**: AI systems are probabilistic, so instead of simple pass/fail, evaluations can have:\n\n* Quantitative scores (0.0 to 1.0)\n* Qualitative labels (\"good\", \"acceptable\", \"poor\")\n* Pass/fail assertions with explanatory reasons\n\nJust like you can run `pytest` multiple times on the same test suite, you can run multiple experiments on the same dataset to compare different implementations or track changes over time.", "url": "https://ai.pydantic.dev/core-concepts/index.html#unit-testing-analogy", "page": "core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Dataset", "anchor": "dataset", "md_text": "A [`Dataset`](../pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset) is a collection of test cases and evaluators that define an evaluation suite.\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import IsInstance\n\ndataset = Dataset(\n    name='my_eval_suite',  # Optional name\n    cases=[\n        Case(inputs='test input', expected_output='test output'),\n    ],\n    evaluators=[\n        IsInstance(type_name='str'),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/core-concepts/index.html#dataset", "page": "core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Key Features", "anchor": "key-features", "md_text": "* **Type-safe**: Generic over `InputsT`, `OutputT`, and `MetadataT` types\n* **Serializable**: Can be saved to/loaded from YAML or JSON files\n* **Evaluable**: Run against any function with matching input/output types", "url": "https://ai.pydantic.dev/core-concepts/index.html#key-features", "page": "core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "`Dataset`-Level vs `Case`-Level Evaluators", "anchor": "dataset-level-vs-case-level-evaluators", "md_text": "Evaluators can be defined at two levels:\n\n* **`Dataset`-level**: Apply to all cases in the dataset\n* **`Case`-level**: Apply only to specific cases\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import EqualsExpected, IsInstance\n\ndataset = Dataset(\n    cases=[\n        Case(\n            name='special_case',\n            inputs='test',\n            expected_output='TEST',\n            evaluators=[\n                # This evaluator only runs for this case\n                EqualsExpected(),\n            ],\n        ),\n    ],\n    evaluators=[\n        # This evaluator runs for ALL cases\n        IsInstance(type_name='str'),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/core-concepts/index.html#dataset-level-vs-case-level-evaluators", "page": "core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Experiments", "anchor": "experiments", "md_text": "An **Experiment** is what happens when you execute a task function against all cases in a dataset. This is the bridge between your static test definition (the Dataset) and your results (the EvaluationReport).", "url": "https://ai.pydantic.dev/core-concepts/index.html#experiments", "page": "core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Running an Experiment", "anchor": "running-an-experiment", "md_text": "You run an experiment by calling [`evaluate()`](../pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset.evaluate) or [`evaluate_sync()`](../pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset.evaluate_sync) on a dataset:\n\n```\nfrom pydantic_evals import Case, Dataset\n\n# Define your dataset (static definition)\ndataset = Dataset(\n    cases=[\n        Case(inputs='hello', expected_output='HELLO'),\n        Case(inputs='world', expected_output='WORLD'),\n    ],\n)\n\n# Define your task\ndef uppercase_task(text: str) -> str:\n    return text.upper()\n\n# Run the experiment (execution)\nreport = dataset.evaluate_sync(uppercase_task)\n```", "url": "https://ai.pydantic.dev/core-concepts/index.html#running-an-experiment", "page": "core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "What Happens During an Experiment", "anchor": "what-happens-during-an-experiment", "md_text": "When you run an experiment:\n\n1. **Setup**: The dataset loads all cases and evaluators\n2. **Execution**: For each case:\n   1. The task function is called with `case.inputs`\n   2. Execution time is measured and OpenTelemetry spans are captured (if `logfire` is configured)\n   3. The outputs of the task function for each case are recorded\n3. **Evaluation**: For each case output:\n   1. All dataset-level evaluators are run\n   2. Case-specific evaluators are run (if any)\n   3. Results are collected (scores, assertions, labels)\n4. **Reporting**: All results are aggregated into an [`EvaluationReport`](../pydantic_evals/reporting/index.html#pydantic_evals.reporting.EvaluationReport)", "url": "https://ai.pydantic.dev/core-concepts/index.html#what-happens-during-an-experiment", "page": "core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Multiple Experiments from One Dataset", "anchor": "multiple-experiments-from-one-dataset", "md_text": "A key feature of Pydantic Evals is that you can run the same dataset against different task implementations:\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import EqualsExpected\n\ndataset = Dataset(\n    cases=[\n        Case(inputs='hello', expected_output='HELLO'),\n    ],\n    evaluators=[EqualsExpected()],\n)\n\n\n# Original implementation\ndef task_v1(text: str) -> str:\n    return text.upper()\n\n\n# Improved implementation (with exclamation)\ndef task_v2(text: str) -> str:\n    return text.upper() + '!'\n\n\n# Compare results\nreport_v1 = dataset.evaluate_sync(task_v1)\nreport_v2 = dataset.evaluate_sync(task_v2)\n\navg_v1 = report_v1.averages()\navg_v2 = report_v2.averages()\nprint(f'V1 pass rate: {avg_v1.assertions if avg_v1 and avg_v1.assertions else 0}')\n#> V1 pass rate: 1.0\nprint(f'V2 pass rate: {avg_v2.assertions if avg_v2 and avg_v2.assertions else 0}')\n#> V2 pass rate: 0\n```\n\nThis allows you to:\n\n* **Compare implementations** across versions\n* **Track performance** over time\n* **A/B test** different approaches\n* **Validate changes** before deployment", "url": "https://ai.pydantic.dev/core-concepts/index.html#multiple-experiments-from-one-dataset", "page": "core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Case", "anchor": "case", "md_text": "A [`Case`](../pydantic_evals/dataset/index.html#pydantic_evals.dataset.Case) represents a single test scenario with specific inputs and optional expected outputs.\n\n```\nfrom pydantic_evals import Case\nfrom pydantic_evals.evaluators import EqualsExpected\n\ncase = Case(\n    name='test_uppercase',  # Optional, but recommended for reporting\n    inputs='hello world',  # Required: inputs to your task\n    expected_output='HELLO WORLD',  # Optional: expected output\n    metadata={'category': 'basic'},  # Optional: arbitrary metadata\n    evaluators=[EqualsExpected()],  # Optional: case-specific evaluators\n)\n```", "url": "https://ai.pydantic.dev/core-concepts/index.html#case", "page": "core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Case Components", "anchor": "case-components", "md_text": "#### Inputs\n\nThe inputs to pass to the task being evaluated. Can be any type:\n\n```\nfrom pydantic import BaseModel\n\nfrom pydantic_evals import Case\n\n\nclass MyInputModel(BaseModel):\n    field1: str\n\n\n# Simple types\nCase(inputs='hello')\nCase(inputs=42)\n\n# Complex types\nCase(inputs={'query': 'What is AI?', 'max_tokens': 100})\nCase(inputs=MyInputModel(field1='value'))\n```\n\n#### Expected Output\n\nThe expected result, used by evaluators like [`EqualsExpected`](../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EqualsExpected):\n\n```\nfrom pydantic_evals import Case\n\nCase(\n    inputs='2 + 2',\n    expected_output='4',\n)\n```\n\nIf no `expected_output` is provided, evaluators that require it (like `EqualsExpected`) will skip that case.\n\n#### Metadata\n\nArbitrary data that evaluators can access via [`EvaluatorContext`](../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorContext):\n\n```\nfrom pydantic_evals import Case\n\nCase(\n    inputs='question',\n    metadata={\n        'difficulty': 'hard',\n        'category': 'math',\n        'source': 'exam_2024',\n    },\n)\n```\n\nMetadata is useful for:\n\n* Filtering cases during analysis\n* Providing context to evaluators\n* Organizing test suites\n\n#### Evaluators\n\nCases can have their own evaluators that only run for that specific case. This is particularly powerful for building comprehensive evaluation suites where different cases have different requirements - if you could write one evaluator rubric that worked perfectly for all cases, you'd just incorporate it into your agent instructions. Case-specific [`LLMJudge`](../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.LLMJudge) evaluators are especially useful for quickly building maintainable golden datasets by describing what \"good\" looks like for each scenario. See [Case-specific evaluators](../evaluators/overview/index.html#case-specific-evaluators) for a more detailed explanation and examples.", "url": "https://ai.pydantic.dev/core-concepts/index.html#case-components", "page": "core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluator", "anchor": "evaluator", "md_text": "An [`Evaluator`](../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.Evaluator) assesses the output of your task and returns one or more scores, labels, or assertions. Each score, label or assertion can also have an optional string-value reason associated.", "url": "https://ai.pydantic.dev/core-concepts/index.html#evaluator", "page": "core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluator Types", "anchor": "evaluator-types", "md_text": "Evaluators return different types of results:\n\n| Return Type | Purpose | Example |\n| --- | --- | --- |\n| `bool` | **Assertion** - Pass/fail check | `True` → ✔, `False` → ✗ |\n| `int` or `float` | **Score** - Numeric quality metric | `0.95`, `87` |\n| `str` | **Label** - Categorical result | `\"correct\"`, `\"hallucination\"` |\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass ExactMatch(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return ctx.output == ctx.expected_output  # Assertion\n\n\n@dataclass\nclass Confidence(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> float:\n        # Analyze output and return confidence score\n        return 0.95  # Score\n\n\n@dataclass\nclass Classifier(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> str:\n        if 'error' in ctx.output.lower():\n            return 'error'  # Label\n        return 'success'\n```\n\nEvaluators can also return instances of [`EvaluationReason`](../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluationReason), and dictionaries mapping labels to output values.\nSee the [custom evaluator return types](../evaluators/custom/index.html#return-types) docs for more detail.", "url": "https://ai.pydantic.dev/core-concepts/index.html#evaluator-types", "page": "core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluatorContext", "anchor": "evaluatorcontext", "md_text": "All evaluators receive an [`EvaluatorContext`](../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorContext) containing:\n\n* `name`: Case name (optional)\n* `inputs`: Task inputs\n* `metadata`: Case metadata (optional)\n* `expected_output`: Expected output (optional)\n* `output`: Actual output from task\n* `duration`: Task execution time in seconds\n* `span_tree`: OpenTelemetry spans (if `logfire` is configured)\n* `attributes`: Custom attributes dict\n* `metrics`: Custom metrics dict", "url": "https://ai.pydantic.dev/core-concepts/index.html#evaluatorcontext", "page": "core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Multiple Evaluations", "anchor": "multiple-evaluations", "md_text": "Evaluators can return multiple results by returning a dictionary:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass MultiCheck(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> dict[str, bool | float | str]:\n        return {\n            'is_valid': isinstance(ctx.output, str),  # Assertion\n            'length': len(ctx.output),  # Metric\n            'category': 'long' if len(ctx.output) > 100 else 'short',  # Label\n        }\n```", "url": "https://ai.pydantic.dev/core-concepts/index.html#multiple-evaluations", "page": "core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluation Reasons", "anchor": "evaluation-reasons", "md_text": "Add explanations to your evaluations using [`EvaluationReason`](../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluationReason):\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import EvaluationReason, Evaluator, EvaluatorContext\n\n\n@dataclass\nclass SmartCheck(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> EvaluationReason:\n        if ctx.output == ctx.expected_output:\n            return EvaluationReason(\n                value=True,\n                reason='Exact match with expected output',\n            )\n        return EvaluationReason(\n            value=False,\n            reason=f'Expected {ctx.expected_output!r}, got {ctx.output!r}',\n        )\n```\n\nReasons appear in reports when using `include_reasons=True`.", "url": "https://ai.pydantic.dev/core-concepts/index.html#evaluation-reasons", "page": "core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluation Report", "anchor": "evaluation-report", "md_text": "An [`EvaluationReport`](../pydantic_evals/reporting/index.html#pydantic_evals.reporting.EvaluationReport) is the result of running an experiment. It contains all the data from executing your task against the dataset's cases and running all evaluators.\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import EqualsExpected\n\ndataset = Dataset(\n    cases=[Case(inputs='hello', expected_output='HELLO')],\n    evaluators=[EqualsExpected()],\n)\n\n\ndef my_task(text: str) -> str:\n    return text.upper()\n\n\n# Run an experiment\nreport = dataset.evaluate_sync(my_task)\n\n# Print to console\nreport.print()\n\"\"\"\n    Evaluation Summary: my_task\n┏━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ Case ID  ┃ Assertions ┃ Duration ┃\n┡━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n│ Case 1   │ ✔          │     10ms │\n├──────────┼────────────┼──────────┤\n│ Averages │ 100.0% ✔   │     10ms │\n└──────────┴────────────┴──────────┘\n\"\"\"\n\n# Access data programmatically\nfor case in report.cases:\n    print(f'{case.name}: {case.scores}')\n    #> Case 1: {}\n```", "url": "https://ai.pydantic.dev/core-concepts/index.html#evaluation-report", "page": "core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Report Structure", "anchor": "report-structure", "md_text": "The [`EvaluationReport`](../pydantic_evals/reporting/index.html#pydantic_evals.reporting.EvaluationReport) contains:\n\n* `name`: Experiment name\n* `cases`: List of successful case evaluations\n* `failures`: List of failed executions\n* `trace_id`: OpenTelemetry trace ID (optional)\n* `span_id`: OpenTelemetry span ID (optional)", "url": "https://ai.pydantic.dev/core-concepts/index.html#report-structure", "page": "core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "ReportCase", "anchor": "reportcase", "md_text": "Each successfulcase result contains:\n\n**Case data:**\n\n* `name`: Case name\n* `inputs`: Task inputs\n* `metadata`: Case metadata (optional)\n* `expected_output`: Expected output (optional)\n* `output`: Actual output from task\n\n**Evaluation results:**\n\n* `scores`: Dictionary of numeric scores from evaluators\n* `labels`: Dictionary of categorical labels from evaluators\n* `assertions`: Dictionary of pass/fail assertions from evaluators\n\n**Performance data:**\n\n* `task_duration`: Task execution time\n* `total_duration`: Total time including evaluators\n\n**Additional data:**\n\n* `metrics`: Custom metrics dict\n* `attributes`: Custom attributes dict\n\n**Tracing:**\n\n* `trace_id`: OpenTelemetry trace ID (optional)\n* `span_id`: OpenTelemetry span ID (optional)\n\n**Errors:**\n\n* `evaluator_failures`: List of evaluator errors", "url": "https://ai.pydantic.dev/core-concepts/index.html#reportcase", "page": "core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Data Model Relationships", "anchor": "data-model-relationships", "md_text": "Here's how the core concepts relate to each other:", "url": "https://ai.pydantic.dev/core-concepts/index.html#data-model-relationships", "page": "core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Static Definition", "anchor": "static-definition", "md_text": "* A **Dataset** contains:\n* Many **Cases** (test scenarios with inputs and expected outputs)\n* Many **Evaluators** (logic for scoring outputs)", "url": "https://ai.pydantic.dev/core-concepts/index.html#static-definition", "page": "core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Execution (Experiment)", "anchor": "execution-experiment", "md_text": "When you call `dataset.evaluate(task)`, an **Experiment** runs:\n\n* The **Task** function is executed against all **Cases** in the **Dataset**\n* All **Evaluators** are run (both dataset-level and case-specific) against each output as appropriate\n* One **EvaluationReport** is produced as the final output", "url": "https://ai.pydantic.dev/core-concepts/index.html#execution-experiment", "page": "core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Results", "anchor": "results", "md_text": "* An **EvaluationReport** contains:\n* Results for each **Case** (inputs, outputs, scores, assertions, labels)\n* Summary statistics (averages, pass rates)\n* Performance data (durations)\n* Tracing information (OpenTelemetry spans)", "url": "https://ai.pydantic.dev/core-concepts/index.html#results", "page": "core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Key Relationships", "anchor": "key-relationships", "md_text": "* **One Dataset → Many Experiments**: You can run the same dataset against different task implementations or multiple times to track changes\n* **One Experiment → One Report**: Each time you call `dataset.evaluate(...)`, you get one report\n* **One Experiment → Many Case Results**: The report contains results for every case in the dataset", "url": "https://ai.pydantic.dev/core-concepts/index.html#key-relationships", "page": "core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "md_text": "* **[Evaluators Overview](../evaluators/overview/index.html)** - When to use different evaluator types\n* **[Built-in Evaluators](../evaluators/built-in/index.html)** - Complete reference of provided evaluators\n* **[Custom Evaluators](../evaluators/custom/index.html)** - Write your own evaluation logic\n* **[Dataset Management](../how-to/dataset-management/index.html)** - Save, load, and generate datasets", "url": "https://ai.pydantic.dev/core-concepts/index.html#next-steps", "page": "core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "md_text": "The beta graph API in `pydantic-graph` provides a powerful builder pattern for constructing parallel execution graphs with:\n\n* **Step nodes** for executing async functions\n* **Decision nodes** for conditional branching\n* **Spread operations** for parallel processing of iterables\n* **Broadcast operations** for sending the same data to multiple parallel paths\n* **Join nodes and Reducers** for aggregating results from parallel execution\n\nThis API is designed for advanced workflows where you want declarative control over parallelism, routing, and data aggregation.", "url": "https://ai.pydantic.dev/beta/index.html#overview", "page": "beta/index.html", "source_site": "pydantic_ai"}
{"title": "Installation", "anchor": "installation", "md_text": "The beta graph API is included with `pydantic-graph`:\n\n```\npip install pydantic-graph\n```\n\nOr as part of `pydantic-ai`:\n\n```\npip install pydantic-ai\n```", "url": "https://ai.pydantic.dev/beta/index.html#installation", "page": "beta/index.html", "source_site": "pydantic_ai"}
{"title": "Quick Start", "anchor": "quick-start", "md_text": "Here's a simple example to get you started:\n\nsimple\\_counter.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\n@dataclass\nclass CounterState:\n    \"\"\"State for tracking a counter value.\"\"\"\n\n    value: int = 0\n\n\nasync def main():\n    # Create a graph builder with state and output types\n    g = GraphBuilder(state_type=CounterState, output_type=int)\n\n    # Define steps using the decorator\n    @g.step\n    async def increment(ctx: StepContext[CounterState, None, None]) -> int:\n        \"\"\"Increment the counter and return its value.\"\"\"\n        ctx.state.value += 1\n        return ctx.state.value\n\n    @g.step\n    async def double_it(ctx: StepContext[CounterState, None, int]) -> int:\n        \"\"\"Double the input value.\"\"\"\n        return ctx.inputs * 2\n\n    # Add edges connecting the nodes\n    g.add(\n        g.edge_from(g.start_node).to(increment),\n        g.edge_from(increment).to(double_it),\n        g.edge_from(double_it).to(g.end_node),\n    )\n\n    # Build and run the graph\n    graph = g.build()\n    state = CounterState()\n    result = await graph.run(state=state)\n    print(f'Result: {result}')\n    #> Result: 2\n    print(f'Final state: {state.value}')\n    #> Final state: 1\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/index.html#quick-start", "page": "beta/index.html", "source_site": "pydantic_ai"}
{"title": "Key Concepts", "anchor": "key-concepts", "md_text": "", "url": "https://ai.pydantic.dev/beta/index.html#key-concepts", "page": "beta/index.html", "source_site": "pydantic_ai"}
{"title": "GraphBuilder", "anchor": "graphbuilder", "md_text": "The [`GraphBuilder`](../pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder) is the main entry point for constructing graphs. It's generic over:\n\n* `StateT` - The type of mutable state shared across all nodes\n* `DepsT` - The type of dependencies injected into nodes\n* `InputT` - The type of initial input to the graph\n* `OutputT` - The type of final output from the graph", "url": "https://ai.pydantic.dev/beta/index.html#graphbuilder", "page": "beta/index.html", "source_site": "pydantic_ai"}
{"title": "Steps", "anchor": "steps", "md_text": "Steps are async functions decorated with [`@g.step`](../pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.step) that define the actual work to be done in each node. They receive a [`StepContext`](../pydantic_graph/beta_step/index.html#pydantic_graph.beta.step.StepContext) with access to:\n\n* `ctx.state` - The mutable graph state\n* `ctx.deps` - Injected dependencies\n* `ctx.inputs` - Input data for this step", "url": "https://ai.pydantic.dev/beta/index.html#steps", "page": "beta/index.html", "source_site": "pydantic_ai"}
{"title": "Edges", "anchor": "edges", "md_text": "Edges define the connections between nodes. The builder provides multiple ways to create edges:\n\n* [`g.add()`](../pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.add) - Add one or more edge paths\n* [`g.add_edge()`](../pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.add_edge) - Add a simple edge between two nodes\n* [`g.edge_from()`](../pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.edge_from) - Start building a complex edge path", "url": "https://ai.pydantic.dev/beta/index.html#edges", "page": "beta/index.html", "source_site": "pydantic_ai"}
{"title": "Start and End Nodes", "anchor": "start-and-end-nodes", "md_text": "Every graph has:\n\n* [`g.start_node`](../pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.start_node) - The entry point receiving initial inputs\n* [`g.end_node`](../pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.end_node) - The exit point producing final outputs", "url": "https://ai.pydantic.dev/beta/index.html#start-and-end-nodes", "page": "beta/index.html", "source_site": "pydantic_ai"}
{"title": "A More Complex Example", "anchor": "a-more-complex-example", "md_text": "Here's an example showcasing parallel execution with a map operation:\n\nparallel\\_processing.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass ProcessingState:\n    \"\"\"State for tracking processing metrics.\"\"\"\n\n    items_processed: int = 0\n\n\nasync def main():\n    g = GraphBuilder(\n        state_type=ProcessingState,\n        input_type=list[int],\n        output_type=list[int],\n    )\n\n    @g.step\n    async def square(ctx: StepContext[ProcessingState, None, int]) -> int:\n        \"\"\"Square a number and track that we processed it.\"\"\"\n        ctx.state.items_processed += 1\n        return ctx.inputs * ctx.inputs\n\n    # Create a join to collect results\n    collect_results = g.join(reduce_list_append, initial_factory=list[int])\n\n    # Build the graph with map operation\n    g.add(\n        g.edge_from(g.start_node).map().to(square),\n        g.edge_from(square).to(collect_results),\n        g.edge_from(collect_results).to(g.end_node),\n    )\n\n    graph = g.build()\n    state = ProcessingState()\n    result = await graph.run(state=state, inputs=[1, 2, 3, 4, 5])\n\n    print(f'Results: {sorted(result)}')\n    #> Results: [1, 4, 9, 16, 25]\n    print(f'Items processed: {state.items_processed}')\n    #> Items processed: 5\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*\n\nIn this example:\n\n1. The start node receives a list of integers\n2. The `.map()` operation fans out each item to a separate parallel execution of the `square` step\n3. All results are collected back together using [`reduce_list_append`](../pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_list_append)\n4. The joined results flow to the end node", "url": "https://ai.pydantic.dev/beta/index.html#a-more-complex-example", "page": "beta/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "md_text": "Explore the detailed documentation for each feature:\n\n* [**Steps**](steps/index.html) - Learn about step nodes and execution contexts\n* [**Joins**](joins/index.html) - Understand join nodes and reducer patterns\n* [**Decisions**](decisions/index.html) - Implement conditional branching\n* [**Parallel Execution**](parallel/index.html) - Master broadcasting and mapping", "url": "https://ai.pydantic.dev/beta/index.html#next-steps", "page": "beta/index.html", "source_site": "pydantic_ai"}
{"title": "Advanced Execution Control", "anchor": "advanced-execution-control", "md_text": "Beyond the basic [`graph.run()`](../pydantic_graph/beta_graph/index.html#pydantic_graph.beta.graph.Graph.run) method, the beta API provides fine-grained control over graph execution.", "url": "https://ai.pydantic.dev/beta/index.html#advanced-execution-control", "page": "beta/index.html", "source_site": "pydantic_ai"}
{"title": "Step-by-Step Execution", "anchor": "step-by-step-execution", "md_text": "Use [`graph.iter()`](../pydantic_graph/beta_graph/index.html#pydantic_graph.beta.graph.Graph.iter) to execute the graph one step at a time:\n\nstep\\_by\\_step.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\n@dataclass\nclass CounterState:\n    value: int = 0\n\n\nasync def main():\n    g = GraphBuilder(state_type=CounterState, output_type=int)\n\n    @g.step\n    async def increment(ctx: StepContext[CounterState, None, None]) -> int:\n        ctx.state.value += 1\n        return ctx.state.value\n\n    @g.step\n    async def double_it(ctx: StepContext[CounterState, None, int]) -> int:\n        return ctx.inputs * 2\n\n    g.add(\n        g.edge_from(g.start_node).to(increment),\n        g.edge_from(increment).to(double_it),\n        g.edge_from(double_it).to(g.end_node),\n    )\n\n    graph = g.build()\n    state = CounterState()\n\n    # Use iter() for step-by-step execution\n    async with graph.iter(state=state) as graph_run:\n        print(f'Initial state: {state.value}')\n        #> Initial state: 0\n\n        # Advance execution step by step\n        async for event in graph_run:\n            print(f'{state.value=} | {event=}')\n            #> state.value=0 | event=[GraphTask(node_id='increment', inputs=None)]\n            #> state.value=1 | event=[GraphTask(node_id='double_it', inputs=1)]\n            #> state.value=1 | event=[GraphTask(node_id='__end__', inputs=2)]\n            #> state.value=1 | event=EndMarker(_value=2)\n            if graph_run.output is not None:\n                print(f'Final output: {graph_run.output}')\n                #> Final output: 2\n                break\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*\n\nThe [`GraphRun`](../pydantic_graph/beta_graph/index.html#pydantic_graph.beta.graph.GraphRun) object provides:\n\n* **Async iteration**: Iterate through execution events\n* **`next_task` property**: Inspect upcoming tasks\n* **`output` property**: Check if the graph has completed and get the final output\n* **`next()` method**: Manually advance execution with optional value injection", "url": "https://ai.pydantic.dev/beta/index.html#step-by-step-execution", "page": "beta/index.html", "source_site": "pydantic_ai"}
{"title": "Visualizing Graphs", "anchor": "visualizing-graphs", "md_text": "Generate Mermaid diagrams of your graph structure using [`graph.render()`](../pydantic_graph/beta_graph/index.html#pydantic_graph.beta.graph.Graph.render):\n\nvisualize\\_graph.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\ng = GraphBuilder(state_type=SimpleState, output_type=str)\n\n@g.step\nasync def step_a(ctx: StepContext[SimpleState, None, None]) -> int:\n    return 10\n\n@g.step\nasync def step_b(ctx: StepContext[SimpleState, None, int]) -> str:\n    return f'Result: {ctx.inputs}'\n\ng.add(\n    g.edge_from(g.start_node).to(step_a),\n    g.edge_from(step_a).to(step_b),\n    g.edge_from(step_b).to(g.end_node),\n)\n\ngraph = g.build()\n\n# Generate a Mermaid diagram\nmermaid_diagram = graph.render(title='My Graph', direction='LR')\nprint(mermaid_diagram)\n\"\"\"\n---\ntitle: My Graph\n---\nstateDiagram-v2\n  direction LR\n  step_a\n  step_b\n\n  [*] --> step_a\n  step_a --> step_b\n  step_b --> [*]\n\"\"\"\n```\n\nThe rendered diagram can be displayed in documentation, notebooks, or any tool that supports Mermaid syntax.", "url": "https://ai.pydantic.dev/beta/index.html#visualizing-graphs", "page": "beta/index.html", "source_site": "pydantic_ai"}
{"title": "Comparison with Original API", "anchor": "comparison-with-original-api", "md_text": "The original graph API (documented in the [main graph page](https://ai.pydantic.dev/graph/)) uses a class-based approach with [`BaseNode`](../pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode) subclasses. The beta API uses a builder pattern with decorated functions, which provides:\n\n**Advantages:**\n- More concise syntax for simple workflows\n- Explicit control over parallelism with map/broadcast\n- Built-in reducers for common aggregation patterns\n- Easier to visualize complex data flows\n\n**Trade-offs:**\n- Requires understanding of builder patterns\n- Less object-oriented, more functional style\n\nBoth APIs are fully supported and can even be integrated together when needed.", "url": "https://ai.pydantic.dev/beta/index.html#comparison-with-original-api", "page": "beta/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "md_text": "[rag.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/rag.py)\n\n```\n\"\"\"RAG example with pydantic-ai — using vector search to augment a chat agent.\n\nRun pgvector with:\n\n    mkdir postgres-data\n    docker run --rm -e POSTGRES_PASSWORD=postgres \\\n        -p 54320:5432 \\\n        -v `pwd`/postgres-data:/var/lib/postgresql/data \\\n        pgvector/pgvector:pg17\n\nBuild the search DB with:\n\n    uv run -m pydantic_ai_examples.rag build\n\nAsk the agent a question with:\n\n    uv run -m pydantic_ai_examples.rag search \"How do I configure logfire to work with FastAPI?\"\n\"\"\"\n\nfrom __future__ import annotations as _annotations\n\nimport asyncio\nimport re\nimport sys\nimport unicodedata\nfrom contextlib import asynccontextmanager\nfrom dataclasses import dataclass\n\nimport asyncpg\nimport httpx\nimport logfire\nimport pydantic_core\nfrom anyio import create_task_group\nfrom openai import AsyncOpenAI\nfrom pydantic import TypeAdapter\nfrom typing_extensions import AsyncGenerator\n\nfrom pydantic_ai import Agent, RunContext\n\n# 'if-token-present' means nothing will be sent (and the example will work) if you don't have logfire configured\nlogfire.configure(send_to_logfire='if-token-present')\nlogfire.instrument_asyncpg()\nlogfire.instrument_pydantic_ai()\n\n\n@dataclass\nclass Deps:\n    openai: AsyncOpenAI\n    pool: asyncpg.Pool\n\n\nagent = Agent('openai:gpt-4o', deps_type=Deps)\n\n\n@agent.tool\nasync def retrieve(context: RunContext[Deps], search_query: str) -> str:\n    \"\"\"Retrieve documentation sections based on a search query.\n\n    Args:\n        context: The call context.\n        search_query: The search query.\n    \"\"\"\n    with logfire.span(\n        'create embedding for {search_query=}', search_query=search_query\n    ):\n        embedding = await context.deps.openai.embeddings.create(\n            input=search_query,\n            model='text-embedding-3-small',\n        )\n\n    assert len(embedding.data) == 1, (\n        f'Expected 1 embedding, got {len(embedding.data)}, doc query: {search_query!r}'\n    )\n    embedding = embedding.data[0].embedding\n    embedding_json = pydantic_core.to_json(embedding).decode()\n    rows = await context.deps.pool.fetch(\n        'SELECT url, title, content FROM doc_sections ORDER BY embedding <-> $1 LIMIT 8',\n        embedding_json,\n    )\n    return '\\n\\n'.join(\n        f'# {row[\"title\"]}\\nDocumentation URL:{row[\"url\"]}\\n\\n{row[\"content\"]}\\n'\n        for row in rows\n    )\n\n\nasync def run_agent(question: str):\n    \"\"\"Entry point to run the agent and perform RAG based question answering.\"\"\"\n    openai = AsyncOpenAI()\n    logfire.instrument_openai(openai)\n\n    logfire.info('Asking \"{question}\"', question=question)\n\n    async with database_connect(False) as pool:\n        deps = Deps(openai=openai, pool=pool)\n        answer = await agent.run(question, deps=deps)\n    print(answer.output)\n\n\n#######################################################\n# The rest of this file is dedicated to preparing the #\n# search database, and some utilities.                #\n#######################################################\n\n# JSON document from\n# https://gist.github.com/samuelcolvin/4b5bb9bb163b1122ff17e29e48c10992\nDOCS_JSON = (\n    'https://gist.githubusercontent.com/'\n    'samuelcolvin/4b5bb9bb163b1122ff17e29e48c10992/raw/'\n    '80c5925c42f1442c24963aaf5eb1a324d47afe95/logfire_docs.json'\n)\n\n\nasync def build_search_db():\n    \"\"\"Build the search database.\"\"\"\n    async with httpx.AsyncClient() as client:\n        response = await client.get(DOCS_JSON)\n        response.raise_for_status()\n    sections = sessions_ta.validate_json(response.content)\n\n    openai = AsyncOpenAI()\n    logfire.instrument_openai(openai)\n\n    async with database_connect(True) as pool:\n        with logfire.span('create schema'):\n            async with pool.acquire() as conn:\n                async with conn.transaction():\n                    await conn.execute(DB_SCHEMA)\n\n        sem = asyncio.Semaphore(10)\n        async with create_task_group() as tg:\n            for section in sections:\n                tg.start_soon(insert_doc_section, sem, openai, pool, section)\n\n\nasync def insert_doc_section(\n    sem: asyncio.Semaphore,\n    openai: AsyncOpenAI,\n    pool: asyncpg.Pool,\n    section: DocsSection,\n) -> None:\n    async with sem:\n        url = section.url()\n        exists = await pool.fetchval('SELECT 1 FROM doc_sections WHERE url = $1', url)\n        if exists:\n            logfire.info('Skipping {url=}', url=url)\n            return", "url": "https://ai.pydantic.dev/rag/index.html#example-code", "page": "rag/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "md_text": "        with logfire.span('create embedding for {url=}', url=url):\n            embedding = await openai.embeddings.create(\n                input=section.embedding_content(),\n                model='text-embedding-3-small',\n            )\n        assert len(embedding.data) == 1, (\n            f'Expected 1 embedding, got {len(embedding.data)}, doc section: {section}'\n        )\n        embedding = embedding.data[0].embedding\n        embedding_json = pydantic_core.to_json(embedding).decode()\n        await pool.execute(\n            'INSERT INTO doc_sections (url, title, content, embedding) VALUES ($1, $2, $3, $4)',\n            url,\n            section.title,\n            section.content,\n            embedding_json,\n        )\n\n\n@dataclass\nclass DocsSection:\n    id: int\n    parent: int | None\n    path: str\n    level: int\n    title: str\n    content: str\n\n    def url(self) -> str:\n        url_path = re.sub(r'\\.md$', '', self.path)\n        return (\n            f'https://logfire.pydantic.dev/docs/{url_path}/#{slugify(self.title, \"-\")}'\n        )\n\n    def embedding_content(self) -> str:\n        return '\\n\\n'.join((f'path: {self.path}', f'title: {self.title}', self.content))\n\n\nsessions_ta = TypeAdapter(list[DocsSection])\n\n\n# pyright: reportUnknownMemberType=false\n# pyright: reportUnknownVariableType=false\n@asynccontextmanager\nasync def database_connect(\n    create_db: bool = False,\n) -> AsyncGenerator[asyncpg.Pool, None]:\n    server_dsn, database = (\n        'postgresql://postgres:postgres@localhost:54320',\n        'pydantic_ai_rag',\n    )\n    if create_db:\n        with logfire.span('check and create DB'):\n            conn = await asyncpg.connect(server_dsn)\n            try:\n                db_exists = await conn.fetchval(\n                    'SELECT 1 FROM pg_database WHERE datname = $1', database\n                )\n                if not db_exists:\n                    await conn.execute(f'CREATE DATABASE {database}')\n            finally:\n                await conn.close()\n\n    pool = await asyncpg.create_pool(f'{server_dsn}/{database}')\n    try:\n        yield pool\n    finally:\n        await pool.close()\n\n\nDB_SCHEMA = \"\"\"\nCREATE EXTENSION IF NOT EXISTS vector;\n\nCREATE TABLE IF NOT EXISTS doc_sections (\n    id serial PRIMARY KEY,\n    url text NOT NULL UNIQUE,\n    title text NOT NULL,\n    content text NOT NULL,\n    -- text-embedding-3-small returns a vector of 1536 floats\n    embedding vector(1536) NOT NULL\n);\nCREATE INDEX IF NOT EXISTS idx_doc_sections_embedding ON doc_sections USING hnsw (embedding vector_l2_ops);\n\"\"\"\n\n\ndef slugify(value: str, separator: str, unicode: bool = False) -> str:\n    \"\"\"Slugify a string, to make it URL friendly.\"\"\"\n    # Taken unchanged from https://github.com/Python-Markdown/markdown/blob/3.7/markdown/extensions/toc.py#L38\n    if not unicode:\n        # Replace Extended Latin characters with ASCII, i.e. `žlutý` => `zluty`\n        value = unicodedata.normalize('NFKD', value)\n        value = value.encode('ascii', 'ignore').decode('ascii')\n    value = re.sub(r'[^\\w\\s-]', '', value).strip().lower()\n    return re.sub(rf'[{separator}\\s]+', separator, value)\n\n\nif __name__ == '__main__':\n    action = sys.argv[1] if len(sys.argv) > 1 else None\n    if action == 'build':\n        asyncio.run(build_search_db())\n    elif action == 'search':\n        if len(sys.argv) == 3:\n            q = sys.argv[2]\n        else:\n            q = 'How do I configure logfire to work with FastAPI?'\n        asyncio.run(run_agent(q))\n    else:\n        print(\n            'uv run --extra examples -m pydantic_ai_examples.rag build|search',\n            file=sys.stderr,\n        )\n        sys.exit(1)\n```", "url": "https://ai.pydantic.dev/rag/index.html#example-code", "page": "rag/index.html", "source_site": "pydantic_ai"}
{"title": "format\\_as\\_xml", "anchor": "formatasxml", "md_text": "```\nformat_as_xml(\n    obj: Any,\n    root_tag: str | None = None,\n    item_tag: str = \"item\",\n    none_str: str = \"null\",\n    indent: str | None = \"  \",\n    include_field_info: Literal[\"once\"] | bool = False,\n) -> str\n```\n\nFormat a Python object as XML.\n\nThis is useful since LLMs often find it easier to read semi-structured data (e.g. examples) as XML,\nrather than JSON etc.\n\nSupports: `str`, `bytes`, `bytearray`, `bool`, `int`, `float`, `date`, `datetime`, `time`, `timedelta`, `Enum`,\n`Mapping`, `Iterable`, `dataclass`, and `BaseModel`.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `obj` | `Any` | Python Object to serialize to XML. | *required* |\n| `root_tag` | `str | None` | Outer tag to wrap the XML in, use `None` to omit the outer tag. | `None` |\n| `item_tag` | `str` | Tag to use for each item in an iterable (e.g. list), this is overridden by the class name for dataclasses and Pydantic models. | `'item'` |\n| `none_str` | `str` | String to use for `None` values. | `'null'` |\n| `indent` | `str | None` | Indentation string to use for pretty printing. | `' '` |\n| `include_field_info` | `Literal['once'] | bool` | Whether to include attributes like Pydantic `Field` attributes and dataclasses `field()` `metadata` as XML attributes. In both cases the allowed `Field` attributes and `field()` metadata keys are `title` and `description`. If a field is repeated in the data (e.g. in a list) by setting `once` the attributes are included only in the first occurrence of an XML element relative to the same field. | `False` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `str` | XML representation of the object. |\n\nExample:\n\nformat\\_as\\_xml\\_example.py\n\n```\nfrom pydantic_ai import format_as_xml\n\nprint(format_as_xml({'name': 'John', 'height': 6, 'weight': 200}, root_tag='user'))\n'''\n<user>\n  <name>John</name>\n  <height>6</height>\n  <weight>200</weight>\n</user>\n'''\n```\n\nSource code in `pydantic_ai_slim/pydantic_ai/format_prompt.py`", "url": "https://ai.pydantic.dev/format_prompt/index.html#formatasxml", "page": "format_prompt/index.html", "source_site": "pydantic_ai"}
{"title": "format\\_as\\_xml", "anchor": "formatasxml", "md_text": "|  |  |\n| --- | --- |\n| ``` 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 ``` | ``` def format_as_xml(     obj: Any,     root_tag: str | None = None,     item_tag: str = 'item',     none_str: str = 'null',     indent: str | None = '  ',     include_field_info: Literal['once'] | bool = False, ) -> str:     \"\"\"Format a Python object as XML.      This is useful since LLMs often find it easier to read semi-structured data (e.g. examples) as XML,     rather than JSON etc.      Supports: `str`, `bytes`, `bytearray`, `bool`, `int`, `float`, `date`, `datetime`, `time`, `timedelta`, `Enum`,     `Mapping`, `Iterable`, `dataclass`, and `BaseModel`.      Args:         obj: Python Object to serialize to XML.         root_tag: Outer tag to wrap the XML in, use `None` to omit the outer tag.         item_tag: Tag to use for each item in an iterable (e.g. list), this is overridden by the class name             for dataclasses and Pydantic models.         none_str: String to use for `None` values.         indent: Indentation string to use for pretty printing.         include_field_info: Whether to include attributes like Pydantic `Field` attributes and dataclasses `field()`             `metadata` as XML attributes. In both cases the allowed `Field` attributes and `field()` metadata keys are             `title` and `description`. If a field is repeated in the data (e.g. in a list) by setting `once`             the attributes are included only in the first occurrence of an XML element relative to the same field.      Returns:         XML representation of the object.      Example:     ```python {title=\"format_as_xml_example.py\" lint=\"skip\"}     from pydantic_ai import format_as_xml      print(format_as_xml({'name': 'John', 'height': 6, 'weight': 200}, root_tag='user'))     '''     <user>       <name>John</name>       <height>6</height>       <weight>200</weight>     </user>     '''     ```     \"\"\"     el = _ToXml(         data=obj,         item_tag=item_tag,         none_str=none_str,         include_field_info=include_field_info,     ).to_xml(root_tag)     if root_tag is None and el.text is None:         join = '' if indent is None else '\\n'         return join.join(_rootless_xml_elements(el, indent))     else:         if indent is not None:             ElementTree.indent(el, space=indent)         return ElementTree.tostring(el, encoding='unicode') ``` |", "url": "https://ai.pydantic.dev/format_prompt/index.html#formatasxml", "page": "format_prompt/index.html", "source_site": "pydantic_ai"}
{"title": "GraphBuilder `dataclass`", "anchor": "graphbuilder-dataclass", "md_text": "Bases: `Generic[StateT, DepsT, GraphInputT, GraphOutputT]`\n\nA builder for constructing executable graph definitions.\n\nGraphBuilder provides a fluent interface for defining nodes, edges, and\nrouting in a graph workflow. It supports typed state, dependencies, and\ninput/output validation.\n\nType Parameters\n\nStateT: The type of the graph state\nDepsT: The type of the dependencies\nGraphInputT: The type of the graph input data\nGraphOutputT: The type of the graph output data\n\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`", "url": "https://ai.pydantic.dev/pydantic_graph/beta_graph_builder/index.html#graphbuilder-dataclass", "page": "pydantic_graph/beta_graph_builder/index.html", "source_site": "pydantic_ai"}
{"title": "GraphBuilder `dataclass`", "anchor": "graphbuilder-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ```  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 ``` | ``` @dataclass(init=False) class GraphBuilder(Generic[StateT, DepsT, GraphInputT, GraphOutputT]):     \"\"\"A builder for constructing executable graph definitions.      GraphBuilder provides a fluent interface for defining nodes, edges, and     routing in a graph workflow. It supports typed state, dependencies, and     input/output validation.      Type Parameters:         StateT: The type of the graph state         DepsT: The type of the dependencies         GraphInputT: The type of the graph input data         GraphOutputT: The type of the graph output data     \"\"\"      name: str | None     \"\"\"Optional name for the graph, if not provided the name will be inferred from the calling frame on the first call to a graph method.\"\"\"      state_type: TypeOrTypeExpression[StateT]     \"\"\"The type of the graph state.\"\"\"      deps_type: TypeOrTypeExpression[DepsT]     \"\"\"The type of the dependencies.\"\"\"      input_type: TypeOrTypeExpression[GraphInputT]     \"\"\"The type of the graph input data.\"\"\"      output_type: TypeOrTypeExpression[GraphOutputT]     \"\"\"The type of the graph output data.\"\"\"      auto_instrument: bool     \"\"\"Whether to automatically create instrumentation spans.\"\"\"      _nodes: dict[NodeID, AnyNode]     \"\"\"Internal storage for nodes in the graph.\"\"\"      _edges_by_source: dict[NodeID, list[Path]]     \"\"\"Internal storage for edges by source node.\"\"\"      _decision_index: int     \"\"\"Counter for generating unique decision node IDs.\"\"\"      Source = TypeAliasType('Source', SourceNode[StateT, DepsT, OutputT], type_params=(OutputT,))     Destination = TypeAliasType('Destination', DestinationNode[StateT, DepsT, InputT], type_params=(InputT,))      def __init__(         self,         *,         name: str | None = None,         state_type: TypeOrTypeExpression[StateT] = NoneType,         deps_type: TypeOrTypeExpression[DepsT] = NoneType,         input_type: TypeOrTypeExpression[GraphInputT] = NoneType,         output_type: TypeOrTypeExpression[GraphOutputT] = NoneType,         auto_instrument: bool = True,     ):         \"\"\"Initialize a graph builder.          Args:             name: Optional name for the graph, if not provided the name will be inferred from the calling frame on the first call to a graph method.             state_type: The type of the graph state             deps_type: The type of the dependencies             input_type: The type of the graph input data             output_type: The type of the graph output data             auto_instrument: Whether to automatically create instrumentation spans         \"\"\"         self.name = name          self.state_type = state_type         self.deps_type = deps_type         self.input_type = input_type         self.output_type = output_type          self.auto_instrument = auto_instrument          self._nodes = {}         self._edges_by_source = defaultdict(list)         self._decision_index = 1          self._start_node = StartNode[GraphInputT]()         self._end_node = EndNode[GraphOutputT]()      # Node building     @property     def start_node(self) -> StartNode[GraphInputT]:         \"\"\"Get the start node for the graph.          Returns:             The start node that receives the initial graph input         \"\"\"         return self._start_node      @property     def end_node(self) -> EndNode[GraphOutputT]:         \"\"\"Get the end node for the graph.          Returns:             The end node that produces the final graph output         \"\"\"         return self._end_node      @overload     def step(         self,         *,         node_id: str | None = None,         label: str | None = None,     ) -> Callable[[StepFunction[StateT, DepsT, InputT, OutputT]], Step[StateT, DepsT, InputT, OutputT]]: ...     @overload     def step(         self,         call: StepFunction[StateT, DepsT, InputT, OutputT],         *,         node_id: str | None = None,         label: str | None = None,     ) -> Step[StateT, DepsT, InputT, OutputT]: ...     def step(         self,         call: StepFunction[StateT, DepsT, InputT, OutputT] | None = None,         *,         node_id: str | None = None,         label: str | None = None,     ) -> (         Step[StateT, DepsT, InputT, OutputT]         | Callable[[StepFunction[StateT, DepsT, InputT, OutputT]], Step[StateT, DepsT, InputT, OutputT]]     ):         \"\"\"Create a step from a step function.          This method can be used as a decorator or called directly to create         a step node from an async function.          Args:             call: The step function to wrap             node_id: Optional ID for the node             label: Optional human-readable label          Returns:             Either a Step instance or a decorator function         \"\"\"         if call is None:              def decorator(                 func: StepFunction[StateT, DepsT, InputT, OutputT],             ) -> Step[StateT, DepsT, InputT, OutputT]:                 return self.step(call=func, node_id=node_id, label=label)              return decorator          node_id = node_id or get_callable_name(call)          step = Step[StateT, DepsT, InputT, OutputT](id=NodeID(node_id), call=call, label=label)          return step      @overload     def stream(         self,         *,         node_id: str | None = None,         label: str | None = None,     ) -> Callable[         [StreamFunction[StateT, DepsT, InputT, OutputT]], Step[StateT, DepsT, InputT, AsyncIterable[OutputT]]     ]: ...     @overload     def stream(         self,         call: StreamFunction[StateT, DepsT, InputT, OutputT],         *,         node_id: str | None = None,         label: str | None = None,     ) -> Step[StateT, DepsT, InputT, AsyncIterable[OutputT]]: ...     @overload     def stream(         self,         call: StreamFunction[StateT, DepsT, InputT, OutputT] | None = None,         *,         node_id: str | None = None,         label: str | None = None,     ) -> (         Step[StateT, DepsT, InputT, AsyncIterable[OutputT]]         | Callable[             [StreamFunction[StateT, DepsT, InputT, OutputT]],             Step[StateT, DepsT, InputT, AsyncIterable[OutputT]],         ]     ): ...     def stream(         self,         call: StreamFunction[StateT, DepsT, InputT, OutputT] | None = None,         *,         node_id: str | None = None,         label: str | None = None,     ) -> (         Step[StateT, DepsT, InputT, AsyncIterable[OutputT]]         | Callable[             [StreamFunction[StateT, DepsT, InputT, OutputT]],             Step[StateT, DepsT, InputT, AsyncIterable[OutputT]],         ]     ):         \"\"\"Create a step from an async iterator (which functions like a \"stream\").          This method can be used as a decorator or called directly to create         a step node from an async function.          Args:             call: The step function to wrap             node_id: Optional ID for the node             label: Optional human-readable label          Returns:             Either a Step instance or a decorator function         \"\"\"         if call is None:              def decorator(                 func: StreamFunction[StateT, DepsT, InputT, OutputT],             ) -> Step[StateT, DepsT, InputT, AsyncIterable[OutputT]]:                 return self.stream(call=func, node_id=node_id, label=label)              return decorator          # We need to wrap the call so that we can call `await` even though the result is an async iterator         async def wrapper(ctx: StepContext[StateT, DepsT, InputT]):             return call(ctx)          return self.step(call=wrapper, node_id=node_id, label=label)      @overload     def join(         self,         reducer: ReducerFunction[StateT, DepsT, InputT, OutputT],         *,         initial: OutputT,         node_id: str | None = None,         parent_fork_id: str | None = None,         preferred_parent_fork: Literal['farthest', 'closest'] = 'farthest',     ) -> Join[StateT, DepsT, InputT, OutputT]: ...     @overload     def join(         self,         reducer: ReducerFunction[StateT, DepsT, InputT, OutputT],         *,         initial_factory: Callable[[], OutputT],         node_id: str | None = None,         parent_fork_id: str | None = None,         preferred_parent_fork: Literal['farthest', 'closest'] = 'farthest',     ) -> Join[StateT, DepsT, InputT, OutputT]: ...      def join(         self,         reducer: ReducerFunction[StateT, DepsT, InputT, OutputT],         *,         initial: OutputT | Unset = UNSET,         initial_factory: Callable[[], OutputT] | Unset = UNSET,         node_id: str | None = None,         parent_fork_id: str | None = None,         preferred_parent_fork: Literal['farthest', 'closest'] = 'farthest',     ) -> Join[StateT, DepsT, InputT, OutputT]:         if initial_factory is UNSET:             initial_factory = lambda: initial  # pyright: ignore[reportAssignmentType]  # noqa E731          return Join[StateT, DepsT, InputT, OutputT](             id=JoinID(NodeID(node_id or generate_placeholder_node_id(get_callable_name(reducer)))),             reducer=reducer,             initial_factory=cast(Callable[[], OutputT], initial_factory),             parent_fork_id=ForkID(parent_fork_id) if parent_fork_id is not None else None,             preferred_parent_fork=preferred_parent_fork,         )      # Edge building     def add(self, *edges: EdgePath[StateT, DepsT]) -> None:  # noqa C901         \"\"\"Add one or more edge paths to the graph.          This method processes edge paths and automatically creates any necessary         fork nodes for broadcasts and maps.          Args:             *edges: The edge paths to add to the graph         \"\"\"          def _handle_path(p: Path):             \"\"\"Process a path and create necessary fork nodes.              Args:                 p: The path to process             \"\"\"             for item in p.items:                 if isinstance(item, BroadcastMarker):                     new_node = Fork[Any, Any](id=item.fork_id, is_map=False, downstream_join_id=None)                     self._insert_node(new_node)                     for path in item.paths:                         _handle_path(Path(items=[*path.items]))                 elif isinstance(item, MapMarker):                     new_node = Fork[Any, Any](id=item.fork_id, is_map=True, downstream_join_id=item.downstream_join_id)                     self._insert_node(new_node)                 elif isinstance(item, DestinationMarker):                     pass          def _handle_destination_node(d: AnyDestinationNode):             if id(d) in destination_ids:                 return  # prevent infinite recursion if there is a cycle of decisions              destination_ids.add(id(d))             destinations.append(d)             self._insert_node(d)             if isinstance(d, Decision):                 for branch in d.branches:                     _handle_path(branch.path)                     for d2 in branch.destinations:                         _handle_destination_node(d2)          destination_ids = set[int]()         destinations: list[AnyDestinationNode] = []         for edge in edges:             for source_node in edge.sources:                 self._insert_node(source_node)                 self._edges_by_source[source_node.id].append(edge.path)             for destination_node in edge.destinations:                 _handle_destination_node(destination_node)             _handle_path(edge.path)          # Automatically create edges from step function return hints including `BaseNode`s         for destination in destinations:             if not isinstance(destination, Step) or isinstance(destination, NodeStep):                 continue             parent_namespace = _utils.get_parent_namespace(inspect.currentframe())             type_hints = get_type_hints(destination.call, localns=parent_namespace, include_extras=True)             try:                 return_hint = type_hints['return']             except KeyError:                 pass             else:                 edge = self._edge_from_return_hint(destination, return_hint)                 if edge is not None:                     self.add(edge)      def add_edge(self, source: Source[T], destination: Destination[T], *, label: str | None = None) -> None:         \"\"\"Add a simple edge between two nodes.          Args:             source: The source node             destination: The destination node             label: Optional label for the edge         \"\"\"         builder = self.edge_from(source)         if label is not None:             builder = builder.label(label)         self.add(builder.to(destination))      def add_mapping_edge(         self,         source: Source[Iterable[T]],         map_to: Destination[T],         *,         pre_map_label: str | None = None,         post_map_label: str | None = None,         fork_id: ForkID | None = None,         downstream_join_id: JoinID | None = None,     ) -> None:         \"\"\"Add an edge that maps iterable data across parallel paths.          Args:             source: The source node that produces iterable data             map_to: The destination node that receives individual items             pre_map_label: Optional label before the map operation             post_map_label: Optional label after the map operation             fork_id: Optional ID for the fork node produced for this map operation             downstream_join_id: Optional ID of a join node that will always be downstream of this map.                 Specifying this ensures correct handling if you try to map an empty iterable.         \"\"\"         builder = self.edge_from(source)         if pre_map_label is not None:             builder = builder.label(pre_map_label)         builder = builder.map(fork_id=fork_id, downstream_join_id=downstream_join_id)         if post_map_label is not None:             builder = builder.label(post_map_label)         self.add(builder.to(map_to))      # TODO(DavidM): Support adding subgraphs; I think this behaves like a step with the same inputs/outputs but gets rendered as a subgraph in mermaid      def edge_from(self, *sources: Source[SourceOutputT]) -> EdgePathBuilder[StateT, DepsT, SourceOutputT]:         \"\"\"Create an edge path builder starting from the given source nodes.          Args:             *sources: The source nodes to start the edge path from          Returns:             An EdgePathBuilder for constructing the complete edge path         \"\"\"         return EdgePathBuilder[StateT, DepsT, SourceOutputT](             sources=sources, path_builder=PathBuilder(working_items=[])         )      def decision(self, *, note: str | None = None, node_id: str | None = None) -> Decision[StateT, DepsT, Never]:         \"\"\"Create a new decision node.          Args:             note: Optional note to describe the decision logic             node_id: Optional ID for the node produced for this decision logic          Returns:             A new Decision node with no branches         \"\"\"         return Decision(id=NodeID(node_id or generate_placeholder_node_id('decision')), branches=[], note=note)      def match(         self,         source: TypeOrTypeExpression[SourceT],         *,         matches: Callable[[Any], bool] | None = None,     ) -> DecisionBranchBuilder[StateT, DepsT, SourceT, SourceT, Never]:         \"\"\"Create a decision branch matcher.          Args:             source: The type or type expression to match against             matches: Optional custom matching function          Returns:             A DecisionBranchBuilder for constructing the branch         \"\"\"         # Note, the following node_id really is just a placeholder and shouldn't end up in the final graph         # This is why we don't expose a way for end users to override the value used here.         node_id = NodeID(generate_placeholder_node_id('match_decision'))         decision = Decision[StateT, DepsT, Never](id=node_id, branches=[], note=None)         new_path_builder = PathBuilder[StateT, DepsT, SourceT](working_items=[])         return DecisionBranchBuilder(decision=decision, source=source, matches=matches, path_builder=new_path_builder)      def match_node(         self,         source: type[SourceNodeT],         *,         matches: Callable[[Any], bool] | None = None,     ) -> DecisionBranch[SourceNodeT]:         \"\"\"Create a decision branch for BaseNode subclasses.          This is similar to match() but specifically designed for matching         against BaseNode types from the v1 system.          Args:             source: The BaseNode subclass to match against             matches: Optional custom matching function          Returns:             A DecisionBranch for the BaseNode type         \"\"\"         node = NodeStep(source)         path = Path(items=[DestinationMarker(node.id)])         return DecisionBranch(source=source, matches=matches, path=path, destinations=[node])      def node(         self,         node_type: type[BaseNode[StateT, DepsT, GraphOutputT]],     ) -> EdgePath[StateT, DepsT]:         \"\"\"Create an edge path from a BaseNode class.          This method integrates v1-style BaseNode classes into the v2 graph         system by analyzing their type hints and creating appropriate edges.          Args:             node_type: The BaseNode subclass to integrate          Returns:             An EdgePath representing the node and its connections          Raises:             GraphSetupError: If the node type is missing required type hints         \"\"\"         parent_namespace = _utils.get_parent_namespace(inspect.currentframe())         type_hints = get_type_hints(node_type.run, localns=parent_namespace, include_extras=True)         try:             return_hint = type_hints['return']         except KeyError as e:  # pragma: no cover             raise exceptions.GraphSetupError(                 f'Node {node_type} is missing a return type hint on its `run` method'             ) from e          node = NodeStep(node_type)          edge = self._edge_from_return_hint(node, return_hint)         if not edge:  # pragma: no cover             raise exceptions.GraphSetupError(f'Node {node_type} is missing a return type hint on its `run` method')          return edge      # Helpers     def _insert_node(self, node: AnyNode) -> None:         \"\"\"Insert a node into the graph, checking for ID conflicts.          Args:             node: The node to insert          Raises:             ValueError: If a different node with the same ID already exists         \"\"\"         existing = self._nodes.get(node.id)         if existing is None:             self._nodes[node.id] = node         elif isinstance(existing, NodeStep) and isinstance(node, NodeStep) and existing.node_type is node.node_type:             pass         elif existing is not node:             raise GraphBuildingError(                 f'All nodes must have unique node IDs. {node.id!r} was the ID for {existing} and {node}'             )      def _edge_from_return_hint(         self, node: SourceNode[StateT, DepsT, Any], return_hint: TypeOrTypeExpression[Any]     ) -> EdgePath[StateT, DepsT] | None:         \"\"\"Create edges from a return type hint.          This method analyzes return type hints from step functions or node methods         to automatically create appropriate edges in the graph.          Args:             node: The source node             return_hint: The return type hint to analyze          Returns:             An EdgePath if edges can be inferred, None otherwise          Raises:             GraphSetupError: If the return type hint is invalid or incomplete         \"\"\"         destinations: list[AnyDestinationNode] = []         union_args = _utils.get_union_args(return_hint)         for return_type in union_args:             return_type, annotations = _utils.unpack_annotated(return_type)             return_type_origin = get_origin(return_type) or return_type             if return_type_origin is End:                 destinations.append(self.end_node)             elif return_type_origin is BaseNode:                 raise exceptions.GraphSetupError(  # pragma: no cover                     f'Node {node} return type hint includes a plain `BaseNode`. '                     'Edge inference requires each possible returned `BaseNode` subclass to be listed explicitly.'                 )             elif return_type_origin is StepNode:                 step = cast(                     Step[StateT, DepsT, Any, Any] | None,                     next((a for a in annotations if isinstance(a, Step)), None),  # pyright: ignore[reportUnknownArgumentType]                 )                 if step is None:                     raise exceptions.GraphSetupError(  # pragma: no cover                         f'Node {node} return type hint includes a `StepNode` without a `Step` annotation. '                         'When returning `my_step.as_node()`, use `Annotated[StepNode[StateT, DepsT], my_step]` as the return type hint.'                     )                 destinations.append(step)             elif return_type_origin is JoinNode:                 join = cast(                     Join[StateT, DepsT, Any, Any] | None,                     next((a for a in annotations if isinstance(a, Join)), None),  # pyright: ignore[reportUnknownArgumentType]                 )                 if join is None:                     raise exceptions.GraphSetupError(  # pragma: no cover                         f'Node {node} return type hint includes a `JoinNode` without a `Join` annotation. '                         'When returning `my_join.as_node()`, use `Annotated[JoinNode[StateT, DepsT], my_join]` as the return type hint.'                     )                 destinations.append(join)             elif inspect.isclass(return_type_origin) and issubclass(return_type_origin, BaseNode):                 destinations.append(NodeStep(return_type))          if len(destinations) < len(union_args):             # Only build edges if all the return types are nodes             return None          edge = self.edge_from(node)         if len(destinations) == 1:             return edge.to(destinations[0])         else:             decision = self.decision()             for destination in destinations:                 # We don't actually use this decision mechanism, but we need to build the edges for parent-fork finding                 decision = decision.branch(self.match(NoneType).to(destination))             return edge.to(decision)      # Graph building     def build(self, validate_graph_structure: bool = True) -> Graph[StateT, DepsT, GraphInputT, GraphOutputT]:         \"\"\"Build the final executable graph from the accumulated nodes and edges.          This method performs validation, normalization, and analysis of the graph         structure to create a complete, executable graph instance.          Args:             validate_graph_structure: whether to perform validation of the graph structure                 See the docstring of _validate_graph_structure below for more details.          Returns:             A complete Graph instance ready for execution          Raises:             ValueError: If the graph structure is invalid (e.g., join without parent fork)         \"\"\"         nodes = self._nodes         edges_by_source = self._edges_by_source          nodes, edges_by_source = _replace_placeholder_node_ids(nodes, edges_by_source)         nodes, edges_by_source = _flatten_paths(nodes, edges_by_source)         nodes, edges_by_source = _normalize_forks(nodes, edges_by_source)         if validate_graph_structure:             _validate_graph_structure(nodes, edges_by_source)         parent_forks = _collect_dominating_forks(nodes, edges_by_source)          return Graph[StateT, DepsT, GraphInputT, GraphOutputT](             name=self.name,             state_type=unpack_type_expression(self.state_type),             deps_type=unpack_type_expression(self.deps_type),             input_type=unpack_type_expression(self.input_type),             output_type=unpack_type_expression(self.output_type),             nodes=nodes,             edges_by_source=edges_by_source,             parent_forks=parent_forks,             auto_instrument=self.auto_instrument,         ) ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta_graph_builder/index.html#graphbuilder-dataclass", "page": "pydantic_graph/beta_graph_builder/index.html", "source_site": "pydantic_ai"}
{"title": "GraphBuilder `dataclass`", "anchor": "graphbuilder-dataclass", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    *,\n    name: str | None = None,\n    state_type: TypeOrTypeExpression[StateT] = NoneType,\n    deps_type: TypeOrTypeExpression[DepsT] = NoneType,\n    input_type: TypeOrTypeExpression[\n        GraphInputT\n    ] = NoneType,\n    output_type: TypeOrTypeExpression[\n        GraphOutputT\n    ] = NoneType,\n    auto_instrument: bool = True\n)\n```\n\nInitialize a graph builder.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `name` | `str | None` | Optional name for the graph, if not provided the name will be inferred from the calling frame on the first call to a graph method. | `None` |\n| `state_type` | `TypeOrTypeExpression[StateT]` | The type of the graph state | `NoneType` |\n| `deps_type` | `TypeOrTypeExpression[DepsT]` | The type of the dependencies | `NoneType` |\n| `input_type` | `TypeOrTypeExpression[GraphInputT]` | The type of the graph input data | `NoneType` |\n| `output_type` | `TypeOrTypeExpression[GraphOutputT]` | The type of the graph output data | `NoneType` |\n| `auto_instrument` | `bool` | Whether to automatically create instrumentation spans | `True` |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`\n\n|  |  |\n| --- | --- |\n| ``` 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 ``` | ``` def __init__(     self,     *,     name: str | None = None,     state_type: TypeOrTypeExpression[StateT] = NoneType,     deps_type: TypeOrTypeExpression[DepsT] = NoneType,     input_type: TypeOrTypeExpression[GraphInputT] = NoneType,     output_type: TypeOrTypeExpression[GraphOutputT] = NoneType,     auto_instrument: bool = True, ):     \"\"\"Initialize a graph builder.      Args:         name: Optional name for the graph, if not provided the name will be inferred from the calling frame on the first call to a graph method.         state_type: The type of the graph state         deps_type: The type of the dependencies         input_type: The type of the graph input data         output_type: The type of the graph output data         auto_instrument: Whether to automatically create instrumentation spans     \"\"\"     self.name = name      self.state_type = state_type     self.deps_type = deps_type     self.input_type = input_type     self.output_type = output_type      self.auto_instrument = auto_instrument      self._nodes = {}     self._edges_by_source = defaultdict(list)     self._decision_index = 1      self._start_node = StartNode[GraphInputT]()     self._end_node = EndNode[GraphOutputT]() ``` |\n\n#### name `instance-attribute`\n\n```\nname: str | None = name\n```\n\nOptional name for the graph, if not provided the name will be inferred from the calling frame on the first call to a graph method.\n\n#### state\\_type `instance-attribute`\n\n```\nstate_type: TypeOrTypeExpression[StateT] = state_type\n```\n\nThe type of the graph state.\n\n#### deps\\_type `instance-attribute`\n\n```\ndeps_type: TypeOrTypeExpression[DepsT] = deps_type\n```\n\nThe type of the dependencies.\n\n#### input\\_type `instance-attribute`\n\n```\ninput_type: TypeOrTypeExpression[GraphInputT] = input_type\n```\n\nThe type of the graph input data.\n\n#### output\\_type `instance-attribute`\n\n```\noutput_type: TypeOrTypeExpression[GraphOutputT] = (\n    output_type\n)\n```\n\nThe type of the graph output data.\n\n#### auto\\_instrument `instance-attribute`\n\n```\nauto_instrument: bool = auto_instrument\n```\n\nWhether to automatically create instrumentation spans.\n\n#### start\\_node `property`\n\n```\nstart_node: StartNode[GraphInputT]\n```\n\nGet the start node for the graph.\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `StartNode[GraphInputT]` | The start node that receives the initial graph input |\n\n#### end\\_node `property`\n\n```\nend_node: EndNode[GraphOutputT]\n```\n\nGet the end node for the graph.\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `EndNode[GraphOutputT]` | The end node that produces the final graph output |\n\n#### step\n\n```\nstep(\n    *, node_id: str | None = None, label: str | None = None\n) -> Callable[\n    [StepFunction[StateT, DepsT, InputT, OutputT]],\n    Step[StateT, DepsT, InputT, OutputT],\n]\n\nstep(\n    call: StepFunction[StateT, DepsT, InputT, OutputT],\n    *,\n    node_id: str | None = None,\n    label: str | None = None\n) -> Step[StateT, DepsT, InputT, OutputT]", "url": "https://ai.pydantic.dev/pydantic_graph/beta_graph_builder/index.html#graphbuilder-dataclass", "page": "pydantic_graph/beta_graph_builder/index.html", "source_site": "pydantic_ai"}
{"title": "GraphBuilder `dataclass`", "anchor": "graphbuilder-dataclass", "md_text": "step(\n    call: (\n        StepFunction[StateT, DepsT, InputT, OutputT] | None\n    ) = None,\n    *,\n    node_id: str | None = None,\n    label: str | None = None\n) -> (\n    Step[StateT, DepsT, InputT, OutputT]\n    | Callable[\n        [StepFunction[StateT, DepsT, InputT, OutputT]],\n        Step[StateT, DepsT, InputT, OutputT],\n    ]\n)\n```\n\nCreate a step from a step function.\n\nThis method can be used as a decorator or called directly to create\na step node from an async function.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `call` | `StepFunction[StateT, DepsT, InputT, OutputT] | None` | The step function to wrap | `None` |\n| `node_id` | `str | None` | Optional ID for the node | `None` |\n| `label` | `str | None` | Optional human-readable label | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `Step[StateT, DepsT, InputT, OutputT] | Callable[[StepFunction[StateT, DepsT, InputT, OutputT]], Step[StateT, DepsT, InputT, OutputT]]` | Either a Step instance or a decorator function |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`\n\n|  |  |\n| --- | --- |\n| ``` 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 ``` | ``` def step(     self,     call: StepFunction[StateT, DepsT, InputT, OutputT] | None = None,     *,     node_id: str | None = None,     label: str | None = None, ) -> (     Step[StateT, DepsT, InputT, OutputT]     | Callable[[StepFunction[StateT, DepsT, InputT, OutputT]], Step[StateT, DepsT, InputT, OutputT]] ):     \"\"\"Create a step from a step function.      This method can be used as a decorator or called directly to create     a step node from an async function.      Args:         call: The step function to wrap         node_id: Optional ID for the node         label: Optional human-readable label      Returns:         Either a Step instance or a decorator function     \"\"\"     if call is None:          def decorator(             func: StepFunction[StateT, DepsT, InputT, OutputT],         ) -> Step[StateT, DepsT, InputT, OutputT]:             return self.step(call=func, node_id=node_id, label=label)          return decorator      node_id = node_id or get_callable_name(call)      step = Step[StateT, DepsT, InputT, OutputT](id=NodeID(node_id), call=call, label=label)      return step ``` |\n\n#### stream\n\n```\nstream(\n    *, node_id: str | None = None, label: str | None = None\n) -> Callable[\n    [StreamFunction[StateT, DepsT, InputT, OutputT]],\n    Step[StateT, DepsT, InputT, AsyncIterable[OutputT]],\n]\n\nstream(\n    call: StreamFunction[StateT, DepsT, InputT, OutputT],\n    *,\n    node_id: str | None = None,\n    label: str | None = None\n) -> Step[StateT, DepsT, InputT, AsyncIterable[OutputT]]\n\nstream(\n    call: (\n        StreamFunction[StateT, DepsT, InputT, OutputT]\n        | None\n    ) = None,\n    *,\n    node_id: str | None = None,\n    label: str | None = None\n) -> (\n    Step[StateT, DepsT, InputT, AsyncIterable[OutputT]]\n    | Callable[\n        [StreamFunction[StateT, DepsT, InputT, OutputT]],\n        Step[StateT, DepsT, InputT, AsyncIterable[OutputT]],\n    ]\n)\n\nstream(\n    call: (\n        StreamFunction[StateT, DepsT, InputT, OutputT]\n        | None\n    ) = None,\n    *,\n    node_id: str | None = None,\n    label: str | None = None\n) -> (\n    Step[StateT, DepsT, InputT, AsyncIterable[OutputT]]\n    | Callable[\n        [StreamFunction[StateT, DepsT, InputT, OutputT]],\n        Step[StateT, DepsT, InputT, AsyncIterable[OutputT]],\n    ]\n)\n```\n\nCreate a step from an async iterator (which functions like a \"stream\").\n\nThis method can be used as a decorator or called directly to create\na step node from an async function.\n\nParameters:", "url": "https://ai.pydantic.dev/pydantic_graph/beta_graph_builder/index.html#graphbuilder-dataclass", "page": "pydantic_graph/beta_graph_builder/index.html", "source_site": "pydantic_ai"}
{"title": "GraphBuilder `dataclass`", "anchor": "graphbuilder-dataclass", "md_text": "| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `call` | `StreamFunction[StateT, DepsT, InputT, OutputT] | None` | The step function to wrap | `None` |\n| `node_id` | `str | None` | Optional ID for the node | `None` |\n| `label` | `str | None` | Optional human-readable label | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `Step[StateT, DepsT, InputT, AsyncIterable[OutputT]] | Callable[[StreamFunction[StateT, DepsT, InputT, OutputT]], Step[StateT, DepsT, InputT, AsyncIterable[OutputT]]]` | Either a Step instance or a decorator function |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`\n\n|  |  |\n| --- | --- |\n| ``` 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 ``` | ``` def stream(     self,     call: StreamFunction[StateT, DepsT, InputT, OutputT] | None = None,     *,     node_id: str | None = None,     label: str | None = None, ) -> (     Step[StateT, DepsT, InputT, AsyncIterable[OutputT]]     | Callable[         [StreamFunction[StateT, DepsT, InputT, OutputT]],         Step[StateT, DepsT, InputT, AsyncIterable[OutputT]],     ] ):     \"\"\"Create a step from an async iterator (which functions like a \"stream\").      This method can be used as a decorator or called directly to create     a step node from an async function.      Args:         call: The step function to wrap         node_id: Optional ID for the node         label: Optional human-readable label      Returns:         Either a Step instance or a decorator function     \"\"\"     if call is None:          def decorator(             func: StreamFunction[StateT, DepsT, InputT, OutputT],         ) -> Step[StateT, DepsT, InputT, AsyncIterable[OutputT]]:             return self.stream(call=func, node_id=node_id, label=label)          return decorator      # We need to wrap the call so that we can call `await` even though the result is an async iterator     async def wrapper(ctx: StepContext[StateT, DepsT, InputT]):         return call(ctx)      return self.step(call=wrapper, node_id=node_id, label=label) ``` |\n\n#### add\n\n```\nadd(*edges: EdgePath[StateT, DepsT]) -> None\n```\n\nAdd one or more edge paths to the graph.\n\nThis method processes edge paths and automatically creates any necessary\nfork nodes for broadcasts and maps.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `*edges` | `EdgePath[StateT, DepsT]` | The edge paths to add to the graph | `()` |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`", "url": "https://ai.pydantic.dev/pydantic_graph/beta_graph_builder/index.html#graphbuilder-dataclass", "page": "pydantic_graph/beta_graph_builder/index.html", "source_site": "pydantic_ai"}
{"title": "GraphBuilder `dataclass`", "anchor": "graphbuilder-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 ``` | ``` def add(self, *edges: EdgePath[StateT, DepsT]) -> None:  # noqa C901     \"\"\"Add one or more edge paths to the graph.      This method processes edge paths and automatically creates any necessary     fork nodes for broadcasts and maps.      Args:         *edges: The edge paths to add to the graph     \"\"\"      def _handle_path(p: Path):         \"\"\"Process a path and create necessary fork nodes.          Args:             p: The path to process         \"\"\"         for item in p.items:             if isinstance(item, BroadcastMarker):                 new_node = Fork[Any, Any](id=item.fork_id, is_map=False, downstream_join_id=None)                 self._insert_node(new_node)                 for path in item.paths:                     _handle_path(Path(items=[*path.items]))             elif isinstance(item, MapMarker):                 new_node = Fork[Any, Any](id=item.fork_id, is_map=True, downstream_join_id=item.downstream_join_id)                 self._insert_node(new_node)             elif isinstance(item, DestinationMarker):                 pass      def _handle_destination_node(d: AnyDestinationNode):         if id(d) in destination_ids:             return  # prevent infinite recursion if there is a cycle of decisions          destination_ids.add(id(d))         destinations.append(d)         self._insert_node(d)         if isinstance(d, Decision):             for branch in d.branches:                 _handle_path(branch.path)                 for d2 in branch.destinations:                     _handle_destination_node(d2)      destination_ids = set[int]()     destinations: list[AnyDestinationNode] = []     for edge in edges:         for source_node in edge.sources:             self._insert_node(source_node)             self._edges_by_source[source_node.id].append(edge.path)         for destination_node in edge.destinations:             _handle_destination_node(destination_node)         _handle_path(edge.path)      # Automatically create edges from step function return hints including `BaseNode`s     for destination in destinations:         if not isinstance(destination, Step) or isinstance(destination, NodeStep):             continue         parent_namespace = _utils.get_parent_namespace(inspect.currentframe())         type_hints = get_type_hints(destination.call, localns=parent_namespace, include_extras=True)         try:             return_hint = type_hints['return']         except KeyError:             pass         else:             edge = self._edge_from_return_hint(destination, return_hint)             if edge is not None:                 self.add(edge) ``` |\n\n#### add\\_edge\n\n```\nadd_edge(\n    source: Source[T],\n    destination: Destination[T],\n    *,\n    label: str | None = None\n) -> None\n```\n\nAdd a simple edge between two nodes.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `source` | `Source[T]` | The source node | *required* |\n| `destination` | `Destination[T]` | The destination node | *required* |\n| `label` | `str | None` | Optional label for the edge | `None` |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`\n\n|  |  |\n| --- | --- |\n| ``` 398 399 400 401 402 403 404 405 406 407 408 409 ``` | ``` def add_edge(self, source: Source[T], destination: Destination[T], *, label: str | None = None) -> None:     \"\"\"Add a simple edge between two nodes.      Args:         source: The source node         destination: The destination node         label: Optional label for the edge     \"\"\"     builder = self.edge_from(source)     if label is not None:         builder = builder.label(label)     self.add(builder.to(destination)) ``` |\n\n#### add\\_mapping\\_edge\n\n```\nadd_mapping_edge(\n    source: Source[Iterable[T]],\n    map_to: Destination[T],\n    *,\n    pre_map_label: str | None = None,\n    post_map_label: str | None = None,\n    fork_id: ForkID | None = None,\n    downstream_join_id: JoinID | None = None\n) -> None\n```\n\nAdd an edge that maps iterable data across parallel paths.\n\nParameters:", "url": "https://ai.pydantic.dev/pydantic_graph/beta_graph_builder/index.html#graphbuilder-dataclass", "page": "pydantic_graph/beta_graph_builder/index.html", "source_site": "pydantic_ai"}
{"title": "GraphBuilder `dataclass`", "anchor": "graphbuilder-dataclass", "md_text": "| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `source` | `Source[Iterable[T]]` | The source node that produces iterable data | *required* |\n| `map_to` | `Destination[T]` | The destination node that receives individual items | *required* |\n| `pre_map_label` | `str | None` | Optional label before the map operation | `None` |\n| `post_map_label` | `str | None` | Optional label after the map operation | `None` |\n| `fork_id` | `ForkID | None` | Optional ID for the fork node produced for this map operation | `None` |\n| `downstream_join_id` | `JoinID | None` | Optional ID of a join node that will always be downstream of this map. Specifying this ensures correct handling if you try to map an empty iterable. | `None` |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`\n\n|  |  |\n| --- | --- |\n| ``` 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 ``` | ``` def add_mapping_edge(     self,     source: Source[Iterable[T]],     map_to: Destination[T],     *,     pre_map_label: str | None = None,     post_map_label: str | None = None,     fork_id: ForkID | None = None,     downstream_join_id: JoinID | None = None, ) -> None:     \"\"\"Add an edge that maps iterable data across parallel paths.      Args:         source: The source node that produces iterable data         map_to: The destination node that receives individual items         pre_map_label: Optional label before the map operation         post_map_label: Optional label after the map operation         fork_id: Optional ID for the fork node produced for this map operation         downstream_join_id: Optional ID of a join node that will always be downstream of this map.             Specifying this ensures correct handling if you try to map an empty iterable.     \"\"\"     builder = self.edge_from(source)     if pre_map_label is not None:         builder = builder.label(pre_map_label)     builder = builder.map(fork_id=fork_id, downstream_join_id=downstream_join_id)     if post_map_label is not None:         builder = builder.label(post_map_label)     self.add(builder.to(map_to)) ``` |\n\n#### edge\\_from\n\n```\nedge_from(\n    *sources: Source[SourceOutputT],\n) -> EdgePathBuilder[StateT, DepsT, SourceOutputT]\n```\n\nCreate an edge path builder starting from the given source nodes.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `*sources` | `Source[SourceOutputT]` | The source nodes to start the edge path from | `()` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `EdgePathBuilder[StateT, DepsT, SourceOutputT]` | An EdgePathBuilder for constructing the complete edge path |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`\n\n|  |  |\n| --- | --- |\n| ``` 442 443 444 445 446 447 448 449 450 451 452 453 ``` | ``` def edge_from(self, *sources: Source[SourceOutputT]) -> EdgePathBuilder[StateT, DepsT, SourceOutputT]:     \"\"\"Create an edge path builder starting from the given source nodes.      Args:         *sources: The source nodes to start the edge path from      Returns:         An EdgePathBuilder for constructing the complete edge path     \"\"\"     return EdgePathBuilder[StateT, DepsT, SourceOutputT](         sources=sources, path_builder=PathBuilder(working_items=[])     ) ``` |\n\n#### decision\n\n```\ndecision(\n    *, note: str | None = None, node_id: str | None = None\n) -> Decision[StateT, DepsT, Never]\n```\n\nCreate a new decision node.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `note` | `str | None` | Optional note to describe the decision logic | `None` |\n| `node_id` | `str | None` | Optional ID for the node produced for this decision logic | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `Decision[StateT, DepsT, Never]` | A new Decision node with no branches |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`\n\n|  |  |\n| --- | --- |\n| ``` 455 456 457 458 459 460 461 462 463 464 465 ``` | ``` def decision(self, *, note: str | None = None, node_id: str | None = None) -> Decision[StateT, DepsT, Never]:     \"\"\"Create a new decision node.      Args:         note: Optional note to describe the decision logic         node_id: Optional ID for the node produced for this decision logic      Returns:         A new Decision node with no branches     \"\"\"     return Decision(id=NodeID(node_id or generate_placeholder_node_id('decision')), branches=[], note=note) ``` |\n\n#### match", "url": "https://ai.pydantic.dev/pydantic_graph/beta_graph_builder/index.html#graphbuilder-dataclass", "page": "pydantic_graph/beta_graph_builder/index.html", "source_site": "pydantic_ai"}
{"title": "GraphBuilder `dataclass`", "anchor": "graphbuilder-dataclass", "md_text": "```\nmatch(\n    source: TypeOrTypeExpression[SourceT],\n    *,\n    matches: Callable[[Any], bool] | None = None\n) -> DecisionBranchBuilder[\n    StateT, DepsT, SourceT, SourceT, Never\n]\n```\n\nCreate a decision branch matcher.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `source` | `TypeOrTypeExpression[SourceT]` | The type or type expression to match against | *required* |\n| `matches` | `Callable[[Any], bool] | None` | Optional custom matching function | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `DecisionBranchBuilder[StateT, DepsT, SourceT, SourceT, Never]` | A DecisionBranchBuilder for constructing the branch |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`\n\n|  |  |\n| --- | --- |\n| ``` 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 ``` | ``` def match(     self,     source: TypeOrTypeExpression[SourceT],     *,     matches: Callable[[Any], bool] | None = None, ) -> DecisionBranchBuilder[StateT, DepsT, SourceT, SourceT, Never]:     \"\"\"Create a decision branch matcher.      Args:         source: The type or type expression to match against         matches: Optional custom matching function      Returns:         A DecisionBranchBuilder for constructing the branch     \"\"\"     # Note, the following node_id really is just a placeholder and shouldn't end up in the final graph     # This is why we don't expose a way for end users to override the value used here.     node_id = NodeID(generate_placeholder_node_id('match_decision'))     decision = Decision[StateT, DepsT, Never](id=node_id, branches=[], note=None)     new_path_builder = PathBuilder[StateT, DepsT, SourceT](working_items=[])     return DecisionBranchBuilder(decision=decision, source=source, matches=matches, path_builder=new_path_builder) ``` |\n\n#### match\\_node\n\n```\nmatch_node(\n    source: type[SourceNodeT],\n    *,\n    matches: Callable[[Any], bool] | None = None\n) -> DecisionBranch[SourceNodeT]\n```\n\nCreate a decision branch for BaseNode subclasses.\n\nThis is similar to match() but specifically designed for matching\nagainst BaseNode types from the v1 system.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `source` | `type[SourceNodeT]` | The BaseNode subclass to match against | *required* |\n| `matches` | `Callable[[Any], bool] | None` | Optional custom matching function | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `DecisionBranch[SourceNodeT]` | A DecisionBranch for the BaseNode type |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`\n\n|  |  |\n| --- | --- |\n| ``` 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 ``` | ``` def match_node(     self,     source: type[SourceNodeT],     *,     matches: Callable[[Any], bool] | None = None, ) -> DecisionBranch[SourceNodeT]:     \"\"\"Create a decision branch for BaseNode subclasses.      This is similar to match() but specifically designed for matching     against BaseNode types from the v1 system.      Args:         source: The BaseNode subclass to match against         matches: Optional custom matching function      Returns:         A DecisionBranch for the BaseNode type     \"\"\"     node = NodeStep(source)     path = Path(items=[DestinationMarker(node.id)])     return DecisionBranch(source=source, matches=matches, path=path, destinations=[node]) ``` |\n\n#### node\n\n```\nnode(\n    node_type: type[BaseNode[StateT, DepsT, GraphOutputT]],\n) -> EdgePath[StateT, DepsT]\n```\n\nCreate an edge path from a BaseNode class.\n\nThis method integrates v1-style BaseNode classes into the v2 graph\nsystem by analyzing their type hints and creating appropriate edges.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `node_type` | `type[BaseNode[StateT, DepsT, GraphOutputT]]` | The BaseNode subclass to integrate | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `EdgePath[StateT, DepsT]` | An EdgePath representing the node and its connections |\n\nRaises:\n\n| Type | Description |\n| --- | --- |\n| `GraphSetupError` | If the node type is missing required type hints |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`", "url": "https://ai.pydantic.dev/pydantic_graph/beta_graph_builder/index.html#graphbuilder-dataclass", "page": "pydantic_graph/beta_graph_builder/index.html", "source_site": "pydantic_ai"}
{"title": "GraphBuilder `dataclass`", "anchor": "graphbuilder-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 ``` | ``` def node(     self,     node_type: type[BaseNode[StateT, DepsT, GraphOutputT]], ) -> EdgePath[StateT, DepsT]:     \"\"\"Create an edge path from a BaseNode class.      This method integrates v1-style BaseNode classes into the v2 graph     system by analyzing their type hints and creating appropriate edges.      Args:         node_type: The BaseNode subclass to integrate      Returns:         An EdgePath representing the node and its connections      Raises:         GraphSetupError: If the node type is missing required type hints     \"\"\"     parent_namespace = _utils.get_parent_namespace(inspect.currentframe())     type_hints = get_type_hints(node_type.run, localns=parent_namespace, include_extras=True)     try:         return_hint = type_hints['return']     except KeyError as e:  # pragma: no cover         raise exceptions.GraphSetupError(             f'Node {node_type} is missing a return type hint on its `run` method'         ) from e      node = NodeStep(node_type)      edge = self._edge_from_return_hint(node, return_hint)     if not edge:  # pragma: no cover         raise exceptions.GraphSetupError(f'Node {node_type} is missing a return type hint on its `run` method')      return edge ``` |\n\n#### build\n\n```\nbuild(\n    validate_graph_structure: bool = True,\n) -> Graph[StateT, DepsT, GraphInputT, GraphOutputT]\n```\n\nBuild the final executable graph from the accumulated nodes and edges.\n\nThis method performs validation, normalization, and analysis of the graph\nstructure to create a complete, executable graph instance.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `validate_graph_structure` | `bool` | whether to perform validation of the graph structure See the docstring of \\_validate\\_graph\\_structure below for more details. | `True` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `Graph[StateT, DepsT, GraphInputT, GraphOutputT]` | A complete Graph instance ready for execution |\n\nRaises:\n\n| Type | Description |\n| --- | --- |\n| `ValueError` | If the graph structure is invalid (e.g., join without parent fork) |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`\n\n|  |  |\n| --- | --- |\n| ``` 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 ``` | ``` def build(self, validate_graph_structure: bool = True) -> Graph[StateT, DepsT, GraphInputT, GraphOutputT]:     \"\"\"Build the final executable graph from the accumulated nodes and edges.      This method performs validation, normalization, and analysis of the graph     structure to create a complete, executable graph instance.      Args:         validate_graph_structure: whether to perform validation of the graph structure             See the docstring of _validate_graph_structure below for more details.      Returns:         A complete Graph instance ready for execution      Raises:         ValueError: If the graph structure is invalid (e.g., join without parent fork)     \"\"\"     nodes = self._nodes     edges_by_source = self._edges_by_source      nodes, edges_by_source = _replace_placeholder_node_ids(nodes, edges_by_source)     nodes, edges_by_source = _flatten_paths(nodes, edges_by_source)     nodes, edges_by_source = _normalize_forks(nodes, edges_by_source)     if validate_graph_structure:         _validate_graph_structure(nodes, edges_by_source)     parent_forks = _collect_dominating_forks(nodes, edges_by_source)      return Graph[StateT, DepsT, GraphInputT, GraphOutputT](         name=self.name,         state_type=unpack_type_expression(self.state_type),         deps_type=unpack_type_expression(self.deps_type),         input_type=unpack_type_expression(self.input_type),         output_type=unpack_type_expression(self.output_type),         nodes=nodes,         edges_by_source=edges_by_source,         parent_forks=parent_forks,         auto_instrument=self.auto_instrument,     ) ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta_graph_builder/index.html#graphbuilder-dataclass", "page": "pydantic_graph/beta_graph_builder/index.html", "source_site": "pydantic_ai"}
{"title": "StateT `module-attribute`", "anchor": "statet-module-attribute", "md_text": "```\nStateT = TypeVar('StateT', default=None)\n```\n\nType variable for the state in a graph.", "url": "https://ai.pydantic.dev/pydantic_graph/nodes/index.html#statet-module-attribute", "page": "pydantic_graph/nodes/index.html", "source_site": "pydantic_ai"}
{"title": "GraphRunContext `dataclass`", "anchor": "graphruncontext-dataclass", "md_text": "Bases: `Generic[StateT, DepsT]`\n\nContext for a graph.\n\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\n\n|  |  |\n| --- | --- |\n| ``` 27 28 29 30 31 32 33 34 ``` | ``` @dataclass(kw_only=True) class GraphRunContext(Generic[StateT, DepsT]):     \"\"\"Context for a graph.\"\"\"      state: StateT     \"\"\"The state of the graph.\"\"\"     deps: DepsT     \"\"\"Dependencies for the graph.\"\"\" ``` |\n\n#### state `instance-attribute`\n\n```\nstate: StateT\n```\n\nThe state of the graph.\n\n#### deps `instance-attribute`\n\n```\ndeps: DepsT\n```\n\nDependencies for the graph.", "url": "https://ai.pydantic.dev/pydantic_graph/nodes/index.html#graphruncontext-dataclass", "page": "pydantic_graph/nodes/index.html", "source_site": "pydantic_ai"}
{"title": "BaseNode", "anchor": "basenode", "md_text": "Bases: `ABC`, `Generic[StateT, DepsT, NodeRunEndT]`\n\nBase class for a node.\n\nSource code in `pydantic_graph/pydantic_graph/nodes.py`", "url": "https://ai.pydantic.dev/pydantic_graph/nodes/index.html#basenode", "page": "pydantic_graph/nodes/index.html", "source_site": "pydantic_ai"}
{"title": "BaseNode", "anchor": "basenode", "md_text": "|  |  |\n| --- | --- |\n| ```  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 ``` | ``` class BaseNode(ABC, Generic[StateT, DepsT, NodeRunEndT]):     \"\"\"Base class for a node.\"\"\"      docstring_notes: ClassVar[bool] = False     \"\"\"Set to `True` to generate mermaid diagram notes from the class's docstring.      While this can add valuable information to the diagram, it can make diagrams harder to view, hence     it is disabled by default. You can also customise notes overriding the     [`get_note`][pydantic_graph.nodes.BaseNode.get_note] method.     \"\"\"      @abstractmethod     async def run(self, ctx: GraphRunContext[StateT, DepsT]) -> BaseNode[StateT, DepsT, Any] | End[NodeRunEndT]:         \"\"\"Run the node.          This is an abstract method that must be implemented by subclasses.          !!! note \"Return types used at runtime\"             The return type of this method are read by `pydantic_graph` at runtime and used to define which             nodes can be called next in the graph. This is displayed in [mermaid diagrams](mermaid.md)             and enforced when running the graph.          Args:             ctx: The graph context.          Returns:             The next node to run or [`End`][pydantic_graph.nodes.End] to signal the end of the graph.         \"\"\"         ...      def get_snapshot_id(self) -> str:         if snapshot_id := getattr(self, '__snapshot_id', None):             return snapshot_id         else:             self.__dict__['__snapshot_id'] = snapshot_id = generate_snapshot_id(self.get_node_id())             return snapshot_id      def set_snapshot_id(self, snapshot_id: str) -> None:         self.__dict__['__snapshot_id'] = snapshot_id      @classmethod     @cache     def get_node_id(cls) -> str:         \"\"\"Get the ID of the node.\"\"\"         return cls.__name__      @classmethod     def get_note(cls) -> str | None:         \"\"\"Get a note about the node to render on mermaid charts.          By default, this returns a note only if [`docstring_notes`][pydantic_graph.nodes.BaseNode.docstring_notes]         is `True`. You can override this method to customise the node notes.         \"\"\"         if not cls.docstring_notes:             return None         docstring = cls.__doc__         # dataclasses get an automatic docstring which is just their signature, we don't want that         if docstring and is_dataclass(cls) and docstring.startswith(f'{cls.__name__}('):             docstring = None  # pragma: no cover         if docstring:  # pragma: no branch             # remove indentation from docstring             import inspect              docstring = inspect.cleandoc(docstring)         return docstring      @classmethod     def get_node_def(cls, local_ns: dict[str, Any] | None) -> NodeDef[StateT, DepsT, NodeRunEndT]:         \"\"\"Get the node definition.\"\"\"         type_hints = get_type_hints(cls.run, localns=local_ns, include_extras=True)         try:             return_hint = type_hints['return']         except KeyError as e:             raise exceptions.GraphSetupError(f'Node {cls} is missing a return type hint on its `run` method') from e          next_node_edges: dict[str, Edge] = {}         end_edge: Edge | None = None         returns_base_node: bool = False         for return_type in _utils.get_union_args(return_hint):             return_type, annotations = _utils.unpack_annotated(return_type)             edge = next((a for a in annotations if isinstance(a, Edge)), Edge(None))             return_type_origin = get_origin(return_type) or return_type             if return_type_origin is End:                 end_edge = edge             elif return_type_origin is BaseNode:                 returns_base_node = True             elif issubclass(return_type_origin, BaseNode):                 next_node_edges[return_type.get_node_id()] = edge             else:                 raise exceptions.GraphSetupError(f'Invalid return type: {return_type}')          return NodeDef(             node=cls,             node_id=cls.get_node_id(),             note=cls.get_note(),             next_node_edges=next_node_edges,             end_edge=end_edge,             returns_base_node=returns_base_node,         )      def deep_copy(self) -> Self:         \"\"\"Returns a deep copy of the node.\"\"\"         return copy.deepcopy(self) ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/nodes/index.html#basenode", "page": "pydantic_graph/nodes/index.html", "source_site": "pydantic_ai"}
{"title": "BaseNode", "anchor": "basenode", "md_text": "#### docstring\\_notes `class-attribute`\n\n```\ndocstring_notes: bool = False\n```\n\nSet to `True` to generate mermaid diagram notes from the class's docstring.\n\nWhile this can add valuable information to the diagram, it can make diagrams harder to view, hence\nit is disabled by default. You can also customise notes overriding the\n[`get_note`](index.html#pydantic_graph.nodes.BaseNode.get_note) method.\n\n#### run `abstractmethod` `async`\n\n```\nrun(\n    ctx: GraphRunContext[StateT, DepsT],\n) -> BaseNode[StateT, DepsT, Any] | End[NodeRunEndT]\n```\n\nRun the node.\n\nThis is an abstract method that must be implemented by subclasses.\n\nThe return type of this method are read by `pydantic_graph` at runtime and used to define which\nnodes can be called next in the graph. This is displayed in [mermaid diagrams](../mermaid/index.html)\nand enforced when running the graph.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `ctx` | `GraphRunContext[StateT, DepsT]` | The graph context. | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `BaseNode[StateT, DepsT, Any] | End[NodeRunEndT]` | The next node to run or [`End`](index.html#pydantic_graph.nodes.End) to signal the end of the graph. |\n\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\n\n|  |  |\n| --- | --- |\n| ``` 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 ``` | ``` @abstractmethod async def run(self, ctx: GraphRunContext[StateT, DepsT]) -> BaseNode[StateT, DepsT, Any] | End[NodeRunEndT]:     \"\"\"Run the node.      This is an abstract method that must be implemented by subclasses.      !!! note \"Return types used at runtime\"         The return type of this method are read by `pydantic_graph` at runtime and used to define which         nodes can be called next in the graph. This is displayed in [mermaid diagrams](mermaid.md)         and enforced when running the graph.      Args:         ctx: The graph context.      Returns:         The next node to run or [`End`][pydantic_graph.nodes.End] to signal the end of the graph.     \"\"\"     ... ``` |\n\n#### get\\_node\\_id `cached` `classmethod`\n\n```\nget_node_id() -> str\n```\n\nGet the ID of the node.\n\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\n\n|  |  |\n| --- | --- |\n| ``` 77 78 79 80 81 ``` | ``` @classmethod @cache def get_node_id(cls) -> str:     \"\"\"Get the ID of the node.\"\"\"     return cls.__name__ ``` |\n\n#### get\\_note `classmethod`\n\n```\nget_note() -> str | None\n```\n\nGet a note about the node to render on mermaid charts.\n\nBy default, this returns a note only if [`docstring_notes`](index.html#pydantic_graph.nodes.BaseNode.docstring_notes)\nis `True`. You can override this method to customise the node notes.\n\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\n\n|  |  |\n| --- | --- |\n| ```  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 ``` | ``` @classmethod def get_note(cls) -> str | None:     \"\"\"Get a note about the node to render on mermaid charts.      By default, this returns a note only if [`docstring_notes`][pydantic_graph.nodes.BaseNode.docstring_notes]     is `True`. You can override this method to customise the node notes.     \"\"\"     if not cls.docstring_notes:         return None     docstring = cls.__doc__     # dataclasses get an automatic docstring which is just their signature, we don't want that     if docstring and is_dataclass(cls) and docstring.startswith(f'{cls.__name__}('):         docstring = None  # pragma: no cover     if docstring:  # pragma: no branch         # remove indentation from docstring         import inspect          docstring = inspect.cleandoc(docstring)     return docstring ``` |\n\n#### get\\_node\\_def `classmethod`\n\n```\nget_node_def(\n    local_ns: dict[str, Any] | None,\n) -> NodeDef[StateT, DepsT, NodeRunEndT]\n```\n\nGet the node definition.\n\nSource code in `pydantic_graph/pydantic_graph/nodes.py`", "url": "https://ai.pydantic.dev/pydantic_graph/nodes/index.html#basenode", "page": "pydantic_graph/nodes/index.html", "source_site": "pydantic_ai"}
{"title": "BaseNode", "anchor": "basenode", "md_text": "|  |  |\n| --- | --- |\n| ``` 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 ``` | ``` @classmethod def get_node_def(cls, local_ns: dict[str, Any] | None) -> NodeDef[StateT, DepsT, NodeRunEndT]:     \"\"\"Get the node definition.\"\"\"     type_hints = get_type_hints(cls.run, localns=local_ns, include_extras=True)     try:         return_hint = type_hints['return']     except KeyError as e:         raise exceptions.GraphSetupError(f'Node {cls} is missing a return type hint on its `run` method') from e      next_node_edges: dict[str, Edge] = {}     end_edge: Edge | None = None     returns_base_node: bool = False     for return_type in _utils.get_union_args(return_hint):         return_type, annotations = _utils.unpack_annotated(return_type)         edge = next((a for a in annotations if isinstance(a, Edge)), Edge(None))         return_type_origin = get_origin(return_type) or return_type         if return_type_origin is End:             end_edge = edge         elif return_type_origin is BaseNode:             returns_base_node = True         elif issubclass(return_type_origin, BaseNode):             next_node_edges[return_type.get_node_id()] = edge         else:             raise exceptions.GraphSetupError(f'Invalid return type: {return_type}')      return NodeDef(         node=cls,         node_id=cls.get_node_id(),         note=cls.get_note(),         next_node_edges=next_node_edges,         end_edge=end_edge,         returns_base_node=returns_base_node,     ) ``` |\n\n#### deep\\_copy\n\n```\ndeep_copy() -> Self\n```\n\nReturns a deep copy of the node.\n\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\n\n|  |  |\n| --- | --- |\n| ``` 137 138 139 ``` | ``` def deep_copy(self) -> Self:     \"\"\"Returns a deep copy of the node.\"\"\"     return copy.deepcopy(self) ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/nodes/index.html#basenode", "page": "pydantic_graph/nodes/index.html", "source_site": "pydantic_ai"}
{"title": "End `dataclass`", "anchor": "end-dataclass", "md_text": "Bases: `Generic[RunEndT]`\n\nType to return from a node to signal the end of the graph.\n\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\n\n|  |  |\n| --- | --- |\n| ``` 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 ``` | ``` @dataclass class End(Generic[RunEndT]):     \"\"\"Type to return from a node to signal the end of the graph.\"\"\"      data: RunEndT     \"\"\"Data to return from the graph.\"\"\"      def deep_copy_data(self) -> End[RunEndT]:         \"\"\"Returns a deep copy of the end of the run.\"\"\"         if self.data is None:             return self         else:             end = End(copy.deepcopy(self.data))             end.set_snapshot_id(self.get_snapshot_id())             return end      def get_snapshot_id(self) -> str:         if snapshot_id := getattr(self, '__snapshot_id', None):             return snapshot_id         else:             self.__dict__['__snapshot_id'] = snapshot_id = generate_snapshot_id('end')             return snapshot_id      def set_snapshot_id(self, set_id: str) -> None:         self.__dict__['__snapshot_id'] = set_id ``` |\n\n#### data `instance-attribute`\n\n```\ndata: RunEndT\n```\n\nData to return from the graph.\n\n#### deep\\_copy\\_data\n\n```\ndeep_copy_data() -> End[RunEndT]\n```\n\nReturns a deep copy of the end of the run.\n\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\n\n|  |  |\n| --- | --- |\n| ``` 149 150 151 152 153 154 155 156 ``` | ``` def deep_copy_data(self) -> End[RunEndT]:     \"\"\"Returns a deep copy of the end of the run.\"\"\"     if self.data is None:         return self     else:         end = End(copy.deepcopy(self.data))         end.set_snapshot_id(self.get_snapshot_id())         return end ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/nodes/index.html#end-dataclass", "page": "pydantic_graph/nodes/index.html", "source_site": "pydantic_ai"}
{"title": "Edge `dataclass`", "anchor": "edge-dataclass", "md_text": "Annotation to apply a label to an edge in a graph.\n\nSource code in `pydantic_graph/pydantic_graph/nodes.py`\n\n|  |  |\n| --- | --- |\n| ``` 174 175 176 177 178 179 ``` | ``` @dataclass(frozen=True) class Edge:     \"\"\"Annotation to apply a label to an edge in a graph.\"\"\"      label: str | None     \"\"\"Label for the edge.\"\"\" ``` |\n\n#### label `instance-attribute`\n\n```\nlabel: str | None\n```\n\nLabel for the edge.", "url": "https://ai.pydantic.dev/pydantic_graph/nodes/index.html#edge-dataclass", "page": "pydantic_graph/nodes/index.html", "source_site": "pydantic_ai"}
{"title": "DepsT `module-attribute`", "anchor": "depst-module-attribute", "md_text": "```\nDepsT = TypeVar('DepsT', default=None, contravariant=True)\n```\n\nType variable for the dependencies of a graph and node.", "url": "https://ai.pydantic.dev/pydantic_graph/nodes/index.html#depst-module-attribute", "page": "pydantic_graph/nodes/index.html", "source_site": "pydantic_ai"}
{"title": "RunEndT `module-attribute`", "anchor": "runendt-module-attribute", "md_text": "```\nRunEndT = TypeVar('RunEndT', covariant=True, default=None)\n```\n\nCovariant type variable for the return type of a graph [`run`](../graph/index.html#pydantic_graph.graph.Graph.run).", "url": "https://ai.pydantic.dev/pydantic_graph/nodes/index.html#runendt-module-attribute", "page": "pydantic_graph/nodes/index.html", "source_site": "pydantic_ai"}
{"title": "NodeRunEndT `module-attribute`", "anchor": "noderunendt-module-attribute", "md_text": "```\nNodeRunEndT = TypeVar(\n    \"NodeRunEndT\", covariant=True, default=Never\n)\n```\n\nCovariant type variable for the return type of a node [`run`](index.html#pydantic_graph.nodes.BaseNode.run).", "url": "https://ai.pydantic.dev/pydantic_graph/nodes/index.html#noderunendt-module-attribute", "page": "pydantic_graph/nodes/index.html", "source_site": "pydantic_ai"}
{"title": "StateT `module-attribute`", "anchor": "statet-module-attribute", "md_text": "```\nStateT = TypeVar('StateT', infer_variance=True)\n```\n\nType variable for graph state.", "url": "https://ai.pydantic.dev/pydantic_graph/beta_decision/index.html#statet-module-attribute", "page": "pydantic_graph/beta_decision/index.html", "source_site": "pydantic_ai"}
{"title": "DepsT `module-attribute`", "anchor": "depst-module-attribute", "md_text": "```\nDepsT = TypeVar('DepsT', infer_variance=True)\n```\n\nType variable for graph dependencies.", "url": "https://ai.pydantic.dev/pydantic_graph/beta_decision/index.html#depst-module-attribute", "page": "pydantic_graph/beta_decision/index.html", "source_site": "pydantic_ai"}
{"title": "HandledT `module-attribute`", "anchor": "handledt-module-attribute", "md_text": "```\nHandledT = TypeVar('HandledT', infer_variance=True)\n```\n\nType variable used to track types handled by the branches of a Decision.", "url": "https://ai.pydantic.dev/pydantic_graph/beta_decision/index.html#handledt-module-attribute", "page": "pydantic_graph/beta_decision/index.html", "source_site": "pydantic_ai"}
{"title": "T `module-attribute`", "anchor": "t-module-attribute", "md_text": "```\nT = TypeVar('T', infer_variance=True)\n```\n\nGeneric type variable.", "url": "https://ai.pydantic.dev/pydantic_graph/beta_decision/index.html#t-module-attribute", "page": "pydantic_graph/beta_decision/index.html", "source_site": "pydantic_ai"}
{"title": "Decision `dataclass`", "anchor": "decision-dataclass", "md_text": "Bases: `Generic[StateT, DepsT, HandledT]`\n\nDecision node for conditional branching in graph execution.\n\nA Decision node evaluates conditions and routes execution to different\nbranches based on the input data type or custom matching logic.\n\nSource code in `pydantic_graph/pydantic_graph/beta/decision.py`\n\n|  |  |\n| --- | --- |\n| ``` 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 ``` | ``` @dataclass(kw_only=True) class Decision(Generic[StateT, DepsT, HandledT]):     \"\"\"Decision node for conditional branching in graph execution.      A Decision node evaluates conditions and routes execution to different     branches based on the input data type or custom matching logic.     \"\"\"      id: NodeID     \"\"\"Unique identifier for this decision node.\"\"\"      branches: list[DecisionBranch[Any]]     \"\"\"List of branches that can be taken from this decision.\"\"\"      note: str | None     \"\"\"Optional documentation note for this decision.\"\"\"      def branch(self, branch: DecisionBranch[T]) -> Decision[StateT, DepsT, HandledT | T]:         \"\"\"Add a new branch to this decision.          Args:             branch: The branch to add to this decision.          Returns:             A new Decision with the additional branch.         \"\"\"         return Decision(id=self.id, branches=self.branches + [branch], note=self.note)      def _force_handled_contravariant(self, inputs: HandledT) -> Never:  # pragma: no cover         \"\"\"Forces this type to be contravariant in the HandledT type variable.          This is an implementation detail of how we can type-check that all possible input types have         been exhaustively covered.          Args:             inputs: Input data of handled types.          Raises:             RuntimeError: Always, as this method should never be executed.         \"\"\"         raise RuntimeError('This method should never be called, it is just defined for typing purposes.') ``` |\n\n#### id `instance-attribute`\n\n```\nid: NodeID\n```\n\nUnique identifier for this decision node.\n\n#### branches `instance-attribute`\n\n```\nbranches: list[DecisionBranch[Any]]\n```\n\nList of branches that can be taken from this decision.\n\n#### note `instance-attribute`\n\n```\nnote: str | None\n```\n\nOptional documentation note for this decision.\n\n#### branch\n\n```\nbranch(\n    branch: DecisionBranch[T],\n) -> Decision[StateT, DepsT, HandledT | T]\n```\n\nAdd a new branch to this decision.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `branch` | `DecisionBranch[T]` | The branch to add to this decision. | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `Decision[StateT, DepsT, HandledT | T]` | A new Decision with the additional branch. |\n\nSource code in `pydantic_graph/pydantic_graph/beta/decision.py`\n\n|  |  |\n| --- | --- |\n| ``` 57 58 59 60 61 62 63 64 65 66 ``` | ``` def branch(self, branch: DecisionBranch[T]) -> Decision[StateT, DepsT, HandledT | T]:     \"\"\"Add a new branch to this decision.      Args:         branch: The branch to add to this decision.      Returns:         A new Decision with the additional branch.     \"\"\"     return Decision(id=self.id, branches=self.branches + [branch], note=self.note) ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta_decision/index.html#decision-dataclass", "page": "pydantic_graph/beta_decision/index.html", "source_site": "pydantic_ai"}
{"title": "SourceT `module-attribute`", "anchor": "sourcet-module-attribute", "md_text": "```\nSourceT = TypeVar('SourceT', infer_variance=True)\n```\n\nType variable for source data for a DecisionBranch.", "url": "https://ai.pydantic.dev/pydantic_graph/beta_decision/index.html#sourcet-module-attribute", "page": "pydantic_graph/beta_decision/index.html", "source_site": "pydantic_ai"}
{"title": "DecisionBranch `dataclass`", "anchor": "decisionbranch-dataclass", "md_text": "Bases: `Generic[SourceT]`\n\nRepresents a single branch within a decision node.\n\nEach branch defines the conditions under which it should be taken\nand the path to follow when those conditions are met.\n\nNote: with the current design, it is actually *critical* that this class is invariant in SourceT for the sake\nof type-checking that inputs to a Decision are actually handled. See the `# type: ignore` comment in\n`tests.graph.beta.test_graph_edge_cases.test_decision_no_matching_branch` for an example of how this works.\n\nSource code in `pydantic_graph/pydantic_graph/beta/decision.py`\n\n|  |  |\n| --- | --- |\n| ```  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 ``` | ``` @dataclass class DecisionBranch(Generic[SourceT]):     \"\"\"Represents a single branch within a decision node.      Each branch defines the conditions under which it should be taken     and the path to follow when those conditions are met.      Note: with the current design, it is actually _critical_ that this class is invariant in SourceT for the sake     of type-checking that inputs to a Decision are actually handled. See the `# type: ignore` comment in     `tests.graph.beta.test_graph_edge_cases.test_decision_no_matching_branch` for an example of how this works.     \"\"\"      source: TypeOrTypeExpression[SourceT]     \"\"\"The expected type of data for this branch.      This is necessary for exhaustiveness-checking when handling the inputs to a decision node.\"\"\"      matches: Callable[[Any], bool] | None     \"\"\"An optional predicate function used to determine whether input data matches this branch.      If `None`, default logic is used which attempts to check the value for type-compatibility with the `source` type:     * If `source` is `Any` or `object`, the branch will always match     * If `source` is a `Literal` type, this branch will match if the value is one of the parametrizing literal values     * If `source` is any other type, the value will be checked for matching using `isinstance`      Inputs are tested against each branch of a decision node in order, and the path of the first matching branch is     used to handle the input value.     \"\"\"      path: Path     \"\"\"The execution path to follow when an input value matches this branch of a decision node.      This can include transforming, mapping, and broadcasting the output before sending to the next node or nodes.      The path can also include position-aware labels which are used when generating mermaid diagrams.\"\"\"      destinations: list[AnyDestinationNode]     \"\"\"The destination nodes that can be referenced by DestinationMarker in the path.\"\"\" ``` |\n\n#### source `instance-attribute`\n\n```\nsource: TypeOrTypeExpression[SourceT]\n```\n\nThe expected type of data for this branch.\n\nThis is necessary for exhaustiveness-checking when handling the inputs to a decision node.\n\n#### matches `instance-attribute`\n\n```\nmatches: Callable[[Any], bool] | None\n```\n\nAn optional predicate function used to determine whether input data matches this branch.\n\nIf `None`, default logic is used which attempts to check the value for type-compatibility with the `source` type:\n\\* If `source` is `Any` or `object`, the branch will always match\n\\* If `source` is a `Literal` type, this branch will match if the value is one of the parametrizing literal values\n\\* If `source` is any other type, the value will be checked for matching using `isinstance`\n\nInputs are tested against each branch of a decision node in order, and the path of the first matching branch is\nused to handle the input value.\n\n#### path `instance-attribute`\n\n```\npath: Path\n```\n\nThe execution path to follow when an input value matches this branch of a decision node.\n\nThis can include transforming, mapping, and broadcasting the output before sending to the next node or nodes.\n\nThe path can also include position-aware labels which are used when generating mermaid diagrams.\n\n#### destinations `instance-attribute`\n\n```\ndestinations: list[AnyDestinationNode]\n```\n\nThe destination nodes that can be referenced by DestinationMarker in the path.", "url": "https://ai.pydantic.dev/pydantic_graph/beta_decision/index.html#decisionbranch-dataclass", "page": "pydantic_graph/beta_decision/index.html", "source_site": "pydantic_ai"}
{"title": "OutputT `module-attribute`", "anchor": "outputt-module-attribute", "md_text": "```\nOutputT = TypeVar('OutputT', infer_variance=True)\n```\n\nType variable for the output data of a node.", "url": "https://ai.pydantic.dev/pydantic_graph/beta_decision/index.html#outputt-module-attribute", "page": "pydantic_graph/beta_decision/index.html", "source_site": "pydantic_ai"}
{"title": "NewOutputT `module-attribute`", "anchor": "newoutputt-module-attribute", "md_text": "```\nNewOutputT = TypeVar('NewOutputT', infer_variance=True)\n```\n\nType variable for transformed output.", "url": "https://ai.pydantic.dev/pydantic_graph/beta_decision/index.html#newoutputt-module-attribute", "page": "pydantic_graph/beta_decision/index.html", "source_site": "pydantic_ai"}
{"title": "DecisionBranchBuilder `dataclass`", "anchor": "decisionbranchbuilder-dataclass", "md_text": "Bases: `Generic[StateT, DepsT, OutputT, SourceT, HandledT]`\n\nBuilder for constructing decision branches with fluent API.\n\nThis builder provides methods to configure branches with destinations,\nforks, and transformations in a type-safe manner.\n\nInstances of this class should be created using [`GraphBuilder.match`](../beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder),\nnot created directly.\n\nSource code in `pydantic_graph/pydantic_graph/beta/decision.py`", "url": "https://ai.pydantic.dev/pydantic_graph/beta_decision/index.html#decisionbranchbuilder-dataclass", "page": "pydantic_graph/beta_decision/index.html", "source_site": "pydantic_ai"}
{"title": "DecisionBranchBuilder `dataclass`", "anchor": "decisionbranchbuilder-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 ``` | ``` @dataclass(init=False) class DecisionBranchBuilder(Generic[StateT, DepsT, OutputT, SourceT, HandledT]):     \"\"\"Builder for constructing decision branches with fluent API.      This builder provides methods to configure branches with destinations,     forks, and transformations in a type-safe manner.      Instances of this class should be created using [`GraphBuilder.match`][pydantic_graph.beta.graph_builder.GraphBuilder],     not created directly.     \"\"\"      _decision: Decision[StateT, DepsT, HandledT]     \"\"\"The parent decision node.\"\"\"     _source: TypeOrTypeExpression[SourceT]     \"\"\"The expected source type for this branch.\"\"\"     _matches: Callable[[Any], bool] | None     \"\"\"Optional matching predicate.\"\"\"      _path_builder: PathBuilder[StateT, DepsT, OutputT]     \"\"\"Builder for the execution path.\"\"\"      def __init__(         self,         *,         decision: Decision[StateT, DepsT, HandledT],         source: TypeOrTypeExpression[SourceT],         matches: Callable[[Any], bool] | None,         path_builder: PathBuilder[StateT, DepsT, OutputT],     ):         # This manually-defined initializer is necessary due to https://github.com/python/mypy/issues/17623.         self._decision = decision         self._source = source         self._matches = matches         self._path_builder = path_builder      def to(         self,         destination: DestinationNode[StateT, DepsT, OutputT] | type[BaseNode[StateT, DepsT, Any]],         /,         *extra_destinations: DestinationNode[StateT, DepsT, OutputT] | type[BaseNode[StateT, DepsT, Any]],         fork_id: str | None = None,     ) -> DecisionBranch[SourceT]:         \"\"\"Set the destination(s) for this branch.          Args:             destination: The primary destination node.             *extra_destinations: Additional destination nodes.             fork_id: Optional node ID to use for the resulting broadcast fork if multiple destinations are provided.          Returns:             A completed DecisionBranch with the specified destinations.         \"\"\"         destination = get_origin(destination) or destination         extra_destinations = tuple(get_origin(d) or d for d in extra_destinations)         destinations = [(NodeStep(d) if inspect.isclass(d) else d) for d in (destination, *extra_destinations)]         return DecisionBranch(             source=self._source,             matches=self._matches,             path=self._path_builder.to(*destinations, fork_id=fork_id),             destinations=destinations,         )      def broadcast(         self, get_forks: Callable[[Self], Sequence[DecisionBranch[SourceT]]], /, *, fork_id: str | None = None     ) -> DecisionBranch[SourceT]:         \"\"\"Broadcast this decision branch into multiple destinations.          Args:             get_forks: The callback that will return a sequence of decision branches to broadcast to.             fork_id: Optional node ID to use for the resulting broadcast fork.          Returns:             A completed DecisionBranch with the specified destinations.         \"\"\"         fork_decision_branches = get_forks(self)         new_paths = [b.path for b in fork_decision_branches]         if not new_paths:             raise GraphBuildingError(f'The call to {get_forks} returned no branches, but must return at least one.')         path = self._path_builder.broadcast(new_paths, fork_id=fork_id)         destinations = [d for fdp in fork_decision_branches for d in fdp.destinations]         return DecisionBranch(source=self._source, matches=self._matches, path=path, destinations=destinations)      def transform(         self, func: TransformFunction[StateT, DepsT, OutputT, NewOutputT], /     ) -> DecisionBranchBuilder[StateT, DepsT, NewOutputT, SourceT, HandledT]:         \"\"\"Apply a transformation to the branch's output.          Args:             func: Transformation function to apply.          Returns:             A new DecisionBranchBuilder where the provided transform is applied prior to generating the final output.         \"\"\"         return DecisionBranchBuilder(             decision=self._decision,             source=self._source,             matches=self._matches,             path_builder=self._path_builder.transform(func),         )      def map(         self: DecisionBranchBuilder[StateT, DepsT, Iterable[T], SourceT, HandledT]         | DecisionBranchBuilder[StateT, DepsT, AsyncIterable[T], SourceT, HandledT],         *,         fork_id: str | None = None,         downstream_join_id: str | None = None,     ) -> DecisionBranchBuilder[StateT, DepsT, T, SourceT, HandledT]:         \"\"\"Spread the branch's output.          To do this, the current output must be iterable, and any subsequent steps in the path being built for this         branch will be applied to each item of the current output in parallel.          Args:             fork_id: Optional ID for the fork, defaults to a generated value             downstream_join_id: Optional ID of a downstream join node which is involved when mapping empty iterables          Returns:             A new DecisionBranchBuilder where mapping is performed prior to generating the final output.         \"\"\"         return DecisionBranchBuilder(             decision=self._decision,             source=self._source,             matches=self._matches,             path_builder=self._path_builder.map(fork_id=fork_id, downstream_join_id=downstream_join_id),         )      def label(self, label: str) -> DecisionBranchBuilder[StateT, DepsT, OutputT, SourceT, HandledT]:         \"\"\"Apply a label to the branch at the current point in the path being built.          These labels are only used in generated mermaid diagrams.          Args:             label: The label to apply.          Returns:             A new DecisionBranchBuilder where the label has been applied at the end of the current path being built.         \"\"\"         return DecisionBranchBuilder(             decision=self._decision,             source=self._source,             matches=self._matches,             path_builder=self._path_builder.label(label),         ) ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta_decision/index.html#decisionbranchbuilder-dataclass", "page": "pydantic_graph/beta_decision/index.html", "source_site": "pydantic_ai"}
{"title": "DecisionBranchBuilder `dataclass`", "anchor": "decisionbranchbuilder-dataclass", "md_text": "#### to\n\n```\nto(\n    destination: (\n        DestinationNode[StateT, DepsT, OutputT]\n        | type[BaseNode[StateT, DepsT, Any]]\n    ),\n    /,\n    *extra_destinations: DestinationNode[\n        StateT, DepsT, OutputT\n    ]\n    | type[BaseNode[StateT, DepsT, Any]],\n    fork_id: str | None = None,\n) -> DecisionBranch[SourceT]\n```\n\nSet the destination(s) for this branch.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `destination` | `DestinationNode[StateT, DepsT, OutputT] | type[BaseNode[StateT, DepsT, Any]]` | The primary destination node. | *required* |\n| `*extra_destinations` | `DestinationNode[StateT, DepsT, OutputT] | type[BaseNode[StateT, DepsT, Any]]` | Additional destination nodes. | `()` |\n| `fork_id` | `str | None` | Optional node ID to use for the resulting broadcast fork if multiple destinations are provided. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `DecisionBranch[SourceT]` | A completed DecisionBranch with the specified destinations. |\n\nSource code in `pydantic_graph/pydantic_graph/beta/decision.py`\n\n|  |  |\n| --- | --- |\n| ``` 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 ``` | ``` def to(     self,     destination: DestinationNode[StateT, DepsT, OutputT] | type[BaseNode[StateT, DepsT, Any]],     /,     *extra_destinations: DestinationNode[StateT, DepsT, OutputT] | type[BaseNode[StateT, DepsT, Any]],     fork_id: str | None = None, ) -> DecisionBranch[SourceT]:     \"\"\"Set the destination(s) for this branch.      Args:         destination: The primary destination node.         *extra_destinations: Additional destination nodes.         fork_id: Optional node ID to use for the resulting broadcast fork if multiple destinations are provided.      Returns:         A completed DecisionBranch with the specified destinations.     \"\"\"     destination = get_origin(destination) or destination     extra_destinations = tuple(get_origin(d) or d for d in extra_destinations)     destinations = [(NodeStep(d) if inspect.isclass(d) else d) for d in (destination, *extra_destinations)]     return DecisionBranch(         source=self._source,         matches=self._matches,         path=self._path_builder.to(*destinations, fork_id=fork_id),         destinations=destinations,     ) ``` |\n\n#### broadcast\n\n```\nbroadcast(\n    get_forks: Callable[\n        [Self], Sequence[DecisionBranch[SourceT]]\n    ],\n    /,\n    *,\n    fork_id: str | None = None,\n) -> DecisionBranch[SourceT]\n```\n\nBroadcast this decision branch into multiple destinations.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `get_forks` | `Callable[[Self], Sequence[DecisionBranch[SourceT]]]` | The callback that will return a sequence of decision branches to broadcast to. | *required* |\n| `fork_id` | `str | None` | Optional node ID to use for the resulting broadcast fork. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `DecisionBranch[SourceT]` | A completed DecisionBranch with the specified destinations. |\n\nSource code in `pydantic_graph/pydantic_graph/beta/decision.py`\n\n|  |  |\n| --- | --- |\n| ``` 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 ``` | ``` def broadcast(     self, get_forks: Callable[[Self], Sequence[DecisionBranch[SourceT]]], /, *, fork_id: str | None = None ) -> DecisionBranch[SourceT]:     \"\"\"Broadcast this decision branch into multiple destinations.      Args:         get_forks: The callback that will return a sequence of decision branches to broadcast to.         fork_id: Optional node ID to use for the resulting broadcast fork.      Returns:         A completed DecisionBranch with the specified destinations.     \"\"\"     fork_decision_branches = get_forks(self)     new_paths = [b.path for b in fork_decision_branches]     if not new_paths:         raise GraphBuildingError(f'The call to {get_forks} returned no branches, but must return at least one.')     path = self._path_builder.broadcast(new_paths, fork_id=fork_id)     destinations = [d for fdp in fork_decision_branches for d in fdp.destinations]     return DecisionBranch(source=self._source, matches=self._matches, path=path, destinations=destinations) ``` |\n\n#### transform\n\n```\ntransform(\n    func: TransformFunction[\n        StateT, DepsT, OutputT, NewOutputT\n    ],\n) -> DecisionBranchBuilder[\n    StateT, DepsT, NewOutputT, SourceT, HandledT\n]\n```\n\nApply a transformation to the branch's output.\n\nParameters:", "url": "https://ai.pydantic.dev/pydantic_graph/beta_decision/index.html#decisionbranchbuilder-dataclass", "page": "pydantic_graph/beta_decision/index.html", "source_site": "pydantic_ai"}
{"title": "DecisionBranchBuilder `dataclass`", "anchor": "decisionbranchbuilder-dataclass", "md_text": "| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `func` | `TransformFunction[StateT, DepsT, OutputT, NewOutputT]` | Transformation function to apply. | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `DecisionBranchBuilder[StateT, DepsT, NewOutputT, SourceT, HandledT]` | A new DecisionBranchBuilder where the provided transform is applied prior to generating the final output. |\n\nSource code in `pydantic_graph/pydantic_graph/beta/decision.py`\n\n|  |  |\n| --- | --- |\n| ``` 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 ``` | ``` def transform(     self, func: TransformFunction[StateT, DepsT, OutputT, NewOutputT], / ) -> DecisionBranchBuilder[StateT, DepsT, NewOutputT, SourceT, HandledT]:     \"\"\"Apply a transformation to the branch's output.      Args:         func: Transformation function to apply.      Returns:         A new DecisionBranchBuilder where the provided transform is applied prior to generating the final output.     \"\"\"     return DecisionBranchBuilder(         decision=self._decision,         source=self._source,         matches=self._matches,         path_builder=self._path_builder.transform(func),     ) ``` |\n\n#### map\n\n```\nmap(\n    *,\n    fork_id: str | None = None,\n    downstream_join_id: str | None = None\n) -> DecisionBranchBuilder[\n    StateT, DepsT, T, SourceT, HandledT\n]\n```\n\nSpread the branch's output.\n\nTo do this, the current output must be iterable, and any subsequent steps in the path being built for this\nbranch will be applied to each item of the current output in parallel.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `fork_id` | `str | None` | Optional ID for the fork, defaults to a generated value | `None` |\n| `downstream_join_id` | `str | None` | Optional ID of a downstream join node which is involved when mapping empty iterables | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `DecisionBranchBuilder[StateT, DepsT, T, SourceT, HandledT]` | A new DecisionBranchBuilder where mapping is performed prior to generating the final output. |\n\nSource code in `pydantic_graph/pydantic_graph/beta/decision.py`\n\n|  |  |\n| --- | --- |\n| ``` 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 ``` | ``` def map(     self: DecisionBranchBuilder[StateT, DepsT, Iterable[T], SourceT, HandledT]     | DecisionBranchBuilder[StateT, DepsT, AsyncIterable[T], SourceT, HandledT],     *,     fork_id: str | None = None,     downstream_join_id: str | None = None, ) -> DecisionBranchBuilder[StateT, DepsT, T, SourceT, HandledT]:     \"\"\"Spread the branch's output.      To do this, the current output must be iterable, and any subsequent steps in the path being built for this     branch will be applied to each item of the current output in parallel.      Args:         fork_id: Optional ID for the fork, defaults to a generated value         downstream_join_id: Optional ID of a downstream join node which is involved when mapping empty iterables      Returns:         A new DecisionBranchBuilder where mapping is performed prior to generating the final output.     \"\"\"     return DecisionBranchBuilder(         decision=self._decision,         source=self._source,         matches=self._matches,         path_builder=self._path_builder.map(fork_id=fork_id, downstream_join_id=downstream_join_id),     ) ``` |\n\n#### label\n\n```\nlabel(\n    label: str,\n) -> DecisionBranchBuilder[\n    StateT, DepsT, OutputT, SourceT, HandledT\n]\n```\n\nApply a label to the branch at the current point in the path being built.\n\nThese labels are only used in generated mermaid diagrams.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `label` | `str` | The label to apply. | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `DecisionBranchBuilder[StateT, DepsT, OutputT, SourceT, HandledT]` | A new DecisionBranchBuilder where the label has been applied at the end of the current path being built. |\n\nSource code in `pydantic_graph/pydantic_graph/beta/decision.py`", "url": "https://ai.pydantic.dev/pydantic_graph/beta_decision/index.html#decisionbranchbuilder-dataclass", "page": "pydantic_graph/beta_decision/index.html", "source_site": "pydantic_ai"}
{"title": "DecisionBranchBuilder `dataclass`", "anchor": "decisionbranchbuilder-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 ``` | ``` def label(self, label: str) -> DecisionBranchBuilder[StateT, DepsT, OutputT, SourceT, HandledT]:     \"\"\"Apply a label to the branch at the current point in the path being built.      These labels are only used in generated mermaid diagrams.      Args:         label: The label to apply.      Returns:         A new DecisionBranchBuilder where the label has been applied at the end of the current path being built.     \"\"\"     return DecisionBranchBuilder(         decision=self._decision,         source=self._source,         matches=self._matches,         path_builder=self._path_builder.label(label),     ) ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta_decision/index.html#decisionbranchbuilder-dataclass", "page": "pydantic_graph/beta_decision/index.html", "source_site": "pydantic_ai"}
{"title": "StepContext `dataclass`", "anchor": "stepcontext-dataclass", "md_text": "Bases: `Generic[StateT, DepsT, InputT]`\n\nContext information passed to step functions during graph execution.\n\nThe step context provides access to the current graph state, dependencies, and input data for a step.\n\nType Parameters\n\nStateT: The type of the graph state\nDepsT: The type of the dependencies\nInputT: The type of the input data\n\n\nSource code in `pydantic_graph/pydantic_graph/beta/step.py`\n\n|  |  |\n| --- | --- |\n| ``` 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 ``` | ``` @dataclass(init=False) class StepContext(Generic[StateT, DepsT, InputT]):     \"\"\"Context information passed to step functions during graph execution.      The step context provides access to the current graph state, dependencies, and input data for a step.      Type Parameters:         StateT: The type of the graph state         DepsT: The type of the dependencies         InputT: The type of the input data     \"\"\"      _state: StateT     \"\"\"The current graph state.\"\"\"     _deps: DepsT     \"\"\"The graph run dependencies.\"\"\"     _inputs: InputT     \"\"\"The input data for this step.\"\"\"      def __init__(self, *, state: StateT, deps: DepsT, inputs: InputT):         self._state = state         self._deps = deps         self._inputs = inputs      @property     def state(self) -> StateT:         return self._state      @property     def deps(self) -> DepsT:         return self._deps      @property     def inputs(self) -> InputT:         \"\"\"The input data for this step.          This must be a property to ensure correct variance behavior         \"\"\"         return self._inputs ``` |\n\n#### inputs `property`\n\n```\ninputs: InputT\n```\n\nThe input data for this step.\n\nThis must be a property to ensure correct variance behavior", "url": "https://ai.pydantic.dev/pydantic_graph/beta_step/index.html#stepcontext-dataclass", "page": "pydantic_graph/beta_step/index.html", "source_site": "pydantic_ai"}
{"title": "StepFunction", "anchor": "stepfunction", "md_text": "Bases: `Protocol[StateT, DepsT, InputT, OutputT]`\n\nProtocol for step functions that can be executed in the graph.\n\nStep functions are async callables that receive a step context and return a result.\n\nType Parameters\n\nStateT: The type of the graph state\nDepsT: The type of the dependencies\nInputT: The type of the input data\nOutputT: The type of the output data\n\n\nSource code in `pydantic_graph/pydantic_graph/beta/step.py`\n\n|  |  |\n| --- | --- |\n| ``` 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 ``` | ``` class StepFunction(Protocol[StateT, DepsT, InputT, OutputT]):     \"\"\"Protocol for step functions that can be executed in the graph.      Step functions are async callables that receive a step context and return a result.      Type Parameters:         StateT: The type of the graph state         DepsT: The type of the dependencies         InputT: The type of the input data         OutputT: The type of the output data     \"\"\"      def __call__(self, ctx: StepContext[StateT, DepsT, InputT]) -> Awaitable[OutputT]:         \"\"\"Execute the step function with the given context.          Args:             ctx: The step context containing state, dependencies, and inputs          Returns:             An awaitable that resolves to the step's output         \"\"\"         raise NotImplementedError ``` |\n\n#### \\_\\_call\\_\\_\n\n```\n__call__(\n    ctx: StepContext[StateT, DepsT, InputT],\n) -> Awaitable[OutputT]\n```\n\nExecute the step function with the given context.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `ctx` | `StepContext[StateT, DepsT, InputT]` | The step context containing state, dependencies, and inputs | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `Awaitable[OutputT]` | An awaitable that resolves to the step's output |\n\nSource code in `pydantic_graph/pydantic_graph/beta/step.py`\n\n|  |  |\n| --- | --- |\n| ``` 78 79 80 81 82 83 84 85 86 87 ``` | ``` def __call__(self, ctx: StepContext[StateT, DepsT, InputT]) -> Awaitable[OutputT]:     \"\"\"Execute the step function with the given context.      Args:         ctx: The step context containing state, dependencies, and inputs      Returns:         An awaitable that resolves to the step's output     \"\"\"     raise NotImplementedError ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta_step/index.html#stepfunction", "page": "pydantic_graph/beta_step/index.html", "source_site": "pydantic_ai"}
{"title": "StreamFunction", "anchor": "streamfunction", "md_text": "Bases: `Protocol[StateT, DepsT, InputT, OutputT]`\n\nProtocol for stream functions that can be executed in the graph.\n\nStream functions are async callables that receive a step context and return an async iterator.\n\nType Parameters\n\nStateT: The type of the graph state\nDepsT: The type of the dependencies\nInputT: The type of the input data\nOutputT: The type of the output data\n\n\nSource code in `pydantic_graph/pydantic_graph/beta/step.py`\n\n|  |  |\n| --- | --- |\n| ```  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 ``` | ``` class StreamFunction(Protocol[StateT, DepsT, InputT, OutputT]):     \"\"\"Protocol for stream functions that can be executed in the graph.      Stream functions are async callables that receive a step context and return an async iterator.      Type Parameters:         StateT: The type of the graph state         DepsT: The type of the dependencies         InputT: The type of the input data         OutputT: The type of the output data     \"\"\"      def __call__(self, ctx: StepContext[StateT, DepsT, InputT]) -> AsyncIterator[OutputT]:         \"\"\"Execute the stream function with the given context.          Args:             ctx: The step context containing state, dependencies, and inputs          Returns:             An async iterator yielding the streamed output         \"\"\"         raise NotImplementedError         yield ``` |\n\n#### \\_\\_call\\_\\_\n\n```\n__call__(\n    ctx: StepContext[StateT, DepsT, InputT],\n) -> AsyncIterator[OutputT]\n```\n\nExecute the stream function with the given context.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `ctx` | `StepContext[StateT, DepsT, InputT]` | The step context containing state, dependencies, and inputs | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AsyncIterator[OutputT]` | An async iterator yielding the streamed output |\n\nSource code in `pydantic_graph/pydantic_graph/beta/step.py`\n\n|  |  |\n| --- | --- |\n| ``` 102 103 104 105 106 107 108 109 110 111 112 ``` | ``` def __call__(self, ctx: StepContext[StateT, DepsT, InputT]) -> AsyncIterator[OutputT]:     \"\"\"Execute the stream function with the given context.      Args:         ctx: The step context containing state, dependencies, and inputs      Returns:         An async iterator yielding the streamed output     \"\"\"     raise NotImplementedError     yield ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta_step/index.html#streamfunction", "page": "pydantic_graph/beta_step/index.html", "source_site": "pydantic_ai"}
{"title": "AnyStepFunction `module-attribute`", "anchor": "anystepfunction-module-attribute", "md_text": "```\nAnyStepFunction = StepFunction[Any, Any, Any, Any]\n```\n\nType alias for a step function with any type parameters.", "url": "https://ai.pydantic.dev/pydantic_graph/beta_step/index.html#anystepfunction-module-attribute", "page": "pydantic_graph/beta_step/index.html", "source_site": "pydantic_ai"}
{"title": "Step `dataclass`", "anchor": "step-dataclass", "md_text": "Bases: `Generic[StateT, DepsT, InputT, OutputT]`\n\nA step in the graph execution that wraps a step function.\n\nSteps represent individual units of execution in the graph, encapsulating\na step function along with metadata like ID and label.\n\nType Parameters\n\nStateT: The type of the graph state\nDepsT: The type of the dependencies\nInputT: The type of the input data\nOutputT: The type of the output data\n\n\nSource code in `pydantic_graph/pydantic_graph/beta/step.py`\n\n|  |  |\n| --- | --- |\n| ``` 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 ``` | ``` @dataclass(init=False) class Step(Generic[StateT, DepsT, InputT, OutputT]):     \"\"\"A step in the graph execution that wraps a step function.      Steps represent individual units of execution in the graph, encapsulating     a step function along with metadata like ID and label.      Type Parameters:         StateT: The type of the graph state         DepsT: The type of the dependencies         InputT: The type of the input data         OutputT: The type of the output data     \"\"\"      id: NodeID     \"\"\"Unique identifier for this step.\"\"\"     _call: StepFunction[StateT, DepsT, InputT, OutputT]     \"\"\"The step function to execute.\"\"\"     label: str | None     \"\"\"Optional human-readable label for this step.\"\"\"      def __init__(self, *, id: NodeID, call: StepFunction[StateT, DepsT, InputT, OutputT], label: str | None = None):         self.id = id         self._call = call         self.label = label      @property     def call(self) -> StepFunction[StateT, DepsT, InputT, OutputT]:         \"\"\"The step function to execute. This needs to be a property for proper variance inference.\"\"\"         return self._call      @overload     def as_node(self, inputs: None = None) -> StepNode[StateT, DepsT]: ...      @overload     def as_node(self, inputs: InputT) -> StepNode[StateT, DepsT]: ...      def as_node(self, inputs: InputT | None = None) -> StepNode[StateT, DepsT]:         \"\"\"Create a step node with bound inputs.          Args:             inputs: The input data to bind to this step, or None          Returns:             A [`StepNode`][pydantic_graph.beta.step.StepNode] with this step and the bound inputs         \"\"\"         return StepNode(self, inputs) ``` |\n\n#### id `instance-attribute`\n\n```\nid: NodeID = id\n```\n\nUnique identifier for this step.\n\n#### label `instance-attribute`\n\n```\nlabel: str | None = label\n```\n\nOptional human-readable label for this step.\n\n#### call `property`\n\n```\ncall: StepFunction[StateT, DepsT, InputT, OutputT]\n```\n\nThe step function to execute. This needs to be a property for proper variance inference.\n\n#### as\\_node\n\n```\nas_node(inputs: None = None) -> StepNode[StateT, DepsT]\n\nas_node(inputs: InputT) -> StepNode[StateT, DepsT]\n\nas_node(\n    inputs: InputT | None = None,\n) -> StepNode[StateT, DepsT]\n```\n\nCreate a step node with bound inputs.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `inputs` | `InputT | None` | The input data to bind to this step, or None | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `StepNode[StateT, DepsT]` | A [`StepNode`](index.html#pydantic_graph.beta.step.StepNode) with this step and the bound inputs |\n\nSource code in `pydantic_graph/pydantic_graph/beta/step.py`\n\n|  |  |\n| --- | --- |\n| ``` 156 157 158 159 160 161 162 163 164 165 ``` | ``` def as_node(self, inputs: InputT | None = None) -> StepNode[StateT, DepsT]:     \"\"\"Create a step node with bound inputs.      Args:         inputs: The input data to bind to this step, or None      Returns:         A [`StepNode`][pydantic_graph.beta.step.StepNode] with this step and the bound inputs     \"\"\"     return StepNode(self, inputs) ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta_step/index.html#step-dataclass", "page": "pydantic_graph/beta_step/index.html", "source_site": "pydantic_ai"}
{"title": "StepNode `dataclass`", "anchor": "stepnode-dataclass", "md_text": "Bases: `BaseNode[StateT, DepsT, Any]`\n\nA base node that represents a step with bound inputs.\n\nStepNode bridges between the v1 and v2 graph execution systems by wrapping\na [`Step`](index.html#pydantic_graph.beta.step.Step) with bound inputs in a BaseNode interface.\nIt is not meant to be run directly but rather used to indicate transitions\nto v2-style steps.\n\nSource code in `pydantic_graph/pydantic_graph/beta/step.py`\n\n|  |  |\n| --- | --- |\n| ``` 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 ``` | ``` @dataclass class StepNode(BaseNode[StateT, DepsT, Any]):     \"\"\"A base node that represents a step with bound inputs.      StepNode bridges between the v1 and v2 graph execution systems by wrapping     a [`Step`][pydantic_graph.beta.step.Step] with bound inputs in a BaseNode interface.     It is not meant to be run directly but rather used to indicate transitions     to v2-style steps.     \"\"\"      step: Step[StateT, DepsT, Any, Any]     \"\"\"The step to execute.\"\"\"      inputs: Any     \"\"\"The inputs bound to this step.\"\"\"      async def run(self, ctx: GraphRunContext[StateT, DepsT]) -> BaseNode[StateT, DepsT, Any] | End[Any]:         \"\"\"Attempt to run the step node.          Args:             ctx: The graph execution context          Returns:             The result of step execution          Raises:             NotImplementedError: Always raised as StepNode is not meant to be run directly         \"\"\"         raise NotImplementedError(             '`StepNode` is not meant to be run directly, it is meant to be used in `BaseNode` subclasses to indicate a transition to v2-style steps.'         ) ``` |\n\n#### step `instance-attribute`\n\n```\nstep: Step[StateT, DepsT, Any, Any]\n```\n\nThe step to execute.\n\n#### inputs `instance-attribute`\n\n```\ninputs: Any\n```\n\nThe inputs bound to this step.\n\n#### run `async`\n\n```\nrun(\n    ctx: GraphRunContext[StateT, DepsT],\n) -> BaseNode[StateT, DepsT, Any] | End[Any]\n```\n\nAttempt to run the step node.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `ctx` | `GraphRunContext[StateT, DepsT]` | The graph execution context | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `BaseNode[StateT, DepsT, Any] | End[Any]` | The result of step execution |\n\nRaises:\n\n| Type | Description |\n| --- | --- |\n| `NotImplementedError` | Always raised as StepNode is not meant to be run directly |\n\nSource code in `pydantic_graph/pydantic_graph/beta/step.py`\n\n|  |  |\n| --- | --- |\n| ``` 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 ``` | ``` async def run(self, ctx: GraphRunContext[StateT, DepsT]) -> BaseNode[StateT, DepsT, Any] | End[Any]:     \"\"\"Attempt to run the step node.      Args:         ctx: The graph execution context      Returns:         The result of step execution      Raises:         NotImplementedError: Always raised as StepNode is not meant to be run directly     \"\"\"     raise NotImplementedError(         '`StepNode` is not meant to be run directly, it is meant to be used in `BaseNode` subclasses to indicate a transition to v2-style steps.'     ) ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta_step/index.html#stepnode-dataclass", "page": "pydantic_graph/beta_step/index.html", "source_site": "pydantic_ai"}
{"title": "NodeStep", "anchor": "nodestep", "md_text": "Bases: `Step[StateT, DepsT, Any, BaseNode[StateT, DepsT, Any] | End[Any]]`\n\nA step that wraps a BaseNode type for execution.\n\nNodeStep allows v1-style BaseNode classes to be used as steps in the\nv2 graph execution system. It validates that the input is of the expected\nnode type and runs it with the appropriate graph context.\n\nSource code in `pydantic_graph/pydantic_graph/beta/step.py`\n\n|  |  |\n| --- | --- |\n| ``` 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 ``` | ``` class NodeStep(Step[StateT, DepsT, Any, BaseNode[StateT, DepsT, Any] | End[Any]]):     \"\"\"A step that wraps a BaseNode type for execution.      NodeStep allows v1-style BaseNode classes to be used as steps in the     v2 graph execution system. It validates that the input is of the expected     node type and runs it with the appropriate graph context.     \"\"\"      node_type: type[BaseNode[StateT, DepsT, Any]]     \"\"\"The BaseNode type this step executes.\"\"\"      def __init__(         self,         node_type: type[BaseNode[StateT, DepsT, Any]],         *,         id: NodeID | None = None,         label: str | None = None,     ):         \"\"\"Initialize a node step.          Args:             node_type: The BaseNode class this step will execute             id: Optional unique identifier, defaults to the node's get_node_id()             label: Optional human-readable label for this step         \"\"\"         super().__init__(             id=id or NodeID(node_type.get_node_id()),             call=self._call_node,             label=label,         )         # `type[BaseNode[StateT, DepsT, Any]]` could actually be a `typing._GenericAlias` like `pydantic_ai._agent_graph.UserPromptNode[~DepsT, ~OutputT]`,         # so we get the origin to get to the actual class         self.node_type = get_origin(node_type) or node_type      async def _call_node(self, ctx: StepContext[StateT, DepsT, Any]) -> BaseNode[StateT, DepsT, Any] | End[Any]:         \"\"\"Execute the wrapped node with the step context.          Args:             ctx: The step context containing the node instance to run          Returns:             The result of running the node, either another BaseNode or End          Raises:             ValueError: If the input node is not of the expected type         \"\"\"         node = ctx.inputs         if not isinstance(node, self.node_type):             raise ValueError(f'Node {node} is not of type {self.node_type}')  # pragma: no cover         node = cast(BaseNode[StateT, DepsT, Any], node)         return await node.run(GraphRunContext(state=ctx.state, deps=ctx.deps)) ``` |\n\n#### \\_\\_init\\_\\_\n\n```\n__init__(\n    node_type: type[BaseNode[StateT, DepsT, Any]],\n    *,\n    id: NodeID | None = None,\n    label: str | None = None\n)\n```\n\nInitialize a node step.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `node_type` | `type[BaseNode[StateT, DepsT, Any]]` | The BaseNode class this step will execute | *required* |\n| `id` | `NodeID | None` | Optional unique identifier, defaults to the node's get\\_node\\_id() | `None` |\n| `label` | `str | None` | Optional human-readable label for this step | `None` |\n\nSource code in `pydantic_graph/pydantic_graph/beta/step.py`\n\n|  |  |\n| --- | --- |\n| ``` 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 ``` | ``` def __init__(     self,     node_type: type[BaseNode[StateT, DepsT, Any]],     *,     id: NodeID | None = None,     label: str | None = None, ):     \"\"\"Initialize a node step.      Args:         node_type: The BaseNode class this step will execute         id: Optional unique identifier, defaults to the node's get_node_id()         label: Optional human-readable label for this step     \"\"\"     super().__init__(         id=id or NodeID(node_type.get_node_id()),         call=self._call_node,         label=label,     )     # `type[BaseNode[StateT, DepsT, Any]]` could actually be a `typing._GenericAlias` like `pydantic_ai._agent_graph.UserPromptNode[~DepsT, ~OutputT]`,     # so we get the origin to get to the actual class     self.node_type = get_origin(node_type) or node_type ``` |\n\n#### node\\_type `instance-attribute`", "url": "https://ai.pydantic.dev/pydantic_graph/beta_step/index.html#nodestep", "page": "pydantic_graph/beta_step/index.html", "source_site": "pydantic_ai"}
{"title": "NodeStep", "anchor": "nodestep", "md_text": "```\nnode_type: type[BaseNode[StateT, DepsT, Any]] = (\n    get_origin(node_type) or node_type\n)\n```\n\nThe BaseNode type this step executes.", "url": "https://ai.pydantic.dev/pydantic_graph/beta_step/index.html#nodestep", "page": "pydantic_graph/beta_step/index.html", "source_site": "pydantic_ai"}
{"title": "StateT `module-attribute`", "anchor": "statet-module-attribute", "md_text": "```\nStateT = TypeVar('StateT', infer_variance=True)\n```\n\nType variable for graph state.", "url": "https://ai.pydantic.dev/pydantic_graph/beta_node/index.html#statet-module-attribute", "page": "pydantic_graph/beta_node/index.html", "source_site": "pydantic_ai"}
{"title": "OutputT `module-attribute`", "anchor": "outputt-module-attribute", "md_text": "```\nOutputT = TypeVar('OutputT', infer_variance=True)\n```\n\nType variable for node output data.", "url": "https://ai.pydantic.dev/pydantic_graph/beta_node/index.html#outputt-module-attribute", "page": "pydantic_graph/beta_node/index.html", "source_site": "pydantic_ai"}
{"title": "InputT `module-attribute`", "anchor": "inputt-module-attribute", "md_text": "```\nInputT = TypeVar('InputT', infer_variance=True)\n```\n\nType variable for node input data.", "url": "https://ai.pydantic.dev/pydantic_graph/beta_node/index.html#inputt-module-attribute", "page": "pydantic_graph/beta_node/index.html", "source_site": "pydantic_ai"}
{"title": "StartNode", "anchor": "startnode", "md_text": "Bases: `Generic[OutputT]`\n\nEntry point node for graph execution.\n\nThe StartNode represents the beginning of a graph execution flow.\n\nSource code in `pydantic_graph/pydantic_graph/beta/node.py`\n\n|  |  |\n| --- | --- |\n| ``` 26 27 28 29 30 31 32 33 ``` | ``` class StartNode(Generic[OutputT]):     \"\"\"Entry point node for graph execution.      The StartNode represents the beginning of a graph execution flow.     \"\"\"      id = NodeID('__start__')     \"\"\"Fixed identifier for the start node.\"\"\" ``` |\n\n#### id `class-attribute` `instance-attribute`\n\n```\nid = NodeID('__start__')\n```\n\nFixed identifier for the start node.", "url": "https://ai.pydantic.dev/pydantic_graph/beta_node/index.html#startnode", "page": "pydantic_graph/beta_node/index.html", "source_site": "pydantic_ai"}
{"title": "EndNode", "anchor": "endnode", "md_text": "Bases: `Generic[InputT]`\n\nTerminal node representing the completion of graph execution.\n\nThe EndNode marks the successful completion of a graph execution flow\nand can collect the final output data.\n\nSource code in `pydantic_graph/pydantic_graph/beta/node.py`\n\n|  |  |\n| --- | --- |\n| ``` 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 ``` | ``` class EndNode(Generic[InputT]):     \"\"\"Terminal node representing the completion of graph execution.      The EndNode marks the successful completion of a graph execution flow     and can collect the final output data.     \"\"\"      id = NodeID('__end__')     \"\"\"Fixed identifier for the end node.\"\"\"      def _force_variance(self, inputs: InputT) -> None:  # pragma: no cover         \"\"\"Force type variance for proper generic typing.          This method exists solely for type checking purposes and should never be called.          Args:             inputs: Input data of type InputT.          Raises:             RuntimeError: Always, as this method should never be executed.         \"\"\"         raise RuntimeError('This method should never be called, it is just defined for typing purposes.') ``` |\n\n#### id `class-attribute` `instance-attribute`\n\n```\nid = NodeID('__end__')\n```\n\nFixed identifier for the end node.", "url": "https://ai.pydantic.dev/pydantic_graph/beta_node/index.html#endnode", "page": "pydantic_graph/beta_node/index.html", "source_site": "pydantic_ai"}
{"title": "Fork `dataclass`", "anchor": "fork-dataclass", "md_text": "Bases: `Generic[InputT, OutputT]`\n\nFork node that creates parallel execution branches.\n\nA Fork node splits the execution flow into multiple parallel branches,\nenabling concurrent execution of downstream nodes. It can either map\na sequence across multiple branches or duplicate data to each branch.\n\nSource code in `pydantic_graph/pydantic_graph/beta/node.py`\n\n|  |  |\n| --- | --- |\n| ``` 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 ``` | ``` @dataclass class Fork(Generic[InputT, OutputT]):     \"\"\"Fork node that creates parallel execution branches.      A Fork node splits the execution flow into multiple parallel branches,     enabling concurrent execution of downstream nodes. It can either map     a sequence across multiple branches or duplicate data to each branch.     \"\"\"      id: ForkID     \"\"\"Unique identifier for this fork node.\"\"\"      is_map: bool     \"\"\"Determines fork behavior.      If True, InputT must be Sequence[OutputT] and each element is sent to a separate branch.     If False, InputT must be OutputT and the same data is sent to all branches.     \"\"\"     downstream_join_id: JoinID | None     \"\"\"Optional identifier of a downstream join node that should be jumped to if mapping an empty iterable.\"\"\"      def _force_variance(self, inputs: InputT) -> OutputT:  # pragma: no cover         \"\"\"Force type variance for proper generic typing.          This method exists solely for type checking purposes and should never be called.          Args:             inputs: Input data to be forked.          Returns:             Output data type (never actually returned).          Raises:             RuntimeError: Always, as this method should never be executed.         \"\"\"         raise RuntimeError('This method should never be called, it is just defined for typing purposes.') ``` |\n\n#### id `instance-attribute`\n\n```\nid: ForkID\n```\n\nUnique identifier for this fork node.\n\n#### is\\_map `instance-attribute`\n\n```\nis_map: bool\n```\n\nDetermines fork behavior.\n\nIf True, InputT must be Sequence[OutputT] and each element is sent to a separate branch.\nIf False, InputT must be OutputT and the same data is sent to all branches.\n\n#### downstream\\_join\\_id `instance-attribute`\n\n```\ndownstream_join_id: JoinID | None\n```\n\nOptional identifier of a downstream join node that should be jumped to if mapping an empty iterable.", "url": "https://ai.pydantic.dev/pydantic_graph/beta_node/index.html#fork-dataclass", "page": "pydantic_graph/beta_node/index.html", "source_site": "pydantic_ai"}
{"title": "Graph `dataclass`", "anchor": "graph-dataclass", "md_text": "Bases: `Generic[StateT, DepsT, RunEndT]`\n\nDefinition of a graph.\n\nIn `pydantic-graph`, a graph is a collection of nodes that can be run in sequence. The nodes define\ntheir outgoing edges — e.g. which nodes may be run next, and thereby the structure of the graph.\n\nHere's a very simple example of a graph which increments a number by 1, but makes sure the number is never\n42 at the end.\n\nnever\\_42.py\n\n```\nfrom __future__ import annotations\n\nfrom dataclasses import dataclass\n\nfrom pydantic_graph import BaseNode, End, Graph, GraphRunContext\n\n@dataclass\nclass MyState:\n    number: int\n\n@dataclass\nclass Increment(BaseNode[MyState]):\n    async def run(self, ctx: GraphRunContext) -> Check42:\n        ctx.state.number += 1\n        return Check42()\n\n@dataclass\nclass Check42(BaseNode[MyState, None, int]):\n    async def run(self, ctx: GraphRunContext) -> Increment | End[int]:\n        if ctx.state.number == 42:\n            return Increment()\n        else:\n            return End(ctx.state.number)\n\nnever_42_graph = Graph(nodes=(Increment, Check42))\n```\n\n*(This example is complete, it can be run \"as is\")*\n\nSee [`run`](index.html#pydantic_graph.graph.Graph.run) For an example of running graph, and\n[`mermaid_code`](index.html#pydantic_graph.graph.Graph.mermaid_code) for an example of generating a mermaid diagram\nfrom the graph.\n\nSource code in `pydantic_graph/pydantic_graph/graph.py`", "url": "https://ai.pydantic.dev/pydantic_graph/graph/index.html#graph-dataclass", "page": "pydantic_graph/graph/index.html", "source_site": "pydantic_ai"}
{"title": "Graph `dataclass`", "anchor": "graph-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ```  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 ``` | ``` @dataclass(init=False) class Graph(Generic[StateT, DepsT, RunEndT]):     \"\"\"Definition of a graph.      In `pydantic-graph`, a graph is a collection of nodes that can be run in sequence. The nodes define     their outgoing edges — e.g. which nodes may be run next, and thereby the structure of the graph.      Here's a very simple example of a graph which increments a number by 1, but makes sure the number is never     42 at the end.      ```py {title=\"never_42.py\" noqa=\"I001\"}     from __future__ import annotations      from dataclasses import dataclass      from pydantic_graph import BaseNode, End, Graph, GraphRunContext      @dataclass     class MyState:         number: int      @dataclass     class Increment(BaseNode[MyState]):         async def run(self, ctx: GraphRunContext) -> Check42:             ctx.state.number += 1             return Check42()      @dataclass     class Check42(BaseNode[MyState, None, int]):         async def run(self, ctx: GraphRunContext) -> Increment | End[int]:             if ctx.state.number == 42:                 return Increment()             else:                 return End(ctx.state.number)      never_42_graph = Graph(nodes=(Increment, Check42))     ```     _(This example is complete, it can be run \"as is\")_      See [`run`][pydantic_graph.graph.Graph.run] For an example of running graph, and     [`mermaid_code`][pydantic_graph.graph.Graph.mermaid_code] for an example of generating a mermaid diagram     from the graph.     \"\"\"      name: str | None     node_defs: dict[str, NodeDef[StateT, DepsT, RunEndT]]     _state_type: type[StateT] | _utils.Unset = field(repr=False)     _run_end_type: type[RunEndT] | _utils.Unset = field(repr=False)     auto_instrument: bool = field(repr=False)      def __init__(         self,         *,         nodes: Sequence[type[BaseNode[StateT, DepsT, RunEndT]]],         name: str | None = None,         state_type: type[StateT] | _utils.Unset = _utils.UNSET,         run_end_type: type[RunEndT] | _utils.Unset = _utils.UNSET,         auto_instrument: bool = True,     ):         \"\"\"Create a graph from a sequence of nodes.          Args:             nodes: The nodes which make up the graph, nodes need to be unique and all be generic in the same                 state type.             name: Optional name for the graph, if not provided the name will be inferred from the calling frame                 on the first call to a graph method.             state_type: The type of the state for the graph, this can generally be inferred from `nodes`.             run_end_type: The type of the result of running the graph, this can generally be inferred from `nodes`.             auto_instrument: Whether to create a span for the graph run and the execution of each node's run method.         \"\"\"         self.name = name         self._state_type = state_type         self._run_end_type = run_end_type         self.auto_instrument = auto_instrument          parent_namespace = _utils.get_parent_namespace(inspect.currentframe())         self.node_defs = {}         for node in nodes:             self._register_node(node, parent_namespace)          self._validate_edges()      async def run(         self,         start_node: BaseNode[StateT, DepsT, RunEndT],         *,         state: StateT = None,         deps: DepsT = None,         persistence: BaseStatePersistence[StateT, RunEndT] | None = None,         infer_name: bool = True,     ) -> GraphRunResult[StateT, RunEndT]:         \"\"\"Run the graph from a starting node until it ends.          Args:             start_node: the first node to run, since the graph definition doesn't define the entry point in the graph,                 you need to provide the starting node.             state: The initial state of the graph.             deps: The dependencies of the graph.             persistence: State persistence interface, defaults to                 [`SimpleStatePersistence`][pydantic_graph.SimpleStatePersistence] if `None`.             infer_name: Whether to infer the graph name from the calling frame.          Returns:             A `GraphRunResult` containing information about the run, including its final result.          Here's an example of running the graph from [above][pydantic_graph.graph.Graph]:          ```py {title=\"run_never_42.py\" noqa=\"I001\" requires=\"never_42.py\"}         from never_42 import Increment, MyState, never_42_graph          async def main():             state = MyState(1)             await never_42_graph.run(Increment(), state=state)             print(state)             #> MyState(number=2)              state = MyState(41)             await never_42_graph.run(Increment(), state=state)             print(state)             #> MyState(number=43)         ```         \"\"\"         if infer_name and self.name is None:             self._infer_name(inspect.currentframe())          async with self.iter(             start_node, state=state, deps=deps, persistence=persistence, infer_name=False         ) as graph_run:             async for _node in graph_run:                 pass          result = graph_run.result         assert result is not None, 'GraphRun should have a result'         return result      def run_sync(         self,         start_node: BaseNode[StateT, DepsT, RunEndT],         *,         state: StateT = None,         deps: DepsT = None,         persistence: BaseStatePersistence[StateT, RunEndT] | None = None,         infer_name: bool = True,     ) -> GraphRunResult[StateT, RunEndT]:         \"\"\"Synchronously run the graph.          This is a convenience method that wraps [`self.run`][pydantic_graph.Graph.run] with `loop.run_until_complete(...)`.         You therefore can't use this method inside async code or if there's an active event loop.          Args:             start_node: the first node to run, since the graph definition doesn't define the entry point in the graph,                 you need to provide the starting node.             state: The initial state of the graph.             deps: The dependencies of the graph.             persistence: State persistence interface, defaults to                 [`SimpleStatePersistence`][pydantic_graph.SimpleStatePersistence] if `None`.             infer_name: Whether to infer the graph name from the calling frame.          Returns:             The result type from ending the run and the history of the run.         \"\"\"         if infer_name and self.name is None:  # pragma: no branch             self._infer_name(inspect.currentframe())          return _utils.get_event_loop().run_until_complete(             self.run(start_node, state=state, deps=deps, persistence=persistence, infer_name=False)         )      @asynccontextmanager     async def iter(         self,         start_node: BaseNode[StateT, DepsT, RunEndT],         *,         state: StateT = None,         deps: DepsT = None,         persistence: BaseStatePersistence[StateT, RunEndT] | None = None,         span: AbstractContextManager[AbstractSpan] | None = None,         infer_name: bool = True,     ) -> AsyncIterator[GraphRun[StateT, DepsT, RunEndT]]:         \"\"\"A contextmanager which can be used to iterate over the graph's nodes as they are executed.          This method returns a `GraphRun` object which can be used to async-iterate over the nodes of this `Graph` as         they are executed. This is the API to use if you want to record or interact with the nodes as the graph         execution unfolds.          The `GraphRun` can also be used to manually drive the graph execution by calling         [`GraphRun.next`][pydantic_graph.graph.GraphRun.next].          The `GraphRun` provides access to the full run history, state, deps, and the final result of the run once         it has completed.          For more details, see the API documentation of [`GraphRun`][pydantic_graph.graph.GraphRun].          Args:             start_node: the first node to run. Since the graph definition doesn't define the entry point in the graph,                 you need to provide the starting node.             state: The initial state of the graph.             deps: The dependencies of the graph.             persistence: State persistence interface, defaults to                 [`SimpleStatePersistence`][pydantic_graph.SimpleStatePersistence] if `None`.             span: The span to use for the graph run. If not provided, a new span will be created.             infer_name: Whether to infer the graph name from the calling frame.          Returns: A GraphRun that can be async iterated over to drive the graph to completion.         \"\"\"         if infer_name and self.name is None:             # f_back because `asynccontextmanager` adds one frame             if frame := inspect.currentframe():  # pragma: no branch                 self._infer_name(frame.f_back)          if persistence is None:             persistence = SimpleStatePersistence()         persistence.set_graph_types(self)          with ExitStack() as stack:             entered_span: AbstractSpan | None = None             if span is None:                 if self.auto_instrument:  # pragma: no branch                     # Separate variable because we actually don't want logfire's f-string magic here,                     # we want the span_name to be preformatted for other backends                     # as requested in https://github.com/pydantic/pydantic-ai/issues/3173.                     span_name = f'run graph {self.name}'                     entered_span = stack.enter_context(logfire_span(span_name, graph=self))             else:                 entered_span = stack.enter_context(span)             traceparent = None if entered_span is None else get_traceparent(entered_span)             yield GraphRun[StateT, DepsT, RunEndT](                 graph=self,                 start_node=start_node,                 persistence=persistence,                 state=state,                 deps=deps,                 traceparent=traceparent,             )      @asynccontextmanager     async def iter_from_persistence(         self,         persistence: BaseStatePersistence[StateT, RunEndT],         *,         deps: DepsT = None,         span: AbstractContextManager[AbstractSpan] | None = None,         infer_name: bool = True,     ) -> AsyncIterator[GraphRun[StateT, DepsT, RunEndT]]:         \"\"\"A contextmanager to iterate over the graph's nodes as they are executed, created from a persistence object.          This method has similar functionality to [`iter`][pydantic_graph.graph.Graph.iter],         but instead of passing the node to run, it will restore the node and state from state persistence.          Args:             persistence: The state persistence interface to use.             deps: The dependencies of the graph.             span: The span to use for the graph run. If not provided, a new span will be created.             infer_name: Whether to infer the graph name from the calling frame.          Returns: A GraphRun that can be async iterated over to drive the graph to completion.         \"\"\"         if infer_name and self.name is None:             # f_back because `asynccontextmanager` adds one frame             if frame := inspect.currentframe():  # pragma: no branch                 self._infer_name(frame.f_back)          persistence.set_graph_types(self)          snapshot = await persistence.load_next()         if snapshot is None:             raise exceptions.GraphRuntimeError('Unable to restore snapshot from state persistence.')          snapshot.node.set_snapshot_id(snapshot.id)          if self.auto_instrument and span is None:  # pragma: no branch             span = logfire_span('run graph {graph.name}', graph=self)          with ExitStack() as stack:             entered_span = None if span is None else stack.enter_context(span)             traceparent = None if entered_span is None else get_traceparent(entered_span)             yield GraphRun[StateT, DepsT, RunEndT](                 graph=self,                 start_node=snapshot.node,                 persistence=persistence,                 state=snapshot.state,                 deps=deps,                 snapshot_id=snapshot.id,                 traceparent=traceparent,             )      async def initialize(         self,         node: BaseNode[StateT, DepsT, RunEndT],         persistence: BaseStatePersistence[StateT, RunEndT],         *,         state: StateT = None,         infer_name: bool = True,     ) -> None:         \"\"\"Initialize a new graph run in persistence without running it.          This is useful if you want to set up a graph run to be run later, e.g. via         [`iter_from_persistence`][pydantic_graph.graph.Graph.iter_from_persistence].          Args:             node: The node to run first.             persistence: State persistence interface.             state: The start state of the graph.             infer_name: Whether to infer the graph name from the calling frame.         \"\"\"         if infer_name and self.name is None:             self._infer_name(inspect.currentframe())          persistence.set_graph_types(self)         await persistence.snapshot_node(state, node)      def mermaid_code(         self,         *,         start_node: Sequence[mermaid.NodeIdent] | mermaid.NodeIdent | None = None,         title: str | None | typing_extensions.Literal[False] = None,         edge_labels: bool = True,         notes: bool = True,         highlighted_nodes: Sequence[mermaid.NodeIdent] | mermaid.NodeIdent | None = None,         highlight_css: str = mermaid.DEFAULT_HIGHLIGHT_CSS,         infer_name: bool = True,         direction: mermaid.StateDiagramDirection | None = None,     ) -> str:         \"\"\"Generate a diagram representing the graph as [mermaid](https://mermaid.js.org/) diagram.          This method calls [`pydantic_graph.mermaid.generate_code`][pydantic_graph.mermaid.generate_code].          Args:             start_node: The node or nodes which can start the graph.             title: The title of the diagram, use `False` to not include a title.             edge_labels: Whether to include edge labels.             notes: Whether to include notes on each node.             highlighted_nodes: Optional node or nodes to highlight.             highlight_css: The CSS to use for highlighting nodes.             infer_name: Whether to infer the graph name from the calling frame.             direction: The direction of flow.          Returns:             The mermaid code for the graph, which can then be rendered as a diagram.          Here's an example of generating a diagram for the graph from [above][pydantic_graph.graph.Graph]:          ```py {title=\"mermaid_never_42.py\" requires=\"never_42.py\"}         from never_42 import Increment, never_42_graph          print(never_42_graph.mermaid_code(start_node=Increment))         '''         ---         title: never_42_graph         ---         stateDiagram-v2           [*] --> Increment           Increment --> Check42           Check42 --> Increment           Check42 --> [*]         '''         ```          The rendered diagram will look like this:          ```mermaid         ---         title: never_42_graph         ---         stateDiagram-v2           [*] --> Increment           Increment --> Check42           Check42 --> Increment           Check42 --> [*]         ```         \"\"\"         if infer_name and self.name is None:             self._infer_name(inspect.currentframe())         if title is None and self.name:             title = self.name         return mermaid.generate_code(             self,             start_node=start_node,             highlighted_nodes=highlighted_nodes,             highlight_css=highlight_css,             title=title or None,             edge_labels=edge_labels,             notes=notes,             direction=direction,         )      def mermaid_image(         self, infer_name: bool = True, **kwargs: typing_extensions.Unpack[mermaid.MermaidConfig]     ) -> bytes:         \"\"\"Generate a diagram representing the graph as an image.          The format and diagram can be customized using `kwargs`,         see [`pydantic_graph.mermaid.MermaidConfig`][pydantic_graph.mermaid.MermaidConfig].          !!! note \"Uses external service\"             This method makes a request to [mermaid.ink](https://mermaid.ink) to render the image, `mermaid.ink`             is a free service not affiliated with Pydantic.          Args:             infer_name: Whether to infer the graph name from the calling frame.             **kwargs: Additional arguments to pass to `mermaid.request_image`.          Returns:             The image bytes.         \"\"\"         if infer_name and self.name is None:             self._infer_name(inspect.currentframe())         if 'title' not in kwargs and self.name:             kwargs['title'] = self.name         return mermaid.request_image(self, **kwargs)      def mermaid_save(         self, path: Path | str, /, *, infer_name: bool = True, **kwargs: typing_extensions.Unpack[mermaid.MermaidConfig]     ) -> None:         \"\"\"Generate a diagram representing the graph and save it as an image.          The format and diagram can be customized using `kwargs`,         see [`pydantic_graph.mermaid.MermaidConfig`][pydantic_graph.mermaid.MermaidConfig].          !!! note \"Uses external service\"             This method makes a request to [mermaid.ink](https://mermaid.ink) to render the image, `mermaid.ink`             is a free service not affiliated with Pydantic.          Args:             path: The path to save the image to.             infer_name: Whether to infer the graph name from the calling frame.             **kwargs: Additional arguments to pass to `mermaid.save_image`.         \"\"\"         if infer_name and self.name is None:             self._infer_name(inspect.currentframe())         if 'title' not in kwargs and self.name:             kwargs['title'] = self.name         mermaid.save_image(path, self, **kwargs)      def get_nodes(self) -> Sequence[type[BaseNode[StateT, DepsT, RunEndT]]]:         \"\"\"Get the nodes in the graph.\"\"\"         return [node_def.node for node_def in self.node_defs.values()]      @cached_property     def inferred_types(self) -> tuple[type[StateT], type[RunEndT]]:         # Get the types of the state and run end from the graph.         if _utils.is_set(self._state_type) and _utils.is_set(self._run_end_type):             return self._state_type, self._run_end_type          state_type = self._state_type         run_end_type = self._run_end_type          for node_def in self.node_defs.values():             for base in typing_extensions.get_original_bases(node_def.node):                 if typing_extensions.get_origin(base) is BaseNode:                     args = typing_extensions.get_args(base)                     if not _utils.is_set(state_type) and args:                         state_type = args[0]                      if not _utils.is_set(run_end_type) and len(args) == 3:                         t = args[2]                         if not typing_objects.is_never(t):                             run_end_type = t                     if _utils.is_set(state_type) and _utils.is_set(run_end_type):                         return state_type, run_end_type  # pyright: ignore[reportReturnType]                     # break the inner (bases) loop                     break          if not _utils.is_set(state_type):  # pragma: no branch             # state defaults to None, so use that if we can't infer it             state_type = None         if not _utils.is_set(run_end_type):             # this happens if a graph has no return nodes, use None so any downstream errors are clear             run_end_type = None         return state_type, run_end_type  # pyright: ignore[reportReturnType]      def _register_node(         self,         node: type[BaseNode[StateT, DepsT, RunEndT]],         parent_namespace: dict[str, Any] | None,     ) -> None:         node_id = node.get_node_id()         if existing_node := self.node_defs.get(node_id):             raise exceptions.GraphSetupError(                 f'Node ID `{node_id}` is not unique — found on {existing_node.node} and {node}'             )         else:             self.node_defs[node_id] = node.get_node_def(parent_namespace)      def _validate_edges(self):         known_node_ids = self.node_defs.keys()         bad_edges: dict[str, list[str]] = {}          for node_id, node_def in self.node_defs.items():             for edge in node_def.next_node_edges.keys():                 if edge not in known_node_ids:                     bad_edges.setdefault(edge, []).append(f'`{node_id}`')          if bad_edges:             bad_edges_list = [f'`{k}` is referenced by {_utils.comma_and(v)}' for k, v in bad_edges.items()]             if len(bad_edges_list) == 1:                 raise exceptions.GraphSetupError(f'{bad_edges_list[0]} but not included in the graph.')             else:                 b = '\\n'.join(f' {be}' for be in bad_edges_list)                 raise exceptions.GraphSetupError(                     f'Nodes are referenced in the graph but not included in the graph:\\n{b}'                 )      def _infer_name(self, function_frame: types.FrameType | None) -> None:         \"\"\"Infer the agent name from the call frame.          Usage should be `self._infer_name(inspect.currentframe())`.          Copied from `Agent`.         \"\"\"         assert self.name is None, 'Name already set'         if function_frame is not None and (parent_frame := function_frame.f_back):  # pragma: no branch             for name, item in parent_frame.f_locals.items():                 if item is self:                     self.name = name                     return             if parent_frame.f_locals != parent_frame.f_globals:  # pragma: no branch                 # if we couldn't find the agent in locals and globals are a different dict, try globals                 for name, item in parent_frame.f_globals.items():  # pragma: no branch                     if item is self:                         self.name = name                         return ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/graph/index.html#graph-dataclass", "page": "pydantic_graph/graph/index.html", "source_site": "pydantic_ai"}
{"title": "Graph `dataclass`", "anchor": "graph-dataclass", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    *,\n    nodes: Sequence[type[BaseNode[StateT, DepsT, RunEndT]]],\n    name: str | None = None,\n    state_type: type[StateT] | Unset = UNSET,\n    run_end_type: type[RunEndT] | Unset = UNSET,\n    auto_instrument: bool = True\n)\n```\n\nCreate a graph from a sequence of nodes.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `nodes` | `Sequence[type[BaseNode[StateT, DepsT, RunEndT]]]` | The nodes which make up the graph, nodes need to be unique and all be generic in the same state type. | *required* |\n| `name` | `str | None` | Optional name for the graph, if not provided the name will be inferred from the calling frame on the first call to a graph method. | `None` |\n| `state_type` | `type[StateT] | Unset` | The type of the state for the graph, this can generally be inferred from `nodes`. | `UNSET` |\n| `run_end_type` | `type[RunEndT] | Unset` | The type of the result of running the graph, this can generally be inferred from `nodes`. | `UNSET` |\n| `auto_instrument` | `bool` | Whether to create a span for the graph run and the execution of each node's run method. | `True` |\n\nSource code in `pydantic_graph/pydantic_graph/graph.py`\n\n|  |  |\n| --- | --- |\n| ```  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 ``` | ``` def __init__(     self,     *,     nodes: Sequence[type[BaseNode[StateT, DepsT, RunEndT]]],     name: str | None = None,     state_type: type[StateT] | _utils.Unset = _utils.UNSET,     run_end_type: type[RunEndT] | _utils.Unset = _utils.UNSET,     auto_instrument: bool = True, ):     \"\"\"Create a graph from a sequence of nodes.      Args:         nodes: The nodes which make up the graph, nodes need to be unique and all be generic in the same             state type.         name: Optional name for the graph, if not provided the name will be inferred from the calling frame             on the first call to a graph method.         state_type: The type of the state for the graph, this can generally be inferred from `nodes`.         run_end_type: The type of the result of running the graph, this can generally be inferred from `nodes`.         auto_instrument: Whether to create a span for the graph run and the execution of each node's run method.     \"\"\"     self.name = name     self._state_type = state_type     self._run_end_type = run_end_type     self.auto_instrument = auto_instrument      parent_namespace = _utils.get_parent_namespace(inspect.currentframe())     self.node_defs = {}     for node in nodes:         self._register_node(node, parent_namespace)      self._validate_edges() ``` |\n\n#### run `async`\n\n```\nrun(\n    start_node: BaseNode[StateT, DepsT, RunEndT],\n    *,\n    state: StateT = None,\n    deps: DepsT = None,\n    persistence: (\n        BaseStatePersistence[StateT, RunEndT] | None\n    ) = None,\n    infer_name: bool = True\n) -> GraphRunResult[StateT, RunEndT]\n```\n\nRun the graph from a starting node until it ends.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `start_node` | `BaseNode[StateT, DepsT, RunEndT]` | the first node to run, since the graph definition doesn't define the entry point in the graph, you need to provide the starting node. | *required* |\n| `state` | `StateT` | The initial state of the graph. | `None` |\n| `deps` | `DepsT` | The dependencies of the graph. | `None` |\n| `persistence` | `BaseStatePersistence[StateT, RunEndT] | None` | State persistence interface, defaults to [`SimpleStatePersistence`](../persistence/index.html#pydantic_graph.persistence.in_mem.SimpleStatePersistence) if `None`. | `None` |\n| `infer_name` | `bool` | Whether to infer the graph name from the calling frame. | `True` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `GraphRunResult[StateT, RunEndT]` | A `GraphRunResult` containing information about the run, including its final result. |\n\nHere's an example of running the graph from [above](index.html#pydantic_graph.graph.Graph):\n\nrun\\_never\\_42.py\n\n```\nfrom never_42 import Increment, MyState, never_42_graph", "url": "https://ai.pydantic.dev/pydantic_graph/graph/index.html#graph-dataclass", "page": "pydantic_graph/graph/index.html", "source_site": "pydantic_ai"}
{"title": "Graph `dataclass`", "anchor": "graph-dataclass", "md_text": "async def main():\n    state = MyState(1)\n    await never_42_graph.run(Increment(), state=state)\n    print(state)\n    #> MyState(number=2)\n\n    state = MyState(41)\n    await never_42_graph.run(Increment(), state=state)\n    print(state)\n    #> MyState(number=43)\n```\n\nSource code in `pydantic_graph/pydantic_graph/graph.py`\n\n|  |  |\n| --- | --- |\n| ``` 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 ``` | ``` async def run(     self,     start_node: BaseNode[StateT, DepsT, RunEndT],     *,     state: StateT = None,     deps: DepsT = None,     persistence: BaseStatePersistence[StateT, RunEndT] | None = None,     infer_name: bool = True, ) -> GraphRunResult[StateT, RunEndT]:     \"\"\"Run the graph from a starting node until it ends.      Args:         start_node: the first node to run, since the graph definition doesn't define the entry point in the graph,             you need to provide the starting node.         state: The initial state of the graph.         deps: The dependencies of the graph.         persistence: State persistence interface, defaults to             [`SimpleStatePersistence`][pydantic_graph.SimpleStatePersistence] if `None`.         infer_name: Whether to infer the graph name from the calling frame.      Returns:         A `GraphRunResult` containing information about the run, including its final result.      Here's an example of running the graph from [above][pydantic_graph.graph.Graph]:      ```py {title=\"run_never_42.py\" noqa=\"I001\" requires=\"never_42.py\"}     from never_42 import Increment, MyState, never_42_graph      async def main():         state = MyState(1)         await never_42_graph.run(Increment(), state=state)         print(state)         #> MyState(number=2)          state = MyState(41)         await never_42_graph.run(Increment(), state=state)         print(state)         #> MyState(number=43)     ```     \"\"\"     if infer_name and self.name is None:         self._infer_name(inspect.currentframe())      async with self.iter(         start_node, state=state, deps=deps, persistence=persistence, infer_name=False     ) as graph_run:         async for _node in graph_run:             pass      result = graph_run.result     assert result is not None, 'GraphRun should have a result'     return result ``` |\n\n#### run\\_sync\n\n```\nrun_sync(\n    start_node: BaseNode[StateT, DepsT, RunEndT],\n    *,\n    state: StateT = None,\n    deps: DepsT = None,\n    persistence: (\n        BaseStatePersistence[StateT, RunEndT] | None\n    ) = None,\n    infer_name: bool = True\n) -> GraphRunResult[StateT, RunEndT]\n```\n\nSynchronously run the graph.\n\nThis is a convenience method that wraps [`self.run`](index.html#pydantic_graph.graph.Graph.run) with `loop.run_until_complete(...)`.\nYou therefore can't use this method inside async code or if there's an active event loop.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `start_node` | `BaseNode[StateT, DepsT, RunEndT]` | the first node to run, since the graph definition doesn't define the entry point in the graph, you need to provide the starting node. | *required* |\n| `state` | `StateT` | The initial state of the graph. | `None` |\n| `deps` | `DepsT` | The dependencies of the graph. | `None` |\n| `persistence` | `BaseStatePersistence[StateT, RunEndT] | None` | State persistence interface, defaults to [`SimpleStatePersistence`](../persistence/index.html#pydantic_graph.persistence.in_mem.SimpleStatePersistence) if `None`. | `None` |\n| `infer_name` | `bool` | Whether to infer the graph name from the calling frame. | `True` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `GraphRunResult[StateT, RunEndT]` | The result type from ending the run and the history of the run. |\n\nSource code in `pydantic_graph/pydantic_graph/graph.py`", "url": "https://ai.pydantic.dev/pydantic_graph/graph/index.html#graph-dataclass", "page": "pydantic_graph/graph/index.html", "source_site": "pydantic_ai"}
{"title": "Graph `dataclass`", "anchor": "graph-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 ``` | ``` def run_sync(     self,     start_node: BaseNode[StateT, DepsT, RunEndT],     *,     state: StateT = None,     deps: DepsT = None,     persistence: BaseStatePersistence[StateT, RunEndT] | None = None,     infer_name: bool = True, ) -> GraphRunResult[StateT, RunEndT]:     \"\"\"Synchronously run the graph.      This is a convenience method that wraps [`self.run`][pydantic_graph.Graph.run] with `loop.run_until_complete(...)`.     You therefore can't use this method inside async code or if there's an active event loop.      Args:         start_node: the first node to run, since the graph definition doesn't define the entry point in the graph,             you need to provide the starting node.         state: The initial state of the graph.         deps: The dependencies of the graph.         persistence: State persistence interface, defaults to             [`SimpleStatePersistence`][pydantic_graph.SimpleStatePersistence] if `None`.         infer_name: Whether to infer the graph name from the calling frame.      Returns:         The result type from ending the run and the history of the run.     \"\"\"     if infer_name and self.name is None:  # pragma: no branch         self._infer_name(inspect.currentframe())      return _utils.get_event_loop().run_until_complete(         self.run(start_node, state=state, deps=deps, persistence=persistence, infer_name=False)     ) ``` |\n\n#### iter `async`\n\n```\niter(\n    start_node: BaseNode[StateT, DepsT, RunEndT],\n    *,\n    state: StateT = None,\n    deps: DepsT = None,\n    persistence: (\n        BaseStatePersistence[StateT, RunEndT] | None\n    ) = None,\n    span: (\n        AbstractContextManager[AbstractSpan] | None\n    ) = None,\n    infer_name: bool = True\n) -> AsyncIterator[GraphRun[StateT, DepsT, RunEndT]]\n```\n\nA contextmanager which can be used to iterate over the graph's nodes as they are executed.\n\nThis method returns a `GraphRun` object which can be used to async-iterate over the nodes of this `Graph` as\nthey are executed. This is the API to use if you want to record or interact with the nodes as the graph\nexecution unfolds.\n\nThe `GraphRun` can also be used to manually drive the graph execution by calling\n[`GraphRun.next`](index.html#pydantic_graph.graph.GraphRun.next).\n\nThe `GraphRun` provides access to the full run history, state, deps, and the final result of the run once\nit has completed.\n\nFor more details, see the API documentation of [`GraphRun`](index.html#pydantic_graph.graph.GraphRun).\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `start_node` | `BaseNode[StateT, DepsT, RunEndT]` | the first node to run. Since the graph definition doesn't define the entry point in the graph, you need to provide the starting node. | *required* |\n| `state` | `StateT` | The initial state of the graph. | `None` |\n| `deps` | `DepsT` | The dependencies of the graph. | `None` |\n| `persistence` | `BaseStatePersistence[StateT, RunEndT] | None` | State persistence interface, defaults to [`SimpleStatePersistence`](../persistence/index.html#pydantic_graph.persistence.in_mem.SimpleStatePersistence) if `None`. | `None` |\n| `span` | `AbstractContextManager[AbstractSpan] | None` | The span to use for the graph run. If not provided, a new span will be created. | `None` |\n| `infer_name` | `bool` | Whether to infer the graph name from the calling frame. | `True` |\n\nReturns: A GraphRun that can be async iterated over to drive the graph to completion.\n\nSource code in `pydantic_graph/pydantic_graph/graph.py`", "url": "https://ai.pydantic.dev/pydantic_graph/graph/index.html#graph-dataclass", "page": "pydantic_graph/graph/index.html", "source_site": "pydantic_ai"}
{"title": "Graph `dataclass`", "anchor": "graph-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 ``` | ``` @asynccontextmanager async def iter(     self,     start_node: BaseNode[StateT, DepsT, RunEndT],     *,     state: StateT = None,     deps: DepsT = None,     persistence: BaseStatePersistence[StateT, RunEndT] | None = None,     span: AbstractContextManager[AbstractSpan] | None = None,     infer_name: bool = True, ) -> AsyncIterator[GraphRun[StateT, DepsT, RunEndT]]:     \"\"\"A contextmanager which can be used to iterate over the graph's nodes as they are executed.      This method returns a `GraphRun` object which can be used to async-iterate over the nodes of this `Graph` as     they are executed. This is the API to use if you want to record or interact with the nodes as the graph     execution unfolds.      The `GraphRun` can also be used to manually drive the graph execution by calling     [`GraphRun.next`][pydantic_graph.graph.GraphRun.next].      The `GraphRun` provides access to the full run history, state, deps, and the final result of the run once     it has completed.      For more details, see the API documentation of [`GraphRun`][pydantic_graph.graph.GraphRun].      Args:         start_node: the first node to run. Since the graph definition doesn't define the entry point in the graph,             you need to provide the starting node.         state: The initial state of the graph.         deps: The dependencies of the graph.         persistence: State persistence interface, defaults to             [`SimpleStatePersistence`][pydantic_graph.SimpleStatePersistence] if `None`.         span: The span to use for the graph run. If not provided, a new span will be created.         infer_name: Whether to infer the graph name from the calling frame.      Returns: A GraphRun that can be async iterated over to drive the graph to completion.     \"\"\"     if infer_name and self.name is None:         # f_back because `asynccontextmanager` adds one frame         if frame := inspect.currentframe():  # pragma: no branch             self._infer_name(frame.f_back)      if persistence is None:         persistence = SimpleStatePersistence()     persistence.set_graph_types(self)      with ExitStack() as stack:         entered_span: AbstractSpan | None = None         if span is None:             if self.auto_instrument:  # pragma: no branch                 # Separate variable because we actually don't want logfire's f-string magic here,                 # we want the span_name to be preformatted for other backends                 # as requested in https://github.com/pydantic/pydantic-ai/issues/3173.                 span_name = f'run graph {self.name}'                 entered_span = stack.enter_context(logfire_span(span_name, graph=self))         else:             entered_span = stack.enter_context(span)         traceparent = None if entered_span is None else get_traceparent(entered_span)         yield GraphRun[StateT, DepsT, RunEndT](             graph=self,             start_node=start_node,             persistence=persistence,             state=state,             deps=deps,             traceparent=traceparent,         ) ``` |\n\n#### iter\\_from\\_persistence `async`\n\n```\niter_from_persistence(\n    persistence: BaseStatePersistence[StateT, RunEndT],\n    *,\n    deps: DepsT = None,\n    span: (\n        AbstractContextManager[AbstractSpan] | None\n    ) = None,\n    infer_name: bool = True\n) -> AsyncIterator[GraphRun[StateT, DepsT, RunEndT]]\n```\n\nA contextmanager to iterate over the graph's nodes as they are executed, created from a persistence object.\n\nThis method has similar functionality to [`iter`](index.html#pydantic_graph.graph.Graph.iter),\nbut instead of passing the node to run, it will restore the node and state from state persistence.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `persistence` | `BaseStatePersistence[StateT, RunEndT]` | The state persistence interface to use. | *required* |\n| `deps` | `DepsT` | The dependencies of the graph. | `None` |\n| `span` | `AbstractContextManager[AbstractSpan] | None` | The span to use for the graph run. If not provided, a new span will be created. | `None` |\n| `infer_name` | `bool` | Whether to infer the graph name from the calling frame. | `True` |\n\nReturns: A GraphRun that can be async iterated over to drive the graph to completion.\n\nSource code in `pydantic_graph/pydantic_graph/graph.py`", "url": "https://ai.pydantic.dev/pydantic_graph/graph/index.html#graph-dataclass", "page": "pydantic_graph/graph/index.html", "source_site": "pydantic_ai"}
{"title": "Graph `dataclass`", "anchor": "graph-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 ``` | ``` @asynccontextmanager async def iter_from_persistence(     self,     persistence: BaseStatePersistence[StateT, RunEndT],     *,     deps: DepsT = None,     span: AbstractContextManager[AbstractSpan] | None = None,     infer_name: bool = True, ) -> AsyncIterator[GraphRun[StateT, DepsT, RunEndT]]:     \"\"\"A contextmanager to iterate over the graph's nodes as they are executed, created from a persistence object.      This method has similar functionality to [`iter`][pydantic_graph.graph.Graph.iter],     but instead of passing the node to run, it will restore the node and state from state persistence.      Args:         persistence: The state persistence interface to use.         deps: The dependencies of the graph.         span: The span to use for the graph run. If not provided, a new span will be created.         infer_name: Whether to infer the graph name from the calling frame.      Returns: A GraphRun that can be async iterated over to drive the graph to completion.     \"\"\"     if infer_name and self.name is None:         # f_back because `asynccontextmanager` adds one frame         if frame := inspect.currentframe():  # pragma: no branch             self._infer_name(frame.f_back)      persistence.set_graph_types(self)      snapshot = await persistence.load_next()     if snapshot is None:         raise exceptions.GraphRuntimeError('Unable to restore snapshot from state persistence.')      snapshot.node.set_snapshot_id(snapshot.id)      if self.auto_instrument and span is None:  # pragma: no branch         span = logfire_span('run graph {graph.name}', graph=self)      with ExitStack() as stack:         entered_span = None if span is None else stack.enter_context(span)         traceparent = None if entered_span is None else get_traceparent(entered_span)         yield GraphRun[StateT, DepsT, RunEndT](             graph=self,             start_node=snapshot.node,             persistence=persistence,             state=snapshot.state,             deps=deps,             snapshot_id=snapshot.id,             traceparent=traceparent,         ) ``` |\n\n#### initialize `async`\n\n```\ninitialize(\n    node: BaseNode[StateT, DepsT, RunEndT],\n    persistence: BaseStatePersistence[StateT, RunEndT],\n    *,\n    state: StateT = None,\n    infer_name: bool = True\n) -> None\n```\n\nInitialize a new graph run in persistence without running it.\n\nThis is useful if you want to set up a graph run to be run later, e.g. via\n[`iter_from_persistence`](index.html#pydantic_graph.graph.Graph.iter_from_persistence).\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `node` | `BaseNode[StateT, DepsT, RunEndT]` | The node to run first. | *required* |\n| `persistence` | `BaseStatePersistence[StateT, RunEndT]` | State persistence interface. | *required* |\n| `state` | `StateT` | The start state of the graph. | `None` |\n| `infer_name` | `bool` | Whether to infer the graph name from the calling frame. | `True` |\n\nSource code in `pydantic_graph/pydantic_graph/graph.py`\n\n|  |  |\n| --- | --- |\n| ``` 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 ``` | ``` async def initialize(     self,     node: BaseNode[StateT, DepsT, RunEndT],     persistence: BaseStatePersistence[StateT, RunEndT],     *,     state: StateT = None,     infer_name: bool = True, ) -> None:     \"\"\"Initialize a new graph run in persistence without running it.      This is useful if you want to set up a graph run to be run later, e.g. via     [`iter_from_persistence`][pydantic_graph.graph.Graph.iter_from_persistence].      Args:         node: The node to run first.         persistence: State persistence interface.         state: The start state of the graph.         infer_name: Whether to infer the graph name from the calling frame.     \"\"\"     if infer_name and self.name is None:         self._infer_name(inspect.currentframe())      persistence.set_graph_types(self)     await persistence.snapshot_node(state, node) ``` |\n\n#### mermaid\\_code", "url": "https://ai.pydantic.dev/pydantic_graph/graph/index.html#graph-dataclass", "page": "pydantic_graph/graph/index.html", "source_site": "pydantic_ai"}
{"title": "Graph `dataclass`", "anchor": "graph-dataclass", "md_text": "```\nmermaid_code(\n    *,\n    start_node: (\n        Sequence[NodeIdent] | NodeIdent | None\n    ) = None,\n    title: str | None | Literal[False] = None,\n    edge_labels: bool = True,\n    notes: bool = True,\n    highlighted_nodes: (\n        Sequence[NodeIdent] | NodeIdent | None\n    ) = None,\n    highlight_css: str = DEFAULT_HIGHLIGHT_CSS,\n    infer_name: bool = True,\n    direction: StateDiagramDirection | None = None\n) -> str\n```\n\nGenerate a diagram representing the graph as [mermaid](https://mermaid.js.org/) diagram.\n\nThis method calls [`pydantic_graph.mermaid.generate_code`](../mermaid/index.html#pydantic_graph.mermaid.generate_code).\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `start_node` | `Sequence[NodeIdent] | NodeIdent | None` | The node or nodes which can start the graph. | `None` |\n| `title` | `str | None | Literal[False]` | The title of the diagram, use `False` to not include a title. | `None` |\n| `edge_labels` | `bool` | Whether to include edge labels. | `True` |\n| `notes` | `bool` | Whether to include notes on each node. | `True` |\n| `highlighted_nodes` | `Sequence[NodeIdent] | NodeIdent | None` | Optional node or nodes to highlight. | `None` |\n| `highlight_css` | `str` | The CSS to use for highlighting nodes. | `DEFAULT_HIGHLIGHT_CSS` |\n| `infer_name` | `bool` | Whether to infer the graph name from the calling frame. | `True` |\n| `direction` | `StateDiagramDirection | None` | The direction of flow. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `str` | The mermaid code for the graph, which can then be rendered as a diagram. |\n\nHere's an example of generating a diagram for the graph from [above](index.html#pydantic_graph.graph.Graph):\n\nmermaid\\_never\\_42.py\n\n```\nfrom never_42 import Increment, never_42_graph\n\nprint(never_42_graph.mermaid_code(start_node=Increment))\n'''\n---\ntitle: never_42_graph\n---\nstateDiagram-v2\n  [*] --> Increment\n  Increment --> Check42\n  Check42 --> Increment\n  Check42 --> [*]\n'''\n```\n\nThe rendered diagram will look like this:\n\n```\n---\ntitle: never_42_graph\n---\nstateDiagram-v2\n  [*] --> Increment\n  Increment --> Check42\n  Check42 --> Increment\n  Check42 --> [*]\n```\n\nSource code in `pydantic_graph/pydantic_graph/graph.py`", "url": "https://ai.pydantic.dev/pydantic_graph/graph/index.html#graph-dataclass", "page": "pydantic_graph/graph/index.html", "source_site": "pydantic_ai"}
{"title": "Graph `dataclass`", "anchor": "graph-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 ``` | ``` def mermaid_code(     self,     *,     start_node: Sequence[mermaid.NodeIdent] | mermaid.NodeIdent | None = None,     title: str | None | typing_extensions.Literal[False] = None,     edge_labels: bool = True,     notes: bool = True,     highlighted_nodes: Sequence[mermaid.NodeIdent] | mermaid.NodeIdent | None = None,     highlight_css: str = mermaid.DEFAULT_HIGHLIGHT_CSS,     infer_name: bool = True,     direction: mermaid.StateDiagramDirection | None = None, ) -> str:     \"\"\"Generate a diagram representing the graph as [mermaid](https://mermaid.js.org/) diagram.      This method calls [`pydantic_graph.mermaid.generate_code`][pydantic_graph.mermaid.generate_code].      Args:         start_node: The node or nodes which can start the graph.         title: The title of the diagram, use `False` to not include a title.         edge_labels: Whether to include edge labels.         notes: Whether to include notes on each node.         highlighted_nodes: Optional node or nodes to highlight.         highlight_css: The CSS to use for highlighting nodes.         infer_name: Whether to infer the graph name from the calling frame.         direction: The direction of flow.      Returns:         The mermaid code for the graph, which can then be rendered as a diagram.      Here's an example of generating a diagram for the graph from [above][pydantic_graph.graph.Graph]:      ```py {title=\"mermaid_never_42.py\" requires=\"never_42.py\"}     from never_42 import Increment, never_42_graph      print(never_42_graph.mermaid_code(start_node=Increment))     '''     ---     title: never_42_graph     ---     stateDiagram-v2       [*] --> Increment       Increment --> Check42       Check42 --> Increment       Check42 --> [*]     '''     ```      The rendered diagram will look like this:      ```mermaid     ---     title: never_42_graph     ---     stateDiagram-v2       [*] --> Increment       Increment --> Check42       Check42 --> Increment       Check42 --> [*]     ```     \"\"\"     if infer_name and self.name is None:         self._infer_name(inspect.currentframe())     if title is None and self.name:         title = self.name     return mermaid.generate_code(         self,         start_node=start_node,         highlighted_nodes=highlighted_nodes,         highlight_css=highlight_css,         title=title or None,         edge_labels=edge_labels,         notes=notes,         direction=direction,     ) ``` |\n\n#### mermaid\\_image\n\n```\nmermaid_image(\n    infer_name: bool = True, **kwargs: Unpack[MermaidConfig]\n) -> bytes\n```\n\nGenerate a diagram representing the graph as an image.\n\nThe format and diagram can be customized using `kwargs`,\nsee [`pydantic_graph.mermaid.MermaidConfig`](../mermaid/index.html#pydantic_graph.mermaid.MermaidConfig).\n\nThis method makes a request to [mermaid.ink](https://mermaid.ink) to render the image, `mermaid.ink`\nis a free service not affiliated with Pydantic.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `infer_name` | `bool` | Whether to infer the graph name from the calling frame. | `True` |\n| `**kwargs` | `Unpack[MermaidConfig]` | Additional arguments to pass to `mermaid.request_image`. | `{}` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `bytes` | The image bytes. |\n\nSource code in `pydantic_graph/pydantic_graph/graph.py`", "url": "https://ai.pydantic.dev/pydantic_graph/graph/index.html#graph-dataclass", "page": "pydantic_graph/graph/index.html", "source_site": "pydantic_ai"}
{"title": "Graph `dataclass`", "anchor": "graph-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 ``` | ``` def mermaid_image(     self, infer_name: bool = True, **kwargs: typing_extensions.Unpack[mermaid.MermaidConfig] ) -> bytes:     \"\"\"Generate a diagram representing the graph as an image.      The format and diagram can be customized using `kwargs`,     see [`pydantic_graph.mermaid.MermaidConfig`][pydantic_graph.mermaid.MermaidConfig].      !!! note \"Uses external service\"         This method makes a request to [mermaid.ink](https://mermaid.ink) to render the image, `mermaid.ink`         is a free service not affiliated with Pydantic.      Args:         infer_name: Whether to infer the graph name from the calling frame.         **kwargs: Additional arguments to pass to `mermaid.request_image`.      Returns:         The image bytes.     \"\"\"     if infer_name and self.name is None:         self._infer_name(inspect.currentframe())     if 'title' not in kwargs and self.name:         kwargs['title'] = self.name     return mermaid.request_image(self, **kwargs) ``` |\n\n#### mermaid\\_save\n\n```\nmermaid_save(\n    path: Path | str,\n    /,\n    *,\n    infer_name: bool = True,\n    **kwargs: Unpack[MermaidConfig],\n) -> None\n```\n\nGenerate a diagram representing the graph and save it as an image.\n\nThe format and diagram can be customized using `kwargs`,\nsee [`pydantic_graph.mermaid.MermaidConfig`](../mermaid/index.html#pydantic_graph.mermaid.MermaidConfig).\n\nThis method makes a request to [mermaid.ink](https://mermaid.ink) to render the image, `mermaid.ink`\nis a free service not affiliated with Pydantic.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `path` | `Path | str` | The path to save the image to. | *required* |\n| `infer_name` | `bool` | Whether to infer the graph name from the calling frame. | `True` |\n| `**kwargs` | `Unpack[MermaidConfig]` | Additional arguments to pass to `mermaid.save_image`. | `{}` |\n\nSource code in `pydantic_graph/pydantic_graph/graph.py`\n\n|  |  |\n| --- | --- |\n| ``` 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 ``` | ``` def mermaid_save(     self, path: Path | str, /, *, infer_name: bool = True, **kwargs: typing_extensions.Unpack[mermaid.MermaidConfig] ) -> None:     \"\"\"Generate a diagram representing the graph and save it as an image.      The format and diagram can be customized using `kwargs`,     see [`pydantic_graph.mermaid.MermaidConfig`][pydantic_graph.mermaid.MermaidConfig].      !!! note \"Uses external service\"         This method makes a request to [mermaid.ink](https://mermaid.ink) to render the image, `mermaid.ink`         is a free service not affiliated with Pydantic.      Args:         path: The path to save the image to.         infer_name: Whether to infer the graph name from the calling frame.         **kwargs: Additional arguments to pass to `mermaid.save_image`.     \"\"\"     if infer_name and self.name is None:         self._infer_name(inspect.currentframe())     if 'title' not in kwargs and self.name:         kwargs['title'] = self.name     mermaid.save_image(path, self, **kwargs) ``` |\n\n#### get\\_nodes\n\n```\nget_nodes() -> (\n    Sequence[type[BaseNode[StateT, DepsT, RunEndT]]]\n)\n```\n\nGet the nodes in the graph.\n\nSource code in `pydantic_graph/pydantic_graph/graph.py`\n\n|  |  |\n| --- | --- |\n| ``` 458 459 460 ``` | ``` def get_nodes(self) -> Sequence[type[BaseNode[StateT, DepsT, RunEndT]]]:     \"\"\"Get the nodes in the graph.\"\"\"     return [node_def.node for node_def in self.node_defs.values()] ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/graph/index.html#graph-dataclass", "page": "pydantic_graph/graph/index.html", "source_site": "pydantic_ai"}
{"title": "GraphRun", "anchor": "graphrun", "md_text": "Bases: `Generic[StateT, DepsT, RunEndT]`\n\nA stateful, async-iterable run of a [`Graph`](index.html#pydantic_graph.graph.Graph).\n\nYou typically get a `GraphRun` instance from calling\n`async with [my_graph.iter(...)][pydantic_graph.graph.Graph.iter] as graph_run:`. That gives you the ability to iterate\nthrough nodes as they run, either by `async for` iteration or by repeatedly calling `.next(...)`.\n\nHere's an example of iterating over the graph from [above](index.html#pydantic_graph.graph.Graph):\n\niter\\_never\\_42.py\n\n```\nfrom copy import deepcopy\nfrom never_42 import Increment, MyState, never_42_graph\n\nasync def main():\n    state = MyState(1)\n    async with never_42_graph.iter(Increment(), state=state) as graph_run:\n        node_states = [(graph_run.next_node, deepcopy(graph_run.state))]\n        async for node in graph_run:\n            node_states.append((node, deepcopy(graph_run.state)))\n        print(node_states)\n        '''\n        [\n            (Increment(), MyState(number=1)),\n            (Increment(), MyState(number=1)),\n            (Check42(), MyState(number=2)),\n            (End(data=2), MyState(number=2)),\n        ]\n        '''\n\n    state = MyState(41)\n    async with never_42_graph.iter(Increment(), state=state) as graph_run:\n        node_states = [(graph_run.next_node, deepcopy(graph_run.state))]\n        async for node in graph_run:\n            node_states.append((node, deepcopy(graph_run.state)))\n        print(node_states)\n        '''\n        [\n            (Increment(), MyState(number=41)),\n            (Increment(), MyState(number=41)),\n            (Check42(), MyState(number=42)),\n            (Increment(), MyState(number=42)),\n            (Check42(), MyState(number=43)),\n            (End(data=43), MyState(number=43)),\n        ]\n        '''\n```\n\nSee the [`GraphRun.next` documentation](index.html#pydantic_graph.graph.GraphRun.next) for an example of how to manually\ndrive the graph run.\n\nSource code in `pydantic_graph/pydantic_graph/graph.py`", "url": "https://ai.pydantic.dev/pydantic_graph/graph/index.html#graphrun", "page": "pydantic_graph/graph/index.html", "source_site": "pydantic_ai"}
{"title": "GraphRun", "anchor": "graphrun", "md_text": "|  |  |\n| --- | --- |\n| ``` 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 ``` | ``` class GraphRun(Generic[StateT, DepsT, RunEndT]):     \"\"\"A stateful, async-iterable run of a [`Graph`][pydantic_graph.graph.Graph].      You typically get a `GraphRun` instance from calling     `async with [my_graph.iter(...)][pydantic_graph.graph.Graph.iter] as graph_run:`. That gives you the ability to iterate     through nodes as they run, either by `async for` iteration or by repeatedly calling `.next(...)`.      Here's an example of iterating over the graph from [above][pydantic_graph.graph.Graph]:     ```py {title=\"iter_never_42.py\" noqa=\"I001\" requires=\"never_42.py\"}     from copy import deepcopy     from never_42 import Increment, MyState, never_42_graph      async def main():         state = MyState(1)         async with never_42_graph.iter(Increment(), state=state) as graph_run:             node_states = [(graph_run.next_node, deepcopy(graph_run.state))]             async for node in graph_run:                 node_states.append((node, deepcopy(graph_run.state)))             print(node_states)             '''             [                 (Increment(), MyState(number=1)),                 (Increment(), MyState(number=1)),                 (Check42(), MyState(number=2)),                 (End(data=2), MyState(number=2)),             ]             '''          state = MyState(41)         async with never_42_graph.iter(Increment(), state=state) as graph_run:             node_states = [(graph_run.next_node, deepcopy(graph_run.state))]             async for node in graph_run:                 node_states.append((node, deepcopy(graph_run.state)))             print(node_states)             '''             [                 (Increment(), MyState(number=41)),                 (Increment(), MyState(number=41)),                 (Check42(), MyState(number=42)),                 (Increment(), MyState(number=42)),                 (Check42(), MyState(number=43)),                 (End(data=43), MyState(number=43)),             ]             '''     ```      See the [`GraphRun.next` documentation][pydantic_graph.graph.GraphRun.next] for an example of how to manually     drive the graph run.     \"\"\"      def __init__(         self,         *,         graph: Graph[StateT, DepsT, RunEndT],         start_node: BaseNode[StateT, DepsT, RunEndT],         persistence: BaseStatePersistence[StateT, RunEndT],         state: StateT,         deps: DepsT,         traceparent: str | None,         snapshot_id: str | None = None,     ):         \"\"\"Create a new run for a given graph, starting at the specified node.          Typically, you'll use [`Graph.iter`][pydantic_graph.graph.Graph.iter] rather than calling this directly.          Args:             graph: The [`Graph`][pydantic_graph.graph.Graph] to run.             start_node: The node where execution will begin.             persistence: State persistence interface.             state: A shared state object or primitive (like a counter, dataclass, etc.) that is available                 to all nodes via `ctx.state`.             deps: Optional dependencies that each node can access via `ctx.deps`, e.g. database connections,                 configuration, or logging clients.             traceparent: The traceparent for the span used for the graph run.             snapshot_id: The ID of the snapshot the node came from.         \"\"\"         self.graph = graph         self.persistence = persistence         self._snapshot_id: str | None = snapshot_id         self.state = state         self.deps = deps          self.__traceparent = traceparent         self._next_node: BaseNode[StateT, DepsT, RunEndT] | End[RunEndT] = start_node         self._is_started: bool = False      @overload     def _traceparent(self, *, required: typing_extensions.Literal[False]) -> str | None: ...     @overload     def _traceparent(self) -> str: ...     def _traceparent(self, *, required: bool = True) -> str | None:         if self.__traceparent is None and required:  # pragma: no cover             raise exceptions.GraphRuntimeError('No span was created for this graph run')         return self.__traceparent      @property     def next_node(self) -> BaseNode[StateT, DepsT, RunEndT] | End[RunEndT]:         \"\"\"The next node that will be run in the graph.          This is the next node that will be used during async iteration, or if a node is not passed to `self.next(...)`.         \"\"\"         return self._next_node      @property     def result(self) -> GraphRunResult[StateT, RunEndT] | None:         \"\"\"The final result of the graph run if the run is completed, otherwise `None`.\"\"\"         if not isinstance(self._next_node, End):             return None  # The GraphRun has not finished running         return GraphRunResult[StateT, RunEndT](             self._next_node.data,             state=self.state,             persistence=self.persistence,             traceparent=self._traceparent(required=False),         )      async def next(         self, node: BaseNode[StateT, DepsT, RunEndT] | None = None     ) -> BaseNode[StateT, DepsT, RunEndT] | End[RunEndT]:         \"\"\"Manually drive the graph run by passing in the node you want to run next.          This lets you inspect or mutate the node before continuing execution, or skip certain nodes         under dynamic conditions. The graph run should stop when you return an [`End`][pydantic_graph.nodes.End] node.          Here's an example of using `next` to drive the graph from [above][pydantic_graph.graph.Graph]:         ```py {title=\"next_never_42.py\" noqa=\"I001\" requires=\"never_42.py\"}         from copy import deepcopy         from pydantic_graph import End         from never_42 import Increment, MyState, never_42_graph          async def main():             state = MyState(48)             async with never_42_graph.iter(Increment(), state=state) as graph_run:                 next_node = graph_run.next_node  # start with the first node                 node_states = [(next_node, deepcopy(graph_run.state))]                  while not isinstance(next_node, End):                     if graph_run.state.number == 50:                         graph_run.state.number = 42                     next_node = await graph_run.next(next_node)                     node_states.append((next_node, deepcopy(graph_run.state)))                  print(node_states)                 '''                 [                     (Increment(), MyState(number=48)),                     (Check42(), MyState(number=49)),                     (End(data=49), MyState(number=49)),                 ]                 '''         ```          Args:             node: The node to run next in the graph. If not specified, uses `self.next_node`, which is initialized to                 the `start_node` of the run and updated each time a new node is returned.          Returns:             The next node returned by the graph logic, or an [`End`][pydantic_graph.nodes.End] node if             the run has completed.         \"\"\"         if node is None:             # This cast is necessary because self._next_node could be an `End`. You'll get a runtime error if that's             # the case, but if it is, the only way to get there would be to have tried calling next manually after             # the run finished. Either way, maybe it would be better to not do this cast...             node = cast(BaseNode[StateT, DepsT, RunEndT], self._next_node)             node_snapshot_id = node.get_snapshot_id()         else:             node_snapshot_id = node.get_snapshot_id()          if node_snapshot_id != self._snapshot_id:             await self.persistence.snapshot_node_if_new(node_snapshot_id, self.state, node)             self._snapshot_id = node_snapshot_id          if not isinstance(node, BaseNode):             # While technically this is not compatible with the documented method signature, it's an easy mistake to             # make, and we should eagerly provide a more helpful error message than you'd get otherwise.             raise TypeError(f'`next` must be called with a `BaseNode` instance, got {node!r}.')          node_id = node.get_node_id()         if node_id not in self.graph.node_defs:             raise exceptions.GraphRuntimeError(f'Node `{node}` is not in the graph.')          with ExitStack() as stack:             if self.graph.auto_instrument:  # pragma: no branch                 # Separate variable because we actually don't want logfire's f-string magic here,                 # we want the span_name to be preformatted for other backends                 # as requested in https://github.com/pydantic/pydantic-ai/issues/3173.                 span_name = f'run node {node_id}'                 stack.enter_context(logfire_span(span_name, node_id=node_id, node=node))              async with self.persistence.record_run(node_snapshot_id):                 ctx = GraphRunContext(state=self.state, deps=self.deps)                 self._next_node = await node.run(ctx)          if isinstance(self._next_node, End):             self._snapshot_id = self._next_node.get_snapshot_id()             await self.persistence.snapshot_end(self.state, self._next_node)         elif isinstance(self._next_node, BaseNode):             self._snapshot_id = self._next_node.get_snapshot_id()             await self.persistence.snapshot_node(self.state, self._next_node)         else:             raise exceptions.GraphRuntimeError(                 f'Invalid node return type: `{type(self._next_node).__name__}`. Expected `BaseNode` or `End`.'             )          return self._next_node      def __aiter__(self) -> AsyncIterator[BaseNode[StateT, DepsT, RunEndT] | End[RunEndT]]:         return self      async def __anext__(self) -> BaseNode[StateT, DepsT, RunEndT] | End[RunEndT]:         \"\"\"Use the last returned node as the input to `Graph.next`.\"\"\"         if not self._is_started:             self._is_started = True             return self._next_node          if isinstance(self._next_node, End):             raise StopAsyncIteration          return await self.next(self._next_node)      def __repr__(self) -> str:         return f'<GraphRun graph={self.graph.name or \"[unnamed]\"}>' ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/graph/index.html#graphrun", "page": "pydantic_graph/graph/index.html", "source_site": "pydantic_ai"}
{"title": "GraphRun", "anchor": "graphrun", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    *,\n    graph: Graph[StateT, DepsT, RunEndT],\n    start_node: BaseNode[StateT, DepsT, RunEndT],\n    persistence: BaseStatePersistence[StateT, RunEndT],\n    state: StateT,\n    deps: DepsT,\n    traceparent: str | None,\n    snapshot_id: str | None = None\n)\n```\n\nCreate a new run for a given graph, starting at the specified node.\n\nTypically, you'll use [`Graph.iter`](index.html#pydantic_graph.graph.Graph.iter) rather than calling this directly.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `graph` | `Graph[StateT, DepsT, RunEndT]` | The [`Graph`](index.html#pydantic_graph.graph.Graph) to run. | *required* |\n| `start_node` | `BaseNode[StateT, DepsT, RunEndT]` | The node where execution will begin. | *required* |\n| `persistence` | `BaseStatePersistence[StateT, RunEndT]` | State persistence interface. | *required* |\n| `state` | `StateT` | A shared state object or primitive (like a counter, dataclass, etc.) that is available to all nodes via `ctx.state`. | *required* |\n| `deps` | `DepsT` | Optional dependencies that each node can access via `ctx.deps`, e.g. database connections, configuration, or logging clients. | *required* |\n| `traceparent` | `str | None` | The traceparent for the span used for the graph run. | *required* |\n| `snapshot_id` | `str | None` | The ID of the snapshot the node came from. | `None` |\n\nSource code in `pydantic_graph/pydantic_graph/graph.py`\n\n|  |  |\n| --- | --- |\n| ``` 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 ``` | ``` def __init__(     self,     *,     graph: Graph[StateT, DepsT, RunEndT],     start_node: BaseNode[StateT, DepsT, RunEndT],     persistence: BaseStatePersistence[StateT, RunEndT],     state: StateT,     deps: DepsT,     traceparent: str | None,     snapshot_id: str | None = None, ):     \"\"\"Create a new run for a given graph, starting at the specified node.      Typically, you'll use [`Graph.iter`][pydantic_graph.graph.Graph.iter] rather than calling this directly.      Args:         graph: The [`Graph`][pydantic_graph.graph.Graph] to run.         start_node: The node where execution will begin.         persistence: State persistence interface.         state: A shared state object or primitive (like a counter, dataclass, etc.) that is available             to all nodes via `ctx.state`.         deps: Optional dependencies that each node can access via `ctx.deps`, e.g. database connections,             configuration, or logging clients.         traceparent: The traceparent for the span used for the graph run.         snapshot_id: The ID of the snapshot the node came from.     \"\"\"     self.graph = graph     self.persistence = persistence     self._snapshot_id: str | None = snapshot_id     self.state = state     self.deps = deps      self.__traceparent = traceparent     self._next_node: BaseNode[StateT, DepsT, RunEndT] | End[RunEndT] = start_node     self._is_started: bool = False ``` |\n\n#### next\\_node `property`\n\n```\nnext_node: BaseNode[StateT, DepsT, RunEndT] | End[RunEndT]\n```\n\nThe next node that will be run in the graph.\n\nThis is the next node that will be used during async iteration, or if a node is not passed to `self.next(...)`.\n\n#### result `property`\n\n```\nresult: GraphRunResult[StateT, RunEndT] | None\n```\n\nThe final result of the graph run if the run is completed, otherwise `None`.\n\n#### next `async`\n\n```\nnext(\n    node: BaseNode[StateT, DepsT, RunEndT] | None = None,\n) -> BaseNode[StateT, DepsT, RunEndT] | End[RunEndT]\n```\n\nManually drive the graph run by passing in the node you want to run next.\n\nThis lets you inspect or mutate the node before continuing execution, or skip certain nodes\nunder dynamic conditions. The graph run should stop when you return an [`End`](../nodes/index.html#pydantic_graph.nodes.End) node.\n\nHere's an example of using `next` to drive the graph from [above](index.html#pydantic_graph.graph.Graph):\n\nnext\\_never\\_42.py\n\n```\nfrom copy import deepcopy\nfrom pydantic_graph import End\nfrom never_42 import Increment, MyState, never_42_graph", "url": "https://ai.pydantic.dev/pydantic_graph/graph/index.html#graphrun", "page": "pydantic_graph/graph/index.html", "source_site": "pydantic_ai"}
{"title": "GraphRun", "anchor": "graphrun", "md_text": "async def main():\n    state = MyState(48)\n    async with never_42_graph.iter(Increment(), state=state) as graph_run:\n        next_node = graph_run.next_node  # start with the first node\n        node_states = [(next_node, deepcopy(graph_run.state))]\n\n        while not isinstance(next_node, End):\n            if graph_run.state.number == 50:\n                graph_run.state.number = 42\n            next_node = await graph_run.next(next_node)\n            node_states.append((next_node, deepcopy(graph_run.state)))\n\n        print(node_states)\n        '''\n        [\n            (Increment(), MyState(number=48)),\n            (Check42(), MyState(number=49)),\n            (End(data=49), MyState(number=49)),\n        ]\n        '''\n```\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `node` | `BaseNode[StateT, DepsT, RunEndT] | None` | The node to run next in the graph. If not specified, uses `self.next_node`, which is initialized to the `start_node` of the run and updated each time a new node is returned. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `BaseNode[StateT, DepsT, RunEndT] | End[RunEndT]` | The next node returned by the graph logic, or an [`End`](../nodes/index.html#pydantic_graph.nodes.End) node if |\n| `BaseNode[StateT, DepsT, RunEndT] | End[RunEndT]` | the run has completed. |\n\nSource code in `pydantic_graph/pydantic_graph/graph.py`", "url": "https://ai.pydantic.dev/pydantic_graph/graph/index.html#graphrun", "page": "pydantic_graph/graph/index.html", "source_site": "pydantic_ai"}
{"title": "GraphRun", "anchor": "graphrun", "md_text": "|  |  |\n| --- | --- |\n| ``` 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 ``` | ``` async def next(     self, node: BaseNode[StateT, DepsT, RunEndT] | None = None ) -> BaseNode[StateT, DepsT, RunEndT] | End[RunEndT]:     \"\"\"Manually drive the graph run by passing in the node you want to run next.      This lets you inspect or mutate the node before continuing execution, or skip certain nodes     under dynamic conditions. The graph run should stop when you return an [`End`][pydantic_graph.nodes.End] node.      Here's an example of using `next` to drive the graph from [above][pydantic_graph.graph.Graph]:     ```py {title=\"next_never_42.py\" noqa=\"I001\" requires=\"never_42.py\"}     from copy import deepcopy     from pydantic_graph import End     from never_42 import Increment, MyState, never_42_graph      async def main():         state = MyState(48)         async with never_42_graph.iter(Increment(), state=state) as graph_run:             next_node = graph_run.next_node  # start with the first node             node_states = [(next_node, deepcopy(graph_run.state))]              while not isinstance(next_node, End):                 if graph_run.state.number == 50:                     graph_run.state.number = 42                 next_node = await graph_run.next(next_node)                 node_states.append((next_node, deepcopy(graph_run.state)))              print(node_states)             '''             [                 (Increment(), MyState(number=48)),                 (Check42(), MyState(number=49)),                 (End(data=49), MyState(number=49)),             ]             '''     ```      Args:         node: The node to run next in the graph. If not specified, uses `self.next_node`, which is initialized to             the `start_node` of the run and updated each time a new node is returned.      Returns:         The next node returned by the graph logic, or an [`End`][pydantic_graph.nodes.End] node if         the run has completed.     \"\"\"     if node is None:         # This cast is necessary because self._next_node could be an `End`. You'll get a runtime error if that's         # the case, but if it is, the only way to get there would be to have tried calling next manually after         # the run finished. Either way, maybe it would be better to not do this cast...         node = cast(BaseNode[StateT, DepsT, RunEndT], self._next_node)         node_snapshot_id = node.get_snapshot_id()     else:         node_snapshot_id = node.get_snapshot_id()      if node_snapshot_id != self._snapshot_id:         await self.persistence.snapshot_node_if_new(node_snapshot_id, self.state, node)         self._snapshot_id = node_snapshot_id      if not isinstance(node, BaseNode):         # While technically this is not compatible with the documented method signature, it's an easy mistake to         # make, and we should eagerly provide a more helpful error message than you'd get otherwise.         raise TypeError(f'`next` must be called with a `BaseNode` instance, got {node!r}.')      node_id = node.get_node_id()     if node_id not in self.graph.node_defs:         raise exceptions.GraphRuntimeError(f'Node `{node}` is not in the graph.')      with ExitStack() as stack:         if self.graph.auto_instrument:  # pragma: no branch             # Separate variable because we actually don't want logfire's f-string magic here,             # we want the span_name to be preformatted for other backends             # as requested in https://github.com/pydantic/pydantic-ai/issues/3173.             span_name = f'run node {node_id}'             stack.enter_context(logfire_span(span_name, node_id=node_id, node=node))          async with self.persistence.record_run(node_snapshot_id):             ctx = GraphRunContext(state=self.state, deps=self.deps)             self._next_node = await node.run(ctx)      if isinstance(self._next_node, End):         self._snapshot_id = self._next_node.get_snapshot_id()         await self.persistence.snapshot_end(self.state, self._next_node)     elif isinstance(self._next_node, BaseNode):         self._snapshot_id = self._next_node.get_snapshot_id()         await self.persistence.snapshot_node(self.state, self._next_node)     else:         raise exceptions.GraphRuntimeError(             f'Invalid node return type: `{type(self._next_node).__name__}`. Expected `BaseNode` or `End`.'         )      return self._next_node ``` |\n\n#### \\_\\_anext\\_\\_ `async`", "url": "https://ai.pydantic.dev/pydantic_graph/graph/index.html#graphrun", "page": "pydantic_graph/graph/index.html", "source_site": "pydantic_ai"}
{"title": "GraphRun", "anchor": "graphrun", "md_text": "```\n__anext__() -> (\n    BaseNode[StateT, DepsT, RunEndT] | End[RunEndT]\n)\n```\n\nUse the last returned node as the input to `Graph.next`.\n\nSource code in `pydantic_graph/pydantic_graph/graph.py`\n\n|  |  |\n| --- | --- |\n| ``` 757 758 759 760 761 762 763 764 765 766 ``` | ``` async def __anext__(self) -> BaseNode[StateT, DepsT, RunEndT] | End[RunEndT]:     \"\"\"Use the last returned node as the input to `Graph.next`.\"\"\"     if not self._is_started:         self._is_started = True         return self._next_node      if isinstance(self._next_node, End):         raise StopAsyncIteration      return await self.next(self._next_node) ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/graph/index.html#graphrun", "page": "pydantic_graph/graph/index.html", "source_site": "pydantic_ai"}
{"title": "GraphRunResult `dataclass`", "anchor": "graphrunresult-dataclass", "md_text": "Bases: `Generic[StateT, RunEndT]`\n\nThe final result of running a graph.\n\nSource code in `pydantic_graph/pydantic_graph/graph.py`\n\n|  |  |\n| --- | --- |\n| ``` 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 ``` | ``` @dataclass(init=False) class GraphRunResult(Generic[StateT, RunEndT]):     \"\"\"The final result of running a graph.\"\"\"      output: RunEndT     state: StateT     persistence: BaseStatePersistence[StateT, RunEndT] = field(repr=False)      def __init__(         self,         output: RunEndT,         state: StateT,         persistence: BaseStatePersistence[StateT, RunEndT],         traceparent: str | None = None,     ):         self.output = output         self.state = state         self.persistence = persistence         self.__traceparent = traceparent      @overload     def _traceparent(self, *, required: typing_extensions.Literal[False]) -> str | None: ...     @overload     def _traceparent(self) -> str: ...     def _traceparent(self, *, required: bool = True) -> str | None:  # pragma: no cover         if self.__traceparent is None and required:             raise exceptions.GraphRuntimeError('No span was created for this graph run.')         return self.__traceparent ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/graph/index.html#graphrunresult-dataclass", "page": "pydantic_graph/graph/index.html", "source_site": "pydantic_ai"}
{"title": "JoinState `dataclass`", "anchor": "joinstate-dataclass", "md_text": "The state of a join during graph execution associated to a particular fork run.\n\nSource code in `pydantic_graph/pydantic_graph/beta/join.py`\n\n|  |  |\n| --- | --- |\n| ``` 31 32 33 34 35 36 37 ``` | ``` @dataclass class JoinState:     \"\"\"The state of a join during graph execution associated to a particular fork run.\"\"\"      current: Any     downstream_fork_stack: ForkStack     cancelled_sibling_tasks: bool = False ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta_join/index.html#joinstate-dataclass", "page": "pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "ReducerContext `dataclass`", "anchor": "reducercontext-dataclass", "md_text": "Bases: `Generic[StateT, DepsT]`\n\nContext information passed to reducer functions during graph execution.\n\nThe reducer context provides access to the current graph state and dependencies.\n\nType Parameters\n\nStateT: The type of the graph state\nDepsT: The type of the dependencies\n\n\nSource code in `pydantic_graph/pydantic_graph/beta/join.py`\n\n|  |  |\n| --- | --- |\n| ``` 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 ``` | ``` @dataclass(init=False) class ReducerContext(Generic[StateT, DepsT]):     \"\"\"Context information passed to reducer functions during graph execution.      The reducer context provides access to the current graph state and dependencies.      Type Parameters:         StateT: The type of the graph state         DepsT: The type of the dependencies     \"\"\"      _state: StateT     \"\"\"The current graph state.\"\"\"     _deps: DepsT     \"\"\"The dependencies of the current graph run.\"\"\"     _join_state: JoinState     \"\"\"The JoinState for this reducer context.\"\"\"      def __init__(self, *, state: StateT, deps: DepsT, join_state: JoinState):         self._state = state         self._deps = deps         self._join_state = join_state      @property     def state(self) -> StateT:         \"\"\"The state of the graph run.\"\"\"         return self._state      @property     def deps(self) -> DepsT:         \"\"\"The deps for the graph run.\"\"\"         return self._deps      def cancel_sibling_tasks(self):         \"\"\"Cancel all sibling tasks created from the same fork.          You can call this if you want your join to have early-stopping behavior.         \"\"\"         self._join_state.cancelled_sibling_tasks = True ``` |\n\n#### state `property`\n\n```\nstate: StateT\n```\n\nThe state of the graph run.\n\n#### deps `property`\n\n```\ndeps: DepsT\n```\n\nThe deps for the graph run.\n\n#### cancel\\_sibling\\_tasks\n\n```\ncancel_sibling_tasks()\n```\n\nCancel all sibling tasks created from the same fork.\n\nYou can call this if you want your join to have early-stopping behavior.\n\nSource code in `pydantic_graph/pydantic_graph/beta/join.py`\n\n|  |  |\n| --- | --- |\n| ``` 73 74 75 76 77 78 ``` | ``` def cancel_sibling_tasks(self):     \"\"\"Cancel all sibling tasks created from the same fork.      You can call this if you want your join to have early-stopping behavior.     \"\"\"     self._join_state.cancelled_sibling_tasks = True ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta_join/index.html#reducercontext-dataclass", "page": "pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "ReducerFunction `module-attribute`", "anchor": "reducerfunction-module-attribute", "md_text": "```\nReducerFunction = TypeAliasType(\n    \"ReducerFunction\",\n    ContextReducerFunction[StateT, DepsT, InputT, OutputT]\n    | PlainReducerFunction[InputT, OutputT],\n    type_params=(StateT, DepsT, InputT, OutputT),\n)\n```\n\nA function used for reducing inputs to a join node.", "url": "https://ai.pydantic.dev/pydantic_graph/beta_join/index.html#reducerfunction-module-attribute", "page": "pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "reduce\\_null", "anchor": "reducenull", "md_text": "```\nreduce_null(current: None, inputs: Any) -> None\n```\n\nA reducer that discards all input data and returns None.\n\nSource code in `pydantic_graph/pydantic_graph/beta/join.py`\n\n|  |  |\n| --- | --- |\n| ``` 101 102 103 ``` | ``` def reduce_null(current: None, inputs: Any) -> None:     \"\"\"A reducer that discards all input data and returns None.\"\"\"     return None ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta_join/index.html#reducenull", "page": "pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "reduce\\_list\\_append", "anchor": "reducelistappend", "md_text": "```\nreduce_list_append(\n    current: list[T], inputs: T\n) -> list[T]\n```\n\nA reducer that appends to a list.\n\nSource code in `pydantic_graph/pydantic_graph/beta/join.py`\n\n|  |  |\n| --- | --- |\n| ``` 106 107 108 109 ``` | ``` def reduce_list_append(current: list[T], inputs: T) -> list[T]:     \"\"\"A reducer that appends to a list.\"\"\"     current.append(inputs)     return current ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta_join/index.html#reducelistappend", "page": "pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "reduce\\_list\\_extend", "anchor": "reducelistextend", "md_text": "```\nreduce_list_extend(\n    current: list[T], inputs: Iterable[T]\n) -> list[T]\n```\n\nA reducer that extends a list.\n\nSource code in `pydantic_graph/pydantic_graph/beta/join.py`\n\n|  |  |\n| --- | --- |\n| ``` 112 113 114 115 ``` | ``` def reduce_list_extend(current: list[T], inputs: Iterable[T]) -> list[T]:     \"\"\"A reducer that extends a list.\"\"\"     current.extend(inputs)     return current ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta_join/index.html#reducelistextend", "page": "pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "reduce\\_dict\\_update", "anchor": "reducedictupdate", "md_text": "```\nreduce_dict_update(\n    current: dict[K, V], inputs: Mapping[K, V]\n) -> dict[K, V]\n```\n\nA reducer that updates a dict.\n\nSource code in `pydantic_graph/pydantic_graph/beta/join.py`\n\n|  |  |\n| --- | --- |\n| ``` 118 119 120 121 ``` | ``` def reduce_dict_update(current: dict[K, V], inputs: Mapping[K, V]) -> dict[K, V]:     \"\"\"A reducer that updates a dict.\"\"\"     current.update(inputs)     return current ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta_join/index.html#reducedictupdate", "page": "pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "SupportsSum", "anchor": "supportssum", "md_text": "Bases: `Protocol`\n\nA protocol for a type that supports adding to itself.\n\nSource code in `pydantic_graph/pydantic_graph/beta/join.py`\n\n|  |  |\n| --- | --- |\n| ``` 124 125 126 127 128 129 ``` | ``` class SupportsSum(Protocol):     \"\"\"A protocol for a type that supports adding to itself.\"\"\"      @abstractmethod     def __add__(self, other: Self, /) -> Self:         pass ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta_join/index.html#supportssum", "page": "pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "reduce\\_sum", "anchor": "reducesum", "md_text": "```\nreduce_sum(current: NumericT, inputs: NumericT) -> NumericT\n```\n\nA reducer that sums numbers.\n\nSource code in `pydantic_graph/pydantic_graph/beta/join.py`\n\n|  |  |\n| --- | --- |\n| ``` 135 136 137 ``` | ``` def reduce_sum(current: NumericT, inputs: NumericT) -> NumericT:     \"\"\"A reducer that sums numbers.\"\"\"     return current + inputs ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta_join/index.html#reducesum", "page": "pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "ReduceFirstValue `dataclass`", "anchor": "reducefirstvalue-dataclass", "md_text": "Bases: `Generic[T]`\n\nA reducer that returns the first value it encounters, and cancels all other tasks.\n\nSource code in `pydantic_graph/pydantic_graph/beta/join.py`\n\n|  |  |\n| --- | --- |\n| ``` 140 141 142 143 144 145 146 147 ``` | ``` @dataclass class ReduceFirstValue(Generic[T]):     \"\"\"A reducer that returns the first value it encounters, and cancels all other tasks.\"\"\"      def __call__(self, ctx: ReducerContext[object, object], current: T, inputs: T) -> T:         \"\"\"The reducer function.\"\"\"         ctx.cancel_sibling_tasks()         return inputs ``` |\n\n#### \\_\\_call\\_\\_\n\n```\n__call__(\n    ctx: ReducerContext[object, object],\n    current: T,\n    inputs: T,\n) -> T\n```\n\nThe reducer function.\n\nSource code in `pydantic_graph/pydantic_graph/beta/join.py`\n\n|  |  |\n| --- | --- |\n| ``` 144 145 146 147 ``` | ``` def __call__(self, ctx: ReducerContext[object, object], current: T, inputs: T) -> T:     \"\"\"The reducer function.\"\"\"     ctx.cancel_sibling_tasks()     return inputs ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta_join/index.html#reducefirstvalue-dataclass", "page": "pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "Join `dataclass`", "anchor": "join-dataclass", "md_text": "Bases: `Generic[StateT, DepsT, InputT, OutputT]`\n\nA join operation that synchronizes and aggregates parallel execution paths.\n\nA join defines how to combine outputs from multiple parallel execution paths\nusing a [`ReducerFunction`](index.html#pydantic_graph.beta.join.ReducerFunction). It specifies which fork\nit joins (if any) and manages the initialization of reducers.\n\nType Parameters\n\nStateT: The type of the graph state\nDepsT: The type of the dependencies\nInputT: The type of input data to join\nOutputT: The type of the final joined output\n\n\nSource code in `pydantic_graph/pydantic_graph/beta/join.py`\n\n|  |  |\n| --- | --- |\n| ``` 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 ``` | ``` @dataclass(init=False) class Join(Generic[StateT, DepsT, InputT, OutputT]):     \"\"\"A join operation that synchronizes and aggregates parallel execution paths.      A join defines how to combine outputs from multiple parallel execution paths     using a [`ReducerFunction`][pydantic_graph.beta.join.ReducerFunction]. It specifies which fork     it joins (if any) and manages the initialization of reducers.      Type Parameters:         StateT: The type of the graph state         DepsT: The type of the dependencies         InputT: The type of input data to join         OutputT: The type of the final joined output     \"\"\"      id: JoinID     _reducer: ReducerFunction[StateT, DepsT, InputT, OutputT]     _initial_factory: Callable[[], OutputT]     parent_fork_id: ForkID | None     preferred_parent_fork: Literal['closest', 'farthest']      def __init__(         self,         *,         id: JoinID,         reducer: ReducerFunction[StateT, DepsT, InputT, OutputT],         initial_factory: Callable[[], OutputT],         parent_fork_id: ForkID | None = None,         preferred_parent_fork: Literal['farthest', 'closest'] = 'farthest',     ):         self.id = id         self._reducer = reducer         self._initial_factory = initial_factory         self.parent_fork_id = parent_fork_id         self.preferred_parent_fork = preferred_parent_fork      @property     def reducer(self):         return self._reducer      @property     def initial_factory(self):         return self._initial_factory      def reduce(self, ctx: ReducerContext[StateT, DepsT], current: OutputT, inputs: InputT) -> OutputT:         n_parameters = len(inspect.signature(self.reducer).parameters)         if n_parameters == 2:             return cast(PlainReducerFunction[InputT, OutputT], self.reducer)(current, inputs)         else:             return cast(ContextReducerFunction[StateT, DepsT, InputT, OutputT], self.reducer)(ctx, current, inputs)      @overload     def as_node(self, inputs: None = None) -> JoinNode[StateT, DepsT]: ...      @overload     def as_node(self, inputs: InputT) -> JoinNode[StateT, DepsT]: ...      def as_node(self, inputs: InputT | None = None) -> JoinNode[StateT, DepsT]:         \"\"\"Create a step node with bound inputs.          Args:             inputs: The input data to bind to this step, or None          Returns:             A [`StepNode`][pydantic_graph.beta.step.StepNode] with this step and the bound inputs         \"\"\"         return JoinNode(self, inputs) ``` |\n\n#### as\\_node\n\n```\nas_node(inputs: None = None) -> JoinNode[StateT, DepsT]\n\nas_node(inputs: InputT) -> JoinNode[StateT, DepsT]\n\nas_node(\n    inputs: InputT | None = None,\n) -> JoinNode[StateT, DepsT]\n```\n\nCreate a step node with bound inputs.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `inputs` | `InputT | None` | The input data to bind to this step, or None | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `JoinNode[StateT, DepsT]` | A [`StepNode`](../beta_step/index.html#pydantic_graph.beta.step.StepNode) with this step and the bound inputs |\n\nSource code in `pydantic_graph/pydantic_graph/beta/join.py`", "url": "https://ai.pydantic.dev/pydantic_graph/beta_join/index.html#join-dataclass", "page": "pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "Join `dataclass`", "anchor": "join-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 207 208 209 210 211 212 213 214 215 216 ``` | ``` def as_node(self, inputs: InputT | None = None) -> JoinNode[StateT, DepsT]:     \"\"\"Create a step node with bound inputs.      Args:         inputs: The input data to bind to this step, or None      Returns:         A [`StepNode`][pydantic_graph.beta.step.StepNode] with this step and the bound inputs     \"\"\"     return JoinNode(self, inputs) ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta_join/index.html#join-dataclass", "page": "pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "JoinNode `dataclass`", "anchor": "joinnode-dataclass", "md_text": "Bases: `BaseNode[StateT, DepsT, Any]`\n\nA base node that represents a join item with bound inputs.\n\nJoinNode bridges between the v1 and v2 graph execution systems by wrapping\na [`Join`](index.html#pydantic_graph.beta.join.Join) with bound inputs in a BaseNode interface.\nIt is not meant to be run directly but rather used to indicate transitions\nto v2-style steps.\n\nSource code in `pydantic_graph/pydantic_graph/beta/join.py`\n\n|  |  |\n| --- | --- |\n| ``` 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 ``` | ``` @dataclass class JoinNode(BaseNode[StateT, DepsT, Any]):     \"\"\"A base node that represents a join item with bound inputs.      JoinNode bridges between the v1 and v2 graph execution systems by wrapping     a [`Join`][pydantic_graph.beta.join.Join] with bound inputs in a BaseNode interface.     It is not meant to be run directly but rather used to indicate transitions     to v2-style steps.     \"\"\"      join: Join[StateT, DepsT, Any, Any]     \"\"\"The step to execute.\"\"\"      inputs: Any     \"\"\"The inputs bound to this step.\"\"\"      async def run(self, ctx: GraphRunContext[StateT, DepsT]) -> BaseNode[StateT, DepsT, Any] | End[Any]:         \"\"\"Attempt to run the join node.          Args:             ctx: The graph execution context          Returns:             The result of step execution          Raises:             NotImplementedError: Always raised as StepNode is not meant to be run directly         \"\"\"         raise NotImplementedError(             '`JoinNode` is not meant to be run directly, it is meant to be used in `BaseNode` subclasses to indicate a transition to v2-style steps.'         ) ``` |\n\n#### join `instance-attribute`\n\n```\njoin: Join[StateT, DepsT, Any, Any]\n```\n\nThe step to execute.\n\n#### inputs `instance-attribute`\n\n```\ninputs: Any\n```\n\nThe inputs bound to this step.\n\n#### run `async`\n\n```\nrun(\n    ctx: GraphRunContext[StateT, DepsT],\n) -> BaseNode[StateT, DepsT, Any] | End[Any]\n```\n\nAttempt to run the join node.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `ctx` | `GraphRunContext[StateT, DepsT]` | The graph execution context | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `BaseNode[StateT, DepsT, Any] | End[Any]` | The result of step execution |\n\nRaises:\n\n| Type | Description |\n| --- | --- |\n| `NotImplementedError` | Always raised as StepNode is not meant to be run directly |\n\nSource code in `pydantic_graph/pydantic_graph/beta/join.py`\n\n|  |  |\n| --- | --- |\n| ``` 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 ``` | ``` async def run(self, ctx: GraphRunContext[StateT, DepsT]) -> BaseNode[StateT, DepsT, Any] | End[Any]:     \"\"\"Attempt to run the join node.      Args:         ctx: The graph execution context      Returns:         The result of step execution      Raises:         NotImplementedError: Always raised as StepNode is not meant to be run directly     \"\"\"     raise NotImplementedError(         '`JoinNode` is not meant to be run directly, it is meant to be used in `BaseNode` subclasses to indicate a transition to v2-style steps.'     ) ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta_join/index.html#joinnode-dataclass", "page": "pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "GraphSetupError", "anchor": "graphsetuperror", "md_text": "Bases: `TypeError`\n\nError caused by an incorrectly configured graph.\n\nSource code in `pydantic_graph/pydantic_graph/exceptions.py`\n\n|  |  |\n| --- | --- |\n| ```  7  8  9 10 11 12 13 14 15 ``` | ``` class GraphSetupError(TypeError):     \"\"\"Error caused by an incorrectly configured graph.\"\"\"      message: str     \"\"\"Description of the mistake.\"\"\"      def __init__(self, message: str):         self.message = message         super().__init__(message) ``` |\n\n#### message `instance-attribute`\n\n```\nmessage: str = message\n```\n\nDescription of the mistake.", "url": "https://ai.pydantic.dev/pydantic_graph/exceptions/index.html#graphsetuperror", "page": "pydantic_graph/exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "GraphBuildingError", "anchor": "graphbuildingerror", "md_text": "Bases: `ValueError`\n\nAn error raised during graph-building.\n\nSource code in `pydantic_graph/pydantic_graph/exceptions.py`\n\n|  |  |\n| --- | --- |\n| ``` 18 19 20 21 22 23 24 25 26 ``` | ``` class GraphBuildingError(ValueError):     \"\"\"An error raised during graph-building.\"\"\"      message: str     \"\"\"The error message.\"\"\"      def __init__(self, message: str):         self.message = message         super().__init__(message) ``` |\n\n#### message `instance-attribute`\n\n```\nmessage: str = message\n```\n\nThe error message.", "url": "https://ai.pydantic.dev/pydantic_graph/exceptions/index.html#graphbuildingerror", "page": "pydantic_graph/exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "GraphValidationError", "anchor": "graphvalidationerror", "md_text": "Bases: `ValueError`\n\nAn error raised during graph validation.\n\nSource code in `pydantic_graph/pydantic_graph/exceptions.py`\n\n|  |  |\n| --- | --- |\n| ``` 29 30 31 32 33 34 35 36 37 ``` | ``` class GraphValidationError(ValueError):     \"\"\"An error raised during graph validation.\"\"\"      message: str     \"\"\"The error message.\"\"\"      def __init__(self, message: str):         self.message = message         super().__init__(message) ``` |\n\n#### message `instance-attribute`\n\n```\nmessage: str = message\n```\n\nThe error message.", "url": "https://ai.pydantic.dev/pydantic_graph/exceptions/index.html#graphvalidationerror", "page": "pydantic_graph/exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "GraphRuntimeError", "anchor": "graphruntimeerror", "md_text": "Bases: `RuntimeError`\n\nError caused by an issue during graph execution.\n\nSource code in `pydantic_graph/pydantic_graph/exceptions.py`\n\n|  |  |\n| --- | --- |\n| ``` 40 41 42 43 44 45 46 47 48 ``` | ``` class GraphRuntimeError(RuntimeError):     \"\"\"Error caused by an issue during graph execution.\"\"\"      message: str     \"\"\"The error message.\"\"\"      def __init__(self, message: str):         self.message = message         super().__init__(message) ``` |\n\n#### message `instance-attribute`\n\n```\nmessage: str = message\n```\n\nThe error message.", "url": "https://ai.pydantic.dev/pydantic_graph/exceptions/index.html#graphruntimeerror", "page": "pydantic_graph/exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "GraphNodeStatusError", "anchor": "graphnodestatuserror", "md_text": "Bases: `GraphRuntimeError`\n\nError caused by trying to run a node that already has status `'running'`, `'success'`, or `'error'`.\n\nSource code in `pydantic_graph/pydantic_graph/exceptions.py`\n\n|  |  |\n| --- | --- |\n| ``` 51 52 53 54 55 56 57 58 59 60 61 62 ``` | ``` class GraphNodeStatusError(GraphRuntimeError):     \"\"\"Error caused by trying to run a node that already has status `'running'`, `'success'`, or `'error'`.\"\"\"      def __init__(self, actual_status: 'SnapshotStatus'):         self.actual_status = actual_status         super().__init__(f\"Incorrect snapshot status {actual_status!r}, must be 'created' or 'pending'.\")      @classmethod     def check(cls, status: 'SnapshotStatus') -> None:         \"\"\"Check if the status is valid.\"\"\"         if status not in {'created', 'pending'}:             raise cls(status) ``` |\n\n#### check `classmethod`\n\n```\ncheck(status: SnapshotStatus) -> None\n```\n\nCheck if the status is valid.\n\nSource code in `pydantic_graph/pydantic_graph/exceptions.py`\n\n|  |  |\n| --- | --- |\n| ``` 58 59 60 61 62 ``` | ``` @classmethod def check(cls, status: 'SnapshotStatus') -> None:     \"\"\"Check if the status is valid.\"\"\"     if status not in {'created', 'pending'}:         raise cls(status) ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/exceptions/index.html#graphnodestatuserror", "page": "pydantic_graph/exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "DEFAULT\\_HIGHLIGHT\\_CSS `module-attribute`", "anchor": "defaulthighlightcss-module-attribute", "md_text": "```\nDEFAULT_HIGHLIGHT_CSS = 'fill:#fdff32'\n```\n\nThe default CSS to use for highlighting nodes.", "url": "https://ai.pydantic.dev/pydantic_graph/mermaid/index.html#defaulthighlightcss-module-attribute", "page": "pydantic_graph/mermaid/index.html", "source_site": "pydantic_ai"}
{"title": "StateDiagramDirection `module-attribute`", "anchor": "statediagramdirection-module-attribute", "md_text": "```\nStateDiagramDirection = Literal['TB', 'LR', 'RL', 'BT']\n```\n\nUsed to specify the direction of the state diagram generated by mermaid.\n\n* `'TB'`: Top to bottom, this is the default for mermaid charts.\n* `'LR'`: Left to right\n* `'RL'`: Right to left\n* `'BT'`: Bottom to top", "url": "https://ai.pydantic.dev/pydantic_graph/mermaid/index.html#statediagramdirection-module-attribute", "page": "pydantic_graph/mermaid/index.html", "source_site": "pydantic_ai"}
{"title": "generate\\_code", "anchor": "generatecode", "md_text": "```\ngenerate_code(\n    graph: Graph[Any, Any, Any],\n    /,\n    *,\n    start_node: (\n        Sequence[NodeIdent] | NodeIdent | None\n    ) = None,\n    highlighted_nodes: (\n        Sequence[NodeIdent] | NodeIdent | None\n    ) = None,\n    highlight_css: str = DEFAULT_HIGHLIGHT_CSS,\n    title: str | None = None,\n    edge_labels: bool = True,\n    notes: bool = True,\n    direction: StateDiagramDirection | None,\n) -> str\n```\n\nGenerate [Mermaid state diagram](https://mermaid.js.org/syntax/stateDiagram.html) code for a graph.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `graph` | `Graph[Any, Any, Any]` | The graph to generate the image for. | *required* |\n| `start_node` | `Sequence[NodeIdent] | NodeIdent | None` | Identifiers of nodes that start the graph. | `None` |\n| `highlighted_nodes` | `Sequence[NodeIdent] | NodeIdent | None` | Identifiers of nodes to highlight. | `None` |\n| `highlight_css` | `str` | CSS to use for highlighting nodes. | `DEFAULT_HIGHLIGHT_CSS` |\n| `title` | `str | None` | The title of the diagram. | `None` |\n| `edge_labels` | `bool` | Whether to include edge labels in the diagram. | `True` |\n| `notes` | `bool` | Whether to include notes in the diagram. | `True` |\n| `direction` | `StateDiagramDirection | None` | The direction of flow. | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `str` | The Mermaid code for the graph. |\n\nSource code in `pydantic_graph/pydantic_graph/mermaid.py`", "url": "https://ai.pydantic.dev/pydantic_graph/mermaid/index.html#generatecode", "page": "pydantic_graph/mermaid/index.html", "source_site": "pydantic_ai"}
{"title": "generate\\_code", "anchor": "generatecode", "md_text": "|  |  |\n| --- | --- |\n| ```  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 ``` | ``` def generate_code(  # noqa: C901     graph: Graph[Any, Any, Any],     /,     *,     start_node: Sequence[NodeIdent] | NodeIdent | None = None,     highlighted_nodes: Sequence[NodeIdent] | NodeIdent | None = None,     highlight_css: str = DEFAULT_HIGHLIGHT_CSS,     title: str | None = None,     edge_labels: bool = True,     notes: bool = True,     direction: StateDiagramDirection | None, ) -> str:     \"\"\"Generate [Mermaid state diagram](https://mermaid.js.org/syntax/stateDiagram.html) code for a graph.      Args:         graph: The graph to generate the image for.         start_node: Identifiers of nodes that start the graph.         highlighted_nodes: Identifiers of nodes to highlight.         highlight_css: CSS to use for highlighting nodes.         title: The title of the diagram.         edge_labels: Whether to include edge labels in the diagram.         notes: Whether to include notes in the diagram.         direction: The direction of flow.       Returns:         The Mermaid code for the graph.     \"\"\"     start_node_ids = set(_node_ids(start_node or ()))     for node_id in start_node_ids:         if node_id not in graph.node_defs:             raise LookupError(f'Start node \"{node_id}\" is not in the graph.')      lines: list[str] = []     if title:         lines = ['---', f'title: {title}', '---']     lines.append('stateDiagram-v2')     if direction is not None:         lines.append(f'  direction {direction}')     for node_id, node_def in graph.node_defs.items():         # we use round brackets (rounded box) for nodes other than the start and end         if node_id in start_node_ids:             lines.append(f'  [*] --> {node_id}')         if node_def.returns_base_node:             for next_node_id in graph.node_defs:                 lines.append(f'  {node_id} --> {next_node_id}')         else:             for next_node_id, edge in node_def.next_node_edges.items():                 line = f'  {node_id} --> {next_node_id}'                 if edge_labels and edge.label:                     line += f': {edge.label}'                 lines.append(line)         if end_edge := node_def.end_edge:             line = f'  {node_id} --> [*]'             if edge_labels and end_edge.label:                 line += f': {end_edge.label}'             lines.append(line)          if notes and node_def.note:             lines.append(f'  note right of {node_id}')             # mermaid doesn't like multiple paragraphs in a note, and shows if so             clean_docs = re.sub('\\n{2,}', '\\n', node_def.note)             lines.append(indent(clean_docs, '    '))             lines.append('  end note')      if highlighted_nodes:         lines.append('')         lines.append(f'classDef highlighted {highlight_css}')         for node_id in _node_ids(highlighted_nodes):             if node_id not in graph.node_defs:                 raise LookupError(f'Highlighted node \"{node_id}\" is not in the graph.')             lines.append(f'class {node_id} highlighted')      return '\\n'.join(lines) ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/mermaid/index.html#generatecode", "page": "pydantic_graph/mermaid/index.html", "source_site": "pydantic_ai"}
{"title": "request\\_image", "anchor": "requestimage", "md_text": "```\nrequest_image(\n    graph: Graph[Any, Any, Any],\n    /,\n    **kwargs: Unpack[MermaidConfig],\n) -> bytes\n```\n\nGenerate an image of a Mermaid diagram using [mermaid.ink](https://mermaid.ink).\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `graph` | `Graph[Any, Any, Any]` | The graph to generate the image for. | *required* |\n| `**kwargs` | `Unpack[MermaidConfig]` | Additional parameters to configure mermaid chart generation. | `{}` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `bytes` | The image data. |\n\nSource code in `pydantic_graph/pydantic_graph/mermaid.py`\n\n|  |  |\n| --- | --- |\n| ``` 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 ``` | ``` def request_image(     graph: Graph[Any, Any, Any],     /,     **kwargs: Unpack[MermaidConfig], ) -> bytes:     \"\"\"Generate an image of a Mermaid diagram using [mermaid.ink](https://mermaid.ink).      Args:         graph: The graph to generate the image for.         **kwargs: Additional parameters to configure mermaid chart generation.      Returns:         The image data.     \"\"\"     code = generate_code(         graph,         start_node=kwargs.get('start_node'),         highlighted_nodes=kwargs.get('highlighted_nodes'),         highlight_css=kwargs.get('highlight_css', DEFAULT_HIGHLIGHT_CSS),         title=kwargs.get('title'),         edge_labels=kwargs.get('edge_labels', True),         notes=kwargs.get('notes', True),         direction=kwargs.get('direction'),     )     code_base64 = base64.b64encode(code.encode()).decode()      params: dict[str, str | float] = {}     if kwargs.get('image_type') == 'pdf':         url = f'https://mermaid.ink/pdf/{code_base64}'         if kwargs.get('pdf_fit'):             params['fit'] = ''         if kwargs.get('pdf_landscape'):             params['landscape'] = ''         if pdf_paper := kwargs.get('pdf_paper'):             params['paper'] = pdf_paper     elif kwargs.get('image_type') == 'svg':         url = f'https://mermaid.ink/svg/{code_base64}'     else:         url = f'https://mermaid.ink/img/{code_base64}'          if image_type := kwargs.get('image_type'):             params['type'] = image_type      if background_color := kwargs.get('background_color'):         params['bgColor'] = background_color     if theme := kwargs.get('theme'):         params['theme'] = theme     if width := kwargs.get('width'):         params['width'] = width     if height := kwargs.get('height'):         params['height'] = height     if scale := kwargs.get('scale'):         params['scale'] = scale      httpx_client = kwargs.get('httpx_client') or httpx.Client()     response = httpx_client.get(url, params=params)     if not response.is_success:         raise httpx.HTTPStatusError(             f'{response.status_code} error generating image:\\n{response.text}',             request=response.request,             response=response,         )     return response.content ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/mermaid/index.html#requestimage", "page": "pydantic_graph/mermaid/index.html", "source_site": "pydantic_ai"}
{"title": "save\\_image", "anchor": "saveimage", "md_text": "```\nsave_image(\n    path: Path | str,\n    graph: Graph[Any, Any, Any],\n    /,\n    **kwargs: Unpack[MermaidConfig],\n) -> None\n```\n\nGenerate an image of a Mermaid diagram using [mermaid.ink](https://mermaid.ink) and save it to a local file.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `path` | `Path | str` | The path to save the image to. | *required* |\n| `graph` | `Graph[Any, Any, Any]` | The graph to generate the image for. | *required* |\n| `**kwargs` | `Unpack[MermaidConfig]` | Additional parameters to configure mermaid chart generation. | `{}` |\n\nSource code in `pydantic_graph/pydantic_graph/mermaid.py`\n\n|  |  |\n| --- | --- |\n| ``` 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 ``` | ``` def save_image(     path: Path | str,     graph: Graph[Any, Any, Any],     /,     **kwargs: Unpack[MermaidConfig], ) -> None:     \"\"\"Generate an image of a Mermaid diagram using [mermaid.ink](https://mermaid.ink) and save it to a local file.      Args:         path: The path to save the image to.         graph: The graph to generate the image for.         **kwargs: Additional parameters to configure mermaid chart generation.     \"\"\"     if isinstance(path, str):         path = Path(path)      if 'image_type' not in kwargs:         ext = path.suffix.lower()[1:]         # no need to check for .jpeg/.jpg, as it is the default         if ext in ('png', 'webp', 'svg', 'pdf'):             kwargs['image_type'] = ext      image_data = request_image(graph, **kwargs)     path.write_bytes(image_data) ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/mermaid/index.html#saveimage", "page": "pydantic_graph/mermaid/index.html", "source_site": "pydantic_ai"}
{"title": "MermaidConfig", "anchor": "mermaidconfig", "md_text": "Bases: `TypedDict`\n\nParameters to configure mermaid chart generation.\n\nSource code in `pydantic_graph/pydantic_graph/mermaid.py`\n\n|  |  |\n| --- | --- |\n| ``` 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 ``` | ``` class MermaidConfig(TypedDict, total=False):     \"\"\"Parameters to configure mermaid chart generation.\"\"\"      start_node: Sequence[NodeIdent] | NodeIdent     \"\"\"Identifiers of nodes that start the graph.\"\"\"     highlighted_nodes: Sequence[NodeIdent] | NodeIdent     \"\"\"Identifiers of nodes to highlight.\"\"\"     highlight_css: str     \"\"\"CSS to use for highlighting nodes.\"\"\"     title: str | None     \"\"\"The title of the diagram.\"\"\"     edge_labels: bool     \"\"\"Whether to include edge labels in the diagram.\"\"\"     notes: bool     \"\"\"Whether to include notes on nodes in the diagram, defaults to true.\"\"\"     image_type: Literal['jpeg', 'png', 'webp', 'svg', 'pdf']     \"\"\"The image type to generate. If unspecified, the default behavior is `'jpeg'`.\"\"\"     pdf_fit: bool     \"\"\"When using image_type='pdf', whether to fit the diagram to the PDF page.\"\"\"     pdf_landscape: bool     \"\"\"When using image_type='pdf', whether to use landscape orientation for the PDF.      This has no effect if using `pdf_fit`.     \"\"\"     pdf_paper: Literal['letter', 'legal', 'tabloid', 'ledger', 'a0', 'a1', 'a2', 'a3', 'a4', 'a5', 'a6']     \"\"\"When using image_type='pdf', the paper size of the PDF.\"\"\"     background_color: str     \"\"\"The background color of the diagram.      If None, the default transparent background is used. The color value is interpreted as a hexadecimal color     code by default (and should not have a leading '#'), but you can also use named colors by prefixing the     value with `'!'`. For example, valid choices include `background_color='!white'` or `background_color='FF0000'`.     \"\"\"     theme: Literal['default', 'neutral', 'dark', 'forest']     \"\"\"The theme of the diagram. Defaults to 'default'.\"\"\"     width: int     \"\"\"The width of the diagram.\"\"\"     height: int     \"\"\"The height of the diagram.\"\"\"     scale: Annotated[float, Ge(1), Le(3)]     \"\"\"The scale of the diagram.      The scale must be a number between 1 and 3, and you can only set a scale if one or both of width and height are set.     \"\"\"     httpx_client: httpx.Client     \"\"\"An HTTPX client to use for requests, mostly for testing purposes.\"\"\"     direction: StateDiagramDirection     \"\"\"The direction of the state diagram.\"\"\" ``` |\n\n#### start\\_node `instance-attribute`\n\n```\nstart_node: Sequence[NodeIdent] | NodeIdent\n```\n\nIdentifiers of nodes that start the graph.\n\n#### highlighted\\_nodes `instance-attribute`\n\n```\nhighlighted_nodes: Sequence[NodeIdent] | NodeIdent\n```\n\nIdentifiers of nodes to highlight.\n\n#### highlight\\_css `instance-attribute`\n\n```\nhighlight_css: str\n```\n\nCSS to use for highlighting nodes.\n\n#### title `instance-attribute`\n\n```\ntitle: str | None\n```\n\nThe title of the diagram.\n\n#### edge\\_labels `instance-attribute`\n\n```\nedge_labels: bool\n```\n\nWhether to include edge labels in the diagram.\n\n#### notes `instance-attribute`\n\n```\nnotes: bool\n```\n\nWhether to include notes on nodes in the diagram, defaults to true.\n\n#### image\\_type `instance-attribute`\n\n```\nimage_type: Literal['jpeg', 'png', 'webp', 'svg', 'pdf']\n```\n\nThe image type to generate. If unspecified, the default behavior is `'jpeg'`.\n\n#### pdf\\_fit `instance-attribute`\n\n```\npdf_fit: bool\n```\n\nWhen using image\\_type='pdf', whether to fit the diagram to the PDF page.\n\n#### pdf\\_landscape `instance-attribute`\n\n```\npdf_landscape: bool\n```\n\nWhen using image\\_type='pdf', whether to use landscape orientation for the PDF.\n\nThis has no effect if using `pdf_fit`.\n\n#### pdf\\_paper `instance-attribute`\n\n```\npdf_paper: Literal[\n    \"letter\",\n    \"legal\",\n    \"tabloid\",\n    \"ledger\",\n    \"a0\",\n    \"a1\",\n    \"a2\",\n    \"a3\",\n    \"a4\",\n    \"a5\",\n    \"a6\",\n]\n```\n\nWhen using image\\_type='pdf', the paper size of the PDF.\n\n#### background\\_color `instance-attribute`\n\n```\nbackground_color: str\n```\n\nThe background color of the diagram.\n\nIf None, the default transparent background is used. The color value is interpreted as a hexadecimal color\ncode by default (and should not have a leading '#'), but you can also use named colors by prefixing the\nvalue with `'!'`. For example, valid choices include `background_color='!white'` or `background_color='FF0000'`.\n\n#### theme `instance-attribute`\n\n```\ntheme: Literal['default', 'neutral', 'dark', 'forest']\n```\n\nThe theme of the diagram. Defaults to 'default'.\n\n#### width `instance-attribute`", "url": "https://ai.pydantic.dev/pydantic_graph/mermaid/index.html#mermaidconfig", "page": "pydantic_graph/mermaid/index.html", "source_site": "pydantic_ai"}
{"title": "MermaidConfig", "anchor": "mermaidconfig", "md_text": "```\nwidth: int\n```\n\nThe width of the diagram.\n\n#### height `instance-attribute`\n\n```\nheight: int\n```\n\nThe height of the diagram.\n\n#### scale `instance-attribute`\n\n```\nscale: Annotated[float, Ge(1), Le(3)]\n```\n\nThe scale of the diagram.\n\nThe scale must be a number between 1 and 3, and you can only set a scale if one or both of width and height are set.\n\n#### httpx\\_client `instance-attribute`\n\n```\nhttpx_client: Client\n```\n\nAn HTTPX client to use for requests, mostly for testing purposes.\n\n#### direction `instance-attribute`\n\n```\ndirection: StateDiagramDirection\n```\n\nThe direction of the state diagram.", "url": "https://ai.pydantic.dev/pydantic_graph/mermaid/index.html#mermaidconfig", "page": "pydantic_graph/mermaid/index.html", "source_site": "pydantic_ai"}
{"title": "NodeIdent `module-attribute`", "anchor": "nodeident-module-attribute", "md_text": "```\nNodeIdent: TypeAlias = (\n    \"type[BaseNode[Any, Any, Any]] | BaseNode[Any, Any, Any] | str\"\n)\n```\n\nA type alias for a node identifier.\n\nThis can be:\n\n* A node instance (instance of a subclass of [`BaseNode`](../nodes/index.html#pydantic_graph.nodes.BaseNode)).\n* A node class (subclass of [`BaseNode`](../nodes/index.html#pydantic_graph.nodes.BaseNode)).\n* A string representing the node ID.", "url": "https://ai.pydantic.dev/pydantic_graph/mermaid/index.html#nodeident-module-attribute", "page": "pydantic_graph/mermaid/index.html", "source_site": "pydantic_ai"}
{"title": "StateT `module-attribute`", "anchor": "statet-module-attribute", "md_text": "```\nStateT = TypeVar('StateT', infer_variance=True)\n```\n\nType variable for graph state.", "url": "https://ai.pydantic.dev/pydantic_graph/beta_graph/index.html#statet-module-attribute", "page": "pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "DepsT `module-attribute`", "anchor": "depst-module-attribute", "md_text": "```\nDepsT = TypeVar('DepsT', infer_variance=True)\n```\n\nType variable for graph dependencies.", "url": "https://ai.pydantic.dev/pydantic_graph/beta_graph/index.html#depst-module-attribute", "page": "pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "InputT `module-attribute`", "anchor": "inputt-module-attribute", "md_text": "```\nInputT = TypeVar('InputT', infer_variance=True)\n```\n\nType variable for graph inputs.", "url": "https://ai.pydantic.dev/pydantic_graph/beta_graph/index.html#inputt-module-attribute", "page": "pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "OutputT `module-attribute`", "anchor": "outputt-module-attribute", "md_text": "```\nOutputT = TypeVar('OutputT', infer_variance=True)\n```\n\nType variable for graph outputs.", "url": "https://ai.pydantic.dev/pydantic_graph/beta_graph/index.html#outputt-module-attribute", "page": "pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "EndMarker `dataclass`", "anchor": "endmarker-dataclass", "md_text": "Bases: `Generic[OutputT]`\n\nA marker indicating the end of graph execution with a final value.\n\nEndMarker is used internally to signal that the graph has completed\nexecution and carries the final output value.\n\nType Parameters\n\nOutputT: The type of the final output value\n\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph.py`\n\n|  |  |\n| --- | --- |\n| ``` 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 ``` | ``` @dataclass(init=False) class EndMarker(Generic[OutputT]):     \"\"\"A marker indicating the end of graph execution with a final value.      EndMarker is used internally to signal that the graph has completed     execution and carries the final output value.      Type Parameters:         OutputT: The type of the final output value     \"\"\"      _value: OutputT     \"\"\"The final output value from the graph execution.\"\"\"      def __init__(self, value: OutputT):         # This manually-defined initializer is necessary due to https://github.com/python/mypy/issues/17623.         self._value = value      @property     def value(self) -> OutputT:         return self._value ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta_graph/index.html#endmarker-dataclass", "page": "pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "JoinItem `dataclass`", "anchor": "joinitem-dataclass", "md_text": "An item representing data flowing into a join operation.\n\nJoinItem carries input data from a parallel execution path to a join\nnode, along with metadata about which execution 'fork' it originated from.\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph.py`\n\n|  |  |\n| --- | --- |\n| ```  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 ``` | ``` @dataclass class JoinItem:     \"\"\"An item representing data flowing into a join operation.      JoinItem carries input data from a parallel execution path to a join     node, along with metadata about which execution 'fork' it originated from.     \"\"\"      join_id: JoinID     \"\"\"The ID of the join node this item is targeting.\"\"\"      inputs: Any     \"\"\"The input data for the join operation.\"\"\"      fork_stack: ForkStack     \"\"\"The stack of ForkStackItems that led to producing this join item.\"\"\" ``` |\n\n#### join\\_id `instance-attribute`\n\n```\njoin_id: JoinID\n```\n\nThe ID of the join node this item is targeting.\n\n#### inputs `instance-attribute`\n\n```\ninputs: Any\n```\n\nThe input data for the join operation.\n\n#### fork\\_stack `instance-attribute`\n\n```\nfork_stack: ForkStack\n```\n\nThe stack of ForkStackItems that led to producing this join item.", "url": "https://ai.pydantic.dev/pydantic_graph/beta_graph/index.html#joinitem-dataclass", "page": "pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "Graph `dataclass`", "anchor": "graph-dataclass", "md_text": "Bases: `Generic[StateT, DepsT, InputT, OutputT]`\n\nA complete graph definition ready for execution.\n\nThe Graph class represents a complete workflow graph with typed inputs,\noutputs, state, and dependencies. It contains all nodes, edges, and\nmetadata needed for execution.\n\nType Parameters\n\nStateT: The type of the graph state\nDepsT: The type of the dependencies\nInputT: The type of the input data\nOutputT: The type of the output data\n\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph.py`", "url": "https://ai.pydantic.dev/pydantic_graph/beta_graph/index.html#graph-dataclass", "page": "pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "Graph `dataclass`", "anchor": "graph-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 ``` | ``` @dataclass(repr=False) class Graph(Generic[StateT, DepsT, InputT, OutputT]):     \"\"\"A complete graph definition ready for execution.      The Graph class represents a complete workflow graph with typed inputs,     outputs, state, and dependencies. It contains all nodes, edges, and     metadata needed for execution.      Type Parameters:         StateT: The type of the graph state         DepsT: The type of the dependencies         InputT: The type of the input data         OutputT: The type of the output data     \"\"\"      name: str | None     \"\"\"Optional name for the graph, if not provided the name will be inferred from the calling frame on the first call to a graph method.\"\"\"      state_type: type[StateT]     \"\"\"The type of the graph state.\"\"\"      deps_type: type[DepsT]     \"\"\"The type of the dependencies.\"\"\"      input_type: type[InputT]     \"\"\"The type of the input data.\"\"\"      output_type: type[OutputT]     \"\"\"The type of the output data.\"\"\"      auto_instrument: bool     \"\"\"Whether to automatically create instrumentation spans.\"\"\"      nodes: dict[NodeID, AnyNode]     \"\"\"All nodes in the graph indexed by their ID.\"\"\"      edges_by_source: dict[NodeID, list[Path]]     \"\"\"Outgoing paths from each source node.\"\"\"      parent_forks: dict[JoinID, ParentFork[NodeID]]     \"\"\"Parent fork information for each join node.\"\"\"      def get_parent_fork(self, join_id: JoinID) -> ParentFork[NodeID]:         \"\"\"Get the parent fork information for a join node.          Args:             join_id: The ID of the join node          Returns:             The parent fork information for the join          Raises:             RuntimeError: If the join ID is not found or has no parent fork         \"\"\"         result = self.parent_forks.get(join_id)         if result is None:             raise RuntimeError(f'Node {join_id} is not a join node or did not have a dominating fork (this is a bug)')         return result      async def run(         self,         *,         state: StateT = None,         deps: DepsT = None,         inputs: InputT = None,         span: AbstractContextManager[AbstractSpan] | None = None,         infer_name: bool = True,     ) -> OutputT:         \"\"\"Execute the graph and return the final output.          This is the main entry point for graph execution. It runs the graph         to completion and returns the final output value.          Args:             state: The graph state instance             deps: The dependencies instance             inputs: The input data for the graph             span: Optional span for tracing/instrumentation             infer_name: Whether to infer the graph name from the calling frame.          Returns:             The final output from the graph execution         \"\"\"         if infer_name and self.name is None:             inferred_name = infer_obj_name(self, depth=2)             if inferred_name is not None:  # pragma: no branch                 self.name = inferred_name          async with self.iter(state=state, deps=deps, inputs=inputs, span=span, infer_name=False) as graph_run:             # Note: This would probably be better using `async for _ in graph_run`, but this tests the `next` method,             # which I'm less confident will be implemented correctly if not used on the critical path. We can change it             # once we have tests, etc.             event: Any = None             while True:                 try:                     event = await graph_run.next(event)                 except StopAsyncIteration:                     assert isinstance(event, EndMarker), 'Graph run should end with an EndMarker.'                     return cast(EndMarker[OutputT], event).value      @asynccontextmanager     async def iter(         self,         *,         state: StateT = None,         deps: DepsT = None,         inputs: InputT = None,         span: AbstractContextManager[AbstractSpan] | None = None,         infer_name: bool = True,     ) -> AsyncIterator[GraphRun[StateT, DepsT, OutputT]]:         \"\"\"Create an iterator for step-by-step graph execution.          This method allows for more fine-grained control over graph execution,         enabling inspection of intermediate states and results.          Args:             state: The graph state instance             deps: The dependencies instance             inputs: The input data for the graph             span: Optional span for tracing/instrumentation             infer_name: Whether to infer the graph name from the calling frame.          Yields:             A GraphRun instance that can be iterated for step-by-step execution         \"\"\"         if infer_name and self.name is None:             inferred_name = infer_obj_name(self, depth=3)  # depth=3 because asynccontextmanager adds one             if inferred_name is not None:  # pragma: no branch                 self.name = inferred_name          with ExitStack() as stack:             entered_span: AbstractSpan | None = None             if span is None:                 if self.auto_instrument:                     entered_span = stack.enter_context(logfire_span('run graph {graph.name}', graph=self))             else:                 entered_span = stack.enter_context(span)             traceparent = None if entered_span is None else get_traceparent(entered_span)             async with GraphRun[StateT, DepsT, OutputT](                 graph=self,                 state=state,                 deps=deps,                 inputs=inputs,                 traceparent=traceparent,             ) as graph_run:                 yield graph_run      def render(self, *, title: str | None = None, direction: StateDiagramDirection | None = None) -> str:         \"\"\"Render the graph as a Mermaid diagram string.          Args:             title: Optional title for the diagram             direction: Optional direction for the diagram layout          Returns:             A string containing the Mermaid diagram representation         \"\"\"         from pydantic_graph.beta.mermaid import build_mermaid_graph          return build_mermaid_graph(self.nodes, self.edges_by_source).render(title=title, direction=direction)      def __repr__(self) -> str:         super_repr = super().__repr__()  # include class and memory address         # Insert the result of calling `__str__` before the final '>' in the repr         return f'{super_repr[:-1]}\\n{self}\\n{super_repr[-1]}'      def __str__(self) -> str:         \"\"\"Return a Mermaid diagram representation of the graph.          Returns:             A string containing the Mermaid diagram of the graph         \"\"\"         return self.render() ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta_graph/index.html#graph-dataclass", "page": "pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "Graph `dataclass`", "anchor": "graph-dataclass", "md_text": "#### name `instance-attribute`\n\n```\nname: str | None\n```\n\nOptional name for the graph, if not provided the name will be inferred from the calling frame on the first call to a graph method.\n\n#### state\\_type `instance-attribute`\n\n```\nstate_type: type[StateT]\n```\n\nThe type of the graph state.\n\n#### deps\\_type `instance-attribute`\n\n```\ndeps_type: type[DepsT]\n```\n\nThe type of the dependencies.\n\n#### input\\_type `instance-attribute`\n\n```\ninput_type: type[InputT]\n```\n\nThe type of the input data.\n\n#### output\\_type `instance-attribute`\n\n```\noutput_type: type[OutputT]\n```\n\nThe type of the output data.\n\n#### auto\\_instrument `instance-attribute`\n\n```\nauto_instrument: bool\n```\n\nWhether to automatically create instrumentation spans.\n\n#### nodes `instance-attribute`\n\n```\nnodes: dict[NodeID, AnyNode]\n```\n\nAll nodes in the graph indexed by their ID.\n\n#### edges\\_by\\_source `instance-attribute`\n\n```\nedges_by_source: dict[NodeID, list[Path]]\n```\n\nOutgoing paths from each source node.\n\n#### parent\\_forks `instance-attribute`\n\n```\nparent_forks: dict[JoinID, ParentFork[NodeID]]\n```\n\nParent fork information for each join node.\n\n#### get\\_parent\\_fork\n\n```\nget_parent_fork(join_id: JoinID) -> ParentFork[NodeID]\n```\n\nGet the parent fork information for a join node.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `join_id` | `JoinID` | The ID of the join node | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `ParentFork[NodeID]` | The parent fork information for the join |\n\nRaises:\n\n| Type | Description |\n| --- | --- |\n| `RuntimeError` | If the join ID is not found or has no parent fork |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph.py`\n\n|  |  |\n| --- | --- |\n| ``` 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 ``` | ``` def get_parent_fork(self, join_id: JoinID) -> ParentFork[NodeID]:     \"\"\"Get the parent fork information for a join node.      Args:         join_id: The ID of the join node      Returns:         The parent fork information for the join      Raises:         RuntimeError: If the join ID is not found or has no parent fork     \"\"\"     result = self.parent_forks.get(join_id)     if result is None:         raise RuntimeError(f'Node {join_id} is not a join node or did not have a dominating fork (this is a bug)')     return result ``` |\n\n#### run `async`\n\n```\nrun(\n    *,\n    state: StateT = None,\n    deps: DepsT = None,\n    inputs: InputT = None,\n    span: (\n        AbstractContextManager[AbstractSpan] | None\n    ) = None,\n    infer_name: bool = True\n) -> OutputT\n```\n\nExecute the graph and return the final output.\n\nThis is the main entry point for graph execution. It runs the graph\nto completion and returns the final output value.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `state` | `StateT` | The graph state instance | `None` |\n| `deps` | `DepsT` | The dependencies instance | `None` |\n| `inputs` | `InputT` | The input data for the graph | `None` |\n| `span` | `AbstractContextManager[AbstractSpan] | None` | Optional span for tracing/instrumentation | `None` |\n| `infer_name` | `bool` | Whether to infer the graph name from the calling frame. | `True` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `OutputT` | The final output from the graph execution |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph.py`", "url": "https://ai.pydantic.dev/pydantic_graph/beta_graph/index.html#graph-dataclass", "page": "pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "Graph `dataclass`", "anchor": "graph-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 ``` | ``` async def run(     self,     *,     state: StateT = None,     deps: DepsT = None,     inputs: InputT = None,     span: AbstractContextManager[AbstractSpan] | None = None,     infer_name: bool = True, ) -> OutputT:     \"\"\"Execute the graph and return the final output.      This is the main entry point for graph execution. It runs the graph     to completion and returns the final output value.      Args:         state: The graph state instance         deps: The dependencies instance         inputs: The input data for the graph         span: Optional span for tracing/instrumentation         infer_name: Whether to infer the graph name from the calling frame.      Returns:         The final output from the graph execution     \"\"\"     if infer_name and self.name is None:         inferred_name = infer_obj_name(self, depth=2)         if inferred_name is not None:  # pragma: no branch             self.name = inferred_name      async with self.iter(state=state, deps=deps, inputs=inputs, span=span, infer_name=False) as graph_run:         # Note: This would probably be better using `async for _ in graph_run`, but this tests the `next` method,         # which I'm less confident will be implemented correctly if not used on the critical path. We can change it         # once we have tests, etc.         event: Any = None         while True:             try:                 event = await graph_run.next(event)             except StopAsyncIteration:                 assert isinstance(event, EndMarker), 'Graph run should end with an EndMarker.'                 return cast(EndMarker[OutputT], event).value ``` |\n\n#### iter `async`\n\n```\niter(\n    *,\n    state: StateT = None,\n    deps: DepsT = None,\n    inputs: InputT = None,\n    span: (\n        AbstractContextManager[AbstractSpan] | None\n    ) = None,\n    infer_name: bool = True\n) -> AsyncIterator[GraphRun[StateT, DepsT, OutputT]]\n```\n\nCreate an iterator for step-by-step graph execution.\n\nThis method allows for more fine-grained control over graph execution,\nenabling inspection of intermediate states and results.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `state` | `StateT` | The graph state instance | `None` |\n| `deps` | `DepsT` | The dependencies instance | `None` |\n| `inputs` | `InputT` | The input data for the graph | `None` |\n| `span` | `AbstractContextManager[AbstractSpan] | None` | Optional span for tracing/instrumentation | `None` |\n| `infer_name` | `bool` | Whether to infer the graph name from the calling frame. | `True` |\n\nYields:\n\n| Type | Description |\n| --- | --- |\n| `AsyncIterator[GraphRun[StateT, DepsT, OutputT]]` | A GraphRun instance that can be iterated for step-by-step execution |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph.py`", "url": "https://ai.pydantic.dev/pydantic_graph/beta_graph/index.html#graph-dataclass", "page": "pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "Graph `dataclass`", "anchor": "graph-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 ``` | ``` @asynccontextmanager async def iter(     self,     *,     state: StateT = None,     deps: DepsT = None,     inputs: InputT = None,     span: AbstractContextManager[AbstractSpan] | None = None,     infer_name: bool = True, ) -> AsyncIterator[GraphRun[StateT, DepsT, OutputT]]:     \"\"\"Create an iterator for step-by-step graph execution.      This method allows for more fine-grained control over graph execution,     enabling inspection of intermediate states and results.      Args:         state: The graph state instance         deps: The dependencies instance         inputs: The input data for the graph         span: Optional span for tracing/instrumentation         infer_name: Whether to infer the graph name from the calling frame.      Yields:         A GraphRun instance that can be iterated for step-by-step execution     \"\"\"     if infer_name and self.name is None:         inferred_name = infer_obj_name(self, depth=3)  # depth=3 because asynccontextmanager adds one         if inferred_name is not None:  # pragma: no branch             self.name = inferred_name      with ExitStack() as stack:         entered_span: AbstractSpan | None = None         if span is None:             if self.auto_instrument:                 entered_span = stack.enter_context(logfire_span('run graph {graph.name}', graph=self))         else:             entered_span = stack.enter_context(span)         traceparent = None if entered_span is None else get_traceparent(entered_span)         async with GraphRun[StateT, DepsT, OutputT](             graph=self,             state=state,             deps=deps,             inputs=inputs,             traceparent=traceparent,         ) as graph_run:             yield graph_run ``` |\n\n#### render\n\n```\nrender(\n    *,\n    title: str | None = None,\n    direction: StateDiagramDirection | None = None\n) -> str\n```\n\nRender the graph as a Mermaid diagram string.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `title` | `str | None` | Optional title for the diagram | `None` |\n| `direction` | `StateDiagramDirection | None` | Optional direction for the diagram layout | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `str` | A string containing the Mermaid diagram representation |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph.py`\n\n|  |  |\n| --- | --- |\n| ``` 251 252 253 254 255 256 257 258 259 260 261 262 263 ``` | ``` def render(self, *, title: str | None = None, direction: StateDiagramDirection | None = None) -> str:     \"\"\"Render the graph as a Mermaid diagram string.      Args:         title: Optional title for the diagram         direction: Optional direction for the diagram layout      Returns:         A string containing the Mermaid diagram representation     \"\"\"     from pydantic_graph.beta.mermaid import build_mermaid_graph      return build_mermaid_graph(self.nodes, self.edges_by_source).render(title=title, direction=direction) ``` |\n\n#### \\_\\_str\\_\\_\n\n```\n__str__() -> str\n```\n\nReturn a Mermaid diagram representation of the graph.\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `str` | A string containing the Mermaid diagram of the graph |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph.py`\n\n|  |  |\n| --- | --- |\n| ``` 270 271 272 273 274 275 276 ``` | ``` def __str__(self) -> str:     \"\"\"Return a Mermaid diagram representation of the graph.      Returns:         A string containing the Mermaid diagram of the graph     \"\"\"     return self.render() ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta_graph/index.html#graph-dataclass", "page": "pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "GraphTask `dataclass`", "anchor": "graphtask-dataclass", "md_text": "A single task representing the execution of a node in the graph.\n\nGraphTask encapsulates all the information needed to execute a specific\nnode, including its inputs and the fork context it's executing within.\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph.py`\n\n|  |  |\n| --- | --- |\n| ``` 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 ``` | ``` @dataclass class GraphTask:     \"\"\"A single task representing the execution of a node in the graph.      GraphTask encapsulates all the information needed to execute a specific     node, including its inputs and the fork context it's executing within.     \"\"\"      # With our current BaseNode thing, next_node_id and next_node_inputs are merged into `next_node` itself     node_id: NodeID     \"\"\"The ID of the node to execute.\"\"\"      inputs: Any     \"\"\"The input data for the node.\"\"\"      fork_stack: ForkStack = field(repr=False)     \"\"\"Stack of forks that have been entered.      Used by the GraphRun to decide when to proceed through joins.     \"\"\"      task_id: TaskID = field(default_factory=lambda: TaskID(str(uuid.uuid4())), repr=False)     \"\"\"Unique identifier for this task.\"\"\" ``` |\n\n#### node\\_id `instance-attribute`\n\n```\nnode_id: NodeID\n```\n\nThe ID of the node to execute.\n\n#### inputs `instance-attribute`\n\n```\ninputs: Any\n```\n\nThe input data for the node.\n\n#### fork\\_stack `class-attribute` `instance-attribute`\n\n```\nfork_stack: ForkStack = field(repr=False)\n```\n\nStack of forks that have been entered.\n\nUsed by the GraphRun to decide when to proceed through joins.\n\n#### task\\_id `class-attribute` `instance-attribute`\n\n```\ntask_id: TaskID = field(\n    default_factory=lambda: TaskID(str(uuid4())), repr=False\n)\n```\n\nUnique identifier for this task.", "url": "https://ai.pydantic.dev/pydantic_graph/beta_graph/index.html#graphtask-dataclass", "page": "pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "GraphRun", "anchor": "graphrun", "md_text": "Bases: `Generic[StateT, DepsT, OutputT]`\n\nA single execution instance of a graph.\n\nGraphRun manages the execution state for a single run of a graph,\nincluding task scheduling, fork/join coordination, and result tracking.\n\nType Parameters\n\nStateT: The type of the graph state\nDepsT: The type of the dependencies\nOutputT: The type of the output data\n\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph.py`", "url": "https://ai.pydantic.dev/pydantic_graph/beta_graph/index.html#graphrun", "page": "pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "GraphRun", "anchor": "graphrun", "md_text": "|  |  |\n| --- | --- |\n| ``` 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 ``` | ``` class GraphRun(Generic[StateT, DepsT, OutputT]):     \"\"\"A single execution instance of a graph.      GraphRun manages the execution state for a single run of a graph,     including task scheduling, fork/join coordination, and result tracking.      Type Parameters:         StateT: The type of the graph state         DepsT: The type of the dependencies         OutputT: The type of the output data     \"\"\"      def __init__(         self,         graph: Graph[StateT, DepsT, InputT, OutputT],         *,         state: StateT,         deps: DepsT,         inputs: InputT,         traceparent: str | None,     ):         \"\"\"Initialize a graph run.          Args:             graph: The graph to execute             state: The graph state instance             deps: The dependencies instance             inputs: The input data for the graph             traceparent: Optional trace parent for instrumentation         \"\"\"         self.graph = graph         \"\"\"The graph being executed.\"\"\"          self.state = state         \"\"\"The graph state instance.\"\"\"          self.deps = deps         \"\"\"The dependencies instance.\"\"\"          self.inputs = inputs         \"\"\"The initial input data.\"\"\"          self._active_reducers: dict[tuple[JoinID, NodeRunID], JoinState] = {}         \"\"\"Active reducers for join operations.\"\"\"          self._next: EndMarker[OutputT] | Sequence[GraphTask] | None = None         \"\"\"The next item to be processed.\"\"\"          run_id = GraphRunID(str(uuid.uuid4()))         initial_fork_stack: ForkStack = (ForkStackItem(StartNode.id, NodeRunID(run_id), 0),)         self._first_task = GraphTask(node_id=StartNode.id, inputs=inputs, fork_stack=initial_fork_stack)         self._iterator_instance = _GraphIterator[StateT, DepsT, OutputT](self.graph, self.state, self.deps)         self._iterator = self._iterator_instance.iter_graph(self._first_task)          self.__traceparent = traceparent      async def __aenter__(self):         return self      async def __aexit__(self, exc_type: Any, exc_val: Any, exc_tb: Any):         self._iterator_instance.iter_stream_sender.close()         self._iterator_instance.iter_stream_receiver.close()         await self._iterator.aclose()      @overload     def _traceparent(self, *, required: Literal[False]) -> str | None: ...     @overload     def _traceparent(self) -> str: ...     def _traceparent(self, *, required: bool = True) -> str | None:         \"\"\"Get the trace parent for instrumentation.          Args:             required: Whether to raise an error if no traceparent exists          Returns:             The traceparent string, or None if not required and not set          Raises:             GraphRuntimeError: If required is True and no traceparent exists         \"\"\"         if self.__traceparent is None and required:  # pragma: no cover             raise exceptions.GraphRuntimeError('No span was created for this graph run')         return self.__traceparent      def __aiter__(self) -> AsyncIterator[EndMarker[OutputT] | Sequence[GraphTask]]:         \"\"\"Return self as an async iterator.          Returns:             Self for async iteration         \"\"\"         return self      async def __anext__(self) -> EndMarker[OutputT] | Sequence[GraphTask]:         \"\"\"Get the next item in the async iteration.          Returns:             The next execution result from the graph         \"\"\"         if self._next is None:             self._next = await anext(self._iterator)         else:             self._next = await self._iterator.asend(self._next)         return self._next      async def next(         self, value: EndMarker[OutputT] | Sequence[GraphTask] | None = None     ) -> EndMarker[OutputT] | Sequence[GraphTask]:         \"\"\"Advance the graph execution by one step.          This method allows for sending a value to the iterator, which is useful         for resuming iteration or overriding intermediate results.          Args:             value: Optional value to send to the iterator          Returns:             The next execution result: either an EndMarker, or sequence of GraphTasks         \"\"\"         if self._next is None:             # Prevent `TypeError: can't send non-None value to a just-started async generator`             # if `next` is called before the `first_node` has run.             await anext(self)         if value is not None:             self._next = value         return await anext(self)      @property     def next_task(self) -> EndMarker[OutputT] | Sequence[GraphTask]:         \"\"\"Get the next task(s) to be executed.          Returns:             The next execution item, or the initial task if none is set         \"\"\"         return self._next or [self._first_task]      @property     def output(self) -> OutputT | None:         \"\"\"Get the final output if the graph has completed.          Returns:             The output value if execution is complete, None otherwise         \"\"\"         if isinstance(self._next, EndMarker):             return self._next.value         return None ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta_graph/index.html#graphrun", "page": "pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "GraphRun", "anchor": "graphrun", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    graph: Graph[StateT, DepsT, InputT, OutputT],\n    *,\n    state: StateT,\n    deps: DepsT,\n    inputs: InputT,\n    traceparent: str | None\n)\n```\n\nInitialize a graph run.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `graph` | `Graph[StateT, DepsT, InputT, OutputT]` | The graph to execute | *required* |\n| `state` | `StateT` | The graph state instance | *required* |\n| `deps` | `DepsT` | The dependencies instance | *required* |\n| `inputs` | `InputT` | The input data for the graph | *required* |\n| `traceparent` | `str | None` | Optional trace parent for instrumentation | *required* |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph.py`\n\n|  |  |\n| --- | --- |\n| ``` 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 ``` | ``` def __init__(     self,     graph: Graph[StateT, DepsT, InputT, OutputT],     *,     state: StateT,     deps: DepsT,     inputs: InputT,     traceparent: str | None, ):     \"\"\"Initialize a graph run.      Args:         graph: The graph to execute         state: The graph state instance         deps: The dependencies instance         inputs: The input data for the graph         traceparent: Optional trace parent for instrumentation     \"\"\"     self.graph = graph     \"\"\"The graph being executed.\"\"\"      self.state = state     \"\"\"The graph state instance.\"\"\"      self.deps = deps     \"\"\"The dependencies instance.\"\"\"      self.inputs = inputs     \"\"\"The initial input data.\"\"\"      self._active_reducers: dict[tuple[JoinID, NodeRunID], JoinState] = {}     \"\"\"Active reducers for join operations.\"\"\"      self._next: EndMarker[OutputT] | Sequence[GraphTask] | None = None     \"\"\"The next item to be processed.\"\"\"      run_id = GraphRunID(str(uuid.uuid4()))     initial_fork_stack: ForkStack = (ForkStackItem(StartNode.id, NodeRunID(run_id), 0),)     self._first_task = GraphTask(node_id=StartNode.id, inputs=inputs, fork_stack=initial_fork_stack)     self._iterator_instance = _GraphIterator[StateT, DepsT, OutputT](self.graph, self.state, self.deps)     self._iterator = self._iterator_instance.iter_graph(self._first_task)      self.__traceparent = traceparent ``` |\n\n#### graph `instance-attribute`\n\n```\ngraph = graph\n```\n\nThe graph being executed.\n\n#### state `instance-attribute`\n\n```\nstate = state\n```\n\nThe graph state instance.\n\n#### deps `instance-attribute`\n\n```\ndeps = deps\n```\n\nThe dependencies instance.\n\n#### inputs `instance-attribute`\n\n```\ninputs = inputs\n```\n\nThe initial input data.\n\n#### \\_\\_aiter\\_\\_\n\n```\n__aiter__() -> (\n    AsyncIterator[EndMarker[OutputT] | Sequence[GraphTask]]\n)\n```\n\nReturn self as an async iterator.\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AsyncIterator[EndMarker[OutputT] | Sequence[GraphTask]]` | Self for async iteration |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph.py`\n\n|  |  |\n| --- | --- |\n| ``` 388 389 390 391 392 393 394 ``` | ``` def __aiter__(self) -> AsyncIterator[EndMarker[OutputT] | Sequence[GraphTask]]:     \"\"\"Return self as an async iterator.      Returns:         Self for async iteration     \"\"\"     return self ``` |\n\n#### \\_\\_anext\\_\\_ `async`\n\n```\n__anext__() -> EndMarker[OutputT] | Sequence[GraphTask]\n```\n\nGet the next item in the async iteration.\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `EndMarker[OutputT] | Sequence[GraphTask]` | The next execution result from the graph |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph.py`\n\n|  |  |\n| --- | --- |\n| ``` 396 397 398 399 400 401 402 403 404 405 406 ``` | ``` async def __anext__(self) -> EndMarker[OutputT] | Sequence[GraphTask]:     \"\"\"Get the next item in the async iteration.      Returns:         The next execution result from the graph     \"\"\"     if self._next is None:         self._next = await anext(self._iterator)     else:         self._next = await self._iterator.asend(self._next)     return self._next ``` |\n\n#### next `async`\n\n```\nnext(\n    value: (\n        EndMarker[OutputT] | Sequence[GraphTask] | None\n    ) = None,\n) -> EndMarker[OutputT] | Sequence[GraphTask]\n```\n\nAdvance the graph execution by one step.", "url": "https://ai.pydantic.dev/pydantic_graph/beta_graph/index.html#graphrun", "page": "pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "GraphRun", "anchor": "graphrun", "md_text": "This method allows for sending a value to the iterator, which is useful\nfor resuming iteration or overriding intermediate results.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `value` | `EndMarker[OutputT] | Sequence[GraphTask] | None` | Optional value to send to the iterator | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `EndMarker[OutputT] | Sequence[GraphTask]` | The next execution result: either an EndMarker, or sequence of GraphTasks |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph.py`\n\n|  |  |\n| --- | --- |\n| ``` 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 ``` | ``` async def next(     self, value: EndMarker[OutputT] | Sequence[GraphTask] | None = None ) -> EndMarker[OutputT] | Sequence[GraphTask]:     \"\"\"Advance the graph execution by one step.      This method allows for sending a value to the iterator, which is useful     for resuming iteration or overriding intermediate results.      Args:         value: Optional value to send to the iterator      Returns:         The next execution result: either an EndMarker, or sequence of GraphTasks     \"\"\"     if self._next is None:         # Prevent `TypeError: can't send non-None value to a just-started async generator`         # if `next` is called before the `first_node` has run.         await anext(self)     if value is not None:         self._next = value     return await anext(self) ``` |\n\n#### next\\_task `property`\n\n```\nnext_task: EndMarker[OutputT] | Sequence[GraphTask]\n```\n\nGet the next task(s) to be executed.\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `EndMarker[OutputT] | Sequence[GraphTask]` | The next execution item, or the initial task if none is set |\n\n#### output `property`\n\n```\noutput: OutputT | None\n```\n\nGet the final output if the graph has completed.\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `OutputT | None` | The output value if execution is complete, None otherwise |", "url": "https://ai.pydantic.dev/pydantic_graph/beta_graph/index.html#graphrun", "page": "pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "SnapshotStatus `module-attribute`", "anchor": "snapshotstatus-module-attribute", "md_text": "```\nSnapshotStatus = Literal[\n    \"created\", \"pending\", \"running\", \"success\", \"error\"\n]\n```\n\nThe status of a snapshot.\n\n* `'created'`: The snapshot has been created but not yet run.\n* `'pending'`: The snapshot has been retrieved with\n  [`load_next`](index.html#pydantic_graph.persistence.BaseStatePersistence.load_next) but not yet run.\n* `'running'`: The snapshot is currently running.\n* `'success'`: The snapshot has been run successfully.\n* `'error'`: The snapshot has been run but an error occurred.", "url": "https://ai.pydantic.dev/pydantic_graph/persistence/index.html#snapshotstatus-module-attribute", "page": "pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "NodeSnapshot `dataclass`", "anchor": "nodesnapshot-dataclass", "md_text": "Bases: `Generic[StateT, RunEndT]`\n\nHistory step describing the execution of a node in a graph.\n\nSource code in `pydantic_graph/pydantic_graph/persistence/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 ``` | ``` @dataclass(kw_only=True) class NodeSnapshot(Generic[StateT, RunEndT]):     \"\"\"History step describing the execution of a node in a graph.\"\"\"      state: StateT     \"\"\"The state of the graph before the node is run.\"\"\"     node: Annotated[BaseNode[StateT, Any, RunEndT], _utils.CustomNodeSchema()]     \"\"\"The node to run next.\"\"\"     start_ts: datetime | None = None     \"\"\"The timestamp when the node started running, `None` until the run starts.\"\"\"     duration: float | None = None     \"\"\"The duration of the node run in seconds, if the node has been run.\"\"\"     status: SnapshotStatus = 'created'     \"\"\"The status of the snapshot.\"\"\"     kind: Literal['node'] = 'node'     \"\"\"The kind of history step, can be used as a discriminator when deserializing history.\"\"\"      id: str = UNSET_SNAPSHOT_ID     \"\"\"Unique ID of the snapshot.\"\"\"      def __post_init__(self) -> None:         if self.id == UNSET_SNAPSHOT_ID:             self.id = self.node.get_snapshot_id() ``` |\n\n#### state `instance-attribute`\n\n```\nstate: StateT\n```\n\nThe state of the graph before the node is run.\n\n#### node `instance-attribute`\n\n```\nnode: Annotated[\n    BaseNode[StateT, Any, RunEndT], CustomNodeSchema()\n]\n```\n\nThe node to run next.\n\n#### start\\_ts `class-attribute` `instance-attribute`\n\n```\nstart_ts: datetime | None = None\n```\n\nThe timestamp when the node started running, `None` until the run starts.\n\n#### duration `class-attribute` `instance-attribute`\n\n```\nduration: float | None = None\n```\n\nThe duration of the node run in seconds, if the node has been run.\n\n#### status `class-attribute` `instance-attribute`\n\n```\nstatus: SnapshotStatus = 'created'\n```\n\nThe status of the snapshot.\n\n#### kind `class-attribute` `instance-attribute`\n\n```\nkind: Literal['node'] = 'node'\n```\n\nThe kind of history step, can be used as a discriminator when deserializing history.\n\n#### id `class-attribute` `instance-attribute`\n\n```\nid: str = UNSET_SNAPSHOT_ID\n```\n\nUnique ID of the snapshot.", "url": "https://ai.pydantic.dev/pydantic_graph/persistence/index.html#nodesnapshot-dataclass", "page": "pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "EndSnapshot `dataclass`", "anchor": "endsnapshot-dataclass", "md_text": "Bases: `Generic[StateT, RunEndT]`\n\nHistory step describing the end of a graph run.\n\nSource code in `pydantic_graph/pydantic_graph/persistence/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 ``` | ``` @dataclass(kw_only=True) class EndSnapshot(Generic[StateT, RunEndT]):     \"\"\"History step describing the end of a graph run.\"\"\"      state: StateT     \"\"\"The state of the graph at the end of the run.\"\"\"     result: End[RunEndT]     \"\"\"The result of the graph run.\"\"\"     ts: datetime = field(default_factory=_utils.now_utc)     \"\"\"The timestamp when the graph run ended.\"\"\"     kind: Literal['end'] = 'end'     \"\"\"The kind of history step, can be used as a discriminator when deserializing history.\"\"\"      id: str = UNSET_SNAPSHOT_ID     \"\"\"Unique ID of the snapshot.\"\"\"      def __post_init__(self) -> None:         if self.id == UNSET_SNAPSHOT_ID:             self.id = self.node.get_snapshot_id()      @property     def node(self) -> End[RunEndT]:         \"\"\"Shim to get the [`result`][pydantic_graph.persistence.EndSnapshot.result].          Useful to allow `[snapshot.node for snapshot in persistence.history]`.         \"\"\"         return self.result ``` |\n\n#### state `instance-attribute`\n\n```\nstate: StateT\n```\n\nThe state of the graph at the end of the run.\n\n#### result `instance-attribute`\n\n```\nresult: End[RunEndT]\n```\n\nThe result of the graph run.\n\n#### ts `class-attribute` `instance-attribute`\n\n```\nts: datetime = field(default_factory=now_utc)\n```\n\nThe timestamp when the graph run ended.\n\n#### kind `class-attribute` `instance-attribute`\n\n```\nkind: Literal['end'] = 'end'\n```\n\nThe kind of history step, can be used as a discriminator when deserializing history.\n\n#### id `class-attribute` `instance-attribute`\n\n```\nid: str = UNSET_SNAPSHOT_ID\n```\n\nUnique ID of the snapshot.\n\n#### node `property`\n\n```\nnode: End[RunEndT]\n```\n\nShim to get the [`result`](index.html#pydantic_graph.persistence.EndSnapshot.result).\n\nUseful to allow `[snapshot.node for snapshot in persistence.history]`.", "url": "https://ai.pydantic.dev/pydantic_graph/persistence/index.html#endsnapshot-dataclass", "page": "pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "Snapshot `module-attribute`", "anchor": "snapshot-module-attribute", "md_text": "```\nSnapshot = (\n    NodeSnapshot[StateT, RunEndT]\n    | EndSnapshot[StateT, RunEndT]\n)\n```\n\nA step in the history of a graph run.\n\n[`Graph.run`](../graph/index.html#pydantic_graph.graph.Graph.run) returns a list of these steps describing the execution of the graph,\ntogether with the run return value.", "url": "https://ai.pydantic.dev/pydantic_graph/persistence/index.html#snapshot-module-attribute", "page": "pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "BaseStatePersistence", "anchor": "basestatepersistence", "md_text": "Bases: `ABC`, `Generic[StateT, RunEndT]`\n\nAbstract base class for storing the state of a graph run.\n\nEach instance of a `BaseStatePersistence` subclass should be used for a single graph run.\n\nSource code in `pydantic_graph/pydantic_graph/persistence/__init__.py`", "url": "https://ai.pydantic.dev/pydantic_graph/persistence/index.html#basestatepersistence", "page": "pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "BaseStatePersistence", "anchor": "basestatepersistence", "md_text": "|  |  |\n| --- | --- |\n| ``` 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 ``` | ``` class BaseStatePersistence(ABC, Generic[StateT, RunEndT]):     \"\"\"Abstract base class for storing the state of a graph run.      Each instance of a `BaseStatePersistence` subclass should be used for a single graph run.     \"\"\"      @abstractmethod     async def snapshot_node(self, state: StateT, next_node: BaseNode[StateT, Any, RunEndT]) -> None:         \"\"\"Snapshot the state of a graph, when the next step is to run a node.          This method should add a [`NodeSnapshot`][pydantic_graph.persistence.NodeSnapshot] to persistence.          Args:             state: The state of the graph.             next_node: The next node to run.         \"\"\"         raise NotImplementedError      @abstractmethod     async def snapshot_node_if_new(         self, snapshot_id: str, state: StateT, next_node: BaseNode[StateT, Any, RunEndT]     ) -> None:         \"\"\"Snapshot the state of a graph if the snapshot ID doesn't already exist in persistence.          This method will generally call [`snapshot_node`][pydantic_graph.persistence.BaseStatePersistence.snapshot_node]         but should do so in an atomic way.          Args:             snapshot_id: The ID of the snapshot to check.             state: The state of the graph.             next_node: The next node to run.         \"\"\"         raise NotImplementedError      @abstractmethod     async def snapshot_end(self, state: StateT, end: End[RunEndT]) -> None:         \"\"\"Snapshot the state of a graph when the graph has ended.          This method should add an [`EndSnapshot`][pydantic_graph.persistence.EndSnapshot] to persistence.          Args:             state: The state of the graph.             end: data from the end of the run.         \"\"\"         raise NotImplementedError      @abstractmethod     def record_run(self, snapshot_id: str) -> AbstractAsyncContextManager[None]:         \"\"\"Record the run of the node, or error if the node is already running.          Args:             snapshot_id: The ID of the snapshot to record.          Raises:             GraphNodeRunningError: if the node status it not `'created'` or `'pending'`.             LookupError: if the snapshot ID is not found in persistence.          Returns:             An async context manager that records the run of the node.          In particular this should set:          - [`NodeSnapshot.status`][pydantic_graph.persistence.NodeSnapshot.status] to `'running'` and           [`NodeSnapshot.start_ts`][pydantic_graph.persistence.NodeSnapshot.start_ts] when the run starts.         - [`NodeSnapshot.status`][pydantic_graph.persistence.NodeSnapshot.status] to `'success'` or `'error'` and           [`NodeSnapshot.duration`][pydantic_graph.persistence.NodeSnapshot.duration] when the run finishes.         \"\"\"         raise NotImplementedError      @abstractmethod     async def load_next(self) -> NodeSnapshot[StateT, RunEndT] | None:         \"\"\"Retrieve a node snapshot with status `'created`' and set its status to `'pending'`.          This is used by [`Graph.iter_from_persistence`][pydantic_graph.graph.Graph.iter_from_persistence]         to get the next node to run.          Returns: The snapshot, or `None` if no snapshot with status `'created`' exists.         \"\"\"         raise NotImplementedError      @abstractmethod     async def load_all(self) -> list[Snapshot[StateT, RunEndT]]:         \"\"\"Load the entire history of snapshots.          `load_all` is not used by pydantic-graph itself, instead it's provided to make it convenient to         get all [snapshots][pydantic_graph.persistence.Snapshot] from persistence.          Returns: The list of snapshots.         \"\"\"         raise NotImplementedError      def set_graph_types(self, graph: Graph[StateT, Any, RunEndT]) -> None:         \"\"\"Set the types of the state and run end from a graph.          You generally won't need to customise this method, instead implement         [`set_types`][pydantic_graph.persistence.BaseStatePersistence.set_types] and         [`should_set_types`][pydantic_graph.persistence.BaseStatePersistence.should_set_types].         \"\"\"         if self.should_set_types():             with _utils.set_nodes_type_context(graph.get_nodes()):                 self.set_types(*graph.inferred_types)      def should_set_types(self) -> bool:         \"\"\"Whether types need to be set.          Implementations should override this method to return `True` when types have not been set if they are needed.         \"\"\"         return False      def set_types(self, state_type: type[StateT], run_end_type: type[RunEndT]) -> None:         \"\"\"Set the types of the state and run end.          This can be used to create [type adapters][pydantic.TypeAdapter] for serializing and deserializing snapshots,         e.g. with [`build_snapshot_list_type_adapter`][pydantic_graph.persistence.build_snapshot_list_type_adapter].          Args:             state_type: The state type.             run_end_type: The run end type.         \"\"\"         pass ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/persistence/index.html#basestatepersistence", "page": "pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "BaseStatePersistence", "anchor": "basestatepersistence", "md_text": "#### snapshot\\_node `abstractmethod` `async`\n\n```\nsnapshot_node(\n    state: StateT, next_node: BaseNode[StateT, Any, RunEndT]\n) -> None\n```\n\nSnapshot the state of a graph, when the next step is to run a node.\n\nThis method should add a [`NodeSnapshot`](index.html#pydantic_graph.persistence.NodeSnapshot) to persistence.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `state` | `StateT` | The state of the graph. | *required* |\n| `next_node` | `BaseNode[StateT, Any, RunEndT]` | The next node to run. | *required* |\n\nSource code in `pydantic_graph/pydantic_graph/persistence/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 112 113 114 115 116 117 118 119 120 121 122 ``` | ``` @abstractmethod async def snapshot_node(self, state: StateT, next_node: BaseNode[StateT, Any, RunEndT]) -> None:     \"\"\"Snapshot the state of a graph, when the next step is to run a node.      This method should add a [`NodeSnapshot`][pydantic_graph.persistence.NodeSnapshot] to persistence.      Args:         state: The state of the graph.         next_node: The next node to run.     \"\"\"     raise NotImplementedError ``` |\n\n#### snapshot\\_node\\_if\\_new `abstractmethod` `async`\n\n```\nsnapshot_node_if_new(\n    snapshot_id: str,\n    state: StateT,\n    next_node: BaseNode[StateT, Any, RunEndT],\n) -> None\n```\n\nSnapshot the state of a graph if the snapshot ID doesn't already exist in persistence.\n\nThis method will generally call [`snapshot_node`](index.html#pydantic_graph.persistence.BaseStatePersistence.snapshot_node)\nbut should do so in an atomic way.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `snapshot_id` | `str` | The ID of the snapshot to check. | *required* |\n| `state` | `StateT` | The state of the graph. | *required* |\n| `next_node` | `BaseNode[StateT, Any, RunEndT]` | The next node to run. | *required* |\n\nSource code in `pydantic_graph/pydantic_graph/persistence/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 ``` | ``` @abstractmethod async def snapshot_node_if_new(     self, snapshot_id: str, state: StateT, next_node: BaseNode[StateT, Any, RunEndT] ) -> None:     \"\"\"Snapshot the state of a graph if the snapshot ID doesn't already exist in persistence.      This method will generally call [`snapshot_node`][pydantic_graph.persistence.BaseStatePersistence.snapshot_node]     but should do so in an atomic way.      Args:         snapshot_id: The ID of the snapshot to check.         state: The state of the graph.         next_node: The next node to run.     \"\"\"     raise NotImplementedError ``` |\n\n#### snapshot\\_end `abstractmethod` `async`\n\n```\nsnapshot_end(state: StateT, end: End[RunEndT]) -> None\n```\n\nSnapshot the state of a graph when the graph has ended.\n\nThis method should add an [`EndSnapshot`](index.html#pydantic_graph.persistence.EndSnapshot) to persistence.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `state` | `StateT` | The state of the graph. | *required* |\n| `end` | `End[RunEndT]` | data from the end of the run. | *required* |\n\nSource code in `pydantic_graph/pydantic_graph/persistence/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 140 141 142 143 144 145 146 147 148 149 150 ``` | ``` @abstractmethod async def snapshot_end(self, state: StateT, end: End[RunEndT]) -> None:     \"\"\"Snapshot the state of a graph when the graph has ended.      This method should add an [`EndSnapshot`][pydantic_graph.persistence.EndSnapshot] to persistence.      Args:         state: The state of the graph.         end: data from the end of the run.     \"\"\"     raise NotImplementedError ``` |\n\n#### record\\_run `abstractmethod`\n\n```\nrecord_run(\n    snapshot_id: str,\n) -> AbstractAsyncContextManager[None]\n```\n\nRecord the run of the node, or error if the node is already running.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `snapshot_id` | `str` | The ID of the snapshot to record. | *required* |\n\nRaises:\n\n| Type | Description |\n| --- | --- |\n| `GraphNodeRunningError` | if the node status it not `'created'` or `'pending'`. |\n| `LookupError` | if the snapshot ID is not found in persistence. |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `AbstractAsyncContextManager[None]` | An async context manager that records the run of the node. |\n\nIn particular this should set:", "url": "https://ai.pydantic.dev/pydantic_graph/persistence/index.html#basestatepersistence", "page": "pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "BaseStatePersistence", "anchor": "basestatepersistence", "md_text": "* [`NodeSnapshot.status`](index.html#pydantic_graph.persistence.NodeSnapshot.status) to `'running'` and\n  [`NodeSnapshot.start_ts`](index.html#pydantic_graph.persistence.NodeSnapshot.start_ts) when the run starts.\n* [`NodeSnapshot.status`](index.html#pydantic_graph.persistence.NodeSnapshot.status) to `'success'` or `'error'` and\n  [`NodeSnapshot.duration`](index.html#pydantic_graph.persistence.NodeSnapshot.duration) when the run finishes.\n\nSource code in `pydantic_graph/pydantic_graph/persistence/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 ``` | ``` @abstractmethod def record_run(self, snapshot_id: str) -> AbstractAsyncContextManager[None]:     \"\"\"Record the run of the node, or error if the node is already running.      Args:         snapshot_id: The ID of the snapshot to record.      Raises:         GraphNodeRunningError: if the node status it not `'created'` or `'pending'`.         LookupError: if the snapshot ID is not found in persistence.      Returns:         An async context manager that records the run of the node.      In particular this should set:      - [`NodeSnapshot.status`][pydantic_graph.persistence.NodeSnapshot.status] to `'running'` and       [`NodeSnapshot.start_ts`][pydantic_graph.persistence.NodeSnapshot.start_ts] when the run starts.     - [`NodeSnapshot.status`][pydantic_graph.persistence.NodeSnapshot.status] to `'success'` or `'error'` and       [`NodeSnapshot.duration`][pydantic_graph.persistence.NodeSnapshot.duration] when the run finishes.     \"\"\"     raise NotImplementedError ``` |\n\n#### load\\_next `abstractmethod` `async`\n\n```\nload_next() -> NodeSnapshot[StateT, RunEndT] | None\n```\n\nRetrieve a node snapshot with status `'created`' and set its status to `'pending'`.\n\nThis is used by [`Graph.iter_from_persistence`](../graph/index.html#pydantic_graph.graph.Graph.iter_from_persistence)\nto get the next node to run.\n\nReturns: The snapshot, or `None` if no snapshot with status `'created`' exists.\n\nSource code in `pydantic_graph/pydantic_graph/persistence/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 175 176 177 178 179 180 181 182 183 184 ``` | ``` @abstractmethod async def load_next(self) -> NodeSnapshot[StateT, RunEndT] | None:     \"\"\"Retrieve a node snapshot with status `'created`' and set its status to `'pending'`.      This is used by [`Graph.iter_from_persistence`][pydantic_graph.graph.Graph.iter_from_persistence]     to get the next node to run.      Returns: The snapshot, or `None` if no snapshot with status `'created`' exists.     \"\"\"     raise NotImplementedError ``` |\n\n#### load\\_all `abstractmethod` `async`\n\n```\nload_all() -> list[Snapshot[StateT, RunEndT]]\n```\n\nLoad the entire history of snapshots.\n\n`load_all` is not used by pydantic-graph itself, instead it's provided to make it convenient to\nget all [snapshots](index.html#pydantic_graph.persistence.Snapshot) from persistence.\n\nReturns: The list of snapshots.\n\nSource code in `pydantic_graph/pydantic_graph/persistence/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 186 187 188 189 190 191 192 193 194 195 ``` | ``` @abstractmethod async def load_all(self) -> list[Snapshot[StateT, RunEndT]]:     \"\"\"Load the entire history of snapshots.      `load_all` is not used by pydantic-graph itself, instead it's provided to make it convenient to     get all [snapshots][pydantic_graph.persistence.Snapshot] from persistence.      Returns: The list of snapshots.     \"\"\"     raise NotImplementedError ``` |\n\n#### set\\_graph\\_types\n\n```\nset_graph_types(graph: Graph[StateT, Any, RunEndT]) -> None\n```\n\nSet the types of the state and run end from a graph.\n\nYou generally won't need to customise this method, instead implement\n[`set_types`](index.html#pydantic_graph.persistence.BaseStatePersistence.set_types) and\n[`should_set_types`](index.html#pydantic_graph.persistence.BaseStatePersistence.should_set_types).\n\nSource code in `pydantic_graph/pydantic_graph/persistence/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 197 198 199 200 201 202 203 204 205 206 ``` | ``` def set_graph_types(self, graph: Graph[StateT, Any, RunEndT]) -> None:     \"\"\"Set the types of the state and run end from a graph.      You generally won't need to customise this method, instead implement     [`set_types`][pydantic_graph.persistence.BaseStatePersistence.set_types] and     [`should_set_types`][pydantic_graph.persistence.BaseStatePersistence.should_set_types].     \"\"\"     if self.should_set_types():         with _utils.set_nodes_type_context(graph.get_nodes()):             self.set_types(*graph.inferred_types) ``` |\n\n#### should\\_set\\_types\n\n```\nshould_set_types() -> bool\n```\n\nWhether types need to be set.", "url": "https://ai.pydantic.dev/pydantic_graph/persistence/index.html#basestatepersistence", "page": "pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "BaseStatePersistence", "anchor": "basestatepersistence", "md_text": "Implementations should override this method to return `True` when types have not been set if they are needed.\n\nSource code in `pydantic_graph/pydantic_graph/persistence/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 208 209 210 211 212 213 ``` | ``` def should_set_types(self) -> bool:     \"\"\"Whether types need to be set.      Implementations should override this method to return `True` when types have not been set if they are needed.     \"\"\"     return False ``` |\n\n#### set\\_types\n\n```\nset_types(\n    state_type: type[StateT], run_end_type: type[RunEndT]\n) -> None\n```\n\nSet the types of the state and run end.\n\nThis can be used to create [type adapters](https://docs.pydantic.dev/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter) for serializing and deserializing snapshots,\ne.g. with [`build_snapshot_list_type_adapter`](index.html#pydantic_graph.persistence.build_snapshot_list_type_adapter).\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `state_type` | `type[StateT]` | The state type. | *required* |\n| `run_end_type` | `type[RunEndT]` | The run end type. | *required* |\n\nSource code in `pydantic_graph/pydantic_graph/persistence/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 215 216 217 218 219 220 221 222 223 224 225 ``` | ``` def set_types(self, state_type: type[StateT], run_end_type: type[RunEndT]) -> None:     \"\"\"Set the types of the state and run end.      This can be used to create [type adapters][pydantic.TypeAdapter] for serializing and deserializing snapshots,     e.g. with [`build_snapshot_list_type_adapter`][pydantic_graph.persistence.build_snapshot_list_type_adapter].      Args:         state_type: The state type.         run_end_type: The run end type.     \"\"\"     pass ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/persistence/index.html#basestatepersistence", "page": "pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "build\\_snapshot\\_list\\_type\\_adapter", "anchor": "buildsnapshotlisttypeadapter", "md_text": "```\nbuild_snapshot_list_type_adapter(\n    state_t: type[StateT], run_end_t: type[RunEndT]\n) -> TypeAdapter[list[Snapshot[StateT, RunEndT]]]\n```\n\nBuild a type adapter for a list of snapshots.\n\nThis method should be called from within\n[`set_types`](index.html#pydantic_graph.persistence.BaseStatePersistence.set_types)\nwhere context variables will be set such that Pydantic can create a schema for\n[`NodeSnapshot.node`](index.html#pydantic_graph.persistence.NodeSnapshot.node).\n\nSource code in `pydantic_graph/pydantic_graph/persistence/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 228 229 230 231 232 233 234 235 236 237 238 ``` | ``` def build_snapshot_list_type_adapter(     state_t: type[StateT], run_end_t: type[RunEndT] ) -> pydantic.TypeAdapter[list[Snapshot[StateT, RunEndT]]]:     \"\"\"Build a type adapter for a list of snapshots.      This method should be called from within     [`set_types`][pydantic_graph.persistence.BaseStatePersistence.set_types]     where context variables will be set such that Pydantic can create a schema for     [`NodeSnapshot.node`][pydantic_graph.persistence.NodeSnapshot.node].     \"\"\"     return pydantic.TypeAdapter(list[Annotated[Snapshot[state_t, run_end_t], pydantic.Discriminator('kind')]]) ``` |\n\nIn memory state persistence.\n\nThis module provides simple in memory state persistence for graphs.", "url": "https://ai.pydantic.dev/pydantic_graph/persistence/index.html#buildsnapshotlisttypeadapter", "page": "pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "SimpleStatePersistence `dataclass`", "anchor": "simplestatepersistence-dataclass", "md_text": "Bases: `BaseStatePersistence[StateT, RunEndT]`\n\nSimple in memory state persistence that just hold the latest snapshot.\n\nIf no state persistence implementation is provided when running a graph, this is used by default.\n\nSource code in `pydantic_graph/pydantic_graph/persistence/in_mem.py`\n\n|  |  |\n| --- | --- |\n| ``` 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 ``` | ``` @dataclass class SimpleStatePersistence(BaseStatePersistence[StateT, RunEndT]):     \"\"\"Simple in memory state persistence that just hold the latest snapshot.      If no state persistence implementation is provided when running a graph, this is used by default.     \"\"\"      last_snapshot: Snapshot[StateT, RunEndT] | None = None     \"\"\"The last snapshot.\"\"\"      async def snapshot_node(self, state: StateT, next_node: BaseNode[StateT, Any, RunEndT]) -> None:         self.last_snapshot = NodeSnapshot(state=state, node=next_node)      async def snapshot_node_if_new(         self, snapshot_id: str, state: StateT, next_node: BaseNode[StateT, Any, RunEndT]     ) -> None:         if self.last_snapshot and self.last_snapshot.id == snapshot_id:             return  # pragma: no cover         else:             await self.snapshot_node(state, next_node)      async def snapshot_end(self, state: StateT, end: End[RunEndT]) -> None:         self.last_snapshot = EndSnapshot(state=state, result=end)      @asynccontextmanager     async def record_run(self, snapshot_id: str) -> AsyncIterator[None]:         if self.last_snapshot is None or snapshot_id != self.last_snapshot.id:             raise LookupError(f'No snapshot found with id={snapshot_id!r}')          assert isinstance(self.last_snapshot, NodeSnapshot), 'Only NodeSnapshot can be recorded'         exceptions.GraphNodeStatusError.check(self.last_snapshot.status)         self.last_snapshot.status = 'running'         self.last_snapshot.start_ts = _utils.now_utc()          start = perf_counter()         try:             yield         except Exception:  # pragma: no cover             self.last_snapshot.duration = perf_counter() - start             self.last_snapshot.status = 'error'             raise         else:             self.last_snapshot.duration = perf_counter() - start             self.last_snapshot.status = 'success'      async def load_next(self) -> NodeSnapshot[StateT, RunEndT] | None:         if isinstance(self.last_snapshot, NodeSnapshot) and self.last_snapshot.status == 'created':             self.last_snapshot.status = 'pending'             return copy.deepcopy(self.last_snapshot)      async def load_all(self) -> list[Snapshot[StateT, RunEndT]]:         raise NotImplementedError('load is not supported for SimpleStatePersistence') ``` |\n\n#### last\\_snapshot `class-attribute` `instance-attribute`\n\n```\nlast_snapshot: Snapshot[StateT, RunEndT] | None = None\n```\n\nThe last snapshot.", "url": "https://ai.pydantic.dev/pydantic_graph/persistence/index.html#simplestatepersistence-dataclass", "page": "pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "FullStatePersistence `dataclass`", "anchor": "fullstatepersistence-dataclass", "md_text": "Bases: `BaseStatePersistence[StateT, RunEndT]`\n\nIn memory state persistence that hold a list of snapshots.\n\nSource code in `pydantic_graph/pydantic_graph/persistence/in_mem.py`\n\n|  |  |\n| --- | --- |\n| ```  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 ``` | ``` @dataclass class FullStatePersistence(BaseStatePersistence[StateT, RunEndT]):     \"\"\"In memory state persistence that hold a list of snapshots.\"\"\"      deep_copy: bool = True     \"\"\"Whether to deep copy the state and nodes when storing them.      Defaults to `True` so even if nodes or state are modified after the snapshot is taken,     the persistence history will record the value at the time of the snapshot.     \"\"\"     history: list[Snapshot[StateT, RunEndT]] = field(default_factory=list)     \"\"\"List of snapshots taken during the graph run.\"\"\"     _snapshots_type_adapter: pydantic.TypeAdapter[list[Snapshot[StateT, RunEndT]]] | None = field(         default=None, init=False, repr=False     )      async def snapshot_node(self, state: StateT, next_node: BaseNode[StateT, Any, RunEndT]) -> None:         snapshot = NodeSnapshot(             state=self._prep_state(state),             node=next_node.deep_copy() if self.deep_copy else next_node,         )         self.history.append(snapshot)      async def snapshot_node_if_new(         self, snapshot_id: str, state: StateT, next_node: BaseNode[StateT, Any, RunEndT]     ) -> None:         if not any(s.id == snapshot_id for s in self.history):             await self.snapshot_node(state, next_node)      async def snapshot_end(self, state: StateT, end: End[RunEndT]) -> None:         snapshot = EndSnapshot(             state=self._prep_state(state),             result=end.deep_copy_data() if self.deep_copy else end,         )         self.history.append(snapshot)      @asynccontextmanager     async def record_run(self, snapshot_id: str) -> AsyncIterator[None]:         try:             snapshot = next(s for s in self.history if s.id == snapshot_id)         except StopIteration as e:             raise LookupError(f'No snapshot found with id={snapshot_id!r}') from e          assert isinstance(snapshot, NodeSnapshot), 'Only NodeSnapshot can be recorded'         exceptions.GraphNodeStatusError.check(snapshot.status)         snapshot.status = 'running'         snapshot.start_ts = _utils.now_utc()         start = perf_counter()         try:             yield         except Exception:             snapshot.duration = perf_counter() - start             snapshot.status = 'error'             raise         else:             snapshot.duration = perf_counter() - start             snapshot.status = 'success'      async def load_next(self) -> NodeSnapshot[StateT, RunEndT] | None:         if snapshot := next((s for s in self.history if isinstance(s, NodeSnapshot) and s.status == 'created'), None):             snapshot.status = 'pending'             return copy.deepcopy(snapshot)      async def load_all(self) -> list[Snapshot[StateT, RunEndT]]:         return self.history      def should_set_types(self) -> bool:         return self._snapshots_type_adapter is None      def set_types(self, state_type: type[StateT], run_end_type: type[RunEndT]) -> None:         self._snapshots_type_adapter = build_snapshot_list_type_adapter(state_type, run_end_type)      def dump_json(self, *, indent: int | None = None) -> bytes:         \"\"\"Dump the history to JSON bytes.\"\"\"         assert self._snapshots_type_adapter is not None, 'type adapter must be set to use `dump_json`'         return self._snapshots_type_adapter.dump_json(self.history, indent=indent)      def load_json(self, json_data: str | bytes | bytearray) -> None:         \"\"\"Load the history from JSON.\"\"\"         assert self._snapshots_type_adapter is not None, 'type adapter must be set to use `load_json`'         self.history = self._snapshots_type_adapter.validate_json(json_data)      def _prep_state(self, state: StateT) -> StateT:         \"\"\"Prepare state for snapshot, uses [`copy.deepcopy`][copy.deepcopy] by default.\"\"\"         if not self.deep_copy or state is None:             return state         else:             return copy.deepcopy(state) ``` |\n\n#### deep\\_copy `class-attribute` `instance-attribute`\n\n```\ndeep_copy: bool = True\n```\n\nWhether to deep copy the state and nodes when storing them.\n\nDefaults to `True` so even if nodes or state are modified after the snapshot is taken,\nthe persistence history will record the value at the time of the snapshot.\n\n#### history `class-attribute` `instance-attribute`", "url": "https://ai.pydantic.dev/pydantic_graph/persistence/index.html#fullstatepersistence-dataclass", "page": "pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "FullStatePersistence `dataclass`", "anchor": "fullstatepersistence-dataclass", "md_text": "```\nhistory: list[Snapshot[StateT, RunEndT]] = field(\n    default_factory=list\n)\n```\n\nList of snapshots taken during the graph run.\n\n#### dump\\_json\n\n```\ndump_json(*, indent: int | None = None) -> bytes\n```\n\nDump the history to JSON bytes.\n\nSource code in `pydantic_graph/pydantic_graph/persistence/in_mem.py`\n\n|  |  |\n| --- | --- |\n| ``` 157 158 159 160 ``` | ``` def dump_json(self, *, indent: int | None = None) -> bytes:     \"\"\"Dump the history to JSON bytes.\"\"\"     assert self._snapshots_type_adapter is not None, 'type adapter must be set to use `dump_json`'     return self._snapshots_type_adapter.dump_json(self.history, indent=indent) ``` |\n\n#### load\\_json\n\n```\nload_json(json_data: str | bytes | bytearray) -> None\n```\n\nLoad the history from JSON.\n\nSource code in `pydantic_graph/pydantic_graph/persistence/in_mem.py`\n\n|  |  |\n| --- | --- |\n| ``` 162 163 164 165 ``` | ``` def load_json(self, json_data: str | bytes | bytearray) -> None:     \"\"\"Load the history from JSON.\"\"\"     assert self._snapshots_type_adapter is not None, 'type adapter must be set to use `load_json`'     self.history = self._snapshots_type_adapter.validate_json(json_data) ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/persistence/index.html#fullstatepersistence-dataclass", "page": "pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "FileStatePersistence `dataclass`", "anchor": "filestatepersistence-dataclass", "md_text": "Bases: `BaseStatePersistence[StateT, RunEndT]`\n\nFile based state persistence that hold graph run state in a JSON file.\n\nSource code in `pydantic_graph/pydantic_graph/persistence/file.py`", "url": "https://ai.pydantic.dev/pydantic_graph/persistence/index.html#filestatepersistence-dataclass", "page": "pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "FileStatePersistence `dataclass`", "anchor": "filestatepersistence-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ```  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 ``` | ``` @dataclass class FileStatePersistence(BaseStatePersistence[StateT, RunEndT]):     \"\"\"File based state persistence that hold graph run state in a JSON file.\"\"\"      json_file: Path     \"\"\"Path to the JSON file where the snapshots are stored.      You should use a different file for each graph run, but a single file should be reused for multiple     steps of the same run.      For example if you have a run ID of the form `run_123abc`, you might create a `FileStatePersistence` thus:      ```py     from pathlib import Path      from pydantic_graph import FullStatePersistence      run_id = 'run_123abc'     persistence = FullStatePersistence(Path('runs') / f'{run_id}.json')     ```     \"\"\"     _snapshots_type_adapter: pydantic.TypeAdapter[list[Snapshot[StateT, RunEndT]]] | None = field(         default=None, init=False, repr=False     )      async def snapshot_node(self, state: StateT, next_node: BaseNode[StateT, Any, RunEndT]) -> None:         await self._append_save(NodeSnapshot(state=state, node=next_node))      async def snapshot_node_if_new(         self, snapshot_id: str, state: StateT, next_node: BaseNode[StateT, Any, RunEndT]     ) -> None:         async with self._lock():             snapshots = await self.load_all()             if not any(s.id == snapshot_id for s in snapshots):  # pragma: no branch                 await self._append_save(NodeSnapshot(state=state, node=next_node), lock=False)      async def snapshot_end(self, state: StateT, end: End[RunEndT]) -> None:         await self._append_save(EndSnapshot(state=state, result=end))      @asynccontextmanager     async def record_run(self, snapshot_id: str) -> AsyncIterator[None]:         async with self._lock():             snapshots = await self.load_all()             try:                 snapshot = next(s for s in snapshots if s.id == snapshot_id)             except StopIteration as e:                 raise LookupError(f'No snapshot found with id={snapshot_id!r}') from e              assert isinstance(snapshot, NodeSnapshot), 'Only NodeSnapshot can be recorded'             exceptions.GraphNodeStatusError.check(snapshot.status)             snapshot.status = 'running'             snapshot.start_ts = _utils.now_utc()             await self._save(snapshots)          start = perf_counter()         try:             yield         except Exception:             duration = perf_counter() - start             async with self._lock():                 await _graph_utils.run_in_executor(self._after_run_sync, snapshot_id, duration, 'error')             raise         else:             snapshot.duration = perf_counter() - start             async with self._lock():                 await _graph_utils.run_in_executor(self._after_run_sync, snapshot_id, snapshot.duration, 'success')      async def load_next(self) -> NodeSnapshot[StateT, RunEndT] | None:         async with self._lock():             snapshots = await self.load_all()             if snapshot := next((s for s in snapshots if isinstance(s, NodeSnapshot) and s.status == 'created'), None):                 snapshot.status = 'pending'                 await self._save(snapshots)                 return snapshot      def should_set_types(self) -> bool:         \"\"\"Whether types need to be set.\"\"\"         return self._snapshots_type_adapter is None      def set_types(self, state_type: type[StateT], run_end_type: type[RunEndT]) -> None:         self._snapshots_type_adapter = build_snapshot_list_type_adapter(state_type, run_end_type)      async def load_all(self) -> list[Snapshot[StateT, RunEndT]]:         return await _graph_utils.run_in_executor(self._load_sync)      def _load_sync(self) -> list[Snapshot[StateT, RunEndT]]:         assert self._snapshots_type_adapter is not None, 'snapshots type adapter must be set'         try:             content = self.json_file.read_bytes()         except FileNotFoundError:             return []         else:             return self._snapshots_type_adapter.validate_json(content)      def _after_run_sync(self, snapshot_id: str, duration: float, status: SnapshotStatus) -> None:         snapshots = self._load_sync()         snapshot = next(s for s in snapshots if s.id == snapshot_id)         assert isinstance(snapshot, NodeSnapshot), 'Only NodeSnapshot can be recorded'         snapshot.duration = duration         snapshot.status = status         self._save_sync(snapshots)      async def _save(self, snapshots: list[Snapshot[StateT, RunEndT]]) -> None:         await _graph_utils.run_in_executor(self._save_sync, snapshots)      def _save_sync(self, snapshots: list[Snapshot[StateT, RunEndT]]) -> None:         assert self._snapshots_type_adapter is not None, 'snapshots type adapter must be set'         self.json_file.write_bytes(self._snapshots_type_adapter.dump_json(snapshots, indent=2))      async def _append_save(self, snapshot: Snapshot[StateT, RunEndT], *, lock: bool = True) -> None:         assert self._snapshots_type_adapter is not None, 'snapshots type adapter must be set'         async with AsyncExitStack() as stack:             if lock:                 await stack.enter_async_context(self._lock())             snapshots = await self.load_all()             snapshots.append(snapshot)             await self._save(snapshots)      @asynccontextmanager     async def _lock(self, *, timeout: float = 1.0) -> AsyncIterator[None]:         \"\"\"Lock a file by checking and writing a `.pydantic-graph-persistence-lock` to it.          Args:             timeout: how long to wait for the lock          Returns: an async context manager that holds the lock         \"\"\"         lock_file = self.json_file.parent / f'{self.json_file.name}.pydantic-graph-persistence-lock'         lock_id = secrets.token_urlsafe().encode()          with anyio.fail_after(timeout):             while not await _file_append_check(lock_file, lock_id):                 await anyio.sleep(0.01)          try:             yield         finally:             await _graph_utils.run_in_executor(lock_file.unlink, missing_ok=True) ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/persistence/index.html#filestatepersistence-dataclass", "page": "pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "FileStatePersistence `dataclass`", "anchor": "filestatepersistence-dataclass", "md_text": "#### json\\_file `instance-attribute`\n\n```\njson_file: Path\n```\n\nPath to the JSON file where the snapshots are stored.\n\nYou should use a different file for each graph run, but a single file should be reused for multiple\nsteps of the same run.\n\nFor example if you have a run ID of the form `run_123abc`, you might create a `FileStatePersistence` thus:\n\n```\nfrom pathlib import Path\n\nfrom pydantic_graph import FullStatePersistence\n\nrun_id = 'run_123abc'\npersistence = FullStatePersistence(Path('runs') / f'{run_id}.json')\n```\n\n#### should\\_set\\_types\n\n```\nshould_set_types() -> bool\n```\n\nWhether types need to be set.\n\nSource code in `pydantic_graph/pydantic_graph/persistence/file.py`\n\n|  |  |\n| --- | --- |\n| ``` 104 105 106 ``` | ``` def should_set_types(self) -> bool:     \"\"\"Whether types need to be set.\"\"\"     return self._snapshots_type_adapter is None ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/persistence/index.html#filestatepersistence-dataclass", "page": "pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "Graph `dataclass`", "anchor": "graph-dataclass", "md_text": "Bases: `Generic[StateT, DepsT, InputT, OutputT]`\n\nA complete graph definition ready for execution.\n\nThe Graph class represents a complete workflow graph with typed inputs,\noutputs, state, and dependencies. It contains all nodes, edges, and\nmetadata needed for execution.\n\nType Parameters\n\nStateT: The type of the graph state\nDepsT: The type of the dependencies\nInputT: The type of the input data\nOutputT: The type of the output data\n\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph.py`", "url": "https://ai.pydantic.dev/pydantic_graph/beta/index.html#graph-dataclass", "page": "pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "Graph `dataclass`", "anchor": "graph-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 ``` | ``` @dataclass(repr=False) class Graph(Generic[StateT, DepsT, InputT, OutputT]):     \"\"\"A complete graph definition ready for execution.      The Graph class represents a complete workflow graph with typed inputs,     outputs, state, and dependencies. It contains all nodes, edges, and     metadata needed for execution.      Type Parameters:         StateT: The type of the graph state         DepsT: The type of the dependencies         InputT: The type of the input data         OutputT: The type of the output data     \"\"\"      name: str | None     \"\"\"Optional name for the graph, if not provided the name will be inferred from the calling frame on the first call to a graph method.\"\"\"      state_type: type[StateT]     \"\"\"The type of the graph state.\"\"\"      deps_type: type[DepsT]     \"\"\"The type of the dependencies.\"\"\"      input_type: type[InputT]     \"\"\"The type of the input data.\"\"\"      output_type: type[OutputT]     \"\"\"The type of the output data.\"\"\"      auto_instrument: bool     \"\"\"Whether to automatically create instrumentation spans.\"\"\"      nodes: dict[NodeID, AnyNode]     \"\"\"All nodes in the graph indexed by their ID.\"\"\"      edges_by_source: dict[NodeID, list[Path]]     \"\"\"Outgoing paths from each source node.\"\"\"      parent_forks: dict[JoinID, ParentFork[NodeID]]     \"\"\"Parent fork information for each join node.\"\"\"      def get_parent_fork(self, join_id: JoinID) -> ParentFork[NodeID]:         \"\"\"Get the parent fork information for a join node.          Args:             join_id: The ID of the join node          Returns:             The parent fork information for the join          Raises:             RuntimeError: If the join ID is not found or has no parent fork         \"\"\"         result = self.parent_forks.get(join_id)         if result is None:             raise RuntimeError(f'Node {join_id} is not a join node or did not have a dominating fork (this is a bug)')         return result      async def run(         self,         *,         state: StateT = None,         deps: DepsT = None,         inputs: InputT = None,         span: AbstractContextManager[AbstractSpan] | None = None,         infer_name: bool = True,     ) -> OutputT:         \"\"\"Execute the graph and return the final output.          This is the main entry point for graph execution. It runs the graph         to completion and returns the final output value.          Args:             state: The graph state instance             deps: The dependencies instance             inputs: The input data for the graph             span: Optional span for tracing/instrumentation             infer_name: Whether to infer the graph name from the calling frame.          Returns:             The final output from the graph execution         \"\"\"         if infer_name and self.name is None:             inferred_name = infer_obj_name(self, depth=2)             if inferred_name is not None:  # pragma: no branch                 self.name = inferred_name          async with self.iter(state=state, deps=deps, inputs=inputs, span=span, infer_name=False) as graph_run:             # Note: This would probably be better using `async for _ in graph_run`, but this tests the `next` method,             # which I'm less confident will be implemented correctly if not used on the critical path. We can change it             # once we have tests, etc.             event: Any = None             while True:                 try:                     event = await graph_run.next(event)                 except StopAsyncIteration:                     assert isinstance(event, EndMarker), 'Graph run should end with an EndMarker.'                     return cast(EndMarker[OutputT], event).value      @asynccontextmanager     async def iter(         self,         *,         state: StateT = None,         deps: DepsT = None,         inputs: InputT = None,         span: AbstractContextManager[AbstractSpan] | None = None,         infer_name: bool = True,     ) -> AsyncIterator[GraphRun[StateT, DepsT, OutputT]]:         \"\"\"Create an iterator for step-by-step graph execution.          This method allows for more fine-grained control over graph execution,         enabling inspection of intermediate states and results.          Args:             state: The graph state instance             deps: The dependencies instance             inputs: The input data for the graph             span: Optional span for tracing/instrumentation             infer_name: Whether to infer the graph name from the calling frame.          Yields:             A GraphRun instance that can be iterated for step-by-step execution         \"\"\"         if infer_name and self.name is None:             inferred_name = infer_obj_name(self, depth=3)  # depth=3 because asynccontextmanager adds one             if inferred_name is not None:  # pragma: no branch                 self.name = inferred_name          with ExitStack() as stack:             entered_span: AbstractSpan | None = None             if span is None:                 if self.auto_instrument:                     entered_span = stack.enter_context(logfire_span('run graph {graph.name}', graph=self))             else:                 entered_span = stack.enter_context(span)             traceparent = None if entered_span is None else get_traceparent(entered_span)             async with GraphRun[StateT, DepsT, OutputT](                 graph=self,                 state=state,                 deps=deps,                 inputs=inputs,                 traceparent=traceparent,             ) as graph_run:                 yield graph_run      def render(self, *, title: str | None = None, direction: StateDiagramDirection | None = None) -> str:         \"\"\"Render the graph as a Mermaid diagram string.          Args:             title: Optional title for the diagram             direction: Optional direction for the diagram layout          Returns:             A string containing the Mermaid diagram representation         \"\"\"         from pydantic_graph.beta.mermaid import build_mermaid_graph          return build_mermaid_graph(self.nodes, self.edges_by_source).render(title=title, direction=direction)      def __repr__(self) -> str:         super_repr = super().__repr__()  # include class and memory address         # Insert the result of calling `__str__` before the final '>' in the repr         return f'{super_repr[:-1]}\\n{self}\\n{super_repr[-1]}'      def __str__(self) -> str:         \"\"\"Return a Mermaid diagram representation of the graph.          Returns:             A string containing the Mermaid diagram of the graph         \"\"\"         return self.render() ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta/index.html#graph-dataclass", "page": "pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "Graph `dataclass`", "anchor": "graph-dataclass", "md_text": "#### name `instance-attribute`\n\n```\nname: str | None\n```\n\nOptional name for the graph, if not provided the name will be inferred from the calling frame on the first call to a graph method.\n\n#### state\\_type `instance-attribute`\n\n```\nstate_type: type[StateT]\n```\n\nThe type of the graph state.\n\n#### deps\\_type `instance-attribute`\n\n```\ndeps_type: type[DepsT]\n```\n\nThe type of the dependencies.\n\n#### input\\_type `instance-attribute`\n\n```\ninput_type: type[InputT]\n```\n\nThe type of the input data.\n\n#### output\\_type `instance-attribute`\n\n```\noutput_type: type[OutputT]\n```\n\nThe type of the output data.\n\n#### auto\\_instrument `instance-attribute`\n\n```\nauto_instrument: bool\n```\n\nWhether to automatically create instrumentation spans.\n\n#### nodes `instance-attribute`\n\n```\nnodes: dict[NodeID, AnyNode]\n```\n\nAll nodes in the graph indexed by their ID.\n\n#### edges\\_by\\_source `instance-attribute`\n\n```\nedges_by_source: dict[NodeID, list[Path]]\n```\n\nOutgoing paths from each source node.\n\n#### parent\\_forks `instance-attribute`\n\n```\nparent_forks: dict[JoinID, ParentFork[NodeID]]\n```\n\nParent fork information for each join node.\n\n#### get\\_parent\\_fork\n\n```\nget_parent_fork(join_id: JoinID) -> ParentFork[NodeID]\n```\n\nGet the parent fork information for a join node.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `join_id` | `JoinID` | The ID of the join node | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `ParentFork[NodeID]` | The parent fork information for the join |\n\nRaises:\n\n| Type | Description |\n| --- | --- |\n| `RuntimeError` | If the join ID is not found or has no parent fork |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph.py`\n\n|  |  |\n| --- | --- |\n| ``` 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 ``` | ``` def get_parent_fork(self, join_id: JoinID) -> ParentFork[NodeID]:     \"\"\"Get the parent fork information for a join node.      Args:         join_id: The ID of the join node      Returns:         The parent fork information for the join      Raises:         RuntimeError: If the join ID is not found or has no parent fork     \"\"\"     result = self.parent_forks.get(join_id)     if result is None:         raise RuntimeError(f'Node {join_id} is not a join node or did not have a dominating fork (this is a bug)')     return result ``` |\n\n#### run `async`\n\n```\nrun(\n    *,\n    state: StateT = None,\n    deps: DepsT = None,\n    inputs: InputT = None,\n    span: (\n        AbstractContextManager[AbstractSpan] | None\n    ) = None,\n    infer_name: bool = True\n) -> OutputT\n```\n\nExecute the graph and return the final output.\n\nThis is the main entry point for graph execution. It runs the graph\nto completion and returns the final output value.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `state` | `StateT` | The graph state instance | `None` |\n| `deps` | `DepsT` | The dependencies instance | `None` |\n| `inputs` | `InputT` | The input data for the graph | `None` |\n| `span` | `AbstractContextManager[AbstractSpan] | None` | Optional span for tracing/instrumentation | `None` |\n| `infer_name` | `bool` | Whether to infer the graph name from the calling frame. | `True` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `OutputT` | The final output from the graph execution |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph.py`", "url": "https://ai.pydantic.dev/pydantic_graph/beta/index.html#graph-dataclass", "page": "pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "Graph `dataclass`", "anchor": "graph-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 ``` | ``` async def run(     self,     *,     state: StateT = None,     deps: DepsT = None,     inputs: InputT = None,     span: AbstractContextManager[AbstractSpan] | None = None,     infer_name: bool = True, ) -> OutputT:     \"\"\"Execute the graph and return the final output.      This is the main entry point for graph execution. It runs the graph     to completion and returns the final output value.      Args:         state: The graph state instance         deps: The dependencies instance         inputs: The input data for the graph         span: Optional span for tracing/instrumentation         infer_name: Whether to infer the graph name from the calling frame.      Returns:         The final output from the graph execution     \"\"\"     if infer_name and self.name is None:         inferred_name = infer_obj_name(self, depth=2)         if inferred_name is not None:  # pragma: no branch             self.name = inferred_name      async with self.iter(state=state, deps=deps, inputs=inputs, span=span, infer_name=False) as graph_run:         # Note: This would probably be better using `async for _ in graph_run`, but this tests the `next` method,         # which I'm less confident will be implemented correctly if not used on the critical path. We can change it         # once we have tests, etc.         event: Any = None         while True:             try:                 event = await graph_run.next(event)             except StopAsyncIteration:                 assert isinstance(event, EndMarker), 'Graph run should end with an EndMarker.'                 return cast(EndMarker[OutputT], event).value ``` |\n\n#### iter `async`\n\n```\niter(\n    *,\n    state: StateT = None,\n    deps: DepsT = None,\n    inputs: InputT = None,\n    span: (\n        AbstractContextManager[AbstractSpan] | None\n    ) = None,\n    infer_name: bool = True\n) -> AsyncIterator[GraphRun[StateT, DepsT, OutputT]]\n```\n\nCreate an iterator for step-by-step graph execution.\n\nThis method allows for more fine-grained control over graph execution,\nenabling inspection of intermediate states and results.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `state` | `StateT` | The graph state instance | `None` |\n| `deps` | `DepsT` | The dependencies instance | `None` |\n| `inputs` | `InputT` | The input data for the graph | `None` |\n| `span` | `AbstractContextManager[AbstractSpan] | None` | Optional span for tracing/instrumentation | `None` |\n| `infer_name` | `bool` | Whether to infer the graph name from the calling frame. | `True` |\n\nYields:\n\n| Type | Description |\n| --- | --- |\n| `AsyncIterator[GraphRun[StateT, DepsT, OutputT]]` | A GraphRun instance that can be iterated for step-by-step execution |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph.py`", "url": "https://ai.pydantic.dev/pydantic_graph/beta/index.html#graph-dataclass", "page": "pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "Graph `dataclass`", "anchor": "graph-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 ``` | ``` @asynccontextmanager async def iter(     self,     *,     state: StateT = None,     deps: DepsT = None,     inputs: InputT = None,     span: AbstractContextManager[AbstractSpan] | None = None,     infer_name: bool = True, ) -> AsyncIterator[GraphRun[StateT, DepsT, OutputT]]:     \"\"\"Create an iterator for step-by-step graph execution.      This method allows for more fine-grained control over graph execution,     enabling inspection of intermediate states and results.      Args:         state: The graph state instance         deps: The dependencies instance         inputs: The input data for the graph         span: Optional span for tracing/instrumentation         infer_name: Whether to infer the graph name from the calling frame.      Yields:         A GraphRun instance that can be iterated for step-by-step execution     \"\"\"     if infer_name and self.name is None:         inferred_name = infer_obj_name(self, depth=3)  # depth=3 because asynccontextmanager adds one         if inferred_name is not None:  # pragma: no branch             self.name = inferred_name      with ExitStack() as stack:         entered_span: AbstractSpan | None = None         if span is None:             if self.auto_instrument:                 entered_span = stack.enter_context(logfire_span('run graph {graph.name}', graph=self))         else:             entered_span = stack.enter_context(span)         traceparent = None if entered_span is None else get_traceparent(entered_span)         async with GraphRun[StateT, DepsT, OutputT](             graph=self,             state=state,             deps=deps,             inputs=inputs,             traceparent=traceparent,         ) as graph_run:             yield graph_run ``` |\n\n#### render\n\n```\nrender(\n    *,\n    title: str | None = None,\n    direction: StateDiagramDirection | None = None\n) -> str\n```\n\nRender the graph as a Mermaid diagram string.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `title` | `str | None` | Optional title for the diagram | `None` |\n| `direction` | `StateDiagramDirection | None` | Optional direction for the diagram layout | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `str` | A string containing the Mermaid diagram representation |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph.py`\n\n|  |  |\n| --- | --- |\n| ``` 251 252 253 254 255 256 257 258 259 260 261 262 263 ``` | ``` def render(self, *, title: str | None = None, direction: StateDiagramDirection | None = None) -> str:     \"\"\"Render the graph as a Mermaid diagram string.      Args:         title: Optional title for the diagram         direction: Optional direction for the diagram layout      Returns:         A string containing the Mermaid diagram representation     \"\"\"     from pydantic_graph.beta.mermaid import build_mermaid_graph      return build_mermaid_graph(self.nodes, self.edges_by_source).render(title=title, direction=direction) ``` |\n\n#### \\_\\_str\\_\\_\n\n```\n__str__() -> str\n```\n\nReturn a Mermaid diagram representation of the graph.\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `str` | A string containing the Mermaid diagram of the graph |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph.py`\n\n|  |  |\n| --- | --- |\n| ``` 270 271 272 273 274 275 276 ``` | ``` def __str__(self) -> str:     \"\"\"Return a Mermaid diagram representation of the graph.      Returns:         A string containing the Mermaid diagram of the graph     \"\"\"     return self.render() ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta/index.html#graph-dataclass", "page": "pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "GraphBuilder `dataclass`", "anchor": "graphbuilder-dataclass", "md_text": "Bases: `Generic[StateT, DepsT, GraphInputT, GraphOutputT]`\n\nA builder for constructing executable graph definitions.\n\nGraphBuilder provides a fluent interface for defining nodes, edges, and\nrouting in a graph workflow. It supports typed state, dependencies, and\ninput/output validation.\n\nType Parameters\n\nStateT: The type of the graph state\nDepsT: The type of the dependencies\nGraphInputT: The type of the graph input data\nGraphOutputT: The type of the graph output data\n\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`", "url": "https://ai.pydantic.dev/pydantic_graph/beta/index.html#graphbuilder-dataclass", "page": "pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "GraphBuilder `dataclass`", "anchor": "graphbuilder-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ```  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 ``` | ``` @dataclass(init=False) class GraphBuilder(Generic[StateT, DepsT, GraphInputT, GraphOutputT]):     \"\"\"A builder for constructing executable graph definitions.      GraphBuilder provides a fluent interface for defining nodes, edges, and     routing in a graph workflow. It supports typed state, dependencies, and     input/output validation.      Type Parameters:         StateT: The type of the graph state         DepsT: The type of the dependencies         GraphInputT: The type of the graph input data         GraphOutputT: The type of the graph output data     \"\"\"      name: str | None     \"\"\"Optional name for the graph, if not provided the name will be inferred from the calling frame on the first call to a graph method.\"\"\"      state_type: TypeOrTypeExpression[StateT]     \"\"\"The type of the graph state.\"\"\"      deps_type: TypeOrTypeExpression[DepsT]     \"\"\"The type of the dependencies.\"\"\"      input_type: TypeOrTypeExpression[GraphInputT]     \"\"\"The type of the graph input data.\"\"\"      output_type: TypeOrTypeExpression[GraphOutputT]     \"\"\"The type of the graph output data.\"\"\"      auto_instrument: bool     \"\"\"Whether to automatically create instrumentation spans.\"\"\"      _nodes: dict[NodeID, AnyNode]     \"\"\"Internal storage for nodes in the graph.\"\"\"      _edges_by_source: dict[NodeID, list[Path]]     \"\"\"Internal storage for edges by source node.\"\"\"      _decision_index: int     \"\"\"Counter for generating unique decision node IDs.\"\"\"      Source = TypeAliasType('Source', SourceNode[StateT, DepsT, OutputT], type_params=(OutputT,))     Destination = TypeAliasType('Destination', DestinationNode[StateT, DepsT, InputT], type_params=(InputT,))      def __init__(         self,         *,         name: str | None = None,         state_type: TypeOrTypeExpression[StateT] = NoneType,         deps_type: TypeOrTypeExpression[DepsT] = NoneType,         input_type: TypeOrTypeExpression[GraphInputT] = NoneType,         output_type: TypeOrTypeExpression[GraphOutputT] = NoneType,         auto_instrument: bool = True,     ):         \"\"\"Initialize a graph builder.          Args:             name: Optional name for the graph, if not provided the name will be inferred from the calling frame on the first call to a graph method.             state_type: The type of the graph state             deps_type: The type of the dependencies             input_type: The type of the graph input data             output_type: The type of the graph output data             auto_instrument: Whether to automatically create instrumentation spans         \"\"\"         self.name = name          self.state_type = state_type         self.deps_type = deps_type         self.input_type = input_type         self.output_type = output_type          self.auto_instrument = auto_instrument          self._nodes = {}         self._edges_by_source = defaultdict(list)         self._decision_index = 1          self._start_node = StartNode[GraphInputT]()         self._end_node = EndNode[GraphOutputT]()      # Node building     @property     def start_node(self) -> StartNode[GraphInputT]:         \"\"\"Get the start node for the graph.          Returns:             The start node that receives the initial graph input         \"\"\"         return self._start_node      @property     def end_node(self) -> EndNode[GraphOutputT]:         \"\"\"Get the end node for the graph.          Returns:             The end node that produces the final graph output         \"\"\"         return self._end_node      @overload     def step(         self,         *,         node_id: str | None = None,         label: str | None = None,     ) -> Callable[[StepFunction[StateT, DepsT, InputT, OutputT]], Step[StateT, DepsT, InputT, OutputT]]: ...     @overload     def step(         self,         call: StepFunction[StateT, DepsT, InputT, OutputT],         *,         node_id: str | None = None,         label: str | None = None,     ) -> Step[StateT, DepsT, InputT, OutputT]: ...     def step(         self,         call: StepFunction[StateT, DepsT, InputT, OutputT] | None = None,         *,         node_id: str | None = None,         label: str | None = None,     ) -> (         Step[StateT, DepsT, InputT, OutputT]         | Callable[[StepFunction[StateT, DepsT, InputT, OutputT]], Step[StateT, DepsT, InputT, OutputT]]     ):         \"\"\"Create a step from a step function.          This method can be used as a decorator or called directly to create         a step node from an async function.          Args:             call: The step function to wrap             node_id: Optional ID for the node             label: Optional human-readable label          Returns:             Either a Step instance or a decorator function         \"\"\"         if call is None:              def decorator(                 func: StepFunction[StateT, DepsT, InputT, OutputT],             ) -> Step[StateT, DepsT, InputT, OutputT]:                 return self.step(call=func, node_id=node_id, label=label)              return decorator          node_id = node_id or get_callable_name(call)          step = Step[StateT, DepsT, InputT, OutputT](id=NodeID(node_id), call=call, label=label)          return step      @overload     def stream(         self,         *,         node_id: str | None = None,         label: str | None = None,     ) -> Callable[         [StreamFunction[StateT, DepsT, InputT, OutputT]], Step[StateT, DepsT, InputT, AsyncIterable[OutputT]]     ]: ...     @overload     def stream(         self,         call: StreamFunction[StateT, DepsT, InputT, OutputT],         *,         node_id: str | None = None,         label: str | None = None,     ) -> Step[StateT, DepsT, InputT, AsyncIterable[OutputT]]: ...     @overload     def stream(         self,         call: StreamFunction[StateT, DepsT, InputT, OutputT] | None = None,         *,         node_id: str | None = None,         label: str | None = None,     ) -> (         Step[StateT, DepsT, InputT, AsyncIterable[OutputT]]         | Callable[             [StreamFunction[StateT, DepsT, InputT, OutputT]],             Step[StateT, DepsT, InputT, AsyncIterable[OutputT]],         ]     ): ...     def stream(         self,         call: StreamFunction[StateT, DepsT, InputT, OutputT] | None = None,         *,         node_id: str | None = None,         label: str | None = None,     ) -> (         Step[StateT, DepsT, InputT, AsyncIterable[OutputT]]         | Callable[             [StreamFunction[StateT, DepsT, InputT, OutputT]],             Step[StateT, DepsT, InputT, AsyncIterable[OutputT]],         ]     ):         \"\"\"Create a step from an async iterator (which functions like a \"stream\").          This method can be used as a decorator or called directly to create         a step node from an async function.          Args:             call: The step function to wrap             node_id: Optional ID for the node             label: Optional human-readable label          Returns:             Either a Step instance or a decorator function         \"\"\"         if call is None:              def decorator(                 func: StreamFunction[StateT, DepsT, InputT, OutputT],             ) -> Step[StateT, DepsT, InputT, AsyncIterable[OutputT]]:                 return self.stream(call=func, node_id=node_id, label=label)              return decorator          # We need to wrap the call so that we can call `await` even though the result is an async iterator         async def wrapper(ctx: StepContext[StateT, DepsT, InputT]):             return call(ctx)          return self.step(call=wrapper, node_id=node_id, label=label)      @overload     def join(         self,         reducer: ReducerFunction[StateT, DepsT, InputT, OutputT],         *,         initial: OutputT,         node_id: str | None = None,         parent_fork_id: str | None = None,         preferred_parent_fork: Literal['farthest', 'closest'] = 'farthest',     ) -> Join[StateT, DepsT, InputT, OutputT]: ...     @overload     def join(         self,         reducer: ReducerFunction[StateT, DepsT, InputT, OutputT],         *,         initial_factory: Callable[[], OutputT],         node_id: str | None = None,         parent_fork_id: str | None = None,         preferred_parent_fork: Literal['farthest', 'closest'] = 'farthest',     ) -> Join[StateT, DepsT, InputT, OutputT]: ...      def join(         self,         reducer: ReducerFunction[StateT, DepsT, InputT, OutputT],         *,         initial: OutputT | Unset = UNSET,         initial_factory: Callable[[], OutputT] | Unset = UNSET,         node_id: str | None = None,         parent_fork_id: str | None = None,         preferred_parent_fork: Literal['farthest', 'closest'] = 'farthest',     ) -> Join[StateT, DepsT, InputT, OutputT]:         if initial_factory is UNSET:             initial_factory = lambda: initial  # pyright: ignore[reportAssignmentType]  # noqa E731          return Join[StateT, DepsT, InputT, OutputT](             id=JoinID(NodeID(node_id or generate_placeholder_node_id(get_callable_name(reducer)))),             reducer=reducer,             initial_factory=cast(Callable[[], OutputT], initial_factory),             parent_fork_id=ForkID(parent_fork_id) if parent_fork_id is not None else None,             preferred_parent_fork=preferred_parent_fork,         )      # Edge building     def add(self, *edges: EdgePath[StateT, DepsT]) -> None:  # noqa C901         \"\"\"Add one or more edge paths to the graph.          This method processes edge paths and automatically creates any necessary         fork nodes for broadcasts and maps.          Args:             *edges: The edge paths to add to the graph         \"\"\"          def _handle_path(p: Path):             \"\"\"Process a path and create necessary fork nodes.              Args:                 p: The path to process             \"\"\"             for item in p.items:                 if isinstance(item, BroadcastMarker):                     new_node = Fork[Any, Any](id=item.fork_id, is_map=False, downstream_join_id=None)                     self._insert_node(new_node)                     for path in item.paths:                         _handle_path(Path(items=[*path.items]))                 elif isinstance(item, MapMarker):                     new_node = Fork[Any, Any](id=item.fork_id, is_map=True, downstream_join_id=item.downstream_join_id)                     self._insert_node(new_node)                 elif isinstance(item, DestinationMarker):                     pass          def _handle_destination_node(d: AnyDestinationNode):             if id(d) in destination_ids:                 return  # prevent infinite recursion if there is a cycle of decisions              destination_ids.add(id(d))             destinations.append(d)             self._insert_node(d)             if isinstance(d, Decision):                 for branch in d.branches:                     _handle_path(branch.path)                     for d2 in branch.destinations:                         _handle_destination_node(d2)          destination_ids = set[int]()         destinations: list[AnyDestinationNode] = []         for edge in edges:             for source_node in edge.sources:                 self._insert_node(source_node)                 self._edges_by_source[source_node.id].append(edge.path)             for destination_node in edge.destinations:                 _handle_destination_node(destination_node)             _handle_path(edge.path)          # Automatically create edges from step function return hints including `BaseNode`s         for destination in destinations:             if not isinstance(destination, Step) or isinstance(destination, NodeStep):                 continue             parent_namespace = _utils.get_parent_namespace(inspect.currentframe())             type_hints = get_type_hints(destination.call, localns=parent_namespace, include_extras=True)             try:                 return_hint = type_hints['return']             except KeyError:                 pass             else:                 edge = self._edge_from_return_hint(destination, return_hint)                 if edge is not None:                     self.add(edge)      def add_edge(self, source: Source[T], destination: Destination[T], *, label: str | None = None) -> None:         \"\"\"Add a simple edge between two nodes.          Args:             source: The source node             destination: The destination node             label: Optional label for the edge         \"\"\"         builder = self.edge_from(source)         if label is not None:             builder = builder.label(label)         self.add(builder.to(destination))      def add_mapping_edge(         self,         source: Source[Iterable[T]],         map_to: Destination[T],         *,         pre_map_label: str | None = None,         post_map_label: str | None = None,         fork_id: ForkID | None = None,         downstream_join_id: JoinID | None = None,     ) -> None:         \"\"\"Add an edge that maps iterable data across parallel paths.          Args:             source: The source node that produces iterable data             map_to: The destination node that receives individual items             pre_map_label: Optional label before the map operation             post_map_label: Optional label after the map operation             fork_id: Optional ID for the fork node produced for this map operation             downstream_join_id: Optional ID of a join node that will always be downstream of this map.                 Specifying this ensures correct handling if you try to map an empty iterable.         \"\"\"         builder = self.edge_from(source)         if pre_map_label is not None:             builder = builder.label(pre_map_label)         builder = builder.map(fork_id=fork_id, downstream_join_id=downstream_join_id)         if post_map_label is not None:             builder = builder.label(post_map_label)         self.add(builder.to(map_to))      # TODO(DavidM): Support adding subgraphs; I think this behaves like a step with the same inputs/outputs but gets rendered as a subgraph in mermaid      def edge_from(self, *sources: Source[SourceOutputT]) -> EdgePathBuilder[StateT, DepsT, SourceOutputT]:         \"\"\"Create an edge path builder starting from the given source nodes.          Args:             *sources: The source nodes to start the edge path from          Returns:             An EdgePathBuilder for constructing the complete edge path         \"\"\"         return EdgePathBuilder[StateT, DepsT, SourceOutputT](             sources=sources, path_builder=PathBuilder(working_items=[])         )      def decision(self, *, note: str | None = None, node_id: str | None = None) -> Decision[StateT, DepsT, Never]:         \"\"\"Create a new decision node.          Args:             note: Optional note to describe the decision logic             node_id: Optional ID for the node produced for this decision logic          Returns:             A new Decision node with no branches         \"\"\"         return Decision(id=NodeID(node_id or generate_placeholder_node_id('decision')), branches=[], note=note)      def match(         self,         source: TypeOrTypeExpression[SourceT],         *,         matches: Callable[[Any], bool] | None = None,     ) -> DecisionBranchBuilder[StateT, DepsT, SourceT, SourceT, Never]:         \"\"\"Create a decision branch matcher.          Args:             source: The type or type expression to match against             matches: Optional custom matching function          Returns:             A DecisionBranchBuilder for constructing the branch         \"\"\"         # Note, the following node_id really is just a placeholder and shouldn't end up in the final graph         # This is why we don't expose a way for end users to override the value used here.         node_id = NodeID(generate_placeholder_node_id('match_decision'))         decision = Decision[StateT, DepsT, Never](id=node_id, branches=[], note=None)         new_path_builder = PathBuilder[StateT, DepsT, SourceT](working_items=[])         return DecisionBranchBuilder(decision=decision, source=source, matches=matches, path_builder=new_path_builder)      def match_node(         self,         source: type[SourceNodeT],         *,         matches: Callable[[Any], bool] | None = None,     ) -> DecisionBranch[SourceNodeT]:         \"\"\"Create a decision branch for BaseNode subclasses.          This is similar to match() but specifically designed for matching         against BaseNode types from the v1 system.          Args:             source: The BaseNode subclass to match against             matches: Optional custom matching function          Returns:             A DecisionBranch for the BaseNode type         \"\"\"         node = NodeStep(source)         path = Path(items=[DestinationMarker(node.id)])         return DecisionBranch(source=source, matches=matches, path=path, destinations=[node])      def node(         self,         node_type: type[BaseNode[StateT, DepsT, GraphOutputT]],     ) -> EdgePath[StateT, DepsT]:         \"\"\"Create an edge path from a BaseNode class.          This method integrates v1-style BaseNode classes into the v2 graph         system by analyzing their type hints and creating appropriate edges.          Args:             node_type: The BaseNode subclass to integrate          Returns:             An EdgePath representing the node and its connections          Raises:             GraphSetupError: If the node type is missing required type hints         \"\"\"         parent_namespace = _utils.get_parent_namespace(inspect.currentframe())         type_hints = get_type_hints(node_type.run, localns=parent_namespace, include_extras=True)         try:             return_hint = type_hints['return']         except KeyError as e:  # pragma: no cover             raise exceptions.GraphSetupError(                 f'Node {node_type} is missing a return type hint on its `run` method'             ) from e          node = NodeStep(node_type)          edge = self._edge_from_return_hint(node, return_hint)         if not edge:  # pragma: no cover             raise exceptions.GraphSetupError(f'Node {node_type} is missing a return type hint on its `run` method')          return edge      # Helpers     def _insert_node(self, node: AnyNode) -> None:         \"\"\"Insert a node into the graph, checking for ID conflicts.          Args:             node: The node to insert          Raises:             ValueError: If a different node with the same ID already exists         \"\"\"         existing = self._nodes.get(node.id)         if existing is None:             self._nodes[node.id] = node         elif isinstance(existing, NodeStep) and isinstance(node, NodeStep) and existing.node_type is node.node_type:             pass         elif existing is not node:             raise GraphBuildingError(                 f'All nodes must have unique node IDs. {node.id!r} was the ID for {existing} and {node}'             )      def _edge_from_return_hint(         self, node: SourceNode[StateT, DepsT, Any], return_hint: TypeOrTypeExpression[Any]     ) -> EdgePath[StateT, DepsT] | None:         \"\"\"Create edges from a return type hint.          This method analyzes return type hints from step functions or node methods         to automatically create appropriate edges in the graph.          Args:             node: The source node             return_hint: The return type hint to analyze          Returns:             An EdgePath if edges can be inferred, None otherwise          Raises:             GraphSetupError: If the return type hint is invalid or incomplete         \"\"\"         destinations: list[AnyDestinationNode] = []         union_args = _utils.get_union_args(return_hint)         for return_type in union_args:             return_type, annotations = _utils.unpack_annotated(return_type)             return_type_origin = get_origin(return_type) or return_type             if return_type_origin is End:                 destinations.append(self.end_node)             elif return_type_origin is BaseNode:                 raise exceptions.GraphSetupError(  # pragma: no cover                     f'Node {node} return type hint includes a plain `BaseNode`. '                     'Edge inference requires each possible returned `BaseNode` subclass to be listed explicitly.'                 )             elif return_type_origin is StepNode:                 step = cast(                     Step[StateT, DepsT, Any, Any] | None,                     next((a for a in annotations if isinstance(a, Step)), None),  # pyright: ignore[reportUnknownArgumentType]                 )                 if step is None:                     raise exceptions.GraphSetupError(  # pragma: no cover                         f'Node {node} return type hint includes a `StepNode` without a `Step` annotation. '                         'When returning `my_step.as_node()`, use `Annotated[StepNode[StateT, DepsT], my_step]` as the return type hint.'                     )                 destinations.append(step)             elif return_type_origin is JoinNode:                 join = cast(                     Join[StateT, DepsT, Any, Any] | None,                     next((a for a in annotations if isinstance(a, Join)), None),  # pyright: ignore[reportUnknownArgumentType]                 )                 if join is None:                     raise exceptions.GraphSetupError(  # pragma: no cover                         f'Node {node} return type hint includes a `JoinNode` without a `Join` annotation. '                         'When returning `my_join.as_node()`, use `Annotated[JoinNode[StateT, DepsT], my_join]` as the return type hint.'                     )                 destinations.append(join)             elif inspect.isclass(return_type_origin) and issubclass(return_type_origin, BaseNode):                 destinations.append(NodeStep(return_type))          if len(destinations) < len(union_args):             # Only build edges if all the return types are nodes             return None          edge = self.edge_from(node)         if len(destinations) == 1:             return edge.to(destinations[0])         else:             decision = self.decision()             for destination in destinations:                 # We don't actually use this decision mechanism, but we need to build the edges for parent-fork finding                 decision = decision.branch(self.match(NoneType).to(destination))             return edge.to(decision)      # Graph building     def build(self, validate_graph_structure: bool = True) -> Graph[StateT, DepsT, GraphInputT, GraphOutputT]:         \"\"\"Build the final executable graph from the accumulated nodes and edges.          This method performs validation, normalization, and analysis of the graph         structure to create a complete, executable graph instance.          Args:             validate_graph_structure: whether to perform validation of the graph structure                 See the docstring of _validate_graph_structure below for more details.          Returns:             A complete Graph instance ready for execution          Raises:             ValueError: If the graph structure is invalid (e.g., join without parent fork)         \"\"\"         nodes = self._nodes         edges_by_source = self._edges_by_source          nodes, edges_by_source = _replace_placeholder_node_ids(nodes, edges_by_source)         nodes, edges_by_source = _flatten_paths(nodes, edges_by_source)         nodes, edges_by_source = _normalize_forks(nodes, edges_by_source)         if validate_graph_structure:             _validate_graph_structure(nodes, edges_by_source)         parent_forks = _collect_dominating_forks(nodes, edges_by_source)          return Graph[StateT, DepsT, GraphInputT, GraphOutputT](             name=self.name,             state_type=unpack_type_expression(self.state_type),             deps_type=unpack_type_expression(self.deps_type),             input_type=unpack_type_expression(self.input_type),             output_type=unpack_type_expression(self.output_type),             nodes=nodes,             edges_by_source=edges_by_source,             parent_forks=parent_forks,             auto_instrument=self.auto_instrument,         ) ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta/index.html#graphbuilder-dataclass", "page": "pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "GraphBuilder `dataclass`", "anchor": "graphbuilder-dataclass", "md_text": "#### name `instance-attribute`\n\n```\nname: str | None = name\n```\n\nOptional name for the graph, if not provided the name will be inferred from the calling frame on the first call to a graph method.\n\n#### state\\_type `instance-attribute`\n\n```\nstate_type: TypeOrTypeExpression[StateT] = state_type\n```\n\nThe type of the graph state.\n\n#### deps\\_type `instance-attribute`\n\n```\ndeps_type: TypeOrTypeExpression[DepsT] = deps_type\n```\n\nThe type of the dependencies.\n\n#### input\\_type `instance-attribute`\n\n```\ninput_type: TypeOrTypeExpression[GraphInputT] = input_type\n```\n\nThe type of the graph input data.\n\n#### output\\_type `instance-attribute`\n\n```\noutput_type: TypeOrTypeExpression[GraphOutputT] = (\n    output_type\n)\n```\n\nThe type of the graph output data.\n\n#### auto\\_instrument `instance-attribute`\n\n```\nauto_instrument: bool = auto_instrument\n```\n\nWhether to automatically create instrumentation spans.\n\n#### \\_\\_init\\_\\_\n\n```\n__init__(\n    *,\n    name: str | None = None,\n    state_type: TypeOrTypeExpression[StateT] = NoneType,\n    deps_type: TypeOrTypeExpression[DepsT] = NoneType,\n    input_type: TypeOrTypeExpression[\n        GraphInputT\n    ] = NoneType,\n    output_type: TypeOrTypeExpression[\n        GraphOutputT\n    ] = NoneType,\n    auto_instrument: bool = True\n)\n```\n\nInitialize a graph builder.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `name` | `str | None` | Optional name for the graph, if not provided the name will be inferred from the calling frame on the first call to a graph method. | `None` |\n| `state_type` | `TypeOrTypeExpression[StateT]` | The type of the graph state | `NoneType` |\n| `deps_type` | `TypeOrTypeExpression[DepsT]` | The type of the dependencies | `NoneType` |\n| `input_type` | `TypeOrTypeExpression[GraphInputT]` | The type of the graph input data | `NoneType` |\n| `output_type` | `TypeOrTypeExpression[GraphOutputT]` | The type of the graph output data | `NoneType` |\n| `auto_instrument` | `bool` | Whether to automatically create instrumentation spans | `True` |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`\n\n|  |  |\n| --- | --- |\n| ``` 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 ``` | ``` def __init__(     self,     *,     name: str | None = None,     state_type: TypeOrTypeExpression[StateT] = NoneType,     deps_type: TypeOrTypeExpression[DepsT] = NoneType,     input_type: TypeOrTypeExpression[GraphInputT] = NoneType,     output_type: TypeOrTypeExpression[GraphOutputT] = NoneType,     auto_instrument: bool = True, ):     \"\"\"Initialize a graph builder.      Args:         name: Optional name for the graph, if not provided the name will be inferred from the calling frame on the first call to a graph method.         state_type: The type of the graph state         deps_type: The type of the dependencies         input_type: The type of the graph input data         output_type: The type of the graph output data         auto_instrument: Whether to automatically create instrumentation spans     \"\"\"     self.name = name      self.state_type = state_type     self.deps_type = deps_type     self.input_type = input_type     self.output_type = output_type      self.auto_instrument = auto_instrument      self._nodes = {}     self._edges_by_source = defaultdict(list)     self._decision_index = 1      self._start_node = StartNode[GraphInputT]()     self._end_node = EndNode[GraphOutputT]() ``` |\n\n#### start\\_node `property`\n\n```\nstart_node: StartNode[GraphInputT]\n```\n\nGet the start node for the graph.\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `StartNode[GraphInputT]` | The start node that receives the initial graph input |\n\n#### end\\_node `property`\n\n```\nend_node: EndNode[GraphOutputT]\n```\n\nGet the end node for the graph.\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `EndNode[GraphOutputT]` | The end node that produces the final graph output |\n\n#### step\n\n```\nstep(\n    *, node_id: str | None = None, label: str | None = None\n) -> Callable[\n    [StepFunction[StateT, DepsT, InputT, OutputT]],\n    Step[StateT, DepsT, InputT, OutputT],\n]\n\nstep(\n    call: StepFunction[StateT, DepsT, InputT, OutputT],\n    *,\n    node_id: str | None = None,\n    label: str | None = None\n) -> Step[StateT, DepsT, InputT, OutputT]", "url": "https://ai.pydantic.dev/pydantic_graph/beta/index.html#graphbuilder-dataclass", "page": "pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "GraphBuilder `dataclass`", "anchor": "graphbuilder-dataclass", "md_text": "step(\n    call: (\n        StepFunction[StateT, DepsT, InputT, OutputT] | None\n    ) = None,\n    *,\n    node_id: str | None = None,\n    label: str | None = None\n) -> (\n    Step[StateT, DepsT, InputT, OutputT]\n    | Callable[\n        [StepFunction[StateT, DepsT, InputT, OutputT]],\n        Step[StateT, DepsT, InputT, OutputT],\n    ]\n)\n```\n\nCreate a step from a step function.\n\nThis method can be used as a decorator or called directly to create\na step node from an async function.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `call` | `StepFunction[StateT, DepsT, InputT, OutputT] | None` | The step function to wrap | `None` |\n| `node_id` | `str | None` | Optional ID for the node | `None` |\n| `label` | `str | None` | Optional human-readable label | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `Step[StateT, DepsT, InputT, OutputT] | Callable[[StepFunction[StateT, DepsT, InputT, OutputT]], Step[StateT, DepsT, InputT, OutputT]]` | Either a Step instance or a decorator function |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`\n\n|  |  |\n| --- | --- |\n| ``` 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 ``` | ``` def step(     self,     call: StepFunction[StateT, DepsT, InputT, OutputT] | None = None,     *,     node_id: str | None = None,     label: str | None = None, ) -> (     Step[StateT, DepsT, InputT, OutputT]     | Callable[[StepFunction[StateT, DepsT, InputT, OutputT]], Step[StateT, DepsT, InputT, OutputT]] ):     \"\"\"Create a step from a step function.      This method can be used as a decorator or called directly to create     a step node from an async function.      Args:         call: The step function to wrap         node_id: Optional ID for the node         label: Optional human-readable label      Returns:         Either a Step instance or a decorator function     \"\"\"     if call is None:          def decorator(             func: StepFunction[StateT, DepsT, InputT, OutputT],         ) -> Step[StateT, DepsT, InputT, OutputT]:             return self.step(call=func, node_id=node_id, label=label)          return decorator      node_id = node_id or get_callable_name(call)      step = Step[StateT, DepsT, InputT, OutputT](id=NodeID(node_id), call=call, label=label)      return step ``` |\n\n#### stream\n\n```\nstream(\n    *, node_id: str | None = None, label: str | None = None\n) -> Callable[\n    [StreamFunction[StateT, DepsT, InputT, OutputT]],\n    Step[StateT, DepsT, InputT, AsyncIterable[OutputT]],\n]\n\nstream(\n    call: StreamFunction[StateT, DepsT, InputT, OutputT],\n    *,\n    node_id: str | None = None,\n    label: str | None = None\n) -> Step[StateT, DepsT, InputT, AsyncIterable[OutputT]]\n\nstream(\n    call: (\n        StreamFunction[StateT, DepsT, InputT, OutputT]\n        | None\n    ) = None,\n    *,\n    node_id: str | None = None,\n    label: str | None = None\n) -> (\n    Step[StateT, DepsT, InputT, AsyncIterable[OutputT]]\n    | Callable[\n        [StreamFunction[StateT, DepsT, InputT, OutputT]],\n        Step[StateT, DepsT, InputT, AsyncIterable[OutputT]],\n    ]\n)\n\nstream(\n    call: (\n        StreamFunction[StateT, DepsT, InputT, OutputT]\n        | None\n    ) = None,\n    *,\n    node_id: str | None = None,\n    label: str | None = None\n) -> (\n    Step[StateT, DepsT, InputT, AsyncIterable[OutputT]]\n    | Callable[\n        [StreamFunction[StateT, DepsT, InputT, OutputT]],\n        Step[StateT, DepsT, InputT, AsyncIterable[OutputT]],\n    ]\n)\n```\n\nCreate a step from an async iterator (which functions like a \"stream\").\n\nThis method can be used as a decorator or called directly to create\na step node from an async function.\n\nParameters:", "url": "https://ai.pydantic.dev/pydantic_graph/beta/index.html#graphbuilder-dataclass", "page": "pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "GraphBuilder `dataclass`", "anchor": "graphbuilder-dataclass", "md_text": "| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `call` | `StreamFunction[StateT, DepsT, InputT, OutputT] | None` | The step function to wrap | `None` |\n| `node_id` | `str | None` | Optional ID for the node | `None` |\n| `label` | `str | None` | Optional human-readable label | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `Step[StateT, DepsT, InputT, AsyncIterable[OutputT]] | Callable[[StreamFunction[StateT, DepsT, InputT, OutputT]], Step[StateT, DepsT, InputT, AsyncIterable[OutputT]]]` | Either a Step instance or a decorator function |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`\n\n|  |  |\n| --- | --- |\n| ``` 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 ``` | ``` def stream(     self,     call: StreamFunction[StateT, DepsT, InputT, OutputT] | None = None,     *,     node_id: str | None = None,     label: str | None = None, ) -> (     Step[StateT, DepsT, InputT, AsyncIterable[OutputT]]     | Callable[         [StreamFunction[StateT, DepsT, InputT, OutputT]],         Step[StateT, DepsT, InputT, AsyncIterable[OutputT]],     ] ):     \"\"\"Create a step from an async iterator (which functions like a \"stream\").      This method can be used as a decorator or called directly to create     a step node from an async function.      Args:         call: The step function to wrap         node_id: Optional ID for the node         label: Optional human-readable label      Returns:         Either a Step instance or a decorator function     \"\"\"     if call is None:          def decorator(             func: StreamFunction[StateT, DepsT, InputT, OutputT],         ) -> Step[StateT, DepsT, InputT, AsyncIterable[OutputT]]:             return self.stream(call=func, node_id=node_id, label=label)          return decorator      # We need to wrap the call so that we can call `await` even though the result is an async iterator     async def wrapper(ctx: StepContext[StateT, DepsT, InputT]):         return call(ctx)      return self.step(call=wrapper, node_id=node_id, label=label) ``` |\n\n#### add\n\n```\nadd(*edges: EdgePath[StateT, DepsT]) -> None\n```\n\nAdd one or more edge paths to the graph.\n\nThis method processes edge paths and automatically creates any necessary\nfork nodes for broadcasts and maps.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `*edges` | `EdgePath[StateT, DepsT]` | The edge paths to add to the graph | `()` |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`", "url": "https://ai.pydantic.dev/pydantic_graph/beta/index.html#graphbuilder-dataclass", "page": "pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "GraphBuilder `dataclass`", "anchor": "graphbuilder-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 ``` | ``` def add(self, *edges: EdgePath[StateT, DepsT]) -> None:  # noqa C901     \"\"\"Add one or more edge paths to the graph.      This method processes edge paths and automatically creates any necessary     fork nodes for broadcasts and maps.      Args:         *edges: The edge paths to add to the graph     \"\"\"      def _handle_path(p: Path):         \"\"\"Process a path and create necessary fork nodes.          Args:             p: The path to process         \"\"\"         for item in p.items:             if isinstance(item, BroadcastMarker):                 new_node = Fork[Any, Any](id=item.fork_id, is_map=False, downstream_join_id=None)                 self._insert_node(new_node)                 for path in item.paths:                     _handle_path(Path(items=[*path.items]))             elif isinstance(item, MapMarker):                 new_node = Fork[Any, Any](id=item.fork_id, is_map=True, downstream_join_id=item.downstream_join_id)                 self._insert_node(new_node)             elif isinstance(item, DestinationMarker):                 pass      def _handle_destination_node(d: AnyDestinationNode):         if id(d) in destination_ids:             return  # prevent infinite recursion if there is a cycle of decisions          destination_ids.add(id(d))         destinations.append(d)         self._insert_node(d)         if isinstance(d, Decision):             for branch in d.branches:                 _handle_path(branch.path)                 for d2 in branch.destinations:                     _handle_destination_node(d2)      destination_ids = set[int]()     destinations: list[AnyDestinationNode] = []     for edge in edges:         for source_node in edge.sources:             self._insert_node(source_node)             self._edges_by_source[source_node.id].append(edge.path)         for destination_node in edge.destinations:             _handle_destination_node(destination_node)         _handle_path(edge.path)      # Automatically create edges from step function return hints including `BaseNode`s     for destination in destinations:         if not isinstance(destination, Step) or isinstance(destination, NodeStep):             continue         parent_namespace = _utils.get_parent_namespace(inspect.currentframe())         type_hints = get_type_hints(destination.call, localns=parent_namespace, include_extras=True)         try:             return_hint = type_hints['return']         except KeyError:             pass         else:             edge = self._edge_from_return_hint(destination, return_hint)             if edge is not None:                 self.add(edge) ``` |\n\n#### add\\_edge\n\n```\nadd_edge(\n    source: Source[T],\n    destination: Destination[T],\n    *,\n    label: str | None = None\n) -> None\n```\n\nAdd a simple edge between two nodes.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `source` | `Source[T]` | The source node | *required* |\n| `destination` | `Destination[T]` | The destination node | *required* |\n| `label` | `str | None` | Optional label for the edge | `None` |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`\n\n|  |  |\n| --- | --- |\n| ``` 398 399 400 401 402 403 404 405 406 407 408 409 ``` | ``` def add_edge(self, source: Source[T], destination: Destination[T], *, label: str | None = None) -> None:     \"\"\"Add a simple edge between two nodes.      Args:         source: The source node         destination: The destination node         label: Optional label for the edge     \"\"\"     builder = self.edge_from(source)     if label is not None:         builder = builder.label(label)     self.add(builder.to(destination)) ``` |\n\n#### add\\_mapping\\_edge\n\n```\nadd_mapping_edge(\n    source: Source[Iterable[T]],\n    map_to: Destination[T],\n    *,\n    pre_map_label: str | None = None,\n    post_map_label: str | None = None,\n    fork_id: ForkID | None = None,\n    downstream_join_id: JoinID | None = None\n) -> None\n```\n\nAdd an edge that maps iterable data across parallel paths.\n\nParameters:", "url": "https://ai.pydantic.dev/pydantic_graph/beta/index.html#graphbuilder-dataclass", "page": "pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "GraphBuilder `dataclass`", "anchor": "graphbuilder-dataclass", "md_text": "| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `source` | `Source[Iterable[T]]` | The source node that produces iterable data | *required* |\n| `map_to` | `Destination[T]` | The destination node that receives individual items | *required* |\n| `pre_map_label` | `str | None` | Optional label before the map operation | `None` |\n| `post_map_label` | `str | None` | Optional label after the map operation | `None` |\n| `fork_id` | `ForkID | None` | Optional ID for the fork node produced for this map operation | `None` |\n| `downstream_join_id` | `JoinID | None` | Optional ID of a join node that will always be downstream of this map. Specifying this ensures correct handling if you try to map an empty iterable. | `None` |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`\n\n|  |  |\n| --- | --- |\n| ``` 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 ``` | ``` def add_mapping_edge(     self,     source: Source[Iterable[T]],     map_to: Destination[T],     *,     pre_map_label: str | None = None,     post_map_label: str | None = None,     fork_id: ForkID | None = None,     downstream_join_id: JoinID | None = None, ) -> None:     \"\"\"Add an edge that maps iterable data across parallel paths.      Args:         source: The source node that produces iterable data         map_to: The destination node that receives individual items         pre_map_label: Optional label before the map operation         post_map_label: Optional label after the map operation         fork_id: Optional ID for the fork node produced for this map operation         downstream_join_id: Optional ID of a join node that will always be downstream of this map.             Specifying this ensures correct handling if you try to map an empty iterable.     \"\"\"     builder = self.edge_from(source)     if pre_map_label is not None:         builder = builder.label(pre_map_label)     builder = builder.map(fork_id=fork_id, downstream_join_id=downstream_join_id)     if post_map_label is not None:         builder = builder.label(post_map_label)     self.add(builder.to(map_to)) ``` |\n\n#### edge\\_from\n\n```\nedge_from(\n    *sources: Source[SourceOutputT],\n) -> EdgePathBuilder[StateT, DepsT, SourceOutputT]\n```\n\nCreate an edge path builder starting from the given source nodes.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `*sources` | `Source[SourceOutputT]` | The source nodes to start the edge path from | `()` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `EdgePathBuilder[StateT, DepsT, SourceOutputT]` | An EdgePathBuilder for constructing the complete edge path |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`\n\n|  |  |\n| --- | --- |\n| ``` 442 443 444 445 446 447 448 449 450 451 452 453 ``` | ``` def edge_from(self, *sources: Source[SourceOutputT]) -> EdgePathBuilder[StateT, DepsT, SourceOutputT]:     \"\"\"Create an edge path builder starting from the given source nodes.      Args:         *sources: The source nodes to start the edge path from      Returns:         An EdgePathBuilder for constructing the complete edge path     \"\"\"     return EdgePathBuilder[StateT, DepsT, SourceOutputT](         sources=sources, path_builder=PathBuilder(working_items=[])     ) ``` |\n\n#### decision\n\n```\ndecision(\n    *, note: str | None = None, node_id: str | None = None\n) -> Decision[StateT, DepsT, Never]\n```\n\nCreate a new decision node.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `note` | `str | None` | Optional note to describe the decision logic | `None` |\n| `node_id` | `str | None` | Optional ID for the node produced for this decision logic | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `Decision[StateT, DepsT, Never]` | A new Decision node with no branches |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`\n\n|  |  |\n| --- | --- |\n| ``` 455 456 457 458 459 460 461 462 463 464 465 ``` | ``` def decision(self, *, note: str | None = None, node_id: str | None = None) -> Decision[StateT, DepsT, Never]:     \"\"\"Create a new decision node.      Args:         note: Optional note to describe the decision logic         node_id: Optional ID for the node produced for this decision logic      Returns:         A new Decision node with no branches     \"\"\"     return Decision(id=NodeID(node_id or generate_placeholder_node_id('decision')), branches=[], note=note) ``` |\n\n#### match", "url": "https://ai.pydantic.dev/pydantic_graph/beta/index.html#graphbuilder-dataclass", "page": "pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "GraphBuilder `dataclass`", "anchor": "graphbuilder-dataclass", "md_text": "```\nmatch(\n    source: TypeOrTypeExpression[SourceT],\n    *,\n    matches: Callable[[Any], bool] | None = None\n) -> DecisionBranchBuilder[\n    StateT, DepsT, SourceT, SourceT, Never\n]\n```\n\nCreate a decision branch matcher.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `source` | `TypeOrTypeExpression[SourceT]` | The type or type expression to match against | *required* |\n| `matches` | `Callable[[Any], bool] | None` | Optional custom matching function | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `DecisionBranchBuilder[StateT, DepsT, SourceT, SourceT, Never]` | A DecisionBranchBuilder for constructing the branch |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`\n\n|  |  |\n| --- | --- |\n| ``` 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 ``` | ``` def match(     self,     source: TypeOrTypeExpression[SourceT],     *,     matches: Callable[[Any], bool] | None = None, ) -> DecisionBranchBuilder[StateT, DepsT, SourceT, SourceT, Never]:     \"\"\"Create a decision branch matcher.      Args:         source: The type or type expression to match against         matches: Optional custom matching function      Returns:         A DecisionBranchBuilder for constructing the branch     \"\"\"     # Note, the following node_id really is just a placeholder and shouldn't end up in the final graph     # This is why we don't expose a way for end users to override the value used here.     node_id = NodeID(generate_placeholder_node_id('match_decision'))     decision = Decision[StateT, DepsT, Never](id=node_id, branches=[], note=None)     new_path_builder = PathBuilder[StateT, DepsT, SourceT](working_items=[])     return DecisionBranchBuilder(decision=decision, source=source, matches=matches, path_builder=new_path_builder) ``` |\n\n#### match\\_node\n\n```\nmatch_node(\n    source: type[SourceNodeT],\n    *,\n    matches: Callable[[Any], bool] | None = None\n) -> DecisionBranch[SourceNodeT]\n```\n\nCreate a decision branch for BaseNode subclasses.\n\nThis is similar to match() but specifically designed for matching\nagainst BaseNode types from the v1 system.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `source` | `type[SourceNodeT]` | The BaseNode subclass to match against | *required* |\n| `matches` | `Callable[[Any], bool] | None` | Optional custom matching function | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `DecisionBranch[SourceNodeT]` | A DecisionBranch for the BaseNode type |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`\n\n|  |  |\n| --- | --- |\n| ``` 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 ``` | ``` def match_node(     self,     source: type[SourceNodeT],     *,     matches: Callable[[Any], bool] | None = None, ) -> DecisionBranch[SourceNodeT]:     \"\"\"Create a decision branch for BaseNode subclasses.      This is similar to match() but specifically designed for matching     against BaseNode types from the v1 system.      Args:         source: The BaseNode subclass to match against         matches: Optional custom matching function      Returns:         A DecisionBranch for the BaseNode type     \"\"\"     node = NodeStep(source)     path = Path(items=[DestinationMarker(node.id)])     return DecisionBranch(source=source, matches=matches, path=path, destinations=[node]) ``` |\n\n#### node\n\n```\nnode(\n    node_type: type[BaseNode[StateT, DepsT, GraphOutputT]],\n) -> EdgePath[StateT, DepsT]\n```\n\nCreate an edge path from a BaseNode class.\n\nThis method integrates v1-style BaseNode classes into the v2 graph\nsystem by analyzing their type hints and creating appropriate edges.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `node_type` | `type[BaseNode[StateT, DepsT, GraphOutputT]]` | The BaseNode subclass to integrate | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `EdgePath[StateT, DepsT]` | An EdgePath representing the node and its connections |\n\nRaises:\n\n| Type | Description |\n| --- | --- |\n| `GraphSetupError` | If the node type is missing required type hints |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`", "url": "https://ai.pydantic.dev/pydantic_graph/beta/index.html#graphbuilder-dataclass", "page": "pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "GraphBuilder `dataclass`", "anchor": "graphbuilder-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 ``` | ``` def node(     self,     node_type: type[BaseNode[StateT, DepsT, GraphOutputT]], ) -> EdgePath[StateT, DepsT]:     \"\"\"Create an edge path from a BaseNode class.      This method integrates v1-style BaseNode classes into the v2 graph     system by analyzing their type hints and creating appropriate edges.      Args:         node_type: The BaseNode subclass to integrate      Returns:         An EdgePath representing the node and its connections      Raises:         GraphSetupError: If the node type is missing required type hints     \"\"\"     parent_namespace = _utils.get_parent_namespace(inspect.currentframe())     type_hints = get_type_hints(node_type.run, localns=parent_namespace, include_extras=True)     try:         return_hint = type_hints['return']     except KeyError as e:  # pragma: no cover         raise exceptions.GraphSetupError(             f'Node {node_type} is missing a return type hint on its `run` method'         ) from e      node = NodeStep(node_type)      edge = self._edge_from_return_hint(node, return_hint)     if not edge:  # pragma: no cover         raise exceptions.GraphSetupError(f'Node {node_type} is missing a return type hint on its `run` method')      return edge ``` |\n\n#### build\n\n```\nbuild(\n    validate_graph_structure: bool = True,\n) -> Graph[StateT, DepsT, GraphInputT, GraphOutputT]\n```\n\nBuild the final executable graph from the accumulated nodes and edges.\n\nThis method performs validation, normalization, and analysis of the graph\nstructure to create a complete, executable graph instance.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `validate_graph_structure` | `bool` | whether to perform validation of the graph structure See the docstring of \\_validate\\_graph\\_structure below for more details. | `True` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `Graph[StateT, DepsT, GraphInputT, GraphOutputT]` | A complete Graph instance ready for execution |\n\nRaises:\n\n| Type | Description |\n| --- | --- |\n| `ValueError` | If the graph structure is invalid (e.g., join without parent fork) |\n\nSource code in `pydantic_graph/pydantic_graph/beta/graph_builder.py`\n\n|  |  |\n| --- | --- |\n| ``` 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 ``` | ``` def build(self, validate_graph_structure: bool = True) -> Graph[StateT, DepsT, GraphInputT, GraphOutputT]:     \"\"\"Build the final executable graph from the accumulated nodes and edges.      This method performs validation, normalization, and analysis of the graph     structure to create a complete, executable graph instance.      Args:         validate_graph_structure: whether to perform validation of the graph structure             See the docstring of _validate_graph_structure below for more details.      Returns:         A complete Graph instance ready for execution      Raises:         ValueError: If the graph structure is invalid (e.g., join without parent fork)     \"\"\"     nodes = self._nodes     edges_by_source = self._edges_by_source      nodes, edges_by_source = _replace_placeholder_node_ids(nodes, edges_by_source)     nodes, edges_by_source = _flatten_paths(nodes, edges_by_source)     nodes, edges_by_source = _normalize_forks(nodes, edges_by_source)     if validate_graph_structure:         _validate_graph_structure(nodes, edges_by_source)     parent_forks = _collect_dominating_forks(nodes, edges_by_source)      return Graph[StateT, DepsT, GraphInputT, GraphOutputT](         name=self.name,         state_type=unpack_type_expression(self.state_type),         deps_type=unpack_type_expression(self.deps_type),         input_type=unpack_type_expression(self.input_type),         output_type=unpack_type_expression(self.output_type),         nodes=nodes,         edges_by_source=edges_by_source,         parent_forks=parent_forks,         auto_instrument=self.auto_instrument,     ) ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta/index.html#graphbuilder-dataclass", "page": "pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "EndNode", "anchor": "endnode", "md_text": "Bases: `Generic[InputT]`\n\nTerminal node representing the completion of graph execution.\n\nThe EndNode marks the successful completion of a graph execution flow\nand can collect the final output data.\n\nSource code in `pydantic_graph/pydantic_graph/beta/node.py`\n\n|  |  |\n| --- | --- |\n| ``` 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 ``` | ``` class EndNode(Generic[InputT]):     \"\"\"Terminal node representing the completion of graph execution.      The EndNode marks the successful completion of a graph execution flow     and can collect the final output data.     \"\"\"      id = NodeID('__end__')     \"\"\"Fixed identifier for the end node.\"\"\"      def _force_variance(self, inputs: InputT) -> None:  # pragma: no cover         \"\"\"Force type variance for proper generic typing.          This method exists solely for type checking purposes and should never be called.          Args:             inputs: Input data of type InputT.          Raises:             RuntimeError: Always, as this method should never be executed.         \"\"\"         raise RuntimeError('This method should never be called, it is just defined for typing purposes.') ``` |\n\n#### id `class-attribute` `instance-attribute`\n\n```\nid = NodeID('__end__')\n```\n\nFixed identifier for the end node.", "url": "https://ai.pydantic.dev/pydantic_graph/beta/index.html#endnode", "page": "pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "StartNode", "anchor": "startnode", "md_text": "Bases: `Generic[OutputT]`\n\nEntry point node for graph execution.\n\nThe StartNode represents the beginning of a graph execution flow.\n\nSource code in `pydantic_graph/pydantic_graph/beta/node.py`\n\n|  |  |\n| --- | --- |\n| ``` 26 27 28 29 30 31 32 33 ``` | ``` class StartNode(Generic[OutputT]):     \"\"\"Entry point node for graph execution.      The StartNode represents the beginning of a graph execution flow.     \"\"\"      id = NodeID('__start__')     \"\"\"Fixed identifier for the start node.\"\"\" ``` |\n\n#### id `class-attribute` `instance-attribute`\n\n```\nid = NodeID('__start__')\n```\n\nFixed identifier for the start node.", "url": "https://ai.pydantic.dev/pydantic_graph/beta/index.html#startnode", "page": "pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "StepContext `dataclass`", "anchor": "stepcontext-dataclass", "md_text": "Bases: `Generic[StateT, DepsT, InputT]`\n\nContext information passed to step functions during graph execution.\n\nThe step context provides access to the current graph state, dependencies, and input data for a step.\n\nType Parameters\n\nStateT: The type of the graph state\nDepsT: The type of the dependencies\nInputT: The type of the input data\n\n\nSource code in `pydantic_graph/pydantic_graph/beta/step.py`\n\n|  |  |\n| --- | --- |\n| ``` 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 ``` | ``` @dataclass(init=False) class StepContext(Generic[StateT, DepsT, InputT]):     \"\"\"Context information passed to step functions during graph execution.      The step context provides access to the current graph state, dependencies, and input data for a step.      Type Parameters:         StateT: The type of the graph state         DepsT: The type of the dependencies         InputT: The type of the input data     \"\"\"      _state: StateT     \"\"\"The current graph state.\"\"\"     _deps: DepsT     \"\"\"The graph run dependencies.\"\"\"     _inputs: InputT     \"\"\"The input data for this step.\"\"\"      def __init__(self, *, state: StateT, deps: DepsT, inputs: InputT):         self._state = state         self._deps = deps         self._inputs = inputs      @property     def state(self) -> StateT:         return self._state      @property     def deps(self) -> DepsT:         return self._deps      @property     def inputs(self) -> InputT:         \"\"\"The input data for this step.          This must be a property to ensure correct variance behavior         \"\"\"         return self._inputs ``` |\n\n#### inputs `property`\n\n```\ninputs: InputT\n```\n\nThe input data for this step.\n\nThis must be a property to ensure correct variance behavior", "url": "https://ai.pydantic.dev/pydantic_graph/beta/index.html#stepcontext-dataclass", "page": "pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "StepNode `dataclass`", "anchor": "stepnode-dataclass", "md_text": "Bases: `BaseNode[StateT, DepsT, Any]`\n\nA base node that represents a step with bound inputs.\n\nStepNode bridges between the v1 and v2 graph execution systems by wrapping\na [`Step`](../beta_step/index.html#pydantic_graph.beta.step.Step) with bound inputs in a BaseNode interface.\nIt is not meant to be run directly but rather used to indicate transitions\nto v2-style steps.\n\nSource code in `pydantic_graph/pydantic_graph/beta/step.py`\n\n|  |  |\n| --- | --- |\n| ``` 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 ``` | ``` @dataclass class StepNode(BaseNode[StateT, DepsT, Any]):     \"\"\"A base node that represents a step with bound inputs.      StepNode bridges between the v1 and v2 graph execution systems by wrapping     a [`Step`][pydantic_graph.beta.step.Step] with bound inputs in a BaseNode interface.     It is not meant to be run directly but rather used to indicate transitions     to v2-style steps.     \"\"\"      step: Step[StateT, DepsT, Any, Any]     \"\"\"The step to execute.\"\"\"      inputs: Any     \"\"\"The inputs bound to this step.\"\"\"      async def run(self, ctx: GraphRunContext[StateT, DepsT]) -> BaseNode[StateT, DepsT, Any] | End[Any]:         \"\"\"Attempt to run the step node.          Args:             ctx: The graph execution context          Returns:             The result of step execution          Raises:             NotImplementedError: Always raised as StepNode is not meant to be run directly         \"\"\"         raise NotImplementedError(             '`StepNode` is not meant to be run directly, it is meant to be used in `BaseNode` subclasses to indicate a transition to v2-style steps.'         ) ``` |\n\n#### step `instance-attribute`\n\n```\nstep: Step[StateT, DepsT, Any, Any]\n```\n\nThe step to execute.\n\n#### inputs `instance-attribute`\n\n```\ninputs: Any\n```\n\nThe inputs bound to this step.\n\n#### run `async`\n\n```\nrun(\n    ctx: GraphRunContext[StateT, DepsT],\n) -> BaseNode[StateT, DepsT, Any] | End[Any]\n```\n\nAttempt to run the step node.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `ctx` | `GraphRunContext[StateT, DepsT]` | The graph execution context | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `BaseNode[StateT, DepsT, Any] | End[Any]` | The result of step execution |\n\nRaises:\n\n| Type | Description |\n| --- | --- |\n| `NotImplementedError` | Always raised as StepNode is not meant to be run directly |\n\nSource code in `pydantic_graph/pydantic_graph/beta/step.py`\n\n|  |  |\n| --- | --- |\n| ``` 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 ``` | ``` async def run(self, ctx: GraphRunContext[StateT, DepsT]) -> BaseNode[StateT, DepsT, Any] | End[Any]:     \"\"\"Attempt to run the step node.      Args:         ctx: The graph execution context      Returns:         The result of step execution      Raises:         NotImplementedError: Always raised as StepNode is not meant to be run directly     \"\"\"     raise NotImplementedError(         '`StepNode` is not meant to be run directly, it is meant to be used in `BaseNode` subclasses to indicate a transition to v2-style steps.'     ) ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta/index.html#stepnode-dataclass", "page": "pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "TypeExpression", "anchor": "typeexpression", "md_text": "Bases: `Generic[T]`\n\nA workaround for type checker limitations when using complex type expressions.\n\n```\nThis class serves as a wrapper for types that cannot normally be used in positions\n```\n\nrequiring `type[T]`, such as `Any`, `Union[...]`, or `Literal[...]`. It provides a\nway to pass these complex type expressions to functions expecting concrete types.\n\nExample\n\nInstead of `output_type=Union[str, int]` (which may cause type errors),\nuse `output_type=TypeExpression[Union[str, int]]`.\n\n\nNote\n\nThis is a workaround for the lack of TypeForm in the Python type system.\n\n\nSource code in `pydantic_graph/pydantic_graph/beta/util.py`\n\n|  |  |\n| --- | --- |\n| ``` 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 ``` | ``` class TypeExpression(Generic[T]):     \"\"\"A workaround for type checker limitations when using complex type expressions.          This class serves as a wrapper for types that cannot normally be used in positions     requiring `type[T]`, such as `Any`, `Union[...]`, or `Literal[...]`. It provides a         way to pass these complex type expressions to functions expecting concrete types.      Example:             Instead of `output_type=Union[str, int]` (which may cause type errors),             use `output_type=TypeExpression[Union[str, int]]`.      Note:             This is a workaround for the lack of TypeForm in the Python type system.     \"\"\"      pass ``` |", "url": "https://ai.pydantic.dev/pydantic_graph/beta/index.html#typeexpression", "page": "pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "KnownModelName `module-attribute`", "anchor": "knownmodelname-module-attribute", "md_text": "```\nKnownModelName = TypeAliasType(\n    \"KnownModelName\",\n    Literal[\n        \"anthropic:claude-3-5-haiku-20241022\",\n        \"anthropic:claude-3-5-haiku-latest\",\n        \"anthropic:claude-3-5-sonnet-20240620\",\n        \"anthropic:claude-3-5-sonnet-20241022\",\n        \"anthropic:claude-3-5-sonnet-latest\",\n        \"anthropic:claude-haiku-4-5\",\n        \"anthropic:claude-haiku-4-5-20251001\",\n        \"anthropic:claude-3-7-sonnet-20250219\",\n        \"anthropic:claude-3-7-sonnet-latest\",\n        \"anthropic:claude-3-haiku-20240307\",\n        \"anthropic:claude-3-opus-20240229\",\n        \"anthropic:claude-3-opus-latest\",\n        \"anthropic:claude-4-opus-20250514\",\n        \"anthropic:claude-4-sonnet-20250514\",\n        \"anthropic:claude-opus-4-0\",\n        \"anthropic:claude-opus-4-1-20250805\",\n        \"anthropic:claude-opus-4-20250514\",\n        \"anthropic:claude-sonnet-4-0\",\n        \"anthropic:claude-sonnet-4-20250514\",\n        \"anthropic:claude-sonnet-4-5\",\n        \"anthropic:claude-sonnet-4-5-20250929\",\n        \"bedrock:amazon.titan-tg1-large\",\n        \"bedrock:amazon.titan-text-lite-v1\",\n        \"bedrock:amazon.titan-text-express-v1\",\n        \"bedrock:us.amazon.nova-pro-v1:0\",\n        \"bedrock:us.amazon.nova-lite-v1:0\",\n        \"bedrock:us.amazon.nova-micro-v1:0\",\n        \"bedrock:anthropic.claude-3-5-sonnet-20241022-v2:0\",\n        \"bedrock:us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n        \"bedrock:anthropic.claude-3-5-haiku-20241022-v1:0\",\n        \"bedrock:us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n        \"bedrock:anthropic.claude-instant-v1\",\n        \"bedrock:anthropic.claude-v2:1\",\n        \"bedrock:anthropic.claude-v2\",\n        \"bedrock:anthropic.claude-3-sonnet-20240229-v1:0\",\n        \"bedrock:us.anthropic.claude-3-sonnet-20240229-v1:0\",\n        \"bedrock:anthropic.claude-3-haiku-20240307-v1:0\",\n        \"bedrock:us.anthropic.claude-3-haiku-20240307-v1:0\",\n        \"bedrock:anthropic.claude-3-opus-20240229-v1:0\",\n        \"bedrock:us.anthropic.claude-3-opus-20240229-v1:0\",\n        \"bedrock:anthropic.claude-3-5-sonnet-20240620-v1:0\",\n        \"bedrock:us.anthropic.claude-3-5-sonnet-20240620-v1:0\",\n        \"bedrock:anthropic.claude-3-7-sonnet-20250219-v1:0\",\n        \"bedrock:us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n        \"bedrock:anthropic.claude-opus-4-20250514-v1:0\",\n        \"bedrock:us.anthropic.claude-opus-4-20250514-v1:0\",\n        \"bedrock:anthropic.claude-sonnet-4-20250514-v1:0\",\n        \"bedrock:us.anthropic.claude-sonnet-4-20250514-v1:0\",\n        \"bedrock:cohere.command-text-v14\",\n        \"bedrock:cohere.command-r-v1:0\",\n        \"bedrock:cohere.command-r-plus-v1:0\",\n        \"bedrock:cohere.command-light-text-v14\",\n        \"bedrock:meta.llama3-8b-instruct-v1:0\",\n        \"bedrock:meta.llama3-70b-instruct-v1:0\",\n        \"bedrock:meta.llama3-1-8b-instruct-v1:0\",\n        \"bedrock:us.meta.llama3-1-8b-instruct-v1:0\",\n        \"bedrock:meta.llama3-1-70b-instruct-v1:0\",\n        \"bedrock:us.meta.llama3-1-70b-instruct-v1:0\",\n        \"bedrock:meta.llama3-1-405b-instruct-v1:0\",\n        \"bedrock:us.meta.llama3-2-11b-instruct-v1:0\",\n        \"bedrock:us.meta.llama3-2-90b-instruct-v1:0\",\n        \"bedrock:us.meta.llama3-2-1b-instruct-v1:0\",\n        \"bedrock:us.meta.llama3-2-3b-instruct-v1:0\",\n        \"bedrock:us.meta.llama3-3-70b-instruct-v1:0\",\n        \"bedrock:mistral.mistral-7b-instruct-v0:2\",\n        \"bedrock:mistral.mixtral-8x7b-instruct-v0:1\",\n        \"bedrock:mistral.mistral-large-2402-v1:0\",\n        \"bedrock:mistral.mistral-large-2407-v1:0\",\n        \"cerebras:gpt-oss-120b\",\n        \"cerebras:llama3.1-8b\",\n        \"cerebras:llama-3.3-70b\",\n        \"cerebras:llama-4-scout-17b-16e-instruct\",\n        \"cerebras:llama-4-maverick-17b-128e-instruct\",\n        \"cerebras:qwen-3-235b-a22b-instruct-2507\",\n        \"cerebras:qwen-3-32b\",\n        \"cerebras:qwen-3-coder-480b\",\n        \"cerebras:qwen-3-235b-a22b-thinking-2507\",\n        \"cohere:c4ai-aya-expanse-32b\",\n        \"cohere:c4ai-aya-expanse-8b\",\n        \"cohere:command-nightly\",\n        \"cohere:command-r-08-2024\",\n        \"cohere:command-r-plus-08-2024\",\n        \"cohere:command-r7b-12-2024\",\n        \"deepseek:deepseek-chat\",\n        \"deepseek:deepseek-reasoner\",\n        \"google-gla:gemini-2.0-flash\",\n        \"google-gla:gemini-2.0-flash-lite\",\n        \"google-gla:gemini-2.5-flash\",\n        \"google-gla:gemini-2.5-flash-preview-09-2025\",\n        \"google-gla:gemini-flash-latest\",\n        \"google-gla:gemini-2.5-flash-lite\",\n        \"google-gla:gemini-2.5-flash-lite-preview-09-2025\",\n        \"google-gla:gemini-flash-lite-latest\",\n        \"google-gla:gemini-2.5-pro\",\n        \"google-vertex:gemini-2.0-flash\",\n        \"google-vertex:gemini-2.0-flash-lite\",\n        \"google-vertex:gemini-2.5-flash\",\n        \"google-vertex:gemini-2.5-flash-preview-09-2025\",\n        \"google-vertex:gemini-flash-latest\",\n        \"google-vertex:gemini-2.5-flash-lite\",\n        \"google-vertex:gemini-2.5-flash-lite-preview-09-2025\",\n        \"google-vertex:gemini-flash-lite-latest\",\n        \"google-vertex:gemini-2.5-pro\",\n        \"grok:grok-4\",\n        \"grok:grok-4-0709\",\n        \"grok:grok-3\",\n        \"grok:grok-3-mini\",\n        \"grok:grok-3-fast\",\n        \"grok:grok-3-mini-fast\",\n        \"grok:grok-2-vision-1212\",\n        \"grok:grok-2-image-1212\",\n        \"groq:distil-whisper-large-v3-en\",\n        \"groq:gemma2-9b-it\",\n        \"groq:llama-3.3-70b-versatile\",\n        \"groq:llama-3.1-8b-instant\",\n        \"groq:llama-guard-3-8b\",\n        \"groq:llama3-70b-8192\",\n        \"groq:llama3-8b-8192\",\n        \"groq:moonshotai/kimi-k2-instruct\",\n        \"groq:whisper-large-v3\",\n        \"groq:whisper-large-v3-turbo\",\n        \"groq:playai-tts\",\n        \"groq:playai-tts-arabic\",\n        \"groq:qwen-qwq-32b\",\n        \"groq:mistral-saba-24b\",\n        \"groq:qwen-2.5-coder-32b\",\n        \"groq:qwen-2.5-32b\",\n        \"groq:deepseek-r1-distill-qwen-32b\",\n        \"groq:deepseek-r1-distill-llama-70b\",\n        \"groq:llama-3.3-70b-specdec\",\n        \"groq:llama-3.2-1b-preview\",\n        \"groq:llama-3.2-3b-preview\",\n        \"groq:llama-3.2-11b-vision-preview\",\n        \"groq:llama-3.2-90b-vision-preview\",\n        \"heroku:claude-3-5-haiku\",\n        \"heroku:claude-3-5-sonnet-latest\",\n        \"heroku:claude-3-7-sonnet\",\n        \"heroku:claude-4-sonnet\",\n        \"heroku:claude-3-haiku\",\n        \"heroku:gpt-oss-120b\",\n        \"heroku:nova-lite\",\n        \"heroku:nova-pro\",\n        \"huggingface:Qwen/QwQ-32B\",\n        \"huggingface:Qwen/Qwen2.5-72B-Instruct\",\n        \"huggingface:Qwen/Qwen3-235B-A22B\",\n        \"huggingface:Qwen/Qwen3-32B\",\n        \"huggingface:deepseek-ai/DeepSeek-R1\",\n        \"huggingface:meta-llama/Llama-3.3-70B-Instruct\",\n        \"huggingface:meta-llama/Llama-4-Maverick-17B-128E-Instruct\",\n        \"huggingface:meta-llama/Llama-4-Scout-17B-16E-Instruct\",\n        \"mistral:codestral-latest\",\n        \"mistral:mistral-large-latest\",\n        \"mistral:mistral-moderation-latest\",\n        \"mistral:mistral-small-latest\",\n        \"moonshotai:moonshot-v1-8k\",\n        \"moonshotai:moonshot-v1-32k\",\n        \"moonshotai:moonshot-v1-128k\",\n        \"moonshotai:moonshot-v1-8k-vision-preview\",\n        \"moonshotai:moonshot-v1-32k-vision-preview\",\n        \"moonshotai:moonshot-v1-128k-vision-preview\",\n        \"moonshotai:kimi-latest\",\n        \"moonshotai:kimi-thinking-preview\",\n        \"moonshotai:kimi-k2-0711-preview\",\n        \"openai:chatgpt-4o-latest\",\n        \"openai:codex-mini-latest\",\n        \"openai:gpt-3.5-turbo\",\n        \"openai:gpt-3.5-turbo-0125\",\n        \"openai:gpt-3.5-turbo-0301\",\n        \"openai:gpt-3.5-turbo-0613\",\n        \"openai:gpt-3.5-turbo-1106\",\n        \"openai:gpt-3.5-turbo-16k\",\n        \"openai:gpt-3.5-turbo-16k-0613\",\n        \"openai:gpt-4\",\n        \"openai:gpt-4-0125-preview\",\n        \"openai:gpt-4-0314\",\n        \"openai:gpt-4-0613\",\n        \"openai:gpt-4-1106-preview\",\n        \"openai:gpt-4-32k\",\n        \"openai:gpt-4-32k-0314\",\n        \"openai:gpt-4-32k-0613\",\n        \"openai:gpt-4-turbo\",\n        \"openai:gpt-4-turbo-2024-04-09\",\n        \"openai:gpt-4-turbo-preview\",\n        \"openai:gpt-4-vision-preview\",\n        \"openai:gpt-4.1\",\n        \"openai:gpt-4.1-2025-04-14\",\n        \"openai:gpt-4.1-mini\",\n        \"openai:gpt-4.1-mini-2025-04-14\",\n        \"openai:gpt-4.1-nano\",\n        \"openai:gpt-4.1-nano-2025-04-14\",\n        \"openai:gpt-4o\",\n        \"openai:gpt-4o-2024-05-13\",\n        \"openai:gpt-4o-2024-08-06\",\n        \"openai:gpt-4o-2024-11-20\",\n        \"openai:gpt-4o-audio-preview\",\n        \"openai:gpt-4o-audio-preview-2024-10-01\",\n        \"openai:gpt-4o-audio-preview-2024-12-17\",\n        \"openai:gpt-4o-audio-preview-2025-06-03\",\n        \"openai:gpt-4o-mini\",\n        \"openai:gpt-4o-mini-2024-07-18\",\n        \"openai:gpt-4o-mini-audio-preview\",\n        \"openai:gpt-4o-mini-audio-preview-2024-12-17\",\n        \"openai:gpt-4o-mini-search-preview\",\n        \"openai:gpt-4o-mini-search-preview-2025-03-11\",\n        \"openai:gpt-4o-search-preview\",\n        \"openai:gpt-4o-search-preview-2025-03-11\",\n        \"openai:gpt-5\",\n        \"openai:gpt-5-2025-08-07\",\n        \"openai:o1\",\n        \"openai:gpt-5-chat-latest\",\n        \"openai:o1-2024-12-17\",\n        \"openai:gpt-5-mini\",\n        \"openai:o1-mini\",\n        \"openai:gpt-5-mini-2025-08-07\",\n        \"openai:o1-mini-2024-09-12\",\n        \"openai:gpt-5-nano\",\n        \"openai:o1-preview\",\n        \"openai:gpt-5-nano-2025-08-07\",\n        \"openai:o1-preview-2024-09-12\",\n        \"openai:o1-pro\",\n        \"openai:o1-pro-2025-03-19\",\n        \"openai:o3\",\n        \"openai:o3-2025-04-16\",\n        \"openai:o3-deep-research\",\n        \"openai:o3-deep-research-2025-06-26\",\n        \"openai:o3-mini\",\n        \"openai:o3-mini-2025-01-31\",\n        \"openai:o4-mini\",\n        \"openai:o4-mini-2025-04-16\",\n        \"openai:o4-mini-deep-research\",\n        \"openai:o4-mini-deep-research-2025-06-26\",\n        \"openai:o3-pro\",\n        \"openai:o3-pro-2025-06-10\",\n        \"openai:computer-use-preview\",\n        \"openai:computer-use-preview-2025-03-11\",\n        \"test\",\n    ],\n)\n```", "url": "https://ai.pydantic.dev/models/base/index.html#knownmodelname-module-attribute", "page": "models/base/index.html", "source_site": "pydantic_ai"}
{"title": "KnownModelName `module-attribute`", "anchor": "knownmodelname-module-attribute", "md_text": "Known model names that can be used with the `model` parameter of [`Agent`](../../agent/index.html#pydantic_ai.agent.Agent).\n\n`KnownModelName` is provided as a concise way to specify a model.", "url": "https://ai.pydantic.dev/models/base/index.html#knownmodelname-module-attribute", "page": "models/base/index.html", "source_site": "pydantic_ai"}
{"title": "ModelRequestParameters `dataclass`", "anchor": "modelrequestparameters-dataclass", "md_text": "Configuration for an agent's request to a model, specifically related to tools and output handling.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 ``` | ``` @dataclass(repr=False, kw_only=True) class ModelRequestParameters:     \"\"\"Configuration for an agent's request to a model, specifically related to tools and output handling.\"\"\"      function_tools: list[ToolDefinition] = field(default_factory=list)     builtin_tools: list[AbstractBuiltinTool] = field(default_factory=list)      output_mode: OutputMode = 'text'     output_object: OutputObjectDefinition | None = None     output_tools: list[ToolDefinition] = field(default_factory=list)     allow_text_output: bool = True     allow_image_output: bool = False      @cached_property     def tool_defs(self) -> dict[str, ToolDefinition]:         return {tool_def.name: tool_def for tool_def in [*self.function_tools, *self.output_tools]}      __repr__ = _utils.dataclasses_no_defaults_repr ``` |", "url": "https://ai.pydantic.dev/models/base/index.html#modelrequestparameters-dataclass", "page": "models/base/index.html", "source_site": "pydantic_ai"}
{"title": "Model", "anchor": "model", "md_text": "Bases: `ABC`\n\nAbstract class for a model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`", "url": "https://ai.pydantic.dev/models/base/index.html#model", "page": "models/base/index.html", "source_site": "pydantic_ai"}
{"title": "Model", "anchor": "model", "md_text": "|  |  |\n| --- | --- |\n| ``` 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 ``` | ``` class Model(ABC):     \"\"\"Abstract class for a model.\"\"\"      _profile: ModelProfileSpec | None = None     _settings: ModelSettings | None = None      def __init__(         self,         *,         settings: ModelSettings | None = None,         profile: ModelProfileSpec | None = None,     ) -> None:         \"\"\"Initialize the model with optional settings and profile.          Args:             settings: Model-specific settings that will be used as defaults for this model.             profile: The model profile to use.         \"\"\"         self._settings = settings         self._profile = profile      @property     def settings(self) -> ModelSettings | None:         \"\"\"Get the model settings.\"\"\"         return self._settings      @abstractmethod     async def request(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> ModelResponse:         \"\"\"Make a request to the model.\"\"\"         raise NotImplementedError()      async def count_tokens(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> RequestUsage:         \"\"\"Make a request to the model for counting tokens.\"\"\"         # This method is not required, but you need to implement it if you want to support `UsageLimits.count_tokens_before_request`.         raise NotImplementedError(f'Token counting ahead of the request is not supported by {self.__class__.__name__}')      @asynccontextmanager     async def request_stream(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,         run_context: RunContext[Any] | None = None,     ) -> AsyncIterator[StreamedResponse]:         \"\"\"Make a request to the model and return a streaming response.\"\"\"         # This method is not required, but you need to implement it if you want to support streamed responses         raise NotImplementedError(f'Streamed requests not supported by this {self.__class__.__name__}')         # yield is required to make this a generator for type checking         # noinspection PyUnreachableCode         yield  # pragma: no cover      def customize_request_parameters(self, model_request_parameters: ModelRequestParameters) -> ModelRequestParameters:         \"\"\"Customize the request parameters for the model.          This method can be overridden by subclasses to modify the request parameters before sending them to the model.         In particular, this method can be used to make modifications to the generated tool JSON schemas if necessary         for vendor/model-specific reasons.         \"\"\"         if transformer := self.profile.json_schema_transformer:             model_request_parameters = replace(                 model_request_parameters,                 function_tools=[_customize_tool_def(transformer, t) for t in model_request_parameters.function_tools],                 output_tools=[_customize_tool_def(transformer, t) for t in model_request_parameters.output_tools],             )             if output_object := model_request_parameters.output_object:                 model_request_parameters = replace(                     model_request_parameters,                     output_object=_customize_output_object(transformer, output_object),                 )          return model_request_parameters      def prepare_request(         self,         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> tuple[ModelSettings | None, ModelRequestParameters]:         \"\"\"Prepare request inputs before they are passed to the provider.          This merges the given ``model_settings`` with the model's own ``settings`` attribute and ensures         ``customize_request_parameters`` is applied to the resolved         [`ModelRequestParameters`][pydantic_ai.models.ModelRequestParameters]. Subclasses can override this method if         they need to customize the preparation flow further, but most implementations should simply call         ``self.prepare_request(...)`` at the start of their ``request`` (and related) methods.         \"\"\"         model_settings = merge_model_settings(self.settings, model_settings)          if builtin_tools := model_request_parameters.builtin_tools:             # Deduplicate builtin tools             model_request_parameters = replace(                 model_request_parameters,                 builtin_tools=list({tool.unique_id: tool for tool in builtin_tools}.values()),             )          model_request_parameters = self.customize_request_parameters(model_request_parameters)         return model_settings, model_request_parameters      @property     @abstractmethod     def model_name(self) -> str:         \"\"\"The model name.\"\"\"         raise NotImplementedError()      @cached_property     def profile(self) -> ModelProfile:         \"\"\"The model profile.\"\"\"         _profile = self._profile         if callable(_profile):             _profile = _profile(self.model_name)          if _profile is None:             return DEFAULT_PROFILE          return _profile      @property     @abstractmethod     def system(self) -> str:         \"\"\"The model provider, ex: openai.          Use to populate the `gen_ai.system` OpenTelemetry semantic convention attribute,         so should use well-known values listed in         https://opentelemetry.io/docs/specs/semconv/attributes-registry/gen-ai/#gen-ai-system         when applicable.         \"\"\"         raise NotImplementedError()      @property     def base_url(self) -> str | None:         \"\"\"The base URL for the provider API, if available.\"\"\"         return None      @staticmethod     def _get_instructions(messages: list[ModelMessage]) -> str | None:         \"\"\"Get instructions from the first ModelRequest found when iterating messages in reverse.          In the case that a \"mock\" request was generated to include a tool-return part for a result tool,         we want to use the instructions from the second-to-most-recent request (which should correspond to the         original request that generated the response that resulted in the tool-return part).         \"\"\"         last_two_requests: list[ModelRequest] = []         for message in reversed(messages):             if isinstance(message, ModelRequest):                 last_two_requests.append(message)                 if len(last_two_requests) == 2:                     break                 if message.instructions is not None:                     return message.instructions          # If we don't have two requests, and we didn't already return instructions, there are definitely not any:         if len(last_two_requests) != 2:             return None          most_recent_request = last_two_requests[0]         second_most_recent_request = last_two_requests[1]          # If we've gotten this far and the most recent request consists of only tool-return parts or retry-prompt parts,         # we use the instructions from the second-to-most-recent request. This is necessary because when handling         # result tools, we generate a \"mock\" ModelRequest with a tool-return part for it, and that ModelRequest will not         # have the relevant instructions from the agent.          # While it's possible that you could have a message history where the most recent request has only tool returns,         # I believe there is no way to achieve that would _change_ the instructions without manually crafting the most         # recent message. That might make sense in principle for some usage pattern, but it's enough of an edge case         # that I think it's not worth worrying about, since you can work around this by inserting another ModelRequest         # with no parts at all immediately before the request that has the tool calls (that works because we only look         # at the two most recent ModelRequests here).          # If you have a use case where this causes pain, please open a GitHub issue and we can discuss alternatives.          if all(p.part_kind == 'tool-return' or p.part_kind == 'retry-prompt' for p in most_recent_request.parts):             return second_most_recent_request.instructions          return None ``` |", "url": "https://ai.pydantic.dev/models/base/index.html#model", "page": "models/base/index.html", "source_site": "pydantic_ai"}
{"title": "Model", "anchor": "model", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    *,\n    settings: ModelSettings | None = None,\n    profile: ModelProfileSpec | None = None\n) -> None\n```\n\nInitialize the model with optional settings and profile.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `settings` | `ModelSettings | None` | Model-specific settings that will be used as defaults for this model. | `None` |\n| `profile` | `ModelProfileSpec | None` | The model profile to use. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 324 325 326 327 328 329 330 331 332 333 334 335 336 337 ``` | ``` def __init__(     self,     *,     settings: ModelSettings | None = None,     profile: ModelProfileSpec | None = None, ) -> None:     \"\"\"Initialize the model with optional settings and profile.      Args:         settings: Model-specific settings that will be used as defaults for this model.         profile: The model profile to use.     \"\"\"     self._settings = settings     self._profile = profile ``` |\n\n#### settings `property`\n\n```\nsettings: ModelSettings | None\n```\n\nGet the model settings.\n\n#### request `abstractmethod` `async`\n\n```\nrequest(\n    messages: list[ModelMessage],\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n) -> ModelResponse\n```\n\nMake a request to the model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 344 345 346 347 348 349 350 351 352 ``` | ``` @abstractmethod async def request(     self,     messages: list[ModelMessage],     model_settings: ModelSettings | None,     model_request_parameters: ModelRequestParameters, ) -> ModelResponse:     \"\"\"Make a request to the model.\"\"\"     raise NotImplementedError() ``` |\n\n#### count\\_tokens `async`\n\n```\ncount_tokens(\n    messages: list[ModelMessage],\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n) -> RequestUsage\n```\n\nMake a request to the model for counting tokens.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 354 355 356 357 358 359 360 361 362 ``` | ``` async def count_tokens(     self,     messages: list[ModelMessage],     model_settings: ModelSettings | None,     model_request_parameters: ModelRequestParameters, ) -> RequestUsage:     \"\"\"Make a request to the model for counting tokens.\"\"\"     # This method is not required, but you need to implement it if you want to support `UsageLimits.count_tokens_before_request`.     raise NotImplementedError(f'Token counting ahead of the request is not supported by {self.__class__.__name__}') ``` |\n\n#### request\\_stream `async`\n\n```\nrequest_stream(\n    messages: list[ModelMessage],\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n    run_context: RunContext[Any] | None = None,\n) -> AsyncIterator[StreamedResponse]\n```\n\nMake a request to the model and return a streaming response.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 364 365 366 367 368 369 370 371 372 373 374 375 376 377 ``` | ``` @asynccontextmanager async def request_stream(     self,     messages: list[ModelMessage],     model_settings: ModelSettings | None,     model_request_parameters: ModelRequestParameters,     run_context: RunContext[Any] | None = None, ) -> AsyncIterator[StreamedResponse]:     \"\"\"Make a request to the model and return a streaming response.\"\"\"     # This method is not required, but you need to implement it if you want to support streamed responses     raise NotImplementedError(f'Streamed requests not supported by this {self.__class__.__name__}')     # yield is required to make this a generator for type checking     # noinspection PyUnreachableCode     yield  # pragma: no cover ``` |\n\n#### customize\\_request\\_parameters\n\n```\ncustomize_request_parameters(\n    model_request_parameters: ModelRequestParameters,\n) -> ModelRequestParameters\n```\n\nCustomize the request parameters for the model.\n\nThis method can be overridden by subclasses to modify the request parameters before sending them to the model.\nIn particular, this method can be used to make modifications to the generated tool JSON schemas if necessary\nfor vendor/model-specific reasons.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`", "url": "https://ai.pydantic.dev/models/base/index.html#model", "page": "models/base/index.html", "source_site": "pydantic_ai"}
{"title": "Model", "anchor": "model", "md_text": "|  |  |\n| --- | --- |\n| ``` 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 ``` | ``` def customize_request_parameters(self, model_request_parameters: ModelRequestParameters) -> ModelRequestParameters:     \"\"\"Customize the request parameters for the model.      This method can be overridden by subclasses to modify the request parameters before sending them to the model.     In particular, this method can be used to make modifications to the generated tool JSON schemas if necessary     for vendor/model-specific reasons.     \"\"\"     if transformer := self.profile.json_schema_transformer:         model_request_parameters = replace(             model_request_parameters,             function_tools=[_customize_tool_def(transformer, t) for t in model_request_parameters.function_tools],             output_tools=[_customize_tool_def(transformer, t) for t in model_request_parameters.output_tools],         )         if output_object := model_request_parameters.output_object:             model_request_parameters = replace(                 model_request_parameters,                 output_object=_customize_output_object(transformer, output_object),             )      return model_request_parameters ``` |\n\n#### prepare\\_request\n\n```\nprepare_request(\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n) -> tuple[ModelSettings | None, ModelRequestParameters]\n```\n\nPrepare request inputs before they are passed to the provider.\n\nThis merges the given `model_settings` with the model's own `settings` attribute and ensures\n`customize_request_parameters` is applied to the resolved\n[`ModelRequestParameters`](index.html#pydantic_ai.models.ModelRequestParameters). Subclasses can override this method if\nthey need to customize the preparation flow further, but most implementations should simply call\n`self.prepare_request(...)` at the start of their `request` (and related) methods.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 ``` | ``` def prepare_request(     self,     model_settings: ModelSettings | None,     model_request_parameters: ModelRequestParameters, ) -> tuple[ModelSettings | None, ModelRequestParameters]:     \"\"\"Prepare request inputs before they are passed to the provider.      This merges the given ``model_settings`` with the model's own ``settings`` attribute and ensures     ``customize_request_parameters`` is applied to the resolved     [`ModelRequestParameters`][pydantic_ai.models.ModelRequestParameters]. Subclasses can override this method if     they need to customize the preparation flow further, but most implementations should simply call     ``self.prepare_request(...)`` at the start of their ``request`` (and related) methods.     \"\"\"     model_settings = merge_model_settings(self.settings, model_settings)      if builtin_tools := model_request_parameters.builtin_tools:         # Deduplicate builtin tools         model_request_parameters = replace(             model_request_parameters,             builtin_tools=list({tool.unique_id: tool for tool in builtin_tools}.values()),         )      model_request_parameters = self.customize_request_parameters(model_request_parameters)     return model_settings, model_request_parameters ``` |\n\n#### model\\_name `abstractmethod` `property`\n\n```\nmodel_name: str\n```\n\nThe model name.\n\n#### profile `cached` `property`\n\n```\nprofile: ModelProfile\n```\n\nThe model profile.\n\n#### system `abstractmethod` `property`\n\n```\nsystem: str\n```\n\nThe model provider, ex: openai.\n\nUse to populate the `gen_ai.system` OpenTelemetry semantic convention attribute,\nso should use well-known values listed in\nhttps://opentelemetry.io/docs/specs/semconv/attributes-registry/gen-ai/#gen-ai-system\nwhen applicable.\n\n#### base\\_url `property`\n\n```\nbase_url: str | None\n```\n\nThe base URL for the provider API, if available.", "url": "https://ai.pydantic.dev/models/base/index.html#model", "page": "models/base/index.html", "source_site": "pydantic_ai"}
{"title": "StreamedResponse `dataclass`", "anchor": "streamedresponse-dataclass", "md_text": "Bases: `ABC`\n\nStreamed response from an LLM when calling a tool.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 ``` | ``` @dataclass class StreamedResponse(ABC):     \"\"\"Streamed response from an LLM when calling a tool.\"\"\"      model_request_parameters: ModelRequestParameters      final_result_event: FinalResultEvent | None = field(default=None, init=False)      provider_response_id: str | None = field(default=None, init=False)     provider_details: dict[str, Any] | None = field(default=None, init=False)     finish_reason: FinishReason | None = field(default=None, init=False)      _parts_manager: ModelResponsePartsManager = field(default_factory=ModelResponsePartsManager, init=False)     _event_iterator: AsyncIterator[ModelResponseStreamEvent] | None = field(default=None, init=False)     _usage: RequestUsage = field(default_factory=RequestUsage, init=False)      def __aiter__(self) -> AsyncIterator[ModelResponseStreamEvent]:         \"\"\"Stream the response as an async iterable of [`ModelResponseStreamEvent`][pydantic_ai.messages.ModelResponseStreamEvent]s.          This proxies the `_event_iterator()` and emits all events, while also checking for matches         on the result schema and emitting a [`FinalResultEvent`][pydantic_ai.messages.FinalResultEvent] if/when the         first match is found.         \"\"\"         if self._event_iterator is None:              async def iterator_with_final_event(                 iterator: AsyncIterator[ModelResponseStreamEvent],             ) -> AsyncIterator[ModelResponseStreamEvent]:                 async for event in iterator:                     yield event                     if (                         final_result_event := _get_final_result_event(event, self.model_request_parameters)                     ) is not None:                         self.final_result_event = final_result_event                         yield final_result_event                         break                  # If we broke out of the above loop, we need to yield the rest of the events                 # If we didn't, this will just be a no-op                 async for event in iterator:                     yield event              self._event_iterator = iterator_with_final_event(self._get_event_iterator())         return self._event_iterator      @abstractmethod     async def _get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:         \"\"\"Return an async iterator of [`ModelResponseStreamEvent`][pydantic_ai.messages.ModelResponseStreamEvent]s.          This method should be implemented by subclasses to translate the vendor-specific stream of events into         pydantic_ai-format events.          It should use the `_parts_manager` to handle deltas, and should update the `_usage` attributes as it goes.         \"\"\"         raise NotImplementedError()         # noinspection PyUnreachableCode         yield      def get(self) -> ModelResponse:         \"\"\"Build a [`ModelResponse`][pydantic_ai.messages.ModelResponse] from the data received from the stream so far.\"\"\"         return ModelResponse(             parts=self._parts_manager.get_parts(),             model_name=self.model_name,             timestamp=self.timestamp,             usage=self.usage(),             provider_name=self.provider_name,             provider_response_id=self.provider_response_id,             provider_details=self.provider_details,             finish_reason=self.finish_reason,         )      # TODO (v2): Make this a property     def usage(self) -> RequestUsage:         \"\"\"Get the usage of the response so far. This will not be the final usage until the stream is exhausted.\"\"\"         return self._usage      @property     @abstractmethod     def model_name(self) -> str:         \"\"\"Get the model name of the response.\"\"\"         raise NotImplementedError()      @property     @abstractmethod     def provider_name(self) -> str | None:         \"\"\"Get the provider name.\"\"\"         raise NotImplementedError()      @property     @abstractmethod     def timestamp(self) -> datetime:         \"\"\"Get the timestamp of the response.\"\"\"         raise NotImplementedError() ``` |\n\n#### \\_\\_aiter\\_\\_\n\n```\n__aiter__() -> AsyncIterator[ModelResponseStreamEvent]\n```\n\nStream the response as an async iterable of [`ModelResponseStreamEvent`](../../messages/index.html#pydantic_ai.messages.ModelResponseStreamEvent)s.\n\nThis proxies the `_event_iterator()` and emits all events, while also checking for matches\non the result schema and emitting a [`FinalResultEvent`](../../messages/index.html#pydantic_ai.messages.FinalResultEvent) if/when the\nfirst match is found.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`", "url": "https://ai.pydantic.dev/models/base/index.html#streamedresponse-dataclass", "page": "models/base/index.html", "source_site": "pydantic_ai"}
{"title": "StreamedResponse `dataclass`", "anchor": "streamedresponse-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 ``` | ``` def __aiter__(self) -> AsyncIterator[ModelResponseStreamEvent]:     \"\"\"Stream the response as an async iterable of [`ModelResponseStreamEvent`][pydantic_ai.messages.ModelResponseStreamEvent]s.      This proxies the `_event_iterator()` and emits all events, while also checking for matches     on the result schema and emitting a [`FinalResultEvent`][pydantic_ai.messages.FinalResultEvent] if/when the     first match is found.     \"\"\"     if self._event_iterator is None:          async def iterator_with_final_event(             iterator: AsyncIterator[ModelResponseStreamEvent],         ) -> AsyncIterator[ModelResponseStreamEvent]:             async for event in iterator:                 yield event                 if (                     final_result_event := _get_final_result_event(event, self.model_request_parameters)                 ) is not None:                     self.final_result_event = final_result_event                     yield final_result_event                     break              # If we broke out of the above loop, we need to yield the rest of the events             # If we didn't, this will just be a no-op             async for event in iterator:                 yield event          self._event_iterator = iterator_with_final_event(self._get_event_iterator())     return self._event_iterator ``` |\n\n#### get\n\n```\nget() -> ModelResponse\n```\n\nBuild a [`ModelResponse`](../../messages/index.html#pydantic_ai.messages.ModelResponse) from the data received from the stream so far.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 562 563 564 565 566 567 568 569 570 571 572 573 ``` | ``` def get(self) -> ModelResponse:     \"\"\"Build a [`ModelResponse`][pydantic_ai.messages.ModelResponse] from the data received from the stream so far.\"\"\"     return ModelResponse(         parts=self._parts_manager.get_parts(),         model_name=self.model_name,         timestamp=self.timestamp,         usage=self.usage(),         provider_name=self.provider_name,         provider_response_id=self.provider_response_id,         provider_details=self.provider_details,         finish_reason=self.finish_reason,     ) ``` |\n\n#### usage\n\n```\nusage() -> RequestUsage\n```\n\nGet the usage of the response so far. This will not be the final usage until the stream is exhausted.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 576 577 578 ``` | ``` def usage(self) -> RequestUsage:     \"\"\"Get the usage of the response so far. This will not be the final usage until the stream is exhausted.\"\"\"     return self._usage ``` |\n\n#### model\\_name `abstractmethod` `property`\n\n```\nmodel_name: str\n```\n\nGet the model name of the response.\n\n#### provider\\_name `abstractmethod` `property`\n\n```\nprovider_name: str | None\n```\n\nGet the provider name.\n\n#### timestamp `abstractmethod` `property`\n\n```\ntimestamp: datetime\n```\n\nGet the timestamp of the response.", "url": "https://ai.pydantic.dev/models/base/index.html#streamedresponse-dataclass", "page": "models/base/index.html", "source_site": "pydantic_ai"}
{"title": "ALLOW\\_MODEL\\_REQUESTS `module-attribute`", "anchor": "allowmodelrequests-module-attribute", "md_text": "```\nALLOW_MODEL_REQUESTS = True\n```\n\nWhether to allow requests to models.\n\nThis global setting allows you to disable request to most models, e.g. to make sure you don't accidentally\nmake costly requests to a model during tests.\n\nThe testing models [`TestModel`](../test/index.html#pydantic_ai.models.test.TestModel) and\n[`FunctionModel`](../function/index.html#pydantic_ai.models.function.FunctionModel) are no affected by this setting.", "url": "https://ai.pydantic.dev/models/base/index.html#allowmodelrequests-module-attribute", "page": "models/base/index.html", "source_site": "pydantic_ai"}
{"title": "check\\_allow\\_model\\_requests", "anchor": "checkallowmodelrequests", "md_text": "```\ncheck_allow_model_requests() -> None\n```\n\nCheck if model requests are allowed.\n\nIf you're defining your own models that have costs or latency associated with their use, you should call this in\n[`Model.request`](index.html#pydantic_ai.models.Model.request) and [`Model.request_stream`](index.html#pydantic_ai.models.Model.request_stream).\n\nRaises:\n\n| Type | Description |\n| --- | --- |\n| `RuntimeError` | If model requests are not allowed. |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 610 611 612 613 614 615 616 617 618 619 620 ``` | ``` def check_allow_model_requests() -> None:     \"\"\"Check if model requests are allowed.      If you're defining your own models that have costs or latency associated with their use, you should call this in     [`Model.request`][pydantic_ai.models.Model.request] and [`Model.request_stream`][pydantic_ai.models.Model.request_stream].      Raises:         RuntimeError: If model requests are not allowed.     \"\"\"     if not ALLOW_MODEL_REQUESTS:         raise RuntimeError('Model requests are not allowed, since ALLOW_MODEL_REQUESTS is False') ``` |", "url": "https://ai.pydantic.dev/models/base/index.html#checkallowmodelrequests", "page": "models/base/index.html", "source_site": "pydantic_ai"}
{"title": "override\\_allow\\_model\\_requests", "anchor": "overrideallowmodelrequests", "md_text": "```\noverride_allow_model_requests(\n    allow_model_requests: bool,\n) -> Iterator[None]\n```\n\nContext manager to temporarily override [`ALLOW_MODEL_REQUESTS`](index.html#pydantic_ai.models.ALLOW_MODEL_REQUESTS).\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `allow_model_requests` | `bool` | Whether to allow model requests within the context. | *required* |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 623 624 625 626 627 628 629 630 631 632 633 634 635 636 ``` | ``` @contextmanager def override_allow_model_requests(allow_model_requests: bool) -> Iterator[None]:     \"\"\"Context manager to temporarily override [`ALLOW_MODEL_REQUESTS`][pydantic_ai.models.ALLOW_MODEL_REQUESTS].      Args:         allow_model_requests: Whether to allow model requests within the context.     \"\"\"     global ALLOW_MODEL_REQUESTS     old_value = ALLOW_MODEL_REQUESTS     ALLOW_MODEL_REQUESTS = allow_model_requests  # pyright: ignore[reportConstantRedefinition]     try:         yield     finally:         ALLOW_MODEL_REQUESTS = old_value  # pyright: ignore[reportConstantRedefinition] ``` |", "url": "https://ai.pydantic.dev/models/base/index.html#overrideallowmodelrequests", "page": "models/base/index.html", "source_site": "pydantic_ai"}
{"title": "FallbackModel `dataclass`", "anchor": "fallbackmodel-dataclass", "md_text": "Bases: `Model`\n\nA model that uses one or more fallback models upon failure.\n\nApart from `__init__`, all methods are private or match those of the base class.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/fallback.py`\n\n|  |  |\n| --- | --- |\n| ```  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 ``` | ``` @dataclass(init=False) class FallbackModel(Model):     \"\"\"A model that uses one or more fallback models upon failure.      Apart from `__init__`, all methods are private or match those of the base class.     \"\"\"      models: list[Model]      _model_name: str = field(repr=False)     _fallback_on: Callable[[Exception], bool]      def __init__(         self,         default_model: Model | KnownModelName | str,         *fallback_models: Model | KnownModelName | str,         fallback_on: Callable[[Exception], bool] | tuple[type[Exception], ...] = (ModelHTTPError,),     ):         \"\"\"Initialize a fallback model instance.          Args:             default_model: The name or instance of the default model to use.             fallback_models: The names or instances of the fallback models to use upon failure.             fallback_on: A callable or tuple of exceptions that should trigger a fallback.         \"\"\"         super().__init__()         self.models = [infer_model(default_model), *[infer_model(m) for m in fallback_models]]          if isinstance(fallback_on, tuple):             self._fallback_on = _default_fallback_condition_factory(fallback_on)         else:             self._fallback_on = fallback_on      @property     def model_name(self) -> str:         \"\"\"The model name.\"\"\"         return f'fallback:{\",\".join(model.model_name for model in self.models)}'      @property     def system(self) -> str:         return f'fallback:{\",\".join(model.system for model in self.models)}'      @property     def base_url(self) -> str | None:         return self.models[0].base_url      async def request(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> ModelResponse:         \"\"\"Try each model in sequence until one succeeds.          In case of failure, raise a FallbackExceptionGroup with all exceptions.         \"\"\"         exceptions: list[Exception] = []          for model in self.models:             try:                 response = await model.request(messages, model_settings, model_request_parameters)             except Exception as exc:                 if self._fallback_on(exc):                     exceptions.append(exc)                     continue                 raise exc              self._set_span_attributes(model)             return response          raise FallbackExceptionGroup('All models from FallbackModel failed', exceptions)      @asynccontextmanager     async def request_stream(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,         run_context: RunContext[Any] | None = None,     ) -> AsyncIterator[StreamedResponse]:         \"\"\"Try each model in sequence until one succeeds.\"\"\"         exceptions: list[Exception] = []          for model in self.models:             async with AsyncExitStack() as stack:                 try:                     response = await stack.enter_async_context(                         model.request_stream(messages, model_settings, model_request_parameters, run_context)                     )                 except Exception as exc:                     if self._fallback_on(exc):                         exceptions.append(exc)                         continue                     raise exc  # pragma: no cover                  self._set_span_attributes(model)                 yield response                 return          raise FallbackExceptionGroup('All models from FallbackModel failed', exceptions)      def _set_span_attributes(self, model: Model):         with suppress(Exception):             span = get_current_span()             if span.is_recording():                 attributes = getattr(span, 'attributes', {})                 if attributes.get('gen_ai.request.model') == self.model_name:  # pragma: no branch                     span.set_attributes(InstrumentedModel.model_attributes(model)) ``` |\n\n#### \\_\\_init\\_\\_", "url": "https://ai.pydantic.dev/models/fallback/index.html#fallbackmodel-dataclass", "page": "models/fallback/index.html", "source_site": "pydantic_ai"}
{"title": "FallbackModel `dataclass`", "anchor": "fallbackmodel-dataclass", "md_text": "```\n__init__(\n    default_model: Model | KnownModelName | str,\n    *fallback_models: Model | KnownModelName | str,\n    fallback_on: (\n        Callable[[Exception], bool]\n        | tuple[type[Exception], ...]\n    ) = (ModelHTTPError,)\n)\n```\n\nInitialize a fallback model instance.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `default_model` | `Model | KnownModelName | str` | The name or instance of the default model to use. | *required* |\n| `fallback_models` | `Model | KnownModelName | str` | The names or instances of the fallback models to use upon failure. | `()` |\n| `fallback_on` | `Callable[[Exception], bool] | tuple[type[Exception], ...]` | A callable or tuple of exceptions that should trigger a fallback. | `(ModelHTTPError,)` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/fallback.py`\n\n|  |  |\n| --- | --- |\n| ``` 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 ``` | ``` def __init__(     self,     default_model: Model | KnownModelName | str,     *fallback_models: Model | KnownModelName | str,     fallback_on: Callable[[Exception], bool] | tuple[type[Exception], ...] = (ModelHTTPError,), ):     \"\"\"Initialize a fallback model instance.      Args:         default_model: The name or instance of the default model to use.         fallback_models: The names or instances of the fallback models to use upon failure.         fallback_on: A callable or tuple of exceptions that should trigger a fallback.     \"\"\"     super().__init__()     self.models = [infer_model(default_model), *[infer_model(m) for m in fallback_models]]      if isinstance(fallback_on, tuple):         self._fallback_on = _default_fallback_condition_factory(fallback_on)     else:         self._fallback_on = fallback_on ``` |\n\n#### model\\_name `property`\n\n```\nmodel_name: str\n```\n\nThe model name.\n\n#### request `async`\n\n```\nrequest(\n    messages: list[ModelMessage],\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n) -> ModelResponse\n```\n\nTry each model in sequence until one succeeds.\n\nIn case of failure, raise a FallbackExceptionGroup with all exceptions.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/fallback.py`\n\n|  |  |\n| --- | --- |\n| ``` 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 ``` | ``` async def request(     self,     messages: list[ModelMessage],     model_settings: ModelSettings | None,     model_request_parameters: ModelRequestParameters, ) -> ModelResponse:     \"\"\"Try each model in sequence until one succeeds.      In case of failure, raise a FallbackExceptionGroup with all exceptions.     \"\"\"     exceptions: list[Exception] = []      for model in self.models:         try:             response = await model.request(messages, model_settings, model_request_parameters)         except Exception as exc:             if self._fallback_on(exc):                 exceptions.append(exc)                 continue             raise exc          self._set_span_attributes(model)         return response      raise FallbackExceptionGroup('All models from FallbackModel failed', exceptions) ``` |\n\n#### request\\_stream `async`\n\n```\nrequest_stream(\n    messages: list[ModelMessage],\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n    run_context: RunContext[Any] | None = None,\n) -> AsyncIterator[StreamedResponse]\n```\n\nTry each model in sequence until one succeeds.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/fallback.py`\n\n|  |  |\n| --- | --- |\n| ```  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 ``` | ``` @asynccontextmanager async def request_stream(     self,     messages: list[ModelMessage],     model_settings: ModelSettings | None,     model_request_parameters: ModelRequestParameters,     run_context: RunContext[Any] | None = None, ) -> AsyncIterator[StreamedResponse]:     \"\"\"Try each model in sequence until one succeeds.\"\"\"     exceptions: list[Exception] = []      for model in self.models:         async with AsyncExitStack() as stack:             try:                 response = await stack.enter_async_context(                     model.request_stream(messages, model_settings, model_request_parameters, run_context)                 )             except Exception as exc:                 if self._fallback_on(exc):                     exceptions.append(exc)                     continue                 raise exc  # pragma: no cover              self._set_span_attributes(model)             yield response             return      raise FallbackExceptionGroup('All models from FallbackModel failed', exceptions) ``` |", "url": "https://ai.pydantic.dev/models/fallback/index.html#fallbackmodel-dataclass", "page": "models/fallback/index.html", "source_site": "pydantic_ai"}
{"title": "Setup", "anchor": "setup", "md_text": "For details on how to set up authentication with this model, see [model configuration for Hugging Face](../../huggingface/index.html).", "url": "https://ai.pydantic.dev/models/huggingface/index.html#setup", "page": "models/huggingface/index.html", "source_site": "pydantic_ai"}
{"title": "HuggingFaceModelSettings", "anchor": "huggingfacemodelsettings", "md_text": "Bases: `ModelSettings`\n\nSettings used for a Hugging Face model request.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/huggingface.py`\n\n|  |  |\n| --- | --- |\n| ``` 106 107 ``` | ``` class HuggingFaceModelSettings(ModelSettings, total=False):     \"\"\"Settings used for a Hugging Face model request.\"\"\" ``` |", "url": "https://ai.pydantic.dev/models/huggingface/index.html#huggingfacemodelsettings", "page": "models/huggingface/index.html", "source_site": "pydantic_ai"}
{"title": "HuggingFaceModel `dataclass`", "anchor": "huggingfacemodel-dataclass", "md_text": "Bases: `Model`\n\nA model that uses Hugging Face Inference Providers.\n\nInternally, this uses the [HF Python client](https://github.com/huggingface/huggingface_hub) to interact with the API.\n\nApart from `__init__`, all methods are private or match those of the base class.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/huggingface.py`", "url": "https://ai.pydantic.dev/models/huggingface/index.html#huggingfacemodel-dataclass", "page": "models/huggingface/index.html", "source_site": "pydantic_ai"}
{"title": "HuggingFaceModel `dataclass`", "anchor": "huggingfacemodel-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 ``` | ``` @dataclass(init=False) class HuggingFaceModel(Model):     \"\"\"A model that uses Hugging Face Inference Providers.      Internally, this uses the [HF Python client](https://github.com/huggingface/huggingface_hub) to interact with the API.      Apart from `__init__`, all methods are private or match those of the base class.     \"\"\"      client: AsyncInferenceClient = field(repr=False)      _model_name: str = field(repr=False)     _provider: Provider[AsyncInferenceClient] = field(repr=False)      def __init__(         self,         model_name: str,         *,         provider: Literal['huggingface'] | Provider[AsyncInferenceClient] = 'huggingface',         profile: ModelProfileSpec | None = None,         settings: ModelSettings | None = None,     ):         \"\"\"Initialize a Hugging Face model.          Args:             model_name: The name of the Model to use. You can browse available models [here](https://huggingface.co/models?pipeline_tag=text-generation&inference_provider=all&sort=trending).             provider: The provider to use for Hugging Face Inference Providers. Can be either the string 'huggingface' or an                 instance of `Provider[AsyncInferenceClient]`. If not provided, the other parameters will be used.             profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.             settings: Model-specific settings that will be used as defaults for this model.         \"\"\"         self._model_name = model_name         if isinstance(provider, str):             provider = infer_provider(provider)         self._provider = provider         self.client = provider.client          super().__init__(settings=settings, profile=profile or provider.model_profile)      @property     def model_name(self) -> HuggingFaceModelName:         \"\"\"The model name.\"\"\"         return self._model_name      @property     def system(self) -> str:         \"\"\"The system / model provider.\"\"\"         return self._provider.name      async def request(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> ModelResponse:         check_allow_model_requests()         model_settings, model_request_parameters = self.prepare_request(             model_settings,             model_request_parameters,         )         response = await self._completions_create(             messages, False, cast(HuggingFaceModelSettings, model_settings or {}), model_request_parameters         )         model_response = self._process_response(response)         return model_response      @asynccontextmanager     async def request_stream(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,         run_context: RunContext[Any] | None = None,     ) -> AsyncIterator[StreamedResponse]:         check_allow_model_requests()         model_settings, model_request_parameters = self.prepare_request(             model_settings,             model_request_parameters,         )         response = await self._completions_create(             messages, True, cast(HuggingFaceModelSettings, model_settings or {}), model_request_parameters         )         yield await self._process_streamed_response(response, model_request_parameters)      @overload     async def _completions_create(         self,         messages: list[ModelMessage],         stream: Literal[True],         model_settings: HuggingFaceModelSettings,         model_request_parameters: ModelRequestParameters,     ) -> AsyncIterable[ChatCompletionStreamOutput]: ...      @overload     async def _completions_create(         self,         messages: list[ModelMessage],         stream: Literal[False],         model_settings: HuggingFaceModelSettings,         model_request_parameters: ModelRequestParameters,     ) -> ChatCompletionOutput: ...      async def _completions_create(         self,         messages: list[ModelMessage],         stream: bool,         model_settings: HuggingFaceModelSettings,         model_request_parameters: ModelRequestParameters,     ) -> ChatCompletionOutput | AsyncIterable[ChatCompletionStreamOutput]:         tools = self._get_tools(model_request_parameters)          if not tools:             tool_choice: Literal['none', 'required', 'auto'] | None = None         elif not model_request_parameters.allow_text_output:             tool_choice = 'required'         else:             tool_choice = 'auto'          if model_request_parameters.builtin_tools:             raise UserError('HuggingFace does not support built-in tools')          hf_messages = await self._map_messages(messages)          try:             return await self.client.chat.completions.create(  # type: ignore                 model=self._model_name,                 messages=hf_messages,  # type: ignore                 tools=tools,                 tool_choice=tool_choice or None,                 stream=stream,                 stop=model_settings.get('stop_sequences', None),                 temperature=model_settings.get('temperature', None),                 top_p=model_settings.get('top_p', None),                 seed=model_settings.get('seed', None),                 presence_penalty=model_settings.get('presence_penalty', None),                 frequency_penalty=model_settings.get('frequency_penalty', None),                 logit_bias=model_settings.get('logit_bias', None),  # type: ignore                 logprobs=model_settings.get('logprobs', None),                 top_logprobs=model_settings.get('top_logprobs', None),                 extra_body=model_settings.get('extra_body'),  # type: ignore             )         except aiohttp.ClientResponseError as e:             raise ModelHTTPError(                 status_code=e.status,                 model_name=self.model_name,                 body=e.response_error_payload,  # type: ignore             ) from e         except HfHubHTTPError as e:             raise ModelHTTPError(                 status_code=e.response.status_code,                 model_name=self.model_name,                 body=e.response.content,             ) from e      def _process_response(self, response: ChatCompletionOutput) -> ModelResponse:         \"\"\"Process a non-streamed response, and prepare a message to return.\"\"\"         if response.created:             timestamp = datetime.fromtimestamp(response.created, tz=timezone.utc)         else:             timestamp = _now_utc()          choice = response.choices[0]         content = choice.message.content         tool_calls = choice.message.tool_calls          items: list[ModelResponsePart] = []          if content:             items.extend(split_content_into_text_and_thinking(content, self.profile.thinking_tags))         if tool_calls is not None:             for c in tool_calls:                 items.append(ToolCallPart(c.function.name, c.function.arguments, tool_call_id=c.id))          raw_finish_reason = choice.finish_reason         provider_details = {'finish_reason': raw_finish_reason}         finish_reason = _FINISH_REASON_MAP.get(cast(TextGenerationOutputFinishReason, raw_finish_reason), None)          return ModelResponse(             parts=items,             usage=_map_usage(response),             model_name=response.model,             timestamp=timestamp,             provider_response_id=response.id,             provider_name=self._provider.name,             finish_reason=finish_reason,             provider_details=provider_details,         )      async def _process_streamed_response(         self, response: AsyncIterable[ChatCompletionStreamOutput], model_request_parameters: ModelRequestParameters     ) -> StreamedResponse:         \"\"\"Process a streamed response, and prepare a streaming response to return.\"\"\"         peekable_response = _utils.PeekableAsyncStream(response)         first_chunk = await peekable_response.peek()         if isinstance(first_chunk, _utils.Unset):             raise UnexpectedModelBehavior(  # pragma: no cover                 'Streamed response ended without content or tool calls'             )          return HuggingFaceStreamedResponse(             model_request_parameters=model_request_parameters,             _model_name=first_chunk.model,             _model_profile=self.profile,             _response=peekable_response,             _timestamp=datetime.fromtimestamp(first_chunk.created, tz=timezone.utc),             _provider_name=self._provider.name,         )      def _get_tools(self, model_request_parameters: ModelRequestParameters) -> list[ChatCompletionInputTool]:         return [self._map_tool_definition(r) for r in model_request_parameters.tool_defs.values()]      async def _map_messages(         self, messages: list[ModelMessage]     ) -> list[ChatCompletionInputMessage | ChatCompletionOutputMessage]:         \"\"\"Just maps a `pydantic_ai.Message` to a `huggingface_hub.ChatCompletionInputMessage`.\"\"\"         hf_messages: list[ChatCompletionInputMessage | ChatCompletionOutputMessage] = []         for message in messages:             if isinstance(message, ModelRequest):                 async for item in self._map_user_message(message):                     hf_messages.append(item)             elif isinstance(message, ModelResponse):                 texts: list[str] = []                 tool_calls: list[ChatCompletionInputToolCall] = []                 for item in message.parts:                     if isinstance(item, TextPart):                         texts.append(item.content)                     elif isinstance(item, ToolCallPart):                         tool_calls.append(self._map_tool_call(item))                     elif isinstance(item, ThinkingPart):                         start_tag, end_tag = self.profile.thinking_tags                         texts.append('\\n'.join([start_tag, item.content, end_tag]))                     elif isinstance(item, BuiltinToolCallPart | BuiltinToolReturnPart):  # pragma: no cover                         # This is currently never returned from huggingface                         pass                     elif isinstance(item, FilePart):  # pragma: no cover                         # Files generated by models are not sent back to models that don't themselves generate files.                         pass                     else:                         assert_never(item)                 message_param = ChatCompletionInputMessage(role='assistant')  # type: ignore                 if texts:                     # Note: model responses from this model should only have one text item, so the following                     # shouldn't merge multiple texts into one unless you switch models between runs:                     message_param['content'] = '\\n\\n'.join(texts)                 if tool_calls:                     message_param['tool_calls'] = tool_calls                 hf_messages.append(message_param)             else:                 assert_never(message)         if instructions := self._get_instructions(messages):             hf_messages.insert(0, ChatCompletionInputMessage(content=instructions, role='system'))  # type: ignore         return hf_messages      @staticmethod     def _map_tool_call(t: ToolCallPart) -> ChatCompletionInputToolCall:         return ChatCompletionInputToolCall.parse_obj_as_instance(  # type: ignore             {                 'id': _guard_tool_call_id(t=t),                 'type': 'function',                 'function': {                     'name': t.tool_name,                     'arguments': t.args_as_json_str(),                 },             }         )      @staticmethod     def _map_tool_definition(f: ToolDefinition) -> ChatCompletionInputTool:         tool_param: ChatCompletionInputTool = ChatCompletionInputTool.parse_obj_as_instance(  # type: ignore             {                 'type': 'function',                 'function': {                     'name': f.name,                     'description': f.description,                     'parameters': f.parameters_json_schema,                 },             }         )         return tool_param      async def _map_user_message(         self, message: ModelRequest     ) -> AsyncIterable[ChatCompletionInputMessage | ChatCompletionOutputMessage]:         for part in message.parts:             if isinstance(part, SystemPromptPart):                 yield ChatCompletionInputMessage.parse_obj_as_instance({'role': 'system', 'content': part.content})  # type: ignore             elif isinstance(part, UserPromptPart):                 yield await self._map_user_prompt(part)             elif isinstance(part, ToolReturnPart):                 yield ChatCompletionOutputMessage.parse_obj_as_instance(  # type: ignore                     {                         'role': 'tool',                         'tool_call_id': _guard_tool_call_id(t=part),                         'content': part.model_response_str(),                     }                 )             elif isinstance(part, RetryPromptPart):                 if part.tool_name is None:                     yield ChatCompletionInputMessage.parse_obj_as_instance(  # type: ignore                         {'role': 'user', 'content': part.model_response()}                     )                 else:                     yield ChatCompletionInputMessage.parse_obj_as_instance(  # type: ignore                         {                             'role': 'tool',                             'tool_call_id': _guard_tool_call_id(t=part),                             'content': part.model_response(),                         }                     )             else:                 assert_never(part)      @staticmethod     async def _map_user_prompt(part: UserPromptPart) -> ChatCompletionInputMessage:         content: str | list[ChatCompletionInputMessage]         if isinstance(part.content, str):             content = part.content         else:             content = []             for item in part.content:                 if isinstance(item, str):                     content.append(ChatCompletionInputMessageChunk(type='text', text=item))  # type: ignore                 elif isinstance(item, ImageUrl):                     url = ChatCompletionInputURL(url=item.url)  # type: ignore                     content.append(ChatCompletionInputMessageChunk(type='image_url', image_url=url))  # type: ignore                 elif isinstance(item, BinaryContent):                     if item.is_image:                         url = ChatCompletionInputURL(url=item.data_uri)  # type: ignore                         content.append(ChatCompletionInputMessageChunk(type='image_url', image_url=url))  # type: ignore                     else:  # pragma: no cover                         raise RuntimeError(f'Unsupported binary content type: {item.media_type}')                 elif isinstance(item, AudioUrl):                     raise NotImplementedError('AudioUrl is not supported for Hugging Face')                 elif isinstance(item, DocumentUrl):                     raise NotImplementedError('DocumentUrl is not supported for Hugging Face')                 elif isinstance(item, VideoUrl):                     raise NotImplementedError('VideoUrl is not supported for Hugging Face')                 else:                     assert_never(item)         return ChatCompletionInputMessage(role='user', content=content)  # type: ignore ``` |", "url": "https://ai.pydantic.dev/models/huggingface/index.html#huggingfacemodel-dataclass", "page": "models/huggingface/index.html", "source_site": "pydantic_ai"}
{"title": "HuggingFaceModel `dataclass`", "anchor": "huggingfacemodel-dataclass", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    model_name: str,\n    *,\n    provider: (\n        Literal[\"huggingface\"]\n        | Provider[AsyncInferenceClient]\n    ) = \"huggingface\",\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n)\n```\n\nInitialize a Hugging Face model.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model_name` | `str` | The name of the Model to use. You can browse available models [here](https://huggingface.co/models?pipeline_tag=text-generation&inference_provider=all&sort=trending). | *required* |\n| `provider` | `Literal['huggingface'] | Provider[AsyncInferenceClient]` | The provider to use for Hugging Face Inference Providers. Can be either the string 'huggingface' or an instance of `Provider[AsyncInferenceClient]`. If not provided, the other parameters will be used. | `'huggingface'` |\n| `profile` | `ModelProfileSpec | None` | The model profile to use. Defaults to a profile picked by the provider based on the model name. | `None` |\n| `settings` | `ModelSettings | None` | Model-specific settings that will be used as defaults for this model. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/huggingface.py`\n\n|  |  |\n| --- | --- |\n| ``` 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 ``` | ``` def __init__(     self,     model_name: str,     *,     provider: Literal['huggingface'] | Provider[AsyncInferenceClient] = 'huggingface',     profile: ModelProfileSpec | None = None,     settings: ModelSettings | None = None, ):     \"\"\"Initialize a Hugging Face model.      Args:         model_name: The name of the Model to use. You can browse available models [here](https://huggingface.co/models?pipeline_tag=text-generation&inference_provider=all&sort=trending).         provider: The provider to use for Hugging Face Inference Providers. Can be either the string 'huggingface' or an             instance of `Provider[AsyncInferenceClient]`. If not provided, the other parameters will be used.         profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.         settings: Model-specific settings that will be used as defaults for this model.     \"\"\"     self._model_name = model_name     if isinstance(provider, str):         provider = infer_provider(provider)     self._provider = provider     self.client = provider.client      super().__init__(settings=settings, profile=profile or provider.model_profile) ``` |\n\n#### model\\_name `property`\n\n```\nmodel_name: HuggingFaceModelName\n```\n\nThe model name.\n\n#### system `property`\n\n```\nsystem: str\n```\n\nThe system / model provider.", "url": "https://ai.pydantic.dev/models/huggingface/index.html#huggingfacemodel-dataclass", "page": "models/huggingface/index.html", "source_site": "pydantic_ai"}
{"title": "WrapperModel `dataclass`", "anchor": "wrappermodel-dataclass", "md_text": "Bases: `Model`\n\nModel which wraps another model.\n\nDoes nothing on its own, used as a base class.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/wrapper.py`\n\n|  |  |\n| --- | --- |\n| ``` 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 ``` | ``` @dataclass(init=False) class WrapperModel(Model):     \"\"\"Model which wraps another model.      Does nothing on its own, used as a base class.     \"\"\"      wrapped: Model     \"\"\"The underlying model being wrapped.\"\"\"      def __init__(self, wrapped: Model | KnownModelName):         super().__init__()         self.wrapped = infer_model(wrapped)      async def request(self, *args: Any, **kwargs: Any) -> ModelResponse:         return await self.wrapped.request(*args, **kwargs)      @asynccontextmanager     async def request_stream(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,         run_context: RunContext[Any] | None = None,     ) -> AsyncIterator[StreamedResponse]:         async with self.wrapped.request_stream(             messages, model_settings, model_request_parameters, run_context         ) as response_stream:             yield response_stream      def customize_request_parameters(self, model_request_parameters: ModelRequestParameters) -> ModelRequestParameters:         return self.wrapped.customize_request_parameters(model_request_parameters)      def prepare_request(         self,         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> tuple[ModelSettings | None, ModelRequestParameters]:         return self.wrapped.prepare_request(model_settings, model_request_parameters)      @property     def model_name(self) -> str:         return self.wrapped.model_name      @property     def system(self) -> str:         return self.wrapped.system      @cached_property     def profile(self) -> ModelProfile:         return self.wrapped.profile      @property     def settings(self) -> ModelSettings | None:         \"\"\"Get the settings from the wrapped model.\"\"\"         return self.wrapped.settings      def __getattr__(self, item: str):         return getattr(self.wrapped, item) ``` |\n\n#### wrapped `instance-attribute`\n\n```\nwrapped: Model = infer_model(wrapped)\n```\n\nThe underlying model being wrapped.\n\n#### settings `property`\n\n```\nsettings: ModelSettings | None\n```\n\nGet the settings from the wrapped model.", "url": "https://ai.pydantic.dev/models/wrapper/index.html#wrappermodel-dataclass", "page": "models/wrapper/index.html", "source_site": "pydantic_ai"}
{"title": "Setup", "anchor": "setup", "md_text": "For details on how to set up authentication with this model, see [model configuration for Cohere](../../cohere/index.html).", "url": "https://ai.pydantic.dev/models/cohere/index.html#setup", "page": "models/cohere/index.html", "source_site": "pydantic_ai"}
{"title": "LatestCohereModelNames `module-attribute`", "anchor": "latestcoheremodelnames-module-attribute", "md_text": "```\nLatestCohereModelNames = Literal[\n    \"c4ai-aya-expanse-32b\",\n    \"c4ai-aya-expanse-8b\",\n    \"command-nightly\",\n    \"command-r-08-2024\",\n    \"command-r-plus-08-2024\",\n    \"command-r7b-12-2024\",\n]\n```\n\nLatest Cohere models.", "url": "https://ai.pydantic.dev/models/cohere/index.html#latestcoheremodelnames-module-attribute", "page": "models/cohere/index.html", "source_site": "pydantic_ai"}
{"title": "CohereModelName `module-attribute`", "anchor": "coheremodelname-module-attribute", "md_text": "```\nCohereModelName = str | LatestCohereModelNames\n```\n\nPossible Cohere model names.\n\nSince Cohere supports a variety of date-stamped models, we explicitly list the latest models but\nallow any name in the type hints.\nSee [Cohere's docs](https://docs.cohere.com/v2/docs/models) for a list of all available models.", "url": "https://ai.pydantic.dev/models/cohere/index.html#coheremodelname-module-attribute", "page": "models/cohere/index.html", "source_site": "pydantic_ai"}
{"title": "CohereModelSettings", "anchor": "coheremodelsettings", "md_text": "Bases: `ModelSettings`\n\nSettings used for a Cohere model request.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/cohere.py`\n\n|  |  |\n| --- | --- |\n| ``` 89 90 ``` | ``` class CohereModelSettings(ModelSettings, total=False):     \"\"\"Settings used for a Cohere model request.\"\"\" ``` |", "url": "https://ai.pydantic.dev/models/cohere/index.html#coheremodelsettings", "page": "models/cohere/index.html", "source_site": "pydantic_ai"}
{"title": "CohereModel `dataclass`", "anchor": "coheremodel-dataclass", "md_text": "Bases: `Model`\n\nA model that uses the Cohere API.\n\nInternally, this uses the [Cohere Python client](https://github.com/cohere-ai/cohere-python) to interact with the API.\n\nApart from `__init__`, all methods are private or match those of the base class.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/cohere.py`", "url": "https://ai.pydantic.dev/models/cohere/index.html#coheremodel-dataclass", "page": "models/cohere/index.html", "source_site": "pydantic_ai"}
{"title": "CohereModel `dataclass`", "anchor": "coheremodel-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ```  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 ``` | ``` @dataclass(init=False) class CohereModel(Model):     \"\"\"A model that uses the Cohere API.      Internally, this uses the [Cohere Python client](     https://github.com/cohere-ai/cohere-python) to interact with the API.      Apart from `__init__`, all methods are private or match those of the base class.     \"\"\"      client: AsyncClientV2 = field(repr=False)      _model_name: CohereModelName = field(repr=False)     _provider: Provider[AsyncClientV2] = field(repr=False)      def __init__(         self,         model_name: CohereModelName,         *,         provider: Literal['cohere'] | Provider[AsyncClientV2] = 'cohere',         profile: ModelProfileSpec | None = None,         settings: ModelSettings | None = None,     ):         \"\"\"Initialize an Cohere model.          Args:             model_name: The name of the Cohere model to use. List of model names                 available [here](https://docs.cohere.com/docs/models#command).             provider: The provider to use for authentication and API access. Can be either the string                 'cohere' or an instance of `Provider[AsyncClientV2]`. If not provided, a new provider will be                 created using the other parameters.             profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.             settings: Model-specific settings that will be used as defaults for this model.         \"\"\"         self._model_name = model_name          if isinstance(provider, str):             provider = infer_provider(provider)         self._provider = provider         self.client = provider.client          super().__init__(settings=settings, profile=profile or provider.model_profile)      @property     def base_url(self) -> str:         client_wrapper = self.client._client_wrapper  # type: ignore         return str(client_wrapper.get_base_url())      @property     def model_name(self) -> CohereModelName:         \"\"\"The model name.\"\"\"         return self._model_name      @property     def system(self) -> str:         \"\"\"The model provider.\"\"\"         return self._provider.name      async def request(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> ModelResponse:         check_allow_model_requests()         model_settings, model_request_parameters = self.prepare_request(             model_settings,             model_request_parameters,         )         response = await self._chat(messages, cast(CohereModelSettings, model_settings or {}), model_request_parameters)         model_response = self._process_response(response)         return model_response      async def _chat(         self,         messages: list[ModelMessage],         model_settings: CohereModelSettings,         model_request_parameters: ModelRequestParameters,     ) -> V2ChatResponse:         tools = self._get_tools(model_request_parameters)          if model_request_parameters.builtin_tools:             raise UserError('Cohere does not support built-in tools')          cohere_messages = self._map_messages(messages)         try:             return await self.client.chat(                 model=self._model_name,                 messages=cohere_messages,                 tools=tools or OMIT,                 max_tokens=model_settings.get('max_tokens', OMIT),                 stop_sequences=model_settings.get('stop_sequences', OMIT),                 temperature=model_settings.get('temperature', OMIT),                 p=model_settings.get('top_p', OMIT),                 seed=model_settings.get('seed', OMIT),                 presence_penalty=model_settings.get('presence_penalty', OMIT),                 frequency_penalty=model_settings.get('frequency_penalty', OMIT),             )         except ApiError as e:             if (status_code := e.status_code) and status_code >= 400:                 raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e             raise  # pragma: lax no cover      def _process_response(self, response: V2ChatResponse) -> ModelResponse:         \"\"\"Process a non-streamed response, and prepare a message to return.\"\"\"         parts: list[ModelResponsePart] = []         if response.message.content is not None:             for content in response.message.content:                 if content.type == 'text':                     parts.append(TextPart(content=content.text))                 elif content.type == 'thinking':  # pragma: no branch                     parts.append(ThinkingPart(content=content.thinking))         for c in response.message.tool_calls or []:             if c.function and c.function.name and c.function.arguments:  # pragma: no branch                 parts.append(                     ToolCallPart(                         tool_name=c.function.name,                         args=c.function.arguments,                         tool_call_id=c.id or _generate_tool_call_id(),                     )                 )          raw_finish_reason = response.finish_reason         provider_details = {'finish_reason': raw_finish_reason}         finish_reason = _FINISH_REASON_MAP.get(raw_finish_reason)          return ModelResponse(             parts=parts,             usage=_map_usage(response),             model_name=self._model_name,             provider_name=self._provider.name,             finish_reason=finish_reason,             provider_details=provider_details,         )      def _map_messages(self, messages: list[ModelMessage]) -> list[ChatMessageV2]:         \"\"\"Just maps a `pydantic_ai.Message` to a `cohere.ChatMessageV2`.\"\"\"         cohere_messages: list[ChatMessageV2] = []         for message in messages:             if isinstance(message, ModelRequest):                 cohere_messages.extend(self._map_user_message(message))             elif isinstance(message, ModelResponse):                 texts: list[str] = []                 thinking: list[str] = []                 tool_calls: list[ToolCallV2] = []                 for item in message.parts:                     if isinstance(item, TextPart):                         texts.append(item.content)                     elif isinstance(item, ThinkingPart):                         thinking.append(item.content)                     elif isinstance(item, ToolCallPart):                         tool_calls.append(self._map_tool_call(item))                     elif isinstance(item, BuiltinToolCallPart | BuiltinToolReturnPart):  # pragma: no cover                         # This is currently never returned from cohere                         pass                     elif isinstance(item, FilePart):  # pragma: no cover                         # Files generated by models are not sent back to models that don't themselves generate files.                         pass                     else:                         assert_never(item)                  message_param = AssistantChatMessageV2(role='assistant')                 if texts or thinking:                     contents: list[AssistantMessageV2ContentItem] = []                     if thinking:                         contents.append(ThinkingAssistantMessageV2ContentItem(thinking='\\n\\n'.join(thinking)))                     if texts:  # pragma: no branch                         contents.append(TextAssistantMessageV2ContentItem(text='\\n\\n'.join(texts)))                     message_param.content = contents                 if tool_calls:                     message_param.tool_calls = tool_calls                 cohere_messages.append(message_param)             else:                 assert_never(message)         if instructions := self._get_instructions(messages):             cohere_messages.insert(0, SystemChatMessageV2(role='system', content=instructions))         return cohere_messages      def _get_tools(self, model_request_parameters: ModelRequestParameters) -> list[ToolV2]:         return [self._map_tool_definition(r) for r in model_request_parameters.tool_defs.values()]      @staticmethod     def _map_tool_call(t: ToolCallPart) -> ToolCallV2:         return ToolCallV2(             id=_guard_tool_call_id(t=t),             type='function',             function=ToolCallV2Function(                 name=t.tool_name,                 arguments=t.args_as_json_str(),             ),         )      @staticmethod     def _map_tool_definition(f: ToolDefinition) -> ToolV2:         return ToolV2(             type='function',             function=ToolV2Function(                 name=f.name,                 description=f.description,                 parameters=f.parameters_json_schema,             ),         )      @classmethod     def _map_user_message(cls, message: ModelRequest) -> Iterable[ChatMessageV2]:         for part in message.parts:             if isinstance(part, SystemPromptPart):                 yield SystemChatMessageV2(role='system', content=part.content)             elif isinstance(part, UserPromptPart):                 if isinstance(part.content, str):                     yield UserChatMessageV2(role='user', content=part.content)                 else:                     raise RuntimeError('Cohere does not yet support multi-modal inputs.')             elif isinstance(part, ToolReturnPart):                 yield ToolChatMessageV2(                     role='tool',                     tool_call_id=_guard_tool_call_id(t=part),                     content=part.model_response_str(),                 )             elif isinstance(part, RetryPromptPart):                 if part.tool_name is None:                     yield UserChatMessageV2(role='user', content=part.model_response())  # pragma: no cover                 else:                     yield ToolChatMessageV2(                         role='tool',                         tool_call_id=_guard_tool_call_id(t=part),                         content=part.model_response(),                     )             else:                 assert_never(part) ``` |", "url": "https://ai.pydantic.dev/models/cohere/index.html#coheremodel-dataclass", "page": "models/cohere/index.html", "source_site": "pydantic_ai"}
{"title": "CohereModel `dataclass`", "anchor": "coheremodel-dataclass", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    model_name: CohereModelName,\n    *,\n    provider: (\n        Literal[\"cohere\"] | Provider[AsyncClientV2]\n    ) = \"cohere\",\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n)\n```\n\nInitialize an Cohere model.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model_name` | `CohereModelName` | The name of the Cohere model to use. List of model names available [here](https://docs.cohere.com/docs/models#command). | *required* |\n| `provider` | `Literal['cohere'] | Provider[AsyncClientV2]` | The provider to use for authentication and API access. Can be either the string 'cohere' or an instance of `Provider[AsyncClientV2]`. If not provided, a new provider will be created using the other parameters. | `'cohere'` |\n| `profile` | `ModelProfileSpec | None` | The model profile to use. Defaults to a profile picked by the provider based on the model name. | `None` |\n| `settings` | `ModelSettings | None` | Model-specific settings that will be used as defaults for this model. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/cohere.py`\n\n|  |  |\n| --- | --- |\n| ``` 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 ``` | ``` def __init__(     self,     model_name: CohereModelName,     *,     provider: Literal['cohere'] | Provider[AsyncClientV2] = 'cohere',     profile: ModelProfileSpec | None = None,     settings: ModelSettings | None = None, ):     \"\"\"Initialize an Cohere model.      Args:         model_name: The name of the Cohere model to use. List of model names             available [here](https://docs.cohere.com/docs/models#command).         provider: The provider to use for authentication and API access. Can be either the string             'cohere' or an instance of `Provider[AsyncClientV2]`. If not provided, a new provider will be             created using the other parameters.         profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.         settings: Model-specific settings that will be used as defaults for this model.     \"\"\"     self._model_name = model_name      if isinstance(provider, str):         provider = infer_provider(provider)     self._provider = provider     self.client = provider.client      super().__init__(settings=settings, profile=profile or provider.model_profile) ``` |\n\n#### model\\_name `property`\n\n```\nmodel_name: CohereModelName\n```\n\nThe model name.\n\n#### system `property`\n\n```\nsystem: str\n```\n\nThe model provider.", "url": "https://ai.pydantic.dev/models/cohere/index.html#coheremodel-dataclass", "page": "models/cohere/index.html", "source_site": "pydantic_ai"}
{"title": "Setup", "anchor": "setup", "md_text": "For details on how to set up authentication with this model, see [model configuration for Google](../../google/index.html).", "url": "https://ai.pydantic.dev/models/google/index.html#setup", "page": "models/google/index.html", "source_site": "pydantic_ai"}
{"title": "LatestGoogleModelNames `module-attribute`", "anchor": "latestgooglemodelnames-module-attribute", "md_text": "```\nLatestGoogleModelNames = Literal[\n    \"gemini-2.0-flash\",\n    \"gemini-2.0-flash-lite\",\n    \"gemini-2.5-flash\",\n    \"gemini-2.5-flash-preview-09-2025\",\n    \"gemini-flash-latest\",\n    \"gemini-2.5-flash-lite\",\n    \"gemini-2.5-flash-lite-preview-09-2025\",\n    \"gemini-flash-lite-latest\",\n    \"gemini-2.5-pro\",\n]\n```\n\nLatest Gemini models.", "url": "https://ai.pydantic.dev/models/google/index.html#latestgooglemodelnames-module-attribute", "page": "models/google/index.html", "source_site": "pydantic_ai"}
{"title": "GoogleModelName `module-attribute`", "anchor": "googlemodelname-module-attribute", "md_text": "```\nGoogleModelName = str | LatestGoogleModelNames\n```\n\nPossible Gemini model names.\n\nSince Gemini supports a variety of date-stamped models, we explicitly list the latest models but\nallow any name in the type hints.\nSee [the Gemini API docs](https://ai.google.dev/gemini-api/docs/models/gemini#model-variations) for a full list.", "url": "https://ai.pydantic.dev/models/google/index.html#googlemodelname-module-attribute", "page": "models/google/index.html", "source_site": "pydantic_ai"}
{"title": "GoogleModelSettings", "anchor": "googlemodelsettings", "md_text": "Bases: `ModelSettings`\n\nSettings used for a Gemini model request.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/google.py`\n\n|  |  |\n| --- | --- |\n| ``` 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 ``` | ``` class GoogleModelSettings(ModelSettings, total=False):     \"\"\"Settings used for a Gemini model request.\"\"\"      # ALL FIELDS MUST BE `gemini_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.      google_safety_settings: list[SafetySettingDict]     \"\"\"The safety settings to use for the model.      See <https://ai.google.dev/gemini-api/docs/safety-settings> for more information.     \"\"\"      google_thinking_config: ThinkingConfigDict     \"\"\"The thinking configuration to use for the model.      See <https://ai.google.dev/gemini-api/docs/thinking> for more information.     \"\"\"      google_labels: dict[str, str]     \"\"\"User-defined metadata to break down billed charges. Only supported by the Vertex AI API.      See the [Gemini API docs](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/add-labels-to-api-calls) for use cases and limitations.     \"\"\"      google_video_resolution: MediaResolution     \"\"\"The video resolution to use for the model.      See <https://ai.google.dev/api/generate-content#MediaResolution> for more information.     \"\"\"      google_cached_content: str     \"\"\"The name of the cached content to use for the model.      See <https://ai.google.dev/gemini-api/docs/caching> for more information.     \"\"\" ``` |\n\n#### google\\_safety\\_settings `instance-attribute`\n\n```\ngoogle_safety_settings: list[SafetySettingDict]\n```\n\nThe safety settings to use for the model.\n\nSee <https://ai.google.dev/gemini-api/docs/safety-settings> for more information.\n\n#### google\\_thinking\\_config `instance-attribute`\n\n```\ngoogle_thinking_config: ThinkingConfigDict\n```\n\nThe thinking configuration to use for the model.\n\nSee <https://ai.google.dev/gemini-api/docs/thinking> for more information.\n\n#### google\\_labels `instance-attribute`\n\n```\ngoogle_labels: dict[str, str]\n```\n\nUser-defined metadata to break down billed charges. Only supported by the Vertex AI API.\n\nSee the [Gemini API docs](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/add-labels-to-api-calls) for use cases and limitations.\n\n#### google\\_video\\_resolution `instance-attribute`\n\n```\ngoogle_video_resolution: MediaResolution\n```\n\nThe video resolution to use for the model.\n\nSee <https://ai.google.dev/api/generate-content#MediaResolution> for more information.\n\n#### google\\_cached\\_content `instance-attribute`\n\n```\ngoogle_cached_content: str\n```\n\nThe name of the cached content to use for the model.\n\nSee <https://ai.google.dev/gemini-api/docs/caching> for more information.", "url": "https://ai.pydantic.dev/models/google/index.html#googlemodelsettings", "page": "models/google/index.html", "source_site": "pydantic_ai"}
{"title": "GoogleModel `dataclass`", "anchor": "googlemodel-dataclass", "md_text": "Bases: `Model`\n\nA model that uses Gemini via `generativelanguage.googleapis.com` API.\n\nThis is implemented from scratch rather than using a dedicated SDK, good API documentation is\navailable [here](https://ai.google.dev/api).\n\nApart from `__init__`, all methods are private or match those of the base class.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/google.py`", "url": "https://ai.pydantic.dev/models/google/index.html#googlemodel-dataclass", "page": "models/google/index.html", "source_site": "pydantic_ai"}
{"title": "GoogleModel `dataclass`", "anchor": "googlemodel-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 ``` | ``` @dataclass(init=False) class GoogleModel(Model):     \"\"\"A model that uses Gemini via `generativelanguage.googleapis.com` API.      This is implemented from scratch rather than using a dedicated SDK, good API documentation is     available [here](https://ai.google.dev/api).      Apart from `__init__`, all methods are private or match those of the base class.     \"\"\"      client: Client = field(repr=False)      _model_name: GoogleModelName = field(repr=False)     _provider: Provider[Client] = field(repr=False)     _url: str | None = field(repr=False)      def __init__(         self,         model_name: GoogleModelName,         *,         provider: Literal['google-gla', 'google-vertex', 'gateway'] | Provider[Client] = 'google-gla',         profile: ModelProfileSpec | None = None,         settings: ModelSettings | None = None,     ):         \"\"\"Initialize a Gemini model.          Args:             model_name: The name of the model to use.             provider: The provider to use for authentication and API access. Can be either the string                 'google-gla' or 'google-vertex' or an instance of `Provider[google.genai.AsyncClient]`.                 Defaults to 'google-gla'.             profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.             settings: The model settings to use. Defaults to None.         \"\"\"         self._model_name = model_name          if isinstance(provider, str):             provider = infer_provider('gateway/google-vertex' if provider == 'gateway' else provider)         self._provider = provider         self.client = provider.client          super().__init__(settings=settings, profile=profile or provider.model_profile)      @property     def base_url(self) -> str:         return self._provider.base_url      @property     def model_name(self) -> GoogleModelName:         \"\"\"The model name.\"\"\"         return self._model_name      @property     def system(self) -> str:         \"\"\"The model provider.\"\"\"         return self._provider.name      async def request(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> ModelResponse:         check_allow_model_requests()         model_settings, model_request_parameters = self.prepare_request(             model_settings,             model_request_parameters,         )         model_settings = cast(GoogleModelSettings, model_settings or {})         response = await self._generate_content(messages, False, model_settings, model_request_parameters)         return self._process_response(response)      async def count_tokens(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> usage.RequestUsage:         check_allow_model_requests()         model_settings, model_request_parameters = self.prepare_request(             model_settings,             model_request_parameters,         )         model_settings = cast(GoogleModelSettings, model_settings or {})         contents, generation_config = await self._build_content_and_config(             messages, model_settings, model_request_parameters         )          # Annoyingly, the type of `GenerateContentConfigDict.get` is \"partially `Unknown`\" because `response_schema` includes `typing._UnionGenericAlias`,         # so without this we'd need `pyright: ignore[reportUnknownMemberType]` on every line and wouldn't get type checking anyway.         generation_config = cast(dict[str, Any], generation_config)          config = CountTokensConfigDict(             http_options=generation_config.get('http_options'),         )         if self._provider.name != 'google-gla':             # The fields are not supported by the Gemini API per https://github.com/googleapis/python-genai/blob/7e4ec284dc6e521949626f3ed54028163ef9121d/google/genai/models.py#L1195-L1214             config.update(  # pragma: lax no cover                 system_instruction=generation_config.get('system_instruction'),                 tools=cast(list[ToolDict], generation_config.get('tools')),                 # Annoyingly, GenerationConfigDict has fewer fields than GenerateContentConfigDict, and no extra fields are allowed.                 generation_config=GenerationConfigDict(                     temperature=generation_config.get('temperature'),                     top_p=generation_config.get('top_p'),                     max_output_tokens=generation_config.get('max_output_tokens'),                     stop_sequences=generation_config.get('stop_sequences'),                     presence_penalty=generation_config.get('presence_penalty'),                     frequency_penalty=generation_config.get('frequency_penalty'),                     seed=generation_config.get('seed'),                     thinking_config=generation_config.get('thinking_config'),                     media_resolution=generation_config.get('media_resolution'),                     response_mime_type=generation_config.get('response_mime_type'),                     response_schema=generation_config.get('response_schema'),                 ),             )          response = await self.client.aio.models.count_tokens(             model=self._model_name,             contents=contents,             config=config,         )         if response.total_tokens is None:             raise UnexpectedModelBehavior(  # pragma: no cover                 'Total tokens missing from Gemini response', str(response)             )         return usage.RequestUsage(             input_tokens=response.total_tokens,         )      @asynccontextmanager     async def request_stream(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,         run_context: RunContext[Any] | None = None,     ) -> AsyncIterator[StreamedResponse]:         check_allow_model_requests()         model_settings, model_request_parameters = self.prepare_request(             model_settings,             model_request_parameters,         )         model_settings = cast(GoogleModelSettings, model_settings or {})         response = await self._generate_content(messages, True, model_settings, model_request_parameters)         yield await self._process_streamed_response(response, model_request_parameters)  # type: ignore      def _get_tools(self, model_request_parameters: ModelRequestParameters) -> list[ToolDict] | None:         tools: list[ToolDict] = [             ToolDict(function_declarations=[_function_declaration_from_tool(t)])             for t in model_request_parameters.tool_defs.values()         ]          if model_request_parameters.builtin_tools:             if model_request_parameters.output_tools:                 raise UserError(                     'Gemini does not support output tools and built-in tools at the same time. Use `output_type=PromptedOutput(...)` instead.'                 )             if model_request_parameters.function_tools:                 raise UserError('Gemini does not support user tools and built-in tools at the same time.')              for tool in model_request_parameters.builtin_tools:                 if isinstance(tool, WebSearchTool):                     tools.append(ToolDict(google_search=GoogleSearchDict()))                 elif isinstance(tool, UrlContextTool):                     tools.append(ToolDict(url_context=UrlContextDict()))                 elif isinstance(tool, CodeExecutionTool):                     tools.append(ToolDict(code_execution=ToolCodeExecutionDict()))                 elif isinstance(tool, ImageGenerationTool):  # pragma: no branch                     if not self.profile.supports_image_output:                         raise UserError(                             \"`ImageGenerationTool` is not supported by this model. Use a model with 'image' in the name instead.\"                         )                 else:  # pragma: no cover                     raise UserError(                         f'`{tool.__class__.__name__}` is not supported by `GoogleModel`. If it should be, please file an issue.'                     )         return tools or None      def _get_tool_config(         self, model_request_parameters: ModelRequestParameters, tools: list[ToolDict] | None     ) -> ToolConfigDict | None:         if not model_request_parameters.allow_text_output and tools:             names: list[str] = []             for tool in tools:                 for function_declaration in tool.get('function_declarations') or []:                     if name := function_declaration.get('name'):  # pragma: no branch                         names.append(name)             return _tool_config(names)         else:             return None      @overload     async def _generate_content(         self,         messages: list[ModelMessage],         stream: Literal[False],         model_settings: GoogleModelSettings,         model_request_parameters: ModelRequestParameters,     ) -> GenerateContentResponse: ...      @overload     async def _generate_content(         self,         messages: list[ModelMessage],         stream: Literal[True],         model_settings: GoogleModelSettings,         model_request_parameters: ModelRequestParameters,     ) -> Awaitable[AsyncIterator[GenerateContentResponse]]: ...      async def _generate_content(         self,         messages: list[ModelMessage],         stream: bool,         model_settings: GoogleModelSettings,         model_request_parameters: ModelRequestParameters,     ) -> GenerateContentResponse | Awaitable[AsyncIterator[GenerateContentResponse]]:         contents, config = await self._build_content_and_config(messages, model_settings, model_request_parameters)         func = self.client.aio.models.generate_content_stream if stream else self.client.aio.models.generate_content         return await func(model=self._model_name, contents=contents, config=config)  # type: ignore      async def _build_content_and_config(         self,         messages: list[ModelMessage],         model_settings: GoogleModelSettings,         model_request_parameters: ModelRequestParameters,     ) -> tuple[list[ContentUnionDict], GenerateContentConfigDict]:         tools = self._get_tools(model_request_parameters)         if tools and not self.profile.supports_tools:             raise UserError('Tools are not supported by this model.')          response_mime_type = None         response_schema = None         if model_request_parameters.output_mode == 'native':             if tools:                 raise UserError(                     'Gemini does not support `NativeOutput` and tools at the same time. Use `output_type=ToolOutput(...)` instead.'                 )             response_mime_type = 'application/json'             output_object = model_request_parameters.output_object             assert output_object is not None             response_schema = self._map_response_schema(output_object)         elif model_request_parameters.output_mode == 'prompted' and not tools:             if not self.profile.supports_json_object_output:                 raise UserError('JSON output is not supported by this model.')             response_mime_type = 'application/json'          tool_config = self._get_tool_config(model_request_parameters, tools)         system_instruction, contents = await self._map_messages(messages)          modalities = [Modality.TEXT.value]         if self.profile.supports_image_output:             modalities.append(Modality.IMAGE.value)          http_options: HttpOptionsDict = {             'headers': {'Content-Type': 'application/json', 'User-Agent': get_user_agent()}         }         if timeout := model_settings.get('timeout'):             if isinstance(timeout, int | float):                 http_options['timeout'] = int(1000 * timeout)             else:                 raise UserError('Google does not support setting ModelSettings.timeout to a httpx.Timeout')          config = GenerateContentConfigDict(             http_options=http_options,             system_instruction=system_instruction,             temperature=model_settings.get('temperature'),             top_p=model_settings.get('top_p'),             max_output_tokens=model_settings.get('max_tokens'),             stop_sequences=model_settings.get('stop_sequences'),             presence_penalty=model_settings.get('presence_penalty'),             frequency_penalty=model_settings.get('frequency_penalty'),             seed=model_settings.get('seed'),             safety_settings=model_settings.get('google_safety_settings'),             thinking_config=model_settings.get('google_thinking_config'),             labels=model_settings.get('google_labels'),             media_resolution=model_settings.get('google_video_resolution'),             cached_content=model_settings.get('google_cached_content'),             tools=cast(ToolListUnionDict, tools),             tool_config=tool_config,             response_mime_type=response_mime_type,             response_schema=response_schema,             response_modalities=modalities,         )         return contents, config      def _process_response(self, response: GenerateContentResponse) -> ModelResponse:         if not response.candidates:             raise UnexpectedModelBehavior('Expected at least one candidate in Gemini response')  # pragma: no cover          candidate = response.candidates[0]          vendor_id = response.response_id         vendor_details: dict[str, Any] | None = None         finish_reason: FinishReason | None = None         raw_finish_reason = candidate.finish_reason         if raw_finish_reason:  # pragma: no branch             vendor_details = {'finish_reason': raw_finish_reason.value}             finish_reason = _FINISH_REASON_MAP.get(raw_finish_reason)          if candidate.content is None or candidate.content.parts is None:             if finish_reason == 'content_filter' and raw_finish_reason:                 raise UnexpectedModelBehavior(                     f'Content filter {raw_finish_reason.value!r} triggered', response.model_dump_json()                 )             else:                 raise UnexpectedModelBehavior(                     'Content field missing from Gemini response', response.model_dump_json()                 )  # pragma: no cover         parts = candidate.content.parts or []          usage = _metadata_as_usage(response)         return _process_response_from_parts(             parts,             candidate.grounding_metadata,             response.model_version or self._model_name,             self._provider.name,             usage,             vendor_id=vendor_id,             vendor_details=vendor_details,             finish_reason=finish_reason,         )      async def _process_streamed_response(         self, response: AsyncIterator[GenerateContentResponse], model_request_parameters: ModelRequestParameters     ) -> StreamedResponse:         \"\"\"Process a streamed response, and prepare a streaming response to return.\"\"\"         peekable_response = _utils.PeekableAsyncStream(response)         first_chunk = await peekable_response.peek()         if isinstance(first_chunk, _utils.Unset):             raise UnexpectedModelBehavior('Streamed response ended without content or tool calls')  # pragma: no cover          return GeminiStreamedResponse(             model_request_parameters=model_request_parameters,             _model_name=first_chunk.model_version or self._model_name,             _response=peekable_response,             _timestamp=first_chunk.create_time or _utils.now_utc(),             _provider_name=self._provider.name,         )      async def _map_messages(self, messages: list[ModelMessage]) -> tuple[ContentDict | None, list[ContentUnionDict]]:         contents: list[ContentUnionDict] = []         system_parts: list[PartDict] = []          for m in messages:             if isinstance(m, ModelRequest):                 message_parts: list[PartDict] = []                  for part in m.parts:                     if isinstance(part, SystemPromptPart):                         system_parts.append({'text': part.content})                     elif isinstance(part, UserPromptPart):                         message_parts.extend(await self._map_user_prompt(part))                     elif isinstance(part, ToolReturnPart):                         message_parts.append(                             {                                 'function_response': {                                     'name': part.tool_name,                                     'response': part.model_response_object(),                                     'id': part.tool_call_id,                                 }                             }                         )                     elif isinstance(part, RetryPromptPart):                         if part.tool_name is None:                             message_parts.append({'text': part.model_response()})  # pragma: no cover                         else:                             message_parts.append(                                 {                                     'function_response': {                                         'name': part.tool_name,                                         'response': {'call_error': part.model_response()},                                         'id': part.tool_call_id,                                     }                                 }                             )                     else:                         assert_never(part)                  # Google GenAI requires at least one part in the message.                 if not message_parts:                     message_parts = [{'text': ''}]                 contents.append({'role': 'user', 'parts': message_parts})             elif isinstance(m, ModelResponse):                 contents.append(_content_model_response(m, self.system))             else:                 assert_never(m)         if instructions := self._get_instructions(messages):             system_parts.insert(0, {'text': instructions})         system_instruction = ContentDict(role='user', parts=system_parts) if system_parts else None         return system_instruction, contents      async def _map_user_prompt(self, part: UserPromptPart) -> list[PartDict]:         if isinstance(part.content, str):             return [{'text': part.content}]         else:             content: list[PartDict] = []             for item in part.content:                 if isinstance(item, str):                     content.append({'text': item})                 elif isinstance(item, BinaryContent):                     inline_data_dict: BlobDict = {'data': item.data, 'mime_type': item.media_type}                     part_dict: PartDict = {'inline_data': inline_data_dict}                     if item.vendor_metadata:                         part_dict['video_metadata'] = cast(VideoMetadataDict, item.vendor_metadata)                     content.append(part_dict)                 elif isinstance(item, VideoUrl) and item.is_youtube:                     file_data_dict: FileDataDict = {'file_uri': item.url, 'mime_type': item.media_type}                     part_dict: PartDict = {'file_data': file_data_dict}                     if item.vendor_metadata:  # pragma: no branch                         part_dict['video_metadata'] = cast(VideoMetadataDict, item.vendor_metadata)                     content.append(part_dict)                 elif isinstance(item, FileUrl):                     if item.force_download or (                         # google-gla does not support passing file urls directly, except for youtube videos                         # (see above) and files uploaded to the file API (which cannot be downloaded anyway)                         self.system == 'google-gla'                         and not item.url.startswith(r'https://generativelanguage.googleapis.com/v1beta/files')                     ):                         downloaded_item = await download_item(item, data_format='bytes')                         inline_data: BlobDict = {                             'data': downloaded_item['data'],                             'mime_type': downloaded_item['data_type'],                         }                         content.append({'inline_data': inline_data})                     else:                         file_data_dict: FileDataDict = {'file_uri': item.url, 'mime_type': item.media_type}                         content.append({'file_data': file_data_dict})  # pragma: lax no cover                 else:                     assert_never(item)         return content      def _map_response_schema(self, o: OutputObjectDefinition) -> dict[str, Any]:         response_schema = o.json_schema.copy()         if o.name:             response_schema['title'] = o.name         if o.description:             response_schema['description'] = o.description          return response_schema ``` |", "url": "https://ai.pydantic.dev/models/google/index.html#googlemodel-dataclass", "page": "models/google/index.html", "source_site": "pydantic_ai"}
{"title": "GoogleModel `dataclass`", "anchor": "googlemodel-dataclass", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    model_name: GoogleModelName,\n    *,\n    provider: (\n        Literal[\"google-gla\", \"google-vertex\", \"gateway\"]\n        | Provider[Client]\n    ) = \"google-gla\",\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n)\n```\n\nInitialize a Gemini model.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model_name` | `GoogleModelName` | The name of the model to use. | *required* |\n| `provider` | `Literal['google-gla', 'google-vertex', 'gateway'] | Provider[Client]` | The provider to use for authentication and API access. Can be either the string 'google-gla' or 'google-vertex' or an instance of `Provider[google.genai.AsyncClient]`. Defaults to 'google-gla'. | `'google-gla'` |\n| `profile` | `ModelProfileSpec | None` | The model profile to use. Defaults to a profile picked by the provider based on the model name. | `None` |\n| `settings` | `ModelSettings | None` | The model settings to use. Defaults to None. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/google.py`\n\n|  |  |\n| --- | --- |\n| ``` 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 ``` | ``` def __init__(     self,     model_name: GoogleModelName,     *,     provider: Literal['google-gla', 'google-vertex', 'gateway'] | Provider[Client] = 'google-gla',     profile: ModelProfileSpec | None = None,     settings: ModelSettings | None = None, ):     \"\"\"Initialize a Gemini model.      Args:         model_name: The name of the model to use.         provider: The provider to use for authentication and API access. Can be either the string             'google-gla' or 'google-vertex' or an instance of `Provider[google.genai.AsyncClient]`.             Defaults to 'google-gla'.         profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.         settings: The model settings to use. Defaults to None.     \"\"\"     self._model_name = model_name      if isinstance(provider, str):         provider = infer_provider('gateway/google-vertex' if provider == 'gateway' else provider)     self._provider = provider     self.client = provider.client      super().__init__(settings=settings, profile=profile or provider.model_profile) ``` |\n\n#### model\\_name `property`\n\n```\nmodel_name: GoogleModelName\n```\n\nThe model name.\n\n#### system `property`\n\n```\nsystem: str\n```\n\nThe model provider.", "url": "https://ai.pydantic.dev/models/google/index.html#googlemodel-dataclass", "page": "models/google/index.html", "source_site": "pydantic_ai"}
{"title": "GeminiStreamedResponse `dataclass`", "anchor": "geministreamedresponse-dataclass", "md_text": "Bases: `StreamedResponse`\n\nImplementation of `StreamedResponse` for the Gemini model.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/google.py`", "url": "https://ai.pydantic.dev/models/google/index.html#geministreamedresponse-dataclass", "page": "models/google/index.html", "source_site": "pydantic_ai"}
{"title": "GeminiStreamedResponse `dataclass`", "anchor": "geministreamedresponse-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 ``` | ``` @dataclass class GeminiStreamedResponse(StreamedResponse):     \"\"\"Implementation of `StreamedResponse` for the Gemini model.\"\"\"      _model_name: GoogleModelName     _response: AsyncIterator[GenerateContentResponse]     _timestamp: datetime     _provider_name: str      async def _get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:  # noqa: C901         code_execution_tool_call_id: str | None = None         async for chunk in self._response:             self._usage = _metadata_as_usage(chunk)              if not chunk.candidates:                 continue  # pragma: no cover              candidate = chunk.candidates[0]              if chunk.response_id:  # pragma: no branch                 self.provider_response_id = chunk.response_id              raw_finish_reason = candidate.finish_reason             if raw_finish_reason:                 self.provider_details = {'finish_reason': raw_finish_reason.value}                 self.finish_reason = _FINISH_REASON_MAP.get(raw_finish_reason)              # Google streams the grounding metadata (including the web search queries and results)             # _after_ the text that was generated using it, so it would show up out of order in the stream,             # and cause issues with the logic that doesn't consider text ahead of built-in tool calls as output.             # If that gets fixed (or we have a workaround), we can uncomment this:             # web_search_call, web_search_return = _map_grounding_metadata(             #     candidate.grounding_metadata, self.provider_name             # )             # if web_search_call and web_search_return:             #     yield self._parts_manager.handle_part(vendor_part_id=uuid4(), part=web_search_call)             #     yield self._parts_manager.handle_part(             #         vendor_part_id=uuid4(), part=web_search_return             #     )              if candidate.content is None or candidate.content.parts is None:                 if self.finish_reason == 'stop':  # pragma: no cover                     # Normal completion - skip this chunk                     continue                 elif self.finish_reason == 'content_filter' and raw_finish_reason:  # pragma: no cover                     raise UnexpectedModelBehavior(                         f'Content filter {raw_finish_reason.value!r} triggered', chunk.model_dump_json()                     )                 else:  # pragma: no cover                     raise UnexpectedModelBehavior(                         'Content field missing from streaming Gemini response', chunk.model_dump_json()                     )              parts = candidate.content.parts             if not parts:                 continue  # pragma: no cover              for part in parts:                 if part.thought_signature:                     signature = base64.b64encode(part.thought_signature).decode('utf-8')                     yield self._parts_manager.handle_thinking_delta(                         vendor_part_id='thinking',                         signature=signature,                         provider_name=self.provider_name,                     )                  if part.text is not None:                     if part.thought:                         yield self._parts_manager.handle_thinking_delta(vendor_part_id='thinking', content=part.text)                     else:                         maybe_event = self._parts_manager.handle_text_delta(vendor_part_id='content', content=part.text)                         if maybe_event is not None:  # pragma: no branch                             yield maybe_event                 elif part.function_call:                     maybe_event = self._parts_manager.handle_tool_call_delta(                         vendor_part_id=uuid4(),                         tool_name=part.function_call.name,                         args=part.function_call.args,                         tool_call_id=part.function_call.id,                     )                     if maybe_event is not None:  # pragma: no branch                         yield maybe_event                 elif part.inline_data is not None:                     data = part.inline_data.data                     mime_type = part.inline_data.mime_type                     assert data and mime_type, 'Inline data must have data and mime type'                     content = BinaryContent(data=data, media_type=mime_type)                     yield self._parts_manager.handle_part(                         vendor_part_id=uuid4(),                         part=FilePart(content=BinaryContent.narrow_type(content)),                     )                 elif part.executable_code is not None:                     code_execution_tool_call_id = _utils.generate_tool_call_id()                     yield self._parts_manager.handle_part(                         vendor_part_id=uuid4(),                         part=_map_executable_code(                             part.executable_code, self.provider_name, code_execution_tool_call_id                         ),                     )                 elif part.code_execution_result is not None:                     assert code_execution_tool_call_id is not None                     yield self._parts_manager.handle_part(                         vendor_part_id=uuid4(),                         part=_map_code_execution_result(                             part.code_execution_result, self.provider_name, code_execution_tool_call_id                         ),                     )                 else:                     assert part.function_response is not None, f'Unexpected part: {part}'  # pragma: no cover      @property     def model_name(self) -> GoogleModelName:         \"\"\"Get the model name of the response.\"\"\"         return self._model_name      @property     def provider_name(self) -> str:         \"\"\"Get the provider name.\"\"\"         return self._provider_name      @property     def timestamp(self) -> datetime:         \"\"\"Get the timestamp of the response.\"\"\"         return self._timestamp ``` |", "url": "https://ai.pydantic.dev/models/google/index.html#geministreamedresponse-dataclass", "page": "models/google/index.html", "source_site": "pydantic_ai"}
{"title": "GeminiStreamedResponse `dataclass`", "anchor": "geministreamedresponse-dataclass", "md_text": "#### model\\_name `property`\n\n```\nmodel_name: GoogleModelName\n```\n\nGet the model name of the response.\n\n#### provider\\_name `property`\n\n```\nprovider_name: str\n```\n\nGet the provider name.\n\n#### timestamp `property`\n\n```\ntimestamp: datetime\n```\n\nGet the timestamp of the response.", "url": "https://ai.pydantic.dev/models/google/index.html#geministreamedresponse-dataclass", "page": "models/google/index.html", "source_site": "pydantic_ai"}
{"title": "FunctionModel `dataclass`", "anchor": "functionmodel-dataclass", "md_text": "Bases: `Model`\n\nA model controlled by a local function.\n\nApart from `__init__`, all methods are private or match those of the base class.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/function.py`", "url": "https://ai.pydantic.dev/models/function/index.html#functionmodel-dataclass", "page": "models/function/index.html", "source_site": "pydantic_ai"}
{"title": "FunctionModel `dataclass`", "anchor": "functionmodel-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ```  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 ``` | ``` @dataclass(init=False) class FunctionModel(Model):     \"\"\"A model controlled by a local function.      Apart from `__init__`, all methods are private or match those of the base class.     \"\"\"      function: FunctionDef | None     stream_function: StreamFunctionDef | None      _model_name: str = field(repr=False)     _system: str = field(default='function', repr=False)      @overload     def __init__(         self,         function: FunctionDef,         *,         model_name: str | None = None,         profile: ModelProfileSpec | None = None,         settings: ModelSettings | None = None,     ) -> None: ...      @overload     def __init__(         self,         *,         stream_function: StreamFunctionDef,         model_name: str | None = None,         profile: ModelProfileSpec | None = None,         settings: ModelSettings | None = None,     ) -> None: ...      @overload     def __init__(         self,         function: FunctionDef,         *,         stream_function: StreamFunctionDef,         model_name: str | None = None,         profile: ModelProfileSpec | None = None,         settings: ModelSettings | None = None,     ) -> None: ...      def __init__(         self,         function: FunctionDef | None = None,         *,         stream_function: StreamFunctionDef | None = None,         model_name: str | None = None,         profile: ModelProfileSpec | None = None,         settings: ModelSettings | None = None,     ):         \"\"\"Initialize a `FunctionModel`.          Either `function` or `stream_function` must be provided, providing both is allowed.          Args:             function: The function to call for non-streamed requests.             stream_function: The function to call for streamed requests.             model_name: The name of the model. If not provided, a name is generated from the function names.             profile: The model profile to use.             settings: Model-specific settings that will be used as defaults for this model.         \"\"\"         if function is None and stream_function is None:             raise TypeError('Either `function` or `stream_function` must be provided')          self.function = function         self.stream_function = stream_function          function_name = self.function.__name__ if self.function is not None else ''         stream_function_name = self.stream_function.__name__ if self.stream_function is not None else ''         self._model_name = model_name or f'function:{function_name}:{stream_function_name}'          # Use a default profile that supports JSON schema and object output if none provided         if profile is None:             profile = ModelProfile(                 supports_json_schema_output=True,                 supports_json_object_output=True,             )         super().__init__(settings=settings, profile=profile)      async def request(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> ModelResponse:         model_settings, model_request_parameters = self.prepare_request(             model_settings,             model_request_parameters,         )         agent_info = AgentInfo(             function_tools=model_request_parameters.function_tools,             allow_text_output=model_request_parameters.allow_text_output,             output_tools=model_request_parameters.output_tools,             model_settings=model_settings,         )          assert self.function is not None, 'FunctionModel must receive a `function` to support non-streamed requests'          if inspect.iscoroutinefunction(self.function):             response = await self.function(messages, agent_info)         else:             response_ = await _utils.run_in_executor(self.function, messages, agent_info)             assert isinstance(response_, ModelResponse), response_             response = response_         response.model_name = self._model_name         # Add usage data if not already present         if not response.usage.has_values():  # pragma: no branch             response.usage = _estimate_usage(chain(messages, [response]))         return response      @asynccontextmanager     async def request_stream(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,         run_context: RunContext[Any] | None = None,     ) -> AsyncIterator[StreamedResponse]:         model_settings, model_request_parameters = self.prepare_request(             model_settings,             model_request_parameters,         )         agent_info = AgentInfo(             function_tools=model_request_parameters.function_tools,             allow_text_output=model_request_parameters.allow_text_output,             output_tools=model_request_parameters.output_tools,             model_settings=model_settings,         )          assert self.stream_function is not None, (             'FunctionModel must receive a `stream_function` to support streamed requests'         )          response_stream = PeekableAsyncStream(self.stream_function(messages, agent_info))          first = await response_stream.peek()         if isinstance(first, _utils.Unset):             raise ValueError('Stream function must return at least one item')          yield FunctionStreamedResponse(             model_request_parameters=model_request_parameters,             _model_name=self._model_name,             _iter=response_stream,         )      @property     def model_name(self) -> str:         \"\"\"The model name.\"\"\"         return self._model_name      @property     def system(self) -> str:         \"\"\"The system / model provider.\"\"\"         return self._system ``` |", "url": "https://ai.pydantic.dev/models/function/index.html#functionmodel-dataclass", "page": "models/function/index.html", "source_site": "pydantic_ai"}
{"title": "FunctionModel `dataclass`", "anchor": "functionmodel-dataclass", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    function: FunctionDef,\n    *,\n    model_name: str | None = None,\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n) -> None\n\n__init__(\n    *,\n    stream_function: StreamFunctionDef,\n    model_name: str | None = None,\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n) -> None\n\n__init__(\n    function: FunctionDef,\n    *,\n    stream_function: StreamFunctionDef,\n    model_name: str | None = None,\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n) -> None\n\n__init__(\n    function: FunctionDef | None = None,\n    *,\n    stream_function: StreamFunctionDef | None = None,\n    model_name: str | None = None,\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n)\n```\n\nInitialize a `FunctionModel`.\n\nEither `function` or `stream_function` must be provided, providing both is allowed.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `function` | `FunctionDef | None` | The function to call for non-streamed requests. | `None` |\n| `stream_function` | `StreamFunctionDef | None` | The function to call for streamed requests. | `None` |\n| `model_name` | `str | None` | The name of the model. If not provided, a name is generated from the function names. | `None` |\n| `profile` | `ModelProfileSpec | None` | The model profile to use. | `None` |\n| `settings` | `ModelSettings | None` | Model-specific settings that will be used as defaults for this model. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/function.py`\n\n|  |  |\n| --- | --- |\n| ```  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 ``` | ``` def __init__(     self,     function: FunctionDef | None = None,     *,     stream_function: StreamFunctionDef | None = None,     model_name: str | None = None,     profile: ModelProfileSpec | None = None,     settings: ModelSettings | None = None, ):     \"\"\"Initialize a `FunctionModel`.      Either `function` or `stream_function` must be provided, providing both is allowed.      Args:         function: The function to call for non-streamed requests.         stream_function: The function to call for streamed requests.         model_name: The name of the model. If not provided, a name is generated from the function names.         profile: The model profile to use.         settings: Model-specific settings that will be used as defaults for this model.     \"\"\"     if function is None and stream_function is None:         raise TypeError('Either `function` or `stream_function` must be provided')      self.function = function     self.stream_function = stream_function      function_name = self.function.__name__ if self.function is not None else ''     stream_function_name = self.stream_function.__name__ if self.stream_function is not None else ''     self._model_name = model_name or f'function:{function_name}:{stream_function_name}'      # Use a default profile that supports JSON schema and object output if none provided     if profile is None:         profile = ModelProfile(             supports_json_schema_output=True,             supports_json_object_output=True,         )     super().__init__(settings=settings, profile=profile) ``` |\n\n#### model\\_name `property`\n\n```\nmodel_name: str\n```\n\nThe model name.\n\n#### system `property`\n\n```\nsystem: str\n```\n\nThe system / model provider.", "url": "https://ai.pydantic.dev/models/function/index.html#functionmodel-dataclass", "page": "models/function/index.html", "source_site": "pydantic_ai"}
{"title": "AgentInfo `dataclass`", "anchor": "agentinfo-dataclass", "md_text": "Information about an agent.\n\nThis is passed as the second to functions used within [`FunctionModel`](index.html#pydantic_ai.models.function.FunctionModel).\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/function.py`\n\n|  |  |\n| --- | --- |\n| ``` 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 ``` | ``` @dataclass(frozen=True, kw_only=True) class AgentInfo:     \"\"\"Information about an agent.      This is passed as the second to functions used within [`FunctionModel`][pydantic_ai.models.function.FunctionModel].     \"\"\"      function_tools: list[ToolDefinition]     \"\"\"The function tools available on this agent.      These are the tools registered via the [`tool`][pydantic_ai.Agent.tool] and     [`tool_plain`][pydantic_ai.Agent.tool_plain] decorators.     \"\"\"     allow_text_output: bool     \"\"\"Whether a plain text output is allowed.\"\"\"     output_tools: list[ToolDefinition]     \"\"\"The tools that can called to produce the final output of the run.\"\"\"     model_settings: ModelSettings | None     \"\"\"The model settings passed to the run call.\"\"\" ``` |\n\n#### function\\_tools `instance-attribute`\n\n```\nfunction_tools: list[ToolDefinition]\n```\n\nThe function tools available on this agent.\n\nThese are the tools registered via the [`tool`](../../agent/index.html#pydantic_ai.agent.Agent.tool) and\n[`tool_plain`](../../agent/index.html#pydantic_ai.agent.Agent.tool_plain) decorators.\n\n#### allow\\_text\\_output `instance-attribute`\n\n```\nallow_text_output: bool\n```\n\nWhether a plain text output is allowed.\n\n#### output\\_tools `instance-attribute`\n\n```\noutput_tools: list[ToolDefinition]\n```\n\nThe tools that can called to produce the final output of the run.\n\n#### model\\_settings `instance-attribute`\n\n```\nmodel_settings: ModelSettings | None\n```\n\nThe model settings passed to the run call.", "url": "https://ai.pydantic.dev/models/function/index.html#agentinfo-dataclass", "page": "models/function/index.html", "source_site": "pydantic_ai"}
{"title": "DeltaToolCall `dataclass`", "anchor": "deltatoolcall-dataclass", "md_text": "Incremental change to a tool call.\n\nUsed to describe a chunk when streaming structured responses.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/function.py`\n\n|  |  |\n| --- | --- |\n| ``` 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 ``` | ``` @dataclass class DeltaToolCall:     \"\"\"Incremental change to a tool call.      Used to describe a chunk when streaming structured responses.     \"\"\"      name: str | None = None     \"\"\"Incremental change to the name of the tool.\"\"\"      json_args: str | None = None     \"\"\"Incremental change to the arguments as JSON\"\"\"      _: KW_ONLY      tool_call_id: str | None = None     \"\"\"Incremental change to the tool call ID.\"\"\" ``` |\n\n#### name `class-attribute` `instance-attribute`\n\n```\nname: str | None = None\n```\n\nIncremental change to the name of the tool.\n\n#### json\\_args `class-attribute` `instance-attribute`\n\n```\njson_args: str | None = None\n```\n\nIncremental change to the arguments as JSON\n\n#### tool\\_call\\_id `class-attribute` `instance-attribute`\n\n```\ntool_call_id: str | None = None\n```\n\nIncremental change to the tool call ID.", "url": "https://ai.pydantic.dev/models/function/index.html#deltatoolcall-dataclass", "page": "models/function/index.html", "source_site": "pydantic_ai"}
{"title": "DeltaThinkingPart `dataclass`", "anchor": "deltathinkingpart-dataclass", "md_text": "Incremental change to a thinking part.\n\nUsed to describe a chunk when streaming thinking responses.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/function.py`\n\n|  |  |\n| --- | --- |\n| ``` 240 241 242 243 244 245 246 247 248 249 250 ``` | ``` @dataclass(kw_only=True) class DeltaThinkingPart:     \"\"\"Incremental change to a thinking part.      Used to describe a chunk when streaming thinking responses.     \"\"\"      content: str | None = None     \"\"\"Incremental change to the thinking content.\"\"\"     signature: str | None = None     \"\"\"Incremental change to the thinking signature.\"\"\" ``` |\n\n#### content `class-attribute` `instance-attribute`\n\n```\ncontent: str | None = None\n```\n\nIncremental change to the thinking content.\n\n#### signature `class-attribute` `instance-attribute`\n\n```\nsignature: str | None = None\n```\n\nIncremental change to the thinking signature.", "url": "https://ai.pydantic.dev/models/function/index.html#deltathinkingpart-dataclass", "page": "models/function/index.html", "source_site": "pydantic_ai"}
{"title": "DeltaToolCalls `module-attribute`", "anchor": "deltatoolcalls-module-attribute", "md_text": "```\nDeltaToolCalls: TypeAlias = dict[int, DeltaToolCall]\n```\n\nA mapping of tool call IDs to incremental changes.", "url": "https://ai.pydantic.dev/models/function/index.html#deltatoolcalls-module-attribute", "page": "models/function/index.html", "source_site": "pydantic_ai"}
{"title": "DeltaThinkingCalls `module-attribute`", "anchor": "deltathinkingcalls-module-attribute", "md_text": "```\nDeltaThinkingCalls: TypeAlias = dict[int, DeltaThinkingPart]\n```\n\nA mapping of thinking call IDs to incremental changes.", "url": "https://ai.pydantic.dev/models/function/index.html#deltathinkingcalls-module-attribute", "page": "models/function/index.html", "source_site": "pydantic_ai"}
{"title": "FunctionDef `module-attribute`", "anchor": "functiondef-module-attribute", "md_text": "```\nFunctionDef: TypeAlias = Callable[\n    [list[ModelMessage], AgentInfo],\n    ModelResponse | Awaitable[ModelResponse],\n]\n```\n\nA function used to generate a non-streamed response.", "url": "https://ai.pydantic.dev/models/function/index.html#functiondef-module-attribute", "page": "models/function/index.html", "source_site": "pydantic_ai"}
{"title": "StreamFunctionDef `module-attribute`", "anchor": "streamfunctiondef-module-attribute", "md_text": "```\nStreamFunctionDef: TypeAlias = Callable[\n    [list[ModelMessage], AgentInfo],\n    AsyncIterator[\n        str\n        | DeltaToolCalls\n        | DeltaThinkingCalls\n        | BuiltinToolCallsReturns\n    ],\n]\n```\n\nA function used to generate a streamed response.\n\nWhile this is defined as having return type of `AsyncIterator[str | DeltaToolCalls | DeltaThinkingCalls | BuiltinTools]`, it should\nreally be considered as `AsyncIterator[str] | AsyncIterator[DeltaToolCalls] | AsyncIterator[DeltaThinkingCalls]`,\n\nE.g. you need to yield all text, all `DeltaToolCalls`, all `DeltaThinkingCalls`, or all `BuiltinToolCallsReturns`, not mix them.", "url": "https://ai.pydantic.dev/models/function/index.html#streamfunctiondef-module-attribute", "page": "models/function/index.html", "source_site": "pydantic_ai"}
{"title": "FunctionStreamedResponse `dataclass`", "anchor": "functionstreamedresponse-dataclass", "md_text": "Bases: `StreamedResponse`\n\nImplementation of `StreamedResponse` for [FunctionModel](index.html#pydantic_ai.models.function.FunctionModel).\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/function.py`\n\n|  |  |\n| --- | --- |\n| ``` 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 ``` | ``` @dataclass class FunctionStreamedResponse(StreamedResponse):     \"\"\"Implementation of `StreamedResponse` for [FunctionModel][pydantic_ai.models.function.FunctionModel].\"\"\"      _model_name: str     _iter: AsyncIterator[str | DeltaToolCalls | DeltaThinkingCalls | BuiltinToolCallsReturns]     _timestamp: datetime = field(default_factory=_utils.now_utc)      def __post_init__(self):         self._usage += _estimate_usage([])      async def _get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:         async for item in self._iter:             if isinstance(item, str):                 response_tokens = _estimate_string_tokens(item)                 self._usage += usage.RequestUsage(output_tokens=response_tokens)                 maybe_event = self._parts_manager.handle_text_delta(vendor_part_id='content', content=item)                 if maybe_event is not None:  # pragma: no branch                     yield maybe_event             elif isinstance(item, dict) and item:                 for dtc_index, delta in item.items():                     if isinstance(delta, DeltaThinkingPart):                         if delta.content:  # pragma: no branch                             response_tokens = _estimate_string_tokens(delta.content)                             self._usage += usage.RequestUsage(output_tokens=response_tokens)                         yield self._parts_manager.handle_thinking_delta(                             vendor_part_id=dtc_index,                             content=delta.content,                             signature=delta.signature,                             provider_name='function' if delta.signature else None,                         )                     elif isinstance(delta, DeltaToolCall):                         if delta.json_args:                             response_tokens = _estimate_string_tokens(delta.json_args)                             self._usage += usage.RequestUsage(output_tokens=response_tokens)                         maybe_event = self._parts_manager.handle_tool_call_delta(                             vendor_part_id=dtc_index,                             tool_name=delta.name,                             args=delta.json_args,                             tool_call_id=delta.tool_call_id,                         )                         if maybe_event is not None:  # pragma: no branch                             yield maybe_event                     elif isinstance(delta, BuiltinToolCallPart):                         if content := delta.args_as_json_str():  # pragma: no branch                             response_tokens = _estimate_string_tokens(content)                             self._usage += usage.RequestUsage(output_tokens=response_tokens)                         yield self._parts_manager.handle_part(vendor_part_id=dtc_index, part=delta)                     elif isinstance(delta, BuiltinToolReturnPart):                         if content := delta.model_response_str():  # pragma: no branch                             response_tokens = _estimate_string_tokens(content)                             self._usage += usage.RequestUsage(output_tokens=response_tokens)                         yield self._parts_manager.handle_part(vendor_part_id=dtc_index, part=delta)                     else:                         assert_never(delta)      @property     def model_name(self) -> str:         \"\"\"Get the model name of the response.\"\"\"         return self._model_name      @property     def provider_name(self) -> None:         \"\"\"Get the provider name.\"\"\"         return None      @property     def timestamp(self) -> datetime:         \"\"\"Get the timestamp of the response.\"\"\"         return self._timestamp ``` |\n\n#### model\\_name `property`\n\n```\nmodel_name: str\n```\n\nGet the model name of the response.\n\n#### provider\\_name `property`\n\n```\nprovider_name: None\n```\n\nGet the provider name.\n\n#### timestamp `property`\n\n```\ntimestamp: datetime\n```\n\nGet the timestamp of the response.", "url": "https://ai.pydantic.dev/models/function/index.html#functionstreamedresponse-dataclass", "page": "models/function/index.html", "source_site": "pydantic_ai"}
{"title": "Setup", "anchor": "setup", "md_text": "For details on how to set up authentication with this model, see [model configuration for Anthropic](../../anthropic/index.html).", "url": "https://ai.pydantic.dev/models/anthropic/index.html#setup", "page": "models/anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "LatestAnthropicModelNames `module-attribute`", "anchor": "latestanthropicmodelnames-module-attribute", "md_text": "```\nLatestAnthropicModelNames = ModelParam\n```\n\nLatest Anthropic models.", "url": "https://ai.pydantic.dev/models/anthropic/index.html#latestanthropicmodelnames-module-attribute", "page": "models/anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "AnthropicModelName `module-attribute`", "anchor": "anthropicmodelname-module-attribute", "md_text": "```\nAnthropicModelName = str | LatestAnthropicModelNames\n```\n\nPossible Anthropic model names.\n\nSince Anthropic supports a variety of date-stamped models, we explicitly list the latest models but\nallow any name in the type hints.\nSee [the Anthropic docs](https://docs.anthropic.com/en/docs/about-claude/models) for a full list.", "url": "https://ai.pydantic.dev/models/anthropic/index.html#anthropicmodelname-module-attribute", "page": "models/anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "AnthropicModelSettings", "anchor": "anthropicmodelsettings", "md_text": "Bases: `ModelSettings`\n\nSettings used for an Anthropic model request.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/anthropic.py`\n\n|  |  |\n| --- | --- |\n| ``` 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 ``` | ``` class AnthropicModelSettings(ModelSettings, total=False):     \"\"\"Settings used for an Anthropic model request.\"\"\"      # ALL FIELDS MUST BE `anthropic_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.      anthropic_metadata: BetaMetadataParam     \"\"\"An object describing metadata about the request.      Contains `user_id`, an external identifier for the user who is associated with the request.     \"\"\"      anthropic_thinking: BetaThinkingConfigParam     \"\"\"Determine whether the model should generate a thinking block.      See [the Anthropic docs](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking) for more information.     \"\"\" ``` |\n\n#### anthropic\\_metadata `instance-attribute`\n\n```\nanthropic_metadata: BetaMetadataParam\n```\n\nAn object describing metadata about the request.\n\nContains `user_id`, an external identifier for the user who is associated with the request.\n\n#### anthropic\\_thinking `instance-attribute`\n\n```\nanthropic_thinking: BetaThinkingConfigParam\n```\n\nDetermine whether the model should generate a thinking block.\n\nSee [the Anthropic docs](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking) for more information.", "url": "https://ai.pydantic.dev/models/anthropic/index.html#anthropicmodelsettings", "page": "models/anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "AnthropicModel `dataclass`", "anchor": "anthropicmodel-dataclass", "md_text": "Bases: `Model`\n\nA model that uses the Anthropic API.\n\nInternally, this uses the [Anthropic Python client](https://github.com/anthropics/anthropic-sdk-python) to interact with the API.\n\nApart from `__init__`, all methods are private or match those of the base class.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/anthropic.py`", "url": "https://ai.pydantic.dev/models/anthropic/index.html#anthropicmodel-dataclass", "page": "models/anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "AnthropicModel `dataclass`", "anchor": "anthropicmodel-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 ``` | ``` @dataclass(init=False) class AnthropicModel(Model):     \"\"\"A model that uses the Anthropic API.      Internally, this uses the [Anthropic Python client](https://github.com/anthropics/anthropic-sdk-python) to interact with the API.      Apart from `__init__`, all methods are private or match those of the base class.     \"\"\"      client: AsyncAnthropicClient = field(repr=False)      _model_name: AnthropicModelName = field(repr=False)     _provider: Provider[AsyncAnthropicClient] = field(repr=False)      def __init__(         self,         model_name: AnthropicModelName,         *,         provider: Literal['anthropic', 'gateway'] | Provider[AsyncAnthropicClient] = 'anthropic',         profile: ModelProfileSpec | None = None,         settings: ModelSettings | None = None,     ):         \"\"\"Initialize an Anthropic model.          Args:             model_name: The name of the Anthropic model to use. List of model names available                 [here](https://docs.anthropic.com/en/docs/about-claude/models).             provider: The provider to use for the Anthropic API. Can be either the string 'anthropic' or an                 instance of `Provider[AsyncAnthropicClient]`. If not provided, the other parameters will be used.             profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.             settings: Default model settings for this model instance.         \"\"\"         self._model_name = model_name          if isinstance(provider, str):             provider = infer_provider('gateway/anthropic' if provider == 'gateway' else provider)         self._provider = provider         self.client = provider.client          super().__init__(settings=settings, profile=profile or provider.model_profile)      @property     def base_url(self) -> str:         return str(self.client.base_url)      @property     def model_name(self) -> AnthropicModelName:         \"\"\"The model name.\"\"\"         return self._model_name      @property     def system(self) -> str:         \"\"\"The model provider.\"\"\"         return self._provider.name      async def request(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> ModelResponse:         check_allow_model_requests()         model_settings, model_request_parameters = self.prepare_request(             model_settings,             model_request_parameters,         )         response = await self._messages_create(             messages, False, cast(AnthropicModelSettings, model_settings or {}), model_request_parameters         )         model_response = self._process_response(response)         return model_response      @asynccontextmanager     async def request_stream(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,         run_context: RunContext[Any] | None = None,     ) -> AsyncIterator[StreamedResponse]:         check_allow_model_requests()         model_settings, model_request_parameters = self.prepare_request(             model_settings,             model_request_parameters,         )         response = await self._messages_create(             messages, True, cast(AnthropicModelSettings, model_settings or {}), model_request_parameters         )         async with response:             yield await self._process_streamed_response(response, model_request_parameters)      @overload     async def _messages_create(         self,         messages: list[ModelMessage],         stream: Literal[True],         model_settings: AnthropicModelSettings,         model_request_parameters: ModelRequestParameters,     ) -> AsyncStream[BetaRawMessageStreamEvent]:         pass      @overload     async def _messages_create(         self,         messages: list[ModelMessage],         stream: Literal[False],         model_settings: AnthropicModelSettings,         model_request_parameters: ModelRequestParameters,     ) -> BetaMessage:         pass      async def _messages_create(         self,         messages: list[ModelMessage],         stream: bool,         model_settings: AnthropicModelSettings,         model_request_parameters: ModelRequestParameters,     ) -> BetaMessage | AsyncStream[BetaRawMessageStreamEvent]:         # standalone function to make it easier to override         tools = self._get_tools(model_request_parameters)         tools, mcp_servers, beta_features = self._add_builtin_tools(tools, model_request_parameters)          tool_choice: BetaToolChoiceParam | None          if not tools:             tool_choice = None         else:             if not model_request_parameters.allow_text_output:                 tool_choice = {'type': 'any'}                 if (thinking := model_settings.get('anthropic_thinking')) and thinking.get('type') == 'enabled':                     raise UserError(                         'Anthropic does not support thinking and output tools at the same time. Use `output_type=PromptedOutput(...)` instead.'                     )             else:                 tool_choice = {'type': 'auto'}              if (allow_parallel_tool_calls := model_settings.get('parallel_tool_calls')) is not None:                 tool_choice['disable_parallel_tool_use'] = not allow_parallel_tool_calls          system_prompt, anthropic_messages = await self._map_message(messages)          try:             extra_headers = model_settings.get('extra_headers', {})             extra_headers.setdefault('User-Agent', get_user_agent())             if beta_features:                 if 'anthropic-beta' in extra_headers:                     beta_features.insert(0, extra_headers['anthropic-beta'])                 extra_headers['anthropic-beta'] = ','.join(beta_features)              return await self.client.beta.messages.create(                 max_tokens=model_settings.get('max_tokens', 4096),                 system=system_prompt or OMIT,                 messages=anthropic_messages,                 model=self._model_name,                 tools=tools or OMIT,                 tool_choice=tool_choice or OMIT,                 mcp_servers=mcp_servers or OMIT,                 stream=stream,                 thinking=model_settings.get('anthropic_thinking', OMIT),                 stop_sequences=model_settings.get('stop_sequences', OMIT),                 temperature=model_settings.get('temperature', OMIT),                 top_p=model_settings.get('top_p', OMIT),                 timeout=model_settings.get('timeout', NOT_GIVEN),                 metadata=model_settings.get('anthropic_metadata', OMIT),                 extra_headers=extra_headers,                 extra_body=model_settings.get('extra_body'),             )         except APIStatusError as e:             if (status_code := e.status_code) >= 400:                 raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e             raise  # pragma: lax no cover      def _process_response(self, response: BetaMessage) -> ModelResponse:         \"\"\"Process a non-streamed response, and prepare a message to return.\"\"\"         items: list[ModelResponsePart] = []         builtin_tool_calls: dict[str, BuiltinToolCallPart] = {}         for item in response.content:             if isinstance(item, BetaTextBlock):                 items.append(TextPart(content=item.text))             elif isinstance(item, BetaServerToolUseBlock):                 call_part = _map_server_tool_use_block(item, self.system)                 builtin_tool_calls[call_part.tool_call_id] = call_part                 items.append(call_part)             elif isinstance(item, BetaWebSearchToolResultBlock):                 items.append(_map_web_search_tool_result_block(item, self.system))             elif isinstance(item, BetaCodeExecutionToolResultBlock):                 items.append(_map_code_execution_tool_result_block(item, self.system))             elif isinstance(item, BetaRedactedThinkingBlock):                 items.append(                     ThinkingPart(id='redacted_thinking', content='', signature=item.data, provider_name=self.system)                 )             elif isinstance(item, BetaThinkingBlock):                 items.append(ThinkingPart(content=item.thinking, signature=item.signature, provider_name=self.system))             elif isinstance(item, BetaMCPToolUseBlock):                 call_part = _map_mcp_server_use_block(item, self.system)                 builtin_tool_calls[call_part.tool_call_id] = call_part                 items.append(call_part)             elif isinstance(item, BetaMCPToolResultBlock):                 call_part = builtin_tool_calls.get(item.tool_use_id)                 items.append(_map_mcp_server_result_block(item, call_part, self.system))             else:                 assert isinstance(item, BetaToolUseBlock), f'unexpected item type {type(item)}'                 items.append(                     ToolCallPart(                         tool_name=item.name,                         args=cast(dict[str, Any], item.input),                         tool_call_id=item.id,                     )                 )          finish_reason: FinishReason | None = None         provider_details: dict[str, Any] | None = None         if raw_finish_reason := response.stop_reason:  # pragma: no branch             provider_details = {'finish_reason': raw_finish_reason}             finish_reason = _FINISH_REASON_MAP.get(raw_finish_reason)          return ModelResponse(             parts=items,             usage=_map_usage(response, self._provider.name, self._provider.base_url, self._model_name),             model_name=response.model,             provider_response_id=response.id,             provider_name=self._provider.name,             finish_reason=finish_reason,             provider_details=provider_details,         )      async def _process_streamed_response(         self, response: AsyncStream[BetaRawMessageStreamEvent], model_request_parameters: ModelRequestParameters     ) -> StreamedResponse:         peekable_response = _utils.PeekableAsyncStream(response)         first_chunk = await peekable_response.peek()         if isinstance(first_chunk, _utils.Unset):             raise UnexpectedModelBehavior('Streamed response ended without content or tool calls')  # pragma: no cover          assert isinstance(first_chunk, BetaRawMessageStartEvent)          return AnthropicStreamedResponse(             model_request_parameters=model_request_parameters,             _model_name=first_chunk.message.model,             _response=peekable_response,             _timestamp=_utils.now_utc(),             _provider_name=self._provider.name,             _provider_url=self._provider.base_url,         )      def _get_tools(self, model_request_parameters: ModelRequestParameters) -> list[BetaToolUnionParam]:         return [self._map_tool_definition(r) for r in model_request_parameters.tool_defs.values()]      def _add_builtin_tools(         self, tools: list[BetaToolUnionParam], model_request_parameters: ModelRequestParameters     ) -> tuple[list[BetaToolUnionParam], list[BetaRequestMCPServerURLDefinitionParam], list[str]]:         beta_features: list[str] = []         mcp_servers: list[BetaRequestMCPServerURLDefinitionParam] = []         for tool in model_request_parameters.builtin_tools:             if isinstance(tool, WebSearchTool):                 user_location = UserLocation(type='approximate', **tool.user_location) if tool.user_location else None                 tools.append(                     BetaWebSearchTool20250305Param(                         name='web_search',                         type='web_search_20250305',                         max_uses=tool.max_uses,                         allowed_domains=tool.allowed_domains,                         blocked_domains=tool.blocked_domains,                         user_location=user_location,                     )                 )             elif isinstance(tool, CodeExecutionTool):  # pragma: no branch                 tools.append(BetaCodeExecutionTool20250522Param(name='code_execution', type='code_execution_20250522'))                 beta_features.append('code-execution-2025-05-22')             elif isinstance(tool, MemoryTool):  # pragma: no branch                 if 'memory' not in model_request_parameters.tool_defs:                     raise UserError(\"Built-in `MemoryTool` requires a 'memory' tool to be defined.\")                 # Replace the memory tool definition with the built-in memory tool                 tools = [tool for tool in tools if tool['name'] != 'memory']                 tools.append(BetaMemoryTool20250818Param(name='memory', type='memory_20250818'))                 beta_features.append('context-management-2025-06-27')             elif isinstance(tool, MCPServerTool) and tool.url:                 mcp_server_url_definition_param = BetaRequestMCPServerURLDefinitionParam(                     type='url',                     name=tool.id,                     url=tool.url,                 )                 if tool.allowed_tools is not None:  # pragma: no branch                     mcp_server_url_definition_param['tool_configuration'] = BetaRequestMCPServerToolConfigurationParam(                         enabled=bool(tool.allowed_tools),                         allowed_tools=tool.allowed_tools,                     )                 if tool.authorization_token:  # pragma: no cover                     mcp_server_url_definition_param['authorization_token'] = tool.authorization_token                 mcp_servers.append(mcp_server_url_definition_param)                 beta_features.append('mcp-client-2025-04-04')             else:  # pragma: no cover                 raise UserError(                     f'`{tool.__class__.__name__}` is not supported by `AnthropicModel`. If it should be, please file an issue.'                 )         return tools, mcp_servers, beta_features      async def _map_message(self, messages: list[ModelMessage]) -> tuple[str, list[BetaMessageParam]]:  # noqa: C901         \"\"\"Just maps a `pydantic_ai.Message` to a `anthropic.types.MessageParam`.\"\"\"         system_prompt_parts: list[str] = []         anthropic_messages: list[BetaMessageParam] = []         for m in messages:             if isinstance(m, ModelRequest):                 user_content_params: list[BetaContentBlockParam] = []                 for request_part in m.parts:                     if isinstance(request_part, SystemPromptPart):                         system_prompt_parts.append(request_part.content)                     elif isinstance(request_part, UserPromptPart):                         async for content in self._map_user_prompt(request_part):                             user_content_params.append(content)                     elif isinstance(request_part, ToolReturnPart):                         tool_result_block_param = BetaToolResultBlockParam(                             tool_use_id=_guard_tool_call_id(t=request_part),                             type='tool_result',                             content=request_part.model_response_str(),                             is_error=False,                         )                         user_content_params.append(tool_result_block_param)                     elif isinstance(request_part, RetryPromptPart):  # pragma: no branch                         if request_part.tool_name is None:                             text = request_part.model_response()  # pragma: no cover                             retry_param = BetaTextBlockParam(type='text', text=text)  # pragma: no cover                         else:                             retry_param = BetaToolResultBlockParam(                                 tool_use_id=_guard_tool_call_id(t=request_part),                                 type='tool_result',                                 content=request_part.model_response(),                                 is_error=True,                             )                         user_content_params.append(retry_param)                 if len(user_content_params) > 0:                     anthropic_messages.append(BetaMessageParam(role='user', content=user_content_params))             elif isinstance(m, ModelResponse):                 assistant_content_params: list[                     BetaTextBlockParam                     | BetaToolUseBlockParam                     | BetaServerToolUseBlockParam                     | BetaWebSearchToolResultBlockParam                     | BetaCodeExecutionToolResultBlockParam                     | BetaThinkingBlockParam                     | BetaRedactedThinkingBlockParam                     | BetaMCPToolUseBlockParam                     | BetaMCPToolResultBlock                 ] = []                 for response_part in m.parts:                     if isinstance(response_part, TextPart):                         if response_part.content:                             assistant_content_params.append(BetaTextBlockParam(text=response_part.content, type='text'))                     elif isinstance(response_part, ToolCallPart):                         tool_use_block_param = BetaToolUseBlockParam(                             id=_guard_tool_call_id(t=response_part),                             type='tool_use',                             name=response_part.tool_name,                             input=response_part.args_as_dict(),                         )                         assistant_content_params.append(tool_use_block_param)                     elif isinstance(response_part, ThinkingPart):                         if (                             response_part.provider_name == self.system and response_part.signature is not None                         ):  # pragma: no branch                             if response_part.id == 'redacted_thinking':                                 assistant_content_params.append(                                     BetaRedactedThinkingBlockParam(                                         data=response_part.signature,                                         type='redacted_thinking',                                     )                                 )                             else:                                 assistant_content_params.append(                                     BetaThinkingBlockParam(                                         thinking=response_part.content,                                         signature=response_part.signature,                                         type='thinking',                                     )                                 )                         elif response_part.content:  # pragma: no branch                             start_tag, end_tag = self.profile.thinking_tags                             assistant_content_params.append(                                 BetaTextBlockParam(                                     text='\\n'.join([start_tag, response_part.content, end_tag]), type='text'                                 )                             )                     elif isinstance(response_part, BuiltinToolCallPart):                         if response_part.provider_name == self.system:                             tool_use_id = _guard_tool_call_id(t=response_part)                             if response_part.tool_name == WebSearchTool.kind:                                 server_tool_use_block_param = BetaServerToolUseBlockParam(                                     id=tool_use_id,                                     type='server_tool_use',                                     name='web_search',                                     input=response_part.args_as_dict(),                                 )                                 assistant_content_params.append(server_tool_use_block_param)                             elif response_part.tool_name == CodeExecutionTool.kind:                                 server_tool_use_block_param = BetaServerToolUseBlockParam(                                     id=tool_use_id,                                     type='server_tool_use',                                     name='code_execution',                                     input=response_part.args_as_dict(),                                 )                                 assistant_content_params.append(server_tool_use_block_param)                             elif (                                 response_part.tool_name.startswith(MCPServerTool.kind)                                 and (server_id := response_part.tool_name.split(':', 1)[1])                                 and (args := response_part.args_as_dict())                                 and (tool_name := args.get('tool_name'))                                 and (tool_args := args.get('tool_args'))                             ):  # pragma: no branch                                 mcp_tool_use_block_param = BetaMCPToolUseBlockParam(                                     id=tool_use_id,                                     type='mcp_tool_use',                                     server_name=server_id,                                     name=tool_name,                                     input=tool_args,                                 )                                 assistant_content_params.append(mcp_tool_use_block_param)                     elif isinstance(response_part, BuiltinToolReturnPart):                         if response_part.provider_name == self.system:                             tool_use_id = _guard_tool_call_id(t=response_part)                             if response_part.tool_name in (                                 WebSearchTool.kind,                                 'web_search_tool_result',  # Backward compatibility                             ) and isinstance(response_part.content, dict | list):                                 assistant_content_params.append(                                     BetaWebSearchToolResultBlockParam(                                         tool_use_id=tool_use_id,                                         type='web_search_tool_result',                                         content=cast(                                             BetaWebSearchToolResultBlockParamContentParam,                                             response_part.content,  # pyright: ignore[reportUnknownMemberType]                                         ),                                     )                                 )                             elif response_part.tool_name in (  # pragma: no branch                                 CodeExecutionTool.kind,                                 'code_execution_tool_result',  # Backward compatibility                             ) and isinstance(response_part.content, dict):                                 assistant_content_params.append(                                     BetaCodeExecutionToolResultBlockParam(                                         tool_use_id=tool_use_id,                                         type='code_execution_tool_result',                                         content=cast(                                             BetaCodeExecutionToolResultBlockParamContentParam,                                             response_part.content,  # pyright: ignore[reportUnknownMemberType]                                         ),                                     )                                 )                             elif response_part.tool_name.startswith(MCPServerTool.kind) and isinstance(                                 response_part.content, dict                             ):  # pragma: no branch                                 assistant_content_params.append(                                     BetaMCPToolResultBlock(                                         tool_use_id=tool_use_id,                                         type='mcp_tool_result',                                         **cast(dict[str, Any], response_part.content),  # pyright: ignore[reportUnknownMemberType]                                     )                                 )                     elif isinstance(response_part, FilePart):  # pragma: no cover                         # Files generated by models are not sent back to models that don't themselves generate files.                         pass                     else:                         assert_never(response_part)                 if len(assistant_content_params) > 0:                     anthropic_messages.append(BetaMessageParam(role='assistant', content=assistant_content_params))             else:                 assert_never(m)         if instructions := self._get_instructions(messages):             system_prompt_parts.insert(0, instructions)         system_prompt = '\\n\\n'.join(system_prompt_parts)         return system_prompt, anthropic_messages      @staticmethod     async def _map_user_prompt(         part: UserPromptPart,     ) -> AsyncGenerator[BetaContentBlockParam]:         if isinstance(part.content, str):             if part.content:  # Only yield non-empty text                 yield BetaTextBlockParam(text=part.content, type='text')         else:             for item in part.content:                 if isinstance(item, str):                     if item:  # Only yield non-empty text                         yield BetaTextBlockParam(text=item, type='text')                 elif isinstance(item, BinaryContent):                     if item.is_image:                         yield BetaImageBlockParam(                             source={'data': io.BytesIO(item.data), 'media_type': item.media_type, 'type': 'base64'},  # type: ignore                             type='image',                         )                     elif item.media_type == 'application/pdf':                         yield BetaBase64PDFBlockParam(                             source=BetaBase64PDFSourceParam(                                 data=io.BytesIO(item.data),                                 media_type='application/pdf',                                 type='base64',                             ),                             type='document',                         )                     else:                         raise RuntimeError('Only images and PDFs are supported for binary content')                 elif isinstance(item, ImageUrl):                     yield BetaImageBlockParam(source={'type': 'url', 'url': item.url}, type='image')                 elif isinstance(item, DocumentUrl):                     if item.media_type == 'application/pdf':                         yield BetaBase64PDFBlockParam(source={'url': item.url, 'type': 'url'}, type='document')                     elif item.media_type == 'text/plain':                         downloaded_item = await download_item(item, data_format='text')                         yield BetaBase64PDFBlockParam(                             source=BetaPlainTextSourceParam(                                 data=downloaded_item['data'], media_type=item.media_type, type='text'                             ),                             type='document',                         )                     else:  # pragma: no cover                         raise RuntimeError(f'Unsupported media type: {item.media_type}')                 else:                     raise RuntimeError(f'Unsupported content type: {type(item)}')  # pragma: no cover      @staticmethod     def _map_tool_definition(f: ToolDefinition) -> BetaToolParam:         return {             'name': f.name,             'description': f.description or '',             'input_schema': f.parameters_json_schema,         } ``` |", "url": "https://ai.pydantic.dev/models/anthropic/index.html#anthropicmodel-dataclass", "page": "models/anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "AnthropicModel `dataclass`", "anchor": "anthropicmodel-dataclass", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    model_name: AnthropicModelName,\n    *,\n    provider: (\n        Literal[\"anthropic\", \"gateway\"]\n        | Provider[AsyncAnthropicClient]\n    ) = \"anthropic\",\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n)\n```\n\nInitialize an Anthropic model.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model_name` | `AnthropicModelName` | The name of the Anthropic model to use. List of model names available [here](https://docs.anthropic.com/en/docs/about-claude/models). | *required* |\n| `provider` | `Literal['anthropic', 'gateway'] | Provider[AsyncAnthropicClient]` | The provider to use for the Anthropic API. Can be either the string 'anthropic' or an instance of `Provider[AsyncAnthropicClient]`. If not provided, the other parameters will be used. | `'anthropic'` |\n| `profile` | `ModelProfileSpec | None` | The model profile to use. Defaults to a profile picked by the provider based on the model name. | `None` |\n| `settings` | `ModelSettings | None` | Default model settings for this model instance. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/anthropic.py`\n\n|  |  |\n| --- | --- |\n| ``` 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 ``` | ``` def __init__(     self,     model_name: AnthropicModelName,     *,     provider: Literal['anthropic', 'gateway'] | Provider[AsyncAnthropicClient] = 'anthropic',     profile: ModelProfileSpec | None = None,     settings: ModelSettings | None = None, ):     \"\"\"Initialize an Anthropic model.      Args:         model_name: The name of the Anthropic model to use. List of model names available             [here](https://docs.anthropic.com/en/docs/about-claude/models).         provider: The provider to use for the Anthropic API. Can be either the string 'anthropic' or an             instance of `Provider[AsyncAnthropicClient]`. If not provided, the other parameters will be used.         profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.         settings: Default model settings for this model instance.     \"\"\"     self._model_name = model_name      if isinstance(provider, str):         provider = infer_provider('gateway/anthropic' if provider == 'gateway' else provider)     self._provider = provider     self.client = provider.client      super().__init__(settings=settings, profile=profile or provider.model_profile) ``` |\n\n#### model\\_name `property`\n\n```\nmodel_name: AnthropicModelName\n```\n\nThe model name.\n\n#### system `property`\n\n```\nsystem: str\n```\n\nThe model provider.", "url": "https://ai.pydantic.dev/models/anthropic/index.html#anthropicmodel-dataclass", "page": "models/anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "AnthropicStreamedResponse `dataclass`", "anchor": "anthropicstreamedresponse-dataclass", "md_text": "Bases: `StreamedResponse`\n\nImplementation of `StreamedResponse` for Anthropic models.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/anthropic.py`", "url": "https://ai.pydantic.dev/models/anthropic/index.html#anthropicstreamedresponse-dataclass", "page": "models/anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "AnthropicStreamedResponse `dataclass`", "anchor": "anthropicstreamedresponse-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 ``` | ``` @dataclass class AnthropicStreamedResponse(StreamedResponse):     \"\"\"Implementation of `StreamedResponse` for Anthropic models.\"\"\"      _model_name: AnthropicModelName     _response: AsyncIterable[BetaRawMessageStreamEvent]     _timestamp: datetime     _provider_name: str     _provider_url: str      async def _get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:  # noqa: C901         current_block: BetaContentBlock | None = None          builtin_tool_calls: dict[str, BuiltinToolCallPart] = {}         async for event in self._response:             if isinstance(event, BetaRawMessageStartEvent):                 self._usage = _map_usage(event, self._provider_name, self._provider_url, self._model_name)                 self.provider_response_id = event.message.id              elif isinstance(event, BetaRawContentBlockStartEvent):                 current_block = event.content_block                 if isinstance(current_block, BetaTextBlock) and current_block.text:                     maybe_event = self._parts_manager.handle_text_delta(                         vendor_part_id=event.index, content=current_block.text                     )                     if maybe_event is not None:  # pragma: no branch                         yield maybe_event                 elif isinstance(current_block, BetaThinkingBlock):                     yield self._parts_manager.handle_thinking_delta(                         vendor_part_id=event.index,                         content=current_block.thinking,                         signature=current_block.signature,                         provider_name=self.provider_name,                     )                 elif isinstance(current_block, BetaRedactedThinkingBlock):                     yield self._parts_manager.handle_thinking_delta(                         vendor_part_id=event.index,                         id='redacted_thinking',                         signature=current_block.data,                         provider_name=self.provider_name,                     )                 elif isinstance(current_block, BetaToolUseBlock):                     maybe_event = self._parts_manager.handle_tool_call_delta(                         vendor_part_id=event.index,                         tool_name=current_block.name,                         args=cast(dict[str, Any], current_block.input) or None,                         tool_call_id=current_block.id,                     )                     if maybe_event is not None:  # pragma: no branch                         yield maybe_event                 elif isinstance(current_block, BetaServerToolUseBlock):                     call_part = _map_server_tool_use_block(current_block, self.provider_name)                     builtin_tool_calls[call_part.tool_call_id] = call_part                     yield self._parts_manager.handle_part(                         vendor_part_id=event.index,                         part=call_part,                     )                 elif isinstance(current_block, BetaWebSearchToolResultBlock):                     yield self._parts_manager.handle_part(                         vendor_part_id=event.index,                         part=_map_web_search_tool_result_block(current_block, self.provider_name),                     )                 elif isinstance(current_block, BetaCodeExecutionToolResultBlock):                     yield self._parts_manager.handle_part(                         vendor_part_id=event.index,                         part=_map_code_execution_tool_result_block(current_block, self.provider_name),                     )                 elif isinstance(current_block, BetaMCPToolUseBlock):                     call_part = _map_mcp_server_use_block(current_block, self.provider_name)                     builtin_tool_calls[call_part.tool_call_id] = call_part                      args_json = call_part.args_as_json_str()                     # Drop the final `{}}` so that we can add tool args deltas                     args_json_delta = args_json[:-3]                     assert args_json_delta.endswith('\"tool_args\":'), (                         f'Expected {args_json_delta!r} to end in `\"tool_args\":`'                     )                      yield self._parts_manager.handle_part(                         vendor_part_id=event.index, part=replace(call_part, args=None)                     )                     maybe_event = self._parts_manager.handle_tool_call_delta(                         vendor_part_id=event.index,                         args=args_json_delta,                     )                     if maybe_event is not None:  # pragma: no branch                         yield maybe_event                 elif isinstance(current_block, BetaMCPToolResultBlock):                     call_part = builtin_tool_calls.get(current_block.tool_use_id)                     yield self._parts_manager.handle_part(                         vendor_part_id=event.index,                         part=_map_mcp_server_result_block(current_block, call_part, self.provider_name),                     )              elif isinstance(event, BetaRawContentBlockDeltaEvent):                 if isinstance(event.delta, BetaTextDelta):                     maybe_event = self._parts_manager.handle_text_delta(                         vendor_part_id=event.index, content=event.delta.text                     )                     if maybe_event is not None:  # pragma: no branch                         yield maybe_event                 elif isinstance(event.delta, BetaThinkingDelta):                     yield self._parts_manager.handle_thinking_delta(                         vendor_part_id=event.index,                         content=event.delta.thinking,                         provider_name=self.provider_name,                     )                 elif isinstance(event.delta, BetaSignatureDelta):                     yield self._parts_manager.handle_thinking_delta(                         vendor_part_id=event.index,                         signature=event.delta.signature,                         provider_name=self.provider_name,                     )                 elif isinstance(event.delta, BetaInputJSONDelta):                     maybe_event = self._parts_manager.handle_tool_call_delta(                         vendor_part_id=event.index,                         args=event.delta.partial_json,                     )                     if maybe_event is not None:  # pragma: no branch                         yield maybe_event                 # TODO(Marcelo): We need to handle citations.                 elif isinstance(event.delta, BetaCitationsDelta):                     pass              elif isinstance(event, BetaRawMessageDeltaEvent):                 self._usage = _map_usage(event, self._provider_name, self._provider_url, self._model_name, self._usage)                 if raw_finish_reason := event.delta.stop_reason:  # pragma: no branch                     self.provider_details = {'finish_reason': raw_finish_reason}                     self.finish_reason = _FINISH_REASON_MAP.get(raw_finish_reason)              elif isinstance(event, BetaRawContentBlockStopEvent):  # pragma: no branch                 if isinstance(current_block, BetaMCPToolUseBlock):                     maybe_event = self._parts_manager.handle_tool_call_delta(                         vendor_part_id=event.index,                         args='}',                     )                     if maybe_event is not None:  # pragma: no branch                         yield maybe_event                 current_block = None             elif isinstance(event, BetaRawMessageStopEvent):  # pragma: no branch                 current_block = None      @property     def model_name(self) -> AnthropicModelName:         \"\"\"Get the model name of the response.\"\"\"         return self._model_name      @property     def provider_name(self) -> str:         \"\"\"Get the provider name.\"\"\"         return self._provider_name      @property     def timestamp(self) -> datetime:         \"\"\"Get the timestamp of the response.\"\"\"         return self._timestamp ``` |", "url": "https://ai.pydantic.dev/models/anthropic/index.html#anthropicstreamedresponse-dataclass", "page": "models/anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "AnthropicStreamedResponse `dataclass`", "anchor": "anthropicstreamedresponse-dataclass", "md_text": "#### model\\_name `property`\n\n```\nmodel_name: AnthropicModelName\n```\n\nGet the model name of the response.\n\n#### provider\\_name `property`\n\n```\nprovider_name: str\n```\n\nGet the provider name.\n\n#### timestamp `property`\n\n```\ntimestamp: datetime\n```\n\nGet the timestamp of the response.", "url": "https://ai.pydantic.dev/models/anthropic/index.html#anthropicstreamedresponse-dataclass", "page": "models/anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "instrument\\_model", "anchor": "instrumentmodel", "md_text": "```\ninstrument_model(\n    model: Model, instrument: InstrumentationSettings | bool\n) -> Model\n```\n\nInstrument a model with OpenTelemetry/logfire.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/instrumented.py`\n\n|  |  |\n| --- | --- |\n| ``` 66 67 68 69 70 71 72 73 74 ``` | ``` def instrument_model(model: Model, instrument: InstrumentationSettings | bool) -> Model:     \"\"\"Instrument a model with OpenTelemetry/logfire.\"\"\"     if instrument and not isinstance(model, InstrumentedModel):         if instrument is True:             instrument = InstrumentationSettings()          model = InstrumentedModel(model, instrument)      return model ``` |", "url": "https://ai.pydantic.dev/models/instrumented/index.html#instrumentmodel", "page": "models/instrumented/index.html", "source_site": "pydantic_ai"}
{"title": "InstrumentationSettings `dataclass`", "anchor": "instrumentationsettings-dataclass", "md_text": "Options for instrumenting models and agents with OpenTelemetry.\n\nUsed in:\n\n* `Agent(instrument=...)`\n* [`Agent.instrument_all()`](../../agent/index.html#pydantic_ai.agent.Agent.instrument_all)\n* [`InstrumentedModel`](index.html#pydantic_ai.models.instrumented.InstrumentedModel)\n\nSee the [Debugging and Monitoring guide](https://ai.pydantic.dev/logfire/) for more info.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/instrumented.py`", "url": "https://ai.pydantic.dev/models/instrumented/index.html#instrumentationsettings-dataclass", "page": "models/instrumented/index.html", "source_site": "pydantic_ai"}
{"title": "InstrumentationSettings `dataclass`", "anchor": "instrumentationsettings-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ```  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 ``` | ``` @dataclass(init=False) class InstrumentationSettings:     \"\"\"Options for instrumenting models and agents with OpenTelemetry.      Used in:      - `Agent(instrument=...)`     - [`Agent.instrument_all()`][pydantic_ai.agent.Agent.instrument_all]     - [`InstrumentedModel`][pydantic_ai.models.instrumented.InstrumentedModel]      See the [Debugging and Monitoring guide](https://ai.pydantic.dev/logfire/) for more info.     \"\"\"      tracer: Tracer = field(repr=False)     event_logger: EventLogger = field(repr=False)     event_mode: Literal['attributes', 'logs'] = 'attributes'     include_binary_content: bool = True     include_content: bool = True     version: Literal[1, 2, 3] = DEFAULT_INSTRUMENTATION_VERSION      def __init__(         self,         *,         tracer_provider: TracerProvider | None = None,         meter_provider: MeterProvider | None = None,         include_binary_content: bool = True,         include_content: bool = True,         version: Literal[1, 2, 3] = DEFAULT_INSTRUMENTATION_VERSION,         event_mode: Literal['attributes', 'logs'] = 'attributes',         event_logger_provider: EventLoggerProvider | None = None,     ):         \"\"\"Create instrumentation options.          Args:             tracer_provider: The OpenTelemetry tracer provider to use.                 If not provided, the global tracer provider is used.                 Calling `logfire.configure()` sets the global tracer provider, so most users don't need this.             meter_provider: The OpenTelemetry meter provider to use.                 If not provided, the global meter provider is used.                 Calling `logfire.configure()` sets the global meter provider, so most users don't need this.             include_binary_content: Whether to include binary content in the instrumentation events.             include_content: Whether to include prompts, completions, and tool call arguments and responses                 in the instrumentation events.             version: Version of the data format. This is unrelated to the Pydantic AI package version.                 Version 1 is based on the legacy event-based OpenTelemetry GenAI spec                     and will be removed in a future release.                     The parameters `event_mode` and `event_logger_provider` are only relevant for version 1.                 Version 2 uses the newer OpenTelemetry GenAI spec and stores messages in the following attributes:                     - `gen_ai.system_instructions` for instructions passed to the agent.                     - `gen_ai.input.messages` and `gen_ai.output.messages` on model request spans.                     - `pydantic_ai.all_messages` on agent run spans.             event_mode: The mode for emitting events in version 1.                 If `'attributes'`, events are attached to the span as attributes.                 If `'logs'`, events are emitted as OpenTelemetry log-based events.             event_logger_provider: The OpenTelemetry event logger provider to use.                 If not provided, the global event logger provider is used.                 Calling `logfire.configure()` sets the global event logger provider, so most users don't need this.                 This is only used if `event_mode='logs'` and `version=1`.         \"\"\"         from pydantic_ai import __version__          tracer_provider = tracer_provider or get_tracer_provider()         meter_provider = meter_provider or get_meter_provider()         event_logger_provider = event_logger_provider or get_event_logger_provider()         scope_name = 'pydantic-ai'         self.tracer = tracer_provider.get_tracer(scope_name, __version__)         self.meter = meter_provider.get_meter(scope_name, __version__)         self.event_logger = event_logger_provider.get_event_logger(scope_name, __version__)         self.event_mode = event_mode         self.include_binary_content = include_binary_content         self.include_content = include_content          if event_mode == 'logs' and version != 1:             warnings.warn(                 'event_mode is only relevant for version=1 which is deprecated and will be removed in a future release.',                 stacklevel=2,             )             version = 1          self.version = version          # As specified in the OpenTelemetry GenAI metrics spec:         # https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-metrics/#metric-gen_aiclienttokenusage         tokens_histogram_kwargs = dict(             name='gen_ai.client.token.usage',             unit='{token}',             description='Measures number of input and output tokens used',         )         try:             self.tokens_histogram = self.meter.create_histogram(                 **tokens_histogram_kwargs,                 explicit_bucket_boundaries_advisory=TOKEN_HISTOGRAM_BOUNDARIES,             )         except TypeError:  # pragma: lax no cover             # Older OTel/logfire versions don't support explicit_bucket_boundaries_advisory             self.tokens_histogram = self.meter.create_histogram(                 **tokens_histogram_kwargs,  # pyright: ignore             )         self.cost_histogram = self.meter.create_histogram(             'operation.cost',             unit='{USD}',             description='Monetary cost',         )      def messages_to_otel_events(self, messages: list[ModelMessage]) -> list[Event]:         \"\"\"Convert a list of model messages to OpenTelemetry events.          Args:             messages: The messages to convert.          Returns:             A list of OpenTelemetry events.         \"\"\"         events: list[Event] = []         instructions = InstrumentedModel._get_instructions(messages)  # pyright: ignore [reportPrivateUsage]         if instructions is not None:             events.append(                 Event(                     'gen_ai.system.message',                     body={**({'content': instructions} if self.include_content else {}), 'role': 'system'},                 )             )          for message_index, message in enumerate(messages):             message_events: list[Event] = []             if isinstance(message, ModelRequest):                 for part in message.parts:                     if hasattr(part, 'otel_event'):                         message_events.append(part.otel_event(self))             elif isinstance(message, ModelResponse):  # pragma: no branch                 message_events = message.otel_events(self)             for event in message_events:                 event.attributes = {                     'gen_ai.message.index': message_index,                     **(event.attributes or {}),                 }             events.extend(message_events)          for event in events:             event.body = InstrumentedModel.serialize_any(event.body)         return events      def messages_to_otel_messages(self, messages: list[ModelMessage]) -> list[_otel_messages.ChatMessage]:         result: list[_otel_messages.ChatMessage] = []         for message in messages:             if isinstance(message, ModelRequest):                 for is_system, group in itertools.groupby(message.parts, key=lambda p: isinstance(p, SystemPromptPart)):                     message_parts: list[_otel_messages.MessagePart] = []                     for part in group:                         if hasattr(part, 'otel_message_parts'):                             message_parts.extend(part.otel_message_parts(self))                     result.append(                         _otel_messages.ChatMessage(role='system' if is_system else 'user', parts=message_parts)                     )             elif isinstance(message, ModelResponse):  # pragma: no branch                 otel_message = _otel_messages.OutputMessage(role='assistant', parts=message.otel_message_parts(self))                 if message.finish_reason is not None:                     otel_message['finish_reason'] = message.finish_reason                 result.append(otel_message)         return result      def handle_messages(self, input_messages: list[ModelMessage], response: ModelResponse, system: str, span: Span):         if self.version == 1:             events = self.messages_to_otel_events(input_messages)             for event in self.messages_to_otel_events([response]):                 events.append(                     Event(                         'gen_ai.choice',                         body={                             'index': 0,                             'message': event.body,                         },                     )                 )             for event in events:                 event.attributes = {                     GEN_AI_SYSTEM_ATTRIBUTE: system,                     **(event.attributes or {}),                 }             self._emit_events(span, events)         else:             output_messages = self.messages_to_otel_messages([response])             assert len(output_messages) == 1             output_message = output_messages[0]             instructions = InstrumentedModel._get_instructions(input_messages)  # pyright: ignore [reportPrivateUsage]             system_instructions_attributes = self.system_instructions_attributes(instructions)             attributes: dict[str, AttributeValue] = {                 'gen_ai.input.messages': json.dumps(self.messages_to_otel_messages(input_messages)),                 'gen_ai.output.messages': json.dumps([output_message]),                 **system_instructions_attributes,                 'logfire.json_schema': json.dumps(                     {                         'type': 'object',                         'properties': {                             'gen_ai.input.messages': {'type': 'array'},                             'gen_ai.output.messages': {'type': 'array'},                             **(                                 {'gen_ai.system_instructions': {'type': 'array'}}                                 if system_instructions_attributes                                 else {}                             ),                             'model_request_parameters': {'type': 'object'},                         },                     }                 ),             }             span.set_attributes(attributes)      def system_instructions_attributes(self, instructions: str | None) -> dict[str, str]:         if instructions and self.include_content:             return {                 'gen_ai.system_instructions': json.dumps([_otel_messages.TextPart(type='text', content=instructions)]),             }         return {}      def _emit_events(self, span: Span, events: list[Event]) -> None:         if self.event_mode == 'logs':             for event in events:                 self.event_logger.emit(event)         else:             attr_name = 'events'             span.set_attributes(                 {                     attr_name: json.dumps([InstrumentedModel.event_to_dict(event) for event in events]),                     'logfire.json_schema': json.dumps(                         {                             'type': 'object',                             'properties': {                                 attr_name: {'type': 'array'},                                 'model_request_parameters': {'type': 'object'},                             },                         }                     ),                 }             )      def record_metrics(         self,         response: ModelResponse,         price_calculation: PriceCalculation | None,         attributes: dict[str, AttributeValue],     ):         for typ in ['input', 'output']:             if not (tokens := getattr(response.usage, f'{typ}_tokens', 0)):  # pragma: no cover                 continue             token_attributes = {**attributes, 'gen_ai.token.type': typ}             self.tokens_histogram.record(tokens, token_attributes)             if price_calculation:                 cost = float(getattr(price_calculation, f'{typ}_price'))                 self.cost_histogram.record(cost, token_attributes) ``` |", "url": "https://ai.pydantic.dev/models/instrumented/index.html#instrumentationsettings-dataclass", "page": "models/instrumented/index.html", "source_site": "pydantic_ai"}
{"title": "InstrumentationSettings `dataclass`", "anchor": "instrumentationsettings-dataclass", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    *,\n    tracer_provider: TracerProvider | None = None,\n    meter_provider: MeterProvider | None = None,\n    include_binary_content: bool = True,\n    include_content: bool = True,\n    version: Literal[\n        1, 2, 3\n    ] = DEFAULT_INSTRUMENTATION_VERSION,\n    event_mode: Literal[\n        \"attributes\", \"logs\"\n    ] = \"attributes\",\n    event_logger_provider: EventLoggerProvider | None = None\n)\n```\n\nCreate instrumentation options.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `tracer_provider` | `TracerProvider | None` | The OpenTelemetry tracer provider to use. If not provided, the global tracer provider is used. Calling `logfire.configure()` sets the global tracer provider, so most users don't need this. | `None` |\n| `meter_provider` | `MeterProvider | None` | The OpenTelemetry meter provider to use. If not provided, the global meter provider is used. Calling `logfire.configure()` sets the global meter provider, so most users don't need this. | `None` |\n| `include_binary_content` | `bool` | Whether to include binary content in the instrumentation events. | `True` |\n| `include_content` | `bool` | Whether to include prompts, completions, and tool call arguments and responses in the instrumentation events. | `True` |\n| `version` | `Literal[1, 2, 3]` | Version of the data format. This is unrelated to the Pydantic AI package version. Version 1 is based on the legacy event-based OpenTelemetry GenAI spec and will be removed in a future release. The parameters `event_mode` and `event_logger_provider` are only relevant for version 1. Version 2 uses the newer OpenTelemetry GenAI spec and stores messages in the following attributes: - `gen_ai.system_instructions` for instructions passed to the agent. - `gen_ai.input.messages` and `gen_ai.output.messages` on model request spans. - `pydantic_ai.all_messages` on agent run spans. | `DEFAULT_INSTRUMENTATION_VERSION` |\n| `event_mode` | `Literal['attributes', 'logs']` | The mode for emitting events in version 1. If `'attributes'`, events are attached to the span as attributes. If `'logs'`, events are emitted as OpenTelemetry log-based events. | `'attributes'` |\n| `event_logger_provider` | `EventLoggerProvider | None` | The OpenTelemetry event logger provider to use. If not provided, the global event logger provider is used. Calling `logfire.configure()` sets the global event logger provider, so most users don't need this. This is only used if `event_mode='logs'` and `version=1`. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/instrumented.py`", "url": "https://ai.pydantic.dev/models/instrumented/index.html#instrumentationsettings-dataclass", "page": "models/instrumented/index.html", "source_site": "pydantic_ai"}
{"title": "InstrumentationSettings `dataclass`", "anchor": "instrumentationsettings-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ```  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 ``` | ``` def __init__(     self,     *,     tracer_provider: TracerProvider | None = None,     meter_provider: MeterProvider | None = None,     include_binary_content: bool = True,     include_content: bool = True,     version: Literal[1, 2, 3] = DEFAULT_INSTRUMENTATION_VERSION,     event_mode: Literal['attributes', 'logs'] = 'attributes',     event_logger_provider: EventLoggerProvider | None = None, ):     \"\"\"Create instrumentation options.      Args:         tracer_provider: The OpenTelemetry tracer provider to use.             If not provided, the global tracer provider is used.             Calling `logfire.configure()` sets the global tracer provider, so most users don't need this.         meter_provider: The OpenTelemetry meter provider to use.             If not provided, the global meter provider is used.             Calling `logfire.configure()` sets the global meter provider, so most users don't need this.         include_binary_content: Whether to include binary content in the instrumentation events.         include_content: Whether to include prompts, completions, and tool call arguments and responses             in the instrumentation events.         version: Version of the data format. This is unrelated to the Pydantic AI package version.             Version 1 is based on the legacy event-based OpenTelemetry GenAI spec                 and will be removed in a future release.                 The parameters `event_mode` and `event_logger_provider` are only relevant for version 1.             Version 2 uses the newer OpenTelemetry GenAI spec and stores messages in the following attributes:                 - `gen_ai.system_instructions` for instructions passed to the agent.                 - `gen_ai.input.messages` and `gen_ai.output.messages` on model request spans.                 - `pydantic_ai.all_messages` on agent run spans.         event_mode: The mode for emitting events in version 1.             If `'attributes'`, events are attached to the span as attributes.             If `'logs'`, events are emitted as OpenTelemetry log-based events.         event_logger_provider: The OpenTelemetry event logger provider to use.             If not provided, the global event logger provider is used.             Calling `logfire.configure()` sets the global event logger provider, so most users don't need this.             This is only used if `event_mode='logs'` and `version=1`.     \"\"\"     from pydantic_ai import __version__      tracer_provider = tracer_provider or get_tracer_provider()     meter_provider = meter_provider or get_meter_provider()     event_logger_provider = event_logger_provider or get_event_logger_provider()     scope_name = 'pydantic-ai'     self.tracer = tracer_provider.get_tracer(scope_name, __version__)     self.meter = meter_provider.get_meter(scope_name, __version__)     self.event_logger = event_logger_provider.get_event_logger(scope_name, __version__)     self.event_mode = event_mode     self.include_binary_content = include_binary_content     self.include_content = include_content      if event_mode == 'logs' and version != 1:         warnings.warn(             'event_mode is only relevant for version=1 which is deprecated and will be removed in a future release.',             stacklevel=2,         )         version = 1      self.version = version      # As specified in the OpenTelemetry GenAI metrics spec:     # https://opentelemetry.io/docs/specs/semconv/gen-ai/gen-ai-metrics/#metric-gen_aiclienttokenusage     tokens_histogram_kwargs = dict(         name='gen_ai.client.token.usage',         unit='{token}',         description='Measures number of input and output tokens used',     )     try:         self.tokens_histogram = self.meter.create_histogram(             **tokens_histogram_kwargs,             explicit_bucket_boundaries_advisory=TOKEN_HISTOGRAM_BOUNDARIES,         )     except TypeError:  # pragma: lax no cover         # Older OTel/logfire versions don't support explicit_bucket_boundaries_advisory         self.tokens_histogram = self.meter.create_histogram(             **tokens_histogram_kwargs,  # pyright: ignore         )     self.cost_histogram = self.meter.create_histogram(         'operation.cost',         unit='{USD}',         description='Monetary cost',     ) ``` |\n\n#### messages\\_to\\_otel\\_events\n\n```\nmessages_to_otel_events(\n    messages: list[ModelMessage],\n) -> list[Event]\n```\n\nConvert a list of model messages to OpenTelemetry events.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `messages` | `list[ModelMessage]` | The messages to convert. | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `list[Event]` | A list of OpenTelemetry events. |", "url": "https://ai.pydantic.dev/models/instrumented/index.html#instrumentationsettings-dataclass", "page": "models/instrumented/index.html", "source_site": "pydantic_ai"}
{"title": "InstrumentationSettings `dataclass`", "anchor": "instrumentationsettings-dataclass", "md_text": "Source code in `pydantic_ai_slim/pydantic_ai/models/instrumented.py`\n\n|  |  |\n| --- | --- |\n| ``` 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 ``` | ``` def messages_to_otel_events(self, messages: list[ModelMessage]) -> list[Event]:     \"\"\"Convert a list of model messages to OpenTelemetry events.      Args:         messages: The messages to convert.      Returns:         A list of OpenTelemetry events.     \"\"\"     events: list[Event] = []     instructions = InstrumentedModel._get_instructions(messages)  # pyright: ignore [reportPrivateUsage]     if instructions is not None:         events.append(             Event(                 'gen_ai.system.message',                 body={**({'content': instructions} if self.include_content else {}), 'role': 'system'},             )         )      for message_index, message in enumerate(messages):         message_events: list[Event] = []         if isinstance(message, ModelRequest):             for part in message.parts:                 if hasattr(part, 'otel_event'):                     message_events.append(part.otel_event(self))         elif isinstance(message, ModelResponse):  # pragma: no branch             message_events = message.otel_events(self)         for event in message_events:             event.attributes = {                 'gen_ai.message.index': message_index,                 **(event.attributes or {}),             }         events.extend(message_events)      for event in events:         event.body = InstrumentedModel.serialize_any(event.body)     return events ``` |", "url": "https://ai.pydantic.dev/models/instrumented/index.html#instrumentationsettings-dataclass", "page": "models/instrumented/index.html", "source_site": "pydantic_ai"}
{"title": "InstrumentedModel `dataclass`", "anchor": "instrumentedmodel-dataclass", "md_text": "Bases: `WrapperModel`\n\nModel which wraps another model so that requests are instrumented with OpenTelemetry.\n\nSee the [Debugging and Monitoring guide](https://ai.pydantic.dev/logfire/) for more info.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/instrumented.py`", "url": "https://ai.pydantic.dev/models/instrumented/index.html#instrumentedmodel-dataclass", "page": "models/instrumented/index.html", "source_site": "pydantic_ai"}
{"title": "InstrumentedModel `dataclass`", "anchor": "instrumentedmodel-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 ``` | ``` @dataclass(init=False) class InstrumentedModel(WrapperModel):     \"\"\"Model which wraps another model so that requests are instrumented with OpenTelemetry.      See the [Debugging and Monitoring guide](https://ai.pydantic.dev/logfire/) for more info.     \"\"\"      instrumentation_settings: InstrumentationSettings     \"\"\"Instrumentation settings for this model.\"\"\"      def __init__(         self,         wrapped: Model | KnownModelName,         options: InstrumentationSettings | None = None,     ) -> None:         super().__init__(wrapped)         self.instrumentation_settings = options or InstrumentationSettings()      async def request(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> ModelResponse:         prepared_settings, prepared_parameters = self.wrapped.prepare_request(             model_settings,             model_request_parameters,         )         with self._instrument(messages, prepared_settings, prepared_parameters) as finish:             response = await self.wrapped.request(messages, model_settings, model_request_parameters)             finish(response)             return response      @asynccontextmanager     async def request_stream(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,         run_context: RunContext[Any] | None = None,     ) -> AsyncIterator[StreamedResponse]:         prepared_settings, prepared_parameters = self.wrapped.prepare_request(             model_settings,             model_request_parameters,         )         with self._instrument(messages, prepared_settings, prepared_parameters) as finish:             response_stream: StreamedResponse | None = None             try:                 async with self.wrapped.request_stream(                     messages, model_settings, model_request_parameters, run_context                 ) as response_stream:                     yield response_stream             finally:                 if response_stream:  # pragma: no branch                     finish(response_stream.get())      @contextmanager     def _instrument(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> Iterator[Callable[[ModelResponse], None]]:         operation = 'chat'         span_name = f'{operation} {self.model_name}'         # TODO Missing attributes:         #  - error.type: unclear if we should do something here or just always rely on span exceptions         #  - gen_ai.request.stop_sequences/top_k: model_settings doesn't include these         attributes: dict[str, AttributeValue] = {             'gen_ai.operation.name': operation,             **self.model_attributes(self.wrapped),             'model_request_parameters': json.dumps(InstrumentedModel.serialize_any(model_request_parameters)),             'logfire.json_schema': json.dumps(                 {                     'type': 'object',                     'properties': {'model_request_parameters': {'type': 'object'}},                 }             ),         }          if model_settings:             for key in MODEL_SETTING_ATTRIBUTES:                 if isinstance(value := model_settings.get(key), float | int):                     attributes[f'gen_ai.request.{key}'] = value          record_metrics: Callable[[], None] | None = None         try:             with self.instrumentation_settings.tracer.start_as_current_span(span_name, attributes=attributes) as span:                  def finish(response: ModelResponse):                     # FallbackModel updates these span attributes.                     attributes.update(getattr(span, 'attributes', {}))                     request_model = attributes[GEN_AI_REQUEST_MODEL_ATTRIBUTE]                     system = cast(str, attributes[GEN_AI_SYSTEM_ATTRIBUTE])                      response_model = response.model_name or request_model                     price_calculation = None                      def _record_metrics():                         metric_attributes = {                             GEN_AI_SYSTEM_ATTRIBUTE: system,                             'gen_ai.operation.name': operation,                             'gen_ai.request.model': request_model,                             'gen_ai.response.model': response_model,                         }                         self.instrumentation_settings.record_metrics(response, price_calculation, metric_attributes)                      nonlocal record_metrics                     record_metrics = _record_metrics                      if not span.is_recording():                         return                      self.instrumentation_settings.handle_messages(messages, response, system, span)                      attributes_to_set = {                         **response.usage.opentelemetry_attributes(),                         'gen_ai.response.model': response_model,                     }                     try:                         price_calculation = response.cost()                     except LookupError:                         # The cost of this provider/model is unknown, which is common.                         pass                     except Exception as e:                         warnings.warn(                             f'Failed to get cost from response: {type(e).__name__}: {e}', CostCalculationFailedWarning                         )                     else:                         attributes_to_set['operation.cost'] = float(price_calculation.total_price)                      if response.provider_response_id is not None:                         attributes_to_set['gen_ai.response.id'] = response.provider_response_id                     if response.finish_reason is not None:                         attributes_to_set['gen_ai.response.finish_reasons'] = [response.finish_reason]                     span.set_attributes(attributes_to_set)                     span.update_name(f'{operation} {request_model}')                  yield finish         finally:             if record_metrics:                 # We only want to record metrics after the span is finished,                 # to prevent them from being redundantly recorded in the span itself by logfire.                 record_metrics()      @staticmethod     def model_attributes(model: Model):         attributes: dict[str, AttributeValue] = {             GEN_AI_SYSTEM_ATTRIBUTE: model.system,             GEN_AI_REQUEST_MODEL_ATTRIBUTE: model.model_name,         }         if base_url := model.base_url:             try:                 parsed = urlparse(base_url)             except Exception:  # pragma: no cover                 pass             else:                 if parsed.hostname:  # pragma: no branch                     attributes['server.address'] = parsed.hostname                 if parsed.port:  # pragma: no branch                     attributes['server.port'] = parsed.port          return attributes      @staticmethod     def event_to_dict(event: Event) -> dict[str, Any]:         if not event.body:             body = {}  # pragma: no cover         elif isinstance(event.body, Mapping):             body = event.body  # type: ignore         else:             body = {'body': event.body}         return {**body, **(event.attributes or {})}      @staticmethod     def serialize_any(value: Any) -> str:         try:             return ANY_ADAPTER.dump_python(value, mode='json')         except Exception:             try:                 return str(value)             except Exception as e:                 return f'Unable to serialize: {e}' ``` |", "url": "https://ai.pydantic.dev/models/instrumented/index.html#instrumentedmodel-dataclass", "page": "models/instrumented/index.html", "source_site": "pydantic_ai"}
{"title": "InstrumentedModel `dataclass`", "anchor": "instrumentedmodel-dataclass", "md_text": "#### instrumentation\\_settings `instance-attribute`\n\n```\ninstrumentation_settings: InstrumentationSettings = (\n    options or InstrumentationSettings()\n)\n```\n\nInstrumentation settings for this model.", "url": "https://ai.pydantic.dev/models/instrumented/index.html#instrumentedmodel-dataclass", "page": "models/instrumented/index.html", "source_site": "pydantic_ai"}
{"title": "Setup", "anchor": "setup", "md_text": "For details on how to set up authentication with this model, see [model configuration for Groq](../../groq/index.html).", "url": "https://ai.pydantic.dev/models/groq/index.html#setup", "page": "models/groq/index.html", "source_site": "pydantic_ai"}
{"title": "ProductionGroqModelNames `module-attribute`", "anchor": "productiongroqmodelnames-module-attribute", "md_text": "```\nProductionGroqModelNames = Literal[\n    \"distil-whisper-large-v3-en\",\n    \"gemma2-9b-it\",\n    \"llama-3.3-70b-versatile\",\n    \"llama-3.1-8b-instant\",\n    \"llama-guard-3-8b\",\n    \"llama3-70b-8192\",\n    \"llama3-8b-8192\",\n    \"whisper-large-v3\",\n    \"whisper-large-v3-turbo\",\n]\n```\n\nProduction Groq models from <https://console.groq.com/docs/models#production-models>.", "url": "https://ai.pydantic.dev/models/groq/index.html#productiongroqmodelnames-module-attribute", "page": "models/groq/index.html", "source_site": "pydantic_ai"}
{"title": "PreviewGroqModelNames `module-attribute`", "anchor": "previewgroqmodelnames-module-attribute", "md_text": "```\nPreviewGroqModelNames = Literal[\n    \"playai-tts\",\n    \"playai-tts-arabic\",\n    \"qwen-qwq-32b\",\n    \"mistral-saba-24b\",\n    \"qwen-2.5-coder-32b\",\n    \"qwen-2.5-32b\",\n    \"deepseek-r1-distill-qwen-32b\",\n    \"deepseek-r1-distill-llama-70b\",\n    \"llama-3.3-70b-specdec\",\n    \"llama-3.2-1b-preview\",\n    \"llama-3.2-3b-preview\",\n    \"llama-3.2-11b-vision-preview\",\n    \"llama-3.2-90b-vision-preview\",\n    \"moonshotai/kimi-k2-instruct\",\n]\n```\n\nPreview Groq models from <https://console.groq.com/docs/models#preview-models>.", "url": "https://ai.pydantic.dev/models/groq/index.html#previewgroqmodelnames-module-attribute", "page": "models/groq/index.html", "source_site": "pydantic_ai"}
{"title": "GroqModelName `module-attribute`", "anchor": "groqmodelname-module-attribute", "md_text": "```\nGroqModelName = (\n    str | ProductionGroqModelNames | PreviewGroqModelNames\n)\n```\n\nPossible Groq model names.\n\nSince Groq supports a variety of models and the list changes frequencly, we explicitly list the named models as of 2025-03-31\nbut allow any name in the type hints.\n\nSee <https://console.groq.com/docs/models> for an up to date date list of models and more details.", "url": "https://ai.pydantic.dev/models/groq/index.html#groqmodelname-module-attribute", "page": "models/groq/index.html", "source_site": "pydantic_ai"}
{"title": "GroqModelSettings", "anchor": "groqmodelsettings", "md_text": "Bases: `ModelSettings`\n\nSettings used for a Groq model request.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/groq.py`\n\n|  |  |\n| --- | --- |\n| ``` 114 115 116 117 118 119 120 121 122 123 ``` | ``` class GroqModelSettings(ModelSettings, total=False):     \"\"\"Settings used for a Groq model request.\"\"\"      # ALL FIELDS MUST BE `groq_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.      groq_reasoning_format: Literal['hidden', 'raw', 'parsed']     \"\"\"The format of the reasoning output.      See [the Groq docs](https://console.groq.com/docs/reasoning#reasoning-format) for more details.     \"\"\" ``` |\n\n#### groq\\_reasoning\\_format `instance-attribute`\n\n```\ngroq_reasoning_format: Literal['hidden', 'raw', 'parsed']\n```\n\nThe format of the reasoning output.\n\nSee [the Groq docs](https://console.groq.com/docs/reasoning#reasoning-format) for more details.", "url": "https://ai.pydantic.dev/models/groq/index.html#groqmodelsettings", "page": "models/groq/index.html", "source_site": "pydantic_ai"}
{"title": "GroqModel `dataclass`", "anchor": "groqmodel-dataclass", "md_text": "Bases: `Model`\n\nA model that uses the Groq API.\n\nInternally, this uses the [Groq Python client](https://github.com/groq/groq-python) to interact with the API.\n\nApart from `__init__`, all methods are private or match those of the base class.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/groq.py`", "url": "https://ai.pydantic.dev/models/groq/index.html#groqmodel-dataclass", "page": "models/groq/index.html", "source_site": "pydantic_ai"}
{"title": "GroqModel `dataclass`", "anchor": "groqmodel-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 ``` | ``` @dataclass(init=False) class GroqModel(Model):     \"\"\"A model that uses the Groq API.      Internally, this uses the [Groq Python client](https://github.com/groq/groq-python) to interact with the API.      Apart from `__init__`, all methods are private or match those of the base class.     \"\"\"      client: AsyncGroq = field(repr=False)      _model_name: GroqModelName = field(repr=False)     _provider: Provider[AsyncGroq] = field(repr=False)      def __init__(         self,         model_name: GroqModelName,         *,         provider: Literal['groq', 'gateway'] | Provider[AsyncGroq] = 'groq',         profile: ModelProfileSpec | None = None,         settings: ModelSettings | None = None,     ):         \"\"\"Initialize a Groq model.          Args:             model_name: The name of the Groq model to use. List of model names available                 [here](https://console.groq.com/docs/models).             provider: The provider to use for authentication and API access. Can be either the string                 'groq' or an instance of `Provider[AsyncGroq]`. If not provided, a new provider will be                 created using the other parameters.             profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.             settings: Model-specific settings that will be used as defaults for this model.         \"\"\"         self._model_name = model_name          if isinstance(provider, str):             provider = infer_provider('gateway/groq' if provider == 'gateway' else provider)         self._provider = provider         self.client = provider.client          super().__init__(settings=settings, profile=profile or provider.model_profile)      @property     def base_url(self) -> str:         return str(self.client.base_url)      @property     def model_name(self) -> GroqModelName:         \"\"\"The model name.\"\"\"         return self._model_name      @property     def system(self) -> str:         \"\"\"The model provider.\"\"\"         return self._provider.name      async def request(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> ModelResponse:         check_allow_model_requests()         model_settings, model_request_parameters = self.prepare_request(             model_settings,             model_request_parameters,         )         try:             response = await self._completions_create(                 messages, False, cast(GroqModelSettings, model_settings or {}), model_request_parameters             )         except ModelHTTPError as e:             if isinstance(e.body, dict):  # pragma: no branch                 # The Groq SDK tries to be helpful by raising an exception when generated tool arguments don't match the schema,                 # but we'd rather handle it ourselves so we can tell the model to retry the tool call.                 try:                     error = _GroqToolUseFailedError.model_validate(e.body)  # pyright: ignore[reportUnknownMemberType]                     tool_call_part = ToolCallPart(                         tool_name=error.error.failed_generation.name,                         args=error.error.failed_generation.arguments,                     )                     return ModelResponse(                         parts=[tool_call_part],                         model_name=e.model_name,                         timestamp=_utils.now_utc(),                         provider_name=self._provider.name,                         finish_reason='error',                     )                 except ValidationError:                     pass             raise         model_response = self._process_response(response)         return model_response      @asynccontextmanager     async def request_stream(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,         run_context: RunContext[Any] | None = None,     ) -> AsyncIterator[StreamedResponse]:         check_allow_model_requests()         model_settings, model_request_parameters = self.prepare_request(             model_settings,             model_request_parameters,         )         response = await self._completions_create(             messages, True, cast(GroqModelSettings, model_settings or {}), model_request_parameters         )         async with response:             yield await self._process_streamed_response(response, model_request_parameters)      @overload     async def _completions_create(         self,         messages: list[ModelMessage],         stream: Literal[True],         model_settings: GroqModelSettings,         model_request_parameters: ModelRequestParameters,     ) -> AsyncStream[chat.ChatCompletionChunk]:         pass      @overload     async def _completions_create(         self,         messages: list[ModelMessage],         stream: Literal[False],         model_settings: GroqModelSettings,         model_request_parameters: ModelRequestParameters,     ) -> chat.ChatCompletion:         pass      async def _completions_create(         self,         messages: list[ModelMessage],         stream: bool,         model_settings: GroqModelSettings,         model_request_parameters: ModelRequestParameters,     ) -> chat.ChatCompletion | AsyncStream[chat.ChatCompletionChunk]:         tools = self._get_tools(model_request_parameters)         tools += self._get_builtin_tools(model_request_parameters)         if not tools:             tool_choice: Literal['none', 'required', 'auto'] | None = None         elif not model_request_parameters.allow_text_output:             tool_choice = 'required'         else:             tool_choice = 'auto'          groq_messages = self._map_messages(messages)          response_format: chat.completion_create_params.ResponseFormat | None = None         if model_request_parameters.output_mode == 'native':             output_object = model_request_parameters.output_object             assert output_object is not None             response_format = self._map_json_schema(output_object)         elif (             model_request_parameters.output_mode == 'prompted'             and not tools             and self.profile.supports_json_object_output         ):  # pragma: no branch             response_format = {'type': 'json_object'}          try:             extra_headers = model_settings.get('extra_headers', {})             extra_headers.setdefault('User-Agent', get_user_agent())             return await self.client.chat.completions.create(                 model=self._model_name,                 messages=groq_messages,                 n=1,                 parallel_tool_calls=model_settings.get('parallel_tool_calls', NOT_GIVEN),                 tools=tools or NOT_GIVEN,                 tool_choice=tool_choice or NOT_GIVEN,                 stop=model_settings.get('stop_sequences', NOT_GIVEN),                 stream=stream,                 response_format=response_format or NOT_GIVEN,                 max_tokens=model_settings.get('max_tokens', NOT_GIVEN),                 temperature=model_settings.get('temperature', NOT_GIVEN),                 top_p=model_settings.get('top_p', NOT_GIVEN),                 timeout=model_settings.get('timeout', NOT_GIVEN),                 seed=model_settings.get('seed', NOT_GIVEN),                 presence_penalty=model_settings.get('presence_penalty', NOT_GIVEN),                 reasoning_format=model_settings.get('groq_reasoning_format', NOT_GIVEN),                 frequency_penalty=model_settings.get('frequency_penalty', NOT_GIVEN),                 logit_bias=model_settings.get('logit_bias', NOT_GIVEN),                 extra_headers=extra_headers,                 extra_body=model_settings.get('extra_body'),             )         except APIStatusError as e:             if (status_code := e.status_code) >= 400:                 raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e             raise  # pragma: lax no cover      def _process_response(self, response: chat.ChatCompletion) -> ModelResponse:         \"\"\"Process a non-streamed response, and prepare a message to return.\"\"\"         timestamp = number_to_datetime(response.created)         choice = response.choices[0]         items: list[ModelResponsePart] = []         if choice.message.reasoning is not None:             # NOTE: The `reasoning` field is only present if `groq_reasoning_format` is set to `parsed`.             items.append(ThinkingPart(content=choice.message.reasoning))         if choice.message.executed_tools:             for tool in choice.message.executed_tools:                 call_part, return_part = _map_executed_tool(tool, self.system)                 if call_part and return_part:  # pragma: no branch                     items.append(call_part)                     items.append(return_part)         if choice.message.content:             # NOTE: The `<think>` tag is only present if `groq_reasoning_format` is set to `raw`.             items.extend(split_content_into_text_and_thinking(choice.message.content, self.profile.thinking_tags))         if choice.message.tool_calls is not None:             for c in choice.message.tool_calls:                 items.append(ToolCallPart(tool_name=c.function.name, args=c.function.arguments, tool_call_id=c.id))          raw_finish_reason = choice.finish_reason         provider_details = {'finish_reason': raw_finish_reason}         finish_reason = _FINISH_REASON_MAP.get(raw_finish_reason)         return ModelResponse(             parts=items,             usage=_map_usage(response),             model_name=response.model,             timestamp=timestamp,             provider_response_id=response.id,             provider_name=self._provider.name,             finish_reason=finish_reason,             provider_details=provider_details,         )      async def _process_streamed_response(         self, response: AsyncStream[chat.ChatCompletionChunk], model_request_parameters: ModelRequestParameters     ) -> GroqStreamedResponse:         \"\"\"Process a streamed response, and prepare a streaming response to return.\"\"\"         peekable_response = _utils.PeekableAsyncStream(response)         first_chunk = await peekable_response.peek()         if isinstance(first_chunk, _utils.Unset):             raise UnexpectedModelBehavior(  # pragma: no cover                 'Streamed response ended without content or tool calls'             )          return GroqStreamedResponse(             model_request_parameters=model_request_parameters,             _response=peekable_response,             _model_name=first_chunk.model,             _model_profile=self.profile,             _timestamp=number_to_datetime(first_chunk.created),             _provider_name=self._provider.name,         )      def _get_tools(self, model_request_parameters: ModelRequestParameters) -> list[chat.ChatCompletionToolParam]:         return [self._map_tool_definition(r) for r in model_request_parameters.tool_defs.values()]      def _get_builtin_tools(         self, model_request_parameters: ModelRequestParameters     ) -> list[chat.ChatCompletionToolParam]:         tools: list[chat.ChatCompletionToolParam] = []         for tool in model_request_parameters.builtin_tools:             if isinstance(tool, WebSearchTool):                 if not GroqModelProfile.from_profile(self.profile).groq_always_has_web_search_builtin_tool:                     raise UserError('`WebSearchTool` is not supported by Groq')  # pragma: no cover             else:                 raise UserError(                     f'`{tool.__class__.__name__}` is not supported by `GroqModel`. If it should be, please file an issue.'                 )         return tools      def _map_messages(self, messages: list[ModelMessage]) -> list[chat.ChatCompletionMessageParam]:         \"\"\"Just maps a `pydantic_ai.Message` to a `groq.types.ChatCompletionMessageParam`.\"\"\"         groq_messages: list[chat.ChatCompletionMessageParam] = []         for message in messages:             if isinstance(message, ModelRequest):                 groq_messages.extend(self._map_user_message(message))             elif isinstance(message, ModelResponse):                 texts: list[str] = []                 tool_calls: list[chat.ChatCompletionMessageToolCallParam] = []                 for item in message.parts:                     if isinstance(item, TextPart):                         texts.append(item.content)                     elif isinstance(item, ToolCallPart):                         tool_calls.append(self._map_tool_call(item))                     elif isinstance(item, ThinkingPart):                         start_tag, end_tag = self.profile.thinking_tags                         texts.append('\\n'.join([start_tag, item.content, end_tag]))                     elif isinstance(item, BuiltinToolCallPart | BuiltinToolReturnPart):  # pragma: no cover                         # These are not currently sent back                         pass                     elif isinstance(item, FilePart):  # pragma: no cover                         # Files generated by models are not sent back to models that don't themselves generate files.                         pass                     else:                         assert_never(item)                 message_param = chat.ChatCompletionAssistantMessageParam(role='assistant')                 if texts:                     # Note: model responses from this model should only have one text item, so the following                     # shouldn't merge multiple texts into one unless you switch models between runs:                     message_param['content'] = '\\n\\n'.join(texts)                 if tool_calls:                     message_param['tool_calls'] = tool_calls                 groq_messages.append(message_param)             else:                 assert_never(message)         if instructions := self._get_instructions(messages):             groq_messages.insert(0, chat.ChatCompletionSystemMessageParam(role='system', content=instructions))         return groq_messages      @staticmethod     def _map_tool_call(t: ToolCallPart) -> chat.ChatCompletionMessageToolCallParam:         return chat.ChatCompletionMessageToolCallParam(             id=_guard_tool_call_id(t=t),             type='function',             function={'name': t.tool_name, 'arguments': t.args_as_json_str()},         )      @staticmethod     def _map_tool_definition(f: ToolDefinition) -> chat.ChatCompletionToolParam:         return {             'type': 'function',             'function': {                 'name': f.name,                 'description': f.description or '',                 'parameters': f.parameters_json_schema,             },         }      def _map_json_schema(self, o: OutputObjectDefinition) -> chat.completion_create_params.ResponseFormat:         response_format_param: chat.completion_create_params.ResponseFormatResponseFormatJsonSchema = {             'type': 'json_schema',             'json_schema': {                 'name': o.name or DEFAULT_OUTPUT_TOOL_NAME,                 'schema': o.json_schema,                 'strict': o.strict,             },         }         if o.description:  # pragma: no branch             response_format_param['json_schema']['description'] = o.description         return response_format_param      @classmethod     def _map_user_message(cls, message: ModelRequest) -> Iterable[chat.ChatCompletionMessageParam]:         for part in message.parts:             if isinstance(part, SystemPromptPart):                 yield chat.ChatCompletionSystemMessageParam(role='system', content=part.content)             elif isinstance(part, UserPromptPart):                 yield cls._map_user_prompt(part)             elif isinstance(part, ToolReturnPart):                 yield chat.ChatCompletionToolMessageParam(                     role='tool',                     tool_call_id=_guard_tool_call_id(t=part),                     content=part.model_response_str(),                 )             elif isinstance(part, RetryPromptPart):  # pragma: no branch                 if part.tool_name is None:                     yield chat.ChatCompletionUserMessageParam(  # pragma: no cover                         role='user', content=part.model_response()                     )                 else:                     yield chat.ChatCompletionToolMessageParam(                         role='tool',                         tool_call_id=_guard_tool_call_id(t=part),                         content=part.model_response(),                     )      @staticmethod     def _map_user_prompt(part: UserPromptPart) -> chat.ChatCompletionUserMessageParam:         content: str | list[chat.ChatCompletionContentPartParam]         if isinstance(part.content, str):             content = part.content         else:             content = []             for item in part.content:                 if isinstance(item, str):                     content.append(chat.ChatCompletionContentPartTextParam(text=item, type='text'))                 elif isinstance(item, ImageUrl):                     image_url = ImageURL(url=item.url)                     content.append(chat.ChatCompletionContentPartImageParam(image_url=image_url, type='image_url'))                 elif isinstance(item, BinaryContent):                     if item.is_image:                         image_url = ImageURL(url=item.data_uri)                         content.append(chat.ChatCompletionContentPartImageParam(image_url=image_url, type='image_url'))                     else:                         raise RuntimeError('Only images are supported for binary content in Groq.')                 elif isinstance(item, DocumentUrl):  # pragma: no cover                     raise RuntimeError('DocumentUrl is not supported in Groq.')                 else:  # pragma: no cover                     raise RuntimeError(f'Unsupported content type: {type(item)}')          return chat.ChatCompletionUserMessageParam(role='user', content=content) ``` |", "url": "https://ai.pydantic.dev/models/groq/index.html#groqmodel-dataclass", "page": "models/groq/index.html", "source_site": "pydantic_ai"}
{"title": "GroqModel `dataclass`", "anchor": "groqmodel-dataclass", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    model_name: GroqModelName,\n    *,\n    provider: (\n        Literal[\"groq\", \"gateway\"] | Provider[AsyncGroq]\n    ) = \"groq\",\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n)\n```\n\nInitialize a Groq model.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model_name` | `GroqModelName` | The name of the Groq model to use. List of model names available [here](https://console.groq.com/docs/models). | *required* |\n| `provider` | `Literal['groq', 'gateway'] | Provider[AsyncGroq]` | The provider to use for authentication and API access. Can be either the string 'groq' or an instance of `Provider[AsyncGroq]`. If not provided, a new provider will be created using the other parameters. | `'groq'` |\n| `profile` | `ModelProfileSpec | None` | The model profile to use. Defaults to a profile picked by the provider based on the model name. | `None` |\n| `settings` | `ModelSettings | None` | Model-specific settings that will be used as defaults for this model. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/groq.py`\n\n|  |  |\n| --- | --- |\n| ``` 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 ``` | ``` def __init__(     self,     model_name: GroqModelName,     *,     provider: Literal['groq', 'gateway'] | Provider[AsyncGroq] = 'groq',     profile: ModelProfileSpec | None = None,     settings: ModelSettings | None = None, ):     \"\"\"Initialize a Groq model.      Args:         model_name: The name of the Groq model to use. List of model names available             [here](https://console.groq.com/docs/models).         provider: The provider to use for authentication and API access. Can be either the string             'groq' or an instance of `Provider[AsyncGroq]`. If not provided, a new provider will be             created using the other parameters.         profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.         settings: Model-specific settings that will be used as defaults for this model.     \"\"\"     self._model_name = model_name      if isinstance(provider, str):         provider = infer_provider('gateway/groq' if provider == 'gateway' else provider)     self._provider = provider     self.client = provider.client      super().__init__(settings=settings, profile=profile or provider.model_profile) ``` |\n\n#### model\\_name `property`\n\n```\nmodel_name: GroqModelName\n```\n\nThe model name.\n\n#### system `property`\n\n```\nsystem: str\n```\n\nThe model provider.", "url": "https://ai.pydantic.dev/models/groq/index.html#groqmodel-dataclass", "page": "models/groq/index.html", "source_site": "pydantic_ai"}
{"title": "GroqStreamedResponse `dataclass`", "anchor": "groqstreamedresponse-dataclass", "md_text": "Bases: `StreamedResponse`\n\nImplementation of `StreamedResponse` for Groq models.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/groq.py`\n\n|  |  |\n| --- | --- |\n| ``` 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 ``` | ``` @dataclass class GroqStreamedResponse(StreamedResponse):     \"\"\"Implementation of `StreamedResponse` for Groq models.\"\"\"      _model_name: GroqModelName     _model_profile: ModelProfile     _response: AsyncIterable[chat.ChatCompletionChunk]     _timestamp: datetime     _provider_name: str      async def _get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:  # noqa: C901         try:             executed_tool_call_id: str | None = None             async for chunk in self._response:                 self._usage += _map_usage(chunk)                  if chunk.id:  # pragma: no branch                     self.provider_response_id = chunk.id                  try:                     choice = chunk.choices[0]                 except IndexError:                     continue                  if raw_finish_reason := choice.finish_reason:                     self.provider_details = {'finish_reason': raw_finish_reason}                     self.finish_reason = _FINISH_REASON_MAP.get(raw_finish_reason)                  if choice.delta.reasoning is not None:                     # NOTE: The `reasoning` field is only present if `groq_reasoning_format` is set to `parsed`.                     yield self._parts_manager.handle_thinking_delta(                         vendor_part_id='reasoning', content=choice.delta.reasoning                     )                  if choice.delta.executed_tools:                     for tool in choice.delta.executed_tools:                         call_part, return_part = _map_executed_tool(                             tool, self.provider_name, streaming=True, tool_call_id=executed_tool_call_id                         )                         if call_part:                             executed_tool_call_id = call_part.tool_call_id                             yield self._parts_manager.handle_part(                                 vendor_part_id=f'executed_tools-{tool.index}-call', part=call_part                             )                         if return_part:                             executed_tool_call_id = None                             yield self._parts_manager.handle_part(                                 vendor_part_id=f'executed_tools-{tool.index}-return', part=return_part                             )                  # Handle the text part of the response                 content = choice.delta.content                 if content:                     maybe_event = self._parts_manager.handle_text_delta(                         vendor_part_id='content',                         content=content,                         thinking_tags=self._model_profile.thinking_tags,                         ignore_leading_whitespace=self._model_profile.ignore_streamed_leading_whitespace,                     )                     if maybe_event is not None:  # pragma: no branch                         yield maybe_event                  # Handle the tool calls                 for dtc in choice.delta.tool_calls or []:                     maybe_event = self._parts_manager.handle_tool_call_delta(                         vendor_part_id=dtc.index,                         tool_name=dtc.function and dtc.function.name,                         args=dtc.function and dtc.function.arguments,                         tool_call_id=dtc.id,                     )                     if maybe_event is not None:                         yield maybe_event         except APIError as e:             if isinstance(e.body, dict):  # pragma: no branch                 # The Groq SDK tries to be helpful by raising an exception when generated tool arguments don't match the schema,                 # but we'd rather handle it ourselves so we can tell the model to retry the tool call                 try:                     error = _GroqToolUseFailedInnerError.model_validate(e.body)  # pyright: ignore[reportUnknownMemberType]                     yield self._parts_manager.handle_tool_call_part(                         vendor_part_id='tool_use_failed',                         tool_name=error.failed_generation.name,                         args=error.failed_generation.arguments,                     )                     return                 except ValidationError as e:  # pragma: no cover                     pass             raise  # pragma: no cover      @property     def model_name(self) -> GroqModelName:         \"\"\"Get the model name of the response.\"\"\"         return self._model_name      @property     def provider_name(self) -> str:         \"\"\"Get the provider name.\"\"\"         return self._provider_name      @property     def timestamp(self) -> datetime:         \"\"\"Get the timestamp of the response.\"\"\"         return self._timestamp ``` |\n\n#### model\\_name `property`\n\n```\nmodel_name: GroqModelName\n```\n\nGet the model name of the response.\n\n#### provider\\_name `property`\n\n```\nprovider_name: str\n```\n\nGet the provider name.\n\n#### timestamp `property`\n\n```\ntimestamp: datetime\n```\n\nGet the timestamp of the response.", "url": "https://ai.pydantic.dev/models/groq/index.html#groqstreamedresponse-dataclass", "page": "models/groq/index.html", "source_site": "pydantic_ai"}
{"title": "Setup", "anchor": "setup", "md_text": "For details on how to set up authentication with this model, see [model configuration for Bedrock](../../bedrock/index.html).", "url": "https://ai.pydantic.dev/models/bedrock/index.html#setup", "page": "models/bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "LatestBedrockModelNames `module-attribute`", "anchor": "latestbedrockmodelnames-module-attribute", "md_text": "```\nLatestBedrockModelNames = Literal[\n    \"amazon.titan-tg1-large\",\n    \"amazon.titan-text-lite-v1\",\n    \"amazon.titan-text-express-v1\",\n    \"us.amazon.nova-pro-v1:0\",\n    \"us.amazon.nova-lite-v1:0\",\n    \"us.amazon.nova-micro-v1:0\",\n    \"anthropic.claude-3-5-sonnet-20241022-v2:0\",\n    \"us.anthropic.claude-3-5-sonnet-20241022-v2:0\",\n    \"anthropic.claude-3-5-haiku-20241022-v1:0\",\n    \"us.anthropic.claude-3-5-haiku-20241022-v1:0\",\n    \"anthropic.claude-instant-v1\",\n    \"anthropic.claude-v2:1\",\n    \"anthropic.claude-v2\",\n    \"anthropic.claude-3-sonnet-20240229-v1:0\",\n    \"us.anthropic.claude-3-sonnet-20240229-v1:0\",\n    \"anthropic.claude-3-haiku-20240307-v1:0\",\n    \"us.anthropic.claude-3-haiku-20240307-v1:0\",\n    \"anthropic.claude-3-opus-20240229-v1:0\",\n    \"us.anthropic.claude-3-opus-20240229-v1:0\",\n    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    \"us.anthropic.claude-3-5-sonnet-20240620-v1:0\",\n    \"anthropic.claude-3-7-sonnet-20250219-v1:0\",\n    \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\",\n    \"anthropic.claude-opus-4-20250514-v1:0\",\n    \"us.anthropic.claude-opus-4-20250514-v1:0\",\n    \"anthropic.claude-sonnet-4-20250514-v1:0\",\n    \"us.anthropic.claude-sonnet-4-20250514-v1:0\",\n    \"cohere.command-text-v14\",\n    \"cohere.command-r-v1:0\",\n    \"cohere.command-r-plus-v1:0\",\n    \"cohere.command-light-text-v14\",\n    \"meta.llama3-8b-instruct-v1:0\",\n    \"meta.llama3-70b-instruct-v1:0\",\n    \"meta.llama3-1-8b-instruct-v1:0\",\n    \"us.meta.llama3-1-8b-instruct-v1:0\",\n    \"meta.llama3-1-70b-instruct-v1:0\",\n    \"us.meta.llama3-1-70b-instruct-v1:0\",\n    \"meta.llama3-1-405b-instruct-v1:0\",\n    \"us.meta.llama3-2-11b-instruct-v1:0\",\n    \"us.meta.llama3-2-90b-instruct-v1:0\",\n    \"us.meta.llama3-2-1b-instruct-v1:0\",\n    \"us.meta.llama3-2-3b-instruct-v1:0\",\n    \"us.meta.llama3-3-70b-instruct-v1:0\",\n    \"mistral.mistral-7b-instruct-v0:2\",\n    \"mistral.mixtral-8x7b-instruct-v0:1\",\n    \"mistral.mistral-large-2402-v1:0\",\n    \"mistral.mistral-large-2407-v1:0\",\n]\n```\n\nLatest Bedrock models.", "url": "https://ai.pydantic.dev/models/bedrock/index.html#latestbedrockmodelnames-module-attribute", "page": "models/bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "BedrockModelName `module-attribute`", "anchor": "bedrockmodelname-module-attribute", "md_text": "```\nBedrockModelName = str | LatestBedrockModelNames\n```\n\nPossible Bedrock model names.\n\nSince Bedrock supports a variety of date-stamped models, we explicitly list the latest models but allow any name in the type hints.\nSee [the Bedrock docs](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html) for a full list.", "url": "https://ai.pydantic.dev/models/bedrock/index.html#bedrockmodelname-module-attribute", "page": "models/bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "BedrockModelSettings", "anchor": "bedrockmodelsettings", "md_text": "Bases: `ModelSettings`\n\nSettings for Bedrock models.\n\nSee [the Bedrock Converse API docs](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html#API_runtime_Converse_RequestSyntax) for a full list.\nSee [the boto3 implementation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html) of the Bedrock Converse API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/bedrock.py`\n\n|  |  |\n| --- | --- |\n| ``` 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 ``` | ``` class BedrockModelSettings(ModelSettings, total=False):     \"\"\"Settings for Bedrock models.      See [the Bedrock Converse API docs](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html#API_runtime_Converse_RequestSyntax) for a full list.     See [the boto3 implementation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html) of the Bedrock Converse API.     \"\"\"      # ALL FIELDS MUST BE `bedrock_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.      bedrock_guardrail_config: GuardrailConfigurationTypeDef     \"\"\"Content moderation and safety settings for Bedrock API requests.      See more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_GuardrailConfiguration.html>.     \"\"\"      bedrock_performance_configuration: PerformanceConfigurationTypeDef     \"\"\"Performance optimization settings for model inference.      See more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_PerformanceConfiguration.html>.     \"\"\"      bedrock_request_metadata: dict[str, str]     \"\"\"Additional metadata to attach to Bedrock API requests.      See more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html#API_runtime_Converse_RequestSyntax>.     \"\"\"      bedrock_additional_model_response_fields_paths: list[str]     \"\"\"JSON paths to extract additional fields from model responses.      See more about it on <https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html>.     \"\"\"      bedrock_prompt_variables: Mapping[str, PromptVariableValuesTypeDef]     \"\"\"Variables for substitution into prompt templates.      See more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_PromptVariableValues.html>.     \"\"\"      bedrock_additional_model_requests_fields: Mapping[str, Any]     \"\"\"Additional model-specific parameters to include in requests.      See more about it on <https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html>.     \"\"\" ``` |\n\n#### bedrock\\_guardrail\\_config `instance-attribute`\n\n```\nbedrock_guardrail_config: GuardrailConfigurationTypeDef\n```\n\nContent moderation and safety settings for Bedrock API requests.\n\nSee more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_GuardrailConfiguration.html>.\n\n#### bedrock\\_performance\\_configuration `instance-attribute`\n\n```\nbedrock_performance_configuration: (\n    PerformanceConfigurationTypeDef\n)\n```\n\nPerformance optimization settings for model inference.\n\nSee more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_PerformanceConfiguration.html>.\n\n#### bedrock\\_request\\_metadata `instance-attribute`\n\n```\nbedrock_request_metadata: dict[str, str]\n```\n\nAdditional metadata to attach to Bedrock API requests.\n\nSee more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html#API_runtime_Converse_RequestSyntax>.\n\n#### bedrock\\_additional\\_model\\_response\\_fields\\_paths `instance-attribute`\n\n```\nbedrock_additional_model_response_fields_paths: list[str]\n```\n\nJSON paths to extract additional fields from model responses.\n\nSee more about it on <https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html>.\n\n#### bedrock\\_prompt\\_variables `instance-attribute`\n\n```\nbedrock_prompt_variables: Mapping[\n    str, PromptVariableValuesTypeDef\n]\n```\n\nVariables for substitution into prompt templates.\n\nSee more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_PromptVariableValues.html>.\n\n#### bedrock\\_additional\\_model\\_requests\\_fields `instance-attribute`\n\n```\nbedrock_additional_model_requests_fields: Mapping[str, Any]\n```\n\nAdditional model-specific parameters to include in requests.\n\nSee more about it on <https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html>.", "url": "https://ai.pydantic.dev/models/bedrock/index.html#bedrockmodelsettings", "page": "models/bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "BedrockConverseModel `dataclass`", "anchor": "bedrockconversemodel-dataclass", "md_text": "Bases: `Model`\n\nA model that uses the Bedrock Converse API.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/bedrock.py`", "url": "https://ai.pydantic.dev/models/bedrock/index.html#bedrockconversemodel-dataclass", "page": "models/bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "BedrockConverseModel `dataclass`", "anchor": "bedrockconversemodel-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 ``` | ``` @dataclass(init=False) class BedrockConverseModel(Model):     \"\"\"A model that uses the Bedrock Converse API.\"\"\"      client: BedrockRuntimeClient      _model_name: BedrockModelName = field(repr=False)     _provider: Provider[BaseClient] = field(repr=False)      def __init__(         self,         model_name: BedrockModelName,         *,         provider: Literal['bedrock', 'gateway'] | Provider[BaseClient] = 'bedrock',         profile: ModelProfileSpec | None = None,         settings: ModelSettings | None = None,     ):         \"\"\"Initialize a Bedrock model.          Args:             model_name: The name of the model to use.             model_name: The name of the Bedrock model to use. List of model names available                 [here](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html).             provider: The provider to use for authentication and API access. Can be either the string                 'bedrock' or an instance of `Provider[BaseClient]`. If not provided, a new provider will be                 created using the other parameters.             profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.             settings: Model-specific settings that will be used as defaults for this model.         \"\"\"         self._model_name = model_name          if isinstance(provider, str):             provider = infer_provider('gateway/bedrock' if provider == 'gateway' else provider)         self._provider = provider         self.client = cast('BedrockRuntimeClient', provider.client)          super().__init__(settings=settings, profile=profile or provider.model_profile)      @property     def base_url(self) -> str:         return str(self.client.meta.endpoint_url)      @property     def model_name(self) -> str:         \"\"\"The model name.\"\"\"         return self._model_name      @property     def system(self) -> str:         \"\"\"The model provider.\"\"\"         return self._provider.name      def _get_tools(self, model_request_parameters: ModelRequestParameters) -> list[ToolTypeDef]:         return [self._map_tool_definition(r) for r in model_request_parameters.tool_defs.values()]      @staticmethod     def _map_tool_definition(f: ToolDefinition) -> ToolTypeDef:         tool_spec: ToolSpecificationTypeDef = {'name': f.name, 'inputSchema': {'json': f.parameters_json_schema}}          if f.description:  # pragma: no branch             tool_spec['description'] = f.description          return {'toolSpec': tool_spec}      async def request(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> ModelResponse:         model_settings, model_request_parameters = self.prepare_request(             model_settings,             model_request_parameters,         )         settings = cast(BedrockModelSettings, model_settings or {})         response = await self._messages_create(messages, False, settings, model_request_parameters)         model_response = await self._process_response(response)         return model_response      @asynccontextmanager     async def request_stream(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,         run_context: RunContext[Any] | None = None,     ) -> AsyncIterator[StreamedResponse]:         model_settings, model_request_parameters = self.prepare_request(             model_settings,             model_request_parameters,         )         settings = cast(BedrockModelSettings, model_settings or {})         response = await self._messages_create(messages, True, settings, model_request_parameters)         yield BedrockStreamedResponse(             model_request_parameters=model_request_parameters,             _model_name=self.model_name,             _event_stream=response['stream'],             _provider_name=self._provider.name,             _provider_response_id=response.get('ResponseMetadata', {}).get('RequestId', None),         )      async def _process_response(self, response: ConverseResponseTypeDef) -> ModelResponse:         items: list[ModelResponsePart] = []         if message := response['output'].get('message'):  # pragma: no branch             for item in message['content']:                 if reasoning_content := item.get('reasoningContent'):                     if redacted_content := reasoning_content.get('redactedContent'):                         items.append(                             ThinkingPart(                                 id='redacted_content',                                 content='',                                 signature=redacted_content.decode('utf-8'),                                 provider_name=self.system,                             )                         )                     elif reasoning_text := reasoning_content.get('reasoningText'):  # pragma: no branch                         signature = reasoning_text.get('signature')                         items.append(                             ThinkingPart(                                 content=reasoning_text['text'],                                 signature=signature,                                 provider_name=self.system if signature else None,                             )                         )                 if text := item.get('text'):                     items.append(TextPart(content=text))                 elif tool_use := item.get('toolUse'):                     items.append(                         ToolCallPart(                             tool_name=tool_use['name'],                             args=tool_use['input'],                             tool_call_id=tool_use['toolUseId'],                         ),                     )         u = usage.RequestUsage(             input_tokens=response['usage']['inputTokens'],             output_tokens=response['usage']['outputTokens'],         )         response_id = response.get('ResponseMetadata', {}).get('RequestId', None)         raw_finish_reason = response['stopReason']         provider_details = {'finish_reason': raw_finish_reason}         finish_reason = _FINISH_REASON_MAP.get(raw_finish_reason)          return ModelResponse(             parts=items,             usage=u,             model_name=self.model_name,             provider_response_id=response_id,             provider_name=self._provider.name,             finish_reason=finish_reason,             provider_details=provider_details,         )      @overload     async def _messages_create(         self,         messages: list[ModelMessage],         stream: Literal[True],         model_settings: BedrockModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> ConverseStreamResponseTypeDef:         pass      @overload     async def _messages_create(         self,         messages: list[ModelMessage],         stream: Literal[False],         model_settings: BedrockModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> ConverseResponseTypeDef:         pass      async def _messages_create(         self,         messages: list[ModelMessage],         stream: bool,         model_settings: BedrockModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> ConverseResponseTypeDef | ConverseStreamResponseTypeDef:         system_prompt, bedrock_messages = await self._map_messages(messages)         inference_config = self._map_inference_config(model_settings)          params: ConverseRequestTypeDef = {             'modelId': self.model_name,             'messages': bedrock_messages,             'system': system_prompt,             'inferenceConfig': inference_config,         }          tool_config = self._map_tool_config(model_request_parameters)         if tool_config:             params['toolConfig'] = tool_config          if model_request_parameters.builtin_tools:             raise UserError('Bedrock does not support built-in tools')          # Bedrock supports a set of specific extra parameters         if model_settings:             if guardrail_config := model_settings.get('bedrock_guardrail_config', None):                 params['guardrailConfig'] = guardrail_config             if performance_configuration := model_settings.get('bedrock_performance_configuration', None):                 params['performanceConfig'] = performance_configuration             if request_metadata := model_settings.get('bedrock_request_metadata', None):                 params['requestMetadata'] = request_metadata             if additional_model_response_fields_paths := model_settings.get(                 'bedrock_additional_model_response_fields_paths', None             ):                 params['additionalModelResponseFieldPaths'] = additional_model_response_fields_paths             if additional_model_requests_fields := model_settings.get('bedrock_additional_model_requests_fields', None):                 params['additionalModelRequestFields'] = additional_model_requests_fields             if prompt_variables := model_settings.get('bedrock_prompt_variables', None):                 params['promptVariables'] = prompt_variables          if stream:             model_response = await anyio.to_thread.run_sync(functools.partial(self.client.converse_stream, **params))         else:             model_response = await anyio.to_thread.run_sync(functools.partial(self.client.converse, **params))         return model_response      @staticmethod     def _map_inference_config(         model_settings: ModelSettings | None,     ) -> InferenceConfigurationTypeDef:         model_settings = model_settings or {}         inference_config: InferenceConfigurationTypeDef = {}          if max_tokens := model_settings.get('max_tokens'):             inference_config['maxTokens'] = max_tokens         if (temperature := model_settings.get('temperature')) is not None:             inference_config['temperature'] = temperature         if top_p := model_settings.get('top_p'):             inference_config['topP'] = top_p         if stop_sequences := model_settings.get('stop_sequences'):             inference_config['stopSequences'] = stop_sequences          return inference_config      def _map_tool_config(self, model_request_parameters: ModelRequestParameters) -> ToolConfigurationTypeDef | None:         tools = self._get_tools(model_request_parameters)         if not tools:             return None          tool_choice: ToolChoiceTypeDef         if not model_request_parameters.allow_text_output:             tool_choice = {'any': {}}         else:             tool_choice = {'auto': {}}          tool_config: ToolConfigurationTypeDef = {'tools': tools}         if tool_choice and BedrockModelProfile.from_profile(self.profile).bedrock_supports_tool_choice:             tool_config['toolChoice'] = tool_choice          return tool_config      async def _map_messages(  # noqa: C901         self, messages: list[ModelMessage]     ) -> tuple[list[SystemContentBlockTypeDef], list[MessageUnionTypeDef]]:         \"\"\"Maps a `pydantic_ai.Message` to the Bedrock `MessageUnionTypeDef`.          Groups consecutive ToolReturnPart objects into a single user message as required by Bedrock Claude/Nova models.         \"\"\"         profile = BedrockModelProfile.from_profile(self.profile)         system_prompt: list[SystemContentBlockTypeDef] = []         bedrock_messages: list[MessageUnionTypeDef] = []         document_count: Iterator[int] = count(1)         for message in messages:             if isinstance(message, ModelRequest):                 for part in message.parts:                     if isinstance(part, SystemPromptPart) and part.content:                         system_prompt.append({'text': part.content})                     elif isinstance(part, UserPromptPart):                         bedrock_messages.extend(await self._map_user_prompt(part, document_count))                     elif isinstance(part, ToolReturnPart):                         assert part.tool_call_id is not None                         bedrock_messages.append(                             {                                 'role': 'user',                                 'content': [                                     {                                         'toolResult': {                                             'toolUseId': part.tool_call_id,                                             'content': [                                                 {'text': part.model_response_str()}                                                 if profile.bedrock_tool_result_format == 'text'                                                 else {'json': part.model_response_object()}                                             ],                                             'status': 'success',                                         }                                     }                                 ],                             }                         )                     elif isinstance(part, RetryPromptPart):                         # TODO(Marcelo): We need to add a test here.                         if part.tool_name is None:  # pragma: no cover                             bedrock_messages.append({'role': 'user', 'content': [{'text': part.model_response()}]})                         else:                             assert part.tool_call_id is not None                             bedrock_messages.append(                                 {                                     'role': 'user',                                     'content': [                                         {                                             'toolResult': {                                                 'toolUseId': part.tool_call_id,                                                 'content': [{'text': part.model_response()}],                                                 'status': 'error',                                             }                                         }                                     ],                                 }                             )             elif isinstance(message, ModelResponse):                 content: list[ContentBlockOutputTypeDef] = []                 for item in message.parts:                     if isinstance(item, TextPart):                         content.append({'text': item.content})                     elif isinstance(item, ThinkingPart):                         if (                             item.provider_name == self.system                             and item.signature                             and BedrockModelProfile.from_profile(self.profile).bedrock_send_back_thinking_parts                         ):                             if item.id == 'redacted_content':                                 reasoning_content: ReasoningContentBlockOutputTypeDef = {                                     'redactedContent': item.signature.encode('utf-8'),                                 }                             else:                                 reasoning_content: ReasoningContentBlockOutputTypeDef = {                                     'reasoningText': {                                         'text': item.content,                                         'signature': item.signature,                                     }                                 }                             content.append({'reasoningContent': reasoning_content})                         else:                             start_tag, end_tag = self.profile.thinking_tags                             content.append({'text': '\\n'.join([start_tag, item.content, end_tag])})                     elif isinstance(item, BuiltinToolCallPart | BuiltinToolReturnPart):                         pass                     else:                         assert isinstance(item, ToolCallPart)                         content.append(self._map_tool_call(item))                 bedrock_messages.append({'role': 'assistant', 'content': content})             else:                 assert_never(message)          # Merge together sequential user messages.         processed_messages: list[MessageUnionTypeDef] = []         last_message: dict[str, Any] | None = None         for current_message in bedrock_messages:             if (                 last_message is not None                 and current_message['role'] == last_message['role']                 and current_message['role'] == 'user'             ):                 # Add the new user content onto the existing user message.                 last_content = list(last_message['content'])                 last_content.extend(current_message['content'])                 last_message['content'] = last_content                 continue              # Add the entire message to the list of messages.             processed_messages.append(current_message)             last_message = cast(dict[str, Any], current_message)          if instructions := self._get_instructions(messages):             system_prompt.insert(0, {'text': instructions})          return system_prompt, processed_messages      @staticmethod     async def _map_user_prompt(part: UserPromptPart, document_count: Iterator[int]) -> list[MessageUnionTypeDef]:         content: list[ContentBlockUnionTypeDef] = []         if isinstance(part.content, str):             content.append({'text': part.content})         else:             for item in part.content:                 if isinstance(item, str):                     content.append({'text': item})                 elif isinstance(item, BinaryContent):                     format = item.format                     if item.is_document:                         name = f'Document {next(document_count)}'                         assert format in ('pdf', 'txt', 'csv', 'doc', 'docx', 'xls', 'xlsx', 'html', 'md')                         content.append({'document': {'name': name, 'format': format, 'source': {'bytes': item.data}}})                     elif item.is_image:                         assert format in ('jpeg', 'png', 'gif', 'webp')                         content.append({'image': {'format': format, 'source': {'bytes': item.data}}})                     elif item.is_video:                         assert format in ('mkv', 'mov', 'mp4', 'webm', 'flv', 'mpeg', 'mpg', 'wmv', 'three_gp')                         content.append({'video': {'format': format, 'source': {'bytes': item.data}}})                     else:                         raise NotImplementedError('Binary content is not supported yet.')                 elif isinstance(item, ImageUrl | DocumentUrl | VideoUrl):                     downloaded_item = await download_item(item, data_format='bytes', type_format='extension')                     format = downloaded_item['data_type']                     if item.kind == 'image-url':                         format = item.media_type.split('/')[1]                         assert format in ('jpeg', 'png', 'gif', 'webp'), f'Unsupported image format: {format}'                         image: ImageBlockTypeDef = {'format': format, 'source': {'bytes': downloaded_item['data']}}                         content.append({'image': image})                      elif item.kind == 'document-url':                         name = f'Document {next(document_count)}'                         document: DocumentBlockTypeDef = {                             'name': name,                             'format': item.format,                             'source': {'bytes': downloaded_item['data']},                         }                         content.append({'document': document})                      elif item.kind == 'video-url':  # pragma: no branch                         format = item.media_type.split('/')[1]                         assert format in (                             'mkv',                             'mov',                             'mp4',                             'webm',                             'flv',                             'mpeg',                             'mpg',                             'wmv',                             'three_gp',                         ), f'Unsupported video format: {format}'                         video: VideoBlockTypeDef = {'format': format, 'source': {'bytes': downloaded_item['data']}}                         content.append({'video': video})                 elif isinstance(item, AudioUrl):  # pragma: no cover                     raise NotImplementedError('Audio is not supported yet.')                 else:                     assert_never(item)         return [{'role': 'user', 'content': content}]      @staticmethod     def _map_tool_call(t: ToolCallPart) -> ContentBlockOutputTypeDef:         return {             'toolUse': {'toolUseId': _utils.guard_tool_call_id(t=t), 'name': t.tool_name, 'input': t.args_as_dict()}         } ``` |", "url": "https://ai.pydantic.dev/models/bedrock/index.html#bedrockconversemodel-dataclass", "page": "models/bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "BedrockConverseModel `dataclass`", "anchor": "bedrockconversemodel-dataclass", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    model_name: BedrockModelName,\n    *,\n    provider: (\n        Literal[\"bedrock\", \"gateway\"] | Provider[BaseClient]\n    ) = \"bedrock\",\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n)\n```\n\nInitialize a Bedrock model.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model_name` | `BedrockModelName` | The name of the model to use. | *required* |\n| `model_name` | `BedrockModelName` | The name of the Bedrock model to use. List of model names available [here](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html). | *required* |\n| `provider` | `Literal['bedrock', 'gateway'] | Provider[BaseClient]` | The provider to use for authentication and API access. Can be either the string 'bedrock' or an instance of `Provider[BaseClient]`. If not provided, a new provider will be created using the other parameters. | `'bedrock'` |\n| `profile` | `ModelProfileSpec | None` | The model profile to use. Defaults to a profile picked by the provider based on the model name. | `None` |\n| `settings` | `ModelSettings | None` | Model-specific settings that will be used as defaults for this model. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/bedrock.py`\n\n|  |  |\n| --- | --- |\n| ``` 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 ``` | ``` def __init__(     self,     model_name: BedrockModelName,     *,     provider: Literal['bedrock', 'gateway'] | Provider[BaseClient] = 'bedrock',     profile: ModelProfileSpec | None = None,     settings: ModelSettings | None = None, ):     \"\"\"Initialize a Bedrock model.      Args:         model_name: The name of the model to use.         model_name: The name of the Bedrock model to use. List of model names available             [here](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html).         provider: The provider to use for authentication and API access. Can be either the string             'bedrock' or an instance of `Provider[BaseClient]`. If not provided, a new provider will be             created using the other parameters.         profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.         settings: Model-specific settings that will be used as defaults for this model.     \"\"\"     self._model_name = model_name      if isinstance(provider, str):         provider = infer_provider('gateway/bedrock' if provider == 'gateway' else provider)     self._provider = provider     self.client = cast('BedrockRuntimeClient', provider.client)      super().__init__(settings=settings, profile=profile or provider.model_profile) ``` |\n\n#### model\\_name `property`\n\n```\nmodel_name: str\n```\n\nThe model name.\n\n#### system `property`\n\n```\nsystem: str\n```\n\nThe model provider.", "url": "https://ai.pydantic.dev/models/bedrock/index.html#bedrockconversemodel-dataclass", "page": "models/bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "BedrockStreamedResponse `dataclass`", "anchor": "bedrockstreamedresponse-dataclass", "md_text": "Bases: `StreamedResponse`\n\nImplementation of `StreamedResponse` for Bedrock models.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/bedrock.py`\n\n|  |  |\n| --- | --- |\n| ``` 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 ``` | ``` @dataclass class BedrockStreamedResponse(StreamedResponse):     \"\"\"Implementation of `StreamedResponse` for Bedrock models.\"\"\"      _model_name: BedrockModelName     _event_stream: EventStream[ConverseStreamOutputTypeDef]     _provider_name: str     _timestamp: datetime = field(default_factory=_utils.now_utc)     _provider_response_id: str | None = None      async def _get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:  # noqa: C901         \"\"\"Return an async iterator of [`ModelResponseStreamEvent`][pydantic_ai.messages.ModelResponseStreamEvent]s.          This method should be implemented by subclasses to translate the vendor-specific stream of events into         pydantic_ai-format events.         \"\"\"         if self._provider_response_id is not None:  # pragma: no cover             self.provider_response_id = self._provider_response_id          chunk: ConverseStreamOutputTypeDef         tool_id: str | None = None         async for chunk in _AsyncIteratorWrapper(self._event_stream):             match chunk:                 case {'messageStart': _}:                     continue                 case {'messageStop': message_stop}:                     raw_finish_reason = message_stop['stopReason']                     self.provider_details = {'finish_reason': raw_finish_reason}                     self.finish_reason = _FINISH_REASON_MAP.get(raw_finish_reason)                 case {'metadata': metadata}:                     if 'usage' in metadata:  # pragma: no branch                         self._usage += self._map_usage(metadata)                 case {'contentBlockStart': content_block_start}:                     index = content_block_start['contentBlockIndex']                     start = content_block_start['start']                     if 'toolUse' in start:  # pragma: no branch                         tool_use_start = start['toolUse']                         tool_id = tool_use_start['toolUseId']                         tool_name = tool_use_start['name']                         maybe_event = self._parts_manager.handle_tool_call_delta(                             vendor_part_id=index,                             tool_name=tool_name,                             args=None,                             tool_call_id=tool_id,                         )                         if maybe_event:  # pragma: no branch                             yield maybe_event                 case {'contentBlockDelta': content_block_delta}:                     index = content_block_delta['contentBlockIndex']                     delta = content_block_delta['delta']                     if 'reasoningContent' in delta:                         if redacted_content := delta['reasoningContent'].get('redactedContent'):                             yield self._parts_manager.handle_thinking_delta(                                 vendor_part_id=index,                                 id='redacted_content',                                 signature=redacted_content.decode('utf-8'),                                 provider_name=self.provider_name,                             )                         else:                             signature = delta['reasoningContent'].get('signature')                             yield self._parts_manager.handle_thinking_delta(                                 vendor_part_id=index,                                 content=delta['reasoningContent'].get('text'),                                 signature=signature,                                 provider_name=self.provider_name if signature else None,                             )                     if text := delta.get('text'):                         maybe_event = self._parts_manager.handle_text_delta(vendor_part_id=index, content=text)                         if maybe_event is not None:  # pragma: no branch                             yield maybe_event                     if 'toolUse' in delta:                         tool_use = delta['toolUse']                         maybe_event = self._parts_manager.handle_tool_call_delta(                             vendor_part_id=index,                             tool_name=tool_use.get('name'),                             args=tool_use.get('input'),                             tool_call_id=tool_id,                         )                         if maybe_event:  # pragma: no branch                             yield maybe_event                 case _:                     pass  # pyright wants match statements to be exhaustive      @property     def model_name(self) -> str:         \"\"\"Get the model name of the response.\"\"\"         return self._model_name      @property     def provider_name(self) -> str:         \"\"\"Get the provider name.\"\"\"         return self._provider_name      @property     def timestamp(self) -> datetime:         return self._timestamp      def _map_usage(self, metadata: ConverseStreamMetadataEventTypeDef) -> usage.RequestUsage:         return usage.RequestUsage(             input_tokens=metadata['usage']['inputTokens'],             output_tokens=metadata['usage']['outputTokens'],         ) ``` |\n\n#### model\\_name `property`\n\n```\nmodel_name: str\n```\n\nGet the model name of the response.\n\n#### provider\\_name `property`\n\n```\nprovider_name: str\n```\n\nGet the provider name.", "url": "https://ai.pydantic.dev/models/bedrock/index.html#bedrockstreamedresponse-dataclass", "page": "models/bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "MCPSamplingModelSettings", "anchor": "mcpsamplingmodelsettings", "md_text": "Bases: `ModelSettings`\n\nSettings used for an MCP Sampling model request.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/mcp_sampling.py`\n\n|  |  |\n| --- | --- |\n| ``` 19 20 21 22 23 24 25 ``` | ``` class MCPSamplingModelSettings(ModelSettings, total=False):     \"\"\"Settings used for an MCP Sampling model request.\"\"\"      # ALL FIELDS MUST BE `mcp_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.      mcp_model_preferences: ModelPreferences     \"\"\"Model preferences to use for MCP Sampling.\"\"\" ``` |\n\n#### mcp\\_model\\_preferences `instance-attribute`\n\n```\nmcp_model_preferences: ModelPreferences\n```\n\nModel preferences to use for MCP Sampling.", "url": "https://ai.pydantic.dev/models/mcp-sampling/index.html#mcpsamplingmodelsettings", "page": "models/mcp-sampling/index.html", "source_site": "pydantic_ai"}
{"title": "MCPSamplingModel `dataclass`", "anchor": "mcpsamplingmodel-dataclass", "md_text": "Bases: `Model`\n\nA model that uses MCP Sampling.\n\n[MCP Sampling](https://modelcontextprotocol.io/docs/concepts/sampling)\nallows an MCP server to make requests to a model by calling back to the MCP client that connected to it.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/mcp_sampling.py`\n\n|  |  |\n| --- | --- |\n| ``` 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 ``` | ``` @dataclass class MCPSamplingModel(Model):     \"\"\"A model that uses MCP Sampling.      [MCP Sampling](https://modelcontextprotocol.io/docs/concepts/sampling)     allows an MCP server to make requests to a model by calling back to the MCP client that connected to it.     \"\"\"      session: ServerSession     \"\"\"The MCP server session to use for sampling.\"\"\"      _: KW_ONLY      default_max_tokens: int = 16_384     \"\"\"Default max tokens to use if not set in [`ModelSettings`][pydantic_ai.settings.ModelSettings.max_tokens].      Max tokens is a required parameter for MCP Sampling, but optional on     [`ModelSettings`][pydantic_ai.settings.ModelSettings], so this value is used as fallback.     \"\"\"      async def request(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> ModelResponse:         system_prompt, sampling_messages = _mcp.map_from_pai_messages(messages)          model_settings, _ = self.prepare_request(model_settings, model_request_parameters)         model_settings = cast(MCPSamplingModelSettings, model_settings or {})          result = await self.session.create_message(             sampling_messages,             max_tokens=model_settings.get('max_tokens', self.default_max_tokens),             system_prompt=system_prompt,             temperature=model_settings.get('temperature'),             model_preferences=model_settings.get('mcp_model_preferences'),             stop_sequences=model_settings.get('stop_sequences'),         )         if result.role == 'assistant':             return ModelResponse(                 parts=[_mcp.map_from_sampling_content(result.content)],                 model_name=result.model,             )         else:             raise exceptions.UnexpectedModelBehavior(                 f'Unexpected result from MCP sampling, expected \"assistant\" role, got {result.role}.'             )      @asynccontextmanager     async def request_stream(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,         run_context: RunContext[Any] | None = None,     ) -> AsyncIterator[StreamedResponse]:         raise NotImplementedError('MCP Sampling does not support streaming')         yield      @property     def model_name(self) -> str:         \"\"\"The model name.          Since the model name isn't known until the request is made, this property always returns `'mcp-sampling'`.         \"\"\"         return 'mcp-sampling'      @property     def system(self) -> str:         \"\"\"The system / model provider, returns `'MCP'`.\"\"\"         return 'MCP' ``` |\n\n#### session `instance-attribute`\n\n```\nsession: ServerSession\n```\n\nThe MCP server session to use for sampling.\n\n#### default\\_max\\_tokens `class-attribute` `instance-attribute`\n\n```\ndefault_max_tokens: int = 16384\n```\n\nDefault max tokens to use if not set in [`ModelSettings`](../../settings/index.html#pydantic_ai.settings.ModelSettings.max_tokens).\n\nMax tokens is a required parameter for MCP Sampling, but optional on\n[`ModelSettings`](../../settings/index.html#pydantic_ai.settings.ModelSettings), so this value is used as fallback.\n\n#### model\\_name `property`\n\n```\nmodel_name: str\n```\n\nThe model name.\n\nSince the model name isn't known until the request is made, this property always returns `'mcp-sampling'`.\n\n#### system `property`\n\n```\nsystem: str\n```\n\nThe system / model provider, returns `'MCP'`.", "url": "https://ai.pydantic.dev/models/mcp-sampling/index.html#mcpsamplingmodel-dataclass", "page": "models/mcp-sampling/index.html", "source_site": "pydantic_ai"}
{"title": "Setup", "anchor": "setup", "md_text": "For details on how to set up authentication with this model, see [model configuration for Mistral](../../mistral/index.html).", "url": "https://ai.pydantic.dev/models/mistral/index.html#setup", "page": "models/mistral/index.html", "source_site": "pydantic_ai"}
{"title": "LatestMistralModelNames `module-attribute`", "anchor": "latestmistralmodelnames-module-attribute", "md_text": "```\nLatestMistralModelNames = Literal[\n    \"mistral-large-latest\",\n    \"mistral-small-latest\",\n    \"codestral-latest\",\n    \"mistral-moderation-latest\",\n]\n```\n\nLatest Mistral models.", "url": "https://ai.pydantic.dev/models/mistral/index.html#latestmistralmodelnames-module-attribute", "page": "models/mistral/index.html", "source_site": "pydantic_ai"}
{"title": "MistralModelName `module-attribute`", "anchor": "mistralmodelname-module-attribute", "md_text": "```\nMistralModelName = str | LatestMistralModelNames\n```\n\nPossible Mistral model names.\n\nSince Mistral supports a variety of date-stamped models, we explicitly list the most popular models but\nallow any name in the type hints.\nSince [the Mistral docs](https://docs.mistral.ai/getting-started/models/models_overview/) for a full list.", "url": "https://ai.pydantic.dev/models/mistral/index.html#mistralmodelname-module-attribute", "page": "models/mistral/index.html", "source_site": "pydantic_ai"}
{"title": "MistralModelSettings", "anchor": "mistralmodelsettings", "md_text": "Bases: `ModelSettings`\n\nSettings used for a Mistral model request.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/mistral.py`\n\n|  |  |\n| --- | --- |\n| ``` 113 114 ``` | ``` class MistralModelSettings(ModelSettings, total=False):     \"\"\"Settings used for a Mistral model request.\"\"\" ``` |", "url": "https://ai.pydantic.dev/models/mistral/index.html#mistralmodelsettings", "page": "models/mistral/index.html", "source_site": "pydantic_ai"}
{"title": "MistralModel `dataclass`", "anchor": "mistralmodel-dataclass", "md_text": "Bases: `Model`\n\nA model that uses Mistral.\n\nInternally, this uses the [Mistral Python client](https://github.com/mistralai/client-python) to interact with the API.\n\n[API Documentation](https://docs.mistral.ai/)\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/mistral.py`", "url": "https://ai.pydantic.dev/models/mistral/index.html#mistralmodel-dataclass", "page": "models/mistral/index.html", "source_site": "pydantic_ai"}
{"title": "MistralModel `dataclass`", "anchor": "mistralmodel-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 ``` | ``` @dataclass(init=False) class MistralModel(Model):     \"\"\"A model that uses Mistral.      Internally, this uses the [Mistral Python client](https://github.com/mistralai/client-python) to interact with the API.      [API Documentation](https://docs.mistral.ai/)     \"\"\"      client: Mistral = field(repr=False)     json_mode_schema_prompt: str      _model_name: MistralModelName = field(repr=False)     _provider: Provider[Mistral] = field(repr=False)      def __init__(         self,         model_name: MistralModelName,         *,         provider: Literal['mistral'] | Provider[Mistral] = 'mistral',         profile: ModelProfileSpec | None = None,         json_mode_schema_prompt: str = \"\"\"Answer in JSON Object, respect the format:\\n```\\n{schema}\\n```\\n\"\"\",         settings: ModelSettings | None = None,     ):         \"\"\"Initialize a Mistral model.          Args:             model_name: The name of the model to use.             provider: The provider to use for authentication and API access. Can be either the string                 'mistral' or an instance of `Provider[Mistral]`. If not provided, a new provider will be                 created using the other parameters.             profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.             json_mode_schema_prompt: The prompt to show when the model expects a JSON object as input.             settings: Model-specific settings that will be used as defaults for this model.         \"\"\"         self._model_name = model_name         self.json_mode_schema_prompt = json_mode_schema_prompt          if isinstance(provider, str):             provider = infer_provider(provider)         self._provider = provider         self.client = provider.client          super().__init__(settings=settings, profile=profile or provider.model_profile)      @property     def base_url(self) -> str:         return self._provider.base_url      @property     def model_name(self) -> MistralModelName:         \"\"\"The model name.\"\"\"         return self._model_name      @property     def system(self) -> str:         \"\"\"The model provider.\"\"\"         return self._provider.name      async def request(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> ModelResponse:         \"\"\"Make a non-streaming request to the model from Pydantic AI call.\"\"\"         check_allow_model_requests()         model_settings, model_request_parameters = self.prepare_request(             model_settings,             model_request_parameters,         )         response = await self._completions_create(             messages, cast(MistralModelSettings, model_settings or {}), model_request_parameters         )         model_response = self._process_response(response)         return model_response      @asynccontextmanager     async def request_stream(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,         run_context: RunContext[Any] | None = None,     ) -> AsyncIterator[StreamedResponse]:         \"\"\"Make a streaming request to the model from Pydantic AI call.\"\"\"         check_allow_model_requests()         model_settings, model_request_parameters = self.prepare_request(             model_settings,             model_request_parameters,         )         response = await self._stream_completions_create(             messages, cast(MistralModelSettings, model_settings or {}), model_request_parameters         )         async with response:             yield await self._process_streamed_response(response, model_request_parameters)      async def _completions_create(         self,         messages: list[ModelMessage],         model_settings: MistralModelSettings,         model_request_parameters: ModelRequestParameters,     ) -> MistralChatCompletionResponse:         \"\"\"Make a non-streaming request to the model.\"\"\"         # TODO(Marcelo): We need to replace the current MistralAI client to use the beta client.         # See https://docs.mistral.ai/agents/connectors/websearch/ to support web search.         if model_request_parameters.builtin_tools:             raise UserError('Mistral does not support built-in tools')          try:             response = await self.client.chat.complete_async(                 model=str(self._model_name),                 messages=self._map_messages(messages),                 n=1,                 tools=self._map_function_and_output_tools_definition(model_request_parameters) or UNSET,                 tool_choice=self._get_tool_choice(model_request_parameters),                 stream=False,                 max_tokens=model_settings.get('max_tokens', UNSET),                 temperature=model_settings.get('temperature', UNSET),                 top_p=model_settings.get('top_p', 1),                 timeout_ms=self._get_timeout_ms(model_settings.get('timeout')),                 random_seed=model_settings.get('seed', UNSET),                 stop=model_settings.get('stop_sequences', None),                 http_headers={'User-Agent': get_user_agent()},             )         except SDKError as e:             if (status_code := e.status_code) >= 400:                 raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e             raise  # pragma: lax no cover          assert response, 'A unexpected empty response from Mistral.'         return response      async def _stream_completions_create(         self,         messages: list[ModelMessage],         model_settings: MistralModelSettings,         model_request_parameters: ModelRequestParameters,     ) -> MistralEventStreamAsync[MistralCompletionEvent]:         \"\"\"Create a streaming completion request to the Mistral model.\"\"\"         response: MistralEventStreamAsync[MistralCompletionEvent] | None         mistral_messages = self._map_messages(messages)          # TODO(Marcelo): We need to replace the current MistralAI client to use the beta client.         # See https://docs.mistral.ai/agents/connectors/websearch/ to support web search.         if model_request_parameters.builtin_tools:             raise UserError('Mistral does not support built-in tools')          if model_request_parameters.function_tools:             # Function Calling             response = await self.client.chat.stream_async(                 model=str(self._model_name),                 messages=mistral_messages,                 n=1,                 tools=self._map_function_and_output_tools_definition(model_request_parameters) or UNSET,                 tool_choice=self._get_tool_choice(model_request_parameters),                 temperature=model_settings.get('temperature', UNSET),                 top_p=model_settings.get('top_p', 1),                 max_tokens=model_settings.get('max_tokens', UNSET),                 timeout_ms=self._get_timeout_ms(model_settings.get('timeout')),                 presence_penalty=model_settings.get('presence_penalty'),                 frequency_penalty=model_settings.get('frequency_penalty'),                 stop=model_settings.get('stop_sequences', None),                 http_headers={'User-Agent': get_user_agent()},             )          elif model_request_parameters.output_tools:             # TODO: Port to native \"manual JSON\" mode             # Json Mode             parameters_json_schemas = [tool.parameters_json_schema for tool in model_request_parameters.output_tools]             user_output_format_message = self._generate_user_output_format(parameters_json_schemas)             mistral_messages.append(user_output_format_message)              response = await self.client.chat.stream_async(                 model=str(self._model_name),                 messages=mistral_messages,                 response_format={                     'type': 'json_object'                 },  # TODO: Should be able to use json_schema now: https://docs.mistral.ai/capabilities/structured-output/custom_structured_output/, https://github.com/mistralai/client-python/blob/bc4adf335968c8a272e1ab7da8461c9943d8e701/src/mistralai/extra/utils/response_format.py#L9                 stream=True,                 http_headers={'User-Agent': get_user_agent()},             )          else:             # Stream Mode             response = await self.client.chat.stream_async(                 model=str(self._model_name),                 messages=mistral_messages,                 stream=True,                 http_headers={'User-Agent': get_user_agent()},             )         assert response, 'A unexpected empty response from Mistral.'         return response      def _get_tool_choice(self, model_request_parameters: ModelRequestParameters) -> MistralToolChoiceEnum | None:         \"\"\"Get tool choice for the model.          - \"auto\": Default mode. Model decides if it uses the tool or not.         - \"any\": Select any tool.         - \"none\": Prevents tool use.         - \"required\": Forces tool use.         \"\"\"         if not model_request_parameters.function_tools and not model_request_parameters.output_tools:             return None         elif not model_request_parameters.allow_text_output:             return 'required'         else:             return 'auto'      def _map_function_and_output_tools_definition(         self, model_request_parameters: ModelRequestParameters     ) -> list[MistralTool] | None:         \"\"\"Map function and output tools to MistralTool format.          Returns None if both function_tools and output_tools are empty.         \"\"\"         tools = [             MistralTool(                 function=MistralFunction(                     name=r.name, parameters=r.parameters_json_schema, description=r.description or ''                 )             )             for r in model_request_parameters.tool_defs.values()         ]         return tools if tools else None      def _process_response(self, response: MistralChatCompletionResponse) -> ModelResponse:         \"\"\"Process a non-streamed response, and prepare a message to return.\"\"\"         assert response.choices, 'Unexpected empty response choice.'          if response.created:             timestamp = number_to_datetime(response.created)         else:             timestamp = _now_utc()          choice = response.choices[0]         content = choice.message.content         tool_calls = choice.message.tool_calls          parts: list[ModelResponsePart] = []         text, thinking = _map_content(content)         for thought in thinking:             parts.append(ThinkingPart(content=thought))         if text:             parts.append(TextPart(content=text))          if isinstance(tool_calls, list):             for tool_call in tool_calls:                 tool = self._map_mistral_to_pydantic_tool_call(tool_call=tool_call)                 parts.append(tool)          raw_finish_reason = choice.finish_reason         provider_details = {'finish_reason': raw_finish_reason}         finish_reason = _FINISH_REASON_MAP.get(raw_finish_reason)          return ModelResponse(             parts=parts,             usage=_map_usage(response),             model_name=response.model,             timestamp=timestamp,             provider_response_id=response.id,             provider_name=self._provider.name,             finish_reason=finish_reason,             provider_details=provider_details,         )      async def _process_streamed_response(         self,         response: MistralEventStreamAsync[MistralCompletionEvent],         model_request_parameters: ModelRequestParameters,     ) -> StreamedResponse:         \"\"\"Process a streamed response, and prepare a streaming response to return.\"\"\"         peekable_response = _utils.PeekableAsyncStream(response)         first_chunk = await peekable_response.peek()         if isinstance(first_chunk, _utils.Unset):             raise UnexpectedModelBehavior(  # pragma: no cover                 'Streamed response ended without content or tool calls'             )          if first_chunk.data.created:             timestamp = number_to_datetime(first_chunk.data.created)         else:             timestamp = _now_utc()          return MistralStreamedResponse(             model_request_parameters=model_request_parameters,             _response=peekable_response,             _model_name=first_chunk.data.model,             _timestamp=timestamp,             _provider_name=self._provider.name,         )      @staticmethod     def _map_mistral_to_pydantic_tool_call(tool_call: MistralToolCall) -> ToolCallPart:         \"\"\"Maps a MistralToolCall to a ToolCall.\"\"\"         tool_call_id = tool_call.id or _generate_tool_call_id()         func_call = tool_call.function          return ToolCallPart(func_call.name, func_call.arguments, tool_call_id)      @staticmethod     def _map_tool_call(t: ToolCallPart) -> MistralToolCall:         \"\"\"Maps a pydantic-ai ToolCall to a MistralToolCall.\"\"\"         return MistralToolCall(             id=_utils.guard_tool_call_id(t=t),             type='function',             function=MistralFunctionCall(name=t.tool_name, arguments=t.args or {}),         )      def _generate_user_output_format(self, schemas: list[dict[str, Any]]) -> MistralUserMessage:         \"\"\"Get a message with an example of the expected output format.\"\"\"         examples: list[dict[str, Any]] = []         for schema in schemas:             typed_dict_definition: dict[str, Any] = {}             for key, value in schema.get('properties', {}).items():                 typed_dict_definition[key] = self._get_python_type(value)             examples.append(typed_dict_definition)          example_schema = examples[0] if len(examples) == 1 else examples         return MistralUserMessage(content=self.json_mode_schema_prompt.format(schema=example_schema))      @classmethod     def _get_python_type(cls, value: dict[str, Any]) -> str:         \"\"\"Return a string representation of the Python type for a single JSON schema property.          This function handles recursion for nested arrays/objects and `anyOf`.         \"\"\"         # 1) Handle anyOf first, because it's a different schema structure         if any_of := value.get('anyOf'):             # Simplistic approach: pick the first option in anyOf             # (In reality, you'd possibly want to merge or union types)             return f'Optional[{cls._get_python_type(any_of[0])}]'          # 2) If we have a top-level \"type\" field         value_type = value.get('type')         if not value_type:             # No explicit type; fallback             return 'Any'          # 3) Direct simple type mapping (string, integer, float, bool, None)         if value_type in SIMPLE_JSON_TYPE_MAPPING and value_type != 'array' and value_type != 'object':             return SIMPLE_JSON_TYPE_MAPPING[value_type]          # 4) Array: Recursively get the item type         if value_type == 'array':             items = value.get('items', {})             return f'list[{cls._get_python_type(items)}]'          # 5) Object: Check for additionalProperties         if value_type == 'object':             additional_properties = value.get('additionalProperties', {})             if isinstance(additional_properties, bool):                 return 'bool'  # pragma: lax no cover             additional_properties_type = additional_properties.get('type')             if (                 additional_properties_type in SIMPLE_JSON_TYPE_MAPPING                 and additional_properties_type != 'array'                 and additional_properties_type != 'object'             ):                 # dict[str, bool/int/float/etc...]                 return f'dict[str, {SIMPLE_JSON_TYPE_MAPPING[additional_properties_type]}]'             elif additional_properties_type == 'array':                 array_items = additional_properties.get('items', {})                 return f'dict[str, list[{cls._get_python_type(array_items)}]]'             elif additional_properties_type == 'object':                 # nested dictionary of unknown shape                 return 'dict[str, dict[str, Any]]'             else:                 # If no additionalProperties type or something else, default to a generic dict                 return 'dict[str, Any]'          # 6) Fallback         return 'Any'      @staticmethod     def _get_timeout_ms(timeout: Timeout | float | None) -> int | None:         \"\"\"Convert a timeout to milliseconds.\"\"\"         if timeout is None:             return None         if isinstance(timeout, float):  # pragma: no cover             return int(1000 * timeout)         raise NotImplementedError('Timeout object is not yet supported for MistralModel.')      def _map_user_message(self, message: ModelRequest) -> Iterable[MistralMessages]:         for part in message.parts:             if isinstance(part, SystemPromptPart):                 yield MistralSystemMessage(content=part.content)             elif isinstance(part, UserPromptPart):                 yield self._map_user_prompt(part)             elif isinstance(part, ToolReturnPart):                 yield MistralToolMessage(                     tool_call_id=part.tool_call_id,                     content=part.model_response_str(),                 )             elif isinstance(part, RetryPromptPart):                 if part.tool_name is None:                     yield MistralUserMessage(content=part.model_response())  # pragma: no cover                 else:                     yield MistralToolMessage(                         tool_call_id=part.tool_call_id,                         content=part.model_response(),                     )             else:                 assert_never(part)      def _map_messages(self, messages: list[ModelMessage]) -> list[MistralMessages]:         \"\"\"Just maps a `pydantic_ai.Message` to a `MistralMessage`.\"\"\"         mistral_messages: list[MistralMessages] = []         for message in messages:             if isinstance(message, ModelRequest):                 mistral_messages.extend(self._map_user_message(message))             elif isinstance(message, ModelResponse):                 content_chunks: list[MistralContentChunk] = []                 thinking_chunks: list[MistralTextChunk | MistralReferenceChunk] = []                 tool_calls: list[MistralToolCall] = []                  for part in message.parts:                     if isinstance(part, TextPart):                         content_chunks.append(MistralTextChunk(text=part.content))                     elif isinstance(part, ThinkingPart):                         thinking_chunks.append(MistralTextChunk(text=part.content))                     elif isinstance(part, ToolCallPart):                         tool_calls.append(self._map_tool_call(part))                     elif isinstance(part, BuiltinToolCallPart | BuiltinToolReturnPart):  # pragma: no cover                         # This is currently never returned from mistral                         pass                     elif isinstance(part, FilePart):  # pragma: no cover                         # Files generated by models are not sent back to models that don't themselves generate files.                         pass                     else:                         assert_never(part)                 if thinking_chunks:                     content_chunks.insert(0, MistralThinkChunk(thinking=thinking_chunks))                 mistral_messages.append(MistralAssistantMessage(content=content_chunks, tool_calls=tool_calls))             else:                 assert_never(message)         if instructions := self._get_instructions(messages):             mistral_messages.insert(0, MistralSystemMessage(content=instructions))          # Post-process messages to insert fake assistant message after tool message if followed by user message         # to work around `Unexpected role 'user' after role 'tool'` error.         processed_messages: list[MistralMessages] = []         for i, current_message in enumerate(mistral_messages):             processed_messages.append(current_message)              if isinstance(current_message, MistralToolMessage) and i + 1 < len(mistral_messages):                 next_message = mistral_messages[i + 1]                 if isinstance(next_message, MistralUserMessage):                     # Insert a dummy assistant message                     processed_messages.append(MistralAssistantMessage(content=[MistralTextChunk(text='OK')]))          return processed_messages      def _map_user_prompt(self, part: UserPromptPart) -> MistralUserMessage:         content: str | list[MistralContentChunk]         if isinstance(part.content, str):             content = part.content         else:             content = []             for item in part.content:                 if isinstance(item, str):                     content.append(MistralTextChunk(text=item))                 elif isinstance(item, ImageUrl):                     content.append(MistralImageURLChunk(image_url=MistralImageURL(url=item.url)))                 elif isinstance(item, BinaryContent):                     if item.is_image:                         image_url = MistralImageURL(url=item.data_uri)                         content.append(MistralImageURLChunk(image_url=image_url, type='image_url'))                     elif item.media_type == 'application/pdf':                         content.append(MistralDocumentURLChunk(document_url=item.data_uri, type='document_url'))                     else:                         raise RuntimeError('BinaryContent other than image or PDF is not supported in Mistral.')                 elif isinstance(item, DocumentUrl):                     if item.media_type == 'application/pdf':                         content.append(MistralDocumentURLChunk(document_url=item.url, type='document_url'))                     else:                         raise RuntimeError('DocumentUrl other than PDF is not supported in Mistral.')                 elif isinstance(item, VideoUrl):                     raise RuntimeError('VideoUrl is not supported in Mistral.')                 else:  # pragma: no cover                     raise RuntimeError(f'Unsupported content type: {type(item)}')         return MistralUserMessage(content=content) ``` |", "url": "https://ai.pydantic.dev/models/mistral/index.html#mistralmodel-dataclass", "page": "models/mistral/index.html", "source_site": "pydantic_ai"}
{"title": "MistralModel `dataclass`", "anchor": "mistralmodel-dataclass", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    model_name: MistralModelName,\n    *,\n    provider: (\n        Literal[\"mistral\"] | Provider[Mistral]\n    ) = \"mistral\",\n    profile: ModelProfileSpec | None = None,\n    json_mode_schema_prompt: str = \"Answer in JSON Object, respect the format:\\n```\\n{schema}\\n```\\n\",\n    settings: ModelSettings | None = None\n)\n```\n\nInitialize a Mistral model.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model_name` | `MistralModelName` | The name of the model to use. | *required* |\n| `provider` | `Literal['mistral'] | Provider[Mistral]` | The provider to use for authentication and API access. Can be either the string 'mistral' or an instance of `Provider[Mistral]`. If not provided, a new provider will be created using the other parameters. | `'mistral'` |\n| `profile` | `ModelProfileSpec | None` | The model profile to use. Defaults to a profile picked by the provider based on the model name. | `None` |\n| `json_mode_schema_prompt` | `str` | The prompt to show when the model expects a JSON object as input. | ```` 'Answer in JSON Object, respect the format:\\n```\\n{schema}\\n```\\n' ```` |\n| `settings` | `ModelSettings | None` | Model-specific settings that will be used as defaults for this model. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/mistral.py`\n\n|  |  |\n| --- | --- |\n| ``` 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 ``` | ``` def __init__(     self,     model_name: MistralModelName,     *,     provider: Literal['mistral'] | Provider[Mistral] = 'mistral',     profile: ModelProfileSpec | None = None,     json_mode_schema_prompt: str = \"\"\"Answer in JSON Object, respect the format:\\n```\\n{schema}\\n```\\n\"\"\",     settings: ModelSettings | None = None, ):     \"\"\"Initialize a Mistral model.      Args:         model_name: The name of the model to use.         provider: The provider to use for authentication and API access. Can be either the string             'mistral' or an instance of `Provider[Mistral]`. If not provided, a new provider will be             created using the other parameters.         profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.         json_mode_schema_prompt: The prompt to show when the model expects a JSON object as input.         settings: Model-specific settings that will be used as defaults for this model.     \"\"\"     self._model_name = model_name     self.json_mode_schema_prompt = json_mode_schema_prompt      if isinstance(provider, str):         provider = infer_provider(provider)     self._provider = provider     self.client = provider.client      super().__init__(settings=settings, profile=profile or provider.model_profile) ``` |\n\n#### model\\_name `property`\n\n```\nmodel_name: MistralModelName\n```\n\nThe model name.\n\n#### system `property`\n\n```\nsystem: str\n```\n\nThe model provider.\n\n#### request `async`\n\n```\nrequest(\n    messages: list[ModelMessage],\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n) -> ModelResponse\n```\n\nMake a non-streaming request to the model from Pydantic AI call.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/mistral.py`\n\n|  |  |\n| --- | --- |\n| ``` 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 ``` | ``` async def request(     self,     messages: list[ModelMessage],     model_settings: ModelSettings | None,     model_request_parameters: ModelRequestParameters, ) -> ModelResponse:     \"\"\"Make a non-streaming request to the model from Pydantic AI call.\"\"\"     check_allow_model_requests()     model_settings, model_request_parameters = self.prepare_request(         model_settings,         model_request_parameters,     )     response = await self._completions_create(         messages, cast(MistralModelSettings, model_settings or {}), model_request_parameters     )     model_response = self._process_response(response)     return model_response ``` |\n\n#### request\\_stream `async`\n\n```\nrequest_stream(\n    messages: list[ModelMessage],\n    model_settings: ModelSettings | None,\n    model_request_parameters: ModelRequestParameters,\n    run_context: RunContext[Any] | None = None,\n) -> AsyncIterator[StreamedResponse]\n```\n\nMake a streaming request to the model from Pydantic AI call.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/mistral.py`", "url": "https://ai.pydantic.dev/models/mistral/index.html#mistralmodel-dataclass", "page": "models/mistral/index.html", "source_site": "pydantic_ai"}
{"title": "MistralModel `dataclass`", "anchor": "mistralmodel-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 ``` | ``` @asynccontextmanager async def request_stream(     self,     messages: list[ModelMessage],     model_settings: ModelSettings | None,     model_request_parameters: ModelRequestParameters,     run_context: RunContext[Any] | None = None, ) -> AsyncIterator[StreamedResponse]:     \"\"\"Make a streaming request to the model from Pydantic AI call.\"\"\"     check_allow_model_requests()     model_settings, model_request_parameters = self.prepare_request(         model_settings,         model_request_parameters,     )     response = await self._stream_completions_create(         messages, cast(MistralModelSettings, model_settings or {}), model_request_parameters     )     async with response:         yield await self._process_streamed_response(response, model_request_parameters) ``` |", "url": "https://ai.pydantic.dev/models/mistral/index.html#mistralmodel-dataclass", "page": "models/mistral/index.html", "source_site": "pydantic_ai"}
{"title": "MistralStreamedResponse `dataclass`", "anchor": "mistralstreamedresponse-dataclass", "md_text": "Bases: `StreamedResponse`\n\nImplementation of `StreamedResponse` for Mistral models.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/mistral.py`", "url": "https://ai.pydantic.dev/models/mistral/index.html#mistralstreamedresponse-dataclass", "page": "models/mistral/index.html", "source_site": "pydantic_ai"}
{"title": "MistralStreamedResponse `dataclass`", "anchor": "mistralstreamedresponse-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 ``` | ``` @dataclass class MistralStreamedResponse(StreamedResponse):     \"\"\"Implementation of `StreamedResponse` for Mistral models.\"\"\"      _model_name: MistralModelName     _response: AsyncIterable[MistralCompletionEvent]     _timestamp: datetime     _provider_name: str      _delta_content: str = field(default='', init=False)      async def _get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:         chunk: MistralCompletionEvent         async for chunk in self._response:             self._usage += _map_usage(chunk.data)              if chunk.data.id:  # pragma: no branch                 self.provider_response_id = chunk.data.id              try:                 choice = chunk.data.choices[0]             except IndexError:                 continue              if raw_finish_reason := choice.finish_reason:                 self.provider_details = {'finish_reason': raw_finish_reason}                 self.finish_reason = _FINISH_REASON_MAP.get(raw_finish_reason)              # Handle the text part of the response             content = choice.delta.content             text, thinking = _map_content(content)             for thought in thinking:                 self._parts_manager.handle_thinking_delta(vendor_part_id='thinking', content=thought)             if text:                 # Attempt to produce an output tool call from the received text                 output_tools = {c.name: c for c in self.model_request_parameters.output_tools}                 if output_tools:                     self._delta_content += text                     # TODO: Port to native \"manual JSON\" mode                     maybe_tool_call_part = self._try_get_output_tool_from_text(self._delta_content, output_tools)                     if maybe_tool_call_part:                         yield self._parts_manager.handle_tool_call_part(                             vendor_part_id='output',                             tool_name=maybe_tool_call_part.tool_name,                             args=maybe_tool_call_part.args_as_dict(),                             tool_call_id=maybe_tool_call_part.tool_call_id,                         )                 else:                     maybe_event = self._parts_manager.handle_text_delta(vendor_part_id='content', content=text)                     if maybe_event is not None:  # pragma: no branch                         yield maybe_event              # Handle the explicit tool calls             for index, dtc in enumerate(choice.delta.tool_calls or []):                 # It seems that mistral just sends full tool calls, so we just use them directly, rather than building                 yield self._parts_manager.handle_tool_call_part(                     vendor_part_id=index, tool_name=dtc.function.name, args=dtc.function.arguments, tool_call_id=dtc.id                 )      @property     def model_name(self) -> MistralModelName:         \"\"\"Get the model name of the response.\"\"\"         return self._model_name      @property     def provider_name(self) -> str:         \"\"\"Get the provider name.\"\"\"         return self._provider_name      @property     def timestamp(self) -> datetime:         \"\"\"Get the timestamp of the response.\"\"\"         return self._timestamp      @staticmethod     def _try_get_output_tool_from_text(text: str, output_tools: dict[str, ToolDefinition]) -> ToolCallPart | None:         output_json: dict[str, Any] | None = pydantic_core.from_json(text, allow_partial='trailing-strings')         if output_json:             for output_tool in output_tools.values():                 # NOTE: Additional verification to prevent JSON validation to crash                 # Ensures required parameters in the JSON schema are respected, especially for stream-based return types.                 # Example with BaseModel and required fields.                 if not MistralStreamedResponse._validate_required_json_schema(                     output_json, output_tool.parameters_json_schema                 ):                     continue                  # The following part_id will be thrown away                 return ToolCallPart(tool_name=output_tool.name, args=output_json)      @staticmethod     def _validate_required_json_schema(json_dict: dict[str, Any], json_schema: dict[str, Any]) -> bool:         \"\"\"Validate that all required parameters in the JSON schema are present in the JSON dictionary.\"\"\"         required_params = json_schema.get('required', [])         properties = json_schema.get('properties', {})          for param in required_params:             if param not in json_dict:                 return False              param_schema = properties.get(param, {})             param_type = param_schema.get('type')             param_items_type = param_schema.get('items', {}).get('type')              if param_type == 'array' and param_items_type:                 if not isinstance(json_dict[param], list):                     return False                 for item in json_dict[param]:                     if not isinstance(item, VALID_JSON_TYPE_MAPPING[param_items_type]):                         return False             elif param_type and not isinstance(json_dict[param], VALID_JSON_TYPE_MAPPING[param_type]):                 return False              if isinstance(json_dict[param], dict) and 'properties' in param_schema:                 nested_schema = param_schema                 if not MistralStreamedResponse._validate_required_json_schema(json_dict[param], nested_schema):                     return False          return True ``` |", "url": "https://ai.pydantic.dev/models/mistral/index.html#mistralstreamedresponse-dataclass", "page": "models/mistral/index.html", "source_site": "pydantic_ai"}
{"title": "MistralStreamedResponse `dataclass`", "anchor": "mistralstreamedresponse-dataclass", "md_text": "#### model\\_name `property`\n\n```\nmodel_name: MistralModelName\n```\n\nGet the model name of the response.\n\n#### provider\\_name `property`\n\n```\nprovider_name: str\n```\n\nGet the provider name.\n\n#### timestamp `property`\n\n```\ntimestamp: datetime\n```\n\nGet the timestamp of the response.", "url": "https://ai.pydantic.dev/models/mistral/index.html#mistralstreamedresponse-dataclass", "page": "models/mistral/index.html", "source_site": "pydantic_ai"}
{"title": "Setup", "anchor": "setup", "md_text": "For details on how to set up authentication with this model, see [model configuration for OpenAI](../../openai/index.html).", "url": "https://ai.pydantic.dev/models/openai/index.html#setup", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIModelName `module-attribute`", "anchor": "openaimodelname-module-attribute", "md_text": "```\nOpenAIModelName = str | AllModels\n```\n\nPossible OpenAI model names.\n\nSince OpenAI supports a variety of date-stamped models, we explicitly list the latest models but\nallow any name in the type hints.\nSee [the OpenAI docs](https://platform.openai.com/docs/models) for a full list.\n\nUsing this more broad type for the model name instead of the ChatModel definition\nallows this model to be used more easily with other model types (ie, Ollama, Deepseek).", "url": "https://ai.pydantic.dev/models/openai/index.html#openaimodelname-module-attribute", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIChatModelSettings", "anchor": "openaichatmodelsettings", "md_text": "Bases: `ModelSettings`\n\nSettings used for an OpenAI model request.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/openai.py`\n\n|  |  |\n| --- | --- |\n| ``` 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 ``` | ``` class OpenAIChatModelSettings(ModelSettings, total=False):     \"\"\"Settings used for an OpenAI model request.\"\"\"      # ALL FIELDS MUST BE `openai_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.      openai_reasoning_effort: ReasoningEffort     \"\"\"Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning).      Currently supported values are `low`, `medium`, and `high`. Reducing reasoning effort can     result in faster responses and fewer tokens used on reasoning in a response.     \"\"\"      openai_logprobs: bool     \"\"\"Include log probabilities in the response.\"\"\"      openai_top_logprobs: int     \"\"\"Include log probabilities of the top n tokens in the response.\"\"\"      openai_user: str     \"\"\"A unique identifier representing the end-user, which can help OpenAI monitor and detect abuse.      See [OpenAI's safety best practices](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids) for more details.     \"\"\"      openai_service_tier: Literal['auto', 'default', 'flex', 'priority']     \"\"\"The service tier to use for the model request.      Currently supported values are `auto`, `default`, `flex`, and `priority`.     For more information, see [OpenAI's service tiers documentation](https://platform.openai.com/docs/api-reference/chat/object#chat/object-service_tier).     \"\"\"      openai_prediction: ChatCompletionPredictionContentParam     \"\"\"Enables [predictive outputs](https://platform.openai.com/docs/guides/predicted-outputs).      This feature is currently only supported for some OpenAI models.     \"\"\" ``` |\n\n#### openai\\_reasoning\\_effort `instance-attribute`\n\n```\nopenai_reasoning_effort: ReasoningEffort\n```\n\nConstrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning).\n\nCurrently supported values are `low`, `medium`, and `high`. Reducing reasoning effort can\nresult in faster responses and fewer tokens used on reasoning in a response.\n\n#### openai\\_logprobs `instance-attribute`\n\n```\nopenai_logprobs: bool\n```\n\nInclude log probabilities in the response.\n\n#### openai\\_top\\_logprobs `instance-attribute`\n\n```\nopenai_top_logprobs: int\n```\n\nInclude log probabilities of the top n tokens in the response.\n\n#### openai\\_user `instance-attribute`\n\n```\nopenai_user: str\n```\n\nA unique identifier representing the end-user, which can help OpenAI monitor and detect abuse.\n\nSee [OpenAI's safety best practices](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids) for more details.\n\n#### openai\\_service\\_tier `instance-attribute`\n\n```\nopenai_service_tier: Literal[\n    \"auto\", \"default\", \"flex\", \"priority\"\n]\n```\n\nThe service tier to use for the model request.\n\nCurrently supported values are `auto`, `default`, `flex`, and `priority`.\nFor more information, see [OpenAI's service tiers documentation](https://platform.openai.com/docs/api-reference/chat/object#chat/object-service_tier).\n\n#### openai\\_prediction `instance-attribute`\n\n```\nopenai_prediction: ChatCompletionPredictionContentParam\n```\n\nEnables [predictive outputs](https://platform.openai.com/docs/guides/predicted-outputs).\n\nThis feature is currently only supported for some OpenAI models.", "url": "https://ai.pydantic.dev/models/openai/index.html#openaichatmodelsettings", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIModelSettings `deprecated`", "anchor": "openaimodelsettings-deprecated", "md_text": "Bases: `OpenAIChatModelSettings`\n\nDeprecated\n\nUse `OpenAIChatModelSettings` instead.\n\nDeprecated alias for `OpenAIChatModelSettings`.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/openai.py`\n\n|  |  |\n| --- | --- |\n| ``` 176 177 178 ``` | ``` @deprecated('Use `OpenAIChatModelSettings` instead.') class OpenAIModelSettings(OpenAIChatModelSettings, total=False):     \"\"\"Deprecated alias for `OpenAIChatModelSettings`.\"\"\" ``` |", "url": "https://ai.pydantic.dev/models/openai/index.html#openaimodelsettings-deprecated", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIResponsesModelSettings", "anchor": "openairesponsesmodelsettings", "md_text": "Bases: `OpenAIChatModelSettings`\n\nSettings used for an OpenAI Responses model request.\n\nALL FIELDS MUST BE `openai_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/openai.py`\n\n|  |  |\n| --- | --- |\n| ``` 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 ``` | ``` class OpenAIResponsesModelSettings(OpenAIChatModelSettings, total=False):     \"\"\"Settings used for an OpenAI Responses model request.      ALL FIELDS MUST BE `openai_` PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS.     \"\"\"      openai_builtin_tools: Sequence[FileSearchToolParam | WebSearchToolParam | ComputerToolParam]     \"\"\"The provided OpenAI built-in tools to use.      See [OpenAI's built-in tools](https://platform.openai.com/docs/guides/tools?api-mode=responses) for more details.     \"\"\"      openai_reasoning_generate_summary: Literal['detailed', 'concise']     \"\"\"Deprecated alias for `openai_reasoning_summary`.\"\"\"      openai_reasoning_summary: Literal['detailed', 'concise']     \"\"\"A summary of the reasoning performed by the model.      This can be useful for debugging and understanding the model's reasoning process.     One of `concise` or `detailed`.      Check the [OpenAI Reasoning documentation](https://platform.openai.com/docs/guides/reasoning?api-mode=responses#reasoning-summaries)     for more details.     \"\"\"      openai_send_reasoning_ids: bool     \"\"\"Whether to send the unique IDs of reasoning, text, and function call parts from the message history to the model. Enabled by default for reasoning models.      This can result in errors like `\"Item 'rs_123' of type 'reasoning' was provided without its required following item.\"`     if the message history you're sending does not match exactly what was received from the Responses API in a previous response,     for example if you're using a [history processor](../../message-history.md#processing-message-history).     In that case, you'll want to disable this.     \"\"\"      openai_truncation: Literal['disabled', 'auto']     \"\"\"The truncation strategy to use for the model response.      It can be either:     - `disabled` (default): If a model response will exceed the context window size for a model, the         request will fail with a 400 error.     - `auto`: If the context of this response and previous ones exceeds the model's context window size,         the model will truncate the response to fit the context window by dropping input items in the         middle of the conversation.     \"\"\"      openai_text_verbosity: Literal['low', 'medium', 'high']     \"\"\"Constrains the verbosity of the model's text response.      Lower values will result in more concise responses, while higher values will     result in more verbose responses. Currently supported values are `low`,     `medium`, and `high`.     \"\"\"      openai_previous_response_id: Literal['auto'] | str     \"\"\"The ID of a previous response from the model to use as the starting point for a continued conversation.      When set to `'auto'`, the request automatically uses the most recent     `provider_response_id` from the message history and omits earlier messages.      This enables the model to use server-side conversation state and faithfully reference previous reasoning.     See the [OpenAI Responses API documentation](https://platform.openai.com/docs/guides/reasoning#keeping-reasoning-items-in-context)     for more information.     \"\"\"      openai_include_code_execution_outputs: bool     \"\"\"Whether to include the code execution results in the response.      Corresponds to the `code_interpreter_call.outputs` value of the `include` parameter in the Responses API.     \"\"\"      openai_include_web_search_sources: bool     \"\"\"Whether to include the web search results in the response.      Corresponds to the `web_search_call.action.sources` value of the `include` parameter in the Responses API.     \"\"\" ``` |\n\n#### openai\\_builtin\\_tools `instance-attribute`\n\n```\nopenai_builtin_tools: Sequence[\n    FileSearchToolParam\n    | WebSearchToolParam\n    | ComputerToolParam\n]\n```\n\nThe provided OpenAI built-in tools to use.\n\nSee [OpenAI's built-in tools](https://platform.openai.com/docs/guides/tools?api-mode=responses) for more details.\n\n#### openai\\_reasoning\\_generate\\_summary `instance-attribute`\n\n```\nopenai_reasoning_generate_summary: Literal[\n    \"detailed\", \"concise\"\n]\n```\n\nDeprecated alias for `openai_reasoning_summary`.\n\n#### openai\\_reasoning\\_summary `instance-attribute`\n\n```\nopenai_reasoning_summary: Literal['detailed', 'concise']\n```\n\nA summary of the reasoning performed by the model.", "url": "https://ai.pydantic.dev/models/openai/index.html#openairesponsesmodelsettings", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIResponsesModelSettings", "anchor": "openairesponsesmodelsettings", "md_text": "This can be useful for debugging and understanding the model's reasoning process.\nOne of `concise` or `detailed`.\n\nCheck the [OpenAI Reasoning documentation](https://platform.openai.com/docs/guides/reasoning?api-mode=responses#reasoning-summaries)\nfor more details.\n\n#### openai\\_send\\_reasoning\\_ids `instance-attribute`\n\n```\nopenai_send_reasoning_ids: bool\n```\n\nWhether to send the unique IDs of reasoning, text, and function call parts from the message history to the model. Enabled by default for reasoning models.\n\nThis can result in errors like `\"Item 'rs_123' of type 'reasoning' was provided without its required following item.\"`\nif the message history you're sending does not match exactly what was received from the Responses API in a previous response,\nfor example if you're using a [history processor](https://ai.pydantic.dev/message-history/#processing-message-history).\nIn that case, you'll want to disable this.\n\n#### openai\\_truncation `instance-attribute`\n\n```\nopenai_truncation: Literal['disabled', 'auto']\n```\n\nThe truncation strategy to use for the model response.\n\nIt can be either:\n- `disabled` (default): If a model response will exceed the context window size for a model, the\nrequest will fail with a 400 error.\n- `auto`: If the context of this response and previous ones exceeds the model's context window size,\nthe model will truncate the response to fit the context window by dropping input items in the\nmiddle of the conversation.\n\n#### openai\\_text\\_verbosity `instance-attribute`\n\n```\nopenai_text_verbosity: Literal['low', 'medium', 'high']\n```\n\nConstrains the verbosity of the model's text response.\n\nLower values will result in more concise responses, while higher values will\nresult in more verbose responses. Currently supported values are `low`,\n`medium`, and `high`.\n\n#### openai\\_previous\\_response\\_id `instance-attribute`\n\n```\nopenai_previous_response_id: Literal['auto'] | str\n```\n\nThe ID of a previous response from the model to use as the starting point for a continued conversation.\n\nWhen set to `'auto'`, the request automatically uses the most recent\n`provider_response_id` from the message history and omits earlier messages.\n\nThis enables the model to use server-side conversation state and faithfully reference previous reasoning.\nSee the [OpenAI Responses API documentation](https://platform.openai.com/docs/guides/reasoning#keeping-reasoning-items-in-context)\nfor more information.\n\n#### openai\\_include\\_code\\_execution\\_outputs `instance-attribute`\n\n```\nopenai_include_code_execution_outputs: bool\n```\n\nWhether to include the code execution results in the response.\n\nCorresponds to the `code_interpreter_call.outputs` value of the `include` parameter in the Responses API.\n\n#### openai\\_include\\_web\\_search\\_sources `instance-attribute`\n\n```\nopenai_include_web_search_sources: bool\n```\n\nWhether to include the web search results in the response.\n\nCorresponds to the `web_search_call.action.sources` value of the `include` parameter in the Responses API.", "url": "https://ai.pydantic.dev/models/openai/index.html#openairesponsesmodelsettings", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIChatModel `dataclass`", "anchor": "openaichatmodel-dataclass", "md_text": "Bases: `Model`\n\nA model that uses the OpenAI API.\n\nInternally, this uses the [OpenAI Python client](https://github.com/openai/openai-python) to interact with the API.\n\nApart from `__init__`, all methods are private or match those of the base class.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/openai.py`", "url": "https://ai.pydantic.dev/models/openai/index.html#openaichatmodel-dataclass", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIChatModel `dataclass`", "anchor": "openaichatmodel-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 803 804 805 806 807 808 809 810 811 812 813 814 815 816 817 818 819 820 821 822 823 824 825 826 827 828 829 830 831 832 833 834 835 836 837 838 839 840 841 842 843 844 845 846 847 848 849 850 851 852 853 854 855 856 857 858 859 860 861 862 863 864 865 866 867 868 869 870 871 872 873 874 875 876 877 878 879 880 881 882 883 884 885 ``` | ``` @dataclass(init=False) class OpenAIChatModel(Model):     \"\"\"A model that uses the OpenAI API.      Internally, this uses the [OpenAI Python client](https://github.com/openai/openai-python) to interact with the API.      Apart from `__init__`, all methods are private or match those of the base class.     \"\"\"      client: AsyncOpenAI = field(repr=False)      _model_name: OpenAIModelName = field(repr=False)     _provider: Provider[AsyncOpenAI] = field(repr=False)      @overload     def __init__(         self,         model_name: OpenAIModelName,         *,         provider: Literal[             'azure',             'deepseek',             'cerebras',             'fireworks',             'github',             'grok',             'heroku',             'moonshotai',             'ollama',             'openai',             'openai-chat',             'openrouter',             'together',             'vercel',             'litellm',             'nebius',             'ovhcloud',             'gateway',         ]         | Provider[AsyncOpenAI] = 'openai',         profile: ModelProfileSpec | None = None,         settings: ModelSettings | None = None,     ) -> None: ...      @deprecated('Set the `system_prompt_role` in the `OpenAIModelProfile` instead.')     @overload     def __init__(         self,         model_name: OpenAIModelName,         *,         provider: Literal[             'azure',             'deepseek',             'cerebras',             'fireworks',             'github',             'grok',             'heroku',             'moonshotai',             'ollama',             'openai',             'openai-chat',             'openrouter',             'together',             'vercel',             'litellm',             'nebius',             'ovhcloud',             'gateway',         ]         | Provider[AsyncOpenAI] = 'openai',         profile: ModelProfileSpec | None = None,         system_prompt_role: OpenAISystemPromptRole | None = None,         settings: ModelSettings | None = None,     ) -> None: ...      def __init__(         self,         model_name: OpenAIModelName,         *,         provider: Literal[             'azure',             'deepseek',             'cerebras',             'fireworks',             'github',             'grok',             'heroku',             'moonshotai',             'ollama',             'openai',             'openai-chat',             'openrouter',             'together',             'vercel',             'litellm',             'nebius',             'ovhcloud',             'gateway',         ]         | Provider[AsyncOpenAI] = 'openai',         profile: ModelProfileSpec | None = None,         system_prompt_role: OpenAISystemPromptRole | None = None,         settings: ModelSettings | None = None,     ):         \"\"\"Initialize an OpenAI model.          Args:             model_name: The name of the OpenAI model to use. List of model names available                 [here](https://github.com/openai/openai-python/blob/v1.54.3/src/openai/types/chat_model.py#L7)                 (Unfortunately, despite being ask to do so, OpenAI do not provide `.inv` files for their API).             provider: The provider to use. Defaults to `'openai'`.             profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.             system_prompt_role: The role to use for the system prompt message. If not provided, defaults to `'system'`.                 In the future, this may be inferred from the model name.             settings: Default model settings for this model instance.         \"\"\"         self._model_name = model_name          if isinstance(provider, str):             provider = infer_provider('gateway/openai' if provider == 'gateway' else provider)         self._provider = provider         self.client = provider.client          super().__init__(settings=settings, profile=profile or provider.model_profile)          if system_prompt_role is not None:             self.profile = OpenAIModelProfile(openai_system_prompt_role=system_prompt_role).update(self.profile)      @property     def base_url(self) -> str:         return str(self.client.base_url)      @property     def model_name(self) -> OpenAIModelName:         \"\"\"The model name.\"\"\"         return self._model_name      @property     def system(self) -> str:         \"\"\"The model provider.\"\"\"         return self._provider.name      @property     @deprecated('Set the `system_prompt_role` in the `OpenAIModelProfile` instead.')     def system_prompt_role(self) -> OpenAISystemPromptRole | None:         return OpenAIModelProfile.from_profile(self.profile).openai_system_prompt_role      async def request(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> ModelResponse:         check_allow_model_requests()         model_settings, model_request_parameters = self.prepare_request(             model_settings,             model_request_parameters,         )         response = await self._completions_create(             messages, False, cast(OpenAIChatModelSettings, model_settings or {}), model_request_parameters         )         model_response = self._process_response(response)         return model_response      @asynccontextmanager     async def request_stream(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,         run_context: RunContext[Any] | None = None,     ) -> AsyncIterator[StreamedResponse]:         check_allow_model_requests()         model_settings, model_request_parameters = self.prepare_request(             model_settings,             model_request_parameters,         )         response = await self._completions_create(             messages, True, cast(OpenAIChatModelSettings, model_settings or {}), model_request_parameters         )         async with response:             yield await self._process_streamed_response(response, model_request_parameters)      @overload     async def _completions_create(         self,         messages: list[ModelMessage],         stream: Literal[True],         model_settings: OpenAIChatModelSettings,         model_request_parameters: ModelRequestParameters,     ) -> AsyncStream[ChatCompletionChunk]: ...      @overload     async def _completions_create(         self,         messages: list[ModelMessage],         stream: Literal[False],         model_settings: OpenAIChatModelSettings,         model_request_parameters: ModelRequestParameters,     ) -> chat.ChatCompletion: ...      async def _completions_create(         self,         messages: list[ModelMessage],         stream: bool,         model_settings: OpenAIChatModelSettings,         model_request_parameters: ModelRequestParameters,     ) -> chat.ChatCompletion | AsyncStream[ChatCompletionChunk]:         tools = self._get_tools(model_request_parameters)         web_search_options = self._get_web_search_options(model_request_parameters)          if not tools:             tool_choice: Literal['none', 'required', 'auto'] | None = None         elif (             not model_request_parameters.allow_text_output             and OpenAIModelProfile.from_profile(self.profile).openai_supports_tool_choice_required         ):             tool_choice = 'required'         else:             tool_choice = 'auto'          openai_messages = await self._map_messages(messages)          response_format: chat.completion_create_params.ResponseFormat | None = None         if model_request_parameters.output_mode == 'native':             output_object = model_request_parameters.output_object             assert output_object is not None             response_format = self._map_json_schema(output_object)         elif (             model_request_parameters.output_mode == 'prompted' and self.profile.supports_json_object_output         ):  # pragma: no branch             response_format = {'type': 'json_object'}          unsupported_model_settings = OpenAIModelProfile.from_profile(self.profile).openai_unsupported_model_settings         for setting in unsupported_model_settings:             model_settings.pop(setting, None)          try:             extra_headers = model_settings.get('extra_headers', {})             extra_headers.setdefault('User-Agent', get_user_agent())             return await self.client.chat.completions.create(                 model=self._model_name,                 messages=openai_messages,                 parallel_tool_calls=model_settings.get('parallel_tool_calls', NOT_GIVEN),                 tools=tools or NOT_GIVEN,                 tool_choice=tool_choice or NOT_GIVEN,                 stream=stream,                 stream_options={'include_usage': True} if stream else NOT_GIVEN,                 stop=model_settings.get('stop_sequences', NOT_GIVEN),                 max_completion_tokens=model_settings.get('max_tokens', NOT_GIVEN),                 timeout=model_settings.get('timeout', NOT_GIVEN),                 response_format=response_format or NOT_GIVEN,                 seed=model_settings.get('seed', NOT_GIVEN),                 reasoning_effort=model_settings.get('openai_reasoning_effort', NOT_GIVEN),                 user=model_settings.get('openai_user', NOT_GIVEN),                 web_search_options=web_search_options or NOT_GIVEN,                 service_tier=model_settings.get('openai_service_tier', NOT_GIVEN),                 prediction=model_settings.get('openai_prediction', NOT_GIVEN),                 temperature=model_settings.get('temperature', NOT_GIVEN),                 top_p=model_settings.get('top_p', NOT_GIVEN),                 presence_penalty=model_settings.get('presence_penalty', NOT_GIVEN),                 frequency_penalty=model_settings.get('frequency_penalty', NOT_GIVEN),                 logit_bias=model_settings.get('logit_bias', NOT_GIVEN),                 logprobs=model_settings.get('openai_logprobs', NOT_GIVEN),                 top_logprobs=model_settings.get('openai_top_logprobs', NOT_GIVEN),                 extra_headers=extra_headers,                 extra_body=model_settings.get('extra_body'),             )         except APIStatusError as e:             if (status_code := e.status_code) >= 400:                 raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e             raise  # pragma: lax no cover      def _process_response(self, response: chat.ChatCompletion | str) -> ModelResponse:         \"\"\"Process a non-streamed response, and prepare a message to return.\"\"\"         # Although the OpenAI SDK claims to return a Pydantic model (`ChatCompletion`) from the chat completions function:         # * it hasn't actually performed validation (presumably they're creating the model with `model_construct` or something?!)         # * if the endpoint returns plain text, the return type is a string         # Thus we validate it fully here.         if not isinstance(response, chat.ChatCompletion):             raise UnexpectedModelBehavior('Invalid response from OpenAI chat completions endpoint, expected JSON data')          if response.created:             timestamp = number_to_datetime(response.created)         else:             timestamp = _now_utc()             response.created = int(timestamp.timestamp())          # Workaround for local Ollama which sometimes returns a `None` finish reason.         if response.choices and (choice := response.choices[0]) and choice.finish_reason is None:  # pyright: ignore[reportUnnecessaryComparison]             choice.finish_reason = 'stop'          try:             response = chat.ChatCompletion.model_validate(response.model_dump())         except ValidationError as e:             raise UnexpectedModelBehavior(f'Invalid response from OpenAI chat completions endpoint: {e}') from e          choice = response.choices[0]         items: list[ModelResponsePart] = []          # The `reasoning_content` field is only present in DeepSeek models.         # https://api-docs.deepseek.com/guides/reasoning_model         if reasoning_content := getattr(choice.message, 'reasoning_content', None):             items.append(ThinkingPart(id='reasoning_content', content=reasoning_content, provider_name=self.system))          # The `reasoning` field is only present in gpt-oss via Ollama and OpenRouter.         # - https://cookbook.openai.com/articles/gpt-oss/handle-raw-cot#chat-completions-api         # - https://openrouter.ai/docs/use-cases/reasoning-tokens#basic-usage-with-reasoning-tokens         if reasoning := getattr(choice.message, 'reasoning', None):             items.append(ThinkingPart(id='reasoning', content=reasoning, provider_name=self.system))          # NOTE: We don't currently handle OpenRouter `reasoning_details`:         # - https://openrouter.ai/docs/use-cases/reasoning-tokens#preserving-reasoning-blocks         # If you need this, please file an issue.          if choice.message.content:             items.extend(                 (replace(part, id='content', provider_name=self.system) if isinstance(part, ThinkingPart) else part)                 for part in split_content_into_text_and_thinking(choice.message.content, self.profile.thinking_tags)             )         if choice.message.tool_calls is not None:             for c in choice.message.tool_calls:                 if isinstance(c, ChatCompletionMessageFunctionToolCall):                     part = ToolCallPart(c.function.name, c.function.arguments, tool_call_id=c.id)                 elif isinstance(c, ChatCompletionMessageCustomToolCall):  # pragma: no cover                     # NOTE: Custom tool calls are not supported.                     # See <https://github.com/pydantic/pydantic-ai/issues/2513> for more details.                     raise RuntimeError('Custom tool calls are not supported')                 else:                     assert_never(c)                 part.tool_call_id = _guard_tool_call_id(part)                 items.append(part)          vendor_details: dict[str, Any] = {}          # Add logprobs to vendor_details if available         if choice.logprobs is not None and choice.logprobs.content:             # Convert logprobs to a serializable format             vendor_details['logprobs'] = [                 {                     'token': lp.token,                     'bytes': lp.bytes,                     'logprob': lp.logprob,                     'top_logprobs': [                         {'token': tlp.token, 'bytes': tlp.bytes, 'logprob': tlp.logprob} for tlp in lp.top_logprobs                     ],                 }                 for lp in choice.logprobs.content             ]          raw_finish_reason = choice.finish_reason         vendor_details['finish_reason'] = raw_finish_reason         finish_reason = _CHAT_FINISH_REASON_MAP.get(raw_finish_reason)          return ModelResponse(             parts=items,             usage=_map_usage(response, self._provider.name, self._provider.base_url, self._model_name),             model_name=response.model,             timestamp=timestamp,             provider_details=vendor_details or None,             provider_response_id=response.id,             provider_name=self._provider.name,             finish_reason=finish_reason,         )      async def _process_streamed_response(         self, response: AsyncStream[ChatCompletionChunk], model_request_parameters: ModelRequestParameters     ) -> OpenAIStreamedResponse:         \"\"\"Process a streamed response, and prepare a streaming response to return.\"\"\"         peekable_response = _utils.PeekableAsyncStream(response)         first_chunk = await peekable_response.peek()         if isinstance(first_chunk, _utils.Unset):             raise UnexpectedModelBehavior(  # pragma: no cover                 'Streamed response ended without content or tool calls'             )          # When using Azure OpenAI and a content filter is enabled, the first chunk will contain a `''` model name,         # so we set it from a later chunk in `OpenAIChatStreamedResponse`.         model_name = first_chunk.model or self._model_name          return OpenAIStreamedResponse(             model_request_parameters=model_request_parameters,             _model_name=model_name,             _model_profile=self.profile,             _response=peekable_response,             _timestamp=number_to_datetime(first_chunk.created),             _provider_name=self._provider.name,             _provider_url=self._provider.base_url,         )      def _get_tools(self, model_request_parameters: ModelRequestParameters) -> list[chat.ChatCompletionToolParam]:         return [self._map_tool_definition(r) for r in model_request_parameters.tool_defs.values()]      def _get_web_search_options(self, model_request_parameters: ModelRequestParameters) -> WebSearchOptions | None:         for tool in model_request_parameters.builtin_tools:             if isinstance(tool, WebSearchTool):  # pragma: no branch                 if not OpenAIModelProfile.from_profile(self.profile).openai_chat_supports_web_search:                     raise UserError(                         f'WebSearchTool is not supported with `OpenAIChatModel` and model {self.model_name!r}. '                         f'Please use `OpenAIResponsesModel` instead.'                     )                  if tool.user_location:                     return WebSearchOptions(                         search_context_size=tool.search_context_size,                         user_location=WebSearchOptionsUserLocation(                             type='approximate',                             approximate=WebSearchOptionsUserLocationApproximate(**tool.user_location),                         ),                     )                 return WebSearchOptions(search_context_size=tool.search_context_size)             else:                 raise UserError(                     f'`{tool.__class__.__name__}` is not supported by `OpenAIChatModel`. If it should be, please file an issue.'                 )      async def _map_messages(self, messages: list[ModelMessage]) -> list[chat.ChatCompletionMessageParam]:         \"\"\"Just maps a `pydantic_ai.Message` to a `openai.types.ChatCompletionMessageParam`.\"\"\"         openai_messages: list[chat.ChatCompletionMessageParam] = []         for message in messages:             if isinstance(message, ModelRequest):                 async for item in self._map_user_message(message):                     openai_messages.append(item)             elif isinstance(message, ModelResponse):                 texts: list[str] = []                 tool_calls: list[ChatCompletionMessageFunctionToolCallParam] = []                 for item in message.parts:                     if isinstance(item, TextPart):                         texts.append(item.content)                     elif isinstance(item, ThinkingPart):                         # NOTE: DeepSeek `reasoning_content` field should NOT be sent back per https://api-docs.deepseek.com/guides/reasoning_model,                         # but we currently just send it in `<think>` tags anyway as we don't want DeepSeek-specific checks here.                         # If you need this changed, please file an issue.                         start_tag, end_tag = self.profile.thinking_tags                         texts.append('\\n'.join([start_tag, item.content, end_tag]))                     elif isinstance(item, ToolCallPart):                         tool_calls.append(self._map_tool_call(item))                     # OpenAI doesn't return built-in tool calls                     elif isinstance(item, BuiltinToolCallPart | BuiltinToolReturnPart):  # pragma: no cover                         pass                     elif isinstance(item, FilePart):  # pragma: no cover                         # Files generated by models are not sent back to models that don't themselves generate files.                         pass                     else:                         assert_never(item)                 message_param = chat.ChatCompletionAssistantMessageParam(role='assistant')                 if texts:                     # Note: model responses from this model should only have one text item, so the following                     # shouldn't merge multiple texts into one unless you switch models between runs:                     message_param['content'] = '\\n\\n'.join(texts)                 else:                     message_param['content'] = None                 if tool_calls:                     message_param['tool_calls'] = tool_calls                 openai_messages.append(message_param)             else:                 assert_never(message)         if instructions := self._get_instructions(messages):             openai_messages.insert(0, chat.ChatCompletionSystemMessageParam(content=instructions, role='system'))         return openai_messages      @staticmethod     def _map_tool_call(t: ToolCallPart) -> ChatCompletionMessageFunctionToolCallParam:         return ChatCompletionMessageFunctionToolCallParam(             id=_guard_tool_call_id(t=t),             type='function',             function={'name': t.tool_name, 'arguments': t.args_as_json_str()},         )      def _map_json_schema(self, o: OutputObjectDefinition) -> chat.completion_create_params.ResponseFormat:         response_format_param: chat.completion_create_params.ResponseFormatJSONSchema = {  # pyright: ignore[reportPrivateImportUsage]             'type': 'json_schema',             'json_schema': {'name': o.name or DEFAULT_OUTPUT_TOOL_NAME, 'schema': o.json_schema},         }         if o.description:             response_format_param['json_schema']['description'] = o.description         if OpenAIModelProfile.from_profile(self.profile).openai_supports_strict_tool_definition:  # pragma: no branch             response_format_param['json_schema']['strict'] = o.strict         return response_format_param      def _map_tool_definition(self, f: ToolDefinition) -> chat.ChatCompletionToolParam:         tool_param: chat.ChatCompletionToolParam = {             'type': 'function',             'function': {                 'name': f.name,                 'description': f.description or '',                 'parameters': f.parameters_json_schema,             },         }         if f.strict and OpenAIModelProfile.from_profile(self.profile).openai_supports_strict_tool_definition:             tool_param['function']['strict'] = f.strict         return tool_param      async def _map_user_message(self, message: ModelRequest) -> AsyncIterable[chat.ChatCompletionMessageParam]:         for part in message.parts:             if isinstance(part, SystemPromptPart):                 system_prompt_role = OpenAIModelProfile.from_profile(self.profile).openai_system_prompt_role                 if system_prompt_role == 'developer':                     yield chat.ChatCompletionDeveloperMessageParam(role='developer', content=part.content)                 elif system_prompt_role == 'user':                     yield chat.ChatCompletionUserMessageParam(role='user', content=part.content)                 else:                     yield chat.ChatCompletionSystemMessageParam(role='system', content=part.content)             elif isinstance(part, UserPromptPart):                 yield await self._map_user_prompt(part)             elif isinstance(part, ToolReturnPart):                 yield chat.ChatCompletionToolMessageParam(                     role='tool',                     tool_call_id=_guard_tool_call_id(t=part),                     content=part.model_response_str(),                 )             elif isinstance(part, RetryPromptPart):                 if part.tool_name is None:                     yield chat.ChatCompletionUserMessageParam(role='user', content=part.model_response())                 else:                     yield chat.ChatCompletionToolMessageParam(                         role='tool',                         tool_call_id=_guard_tool_call_id(t=part),                         content=part.model_response(),                     )             else:                 assert_never(part)      async def _map_user_prompt(self, part: UserPromptPart) -> chat.ChatCompletionUserMessageParam:  # noqa: C901         content: str | list[ChatCompletionContentPartParam]         if isinstance(part.content, str):             content = part.content         else:             content = []             for item in part.content:                 if isinstance(item, str):                     content.append(ChatCompletionContentPartTextParam(text=item, type='text'))                 elif isinstance(item, ImageUrl):                     image_url: ImageURL = {'url': item.url}                     if metadata := item.vendor_metadata:                         image_url['detail'] = metadata.get('detail', 'auto')                     if item.force_download:                         image_content = await download_item(item, data_format='base64_uri', type_format='extension')                         image_url['url'] = image_content['data']                     content.append(ChatCompletionContentPartImageParam(image_url=image_url, type='image_url'))                 elif isinstance(item, BinaryContent):                     if self._is_text_like_media_type(item.media_type):                         # Inline text-like binary content as a text block                         content.append(                             self._inline_text_file_part(                                 item.data.decode('utf-8'),                                 media_type=item.media_type,                                 identifier=item.identifier,                             )                         )                     elif item.is_image:                         image_url = ImageURL(url=item.data_uri)                         if metadata := item.vendor_metadata:                             image_url['detail'] = metadata.get('detail', 'auto')                         content.append(ChatCompletionContentPartImageParam(image_url=image_url, type='image_url'))                     elif item.is_audio:                         assert item.format in ('wav', 'mp3')                         audio = InputAudio(data=base64.b64encode(item.data).decode('utf-8'), format=item.format)                         content.append(ChatCompletionContentPartInputAudioParam(input_audio=audio, type='input_audio'))                     elif item.is_document:                         content.append(                             File(                                 file=FileFile(                                     file_data=item.data_uri,                                     filename=f'filename.{item.format}',                                 ),                                 type='file',                             )                         )                     else:  # pragma: no cover                         raise RuntimeError(f'Unsupported binary content type: {item.media_type}')                 elif isinstance(item, AudioUrl):                     downloaded_item = await download_item(item, data_format='base64', type_format='extension')                     assert downloaded_item['data_type'] in (                         'wav',                         'mp3',                     ), f'Unsupported audio format: {downloaded_item[\"data_type\"]}'                     audio = InputAudio(data=downloaded_item['data'], format=downloaded_item['data_type'])                     content.append(ChatCompletionContentPartInputAudioParam(input_audio=audio, type='input_audio'))                 elif isinstance(item, DocumentUrl):                     if self._is_text_like_media_type(item.media_type):                         downloaded_text = await download_item(item, data_format='text')                         content.append(                             self._inline_text_file_part(                                 downloaded_text['data'],                                 media_type=item.media_type,                                 identifier=item.identifier,                             )                         )                     else:                         downloaded_item = await download_item(item, data_format='base64_uri', type_format='extension')                         content.append(                             File(                                 file=FileFile(                                     file_data=downloaded_item['data'],                                     filename=f'filename.{downloaded_item[\"data_type\"]}',                                 ),                                 type='file',                             )                         )                 elif isinstance(item, VideoUrl):  # pragma: no cover                     raise NotImplementedError('VideoUrl is not supported for OpenAI')                 else:                     assert_never(item)         return chat.ChatCompletionUserMessageParam(role='user', content=content)      @staticmethod     def _is_text_like_media_type(media_type: str) -> bool:         return (             media_type.startswith('text/')             or media_type == 'application/json'             or media_type.endswith('+json')             or media_type == 'application/xml'             or media_type.endswith('+xml')             or media_type in ('application/x-yaml', 'application/yaml')         )      @staticmethod     def _inline_text_file_part(text: str, *, media_type: str, identifier: str) -> ChatCompletionContentPartTextParam:         text = '\\n'.join(             [                 f'-----BEGIN FILE id=\"{identifier}\" type=\"{media_type}\"-----',                 text,                 f'-----END FILE id=\"{identifier}\"-----',             ]         )         return ChatCompletionContentPartTextParam(text=text, type='text') ``` |", "url": "https://ai.pydantic.dev/models/openai/index.html#openaichatmodel-dataclass", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIChatModel `dataclass`", "anchor": "openaichatmodel-dataclass", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    model_name: OpenAIModelName,\n    *,\n    provider: (\n        Literal[\n            \"azure\",\n            \"deepseek\",\n            \"cerebras\",\n            \"fireworks\",\n            \"github\",\n            \"grok\",\n            \"heroku\",\n            \"moonshotai\",\n            \"ollama\",\n            \"openai\",\n            \"openai-chat\",\n            \"openrouter\",\n            \"together\",\n            \"vercel\",\n            \"litellm\",\n            \"nebius\",\n            \"ovhcloud\",\n            \"gateway\",\n        ]\n        | Provider[AsyncOpenAI]\n    ) = \"openai\",\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n) -> None\n\n__init__(\n    model_name: OpenAIModelName,\n    *,\n    provider: (\n        Literal[\n            \"azure\",\n            \"deepseek\",\n            \"cerebras\",\n            \"fireworks\",\n            \"github\",\n            \"grok\",\n            \"heroku\",\n            \"moonshotai\",\n            \"ollama\",\n            \"openai\",\n            \"openai-chat\",\n            \"openrouter\",\n            \"together\",\n            \"vercel\",\n            \"litellm\",\n            \"nebius\",\n            \"ovhcloud\",\n            \"gateway\",\n        ]\n        | Provider[AsyncOpenAI]\n    ) = \"openai\",\n    profile: ModelProfileSpec | None = None,\n    system_prompt_role: (\n        OpenAISystemPromptRole | None\n    ) = None,\n    settings: ModelSettings | None = None\n) -> None\n\n__init__(\n    model_name: OpenAIModelName,\n    *,\n    provider: (\n        Literal[\n            \"azure\",\n            \"deepseek\",\n            \"cerebras\",\n            \"fireworks\",\n            \"github\",\n            \"grok\",\n            \"heroku\",\n            \"moonshotai\",\n            \"ollama\",\n            \"openai\",\n            \"openai-chat\",\n            \"openrouter\",\n            \"together\",\n            \"vercel\",\n            \"litellm\",\n            \"nebius\",\n            \"ovhcloud\",\n            \"gateway\",\n        ]\n        | Provider[AsyncOpenAI]\n    ) = \"openai\",\n    profile: ModelProfileSpec | None = None,\n    system_prompt_role: (\n        OpenAISystemPromptRole | None\n    ) = None,\n    settings: ModelSettings | None = None\n)\n```\n\nInitialize an OpenAI model.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model_name` | `OpenAIModelName` | The name of the OpenAI model to use. List of model names available [here](https://github.com/openai/openai-python/blob/v1.54.3/src/openai/types/chat_model.py#L7) (Unfortunately, despite being ask to do so, OpenAI do not provide `.inv` files for their API). | *required* |\n| `provider` | `Literal['azure', 'deepseek', 'cerebras', 'fireworks', 'github', 'grok', 'heroku', 'moonshotai', 'ollama', 'openai', 'openai-chat', 'openrouter', 'together', 'vercel', 'litellm', 'nebius', 'ovhcloud', 'gateway'] | Provider[AsyncOpenAI]` | The provider to use. Defaults to `'openai'`. | `'openai'` |\n| `profile` | `ModelProfileSpec | None` | The model profile to use. Defaults to a profile picked by the provider based on the model name. | `None` |\n| `system_prompt_role` | `OpenAISystemPromptRole | None` | The role to use for the system prompt message. If not provided, defaults to `'system'`. In the future, this may be inferred from the model name. | `None` |\n| `settings` | `ModelSettings | None` | Default model settings for this model instance. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/openai.py`", "url": "https://ai.pydantic.dev/models/openai/index.html#openaichatmodel-dataclass", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIChatModel `dataclass`", "anchor": "openaichatmodel-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 ``` | ``` def __init__(     self,     model_name: OpenAIModelName,     *,     provider: Literal[         'azure',         'deepseek',         'cerebras',         'fireworks',         'github',         'grok',         'heroku',         'moonshotai',         'ollama',         'openai',         'openai-chat',         'openrouter',         'together',         'vercel',         'litellm',         'nebius',         'ovhcloud',         'gateway',     ]     | Provider[AsyncOpenAI] = 'openai',     profile: ModelProfileSpec | None = None,     system_prompt_role: OpenAISystemPromptRole | None = None,     settings: ModelSettings | None = None, ):     \"\"\"Initialize an OpenAI model.      Args:         model_name: The name of the OpenAI model to use. List of model names available             [here](https://github.com/openai/openai-python/blob/v1.54.3/src/openai/types/chat_model.py#L7)             (Unfortunately, despite being ask to do so, OpenAI do not provide `.inv` files for their API).         provider: The provider to use. Defaults to `'openai'`.         profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.         system_prompt_role: The role to use for the system prompt message. If not provided, defaults to `'system'`.             In the future, this may be inferred from the model name.         settings: Default model settings for this model instance.     \"\"\"     self._model_name = model_name      if isinstance(provider, str):         provider = infer_provider('gateway/openai' if provider == 'gateway' else provider)     self._provider = provider     self.client = provider.client      super().__init__(settings=settings, profile=profile or provider.model_profile)      if system_prompt_role is not None:         self.profile = OpenAIModelProfile(openai_system_prompt_role=system_prompt_role).update(self.profile) ``` |\n\n#### model\\_name `property`\n\n```\nmodel_name: OpenAIModelName\n```\n\nThe model name.\n\n#### system `property`\n\n```\nsystem: str\n```\n\nThe model provider.", "url": "https://ai.pydantic.dev/models/openai/index.html#openaichatmodel-dataclass", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIModel `dataclass` `deprecated`", "anchor": "openaimodel-dataclass-deprecated", "md_text": "Bases: `OpenAIChatModel`\n\nDeprecated\n\n`OpenAIModel` was renamed to `OpenAIChatModel` to clearly distinguish it from `OpenAIResponsesModel` which uses OpenAI's newer Responses API. Use that unless you're using an OpenAI Chat Completions-compatible API, or require a feature that the Responses API doesn't support yet like audio.\n\nDeprecated alias for `OpenAIChatModel`.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/openai.py`\n\n|  |  |\n| --- | --- |\n| ``` 888 889 890 891 892 893 894 895 ``` | ``` @deprecated(     '`OpenAIModel` was renamed to `OpenAIChatModel` to clearly distinguish it from `OpenAIResponsesModel` which '     \"uses OpenAI's newer Responses API. Use that unless you're using an OpenAI Chat Completions-compatible API, or \"     \"require a feature that the Responses API doesn't support yet like audio.\" ) @dataclass(init=False) class OpenAIModel(OpenAIChatModel):     \"\"\"Deprecated alias for `OpenAIChatModel`.\"\"\" ``` |", "url": "https://ai.pydantic.dev/models/openai/index.html#openaimodel-dataclass-deprecated", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIResponsesModel `dataclass`", "anchor": "openairesponsesmodel-dataclass", "md_text": "Bases: `Model`\n\nA model that uses the OpenAI Responses API.\n\nThe [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses) is the\nnew API for OpenAI models.\n\nIf you are interested in the differences between the Responses API and the Chat Completions API,\nsee the [OpenAI API docs](https://platform.openai.com/docs/guides/responses-vs-chat-completions).\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/openai.py`", "url": "https://ai.pydantic.dev/models/openai/index.html#openairesponsesmodel-dataclass", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIResponsesModel `dataclass`", "anchor": "openairesponsesmodel-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ```  898  899  900  901  902  903  904  905  906  907  908  909  910  911  912  913  914  915  916  917  918  919  920  921  922  923  924  925  926  927  928  929  930  931  932  933  934  935  936  937  938  939  940  941  942  943  944  945  946  947  948  949  950  951  952  953  954  955  956  957  958  959  960  961  962  963  964  965  966  967  968  969  970  971  972  973  974  975  976  977  978  979  980  981  982  983  984  985  986  987  988  989  990  991  992  993  994  995  996  997  998  999 1000 1001 1002 1003 1004 1005 1006 1007 1008 1009 1010 1011 1012 1013 1014 1015 1016 1017 1018 1019 1020 1021 1022 1023 1024 1025 1026 1027 1028 1029 1030 1031 1032 1033 1034 1035 1036 1037 1038 1039 1040 1041 1042 1043 1044 1045 1046 1047 1048 1049 1050 1051 1052 1053 1054 1055 1056 1057 1058 1059 1060 1061 1062 1063 1064 1065 1066 1067 1068 1069 1070 1071 1072 1073 1074 1075 1076 1077 1078 1079 1080 1081 1082 1083 1084 1085 1086 1087 1088 1089 1090 1091 1092 1093 1094 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 1316 1317 1318 1319 1320 1321 1322 1323 1324 1325 1326 1327 1328 1329 1330 1331 1332 1333 1334 1335 1336 1337 1338 1339 1340 1341 1342 1343 1344 1345 1346 1347 1348 1349 1350 1351 1352 1353 1354 1355 1356 1357 1358 1359 1360 1361 1362 1363 1364 1365 1366 1367 1368 1369 1370 1371 1372 1373 1374 1375 1376 1377 1378 1379 1380 1381 1382 1383 1384 1385 1386 1387 1388 1389 1390 1391 1392 1393 1394 1395 1396 1397 1398 1399 1400 1401 1402 1403 1404 1405 1406 1407 1408 1409 1410 1411 1412 1413 1414 1415 1416 1417 1418 1419 1420 1421 1422 1423 1424 1425 1426 1427 1428 1429 1430 1431 1432 1433 1434 1435 1436 1437 1438 1439 1440 1441 1442 1443 1444 1445 1446 1447 1448 1449 1450 1451 1452 1453 1454 1455 1456 1457 1458 1459 1460 1461 1462 1463 1464 1465 1466 1467 1468 1469 1470 1471 1472 1473 1474 1475 1476 1477 1478 1479 1480 1481 1482 1483 1484 1485 1486 1487 1488 1489 1490 1491 1492 1493 1494 1495 1496 1497 1498 1499 1500 1501 1502 1503 1504 1505 1506 1507 1508 1509 1510 1511 1512 1513 1514 1515 1516 1517 1518 1519 1520 1521 1522 1523 1524 1525 1526 1527 1528 1529 1530 1531 1532 1533 1534 1535 1536 1537 1538 1539 1540 1541 1542 1543 1544 1545 1546 1547 1548 1549 1550 1551 1552 1553 1554 1555 1556 1557 1558 1559 1560 1561 1562 1563 1564 1565 1566 1567 1568 1569 1570 1571 1572 1573 1574 1575 1576 1577 1578 1579 1580 1581 1582 1583 1584 1585 1586 1587 1588 1589 1590 1591 1592 1593 1594 1595 1596 1597 1598 1599 1600 1601 1602 1603 1604 1605 1606 1607 1608 1609 1610 1611 1612 1613 1614 1615 1616 1617 1618 1619 1620 1621 1622 1623 1624 1625 1626 1627 1628 1629 1630 1631 1632 1633 1634 1635 1636 1637 1638 1639 1640 1641 1642 1643 1644 1645 1646 1647 1648 1649 1650 1651 1652 1653 1654 1655 1656 1657 1658 1659 1660 1661 1662 1663 1664 1665 1666 1667 1668 1669 1670 ``` | ``` @dataclass(init=False) class OpenAIResponsesModel(Model):     \"\"\"A model that uses the OpenAI Responses API.      The [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses) is the     new API for OpenAI models.      If you are interested in the differences between the Responses API and the Chat Completions API,     see the [OpenAI API docs](https://platform.openai.com/docs/guides/responses-vs-chat-completions).     \"\"\"      client: AsyncOpenAI = field(repr=False)      _model_name: OpenAIModelName = field(repr=False)     _provider: Provider[AsyncOpenAI] = field(repr=False)      def __init__(         self,         model_name: OpenAIModelName,         *,         provider: Literal[             'openai',             'deepseek',             'azure',             'openrouter',             'grok',             'fireworks',             'together',             'nebius',             'ovhcloud',             'gateway',         ]         | Provider[AsyncOpenAI] = 'openai',         profile: ModelProfileSpec | None = None,         settings: ModelSettings | None = None,     ):         \"\"\"Initialize an OpenAI Responses model.          Args:             model_name: The name of the OpenAI model to use.             provider: The provider to use. Defaults to `'openai'`.             profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.             settings: Default model settings for this model instance.         \"\"\"         self._model_name = model_name          if isinstance(provider, str):             provider = infer_provider('gateway/openai' if provider == 'gateway' else provider)         self._provider = provider         self.client = provider.client          super().__init__(settings=settings, profile=profile or provider.model_profile)      @property     def model_name(self) -> OpenAIModelName:         \"\"\"The model name.\"\"\"         return self._model_name      @property     def system(self) -> str:         \"\"\"The model provider.\"\"\"         return self._provider.name      async def request(         self,         messages: list[ModelRequest | ModelResponse],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> ModelResponse:         check_allow_model_requests()         model_settings, model_request_parameters = self.prepare_request(             model_settings,             model_request_parameters,         )         response = await self._responses_create(             messages, False, cast(OpenAIResponsesModelSettings, model_settings or {}), model_request_parameters         )         return self._process_response(response, model_request_parameters)      @asynccontextmanager     async def request_stream(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,         run_context: RunContext[Any] | None = None,     ) -> AsyncIterator[StreamedResponse]:         check_allow_model_requests()         model_settings, model_request_parameters = self.prepare_request(             model_settings,             model_request_parameters,         )         response = await self._responses_create(             messages, True, cast(OpenAIResponsesModelSettings, model_settings or {}), model_request_parameters         )         async with response:             yield await self._process_streamed_response(response, model_request_parameters)      def _process_response(  # noqa: C901         self, response: responses.Response, model_request_parameters: ModelRequestParameters     ) -> ModelResponse:         \"\"\"Process a non-streamed response, and prepare a message to return.\"\"\"         timestamp = number_to_datetime(response.created_at)         items: list[ModelResponsePart] = []         for item in response.output:             if isinstance(item, responses.ResponseReasoningItem):                 signature = item.encrypted_content                 if item.summary:                     for summary in item.summary:                         # We use the same id for all summaries so that we can merge them on the round trip.                         items.append(                             ThinkingPart(                                 content=summary.text,                                 id=item.id,                                 signature=signature,                                 provider_name=self.system if signature else None,                             )                         )                         # We only need to store the signature once.                         signature = None                 elif signature:                     items.append(                         ThinkingPart(                             content='',                             id=item.id,                             signature=signature,                             provider_name=self.system,                         )                     )                 # NOTE: We don't currently handle the raw CoT from gpt-oss `reasoning_text`: https://cookbook.openai.com/articles/gpt-oss/handle-raw-cot                 # If you need this, please file an issue.             elif isinstance(item, responses.ResponseOutputMessage):                 for content in item.content:                     if isinstance(content, responses.ResponseOutputText):  # pragma: no branch                         items.append(TextPart(content.text, id=item.id))             elif isinstance(item, responses.ResponseFunctionToolCall):                 items.append(                     ToolCallPart(                         item.name,                         item.arguments,                         tool_call_id=item.call_id,                         id=item.id,                     )                 )             elif isinstance(item, responses.ResponseCodeInterpreterToolCall):                 call_part, return_part, file_parts = _map_code_interpreter_tool_call(item, self.system)                 items.append(call_part)                 if file_parts:                     items.extend(file_parts)                 items.append(return_part)             elif isinstance(item, responses.ResponseFunctionWebSearch):                 call_part, return_part = _map_web_search_tool_call(item, self.system)                 items.append(call_part)                 items.append(return_part)             elif isinstance(item, responses.response_output_item.ImageGenerationCall):                 call_part, return_part, file_part = _map_image_generation_tool_call(item, self.system)                 items.append(call_part)                 if file_part:  # pragma: no branch                     items.append(file_part)                 items.append(return_part)             elif isinstance(item, responses.ResponseComputerToolCall):  # pragma: no cover                 # Pydantic AI doesn't yet support the ComputerUse built-in tool                 pass             elif isinstance(item, responses.ResponseCustomToolCall):  # pragma: no cover                 # Support is being implemented in https://github.com/pydantic/pydantic-ai/pull/2572                 pass             elif isinstance(item, responses.response_output_item.LocalShellCall):  # pragma: no cover                 # Pydantic AI doesn't yet support the `codex-mini-latest` LocalShell built-in tool                 pass             elif isinstance(item, responses.ResponseFileSearchToolCall):  # pragma: no cover                 # Pydantic AI doesn't yet support the FileSearch built-in tool                 pass             elif isinstance(item, responses.response_output_item.McpCall):                 call_part, return_part = _map_mcp_call(item, self.system)                 items.append(call_part)                 items.append(return_part)             elif isinstance(item, responses.response_output_item.McpListTools):                 call_part, return_part = _map_mcp_list_tools(item, self.system)                 items.append(call_part)                 items.append(return_part)             elif isinstance(item, responses.response_output_item.McpApprovalRequest):  # pragma: no cover                 # Pydantic AI doesn't yet support McpApprovalRequest (explicit tool usage approval)                 pass          finish_reason: FinishReason | None = None         provider_details: dict[str, Any] | None = None         raw_finish_reason = details.reason if (details := response.incomplete_details) else response.status         if raw_finish_reason:             provider_details = {'finish_reason': raw_finish_reason}             finish_reason = _RESPONSES_FINISH_REASON_MAP.get(raw_finish_reason)          return ModelResponse(             parts=items,             usage=_map_usage(response, self._provider.name, self._provider.base_url, self._model_name),             model_name=response.model,             provider_response_id=response.id,             timestamp=timestamp,             provider_name=self._provider.name,             finish_reason=finish_reason,             provider_details=provider_details,         )      async def _process_streamed_response(         self,         response: AsyncStream[responses.ResponseStreamEvent],         model_request_parameters: ModelRequestParameters,     ) -> OpenAIResponsesStreamedResponse:         \"\"\"Process a streamed response, and prepare a streaming response to return.\"\"\"         peekable_response = _utils.PeekableAsyncStream(response)         first_chunk = await peekable_response.peek()         if isinstance(first_chunk, _utils.Unset):  # pragma: no cover             raise UnexpectedModelBehavior('Streamed response ended without content or tool calls')          assert isinstance(first_chunk, responses.ResponseCreatedEvent)         return OpenAIResponsesStreamedResponse(             model_request_parameters=model_request_parameters,             _model_name=first_chunk.response.model,             _response=peekable_response,             _timestamp=number_to_datetime(first_chunk.response.created_at),             _provider_name=self._provider.name,             _provider_url=self._provider.base_url,         )      @overload     async def _responses_create(         self,         messages: list[ModelRequest | ModelResponse],         stream: Literal[False],         model_settings: OpenAIResponsesModelSettings,         model_request_parameters: ModelRequestParameters,     ) -> responses.Response: ...      @overload     async def _responses_create(         self,         messages: list[ModelRequest | ModelResponse],         stream: Literal[True],         model_settings: OpenAIResponsesModelSettings,         model_request_parameters: ModelRequestParameters,     ) -> AsyncStream[responses.ResponseStreamEvent]: ...      async def _responses_create(         self,         messages: list[ModelRequest | ModelResponse],         stream: bool,         model_settings: OpenAIResponsesModelSettings,         model_request_parameters: ModelRequestParameters,     ) -> responses.Response | AsyncStream[responses.ResponseStreamEvent]:         tools = (             self._get_builtin_tools(model_request_parameters)             + list(model_settings.get('openai_builtin_tools', []))             + self._get_tools(model_request_parameters)         )          if not tools:             tool_choice: Literal['none', 'required', 'auto'] | None = None         elif not model_request_parameters.allow_text_output:             tool_choice = 'required'         else:             tool_choice = 'auto'          previous_response_id = model_settings.get('openai_previous_response_id')         if previous_response_id == 'auto':             previous_response_id, messages = self._get_previous_response_id_and_new_messages(messages)          instructions, openai_messages = await self._map_messages(messages, model_settings)         reasoning = self._get_reasoning(model_settings)          text: responses.ResponseTextConfigParam | None = None         if model_request_parameters.output_mode == 'native':             output_object = model_request_parameters.output_object             assert output_object is not None             text = {'format': self._map_json_schema(output_object)}         elif (             model_request_parameters.output_mode == 'prompted' and self.profile.supports_json_object_output         ):  # pragma: no branch             text = {'format': {'type': 'json_object'}}              # Without this trick, we'd hit this error:             # > Response input messages must contain the word 'json' in some form to use 'text.format' of type 'json_object'.             # Apparently they're only checking input messages for \"JSON\", not instructions.             assert isinstance(instructions, str)             openai_messages.insert(0, responses.EasyInputMessageParam(role='system', content=instructions))             instructions = NOT_GIVEN          if verbosity := model_settings.get('openai_text_verbosity'):             text = text or {}             text['verbosity'] = verbosity          profile = OpenAIModelProfile.from_profile(self.profile)         unsupported_model_settings = profile.openai_unsupported_model_settings         for setting in unsupported_model_settings:             model_settings.pop(setting, None)          include: list[responses.ResponseIncludable] = []         if profile.openai_supports_encrypted_reasoning_content:             include.append('reasoning.encrypted_content')         if model_settings.get('openai_include_code_execution_outputs'):             include.append('code_interpreter_call.outputs')         if model_settings.get('openai_include_web_search_sources'):             include.append('web_search_call.action.sources')  # pyright: ignore[reportArgumentType]          try:             extra_headers = model_settings.get('extra_headers', {})             extra_headers.setdefault('User-Agent', get_user_agent())             return await self.client.responses.create(                 input=openai_messages,                 model=self._model_name,                 instructions=instructions,                 parallel_tool_calls=model_settings.get('parallel_tool_calls', NOT_GIVEN),                 tools=tools or NOT_GIVEN,                 tool_choice=tool_choice or NOT_GIVEN,                 max_output_tokens=model_settings.get('max_tokens', NOT_GIVEN),                 stream=stream,                 temperature=model_settings.get('temperature', NOT_GIVEN),                 top_p=model_settings.get('top_p', NOT_GIVEN),                 truncation=model_settings.get('openai_truncation', NOT_GIVEN),                 timeout=model_settings.get('timeout', NOT_GIVEN),                 service_tier=model_settings.get('openai_service_tier', NOT_GIVEN),                 previous_response_id=previous_response_id or NOT_GIVEN,                 reasoning=reasoning,                 user=model_settings.get('openai_user', NOT_GIVEN),                 text=text or NOT_GIVEN,                 include=include or NOT_GIVEN,                 extra_headers=extra_headers,                 extra_body=model_settings.get('extra_body'),             )         except APIStatusError as e:             if (status_code := e.status_code) >= 400:                 raise ModelHTTPError(status_code=status_code, model_name=self.model_name, body=e.body) from e             raise  # pragma: lax no cover      def _get_reasoning(self, model_settings: OpenAIResponsesModelSettings) -> Reasoning | NotGiven:         reasoning_effort = model_settings.get('openai_reasoning_effort', None)         reasoning_summary = model_settings.get('openai_reasoning_summary', None)         reasoning_generate_summary = model_settings.get('openai_reasoning_generate_summary', None)          if reasoning_summary and reasoning_generate_summary:  # pragma: no cover             raise ValueError('`openai_reasoning_summary` and `openai_reasoning_generate_summary` cannot both be set.')          if reasoning_generate_summary is not None:  # pragma: no cover             warnings.warn(                 '`openai_reasoning_generate_summary` is deprecated, use `openai_reasoning_summary` instead',                 DeprecationWarning,             )             reasoning_summary = reasoning_generate_summary          if reasoning_effort is None and reasoning_summary is None:             return NOT_GIVEN         return Reasoning(effort=reasoning_effort, summary=reasoning_summary)      def _get_tools(self, model_request_parameters: ModelRequestParameters) -> list[responses.FunctionToolParam]:         return [self._map_tool_definition(r) for r in model_request_parameters.tool_defs.values()]      def _get_builtin_tools(self, model_request_parameters: ModelRequestParameters) -> list[responses.ToolParam]:         tools: list[responses.ToolParam] = []         has_image_generating_tool = False         for tool in model_request_parameters.builtin_tools:             if isinstance(tool, WebSearchTool):                 web_search_tool = responses.WebSearchToolParam(                     type='web_search', search_context_size=tool.search_context_size                 )                 if tool.user_location:                     web_search_tool['user_location'] = responses.web_search_tool_param.UserLocation(                         type='approximate', **tool.user_location                     )                 tools.append(web_search_tool)             elif isinstance(tool, CodeExecutionTool):                 has_image_generating_tool = True                 tools.append({'type': 'code_interpreter', 'container': {'type': 'auto'}})             elif isinstance(tool, MCPServerTool):                 mcp_tool = responses.tool_param.Mcp(                     type='mcp',                     server_label=tool.id,                     require_approval='never',                 )                  if tool.authorization_token:  # pragma: no branch                     mcp_tool['authorization'] = tool.authorization_token                  if tool.allowed_tools is not None:  # pragma: no branch                     mcp_tool['allowed_tools'] = tool.allowed_tools                  if tool.description:  # pragma: no branch                     mcp_tool['server_description'] = tool.description                  if tool.headers:  # pragma: no branch                     mcp_tool['headers'] = tool.headers                  if tool.url.startswith(MCP_SERVER_TOOL_CONNECTOR_URI_SCHEME + ':'):                     _, connector_id = tool.url.split(':', maxsplit=1)                     mcp_tool['connector_id'] = connector_id  # pyright: ignore[reportGeneralTypeIssues]                 else:                     mcp_tool['server_url'] = tool.url                  tools.append(mcp_tool)             elif isinstance(tool, ImageGenerationTool):  # pragma: no branch                 has_image_generating_tool = True                 tools.append(                     responses.tool_param.ImageGeneration(                         type='image_generation',                         background=tool.background,                         input_fidelity=tool.input_fidelity,                         moderation=tool.moderation,                         output_compression=tool.output_compression,                         output_format=tool.output_format or 'png',                         partial_images=tool.partial_images,                         quality=tool.quality,                         size=tool.size,                     )                 )             else:                 raise UserError(  # pragma: no cover                     f'`{tool.__class__.__name__}` is not supported by `OpenAIResponsesModel`. If it should be, please file an issue.'                 )          if model_request_parameters.allow_image_output and not has_image_generating_tool:             tools.append({'type': 'image_generation'})         return tools      def _map_tool_definition(self, f: ToolDefinition) -> responses.FunctionToolParam:         return {             'name': f.name,             'parameters': f.parameters_json_schema,             'type': 'function',             'description': f.description,             'strict': bool(                 f.strict and OpenAIModelProfile.from_profile(self.profile).openai_supports_strict_tool_definition             ),         }      def _get_previous_response_id_and_new_messages(         self, messages: list[ModelMessage]     ) -> tuple[str | None, list[ModelMessage]]:         # When `openai_previous_response_id` is set to 'auto', the most recent         # `provider_response_id` from the message history is selected and all         # earlier messages are omitted. This allows the OpenAI SDK to reuse         # server-side history for efficiency. The returned tuple contains the         # `previous_response_id` (if found) and the trimmed list of messages.         previous_response_id = None         trimmed_messages: list[ModelMessage] = []         for m in reversed(messages):             if isinstance(m, ModelResponse) and m.provider_name == self.system:                 previous_response_id = m.provider_response_id                 break             else:                 trimmed_messages.append(m)          if previous_response_id and trimmed_messages:             return previous_response_id, list(reversed(trimmed_messages))         else:             return None, messages      async def _map_messages(  # noqa: C901         self, messages: list[ModelMessage], model_settings: OpenAIResponsesModelSettings     ) -> tuple[str | NotGiven, list[responses.ResponseInputItemParam]]:         \"\"\"Just maps a `pydantic_ai.Message` to a `openai.types.responses.ResponseInputParam`.\"\"\"         profile = OpenAIModelProfile.from_profile(self.profile)         send_item_ids = model_settings.get(             'openai_send_reasoning_ids', profile.openai_supports_encrypted_reasoning_content         )          openai_messages: list[responses.ResponseInputItemParam] = []         for message in messages:             if isinstance(message, ModelRequest):                 for part in message.parts:                     if isinstance(part, SystemPromptPart):                         openai_messages.append(responses.EasyInputMessageParam(role='system', content=part.content))                     elif isinstance(part, UserPromptPart):                         openai_messages.append(await self._map_user_prompt(part))                     elif isinstance(part, ToolReturnPart):                         call_id = _guard_tool_call_id(t=part)                         call_id, _ = _split_combined_tool_call_id(call_id)                         item = FunctionCallOutput(                             type='function_call_output',                             call_id=call_id,                             output=part.model_response_str(),                         )                         openai_messages.append(item)                     elif isinstance(part, RetryPromptPart):                         if part.tool_name is None:                             openai_messages.append(                                 Message(role='user', content=[{'type': 'input_text', 'text': part.model_response()}])                             )                         else:                             call_id = _guard_tool_call_id(t=part)                             call_id, _ = _split_combined_tool_call_id(call_id)                             item = FunctionCallOutput(                                 type='function_call_output',                                 call_id=call_id,                                 output=part.model_response(),                             )                             openai_messages.append(item)                     else:                         assert_never(part)             elif isinstance(message, ModelResponse):                 send_item_ids = send_item_ids and message.provider_name == self.system                  message_item: responses.ResponseOutputMessageParam | None = None                 reasoning_item: responses.ResponseReasoningItemParam | None = None                 web_search_item: responses.ResponseFunctionWebSearchParam | None = None                 code_interpreter_item: responses.ResponseCodeInterpreterToolCallParam | None = None                 for item in message.parts:                     if isinstance(item, TextPart):                         if item.id and send_item_ids:                             if message_item is None or message_item['id'] != item.id:  # pragma: no branch                                 message_item = responses.ResponseOutputMessageParam(                                     role='assistant',                                     id=item.id,                                     content=[],                                     type='message',                                     status='completed',                                 )                                 openai_messages.append(message_item)                              message_item['content'] = [                                 *message_item['content'],                                 responses.ResponseOutputTextParam(                                     text=item.content, type='output_text', annotations=[]                                 ),                             ]                         else:                             openai_messages.append(                                 responses.EasyInputMessageParam(role='assistant', content=item.content)                             )                     elif isinstance(item, ToolCallPart):                         call_id = _guard_tool_call_id(t=item)                         call_id, id = _split_combined_tool_call_id(call_id)                         id = id or item.id                          param = responses.ResponseFunctionToolCallParam(                             name=item.tool_name,                             arguments=item.args_as_json_str(),                             call_id=call_id,                             type='function_call',                         )                         if profile.openai_responses_requires_function_call_status_none:                             param['status'] = None  # type: ignore[reportGeneralTypeIssues]                         if id and send_item_ids:  # pragma: no branch                             param['id'] = id                         openai_messages.append(param)                     elif isinstance(item, BuiltinToolCallPart):                         if item.provider_name == self.system and send_item_ids:                             if (                                 item.tool_name == CodeExecutionTool.kind                                 and item.tool_call_id                                 and (args := item.args_as_dict())                                 and (container_id := args.get('container_id'))                             ):                                 code_interpreter_item = responses.ResponseCodeInterpreterToolCallParam(                                     id=item.tool_call_id,                                     code=args.get('code'),                                     container_id=container_id,                                     outputs=None,  # These can be read server-side                                     status='completed',                                     type='code_interpreter_call',                                 )                                 openai_messages.append(code_interpreter_item)                             elif (                                 item.tool_name == WebSearchTool.kind                                 and item.tool_call_id                                 and (args := item.args_as_dict())                             ):                                 web_search_item = responses.ResponseFunctionWebSearchParam(                                     id=item.tool_call_id,                                     action=cast(responses.response_function_web_search_param.Action, args),                                     status='completed',                                     type='web_search_call',                                 )                                 openai_messages.append(web_search_item)                             elif item.tool_name == ImageGenerationTool.kind and item.tool_call_id:                                 # The cast is necessary because of https://github.com/openai/openai-python/issues/2648                                 image_generation_item = cast(                                     responses.response_input_item_param.ImageGenerationCall,                                     {                                         'id': item.tool_call_id,                                         'type': 'image_generation_call',                                     },                                 )                                 openai_messages.append(image_generation_item)                             elif (  # pragma: no branch                                 item.tool_name.startswith(MCPServerTool.kind)                                 and item.tool_call_id                                 and (server_id := item.tool_name.split(':', 1)[1])                                 and (args := item.args_as_dict())                                 and (action := args.get('action'))                             ):                                 if action == 'list_tools':                                     mcp_list_tools_item = responses.response_input_item_param.McpListTools(                                         id=item.tool_call_id,                                         type='mcp_list_tools',                                         server_label=server_id,                                         tools=[],  # These can be read server-side                                     )                                     openai_messages.append(mcp_list_tools_item)                                 elif (  # pragma: no branch                                     action == 'call_tool'                                     and (tool_name := args.get('tool_name'))                                     and (tool_args := args.get('tool_args'))                                 ):                                     mcp_call_item = responses.response_input_item_param.McpCall(                                         id=item.tool_call_id,                                         server_label=server_id,                                         name=tool_name,                                         arguments=to_json(tool_args).decode(),                                         error=None,  # These can be read server-side                                         output=None,  # These can be read server-side                                         type='mcp_call',                                     )                                     openai_messages.append(mcp_call_item)                      elif isinstance(item, BuiltinToolReturnPart):                         if item.provider_name == self.system and send_item_ids:                             if (                                 item.tool_name == CodeExecutionTool.kind                                 and code_interpreter_item is not None                                 and isinstance(item.content, dict)                                 and (content := cast(dict[str, Any], item.content))  # pyright: ignore[reportUnknownMemberType]                                 and (status := content.get('status'))                             ):                                 code_interpreter_item['status'] = status                             elif (                                 item.tool_name == WebSearchTool.kind                                 and web_search_item is not None                                 and isinstance(item.content, dict)  # pyright: ignore[reportUnknownMemberType]                                 and (content := cast(dict[str, Any], item.content))  # pyright: ignore[reportUnknownMemberType]                                 and (status := content.get('status'))                             ):                                 web_search_item['status'] = status                             elif item.tool_name == ImageGenerationTool.kind:                                 # Image generation result does not need to be sent back, just the `id` off of `BuiltinToolCallPart`.                                 pass                             elif item.tool_name.startswith(MCPServerTool.kind):  # pragma: no branch                                 # MCP call result does not need to be sent back, just the fields off of `BuiltinToolCallPart`.                                 pass                     elif isinstance(item, FilePart):                         # This was generated by the `ImageGenerationTool` or `CodeExecutionTool`,                         # and does not need to be sent back separately from the corresponding `BuiltinToolReturnPart`.                         # If `send_item_ids` is false, we won't send the `BuiltinToolReturnPart`, but OpenAI does not have a type for files from the assistant.                         pass                     elif isinstance(item, ThinkingPart):                         if item.id and send_item_ids:                             signature: str | None = None                             if (                                 item.signature                                 and item.provider_name == self.system                                 and profile.openai_supports_encrypted_reasoning_content                             ):                                 signature = item.signature                              if (reasoning_item is None or reasoning_item['id'] != item.id) and (                                 signature or item.content                             ):  # pragma: no branch                                 reasoning_item = responses.ResponseReasoningItemParam(                                     id=item.id,                                     summary=[],                                     encrypted_content=signature,                                     type='reasoning',                                 )                                 openai_messages.append(reasoning_item)                              if item.content:                                 # The check above guarantees that `reasoning_item` is not None                                 assert reasoning_item is not None                                 reasoning_item['summary'] = [                                     *reasoning_item['summary'],                                     Summary(text=item.content, type='summary_text'),                                 ]                         else:                             start_tag, end_tag = profile.thinking_tags                             openai_messages.append(                                 responses.EasyInputMessageParam(                                     role='assistant', content='\\n'.join([start_tag, item.content, end_tag])                                 )                             )                     else:                         assert_never(item)             else:                 assert_never(message)         instructions = self._get_instructions(messages) or NOT_GIVEN         return instructions, openai_messages      def _map_json_schema(self, o: OutputObjectDefinition) -> responses.ResponseFormatTextJSONSchemaConfigParam:         response_format_param: responses.ResponseFormatTextJSONSchemaConfigParam = {             'type': 'json_schema',             'name': o.name or DEFAULT_OUTPUT_TOOL_NAME,             'schema': o.json_schema,         }         if o.description:             response_format_param['description'] = o.description         if OpenAIModelProfile.from_profile(self.profile).openai_supports_strict_tool_definition:  # pragma: no branch             response_format_param['strict'] = o.strict         return response_format_param      @staticmethod     async def _map_user_prompt(part: UserPromptPart) -> responses.EasyInputMessageParam:         content: str | list[responses.ResponseInputContentParam]         if isinstance(part.content, str):             content = part.content         else:             content = []             for item in part.content:                 if isinstance(item, str):                     content.append(responses.ResponseInputTextParam(text=item, type='input_text'))                 elif isinstance(item, BinaryContent):                     if item.is_image:                         detail: Literal['auto', 'low', 'high'] = 'auto'                         if metadata := item.vendor_metadata:                             detail = cast(                                 Literal['auto', 'low', 'high'],                                 metadata.get('detail', 'auto'),                             )                         content.append(                             responses.ResponseInputImageParam(                                 image_url=item.data_uri,                                 type='input_image',                                 detail=detail,                             )                         )                     elif item.is_document:                         content.append(                             responses.ResponseInputFileParam(                                 type='input_file',                                 file_data=item.data_uri,                                 # NOTE: Type wise it's not necessary to include the filename, but it's required by the                                 # API itself. If we add empty string, the server sends a 500 error - which OpenAI needs                                 # to fix. In any case, we add a placeholder name.                                 filename=f'filename.{item.format}',                             )                         )                     elif item.is_audio:                         raise NotImplementedError('Audio as binary content is not supported for OpenAI Responses API.')                     else:  # pragma: no cover                         raise RuntimeError(f'Unsupported binary content type: {item.media_type}')                 elif isinstance(item, ImageUrl):                     detail: Literal['auto', 'low', 'high'] = 'auto'                     image_url = item.url                     if metadata := item.vendor_metadata:                         detail = cast(Literal['auto', 'low', 'high'], metadata.get('detail', 'auto'))                     if item.force_download:                         downloaded_item = await download_item(item, data_format='base64_uri', type_format='extension')                         image_url = downloaded_item['data']                      content.append(                         responses.ResponseInputImageParam(                             image_url=image_url,                             type='input_image',                             detail=detail,                         )                     )                 elif isinstance(item, AudioUrl):  # pragma: no cover                     downloaded_item = await download_item(item, data_format='base64_uri', type_format='extension')                     content.append(                         responses.ResponseInputFileParam(                             type='input_file',                             file_data=downloaded_item['data'],                             filename=f'filename.{downloaded_item[\"data_type\"]}',                         )                     )                 elif isinstance(item, DocumentUrl):                     downloaded_item = await download_item(item, data_format='base64_uri', type_format='extension')                     content.append(                         responses.ResponseInputFileParam(                             type='input_file',                             file_data=downloaded_item['data'],                             filename=f'filename.{downloaded_item[\"data_type\"]}',                         )                     )                 elif isinstance(item, VideoUrl):  # pragma: no cover                     raise NotImplementedError('VideoUrl is not supported for OpenAI.')                 else:                     assert_never(item)         return responses.EasyInputMessageParam(role='user', content=content) ``` |", "url": "https://ai.pydantic.dev/models/openai/index.html#openairesponsesmodel-dataclass", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIResponsesModel `dataclass`", "anchor": "openairesponsesmodel-dataclass", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    model_name: OpenAIModelName,\n    *,\n    provider: (\n        Literal[\n            \"openai\",\n            \"deepseek\",\n            \"azure\",\n            \"openrouter\",\n            \"grok\",\n            \"fireworks\",\n            \"together\",\n            \"nebius\",\n            \"ovhcloud\",\n            \"gateway\",\n        ]\n        | Provider[AsyncOpenAI]\n    ) = \"openai\",\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n)\n```\n\nInitialize an OpenAI Responses model.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `model_name` | `OpenAIModelName` | The name of the OpenAI model to use. | *required* |\n| `provider` | `Literal['openai', 'deepseek', 'azure', 'openrouter', 'grok', 'fireworks', 'together', 'nebius', 'ovhcloud', 'gateway'] | Provider[AsyncOpenAI]` | The provider to use. Defaults to `'openai'`. | `'openai'` |\n| `profile` | `ModelProfileSpec | None` | The model profile to use. Defaults to a profile picked by the provider based on the model name. | `None` |\n| `settings` | `ModelSettings | None` | Default model settings for this model instance. | `None` |\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/openai.py`\n\n|  |  |\n| --- | --- |\n| ``` 914 915 916 917 918 919 920 921 922 923 924 925 926 927 928 929 930 931 932 933 934 935 936 937 938 939 940 941 942 943 944 945 946 947 948 949 ``` | ``` def __init__(     self,     model_name: OpenAIModelName,     *,     provider: Literal[         'openai',         'deepseek',         'azure',         'openrouter',         'grok',         'fireworks',         'together',         'nebius',         'ovhcloud',         'gateway',     ]     | Provider[AsyncOpenAI] = 'openai',     profile: ModelProfileSpec | None = None,     settings: ModelSettings | None = None, ):     \"\"\"Initialize an OpenAI Responses model.      Args:         model_name: The name of the OpenAI model to use.         provider: The provider to use. Defaults to `'openai'`.         profile: The model profile to use. Defaults to a profile picked by the provider based on the model name.         settings: Default model settings for this model instance.     \"\"\"     self._model_name = model_name      if isinstance(provider, str):         provider = infer_provider('gateway/openai' if provider == 'gateway' else provider)     self._provider = provider     self.client = provider.client      super().__init__(settings=settings, profile=profile or provider.model_profile) ``` |\n\n#### model\\_name `property`\n\n```\nmodel_name: OpenAIModelName\n```\n\nThe model name.\n\n#### system `property`\n\n```\nsystem: str\n```\n\nThe model provider.", "url": "https://ai.pydantic.dev/models/openai/index.html#openairesponsesmodel-dataclass", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "TestModel `dataclass`", "anchor": "testmodel-dataclass", "md_text": "Bases: `Model`\n\nA model specifically for testing purposes.\n\nThis will (by default) call all tools in the agent, then return a tool response if possible,\notherwise a plain response.\n\nHow useful this model is will vary significantly.\n\nApart from `__init__` derived by the `dataclass` decorator, all methods are private or match those\nof the base class.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/test.py`", "url": "https://ai.pydantic.dev/models/test/index.html#testmodel-dataclass", "page": "models/test/index.html", "source_site": "pydantic_ai"}
{"title": "TestModel `dataclass`", "anchor": "testmodel-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ```  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 ``` | ``` @dataclass(init=False) class TestModel(Model):     \"\"\"A model specifically for testing purposes.      This will (by default) call all tools in the agent, then return a tool response if possible,     otherwise a plain response.      How useful this model is will vary significantly.      Apart from `__init__` derived by the `dataclass` decorator, all methods are private or match those     of the base class.     \"\"\"      # NOTE: Avoid test discovery by pytest.     __test__ = False      call_tools: list[str] | Literal['all'] = 'all'     \"\"\"List of tools to call. If `'all'`, all tools will be called.\"\"\"     custom_output_text: str | None = None     \"\"\"If set, this text is returned as the final output.\"\"\"     custom_output_args: Any | None = None     \"\"\"If set, these args will be passed to the output tool.\"\"\"     seed: int = 0     \"\"\"Seed for generating random data.\"\"\"     last_model_request_parameters: ModelRequestParameters | None = field(default=None, init=False)     \"\"\"The last ModelRequestParameters passed to the model in a request.      The ModelRequestParameters contains information about the function and output tools available during request handling.      This is set when a request is made, so will reflect the function tools from the last step of the last run.     \"\"\"     _model_name: str = field(default='test', repr=False)     _system: str = field(default='test', repr=False)      def __init__(         self,         *,         call_tools: list[str] | Literal['all'] = 'all',         custom_output_text: str | None = None,         custom_output_args: Any | None = None,         seed: int = 0,         profile: ModelProfileSpec | None = None,         settings: ModelSettings | None = None,     ):         \"\"\"Initialize TestModel with optional settings and profile.\"\"\"         self.call_tools = call_tools         self.custom_output_text = custom_output_text         self.custom_output_args = custom_output_args         self.seed = seed         self.last_model_request_parameters = None         self._model_name = 'test'         self._system = 'test'         super().__init__(settings=settings, profile=profile)      async def request(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> ModelResponse:         model_settings, model_request_parameters = self.prepare_request(             model_settings,             model_request_parameters,         )         self.last_model_request_parameters = model_request_parameters         model_response = self._request(messages, model_settings, model_request_parameters)         model_response.usage = _estimate_usage([*messages, model_response])         return model_response      @asynccontextmanager     async def request_stream(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,         run_context: RunContext[Any] | None = None,     ) -> AsyncIterator[StreamedResponse]:         model_settings, model_request_parameters = self.prepare_request(             model_settings,             model_request_parameters,         )         self.last_model_request_parameters = model_request_parameters          model_response = self._request(messages, model_settings, model_request_parameters)         yield TestStreamedResponse(             model_request_parameters=model_request_parameters,             _model_name=self._model_name,             _structured_response=model_response,             _messages=messages,             _provider_name=self._system,         )      @property     def model_name(self) -> str:         \"\"\"The model name.\"\"\"         return self._model_name      @property     def system(self) -> str:         \"\"\"The model provider.\"\"\"         return self._system      def gen_tool_args(self, tool_def: ToolDefinition) -> Any:         return _JsonSchemaTestData(tool_def.parameters_json_schema, self.seed).generate()      def _get_tool_calls(self, model_request_parameters: ModelRequestParameters) -> list[tuple[str, ToolDefinition]]:         if self.call_tools == 'all':             return [(r.name, r) for r in model_request_parameters.function_tools]         else:             function_tools_lookup = {t.name: t for t in model_request_parameters.function_tools}             tools_to_call = (function_tools_lookup[name] for name in self.call_tools)             return [(r.name, r) for r in tools_to_call]      def _get_output(self, model_request_parameters: ModelRequestParameters) -> _WrappedTextOutput | _WrappedToolOutput:         if self.custom_output_text is not None:             assert model_request_parameters.output_mode != 'tool', (                 'Plain response not allowed, but `custom_output_text` is set.'             )             assert self.custom_output_args is None, 'Cannot set both `custom_output_text` and `custom_output_args`.'             return _WrappedTextOutput(self.custom_output_text)         elif self.custom_output_args is not None:             assert model_request_parameters.output_tools is not None, (                 'No output tools provided, but `custom_output_args` is set.'             )             output_tool = model_request_parameters.output_tools[0]              if k := output_tool.outer_typed_dict_key:                 return _WrappedToolOutput({k: self.custom_output_args})             else:                 return _WrappedToolOutput(self.custom_output_args)         elif model_request_parameters.allow_text_output:             return _WrappedTextOutput(None)         elif model_request_parameters.output_tools:             return _WrappedToolOutput(None)         else:             return _WrappedTextOutput(None)      def _request(         self,         messages: list[ModelMessage],         model_settings: ModelSettings | None,         model_request_parameters: ModelRequestParameters,     ) -> ModelResponse:         if model_request_parameters.builtin_tools:             raise UserError('TestModel does not support built-in tools')          tool_calls = self._get_tool_calls(model_request_parameters)         output_wrapper = self._get_output(model_request_parameters)         output_tools = model_request_parameters.output_tools          # if there are tools, the first thing we want to do is call all of them         if tool_calls and not any(isinstance(m, ModelResponse) for m in messages):             return ModelResponse(                 parts=[                     ToolCallPart(name, self.gen_tool_args(args), tool_call_id=f'pyd_ai_tool_call_id__{name}')                     for name, args in tool_calls                 ],                 model_name=self._model_name,             )          if messages:  # pragma: no branch             last_message = messages[-1]             assert isinstance(last_message, ModelRequest), 'Expected last message to be a `ModelRequest`.'              # check if there are any retry prompts, if so retry them             new_retry_names = {p.tool_name for p in last_message.parts if isinstance(p, RetryPromptPart)}             if new_retry_names:                 # Handle retries for both function tools and output tools                 # Check function tools first                 retry_parts: list[ModelResponsePart] = [                     ToolCallPart(name, self.gen_tool_args(args)) for name, args in tool_calls if name in new_retry_names                 ]                 # Check output tools                 if output_tools:                     retry_parts.extend(                         [                             ToolCallPart(                                 tool.name,                                 output_wrapper.value                                 if isinstance(output_wrapper, _WrappedToolOutput) and output_wrapper.value is not None                                 else self.gen_tool_args(tool),                                 tool_call_id=f'pyd_ai_tool_call_id__{tool.name}',                             )                             for tool in output_tools                             if tool.name in new_retry_names                         ]                     )                 return ModelResponse(parts=retry_parts, model_name=self._model_name)          if isinstance(output_wrapper, _WrappedTextOutput):             if (response_text := output_wrapper.value) is None:                 # build up details of tool responses                 output: dict[str, Any] = {}                 for message in messages:                     if isinstance(message, ModelRequest):                         for part in message.parts:                             if isinstance(part, ToolReturnPart):                                 output[part.tool_name] = part.content                 if output:                     return ModelResponse(                         parts=[TextPart(pydantic_core.to_json(output).decode())], model_name=self._model_name                     )                 else:                     return ModelResponse(parts=[TextPart('success (no tool calls)')], model_name=self._model_name)             else:                 return ModelResponse(parts=[TextPart(response_text)], model_name=self._model_name)         else:             assert output_tools, 'No output tools provided'             custom_output_args = output_wrapper.value             output_tool = output_tools[self.seed % len(output_tools)]             if custom_output_args is not None:                 return ModelResponse(                     parts=[                         ToolCallPart(                             output_tool.name,                             custom_output_args,                             tool_call_id=f'pyd_ai_tool_call_id__{output_tool.name}',                         )                     ],                     model_name=self._model_name,                 )             else:                 response_args = self.gen_tool_args(output_tool)                 return ModelResponse(                     parts=[                         ToolCallPart(                             output_tool.name,                             response_args,                             tool_call_id=f'pyd_ai_tool_call_id__{output_tool.name}',                         )                     ],                     model_name=self._model_name,                 ) ``` |", "url": "https://ai.pydantic.dev/models/test/index.html#testmodel-dataclass", "page": "models/test/index.html", "source_site": "pydantic_ai"}
{"title": "TestModel `dataclass`", "anchor": "testmodel-dataclass", "md_text": "#### \\_\\_init\\_\\_\n\n```\n__init__(\n    *,\n    call_tools: list[str] | Literal[\"all\"] = \"all\",\n    custom_output_text: str | None = None,\n    custom_output_args: Any | None = None,\n    seed: int = 0,\n    profile: ModelProfileSpec | None = None,\n    settings: ModelSettings | None = None\n)\n```\n\nInitialize TestModel with optional settings and profile.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/test.py`\n\n|  |  |\n| --- | --- |\n| ```  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 ``` | ``` def __init__(     self,     *,     call_tools: list[str] | Literal['all'] = 'all',     custom_output_text: str | None = None,     custom_output_args: Any | None = None,     seed: int = 0,     profile: ModelProfileSpec | None = None,     settings: ModelSettings | None = None, ):     \"\"\"Initialize TestModel with optional settings and profile.\"\"\"     self.call_tools = call_tools     self.custom_output_text = custom_output_text     self.custom_output_args = custom_output_args     self.seed = seed     self.last_model_request_parameters = None     self._model_name = 'test'     self._system = 'test'     super().__init__(settings=settings, profile=profile) ``` |\n\n#### call\\_tools `class-attribute` `instance-attribute`\n\n```\ncall_tools: list[str] | Literal['all'] = call_tools\n```\n\nList of tools to call. If `'all'`, all tools will be called.\n\n#### custom\\_output\\_text `class-attribute` `instance-attribute`\n\n```\ncustom_output_text: str | None = custom_output_text\n```\n\nIf set, this text is returned as the final output.\n\n#### custom\\_output\\_args `class-attribute` `instance-attribute`\n\n```\ncustom_output_args: Any | None = custom_output_args\n```\n\nIf set, these args will be passed to the output tool.\n\n#### seed `class-attribute` `instance-attribute`\n\n```\nseed: int = seed\n```\n\nSeed for generating random data.\n\n#### last\\_model\\_request\\_parameters `class-attribute` `instance-attribute`\n\n```\nlast_model_request_parameters: (\n    ModelRequestParameters | None\n) = None\n```\n\nThe last ModelRequestParameters passed to the model in a request.\n\nThe ModelRequestParameters contains information about the function and output tools available during request handling.\n\nThis is set when a request is made, so will reflect the function tools from the last step of the last run.\n\n#### model\\_name `property`\n\n```\nmodel_name: str\n```\n\nThe model name.\n\n#### system `property`\n\n```\nsystem: str\n```\n\nThe model provider.", "url": "https://ai.pydantic.dev/models/test/index.html#testmodel-dataclass", "page": "models/test/index.html", "source_site": "pydantic_ai"}
{"title": "TestStreamedResponse `dataclass`", "anchor": "teststreamedresponse-dataclass", "md_text": "Bases: `StreamedResponse`\n\nA structured response that streams test data.\n\nSource code in `pydantic_ai_slim/pydantic_ai/models/test.py`\n\n|  |  |\n| --- | --- |\n| ``` 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 ``` | ``` @dataclass class TestStreamedResponse(StreamedResponse):     \"\"\"A structured response that streams test data.\"\"\"      _model_name: str     _structured_response: ModelResponse     _messages: InitVar[Iterable[ModelMessage]]     _provider_name: str     _timestamp: datetime = field(default_factory=_utils.now_utc, init=False)      def __post_init__(self, _messages: Iterable[ModelMessage]):         self._usage = _estimate_usage(_messages)      async def _get_event_iterator(self) -> AsyncIterator[ModelResponseStreamEvent]:         for i, part in enumerate(self._structured_response.parts):             if isinstance(part, TextPart):                 text = part.content                 *words, last_word = text.split(' ')                 words = [f'{word} ' for word in words]                 words.append(last_word)                 if len(words) == 1 and len(text) > 2:                     mid = len(text) // 2                     words = [text[:mid], text[mid:]]                 self._usage += _get_string_usage('')                 maybe_event = self._parts_manager.handle_text_delta(vendor_part_id=i, content='')                 if maybe_event is not None:  # pragma: no branch                     yield maybe_event                 for word in words:                     self._usage += _get_string_usage(word)                     maybe_event = self._parts_manager.handle_text_delta(vendor_part_id=i, content=word)                     if maybe_event is not None:  # pragma: no branch                         yield maybe_event             elif isinstance(part, ToolCallPart):                 yield self._parts_manager.handle_tool_call_part(                     vendor_part_id=i, tool_name=part.tool_name, args=part.args, tool_call_id=part.tool_call_id                 )             elif isinstance(part, BuiltinToolCallPart | BuiltinToolReturnPart):  # pragma: no cover                 # NOTE: These parts are not generated by TestModel, but we need to handle them for type checking                 assert False, f'Unexpected part type in TestModel: {type(part).__name__}'             elif isinstance(part, ThinkingPart):  # pragma: no cover                 # NOTE: There's no way to reach this part of the code, since we don't generate ThinkingPart on TestModel.                 assert False, \"This should be unreachable — we don't generate ThinkingPart on TestModel.\"             elif isinstance(part, FilePart):  # pragma: no cover                 # NOTE: There's no way to reach this part of the code, since we don't generate FilePart on TestModel.                 assert False, \"This should be unreachable — we don't generate FilePart on TestModel.\"             else:                 assert_never(part)      @property     def model_name(self) -> str:         \"\"\"Get the model name of the response.\"\"\"         return self._model_name      @property     def provider_name(self) -> str:         \"\"\"Get the provider name.\"\"\"         return self._provider_name      @property     def timestamp(self) -> datetime:         \"\"\"Get the timestamp of the response.\"\"\"         return self._timestamp ``` |\n\n#### model\\_name `property`\n\n```\nmodel_name: str\n```\n\nGet the model name of the response.\n\n#### provider\\_name `property`\n\n```\nprovider_name: str\n```\n\nGet the provider name.\n\n#### timestamp `property`\n\n```\ntimestamp: datetime\n```\n\nGet the timestamp of the response.", "url": "https://ai.pydantic.dev/models/test/index.html#teststreamedresponse-dataclass", "page": "models/test/index.html", "source_site": "pydantic_ai"}
{"title": "Scenario", "anchor": "scenario", "md_text": "We're testing a function that converts text to title case. We want to verify:\n\n* Output is always a string\n* Output matches expected format\n* Function handles edge cases correctly\n* Performance meets requirements", "url": "https://ai.pydantic.dev/examples/simple-validation/index.html#scenario", "page": "examples/simple-validation/index.html", "source_site": "pydantic_ai"}
{"title": "Complete Example", "anchor": "complete-example", "md_text": "```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import (\n    Contains,\n    EqualsExpected,\n    IsInstance,\n    MaxDuration,\n)\n\n\n# The function we're testing\ndef to_title_case(text: str) -> str:\n    \"\"\"Convert text to title case.\"\"\"\n    return text.title()\n\n\n# Create evaluation dataset\ndataset = Dataset(\n    name='title_case_validation',\n    cases=[\n        # Basic functionality\n        Case(\n            name='basic_lowercase',\n            inputs='hello world',\n            expected_output='Hello World',\n        ),\n        Case(\n            name='basic_uppercase',\n            inputs='HELLO WORLD',\n            expected_output='Hello World',\n        ),\n        Case(\n            name='mixed_case',\n            inputs='HeLLo WoRLd',\n            expected_output='Hello World',\n        ),\n\n        # Edge cases\n        Case(\n            name='empty_string',\n            inputs='',\n            expected_output='',\n        ),\n        Case(\n            name='single_word',\n            inputs='hello',\n            expected_output='Hello',\n        ),\n        Case(\n            name='with_punctuation',\n            inputs='hello, world!',\n            expected_output='Hello, World!',\n        ),\n        Case(\n            name='with_numbers',\n            inputs='hello 123 world',\n            expected_output='Hello 123 World',\n        ),\n        Case(\n            name='apostrophes',\n            inputs=\"don't stop believin'\",\n            expected_output=\"Don'T Stop Believin'\",\n        ),\n    ],\n    evaluators=[\n        # Always returns a string\n        IsInstance(type_name='str'),\n\n        # Matches expected output\n        EqualsExpected(),\n\n        # Output should contain capital letters\n        Contains(value='H', evaluation_name='has_capitals'),\n\n        # Should be fast (under 1ms)\n        MaxDuration(seconds=0.001),\n    ],\n)\n\n\n# Run evaluation\nif __name__ == '__main__':\n    report = dataset.evaluate_sync(to_title_case)\n\n    # Print results\n    report.print(include_input=True, include_output=True)\n\"\"\"\n                            Evaluation Summary: to_title_case\n┏━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ Case ID          ┃ Inputs               ┃ Outputs              ┃ Assertions ┃ Duration ┃\n┡━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n│ basic_lowercase  │ hello world          │ Hello World          │ ✔✔✔✗       │     10ms │\n├──────────────────┼──────────────────────┼──────────────────────┼────────────┼──────────┤\n│ basic_uppercase  │ HELLO WORLD          │ Hello World          │ ✔✔✔✗       │     10ms │\n├──────────────────┼──────────────────────┼──────────────────────┼────────────┼──────────┤\n│ mixed_case       │ HeLLo WoRLd          │ Hello World          │ ✔✔✔✗       │     10ms │\n├──────────────────┼──────────────────────┼──────────────────────┼────────────┼──────────┤\n│ empty_string     │ -                    │ -                    │ ✔✔✗✗       │     10ms │\n├──────────────────┼──────────────────────┼──────────────────────┼────────────┼──────────┤\n│ single_word      │ hello                │ Hello                │ ✔✔✔✗       │     10ms │\n├──────────────────┼──────────────────────┼──────────────────────┼────────────┼──────────┤\n│ with_punctuation │ hello, world!        │ Hello, World!        │ ✔✔✔✗       │     10ms │\n├──────────────────┼──────────────────────┼──────────────────────┼────────────┼──────────┤\n│ with_numbers     │ hello 123 world      │ Hello 123 World      │ ✔✔✔✗       │     10ms │\n├──────────────────┼──────────────────────┼──────────────────────┼────────────┼──────────┤\n│ apostrophes      │ don't stop believin' │ Don'T Stop Believin' │ ✔✔✗✗       │     10ms │\n├──────────────────┼──────────────────────┼──────────────────────┼────────────┼──────────┤\n│ Averages         │                      │                      │ 68.8% ✔    │     10ms │\n└──────────────────┴──────────────────────┴──────────────────────┴────────────┴──────────┘\n\"\"\"\n# Check if all passed\navg = report.averages()\nif avg and avg.assertions == 1.0:\n    print('\\n✅ All tests passed!')\nelse:\n    print(f'\\n❌ Some tests failed (pass rate: {avg.assertions:.1%})')\n    \"\"\"\n    ❌ Some tests failed (pass rate: 68.8%)\n    \"\"\"\n```", "url": "https://ai.pydantic.dev/examples/simple-validation/index.html#complete-example", "page": "examples/simple-validation/index.html", "source_site": "pydantic_ai"}
{"title": "Expected Output", "anchor": "expected-output", "md_text": "```\n                        Evaluation Summary: to_title_case\n┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ Case ID           ┃ Inputs               ┃ Outputs               ┃ Assertions ┃ Duration ┃\n┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n│ basic_lowercase   │ hello world          │ Hello World           │ ✔✔✔✔       │      <1ms│\n├───────────────────┼──────────────────────┼───────────────────────┼────────────┼──────────┤\n│ basic_uppercase   │ HELLO WORLD          │ Hello World           │ ✔✔✔✔       │      <1ms│\n├───────────────────┼──────────────────────┼───────────────────────┼────────────┼──────────┤\n│ mixed_case        │ HeLLo WoRLd          │ Hello World           │ ✔✔✔✔       │      <1ms│\n├───────────────────┼──────────────────────┼───────────────────────┼────────────┼──────────┤\n│ empty_string      │                      │                       │ ✔✔✗✔       │      <1ms│\n├───────────────────┼──────────────────────┼───────────────────────┼────────────┼──────────┤\n│ single_word       │ hello                │ Hello                 │ ✔✔✔✔       │      <1ms│\n├───────────────────┼──────────────────────┼───────────────────────┼────────────┼──────────┤\n│ with_punctuation  │ hello, world!        │ Hello, World!         │ ✔✔✔✔       │      <1ms│\n├───────────────────┼──────────────────────┼───────────────────────┼────────────┼──────────┤\n│ with_numbers      │ hello 123 world      │ Hello 123 World       │ ✔✔✔✔       │      <1ms│\n├───────────────────┼──────────────────────┼───────────────────────┼────────────┼──────────┤\n│ apostrophes       │ don't stop believin' │ Don'T Stop Believin'  │ ✔✔✔✔       │      <1ms│\n├───────────────────┼──────────────────────┼───────────────────────┼────────────┼──────────┤\n│ Averages          │                      │                       │ 96.9% ✔    │      <1ms│\n└───────────────────┴──────────────────────┴───────────────────────┴────────────┴──────────┘\n\n✅ All tests passed!\n```\n\nNote: The `empty_string` case has one failed assertion (`has_capitals`) because an empty string contains no capital letters.", "url": "https://ai.pydantic.dev/examples/simple-validation/index.html#expected-output", "page": "examples/simple-validation/index.html", "source_site": "pydantic_ai"}
{"title": "Saving and Loading", "anchor": "saving-and-loading", "md_text": "Save the dataset for future use:\n\n```\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import EqualsExpected\n\n\n# The function we're testing\ndef to_title_case(text: str) -> str:\n    \"\"\"Convert text to title case.\"\"\"\n    return text.title()\n\n\n# Create dataset\ndataset: Dataset[str, str, Any] = Dataset(\n    cases=[Case(inputs='test', expected_output='Test')],\n    evaluators=[EqualsExpected()],\n)\n\n# Save to YAML\ndataset.to_file('title_case_tests.yaml')\n\n# Load later\ndataset = Dataset.from_file('title_case_tests.yaml')\nreport = dataset.evaluate_sync(to_title_case)\n```", "url": "https://ai.pydantic.dev/examples/simple-validation/index.html#saving-and-loading", "page": "examples/simple-validation/index.html", "source_site": "pydantic_ai"}
{"title": "Adding More Cases", "anchor": "adding-more-cases", "md_text": "As you find bugs or edge cases, add them to the dataset:\n\n```\nfrom pydantic_evals import Dataset\n\n# Load existing dataset\ndataset = Dataset.from_file('title_case_tests.yaml')\n\n# Found a bug with unicode\ndataset.add_case(\n    name='unicode_chars',\n    inputs='café résumé',\n    expected_output='Café Résumé',\n)\n\n# Found a bug with all caps words\ndataset.add_case(\n    name='acronyms',\n    inputs='the USA and FBI',\n    expected_output='The Usa And Fbi',  # Python's title() behavior\n)\n\n# Test with very long input\ndataset.add_case(\n    name='long_input',\n    inputs=' '.join(['word'] * 1000),\n    expected_output=' '.join(['Word'] * 1000),\n)\n\n# Save updated dataset\ndataset.to_file('title_case_tests.yaml')\n```", "url": "https://ai.pydantic.dev/examples/simple-validation/index.html#adding-more-cases", "page": "examples/simple-validation/index.html", "source_site": "pydantic_ai"}
{"title": "Using with pytest", "anchor": "using-with-pytest", "md_text": "Integrate with pytest for CI/CD:\n\n```\nimport pytest\n\nfrom pydantic_evals import Dataset\n\n\n# The function we're testing\ndef to_title_case(text: str) -> str:\n    \"\"\"Convert text to title case.\"\"\"\n    return text.title()\n\n\n@pytest.fixture\ndef title_case_dataset():\n    return Dataset.from_file('title_case_tests.yaml')\n\n\ndef test_title_case_evaluation(title_case_dataset):\n    \"\"\"Run evaluation tests.\"\"\"\n    report = title_case_dataset.evaluate_sync(to_title_case)\n\n    # All cases should pass\n    avg = report.averages()\n    assert avg is not None\n    assert avg.assertions == 1.0, f'Some tests failed (pass rate: {avg.assertions:.1%})'\n\n\ndef test_title_case_performance(title_case_dataset):\n    \"\"\"Verify performance.\"\"\"\n    report = title_case_dataset.evaluate_sync(to_title_case)\n\n    # All cases should complete quickly\n    for case in report.cases:\n        assert case.task_duration < 0.001, f'{case.name} took {case.task_duration}s'\n```", "url": "https://ai.pydantic.dev/examples/simple-validation/index.html#using-with-pytest", "page": "examples/simple-validation/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "md_text": "* **[Built-in Evaluators](../../evaluators/built-in/index.html)** - Explore all available evaluators\n* **[Custom Evaluators](../../evaluators/custom/index.html)** - Write your own evaluation logic\n* **[Dataset Management](../../how-to/dataset-management/index.html)** - Save, load, and manage datasets\n* **[Concurrency & Performance](../../how-to/concurrency/index.html)** - Optimize evaluation performance", "url": "https://ai.pydantic.dev/examples/simple-validation/index.html#next-steps", "page": "examples/simple-validation/index.html", "source_site": "pydantic_ai"}
{"title": "Comparison Evaluators", "anchor": "comparison-evaluators", "md_text": "", "url": "https://ai.pydantic.dev/evaluators/built-in/index.html#comparison-evaluators", "page": "evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "EqualsExpected", "anchor": "equalsexpected", "md_text": "Check if the output exactly equals the expected output from the case.\n\n```\nfrom pydantic_evals.evaluators import EqualsExpected\n\nEqualsExpected()\n```\n\n**Parameters:** None\n\n**Returns:** `bool` - `True` if `ctx.output == ctx.expected_output`\n\n**Example:**\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import EqualsExpected\n\ndataset = Dataset(\n    cases=[\n        Case(\n            name='addition',\n            inputs='2 + 2',\n            expected_output='4',\n        ),\n    ],\n    evaluators=[EqualsExpected()],\n)\n```\n\n**Notes:**\n\n* Skips evaluation if `expected_output` is `None` (returns empty dict `{}`)\n* Uses Python's `==` operator, so works with any comparable types\n* For structured data, considers nested equality\n\n---", "url": "https://ai.pydantic.dev/evaluators/built-in/index.html#equalsexpected", "page": "evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "Equals", "anchor": "equals", "md_text": "Check if the output equals a specific value.\n\n```\nfrom pydantic_evals.evaluators import Equals\n\nEquals(value='expected_result')\n```\n\n**Parameters:**\n\n* `value` (Any): The value to compare against\n* `evaluation_name` (str | None): Custom name for this evaluation in reports\n\n**Returns:** `bool` - `True` if `ctx.output == value`\n\n**Example:**\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Equals\n\n# Check output is always \"success\"\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[\n        Equals(value='success', evaluation_name='is_success'),\n    ],\n)\n```\n\n**Use Cases:**\n\n* Checking for sentinel values\n* Validating consistent outputs\n* Testing classification into specific categories\n\n---", "url": "https://ai.pydantic.dev/evaluators/built-in/index.html#equals", "page": "evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "Contains", "anchor": "contains", "md_text": "Check if the output contains a specific value or substring.\n\n```\nfrom pydantic_evals.evaluators import Contains\n\nContains(\n    value='substring',\n    case_sensitive=True,\n    as_strings=False,\n)\n```\n\n**Parameters:**\n\n* `value` (Any): The value to search for\n* `case_sensitive` (bool): Case-sensitive comparison for strings (default: `True`)\n* `as_strings` (bool): Convert both values to strings before checking (default: `False`)\n* `evaluation_name` (str | None): Custom name for this evaluation in reports\n\n**Returns:** [`EvaluationReason`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluationReason) - Pass/fail with explanation\n\n**Behavior:**\n\nFor **strings**: checks substring containment\n\n* `Contains(value='hello', case_sensitive=False)`\n* Matches: \"Hello World\", \"say hello\", \"HELLO\"\n* Doesn't match: \"hi there\"\n\nFor **lists/tuples**: checks membership\n\n* `Contains(value='apple')`\n* Matches: `['apple', 'banana']`, `('apple',)`\n* Doesn't match: `['apples', 'orange']`\n\nFor **dicts**: checks key-value pairs\n\n* `Contains(value={'name': 'Alice'})`\n* Matches: `{'name': 'Alice', 'age': 30}`\n* Doesn't match: `{'name': 'Bob'}`\n\n**Example:**\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Contains\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[\n        # Check for required keywords\n        Contains(value='terms and conditions', case_sensitive=False),\n        # Check for PII (fail if found)\n        # Note: Use a custom evaluator that returns False when PII found\n    ],\n)\n```\n\n**Use Cases:**\n\n* Required content verification\n* Keyword detection\n* PII/sensitive data detection\n* Multi-value validation\n\n---", "url": "https://ai.pydantic.dev/evaluators/built-in/index.html#contains", "page": "evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "Type Validation", "anchor": "type-validation", "md_text": "", "url": "https://ai.pydantic.dev/evaluators/built-in/index.html#type-validation", "page": "evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "IsInstance", "anchor": "isinstance", "md_text": "Check if the output is an instance of a type with the given name.\n\n```\nfrom pydantic_evals.evaluators import IsInstance\n\nIsInstance(type_name='str')\n```\n\n**Parameters:**\n\n* `type_name` (str): The type name to check (uses `__name__` or `__qualname__`)\n* `evaluation_name` (str | None): Custom name for this evaluation in reports\n\n**Returns:** [`EvaluationReason`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluationReason) - Pass/fail with type information\n\n**Example:**\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import IsInstance\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[\n        # Check output is always a string\n        IsInstance(type_name='str'),\n        # Check for Pydantic model\n        IsInstance(type_name='MyModel'),\n        # Check for dict\n        IsInstance(type_name='dict'),\n    ],\n)\n```\n\n**Notes:**\n\n* Matches against both `__name__` and `__qualname__` of the type\n* Works with built-in types (`str`, `int`, `dict`, `list`, etc.)\n* Works with custom classes and Pydantic models\n* Checks the entire MRO (Method Resolution Order) for inheritance\n\n**Use Cases:**\n\n* Format validation\n* Structured output verification\n* Type consistency checks\n\n---", "url": "https://ai.pydantic.dev/evaluators/built-in/index.html#isinstance", "page": "evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "Performance Evaluation", "anchor": "performance-evaluation", "md_text": "", "url": "https://ai.pydantic.dev/evaluators/built-in/index.html#performance-evaluation", "page": "evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "MaxDuration", "anchor": "maxduration", "md_text": "Check if task execution time is under a maximum threshold.\n\n```\nfrom datetime import timedelta\n\nfrom pydantic_evals.evaluators import MaxDuration\n\nMaxDuration(seconds=2.0)\n# or\nMaxDuration(seconds=timedelta(seconds=2))\n```\n\n**Parameters:**\n\n* `seconds` (float | timedelta): Maximum allowed duration\n\n**Returns:** `bool` - `True` if `ctx.duration <= seconds`\n\n**Example:**\n\n```\nfrom datetime import timedelta\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import MaxDuration\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[\n        # SLA: must respond in under 2 seconds\n        MaxDuration(seconds=2.0),\n        # Or using timedelta\n        MaxDuration(seconds=timedelta(milliseconds=500)),\n    ],\n)\n```\n\n**Use Cases:**\n\n* SLA compliance\n* Performance regression testing\n* Latency requirements\n* Timeout validation\n\n**See Also:** [Concurrency & Performance](../../how-to/concurrency/index.html)\n\n---", "url": "https://ai.pydantic.dev/evaluators/built-in/index.html#maxduration", "page": "evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "LLM-as-a-Judge", "anchor": "llm-as-a-judge", "md_text": "", "url": "https://ai.pydantic.dev/evaluators/built-in/index.html#llm-as-a-judge", "page": "evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "LLMJudge", "anchor": "llmjudge", "md_text": "Use an LLM to evaluate subjective qualities based on a rubric.\n\n```\nfrom pydantic_evals.evaluators import LLMJudge\n\nLLMJudge(\n    rubric='Response is accurate and helpful',\n    model='openai:gpt-4o',\n    include_input=False,\n    include_expected_output=False,\n    model_settings=None,\n    score=False,\n    assertion={'include_reason': True},\n)\n```\n\n**Parameters:**\n\n* `rubric` (str): The evaluation criteria (required)\n* `model` (Model | KnownModelName | None): Model to use (default: `'openai:gpt-4o'`)\n* `include_input` (bool): Include task inputs in the prompt (default: `False`)\n* `include_expected_output` (bool): Include expected output in the prompt (default: `False`)\n* `model_settings` (ModelSettings | None): Custom model settings\n* `score` (OutputConfig | False): Configure score output (default: `False`)\n* `assertion` (OutputConfig | False): Configure assertion output (default: includes reason)\n\n**Returns:** Depends on `score` and `assertion` parameters (see below)\n\n**Output Modes:**\n\nBy default, returns a **boolean assertion** with reason:\n\n* `LLMJudge(rubric='Response is polite')`\n* Returns: `{'LLMJudge_pass': EvaluationReason(value=True, reason='...')}`\n\nReturn a **score** (0.0 to 1.0) instead:\n\n* `LLMJudge(rubric='Response quality', score={'include_reason': True}, assertion=False)`\n* Returns: `{'LLMJudge_score': EvaluationReason(value=0.85, reason='...')}`\n\nReturn **both** score and assertion:\n\n* `LLMJudge(rubric='Response quality', score={'include_reason': True}, assertion={'include_reason': True})`\n* Returns: `{'LLMJudge_score': EvaluationReason(value=0.85, reason='...'), 'LLMJudge_pass': EvaluationReason(value=True, reason='...')}`\n\n**Customize evaluation names:**\n\n* `LLMJudge(rubric='Response is factually accurate', assertion={'evaluation_name': 'accuracy', 'include_reason': True})`\n* Returns: `{'accuracy': EvaluationReason(value=True, reason='...')}`\n\n**Example:**\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\ndataset = Dataset(\n    cases=[Case(inputs='test', expected_output='result')],\n    evaluators=[\n        # Basic accuracy check\n        LLMJudge(\n            rubric='Response is factually accurate',\n            include_input=True,\n        ),\n        # Quality score with different model\n        LLMJudge(\n            rubric='Overall response quality',\n            model='anthropic:claude-3-7-sonnet-latest',\n            score={'evaluation_name': 'quality', 'include_reason': False},\n            assertion=False,\n        ),\n        # Check against expected output\n        LLMJudge(\n            rubric='Response matches the expected answer semantically',\n            include_input=True,\n            include_expected_output=True,\n        ),\n    ],\n)\n```\n\n**See Also:** [LLM Judge Deep Dive](../llm-judge/index.html)\n\n---", "url": "https://ai.pydantic.dev/evaluators/built-in/index.html#llmjudge", "page": "evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "Span-Based Evaluation", "anchor": "span-based-evaluation", "md_text": "", "url": "https://ai.pydantic.dev/evaluators/built-in/index.html#span-based-evaluation", "page": "evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "HasMatchingSpan", "anchor": "hasmatchingspan", "md_text": "Check if OpenTelemetry spans match a query (requires Logfire configuration).\n\n```\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\nHasMatchingSpan(\n    query={'name_contains': 'tool_call'},\n    evaluation_name='called_tool',\n)\n```\n\n**Parameters:**\n\n* `query` ([`SpanQuery`](../../pydantic_evals/otel/index.html#pydantic_evals.otel.SpanQuery)): Query to match against spans\n* `evaluation_name` (str | None): Custom name for this evaluation in reports\n\n**Returns:** `bool` - `True` if any span matches the query\n\n**Example:**\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[\n        # Check that a specific tool was called\n        HasMatchingSpan(\n            query={'name_contains': 'search_database'},\n            evaluation_name='used_database',\n        ),\n        # Check for errors\n        HasMatchingSpan(\n            query={'has_attributes': {'error': True}},\n            evaluation_name='had_errors',\n        ),\n        # Check duration constraints\n        HasMatchingSpan(\n            query={\n                'name_equals': 'llm_call',\n                'max_duration': 2.0,  # seconds\n            },\n            evaluation_name='llm_fast_enough',\n        ),\n    ],\n)\n```\n\n**See Also:** [Span-Based Evaluation](../span-based/index.html)\n\n---", "url": "https://ai.pydantic.dev/evaluators/built-in/index.html#hasmatchingspan", "page": "evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "Quick Reference Table", "anchor": "quick-reference-table", "md_text": "| Evaluator | Purpose | Return Type | Cost | Speed |\n| --- | --- | --- | --- | --- |\n| [`EqualsExpected`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EqualsExpected) | Exact match with expected | `bool` | Free | Instant |\n| [`Equals`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.Equals) | Equals specific value | `bool` | Free | Instant |\n| [`Contains`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.Contains) | Contains value/substring | `bool` + reason | Free | Instant |\n| [`IsInstance`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.IsInstance) | Type validation | `bool` + reason | Free | Instant |\n| [`MaxDuration`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.MaxDuration) | Performance threshold | `bool` | Free | Instant |\n| [`LLMJudge`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.LLMJudge) | Subjective quality | `bool` and/or `float` | $$ | Slow |\n| [`HasMatchingSpan`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.HasMatchingSpan) | Behavioral check | `bool` | Free | Fast |", "url": "https://ai.pydantic.dev/evaluators/built-in/index.html#quick-reference-table", "page": "evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "Combining Evaluators", "anchor": "combining-evaluators", "md_text": "Best practice is to combine fast deterministic checks with slower LLM evaluations:\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import (\n    Contains,\n    IsInstance,\n    LLMJudge,\n    MaxDuration,\n)\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[\n        # Fast checks first (fail fast)\n        IsInstance(type_name='str'),\n        Contains(value='required_field'),\n        MaxDuration(seconds=2.0),\n        # Expensive LLM checks last\n        LLMJudge(rubric='Response is helpful and accurate'),\n    ],\n)\n```\n\nThis approach:\n\n1. Catches format/structure issues immediately\n2. Validates required content quickly\n3. Only runs expensive LLM evaluation if basic checks pass\n4. Provides comprehensive quality assessment", "url": "https://ai.pydantic.dev/evaluators/built-in/index.html#combining-evaluators", "page": "evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "md_text": "* **[LLM Judge](../llm-judge/index.html)** - Deep dive on LLM-as-a-Judge evaluation\n* **[Custom Evaluators](../custom/index.html)** - Write your own evaluation logic\n* **[Span-Based Evaluation](../span-based/index.html)** - Using OpenTelemetry spans for behavioral checks", "url": "https://ai.pydantic.dev/evaluators/built-in/index.html#next-steps", "page": "evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "When to Use Different Evaluators", "anchor": "when-to-use-different-evaluators", "md_text": "", "url": "https://ai.pydantic.dev/evaluators/overview/index.html#when-to-use-different-evaluators", "page": "evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Deterministic Checks (Fast & Reliable)", "anchor": "deterministic-checks-fast--reliable", "md_text": "Use deterministic evaluators when you can define exact rules:\n\n| Evaluator | Use Case | Example |\n| --- | --- | --- |\n| [`EqualsExpected`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EqualsExpected) | Exact output match | Structured data, classification |\n| [`Equals`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.Equals) | Equals specific value | Checking for sentinel values |\n| [`Contains`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.Contains) | Substring/element check | Required keywords, PII detection |\n| [`IsInstance`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.IsInstance) | Type validation | Format validation |\n| [`MaxDuration`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.MaxDuration) | Performance threshold | SLA compliance |\n| [`HasMatchingSpan`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.HasMatchingSpan) | Behavior verification | Tool calls, code paths |\n\n**Advantages:**\n\n* Fast execution (microseconds to milliseconds)\n* Deterministic results\n* No cost\n* Easy to debug\n\n**When to use:**\n\n* Format validation (JSON structure, type checking)\n* Required content checks (must contain X, must not contain Y)\n* Performance requirements (latency, token counts)\n* Behavioral checks (which tools were called, which code paths executed)", "url": "https://ai.pydantic.dev/evaluators/overview/index.html#deterministic-checks-fast--reliable", "page": "evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "LLM-as-a-Judge (Flexible & Nuanced)", "anchor": "llm-as-a-judge-flexible--nuanced", "md_text": "Use [`LLMJudge`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.LLMJudge) when evaluation requires understanding or judgment:\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\ndataset = Dataset(\n    cases=[Case(inputs='What is 2+2?', expected_output='4')],\n    evaluators=[\n        LLMJudge(\n            rubric='Response is factually accurate based on the input',\n            include_input=True,\n        )\n    ],\n)\n```\n\n**Advantages:**\n\n* Can evaluate subjective qualities (helpfulness, tone, creativity)\n* Understands natural language\n* Can follow complex rubrics\n* Flexible across domains\n\n**Disadvantages:**\n\n* Slower (seconds per evaluation)\n* Costs money\n* Non-deterministic\n* Can have biases\n\n**When to use:**\n\n* Factual accuracy\n* Relevance and helpfulness\n* Tone and style\n* Completeness\n* Following instructions\n* RAG quality (groundedness, citation accuracy)", "url": "https://ai.pydantic.dev/evaluators/overview/index.html#llm-as-a-judge-flexible--nuanced", "page": "evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Evaluators", "anchor": "custom-evaluators", "md_text": "Custom evaluators can be useful if you want to make use of any evaluation logic we don't provide with the framework.\nThey are frequently useful for domain-specific logic:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass ValidSQL(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        try:\n            import sqlparse\n            sqlparse.parse(ctx.output)\n            return True\n        except Exception:\n            return False\n```\n\n**When to use:**\n\n* Domain-specific validation (SQL syntax, regex patterns, business rules)\n* External API calls (running generated code, checking databases)\n* Complex calculations (precision/recall, BLEU scores)\n* Integration checks (does API call succeed?)", "url": "https://ai.pydantic.dev/evaluators/overview/index.html#custom-evaluators", "page": "evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluation Types", "anchor": "evaluation-types", "md_text": "For full detail about precisely what custom Evaluators may return, see [Custom Evaluator Return Types](../custom/index.html#return-types).\n\nEvaluators essentially return three types of results:", "url": "https://ai.pydantic.dev/evaluators/overview/index.html#evaluation-types", "page": "evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "1. Assertions (bool)", "anchor": "1-assertions-bool", "md_text": "Pass/fail checks that appear as ✔ or ✗ in reports:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass HasKeyword(Evaluator):\n    keyword: str\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return self.keyword in ctx.output\n```\n\n**Use for:** Binary checks, quality gates, compliance requirements", "url": "https://ai.pydantic.dev/evaluators/overview/index.html#1-assertions-bool", "page": "evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "2. Scores (int or float)", "anchor": "2-scores-int-or-float", "md_text": "Numeric metrics:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass ConfidenceScore(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> float:\n        # Analyze and return score\n        return 0.87  # 87% confidence\n```\n\n**Use for:** Quality metrics, ranking, A/B testing, regression tracking", "url": "https://ai.pydantic.dev/evaluators/overview/index.html#2-scores-int-or-float", "page": "evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "3. Labels (str)", "anchor": "3-labels-str", "md_text": "Categorical classifications:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass SentimentClassifier(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> str:\n        if 'error' in ctx.output.lower():\n            return 'error'\n        elif 'success' in ctx.output.lower():\n            return 'success'\n        return 'neutral'\n```\n\n**Use for:** Classification, error categorization, quality buckets", "url": "https://ai.pydantic.dev/evaluators/overview/index.html#3-labels-str", "page": "evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Multiple Results", "anchor": "multiple-results", "md_text": "You can return multiple evaluations from a single evaluator:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass ComprehensiveCheck(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> dict[str, bool | float | str]:\n        return {\n            'valid_format': self._check_format(ctx.output),  # bool\n            'quality_score': self._score_quality(ctx.output),  # float\n            'category': self._classify(ctx.output),  # str\n        }\n\n    def _check_format(self, output: str) -> bool:\n        return True\n\n    def _score_quality(self, output: str) -> float:\n        return 0.85\n\n    def _classify(self, output: str) -> str:\n        return 'good'\n```", "url": "https://ai.pydantic.dev/evaluators/overview/index.html#multiple-results", "page": "evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Combining Evaluators", "anchor": "combining-evaluators", "md_text": "Mix and match evaluators to create comprehensive evaluation suites:\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import (\n    Contains,\n    IsInstance,\n    LLMJudge,\n    MaxDuration,\n)\n\ndataset = Dataset(\n    cases=[Case(inputs='test', expected_output='result')],\n    evaluators=[\n        # Fast deterministic checks first\n        IsInstance(type_name='str'),\n        Contains(value='required_field'),\n        MaxDuration(seconds=2.0),\n        # Slower LLM checks after\n        LLMJudge(\n            rubric='Response is accurate and helpful',\n            include_input=True,\n        ),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/evaluators/overview/index.html#combining-evaluators", "page": "evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Case-specific evaluators", "anchor": "case-specific-evaluators", "md_text": "Case-specific evaluators are one of the most powerful features for building comprehensive evaluation suites. You can attach evaluators to individual [`Case`](../../pydantic_evals/dataset/index.html#pydantic_evals.dataset.Case) objects that only run for those specific cases:\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import IsInstance, LLMJudge\n\ndataset = Dataset(\n    cases=[\n        Case(\n            name='greeting_response',\n            inputs='Say hello',\n            evaluators=[\n                # This evaluator only runs for this case\n                LLMJudge(\n                    rubric='Response is warm and friendly, uses casual tone',\n                    include_input=True,\n                ),\n            ],\n        ),\n        Case(\n            name='formal_response',\n            inputs='Write a business email',\n            evaluators=[\n                # Different requirements for this case\n                LLMJudge(\n                    rubric='Response is professional and formal, uses business language',\n                    include_input=True,\n                ),\n            ],\n        ),\n    ],\n    evaluators=[\n        # This runs for ALL cases\n        IsInstance(type_name='str'),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/evaluators/overview/index.html#case-specific-evaluators", "page": "evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Why Case-Specific Evaluators Matter", "anchor": "why-case-specific-evaluators-matter", "md_text": "Case-specific evaluators solve a fundamental problem with one-size-fits-all evaluation: **if you could write a single evaluator rubric that perfectly captured your requirements across all cases, you'd just incorporate that rubric into your agent's instructions**. (Note: this is less relevant in cases where you want to use a cheaper model in production and assess it using a more expensive model, but in many cases it makes sense to use the best model you can in production.)\n\nThe power of case-specific evaluation comes from the nuance:\n\n* **Different cases have different requirements**: A customer support response needs empathy; a technical API response needs precision\n* **Avoid \"inmates running the asylum\"**: If your LLMJudge rubric is generic enough to work everywhere, your agent should already be following it\n* **Capture nuanced golden behavior**: Each case can specify exactly what \"good\" looks like for that scenario", "url": "https://ai.pydantic.dev/evaluators/overview/index.html#why-case-specific-evaluators-matter", "page": "evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Building Golden Datasets with Case-Specific LLMJudge", "anchor": "building-golden-datasets-with-case-specific-llmjudge", "md_text": "A particularly powerful pattern is using case-specific [`LLMJudge`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.LLMJudge) evaluators to quickly build comprehensive, maintainable evaluation suites. Instead of needing exact `expected_output` values, you can describe what you care about:\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\ndataset = Dataset(\n    cases=[\n        Case(\n            name='handle_refund_request',\n            inputs={'query': 'I want my money back', 'order_id': '12345'},\n            evaluators=[\n                LLMJudge(\n                    rubric=\"\"\"\n                    Response should:\n                    1. Acknowledge the refund request empathetically\n                    2. Ask for the reason for the refund\n                    3. Mention our 30-day refund policy\n                    4. NOT process the refund immediately (needs manager approval)\n                    \"\"\",\n                    include_input=True,\n                ),\n            ],\n        ),\n        Case(\n            name='handle_shipping_question',\n            inputs={'query': 'Where is my order?', 'order_id': '12345'},\n            evaluators=[\n                LLMJudge(\n                    rubric=\"\"\"\n                    Response should:\n                    1. Confirm the order number\n                    2. Provide tracking information\n                    3. Give estimated delivery date\n                    4. Be brief and factual (not overly apologetic)\n                    \"\"\",\n                    include_input=True,\n                ),\n            ],\n        ),\n        Case(\n            name='handle_angry_customer',\n            inputs={'query': 'This is completely unacceptable!', 'order_id': '12345'},\n            evaluators=[\n                LLMJudge(\n                    rubric=\"\"\"\n                    Response should:\n                    1. Prioritize de-escalation with empathy\n                    2. Avoid being defensive\n                    3. Offer concrete next steps\n                    4. Use phrases like \"I understand\" and \"Let me help\"\n                    \"\"\",\n                    include_input=True,\n                ),\n            ],\n        ),\n    ],\n)\n```\n\nThis approach lets you:\n\n* **Build comprehensive test suites quickly**: Just describe what you want per case\n* **Maintain easily**: Update rubrics as requirements change, without regenerating outputs\n* **Cover edge cases naturally**: Add new cases with specific requirements as you discover them\n* **Capture domain knowledge**: Each rubric documents what \"good\" means for that scenario\n\nThe LLM evaluator excels at understanding nuanced requirements and assessing compliance, making this a practical way to create thorough evaluation coverage without brittleness.", "url": "https://ai.pydantic.dev/evaluators/overview/index.html#building-golden-datasets-with-case-specific-llmjudge", "page": "evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Async vs Sync", "anchor": "async-vs-sync", "md_text": "Evaluators can be sync or async:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass SyncEvaluator(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return True\n\n\nasync def some_async_operation() -> bool:\n    return True\n\n\n@dataclass\nclass AsyncEvaluator(Evaluator):\n    async def evaluate(self, ctx: EvaluatorContext) -> bool:\n        result = await some_async_operation()\n        return result\n```\n\nPydantic Evals handles both automatically. Use async when:\n- Making API calls\n- Running database queries\n- Performing I/O operations\n- Calling LLMs (like [`LLMJudge`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.LLMJudge))", "url": "https://ai.pydantic.dev/evaluators/overview/index.html#async-vs-sync", "page": "evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluation Context", "anchor": "evaluation-context", "md_text": "All evaluators receive an [`EvaluatorContext`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorContext):\n\n* `ctx.inputs` - Task inputs\n* `ctx.output` - Task output (to evaluate)\n* `ctx.expected_output` - Expected output (if provided)\n* `ctx.metadata` - Case metadata (if provided)\n* `ctx.duration` - Task execution time (seconds)\n* `ctx.span_tree` - OpenTelemetry spans (if logfire configured)\n* `ctx.metrics` - Custom metrics dict\n* `ctx.attributes` - Custom attributes dict\n\nThis gives evaluators full context to make informed assessments.", "url": "https://ai.pydantic.dev/evaluators/overview/index.html#evaluation-context", "page": "evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Error Handling", "anchor": "error-handling", "md_text": "If an evaluator raises an exception, it's captured as an [`EvaluatorFailure`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorFailure):\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\ndef risky_operation(output: str) -> bool:\n    # This might raise an exception\n    if 'error' in output:\n        raise ValueError('Found error in output')\n    return True\n\n\n@dataclass\nclass RiskyEvaluator(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        # If this raises an exception, it will be captured\n        result = risky_operation(ctx.output)\n        return result\n```\n\nFailures appear in `report.cases[i].evaluator_failures` with:\n\n* Evaluator name\n* Error message\n* Full stacktrace\n\nUse retry configuration to handle transient failures (see [Retry Strategies](../../how-to/retry-strategies/index.html)).", "url": "https://ai.pydantic.dev/evaluators/overview/index.html#error-handling", "page": "evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "md_text": "* **[Built-in Evaluators](../built-in/index.html)** - Complete reference of all provided evaluators\n* **[LLM Judge](../llm-judge/index.html)** - Deep dive on LLM-as-a-Judge evaluation\n* **[Custom Evaluators](../custom/index.html)** - Write your own evaluation logic\n* **[Span-Based Evaluation](../span-based/index.html)** - Evaluate using OpenTelemetry spans", "url": "https://ai.pydantic.dev/evaluators/overview/index.html#next-steps", "page": "evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Basic Custom Evaluator", "anchor": "basic-custom-evaluator", "md_text": "All evaluators inherit from [`Evaluator`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.Evaluator) and must implement `evaluate`:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass ExactMatch(Evaluator):\n    \"\"\"Check if output exactly matches expected output.\"\"\"\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return ctx.output == ctx.expected_output\n```\n\n**Key Points:**\n\n* Use `@dataclass` decorator (required)\n* Inherit from `Evaluator`\n* Implement `evaluate(self, ctx: EvaluatorContext) -> EvaluatorOutput`\n* Return `bool`, `int`, `float`, `str`, [`EvaluationReason`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluationReason), or `dict` of these", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#basic-custom-evaluator", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluatorContext", "anchor": "evaluatorcontext", "md_text": "The context provides all information about the case execution:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass MyEvaluator(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        # Access case data\n        ctx.name              # Case name\n        ctx.inputs            # Task inputs\n        ctx.metadata          # Case metadata\n        ctx.expected_output   # Expected output (may be None)\n        ctx.output            # Actual output\n\n        # Performance data\n        ctx.duration          # Task execution time (seconds)\n\n        # Custom metrics/attributes (see metrics guide)\n        ctx.metrics           # dict[str, int | float]\n        ctx.attributes        # dict[str, Any]\n\n        # OpenTelemetry spans (if logfire configured)\n        ctx.span_tree         # SpanTree for behavioral checks\n\n        return True\n```", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#evaluatorcontext", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluator Parameters", "anchor": "evaluator-parameters", "md_text": "Add configurable parameters as dataclass fields:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass ContainsKeyword(Evaluator):\n    keyword: str\n    case_sensitive: bool = True\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        output = ctx.output\n        keyword = self.keyword\n\n        if not self.case_sensitive:\n            output = output.lower()\n            keyword = keyword.lower()\n\n        return keyword in output\n\n\n# Usage\ndataset = Dataset(\n    cases=[Case(name='test', inputs='This is important')],\n    evaluators=[\n        ContainsKeyword(keyword='important', case_sensitive=False),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#evaluator-parameters", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Return Types", "anchor": "return-types", "md_text": "", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#return-types", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Boolean Assertions", "anchor": "boolean-assertions", "md_text": "Simple pass/fail checks:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass IsValidJSON(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        try:\n            import json\n            json.loads(ctx.output)\n            return True\n        except Exception:\n            return False\n```", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#boolean-assertions", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Numeric Scores", "anchor": "numeric-scores", "md_text": "Quality metrics:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass LengthScore(Evaluator):\n    \"\"\"Score based on output length (0.0 = too short, 1.0 = ideal).\"\"\"\n\n    ideal_length: int = 100\n    tolerance: int = 20\n\n    def evaluate(self, ctx: EvaluatorContext) -> float:\n        length = len(ctx.output)\n        diff = abs(length - self.ideal_length)\n\n        if diff <= self.tolerance:\n            return 1.0\n        else:\n            # Decay score as we move away from ideal\n            score = max(0.0, 1.0 - (diff - self.tolerance) / self.ideal_length)\n            return score\n```", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#numeric-scores", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "String Labels", "anchor": "string-labels", "md_text": "Categorical classifications:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass SentimentClassifier(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> str:\n        output_lower = ctx.output.lower()\n\n        if any(word in output_lower for word in ['error', 'failed', 'wrong']):\n            return 'negative'\n        elif any(word in output_lower for word in ['success', 'correct', 'great']):\n            return 'positive'\n        else:\n            return 'neutral'\n```", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#string-labels", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "With Reasons", "anchor": "with-reasons", "md_text": "Add explanations to any result:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import EvaluationReason, Evaluator, EvaluatorContext\n\n\n@dataclass\nclass SmartCheck(Evaluator):\n    threshold: float = 0.8\n\n    def evaluate(self, ctx: EvaluatorContext) -> EvaluationReason:\n        score = self._calculate_score(ctx.output)\n\n        if score >= self.threshold:\n            return EvaluationReason(\n                value=True,\n                reason=f'Score {score:.2f} exceeds threshold {self.threshold}',\n            )\n        else:\n            return EvaluationReason(\n                value=False,\n                reason=f'Score {score:.2f} below threshold {self.threshold}',\n            )\n\n    def _calculate_score(self, output: str) -> float:\n        # Your scoring logic\n        return 0.75\n```", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#with-reasons", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Multiple Results", "anchor": "multiple-results", "md_text": "You can return multiple evaluations from one evaluator by returning a dictionary of key-value pairs.\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import (\n    EvaluationReason,\n    Evaluator,\n    EvaluatorContext,\n    EvaluatorOutput,\n)\n\n\n@dataclass\nclass ComprehensiveCheck(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> EvaluatorOutput:\n        format_valid = self._check_format(ctx.output)\n\n        return {\n            'valid_format': EvaluationReason(\n                value=format_valid,\n                reason='Valid JSON format' if format_valid else 'Invalid JSON format',\n            ),\n            'quality_score': self._score_quality(ctx.output),  # float\n            'category': self._classify(ctx.output),  # str\n        }\n\n    def _check_format(self, output: str) -> bool:\n        return output.startswith('{') and output.endswith('}')\n\n    def _score_quality(self, output: str) -> float:\n        return len(output) / 100.0\n\n    def _classify(self, output: str) -> str:\n        return 'short' if len(output) < 50 else 'long'\n```\n\nEach key in the returned dictionary becomes a separate result in the report. Values can be:\n\n* Primitives (`bool`, `int`, `float`, `str`)\n* [`EvaluationReason`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluationReason) (value with explanation)\n* Nested dicts of these types\n\nThe [`EvaluatorOutput`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorOutput) type represents all legal values\nthat can be returned by an evaluator, and can be used as the return type annotation for your custom `evaluate` method.", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#multiple-results", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Conditional Results", "anchor": "conditional-results", "md_text": "Evaluators can dynamically choose whether to produce results for a given case by returning an empty dict when not applicable:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import (\n    EvaluationReason,\n    Evaluator,\n    EvaluatorContext,\n    EvaluatorOutput,\n)\n\n\n@dataclass\nclass SQLValidator(Evaluator):\n    \"\"\"Only evaluates SQL queries, skips other outputs.\"\"\"\n\n    def evaluate(self, ctx: EvaluatorContext) -> EvaluatorOutput:\n        # Check if this case is relevant for SQL validation\n        if not isinstance(ctx.output, str) or not ctx.output.strip().upper().startswith(\n            ('SELECT', 'INSERT', 'UPDATE', 'DELETE')\n        ):\n            # Return empty dict - this evaluator doesn't apply to this case\n            return {}\n\n        # This is a SQL query, perform validation\n        try:\n            # In real implementation, use sqlparse or similar\n            is_valid = self._validate_sql(ctx.output)\n            return {\n                'sql_valid': is_valid,\n                'sql_complexity': self._measure_complexity(ctx.output),\n            }\n        except Exception as e:\n            return {'sql_valid': EvaluationReason(False, reason=f'Exception: {e}')}\n\n    def _validate_sql(self, query: str) -> bool:\n        # Simplified validation\n        return 'FROM' in query.upper() or 'INTO' in query.upper()\n\n    def _measure_complexity(self, query: str) -> str:\n        joins = query.upper().count('JOIN')\n        if joins == 0:\n            return 'simple'\n        elif joins <= 2:\n            return 'moderate'\n        else:\n            return 'complex'\n```\n\nThis pattern is useful when:\n\n* An evaluator only applies to certain types of outputs (e.g., code validation only for code outputs)\n* Validation depends on metadata tags (e.g., only evaluate cases marked with `language='python'`)\n* You want to run expensive checks conditionally based on other evaluator results\n\n**Key Points:**\n\n* Returning `{}` means \"this evaluator doesn't apply here\" - the case won't show results from this evaluator\n* Returning `{'key': value}` means \"this evaluator applies and here are the results\"\n* This is more practical than using case-level evaluators when it applies to a large fraction of cases, or when the\n  condition is based on the output itself\n* The evaluator still runs for every case, but can short-circuit when not relevant", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#conditional-results", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Async Evaluators", "anchor": "async-evaluators", "md_text": "Use `async def` for I/O-bound operations:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass APIValidator(Evaluator):\n    api_url: str\n\n    async def evaluate(self, ctx: EvaluatorContext) -> bool:\n        import httpx\n\n        async with httpx.AsyncClient() as client:\n            response = await client.post(\n                self.api_url,\n                json={'output': ctx.output},\n            )\n            return response.json()['valid']\n```\n\nPydantic Evals handles both sync and async evaluators automatically.", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#async-evaluators", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Using Metadata", "anchor": "using-metadata", "md_text": "Access case metadata for context-aware evaluation:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass DifficultyAwareScore(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> float:\n        # Base score\n        base_score = self._score_output(ctx.output)\n\n        # Adjust based on difficulty from metadata\n        if ctx.metadata and 'difficulty' in ctx.metadata:\n            difficulty = ctx.metadata['difficulty']\n\n            if difficulty == 'easy':\n                # Penalize mistakes more on easy questions\n                return base_score\n            elif difficulty == 'hard':\n                # Be more lenient on hard questions\n                return min(1.0, base_score * 1.2)\n\n        return base_score\n\n    def _score_output(self, output: str) -> float:\n        # Your scoring logic\n        return 0.8\n```", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#using-metadata", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Using Metrics", "anchor": "using-metrics", "md_text": "Access custom metrics set during task execution:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals import increment_eval_metric, set_eval_attribute\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n# In your task\ndef my_task(inputs: str) -> str:\n    result = f'processed: {inputs}'\n\n    # Record metrics\n    increment_eval_metric('api_calls', 3)\n    set_eval_attribute('used_cache', True)\n\n    return result\n\n\n# In your evaluator\n@dataclass\nclass EfficiencyCheck(Evaluator):\n    max_api_calls: int = 5\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        api_calls = ctx.metrics.get('api_calls', 0)\n        return api_calls <= self.max_api_calls\n```\n\nSee [Metrics & Attributes Guide](../../how-to/metrics-attributes/index.html) for more.", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#using-metrics", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Generic Type Parameters", "anchor": "generic-type-parameters", "md_text": "Make evaluators type-safe with generics:\n\n```\nfrom dataclasses import dataclass\nfrom typing import TypeVar\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\nInputsT = TypeVar('InputsT')\nOutputT = TypeVar('OutputT')\n\n\n@dataclass\nclass TypedEvaluator(Evaluator[InputsT, OutputT, dict]):\n    def evaluate(self, ctx: EvaluatorContext[InputsT, OutputT, dict]) -> bool:\n        # ctx.inputs and ctx.output are now properly typed\n        return True\n```", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#generic-type-parameters", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Evaluation Names", "anchor": "custom-evaluation-names", "md_text": "Control how evaluations appear in reports:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass CustomNameEvaluator(Evaluator):\n    check_type: str\n\n    def get_default_evaluation_name(self) -> str:\n        # Use check_type as the name instead of class name\n        return f'{self.check_type}_check'\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return True\n\n\n# In reports, appears as \"format_check\" instead of \"CustomNameEvaluator\"\nevaluator = CustomNameEvaluator(check_type='format')\n```\n\nOr use the `evaluation_name` field (if using the built-in pattern):\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass MyEvaluator(Evaluator):\n    evaluation_name: str | None = None\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return True\n\n\n# Usage\nMyEvaluator(evaluation_name='my_custom_name')\n```", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#custom-evaluation-names", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Real-World Examples", "anchor": "real-world-examples", "md_text": "", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#real-world-examples", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "SQL Validation", "anchor": "sql-validation", "md_text": "```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import EvaluationReason, Evaluator, EvaluatorContext\n\n\n@dataclass\nclass ValidSQL(Evaluator):\n    dialect: str = 'postgresql'\n\n    def evaluate(self, ctx: EvaluatorContext) -> EvaluationReason:\n        try:\n            import sqlparse\n            parsed = sqlparse.parse(ctx.output)\n\n            if not parsed:\n                return EvaluationReason(\n                    value=False,\n                    reason='Could not parse SQL',\n                )\n\n            # Check for dangerous operations\n            sql_upper = ctx.output.upper()\n            if 'DROP' in sql_upper or 'DELETE' in sql_upper:\n                return EvaluationReason(\n                    value=False,\n                    reason='Contains dangerous operations (DROP/DELETE)',\n                )\n\n            return EvaluationReason(\n                value=True,\n                reason='Valid SQL syntax',\n            )\n        except Exception as e:\n            return EvaluationReason(\n                value=False,\n                reason=f'SQL parsing error: {e}',\n            )\n```", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#sql-validation", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Code Execution", "anchor": "code-execution", "md_text": "```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import EvaluationReason, Evaluator, EvaluatorContext\n\n\n@dataclass\nclass ExecutablePython(Evaluator):\n    timeout_seconds: float = 5.0\n\n    async def evaluate(self, ctx: EvaluatorContext) -> EvaluationReason:\n        import asyncio\n        import os\n        import tempfile\n\n        # Write code to temp file\n        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n            f.write(ctx.output)\n            temp_path = f.name\n\n        try:\n            # Execute with timeout\n            process = await asyncio.create_subprocess_exec(\n                'python', temp_path,\n                stdout=asyncio.subprocess.PIPE,\n                stderr=asyncio.subprocess.PIPE,\n            )\n\n            try:\n                stdout, stderr = await asyncio.wait_for(\n                    process.communicate(),\n                    timeout=self.timeout_seconds,\n                )\n            except asyncio.TimeoutError:\n                process.kill()\n                return EvaluationReason(\n                    value=False,\n                    reason=f'Execution timeout after {self.timeout_seconds}s',\n                )\n\n            if process.returncode == 0:\n                return EvaluationReason(\n                    value=True,\n                    reason='Code executed successfully',\n                )\n            else:\n                return EvaluationReason(\n                    value=False,\n                    reason=f'Execution failed: {stderr.decode()}',\n                )\n        finally:\n            os.unlink(temp_path)\n```", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#code-execution", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "External API Validation", "anchor": "external-api-validation", "md_text": "```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass APIResponseValid(Evaluator):\n    api_endpoint: str\n    api_key: str\n\n    async def evaluate(self, ctx: EvaluatorContext) -> dict[str, bool | float]:\n        import httpx\n\n        try:\n            async with httpx.AsyncClient() as client:\n                response = await client.post(\n                    self.api_endpoint,\n                    headers={'Authorization': f'Bearer {self.api_key}'},\n                    json={'data': ctx.output},\n                    timeout=10.0,\n                )\n\n                result = response.json()\n\n                return {\n                    'api_reachable': True,\n                    'validation_passed': result.get('valid', False),\n                    'confidence_score': result.get('confidence', 0.0),\n                }\n        except Exception:\n            return {\n                'api_reachable': False,\n                'validation_passed': False,\n                'confidence_score': 0.0,\n            }\n```", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#external-api-validation", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Testing Evaluators", "anchor": "testing-evaluators", "md_text": "Test evaluators like any other Python code:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass ExactMatch(Evaluator):\n    \"\"\"Check if output exactly matches expected output.\"\"\"\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return ctx.output == ctx.expected_output\n\n\ndef test_exact_match():\n    evaluator = ExactMatch()\n\n    # Test match\n    ctx = EvaluatorContext(\n        name='test',\n        inputs='input',\n        metadata=None,\n        expected_output='expected',\n        output='expected',\n        duration=0.1,\n        _span_tree=None,\n        attributes={},\n        metrics={},\n    )\n    assert evaluator.evaluate(ctx) is True\n\n    # Test mismatch\n    ctx.output = 'different'\n    assert evaluator.evaluate(ctx) is False\n```", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#testing-evaluators", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Best Practices", "anchor": "best-practices", "md_text": "", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#best-practices", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "1. Keep Evaluators Focused", "anchor": "1-keep-evaluators-focused", "md_text": "Each evaluator should check one thing:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\ndef check_format(output: str) -> bool:\n    return output.startswith('{')\n\n\ndef check_content(output: str) -> bool:\n    return len(output) > 10\n\n\ndef check_length(output: str) -> bool:\n    return len(output) < 1000\n\n\ndef check_spelling(output: str) -> bool:\n    return True  # Placeholder\n\n\n# Bad: Doing too much\n@dataclass\nclass EverythingChecker(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> dict:\n        return {\n            'format_valid': check_format(ctx.output),\n            'content_good': check_content(ctx.output),\n            'length_ok': check_length(ctx.output),\n            'spelling_correct': check_spelling(ctx.output),\n        }\n\n\n# Good: Separate evaluators\n@dataclass\nclass FormatValidator(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return check_format(ctx.output)\n\n\n@dataclass\nclass ContentChecker(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return check_content(ctx.output)\n\n\n@dataclass\nclass LengthChecker(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return check_length(ctx.output)\n\n\n@dataclass\nclass SpellingChecker(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return check_spelling(ctx.output)\n```\n\nSome exceptions to this:\n\n* When there is a significant amount of shared computation or network request latency, it may be better to have a single evaluator calculate all dependent outputs together.\n* If multiple checks are tightly coupled or very closely related to each other, it may make sense to include all their logic in one evaluator.", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#1-keep-evaluators-focused", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "2. Handle Missing Data Gracefully", "anchor": "2-handle-missing-data-gracefully", "md_text": "```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import EvaluationReason, Evaluator, EvaluatorContext\n\n\n@dataclass\nclass SafeEvaluator(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> EvaluationReason:\n        if ctx.expected_output is None:\n            return EvaluationReason(\n                value=True,\n                reason='Skipped: no expected output provided',\n            )\n\n        # Your evaluation logic\n        ...\n```", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#2-handle-missing-data-gracefully", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "3. Provide Helpful Reasons", "anchor": "3-provide-helpful-reasons", "md_text": "```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import EvaluationReason, Evaluator, EvaluatorContext\n\n\n@dataclass\nclass HelpfulEvaluator(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> EvaluationReason:\n        # Bad\n        return EvaluationReason(value=False, reason='Failed')\n\n        # Good\n        return EvaluationReason(\n            value=False,\n            reason=f'Expected {ctx.expected_output!r}, got {ctx.output!r}',\n        )\n```", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#3-provide-helpful-reasons", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "4. Use Timeouts for External Calls", "anchor": "4-use-timeouts-for-external-calls", "md_text": "```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass APIEvaluator(Evaluator):\n    timeout: float = 10.0\n\n    async def _call_api(self, output: str) -> bool:\n        # Placeholder for API call\n        return True\n\n    async def evaluate(self, ctx: EvaluatorContext) -> bool:\n        import asyncio\n\n        try:\n            return await asyncio.wait_for(\n                self._call_api(ctx.output),\n                timeout=self.timeout,\n            )\n        except asyncio.TimeoutError:\n            return False\n```", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#4-use-timeouts-for-external-calls", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "md_text": "* **[Span-Based Evaluation](../span-based/index.html)** - Using OpenTelemetry spans\n* **[Examples](../../examples/simple-validation/index.html)** - Practical examples", "url": "https://ai.pydantic.dev/evaluators/custom/index.html#next-steps", "page": "evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "md_text": "Span-based evaluation enables you to evaluate **how** your AI system executes, not just **what** it produces. This is essential for complex agents where ensuring the desired behavior depends on the execution path taken, not just the final output.", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#overview", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Why Span-Based Evaluation?", "anchor": "why-span-based-evaluation", "md_text": "Traditional evaluators assess task inputs and outputs. For simple tasks, this may be sufficient—if the output is correct, the task succeeded. But for complex multi-step agents, the *process* matters as much as the result:\n\n* **A correct answer reached incorrectly** - An agent might produce the right output by accident (e.g., guessing, using cached data when it should have searched, calling the wrong tools but getting lucky)\n* **Verification of required behaviors** - You need to ensure specific tools were called, certain code paths executed, or particular patterns followed\n* **Performance and efficiency** - The agent should reach the answer efficiently, without unnecessary tool calls, infinite loops, or excessive retries\n* **Safety and compliance** - Critical to verify that dangerous operations weren't attempted, sensitive data wasn't accessed inappropriately, or guardrails weren't bypassed", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#why-span-based-evaluation", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Real-World Scenarios", "anchor": "real-world-scenarios", "md_text": "Span-based evaluation is particularly valuable for:\n\n* **RAG systems** - Verify documents were retrieved and reranked before generation, not just that the answer included citations\n* **Multi-agent coordination** - Ensure the orchestrator delegated to the right specialist agents in the correct order\n* **Tool-calling agents** - Confirm specific tools were used (or avoided), and in the expected sequence\n* **Debugging and regression testing** - Catch behavioral regressions where outputs remain correct but the internal logic deteriorates\n* **Production alignment** - Ensure your evaluation assertions operate on the same telemetry data captured in production, so eval insights directly translate to production monitoring", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#real-world-scenarios", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "How It Works", "anchor": "how-it-works", "md_text": "When you configure logfire (`logfire.configure()`), Pydantic Evals captures all OpenTelemetry spans generated during task execution. You can then write evaluators that assert conditions on:\n\n* **Which tools were called** - `HasMatchingSpan(query={'name_contains': 'search_tool'})`\n* **Code paths executed** - Verify specific functions ran or particular branches taken\n* **Timing characteristics** - Check that operations complete within SLA bounds\n* **Error conditions** - Detect retries, fallbacks, or specific failure modes\n* **Execution structure** - Verify parent-child relationships, delegation patterns, or execution order\n\nThis creates a fundamentally different evaluation paradigm: you're testing behavioral contracts, not just input-output relationships.", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#how-it-works", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Basic Usage", "anchor": "basic-usage", "md_text": "```\nimport logfire\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\n# Configure logfire to capture spans\nlogfire.configure(send_to_logfire='if-token-present')\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[\n        # Check that database was queried\n        HasMatchingSpan(\n            query={'name_contains': 'database_query'},\n            evaluation_name='used_database',\n        ),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#basic-usage", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "HasMatchingSpan Evaluator", "anchor": "hasmatchingspan-evaluator", "md_text": "The [`HasMatchingSpan`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.HasMatchingSpan) evaluator checks if any span matches a query:\n\n```\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\nHasMatchingSpan(\n    query={'name_contains': 'test'},\n    evaluation_name='span_check',\n)\n```\n\n**Returns:** `bool` - `True` if any span matches the query", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#hasmatchingspan-evaluator", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "SpanQuery Reference", "anchor": "spanquery-reference", "md_text": "A [`SpanQuery`](../../pydantic_evals/otel/index.html#pydantic_evals.otel.SpanQuery) is a dictionary with query conditions:", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#spanquery-reference", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Name Conditions", "anchor": "name-conditions", "md_text": "Match spans by name:\n\n```\n# Exact name match\n{'name_equals': 'search_database'}\n\n# Contains substring\n{'name_contains': 'tool_call'}\n\n# Regex pattern\n{'name_matches_regex': r'llm_call_\\d+'}\n```", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#name-conditions", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Attribute Conditions", "anchor": "attribute-conditions", "md_text": "Match spans with specific attributes:\n\n```\n# Has specific attribute values\n{'has_attributes': {'operation': 'search', 'status': 'success'}}\n\n# Has attribute keys (any value)\n{'has_attribute_keys': ['user_id', 'request_id']}\n```", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#attribute-conditions", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Duration Conditions", "anchor": "duration-conditions", "md_text": "Match based on execution time:\n\n```\nfrom datetime import timedelta\n\n# Minimum duration\n{'min_duration': 1.0}  # seconds\n{'min_duration': timedelta(seconds=1)}\n\n# Maximum duration\n{'max_duration': 5.0}  # seconds\n{'max_duration': timedelta(seconds=5)}\n\n# Range\n{'min_duration': 0.5, 'max_duration': 2.0}\n```", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#duration-conditions", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Logical Operators", "anchor": "logical-operators", "md_text": "Combine conditions:\n\n```\n# NOT\n{'not_': {'name_contains': 'error'}}\n\n# AND (all must match)\n{'and_': [\n    {'name_contains': 'tool'},\n    {'max_duration': 1.0},\n]}\n\n# OR (any must match)\n{'or_': [\n    {'name_equals': 'search'},\n    {'name_equals': 'query'},\n]}\n```", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#logical-operators", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Child/Descendant Conditions", "anchor": "childdescendant-conditions", "md_text": "Query relationships between spans:\n\n```\n# Count direct children\n{'min_child_count': 1}\n{'max_child_count': 5}\n\n# Some child matches query\n{'some_child_has': {'name_contains': 'retry'}}\n\n# All children match query\n{'all_children_have': {'max_duration': 0.5}}\n\n# No children match query\n{'no_child_has': {'has_attributes': {'error': True}}}\n\n# Descendant queries (recursive)\n{'min_descendant_count': 5}\n{'some_descendant_has': {'name_contains': 'api_call'}}\n```", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#childdescendant-conditions", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Ancestor/Depth Conditions", "anchor": "ancestordepth-conditions", "md_text": "Query span hierarchy:\n\n```\n# Depth (root spans have depth 0)\n{'min_depth': 1}  # Not a root span\n{'max_depth': 2}  # At most 2 levels deep\n\n# Ancestor queries\n{'some_ancestor_has': {'name_equals': 'agent_run'}}\n{'all_ancestors_have': {'max_duration': 10.0}}\n{'no_ancestor_has': {'has_attributes': {'error': True}}}\n```", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#ancestordepth-conditions", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Stop Recursing", "anchor": "stop-recursing", "md_text": "Control recursive queries:\n\n```\n{\n    'some_descendant_has': {'name_contains': 'expensive'},\n    'stop_recursing_when': {'name_equals': 'boundary'},\n}\n# Only search descendants until hitting a span named 'boundary'\n```", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#stop-recursing", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Practical Examples", "anchor": "practical-examples", "md_text": "", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#practical-examples", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Verify Tool Usage", "anchor": "verify-tool-usage", "md_text": "Check that specific tools were called:\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[\n        # Must call search tool\n        HasMatchingSpan(\n            query={'name_contains': 'search_tool'},\n            evaluation_name='used_search',\n        ),\n\n        # Must NOT call dangerous tool\n        HasMatchingSpan(\n            query={'not_': {'name_contains': 'delete_database'}},\n            evaluation_name='safe_execution',\n        ),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#verify-tool-usage", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Check Multiple Tools", "anchor": "check-multiple-tools", "md_text": "Verify a sequence of operations:\n\n```\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\nevaluators = [\n    HasMatchingSpan(\n        query={'name_contains': 'retrieve_context'},\n        evaluation_name='retrieved_context',\n    ),\n    HasMatchingSpan(\n        query={'name_contains': 'generate_response'},\n        evaluation_name='generated_response',\n    ),\n    HasMatchingSpan(\n        query={'and_': [\n            {'name_contains': 'cite'},\n            {'has_attribute_keys': ['source_id']},\n        ]},\n        evaluation_name='added_citations',\n    ),\n]\n```", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#check-multiple-tools", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Performance Assertions", "anchor": "performance-assertions", "md_text": "Ensure operations meet latency requirements:\n\n```\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\nevaluators = [\n    # Database queries should be fast\n    HasMatchingSpan(\n        query={'and_': [\n            {'name_contains': 'database'},\n            {'max_duration': 0.1},  # 100ms max\n        ]},\n        evaluation_name='fast_db_queries',\n    ),\n\n    # Overall should complete quickly\n    HasMatchingSpan(\n        query={'and_': [\n            {'name_equals': 'task_execution'},\n            {'max_duration': 2.0},\n        ]},\n        evaluation_name='within_sla',\n    ),\n]\n```", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#performance-assertions", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Error Detection", "anchor": "error-detection", "md_text": "Check for error conditions:\n\n```\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\nevaluators = [\n    # No errors occurred\n    HasMatchingSpan(\n        query={'not_': {'has_attributes': {'error': True}}},\n        evaluation_name='no_errors',\n    ),\n\n    # Retries happened\n    HasMatchingSpan(\n        query={'name_contains': 'retry'},\n        evaluation_name='had_retries',\n    ),\n\n    # Fallback was used\n    HasMatchingSpan(\n        query={'name_contains': 'fallback_model'},\n        evaluation_name='used_fallback',\n    ),\n]\n```", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#error-detection", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Complex Behavioral Checks", "anchor": "complex-behavioral-checks", "md_text": "Verify sophisticated behavior patterns:\n\n```\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\nevaluators = [\n    # Agent delegated to sub-agent\n    HasMatchingSpan(\n        query={'and_': [\n            {'name_contains': 'agent'},\n            {'some_child_has': {'name_contains': 'delegate'}},\n        ]},\n        evaluation_name='used_delegation',\n    ),\n\n    # Made multiple LLM calls with retries\n    HasMatchingSpan(\n        query={'and_': [\n            {'name_contains': 'llm_call'},\n            {'some_descendant_has': {'name_contains': 'retry'}},\n            {'min_descendant_count': 3},\n        ]},\n        evaluation_name='retry_pattern',\n    ),\n]\n```", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#complex-behavioral-checks", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Evaluators with SpanTree", "anchor": "custom-evaluators-with-spantree", "md_text": "For more complex span analysis, write custom evaluators:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass CustomSpanCheck(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> dict[str, bool | int]:\n        span_tree = ctx.span_tree\n\n        # Find specific spans\n        llm_spans = span_tree.find(lambda node: 'llm' in node.name)\n        tool_spans = span_tree.find(lambda node: 'tool' in node.name)\n\n        # Calculate metrics\n        total_llm_time = sum(\n            span.duration.total_seconds() for span in llm_spans\n        )\n\n        return {\n            'used_llm': len(llm_spans) > 0,\n            'used_tools': len(tool_spans) > 0,\n            'tool_count': len(tool_spans),\n            'llm_fast': total_llm_time < 2.0,\n        }\n```", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#custom-evaluators-with-spantree", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "SpanTree API", "anchor": "spantree-api", "md_text": "The [`SpanTree`](../../pydantic_evals/otel/index.html#pydantic_evals.otel.SpanTree) provides methods for span analysis:\n\n```\nfrom pydantic_evals.otel import SpanTree\n\n\n# Example API (requires span_tree from context)\ndef example_api(span_tree: SpanTree) -> None:\n    span_tree.find(lambda n: True)  # Find all matching nodes\n    span_tree.any({'name_contains': 'test'})  # Check if any span matches\n    span_tree.all({'name_contains': 'test'})  # Check if all spans match\n    span_tree.count({'name_contains': 'test'})  # Count matching spans\n\n    # Iteration\n    for node in span_tree:\n        print(node.name, node.duration, node.attributes)\n```", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#spantree-api", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "SpanNode Properties", "anchor": "spannode-properties", "md_text": "Each [`SpanNode`](../../pydantic_evals/otel/index.html#pydantic_evals.otel.SpanNode) has:\n\n```\nfrom pydantic_evals.otel import SpanNode\n\n\n# Example properties (requires node from context)\ndef example_properties(node: SpanNode) -> None:\n    _ = node.name  # Span name\n    _ = node.duration  # timedelta\n    _ = node.attributes  # dict[str, AttributeValue]\n    _ = node.start_timestamp  # datetime\n    _ = node.end_timestamp  # datetime\n    _ = node.children  # list[SpanNode]\n    _ = node.descendants  # list[SpanNode] (recursive)\n    _ = node.ancestors  # list[SpanNode]\n    _ = node.parent  # SpanNode | None\n```", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#spannode-properties", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Debugging Span Queries", "anchor": "debugging-span-queries", "md_text": "", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#debugging-span-queries", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "View Spans in Logfire", "anchor": "view-spans-in-logfire", "md_text": "If you're sending data to Logfire, you can view all spans in the web UI to understand the trace structure.", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#view-spans-in-logfire", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Print Span Tree", "anchor": "print-span-tree", "md_text": "```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass DebugSpans(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        for node in ctx.span_tree:\n            print(f\"{'  ' * len(node.ancestors)}{node.name} ({node.duration})\")\n        return True\n```", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#print-span-tree", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Query Testing", "anchor": "query-testing", "md_text": "Test queries incrementally:\n\n```\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\n# Start simple\nquery = {'name_contains': 'tool'}\n\n# Add conditions gradually\nquery = {'and_': [\n    {'name_contains': 'tool'},\n    {'max_duration': 1.0},\n]}\n\n# Test in evaluator\nHasMatchingSpan(query=query, evaluation_name='test')\n```", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#query-testing", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Use Cases", "anchor": "use-cases", "md_text": "", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#use-cases", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "RAG System Verification", "anchor": "rag-system-verification", "md_text": "Verify retrieval-augmented generation workflow:\n\n```\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\nevaluators = [\n    # Retrieved documents\n    HasMatchingSpan(\n        query={'name_contains': 'vector_search'},\n        evaluation_name='retrieved_docs',\n    ),\n\n    # Reranked results\n    HasMatchingSpan(\n        query={'name_contains': 'rerank'},\n        evaluation_name='reranked_results',\n    ),\n\n    # Generated with context\n    HasMatchingSpan(\n        query={'and_': [\n            {'name_contains': 'generate'},\n            {'has_attribute_keys': ['context_ids']},\n        ]},\n        evaluation_name='used_context',\n    ),\n]\n```", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#rag-system-verification", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Multi-Agent Systems", "anchor": "multi-agent-systems", "md_text": "Verify agent coordination:\n\n```\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\nevaluators = [\n    # Master agent ran\n    HasMatchingSpan(\n        query={'name_equals': 'master_agent'},\n        evaluation_name='master_ran',\n    ),\n\n    # Delegated to specialist\n    HasMatchingSpan(\n        query={'and_': [\n            {'name_contains': 'specialist_agent'},\n            {'some_ancestor_has': {'name_equals': 'master_agent'}},\n        ]},\n        evaluation_name='delegated_correctly',\n    ),\n\n    # No circular delegation\n    HasMatchingSpan(\n        query={'not_': {'and_': [\n            {'name_contains': 'agent'},\n            {'some_descendant_has': {'name_contains': 'agent'}},\n            {'some_ancestor_has': {'name_contains': 'agent'}},\n        ]}},\n        evaluation_name='no_circular_delegation',\n    ),\n]\n```", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#multi-agent-systems", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Tool Usage Patterns", "anchor": "tool-usage-patterns", "md_text": "Verify intelligent tool selection:\n\n```\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\nevaluators = [\n    # Used search before answering\n    HasMatchingSpan(\n        query={'and_': [\n            {'name_contains': 'search'},\n            {'some_ancestor_has': {'name_contains': 'answer'}},\n        ]},\n        evaluation_name='searched_before_answering',\n    ),\n\n    # Limited tool calls (no loops)\n    HasMatchingSpan(\n        query={'and_': [\n            {'name_contains': 'tool'},\n            {'max_child_count': 5},\n        ]},\n        evaluation_name='reasonable_tool_usage',\n    ),\n]\n```", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#tool-usage-patterns", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Best Practices", "anchor": "best-practices", "md_text": "1. **Start Simple**: Begin with basic name queries, add complexity as needed\n2. **Use Descriptive Names**: Name your spans well in your application code\n3. **Test Queries**: Verify queries work before running full evaluations\n4. **Combine with Other Evaluators**: Use span checks alongside output validation\n5. **Document Expectations**: Comment why specific spans should/shouldn't exist", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#best-practices", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "md_text": "* **[Logfire Integration](../../how-to/logfire-integration/index.html)** - Set up Logfire for span capture\n* **[Custom Evaluators](../custom/index.html)** - Write advanced span analysis\n* **[Built-in Evaluators](../built-in/index.html)** - Other evaluator types", "url": "https://ai.pydantic.dev/evaluators/span-based/index.html#next-steps", "page": "evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "When to Use LLM-as-a-Judge", "anchor": "when-to-use-llm-as-a-judge", "md_text": "LLM judges are ideal for evaluating qualities that require understanding and judgment:\n\n**Good Use Cases:**\n\n* Factual accuracy\n* Helpfulness and relevance\n* Tone and style compliance\n* Completeness of responses\n* Following complex instructions\n* RAG groundedness (does the answer use provided context?)\n* Citation accuracy\n\n**Poor Use Cases:**\n\n* Format validation (use [`IsInstance`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.IsInstance) instead)\n* Exact matching (use [`EqualsExpected`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EqualsExpected))\n* Performance checks (use [`MaxDuration`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.MaxDuration))\n* Deterministic logic (write a custom evaluator)", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#when-to-use-llm-as-a-judge", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Basic Usage", "anchor": "basic-usage", "md_text": "```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[\n        LLMJudge(rubric='Response is factually accurate'),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#basic-usage", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Configuration Options", "anchor": "configuration-options", "md_text": "", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#configuration-options", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Rubric", "anchor": "rubric", "md_text": "The `rubric` is your evaluation criteria. Be specific and clear:\n\n**Bad rubrics (vague):**\n\n```\nfrom pydantic_evals.evaluators import LLMJudge\n\nLLMJudge(rubric='Good response')  # Too vague\nLLMJudge(rubric='Check quality')  # What aspect of quality?\n```\n\n**Good rubrics (specific):**\n\n```\nfrom pydantic_evals.evaluators import LLMJudge\n\nLLMJudge(rubric='Response directly answers the user question without hallucination')\nLLMJudge(rubric='Response uses formal, professional language appropriate for business communication')\nLLMJudge(rubric='All factual claims in the response are supported by the provided context')\n```", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#rubric", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Including Context", "anchor": "including-context", "md_text": "Control what information the judge sees:\n\n```\nfrom pydantic_evals.evaluators import LLMJudge\n\n# Output only (default)\nLLMJudge(rubric='Response is polite')\n\n# Output + Input\nLLMJudge(\n    rubric='Response accurately answers the input question',\n    include_input=True,\n)\n\n# Output + Input + Expected Output\nLLMJudge(\n    rubric='Response is semantically equivalent to the expected output',\n    include_input=True,\n    include_expected_output=True,\n)\n```\n\n**Example:**\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\ndataset = Dataset(\n    cases=[\n        Case(\n            inputs='What is 2+2?',\n            expected_output='4',\n        ),\n    ],\n    evaluators=[\n        # This judge sees: output + inputs + expected_output\n        LLMJudge(\n            rubric='Response provides the same answer as expected, possibly with explanation',\n            include_input=True,\n            include_expected_output=True,\n        ),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#including-context", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Model Selection", "anchor": "model-selection", "md_text": "Choose the judge model based on cost/quality tradeoffs:\n\n```\nfrom pydantic_evals.evaluators import LLMJudge\n\n# Default: GPT-4o (good balance)\nLLMJudge(rubric='...')\n\n# Anthropic Claude (alternative default)\nLLMJudge(\n    rubric='...',\n    model='anthropic:claude-3-7-sonnet-latest',\n)\n\n# Cheaper option for simple checks\nLLMJudge(\n    rubric='Response contains profanity',\n    model='openai:gpt-4o-mini',\n)\n\n# Premium option for nuanced evaluation\nLLMJudge(\n    rubric='Response demonstrates deep understanding of quantum mechanics',\n    model='anthropic:claude-opus-4-20250514',\n)\n```", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#model-selection", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Model Settings", "anchor": "model-settings", "md_text": "Customize model behavior:\n\n```\nfrom pydantic_ai.settings import ModelSettings\nfrom pydantic_evals.evaluators import LLMJudge\n\nLLMJudge(\n    rubric='...',\n    model_settings=ModelSettings(\n        temperature=0.0,  # Deterministic evaluation\n        max_tokens=100,  # Shorter responses\n    ),\n)\n```", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#model-settings", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Output Modes", "anchor": "output-modes", "md_text": "", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#output-modes", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Assertion Only (Default)", "anchor": "assertion-only-default", "md_text": "Returns pass/fail with reason:\n\n```\nfrom pydantic_evals.evaluators import LLMJudge\n\nLLMJudge(rubric='Response is accurate')\n# Returns: {'LLMJudge_pass': EvaluationReason(value=True, reason='...')}\n```\n\nIn reports:\n\n```\n┃ Assertions ┃\n┃ ✔          ┃\n```", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#assertion-only-default", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Score Only", "anchor": "score-only", "md_text": "Returns a numeric score (0.0 to 1.0):\n\n```\nfrom pydantic_evals.evaluators import LLMJudge\n\nLLMJudge(\n    rubric='Response quality',\n    score={'include_reason': True},\n    assertion=False,\n)\n# Returns: {'LLMJudge_score': EvaluationReason(value=0.85, reason='...')}\n```\n\nIn reports:\n\n```\n┃ Scores             ┃\n┃ LLMJudge_score: 0.85 ┃\n```", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#score-only", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Both Score and Assertion", "anchor": "both-score-and-assertion", "md_text": "```\nfrom pydantic_evals.evaluators import LLMJudge\n\nLLMJudge(\n    rubric='Response quality',\n    score={'include_reason': True},\n    assertion={'include_reason': True},\n)\n# Returns: {\n#     'LLMJudge_score': EvaluationReason(value=0.85, reason='...'),\n#     'LLMJudge_pass': EvaluationReason(value=True, reason='...'),\n# }\n```", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#both-score-and-assertion", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Names", "anchor": "custom-names", "md_text": "```\nfrom pydantic_evals.evaluators import LLMJudge\n\nLLMJudge(\n    rubric='Response is factually accurate',\n    assertion={\n        'evaluation_name': 'accuracy',\n        'include_reason': True,\n    },\n)\n# Returns: {'accuracy': EvaluationReason(value=True, reason='...')}\n```\n\nIn reports:\n\n```\n┃ Assertions ┃\n┃ accuracy: ✔ ┃\n```", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#custom-names", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Practical Examples", "anchor": "practical-examples", "md_text": "", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#practical-examples", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "RAG Evaluation", "anchor": "rag-evaluation", "md_text": "Evaluate whether a RAG system uses provided context:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\n\n@dataclass\nclass RAGInput:\n    question: str\n    context: str\n\n\ndataset = Dataset(\n    cases=[\n        Case(\n            inputs=RAGInput(\n                question='What is the capital of France?',\n                context='France is a country in Europe. Its capital is Paris.',\n            ),\n        ),\n    ],\n    evaluators=[\n        LLMJudge(\n            rubric='Response answers the question using only information from the provided context',\n            include_input=True,\n            assertion={'evaluation_name': 'grounded', 'include_reason': True},\n        ),\n        LLMJudge(\n            rubric='Response cites specific quotes or facts from the context',\n            include_input=True,\n            assertion={'evaluation_name': 'uses_citations', 'include_reason': True},\n        ),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#rag-evaluation", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Recipe Generation with Case-Specific Rubrics", "anchor": "recipe-generation-with-case-specific-rubrics", "md_text": "This example shows how to use both dataset-level and case-specific evaluators:\n\nrecipe\\_evaluation.py\n\n```\nfrom __future__ import annotations\n\nfrom typing import Any\n\nfrom pydantic import BaseModel\n\nfrom pydantic_ai import Agent, format_as_xml\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import IsInstance, LLMJudge\n\n\nclass CustomerOrder(BaseModel):\n    dish_name: str\n    dietary_restriction: str | None = None\n\n\nclass Recipe(BaseModel):\n    ingredients: list[str]\n    steps: list[str]\n\n\nrecipe_agent = Agent(\n    'openai:gpt-5-mini',\n    output_type=Recipe,\n    system_prompt=(\n        'Generate a recipe to cook the dish that meets the dietary restrictions.'\n    ),\n)\n\n\nasync def transform_recipe(customer_order: CustomerOrder) -> Recipe:\n    r = await recipe_agent.run(format_as_xml(customer_order))\n    return r.output\n\n\nrecipe_dataset = Dataset[CustomerOrder, Recipe, Any](\n    cases=[\n        Case(\n            name='vegetarian_recipe',\n            inputs=CustomerOrder(\n                dish_name='Spaghetti Bolognese', dietary_restriction='vegetarian'\n            ),\n            expected_output=None,\n            metadata={'focus': 'vegetarian'},\n            evaluators=(  # (1)!\n                LLMJudge(\n                    rubric='Recipe should not contain meat or animal products',\n                ),\n            ),\n        ),\n        Case(\n            name='gluten_free_recipe',\n            inputs=CustomerOrder(\n                dish_name='Chocolate Cake', dietary_restriction='gluten-free'\n            ),\n            expected_output=None,\n            metadata={'focus': 'gluten-free'},\n            evaluators=(  # (2)!\n                LLMJudge(\n                    rubric='Recipe should not contain gluten or wheat products',\n                ),\n            ),\n        ),\n    ],\n    evaluators=[  # (3)!\n        IsInstance(type_name='Recipe'),\n        LLMJudge(\n            rubric='Recipe should have clear steps and relevant ingredients',\n            include_input=True,\n            model='anthropic:claude-3-7-sonnet-latest',\n        ),\n    ],\n)\n\n\nreport = recipe_dataset.evaluate_sync(transform_recipe)\nprint(report)\n\"\"\"\n     Evaluation Summary: transform_recipe\n┏━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ Case ID            ┃ Assertions ┃ Duration ┃\n┡━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n│ vegetarian_recipe  │ ✔✔✔        │    38.1s │\n├────────────────────┼────────────┼──────────┤\n│ gluten_free_recipe │ ✔✔✔        │    22.4s │\n├────────────────────┼────────────┼──────────┤\n│ Averages           │ 100.0% ✔   │    30.3s │\n└────────────────────┴────────────┴──────────┘\n\"\"\"\n```\n\n1. Case-specific evaluator - only runs for the vegetarian recipe case\n2. Case-specific evaluator - only runs for the gluten-free recipe case\n3. Dataset-level evaluators - run for all cases", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#recipe-generation-with-case-specific-rubrics", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Multi-Aspect Evaluation", "anchor": "multi-aspect-evaluation", "md_text": "Use multiple judges for different quality dimensions:\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[\n        # Accuracy\n        LLMJudge(\n            rubric='Response is factually accurate',\n            include_input=True,\n            assertion={'evaluation_name': 'accurate'},\n        ),\n\n        # Helpfulness\n        LLMJudge(\n            rubric='Response is helpful and actionable',\n            include_input=True,\n            score={'evaluation_name': 'helpfulness'},\n            assertion=False,\n        ),\n\n        # Tone\n        LLMJudge(\n            rubric='Response uses professional, respectful language',\n            assertion={'evaluation_name': 'professional_tone'},\n        ),\n\n        # Safety\n        LLMJudge(\n            rubric='Response contains no harmful, biased, or inappropriate content',\n            assertion={'evaluation_name': 'safe'},\n        ),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#multi-aspect-evaluation", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Comparative Evaluation", "anchor": "comparative-evaluation", "md_text": "Compare output against expected output:\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\ndataset = Dataset(\n    cases=[\n        Case(\n            name='translation',\n            inputs='Hello world',\n            expected_output='Bonjour le monde',\n        ),\n    ],\n    evaluators=[\n        LLMJudge(\n            rubric='Response is semantically equivalent to the expected output',\n            include_input=True,\n            include_expected_output=True,\n            score={'evaluation_name': 'semantic_similarity'},\n            assertion={'evaluation_name': 'correct_meaning'},\n        ),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#comparative-evaluation", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Best Practices", "anchor": "best-practices", "md_text": "", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#best-practices", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "1. Be Specific in Rubrics", "anchor": "1-be-specific-in-rubrics", "md_text": "**Bad:**\n\n```\nfrom pydantic_evals.evaluators import LLMJudge\n\nLLMJudge(rubric='Good answer')\n```\n\n**Better:**\n\n```\nfrom pydantic_evals.evaluators import LLMJudge\n\nLLMJudge(rubric='Response accurately answers the question without hallucinating facts')\n```\n\n**Best:**\n\n```\nfrom pydantic_evals.evaluators import LLMJudge\n\nLLMJudge(\n    rubric='''\n    Response must:\n    1. Directly answer the question asked\n    2. Use only information from the provided context\n    3. Cite specific passages from the context\n    4. Acknowledge if information is insufficient\n    ''',\n    include_input=True,\n)\n```", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#1-be-specific-in-rubrics", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "2. Use Multiple Judges", "anchor": "2-use-multiple-judges", "md_text": "Don't always try to evaluate everything with one rubric:\n\n```\nfrom pydantic_evals.evaluators import LLMJudge\n\n# Instead of this:\nLLMJudge(rubric='Response is good, accurate, helpful, and safe')\n\n# Do this:\nevaluators = [\n    LLMJudge(rubric='Response is factually accurate'),\n    LLMJudge(rubric='Response is helpful and actionable'),\n    LLMJudge(rubric='Response is safe and appropriate'),\n]\n```", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#2-use-multiple-judges", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "3. Combine with Deterministic Checks", "anchor": "3-combine-with-deterministic-checks", "md_text": "Don't use LLM evaluation for checks that can be done deterministically:\n\n```\nfrom pydantic_evals.evaluators import Contains, IsInstance, LLMJudge\n\nevaluators = [\n    IsInstance(type_name='str'),\n    Contains(value='required_section'),\n    LLMJudge(rubric='Response quality is high'),\n]\n```", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#3-combine-with-deterministic-checks", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "4. Use Temperature 0 for Consistency", "anchor": "4-use-temperature-0-for-consistency", "md_text": "```\nfrom pydantic_ai.settings import ModelSettings\nfrom pydantic_evals.evaluators import LLMJudge\n\nLLMJudge(\n    rubric='...',\n    model_settings=ModelSettings(temperature=0.0),\n)\n```", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#4-use-temperature-0-for-consistency", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Limitations", "anchor": "limitations", "md_text": "", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#limitations", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Non-Determinism", "anchor": "non-determinism", "md_text": "LLM judges are not deterministic. The same output may receive different scores across runs.\n\n**Mitigation:**\n\n* Use `temperature=0.0` for more consistency\n* Run multiple evaluations and average\n* Use retry strategies for flaky evaluations", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#non-determinism", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Cost", "anchor": "cost", "md_text": "LLM judges make API calls, which cost money and time.\n\n**Mitigation:**\n\n* Use cheaper models for simple checks (`gpt-4o-mini`)\n* Run deterministic checks first to fail fast\n* Cache results when possible\n* Limit evaluation to changed cases", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#cost", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Model Biases", "anchor": "model-biases", "md_text": "LLM judges inherit biases from their training data.\n\n**Mitigation:**\n\n* Use multiple judge models and compare\n* Review evaluation reasons, not just scores\n* Validate judges against human-labeled test sets\n* Be aware of known biases (length bias, style preferences)", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#model-biases", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Context Limits", "anchor": "context-limits", "md_text": "Judges have token limits for inputs.\n\n**Mitigation:**\n\n* Truncate long inputs/outputs intelligently\n* Use focused rubrics that don't require full context\n* Consider chunked evaluation for very long content", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#context-limits", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Debugging LLM Judges", "anchor": "debugging-llm-judges", "md_text": "", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#debugging-llm-judges", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "View Reasons", "anchor": "view-reasons", "md_text": "```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\n\ndef my_task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[LLMJudge(rubric='Response is clear')],\n)\nreport = dataset.evaluate_sync(my_task)\nreport.print(include_reasons=True)\n\"\"\"\n     Evaluation Summary: my_task\n┏━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ Case ID  ┃ Assertions  ┃ Duration ┃\n┡━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━┩\n│ Case 1   │ LLMJudge: ✔ │     10ms │\n│          │   Reason: - │          │\n│          │             │          │\n│          │             │          │\n├──────────┼─────────────┼──────────┤\n│ Averages │ 100.0% ✔    │     10ms │\n└──────────┴─────────────┴──────────┘\n\"\"\"\n```\n\nOutput:\n\n```\n┃ Assertions              ┃\n┃ accuracy: ✔            ┃\n┃   Reason: The response │\n┃   correctly states...  │\n```", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#view-reasons", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Access Programmatically", "anchor": "access-programmatically", "md_text": "```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\n\ndef my_task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[LLMJudge(rubric='Response is clear')],\n)\nreport = dataset.evaluate_sync(my_task)\nfor case in report.cases:\n    for name, result in case.assertions.items():\n        print(f'{name}: {result.value}')\n        #> LLMJudge: True\n        if result.reason:\n            print(f'  Reason: {result.reason}')\n            #>   Reason: -\n```", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#access-programmatically", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Compare Judges", "anchor": "compare-judges", "md_text": "Test the same cases with different judge models:\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\n\ndef my_task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\njudges = [\n    LLMJudge(rubric='Response is clear', model='openai:gpt-4o'),\n    LLMJudge(rubric='Response is clear', model='anthropic:claude-3-7-sonnet-latest'),\n    LLMJudge(rubric='Response is clear', model='openai:gpt-4o-mini'),\n]\n\nfor judge in judges:\n    dataset = Dataset(cases=[Case(inputs='test')], evaluators=[judge])\n    report = dataset.evaluate_sync(my_task)\n    # Compare results\n```", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#compare-judges", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Advanced: Custom Judge Models", "anchor": "advanced-custom-judge-models", "md_text": "Set a default judge model for all `LLMJudge` evaluators:\n\n```\nfrom pydantic_evals.evaluators import LLMJudge\nfrom pydantic_evals.evaluators.llm_as_a_judge import set_default_judge_model\n\n# Set default to Claude\nset_default_judge_model('anthropic:claude-3-7-sonnet-latest')\n\n# Now all LLMJudge instances use Claude by default\nLLMJudge(rubric='...')  # Uses Claude\n```", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#advanced-custom-judge-models", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "md_text": "* **[Custom Evaluators](../custom/index.html)** - Write custom evaluation logic\n* **[Built-in Evaluators](../built-in/index.html)** - Complete evaluator reference", "url": "https://ai.pydantic.dev/evaluators/llm-judge/index.html#next-steps", "page": "evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "md_text": "While executing evaluation tasks, you can record:\n\n* **Metrics** - Numeric values (int/float) for quantitative measurements\n* **Attributes** - Any data for qualitative information\n\nThese appear in evaluation reports and can be used by evaluators for assessment.", "url": "https://ai.pydantic.dev/how-to/metrics-attributes/index.html#overview", "page": "how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Recording Metrics", "anchor": "recording-metrics", "md_text": "Use [`increment_eval_metric`](../../pydantic_evals/dataset/index.html#pydantic_evals.dataset.increment_eval_metric) to track numeric values:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.dataset import increment_eval_metric\n\n\n@dataclass\nclass APIResult:\n    output: str\n    usage: 'Usage'\n\n\n@dataclass\nclass Usage:\n    total_tokens: int\n\n\ndef call_api(inputs: str) -> APIResult:\n    return APIResult(output=f'Result: {inputs}', usage=Usage(total_tokens=100))\n\n\ndef my_task(inputs: str) -> str:\n    # Track API calls\n    increment_eval_metric('api_calls', 1)\n\n    result = call_api(inputs)\n\n    # Track tokens used\n    increment_eval_metric('tokens_used', result.usage.total_tokens)\n\n    return result.output\n```", "url": "https://ai.pydantic.dev/how-to/metrics-attributes/index.html#recording-metrics", "page": "how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Recording Attributes", "anchor": "recording-attributes", "md_text": "Use [`set_eval_attribute`](../../pydantic_evals/dataset/index.html#pydantic_evals.dataset.set_eval_attribute) to store any data:\n\n```\nfrom pydantic_evals import set_eval_attribute\n\n\ndef process(inputs: str) -> str:\n    return f'Processed: {inputs}'\n\n\ndef my_task(inputs: str) -> str:\n    # Record which model was used\n    set_eval_attribute('model', 'gpt-4o')\n\n    # Record feature flags\n    set_eval_attribute('used_cache', True)\n    set_eval_attribute('retry_count', 2)\n\n    # Record structured data\n    set_eval_attribute('config', {\n        'temperature': 0.7,\n        'max_tokens': 100,\n    })\n\n    return process(inputs)\n```", "url": "https://ai.pydantic.dev/how-to/metrics-attributes/index.html#recording-attributes", "page": "how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Accessing in Evaluators", "anchor": "accessing-in-evaluators", "md_text": "Metrics and attributes are available in the [`EvaluatorContext`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorContext):\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass EfficiencyChecker(Evaluator):\n    max_api_calls: int = 5\n\n    def evaluate(self, ctx: EvaluatorContext) -> dict[str, bool]:\n        # Access metrics\n        api_calls = ctx.metrics.get('api_calls', 0)\n        tokens_used = ctx.metrics.get('tokens_used', 0)\n\n        # Access attributes\n        used_cache = ctx.attributes.get('used_cache', False)\n\n        return {\n            'efficient_api_usage': api_calls <= self.max_api_calls,\n            'used_caching': used_cache,\n            'token_efficient': tokens_used < 1000,\n        }\n```", "url": "https://ai.pydantic.dev/how-to/metrics-attributes/index.html#accessing-in-evaluators", "page": "how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Viewing in Reports", "anchor": "viewing-in-reports", "md_text": "Metrics and attributes appear in report data:\n\n```\nfrom pydantic_evals import Case, Dataset\n\n\ndef task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test')], evaluators=[])\nreport = dataset.evaluate_sync(task)\n\nfor case in report.cases:\n    print(f'{case.name}:')\n    #> Case 1:\n    print(f'  Metrics: {case.metrics}')\n    #>   Metrics: {}\n    print(f'  Attributes: {case.attributes}')\n    #>   Attributes: {}\n```\n\nYou can also display them in printed reports:\n\n```\nfrom pydantic_evals import Case, Dataset\n\n\ndef task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test')], evaluators=[])\nreport = dataset.evaluate_sync(task)\n\n# Metrics and attributes are available but not shown by default\n# Access them programmatically or via Logfire\n\nfor case in report.cases:\n    print(f'\\nCase: {case.name}')\n    \"\"\"\n    Case: Case 1\n    \"\"\"\n    print(f'Metrics: {case.metrics}')\n    #> Metrics: {}\n    print(f'Attributes: {case.attributes}')\n    #> Attributes: {}\n```", "url": "https://ai.pydantic.dev/how-to/metrics-attributes/index.html#viewing-in-reports", "page": "how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Automatic Metrics", "anchor": "automatic-metrics", "md_text": "When using Pydantic AI and Logfire, some metrics are automatically tracked:\n\n```\nimport logfire\n\nfrom pydantic_ai import Agent\n\nlogfire.configure(send_to_logfire='if-token-present')\n\nagent = Agent('openai:gpt-4o')\n\n\nasync def ai_task(inputs: str) -> str:\n    result = await agent.run(inputs)\n    return result.output\n\n\n# Automatically tracked metrics:\n# - requests: Number of LLM calls\n# - input_tokens: Total input tokens\n# - output_tokens: Total output tokens\n# - prompt_tokens: Prompt tokens (if available)\n# - completion_tokens: Completion tokens (if available)\n# - cost: Estimated cost (if using genai-prices)\n```\n\nAccess these in evaluators:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass CostChecker(Evaluator):\n    max_cost: float = 0.01  # $0.01\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        cost = ctx.metrics.get('cost', 0.0)\n        return cost <= self.max_cost\n```", "url": "https://ai.pydantic.dev/how-to/metrics-attributes/index.html#automatic-metrics", "page": "how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Practical Examples", "anchor": "practical-examples", "md_text": "", "url": "https://ai.pydantic.dev/how-to/metrics-attributes/index.html#practical-examples", "page": "how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "API Usage Tracking", "anchor": "api-usage-tracking", "md_text": "```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals import increment_eval_metric, set_eval_attribute\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\ndef check_cache(inputs: str) -> str | None:\n    return None  # No cache hit for demo\n\n\n@dataclass\nclass APIResult:\n    text: str\n    usage: 'Usage'\n\n\n@dataclass\nclass Usage:\n    total_tokens: int\n\n\nasync def call_api(inputs: str) -> APIResult:\n    return APIResult(text=f'Result: {inputs}', usage=Usage(total_tokens=100))\n\n\ndef save_to_cache(inputs: str, result: str) -> None:\n    pass  # Save to cache\n\n\nasync def smart_task(inputs: str) -> str:\n    # Try cache first\n    if cached := check_cache(inputs):\n        set_eval_attribute('cache_hit', True)\n        return cached\n\n    set_eval_attribute('cache_hit', False)\n\n    # Call API\n    increment_eval_metric('api_calls', 1)\n    result = await call_api(inputs)\n\n    increment_eval_metric('tokens', result.usage.total_tokens)\n\n    # Cache result\n    save_to_cache(inputs, result.text)\n\n    return result.text\n\n\n# Evaluate efficiency\n@dataclass\nclass EfficiencyEvaluator(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> dict[str, bool | float]:\n        api_calls = ctx.metrics.get('api_calls', 0)\n        cache_hit = ctx.attributes.get('cache_hit', False)\n\n        return {\n            'used_cache': cache_hit,\n            'made_api_call': api_calls > 0,\n            'efficiency_score': 1.0 if cache_hit else 0.5,\n        }\n```", "url": "https://ai.pydantic.dev/how-to/metrics-attributes/index.html#api-usage-tracking", "page": "how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Tool Usage Tracking", "anchor": "tool-usage-tracking", "md_text": "```\nfrom dataclasses import dataclass\n\nfrom pydantic_ai import Agent, RunContext\nfrom pydantic_evals import increment_eval_metric, set_eval_attribute\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\nagent = Agent('openai:gpt-4o')\n\n\ndef search(query: str) -> str:\n    return f'Search results for: {query}'\n\n\ndef call(endpoint: str) -> str:\n    return f'API response from: {endpoint}'\n\n\n@agent.tool\ndef search_database(ctx: RunContext, query: str) -> str:\n    increment_eval_metric('db_searches', 1)\n    set_eval_attribute('last_query', query)\n    return search(query)\n\n\n@agent.tool\ndef call_api(ctx: RunContext, endpoint: str) -> str:\n    increment_eval_metric('api_calls', 1)\n    set_eval_attribute('last_endpoint', endpoint)\n    return call(endpoint)\n\n\n# Evaluate tool usage\n@dataclass\nclass ToolUsageEvaluator(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> dict[str, bool | int]:\n        db_searches = ctx.metrics.get('db_searches', 0)\n        api_calls = ctx.metrics.get('api_calls', 0)\n\n        return {\n            'used_database': db_searches > 0,\n            'used_api': api_calls > 0,\n            'tool_call_count': db_searches + api_calls,\n            'reasonable_tool_usage': (db_searches + api_calls) <= 5,\n        }\n```", "url": "https://ai.pydantic.dev/how-to/metrics-attributes/index.html#tool-usage-tracking", "page": "how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Performance Tracking", "anchor": "performance-tracking", "md_text": "```\nimport time\nfrom dataclasses import dataclass\n\nfrom pydantic_evals import increment_eval_metric, set_eval_attribute\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\nasync def retrieve_context(inputs: str) -> list[str]:\n    return ['context1', 'context2']\n\n\nasync def generate_response(context: list[str], inputs: str) -> str:\n    return f'Generated response for {inputs}'\n\n\nasync def monitored_task(inputs: str) -> str:\n    # Track sub-operation timing\n    t0 = time.perf_counter()\n    context = await retrieve_context(inputs)\n    retrieve_time = time.perf_counter() - t0\n\n    increment_eval_metric('retrieve_time', retrieve_time)\n\n    t0 = time.perf_counter()\n    result = await generate_response(context, inputs)\n    generate_time = time.perf_counter() - t0\n\n    increment_eval_metric('generate_time', generate_time)\n\n    # Record which operations were needed\n    set_eval_attribute('needed_retrieval', len(context) > 0)\n    set_eval_attribute('context_chunks', len(context))\n\n    return result\n\n\n# Evaluate performance\n@dataclass\nclass PerformanceEvaluator(Evaluator):\n    max_retrieve_time: float = 0.5\n    max_generate_time: float = 2.0\n\n    def evaluate(self, ctx: EvaluatorContext) -> dict[str, bool]:\n        retrieve_time = ctx.metrics.get('retrieve_time', 0.0)\n        generate_time = ctx.metrics.get('generate_time', 0.0)\n\n        return {\n            'fast_retrieval': retrieve_time <= self.max_retrieve_time,\n            'fast_generation': generate_time <= self.max_generate_time,\n        }\n```", "url": "https://ai.pydantic.dev/how-to/metrics-attributes/index.html#performance-tracking", "page": "how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Quality Tracking", "anchor": "quality-tracking", "md_text": "```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals import set_eval_attribute\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\nasync def llm_call(inputs: str) -> dict:\n    return {'text': f'Response: {inputs}', 'confidence': 0.85, 'sources': ['doc1', 'doc2']}\n\n\nasync def quality_task(inputs: str) -> str:\n    result = await llm_call(inputs)\n\n    # Extract quality indicators\n    confidence = result.get('confidence', 0.0)\n    sources_used = result.get('sources', [])\n\n    set_eval_attribute('confidence', confidence)\n    set_eval_attribute('source_count', len(sources_used))\n    set_eval_attribute('sources', sources_used)\n\n    return result['text']\n\n\n# Evaluate based on quality signals\n@dataclass\nclass QualityEvaluator(Evaluator):\n    min_confidence: float = 0.7\n\n    def evaluate(self, ctx: EvaluatorContext) -> dict[str, bool | float]:\n        confidence = ctx.attributes.get('confidence', 0.0)\n        source_count = ctx.attributes.get('source_count', 0)\n\n        return {\n            'high_confidence': confidence >= self.min_confidence,\n            'used_sources': source_count > 0,\n            'quality_score': confidence * (1.0 + 0.1 * source_count),\n        }\n```", "url": "https://ai.pydantic.dev/how-to/metrics-attributes/index.html#quality-tracking", "page": "how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Metrics vs Attributes vs Metadata", "anchor": "metrics-vs-attributes-vs-metadata", "md_text": "Understanding the differences:\n\n| Feature | Metrics | Attributes | Metadata |\n| --- | --- | --- | --- |\n| **Set in** | Task execution | Task execution | Case definition |\n| **Type** | int, float | Any | Any |\n| **Purpose** | Quantitative | Qualitative | Test data |\n| **Used for** | Aggregation | Context | Input to task |\n| **Available to** | Evaluators | Evaluators | Task & Evaluators |\n\n```\nfrom pydantic_evals import Case, increment_eval_metric, set_eval_attribute\n\n# Metadata: Defined in case (before execution)\nCase(\n    inputs='question',\n    metadata={'difficulty': 'hard', 'category': 'math'},\n)\n\n\n# Metrics & Attributes: Recorded during execution\ndef task(inputs):\n    # These are recorded during execution\n    increment_eval_metric('tokens', 100)\n    set_eval_attribute('model', 'gpt-4o')\n    return f'Result: {inputs}'\n```", "url": "https://ai.pydantic.dev/how-to/metrics-attributes/index.html#metrics-vs-attributes-vs-metadata", "page": "how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Troubleshooting", "anchor": "troubleshooting", "md_text": "", "url": "https://ai.pydantic.dev/how-to/metrics-attributes/index.html#troubleshooting", "page": "how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "\"Metrics/attributes not appearing\"", "anchor": "metricsattributes-not-appearing", "md_text": "Ensure you're calling the functions inside the task:\n\n```\nfrom pydantic_evals import increment_eval_metric\n\n\ndef process(inputs: str) -> str:\n    return f'Processed: {inputs}'\n\n\n# Bad: Called outside task\nincrement_eval_metric('count', 1)\n\n\ndef bad_task(inputs):\n    return process(inputs)\n\n\n# Good: Called inside task\ndef good_task(inputs):\n    increment_eval_metric('count', 1)\n    return process(inputs)\n```", "url": "https://ai.pydantic.dev/how-to/metrics-attributes/index.html#metricsattributes-not-appearing", "page": "how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "\"Metrics not incrementing\"", "anchor": "metrics-not-incrementing", "md_text": "Check you're using `increment_eval_metric`, not `set_eval_attribute`:\n\n```\nfrom pydantic_evals import increment_eval_metric, set_eval_attribute\n\n# Bad: This will overwrite, not increment\nset_eval_attribute('count', 1)\nset_eval_attribute('count', 1)  # Still 1\n\n# Good: This increments\nincrement_eval_metric('count', 1)\nincrement_eval_metric('count', 1)  # Now 2\n```", "url": "https://ai.pydantic.dev/how-to/metrics-attributes/index.html#metrics-not-incrementing", "page": "how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "\"Too much data in attributes\"", "anchor": "too-much-data-in-attributes", "md_text": "Store summaries, not raw data:\n\n```\nfrom pydantic_evals import set_eval_attribute\n\ngiant_response_object = {'key' + str(i): 'value' * 100 for i in range(1000)}\n\n# Bad: Huge object\nset_eval_attribute('full_response', giant_response_object)\n\n# Good: Summary\nset_eval_attribute('response_size_kb', len(str(giant_response_object)) / 1024)\nset_eval_attribute('response_keys', list(giant_response_object.keys())[:10])  # First 10 keys\n```", "url": "https://ai.pydantic.dev/how-to/metrics-attributes/index.html#too-much-data-in-attributes", "page": "how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "md_text": "* **[Custom Evaluators](../../evaluators/custom/index.html)** - Use metrics/attributes in evaluators\n* **[Logfire Integration](../logfire-integration/index.html)** - View metrics in Logfire\n* **[Concurrency & Performance](../concurrency/index.html)** - Optimize evaluation performance", "url": "https://ai.pydantic.dev/how-to/metrics-attributes/index.html#next-steps", "page": "how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Creating Datasets", "anchor": "creating-datasets", "md_text": "", "url": "https://ai.pydantic.dev/how-to/dataset-management/index.html#creating-datasets", "page": "how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "From Code", "anchor": "from-code", "md_text": "Define datasets directly in Python:\n\n```\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import EqualsExpected, IsInstance\n\ndataset = Dataset[str, str, Any](\n    name='my_eval_suite',\n    cases=[\n        Case(\n            name='test_1',\n            inputs='input 1',\n            expected_output='output 1',\n        ),\n        Case(\n            name='test_2',\n            inputs='input 2',\n            expected_output='output 2',\n        ),\n    ],\n    evaluators=[\n        IsInstance(type_name='str'),\n        EqualsExpected(),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/how-to/dataset-management/index.html#from-code", "page": "how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Adding Cases Dynamically", "anchor": "adding-cases-dynamically", "md_text": "```\nfrom typing import Any\n\nfrom pydantic_evals import Dataset\nfrom pydantic_evals.evaluators import IsInstance\n\ndataset = Dataset[str, str, Any](cases=[], evaluators=[])\n\n# Add cases one at a time\ndataset.add_case(\n    name='dynamic_case',\n    inputs='test input',\n    expected_output='test output',\n)\n\n# Add evaluators\ndataset.add_evaluator(IsInstance(type_name='str'))\n```", "url": "https://ai.pydantic.dev/how-to/dataset-management/index.html#adding-cases-dynamically", "page": "how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Saving Datasets", "anchor": "saving-datasets", "md_text": "For complete details on serialization formats, JSON schema generation, and custom evaluators, see [Dataset Serialization](../dataset-serialization/index.html).", "url": "https://ai.pydantic.dev/how-to/dataset-management/index.html#saving-datasets", "page": "how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Save to YAML", "anchor": "save-to-yaml", "md_text": "```\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\n\ndataset = Dataset[str, str, Any](cases=[Case(name='test', inputs='example')])\ndataset.to_file('my_dataset.yaml')\n\n# Also saves schema file: my_dataset_schema.json\n```\n\nOutput (`my_dataset.yaml`):\n\n```\n# yaml-language-server: $schema=my_dataset_schema.json\nname: my_eval_suite\ncases:\n- name: test_1\n  inputs: input 1\n  expected_output: output 1\n  evaluators:\n  - EqualsExpected\n- name: test_2\n  inputs: input 2\n  expected_output: output 2\n  evaluators:\n  - EqualsExpected\nevaluators:\n- IsInstance: str\n```", "url": "https://ai.pydantic.dev/how-to/dataset-management/index.html#save-to-yaml", "page": "how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Save to JSON", "anchor": "save-to-json", "md_text": "```\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\n\ndataset = Dataset[str, str, Any](cases=[Case(name='test', inputs='example')])\ndataset.to_file('my_dataset.json')\n\n# Also saves schema file: my_dataset_schema.json\n```", "url": "https://ai.pydantic.dev/how-to/dataset-management/index.html#save-to-json", "page": "how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Schema Path", "anchor": "custom-schema-path", "md_text": "```\nfrom pathlib import Path\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\n\ndataset = Dataset[str, str, Any](cases=[Case(name='test', inputs='example')])\n\n# Custom schema location\nPath('data').mkdir(exist_ok=True)\nPath('data/schemas').mkdir(parents=True, exist_ok=True)\ndataset.to_file(\n    'data/my_dataset.yaml',\n    schema_path='schemas/my_schema.json',\n)\n\n# No schema file\ndataset.to_file('my_dataset.yaml', schema_path=None)\n```", "url": "https://ai.pydantic.dev/how-to/dataset-management/index.html#custom-schema-path", "page": "how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Loading Datasets", "anchor": "loading-datasets", "md_text": "", "url": "https://ai.pydantic.dev/how-to/dataset-management/index.html#loading-datasets", "page": "how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "From YAML/JSON", "anchor": "from-yamljson", "md_text": "```\nfrom typing import Any\n\nfrom pydantic_evals import Dataset\n\n# Infers format from extension\ndataset = Dataset[str, str, Any].from_file('my_dataset.yaml')\ndataset = Dataset[str, str, Any].from_file('my_dataset.json')\n\n# Explicit format for non-standard extensions\ndataset = Dataset[str, str, Any].from_file('data.txt', fmt='yaml')\n```", "url": "https://ai.pydantic.dev/how-to/dataset-management/index.html#from-yamljson", "page": "how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "From String", "anchor": "from-string", "md_text": "```\nfrom typing import Any\n\nfrom pydantic_evals import Dataset\n\nyaml_content = \"\"\"\ncases:\n- name: test\n  inputs: hello\n  expected_output: HELLO\nevaluators:\n- EqualsExpected\n\"\"\"\n\ndataset = Dataset[str, str, Any].from_text(yaml_content, fmt='yaml')\n```", "url": "https://ai.pydantic.dev/how-to/dataset-management/index.html#from-string", "page": "how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "From Dict", "anchor": "from-dict", "md_text": "```\nfrom typing import Any\n\nfrom pydantic_evals import Dataset\n\ndata = {\n    'cases': [\n        {\n            'name': 'test',\n            'inputs': 'hello',\n            'expected_output': 'HELLO',\n        },\n    ],\n    'evaluators': [{'EqualsExpected': {}}],\n}\n\ndataset = Dataset[str, str, Any].from_dict(data)\n```", "url": "https://ai.pydantic.dev/how-to/dataset-management/index.html#from-dict", "page": "how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "With Custom Evaluators", "anchor": "with-custom-evaluators", "md_text": "When loading datasets that use custom evaluators, you must pass them to `from_file()`:\n\n```\nfrom dataclasses import dataclass\nfrom typing import Any\n\nfrom pydantic_evals import Dataset\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass MyCustomEvaluator(Evaluator):\n    threshold: float = 0.5\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return True\n\n\n# Load with custom evaluator registry\ndataset = Dataset[str, str, Any].from_file(\n    'my_dataset.yaml',\n    custom_evaluator_types=[MyCustomEvaluator],\n)\n```\n\nFor complete details on serialization with custom evaluators, see [Dataset Serialization](../dataset-serialization/index.html).", "url": "https://ai.pydantic.dev/how-to/dataset-management/index.html#with-custom-evaluators", "page": "how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Generating Datasets", "anchor": "generating-datasets", "md_text": "Pydantic Evals allows you to generate test datasets using LLMs with [`generate_dataset`](../../pydantic_evals/generation/index.html#pydantic_evals.generation.generate_dataset).\n\nDatasets can be generated in either JSON or YAML format, in both cases a JSON schema file is generated alongside the dataset and referenced in the dataset, so you should get type checking and auto-completion in your editor.\n\ngenerate\\_dataset\\_example.py\n\n```\nfrom __future__ import annotations\n\nfrom pathlib import Path\n\nfrom pydantic import BaseModel, Field\n\nfrom pydantic_evals import Dataset\nfrom pydantic_evals.generation import generate_dataset\n\n\nclass QuestionInputs(BaseModel, use_attribute_docstrings=True):  # (1)!\n    \"\"\"Model for question inputs.\"\"\"\n\n    question: str\n    \"\"\"A question to answer\"\"\"\n    context: str | None = None\n    \"\"\"Optional context for the question\"\"\"\n\n\nclass AnswerOutput(BaseModel, use_attribute_docstrings=True):  # (2)!\n    \"\"\"Model for expected answer outputs.\"\"\"\n\n    answer: str\n    \"\"\"The answer to the question\"\"\"\n    confidence: float = Field(ge=0, le=1)\n    \"\"\"Confidence level (0-1)\"\"\"\n\n\nclass MetadataType(BaseModel, use_attribute_docstrings=True):  # (3)!\n    \"\"\"Metadata model for test cases.\"\"\"\n\n    difficulty: str\n    \"\"\"Difficulty level (easy, medium, hard)\"\"\"\n    category: str\n    \"\"\"Question category\"\"\"\n\n\nasync def main():\n    dataset = await generate_dataset(  # (4)!\n        dataset_type=Dataset[QuestionInputs, AnswerOutput, MetadataType],\n        n_examples=2,\n        extra_instructions=\"\"\"\n        Generate question-answer pairs about world capitals and landmarks.\n        Make sure to include both easy and challenging questions.\n        \"\"\",\n    )\n    output_file = Path('questions_cases.yaml')\n    dataset.to_file(output_file)  # (5)!\n    print(output_file.read_text())\n    \"\"\"\n    # yaml-language-server: $schema=questions_cases_schema.json\n    name: null\n    cases:\n    - name: Easy Capital Question\n      inputs:\n        question: What is the capital of France?\n        context: null\n      metadata:\n        difficulty: easy\n        category: Geography\n      expected_output:\n        answer: Paris\n        confidence: 0.95\n      evaluators:\n      - EqualsExpected\n    - name: Challenging Landmark Question\n      inputs:\n        question: Which world-famous landmark is located on the banks of the Seine River?\n        context: null\n      metadata:\n        difficulty: hard\n        category: Landmarks\n      expected_output:\n        answer: Eiffel Tower\n        confidence: 0.9\n      evaluators:\n      - EqualsExpected\n    evaluators: []\n    \"\"\"\n```\n\n1. Define the schema for the inputs to the task.\n2. Define the schema for the expected outputs of the task.\n3. Define the schema for the metadata of the test cases.\n4. Call [`generate_dataset`](../../pydantic_evals/generation/index.html#pydantic_evals.generation.generate_dataset) to create a [`Dataset`](../../pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset) with 2 cases confirming to the schema.\n5. Save the dataset to a YAML file, this will also write `questions_cases_schema.json` with the schema JSON schema for `questions_cases.yaml` to make editing easier. The magic `yaml-language-server` comment is supported by at least vscode, jetbrains/pycharm (more details [here](https://github.com/redhat-developer/yaml-language-server#using-inlined-schema)).\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main(answer))` to run `main`)*\n\nYou can also write datasets as JSON files:\n\ngenerate\\_dataset\\_example\\_json.py\n\n```\nfrom pathlib import Path\n\nfrom pydantic_evals import Dataset\nfrom pydantic_evals.generation import generate_dataset\n\nfrom generate_dataset_example import AnswerOutput, MetadataType, QuestionInputs\n\n\nasync def main():\n    dataset = await generate_dataset(  # (1)!\n        dataset_type=Dataset[QuestionInputs, AnswerOutput, MetadataType],\n        n_examples=2,\n        extra_instructions=\"\"\"\n        Generate question-answer pairs about world capitals and landmarks.\n        Make sure to include both easy and challenging questions.\n        \"\"\",\n    )\n    output_file = Path('questions_cases.json')\n    dataset.to_file(output_file)  # (2)!\n    print(output_file.read_text())\n    \"\"\"\n    {\n      \"$schema\": \"questions_cases_schema.json\",\n      \"name\": null,\n      \"cases\": [\n        {\n          \"name\": \"Easy Capital Question\",\n          \"inputs\": {\n            \"question\": \"What is the capital of France?\",\n            \"context\": null\n          },\n          \"metadata\": {\n            \"difficulty\": \"easy\",\n            \"category\": \"Geography\"\n          },\n          \"expected_output\": {\n            \"answer\": \"Paris\",\n            \"confidence\": 0.95\n          },\n          \"evaluators\": [\n            \"EqualsExpected\"\n          ]\n        },\n        {\n          \"name\": \"Challenging Landmark Question\",\n          \"inputs\": {\n            \"question\": \"Which world-famous landmark is located on the banks of the Seine River?\",\n            \"context\": null\n          },\n          \"metadata\": {\n            \"difficulty\": \"hard\",\n            \"category\": \"Landmarks\"\n          },\n          \"expected_output\": {\n            \"answer\": \"Eiffel Tower\",\n            \"confidence\": 0.9\n          },\n          \"evaluators\": [\n            \"EqualsExpected\"\n          ]\n        }\n      ],\n      \"evaluators\": []\n    }\n    \"\"\"\n```", "url": "https://ai.pydantic.dev/how-to/dataset-management/index.html#generating-datasets", "page": "how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Generating Datasets", "anchor": "generating-datasets", "md_text": "1. Generate the [`Dataset`](../../pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset) exactly as above.\n2. Save the dataset to a JSON file, this will also write `questions_cases_schema.json` with th JSON schema for `questions_cases.json`. This time the `$schema` key is included in the JSON file to define the schema for IDEs to use while you edit the file, there's no formal spec for this, but it works in vscode and pycharm and is discussed at length in [json-schema-org/json-schema-spec#828](https://github.com/json-schema-org/json-schema-spec/issues/828).\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `asyncio.run(main(answer))` to run `main`)*", "url": "https://ai.pydantic.dev/how-to/dataset-management/index.html#generating-datasets", "page": "how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Type-Safe Datasets", "anchor": "type-safe-datasets", "md_text": "Use generic type parameters for type safety:\n\n```\nfrom typing_extensions import TypedDict\n\nfrom pydantic_evals import Case, Dataset\n\n\nclass MyInput(TypedDict):\n    query: str\n    max_results: int\n\n\nclass MyOutput(TypedDict):\n    results: list[str]\n\n\nclass MyMetadata(TypedDict):\n    category: str\n\n\n# Type-safe dataset\ndataset: Dataset[MyInput, MyOutput, MyMetadata] = Dataset(\n    cases=[\n        Case(\n            name='test',\n            inputs={'query': 'test', 'max_results': 10},\n            expected_output={'results': ['a', 'b']},\n            metadata={'category': 'search'},\n        ),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/how-to/dataset-management/index.html#type-safe-datasets", "page": "how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Schema Generation", "anchor": "schema-generation", "md_text": "Generate JSON Schema for IDE support:\n\n```\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\n\ndataset = Dataset[str, str, Any](cases=[Case(name='test', inputs='example')])\n\n# Save with schema\ndataset.to_file('my_dataset.yaml')  # Creates my_dataset_schema.json\n\n# Schema enables:\n# - Autocomplete in VS Code/PyCharm\n# - Validation while editing\n# - Inline documentation\n```\n\nManual schema generation:\n\n```\nimport json\nfrom dataclasses import dataclass\nfrom typing import Any\n\nfrom pydantic_evals import Dataset\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass MyCustomEvaluator(Evaluator):\n    threshold: float = 0.5\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return True\n\n\nschema = Dataset[str, str, Any].model_json_schema_with_evaluators(\n    custom_evaluator_types=[MyCustomEvaluator],\n)\nprint(json.dumps(schema, indent=2)[:66] + '...')\n\"\"\"\n{\n  \"$defs\": {\n    \"Case\": {\n      \"additionalProperties\": false,\n...\n\"\"\"\n```", "url": "https://ai.pydantic.dev/how-to/dataset-management/index.html#schema-generation", "page": "how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Best Practices", "anchor": "best-practices", "md_text": "", "url": "https://ai.pydantic.dev/how-to/dataset-management/index.html#best-practices", "page": "how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "1. Use Clear Names", "anchor": "1-use-clear-names", "md_text": "```\nfrom pydantic_evals import Case\n\n# Good\nCase(name='uppercase_basic_ascii', inputs='hello')\nCase(name='uppercase_unicode_emoji', inputs='hello 😀')\nCase(name='uppercase_empty_string', inputs='')\n\n# Bad\nCase(name='test1', inputs='hello')\nCase(name='test2', inputs='world')\nCase(name='test3', inputs='foo')\n```", "url": "https://ai.pydantic.dev/how-to/dataset-management/index.html#1-use-clear-names", "page": "how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "2. Organize by Difficulty", "anchor": "2-organize-by-difficulty", "md_text": "```\nfrom pydantic_evals import Case, Dataset\n\ndataset = Dataset(\n    cases=[\n        Case(name='easy_1', inputs='test', metadata={'difficulty': 'easy'}),\n        Case(name='easy_2', inputs='test2', metadata={'difficulty': 'easy'}),\n        Case(name='medium_1', inputs='test3', metadata={'difficulty': 'medium'}),\n        Case(name='hard_1', inputs='test4', metadata={'difficulty': 'hard'}),\n    ],\n)\n```", "url": "https://ai.pydantic.dev/how-to/dataset-management/index.html#2-organize-by-difficulty", "page": "how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "3. Start Small, Grow Gradually", "anchor": "3-start-small-grow-gradually", "md_text": "```\nfrom pydantic_evals import Case, Dataset\n\n# Start with representative cases\ndataset = Dataset(\n    cases=[\n        Case(name='happy_path', inputs='test'),\n        Case(name='edge_case', inputs=''),\n        Case(name='error_case', inputs='invalid'),\n    ],\n)\n\n# Add more as you find issues\ndataset.add_case(name='newly_discovered_edge_case', inputs='edge')\n```", "url": "https://ai.pydantic.dev/how-to/dataset-management/index.html#3-start-small-grow-gradually", "page": "how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "4. Use Case-specific Evaluators Where Appropriate", "anchor": "4-use-case-specific-evaluators-where-appropriate", "md_text": "Case-specific evaluators let different cases have different evaluation criteria, which is essential for comprehensive \"test coverage\". Rather than trying to write one-size-fits-all evaluators, you can specify exactly what \"good\" looks like for each scenario. This is particularly powerful with [`LLMJudge`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.LLMJudge) evaluators where you can describe nuanced requirements per case, making it easy to build and maintain golden datasets. See [Case-specific evaluators](../../evaluators/overview/index.html#case-specific-evaluators) for detailed guidance.", "url": "https://ai.pydantic.dev/how-to/dataset-management/index.html#4-use-case-specific-evaluators-where-appropriate", "page": "how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "5. Separate Datasets by Purpose", "anchor": "5-separate-datasets-by-purpose", "md_text": "```\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\n\n# First create some test datasets\nfor name in ['smoke_tests', 'comprehensive_tests', 'regression_tests']:\n    test_dataset = Dataset[str, Any, Any](cases=[Case(name='test', inputs='example')])\n    test_dataset.to_file(f'{name}.yaml')\n\n# Smoke tests (fast, critical paths)\nsmoke_tests = Dataset[str, Any, Any].from_file('smoke_tests.yaml')\n\n# Comprehensive tests (slow, thorough)\ncomprehensive = Dataset[str, Any, Any].from_file('comprehensive_tests.yaml')\n\n# Regression tests (specific bugs)\nregression = Dataset[str, Any, Any].from_file('regression_tests.yaml')\n```", "url": "https://ai.pydantic.dev/how-to/dataset-management/index.html#5-separate-datasets-by-purpose", "page": "how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "md_text": "* **[Dataset Serialization](../dataset-serialization/index.html)** - In-depth guide to saving and loading datasets\n* **[Generating Datasets](index.html#generating-datasets)** - Use LLMs to generate test cases\n* **[Examples: Simple Validation](../../examples/simple-validation/index.html)** - Practical examples", "url": "https://ai.pydantic.dev/how-to/dataset-management/index.html#next-steps", "page": "how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "md_text": "By default, Pydantic Evals runs all cases concurrently to maximize throughput. You can control this behavior using the `max_concurrency` parameter.", "url": "https://ai.pydantic.dev/how-to/concurrency/index.html#overview", "page": "how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Basic Usage", "anchor": "basic-usage", "md_text": "```\nfrom pydantic_evals import Case, Dataset\n\n\ndef my_task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test1'), Case(inputs='test2')])\n\n# Run all cases concurrently (default)\nreport = dataset.evaluate_sync(my_task)\n\n# Limit to 5 concurrent cases\nreport = dataset.evaluate_sync(my_task, max_concurrency=5)\n\n# Run sequentially (one at a time)\nreport = dataset.evaluate_sync(my_task, max_concurrency=1)\n```", "url": "https://ai.pydantic.dev/how-to/concurrency/index.html#basic-usage", "page": "how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "When to Limit Concurrency", "anchor": "when-to-limit-concurrency", "md_text": "", "url": "https://ai.pydantic.dev/how-to/concurrency/index.html#when-to-limit-concurrency", "page": "how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Rate Limiting", "anchor": "rate-limiting", "md_text": "Many APIs have rate limits that restrict concurrent requests:\n\n```\nfrom pydantic_evals import Case, Dataset\n\n\nasync def my_llm_task(inputs: str) -> str:\n    return f'LLM Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test1')])\n\n# If your API allows 10 requests/second\nreport = dataset.evaluate_sync(\n    my_llm_task,\n    max_concurrency=10,\n)\n```", "url": "https://ai.pydantic.dev/how-to/concurrency/index.html#rate-limiting", "page": "how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Resource Constraints", "anchor": "resource-constraints", "md_text": "Limit concurrency to avoid overwhelming system resources:\n\n```\nfrom pydantic_evals import Case, Dataset\n\n\ndef heavy_computation(inputs: str) -> str:\n    return f'Heavy: {inputs}'\n\n\ndef db_query_task(inputs: str) -> str:\n    return f'DB: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test1')])\n\n# Memory-intensive operations\nreport = dataset.evaluate_sync(\n    heavy_computation,\n    max_concurrency=2,  # Only 2 at a time\n)\n\n# Database connection pool limits\nreport = dataset.evaluate_sync(\n    db_query_task,\n    max_concurrency=5,  # Match connection pool size\n)\n```", "url": "https://ai.pydantic.dev/how-to/concurrency/index.html#resource-constraints", "page": "how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Debugging", "anchor": "debugging", "md_text": "Run sequentially to see clear error traces:\n\n```\nfrom pydantic_evals import Case, Dataset\n\n\ndef my_task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test1')])\n\n# Easier to debug\nreport = dataset.evaluate_sync(\n    my_task,\n    max_concurrency=1,\n)\n```", "url": "https://ai.pydantic.dev/how-to/concurrency/index.html#debugging", "page": "how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Performance Comparison", "anchor": "performance-comparison", "md_text": "Here's an example showing the performance difference:\n\nconcurrency\\_example.py\n\n```\nimport asyncio\n\nfrom pydantic_evals import Case, Dataset\n\n# Create a dataset with multiple test cases\ndataset = Dataset(\n    cases=[\n        Case(\n            name=f'case_{i}',\n            inputs=i,\n            expected_output=i * 2,\n        )\n        for i in range(10)\n    ]\n)\n\n\nasync def slow_task(input_value: int) -> int:\n    \"\"\"Simulates a slow operation (e.g., API call).\"\"\"\n    await asyncio.sleep(0.1)  # 100ms per case\n    return input_value * 2\n\n\n# Unlimited concurrency: ~0.1s total (all cases run in parallel)\nreport = dataset.evaluate_sync(slow_task)\n\n# Limited concurrency: ~0.5s total (2 at a time, 5 batches)\nreport = dataset.evaluate_sync(slow_task, max_concurrency=2)\n\n# Sequential: ~1.0s total (one at a time, 10 cases)\nreport = dataset.evaluate_sync(slow_task, max_concurrency=1)\n```", "url": "https://ai.pydantic.dev/how-to/concurrency/index.html#performance-comparison", "page": "how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Concurrency with Evaluators", "anchor": "concurrency-with-evaluators", "md_text": "Both task execution and evaluator execution happen concurrently by default:\n\n```\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\n\ndef my_task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(\n    cases=[Case(inputs=f'test{i}') for i in range(100)],  # 100 cases\n    evaluators=[\n        LLMJudge(rubric='Quality check'),  # Makes API calls\n    ],\n)\n\n# Both task and evaluator run with controlled concurrency\nreport = dataset.evaluate_sync(\n    my_task,\n    max_concurrency=10,\n)\n```\n\nIf your evaluators are expensive (e.g., [`LLMJudge`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.LLMJudge)), limiting concurrency helps manage:\n- API rate limits\n- Cost (fewer concurrent API calls)\n- Memory usage", "url": "https://ai.pydantic.dev/how-to/concurrency/index.html#concurrency-with-evaluators", "page": "how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Async vs Sync", "anchor": "async-vs-sync", "md_text": "Both sync and async evaluation support concurrency control:", "url": "https://ai.pydantic.dev/how-to/concurrency/index.html#async-vs-sync", "page": "how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Sync API", "anchor": "sync-api", "md_text": "```\nfrom pydantic_evals import Case, Dataset\n\n\ndef my_task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test1')])\n\n# Runs async operations internally with controlled concurrency\nreport = dataset.evaluate_sync(my_task, max_concurrency=10)\n```", "url": "https://ai.pydantic.dev/how-to/concurrency/index.html#sync-api", "page": "how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Async API", "anchor": "async-api", "md_text": "```\nfrom pydantic_evals import Case, Dataset\n\n\nasync def my_task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\nasync def run_evaluation():\n    dataset = Dataset(cases=[Case(inputs='test1')])\n    # Same behavior, but in async context\n    report = await dataset.evaluate(my_task, max_concurrency=10)\n    return report\n```", "url": "https://ai.pydantic.dev/how-to/concurrency/index.html#async-api", "page": "how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Monitoring Concurrency", "anchor": "monitoring-concurrency", "md_text": "Track execution to optimize settings:\n\n```\nimport time\n\nfrom pydantic_evals import Case, Dataset\n\n\ndef task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs=f'test{i}') for i in range(10)])\n\nt0 = time.time()\nreport = dataset.evaluate_sync(task, max_concurrency=10)\nduration = time.time() - t0\n\nnum_cases = len(report.cases) + len(report.failures)\navg_duration = duration / num_cases\n\nprint(f'Total: {duration:.2f}s')\n#> Total: 0.01s\nprint(f'Cases: {num_cases}')\n#> Cases: 10\nprint(f'Avg per case: {avg_duration:.2f}s')\n#> Avg per case: 0.00s\nprint(f'Effective concurrency: ~{num_cases * avg_duration / duration:.1f}')\n#> Effective concurrency: ~1.0\n```", "url": "https://ai.pydantic.dev/how-to/concurrency/index.html#monitoring-concurrency", "page": "how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Handling Rate Limits", "anchor": "handling-rate-limits", "md_text": "If you hit rate limits, the evaluation will fail. Use retry strategies:\n\n```\nfrom pydantic_evals import Case, Dataset\n\n\ndef task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test1')])\n\n# Reduce concurrency to avoid rate limits\nreport = dataset.evaluate_sync(\n    task,\n    max_concurrency=5,  # Stay under rate limit\n)\n```\n\nSee [Retry Strategies](../retry-strategies/index.html) for handling transient failures.", "url": "https://ai.pydantic.dev/how-to/concurrency/index.html#handling-rate-limits", "page": "how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "md_text": "* **[Retry Strategies](../retry-strategies/index.html)** - Handle transient failures\n* **[Dataset Management](../dataset-management/index.html)** - Work with large datasets\n* **[Logfire Integration](../logfire-integration/index.html)** - Monitor performance", "url": "https://ai.pydantic.dev/how-to/concurrency/index.html#next-steps", "page": "how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "md_text": "Pydantic Evals supports serializing datasets to files in two formats:\n\n* **YAML** (`.yaml`, `.yml`) - Human-readable, great for version control\n* **JSON** (`.json`) - Structured, machine-readable\n\nBoth formats support:\n- Automatic JSON schema generation for IDE autocomplete and validation\n- Custom evaluator serialization/deserialization\n- Type-safe loading with generic parameters", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#overview", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "YAML Format", "anchor": "yaml-format", "md_text": "YAML is the recommended format for most use cases due to its readability and compact syntax.", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#yaml-format", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Basic Example", "anchor": "basic-example", "md_text": "```\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import EqualsExpected, IsInstance\n\n# Create a dataset with typed parameters\ndataset = Dataset[str, str, Any](\n    name='my_tests',\n    cases=[\n        Case(\n            name='test_1',\n            inputs='hello',\n            expected_output='HELLO',\n        ),\n    ],\n    evaluators=[\n        IsInstance(type_name='str'),\n        EqualsExpected(),\n    ],\n)\n\n# Save to YAML\ndataset.to_file('my_tests.yaml')\n```\n\nThis creates two files:\n\n1. **`my_tests.yaml`** - The dataset\n2. **`my_tests_schema.json`** - JSON schema for IDE support", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#basic-example", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "YAML Output", "anchor": "yaml-output", "md_text": "```\n# yaml-language-server: $schema=my_tests_schema.json\nname: my_tests\ncases:\n- name: test_1\n  inputs: hello\n  expected_output: HELLO\nevaluators:\n- IsInstance: str\n- EqualsExpected\n```", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#yaml-output", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "JSON Schema for IDEs", "anchor": "json-schema-for-ides", "md_text": "The first line references the schema file:\n\n```\n# yaml-language-server: $schema=my_tests_schema.json\n```\n\nThis enables:\n- ✅ **Autocomplete** in VS Code, PyCharm, and other editors\n- ✅ **Inline validation** while editing\n- ✅ **Documentation tooltips** for fields\n- ✅ **Error highlighting** for invalid data\n\nThe `yaml-language-server` comment is supported by:\n\n* VS Code (with YAML extension)\n* JetBrains IDEs (PyCharm, IntelliJ, etc.)\n* Most editors with YAML language server support\n\nSee the [YAML Language Server docs](https://github.com/redhat-developer/yaml-language-server#using-inlined-schema) for more details.", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#json-schema-for-ides", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Loading from YAML", "anchor": "loading-from-yaml", "md_text": "```\nfrom pathlib import Path\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import EqualsExpected, IsInstance\n\n# First create and save the dataset\nPath('my_tests.yaml').parent.mkdir(exist_ok=True)\ndataset = Dataset[str, str, Any](\n    name='my_tests',\n    cases=[Case(name='test_1', inputs='hello', expected_output='HELLO')],\n    evaluators=[IsInstance(type_name='str'), EqualsExpected()],\n)\ndataset.to_file('my_tests.yaml')\n\n# Load the dataset with type parameters\ndataset = Dataset[str, str, Any].from_file('my_tests.yaml')\n\n\ndef my_task(text: str) -> str:\n    return text.upper()\n\n\n# Run evaluation\nreport = dataset.evaluate_sync(my_task)\n```", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#loading-from-yaml", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "JSON Format", "anchor": "json-format", "md_text": "JSON format is useful for programmatic generation or when strict structure is required.", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#json-format", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Basic Example", "anchor": "basic-example", "md_text": "```\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import EqualsExpected\n\ndataset = Dataset[str, str, Any](\n    name='my_tests',\n    cases=[\n        Case(name='test_1', inputs='hello', expected_output='HELLO'),\n    ],\n    evaluators=[EqualsExpected()],\n)\n\n# Save to JSON\ndataset.to_file('my_tests.json')\n```", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#basic-example", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "JSON Output", "anchor": "json-output", "md_text": "```\n{\n  \"$schema\": \"my_tests_schema.json\",\n  \"name\": \"my_tests\",\n  \"cases\": [\n    {\n      \"name\": \"test_1\",\n      \"inputs\": \"hello\",\n      \"expected_output\": \"HELLO\"\n    }\n  ],\n  \"evaluators\": [\n    \"EqualsExpected\"\n  ]\n}\n```\n\nThe `$schema` key at the top enables IDE support similar to YAML.", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#json-output", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Loading from JSON", "anchor": "loading-from-json", "md_text": "```\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import EqualsExpected\n\n# First create and save the dataset\ndataset = Dataset[str, str, Any](\n    name='my_tests',\n    cases=[Case(name='test_1', inputs='hello', expected_output='HELLO')],\n    evaluators=[EqualsExpected()],\n)\ndataset.to_file('my_tests.json')\n\n# Load from JSON\ndataset = Dataset[str, str, Any].from_file('my_tests.json')\n```", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#loading-from-json", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Schema Generation", "anchor": "schema-generation", "md_text": "", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#schema-generation", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Automatic Schema Creation", "anchor": "automatic-schema-creation", "md_text": "By default, `to_file()` creates a JSON schema file alongside your dataset:\n\n```\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\n\ndataset = Dataset[str, str, Any](cases=[Case(inputs='test')])\n\n# Creates both my_tests.yaml AND my_tests_schema.json\ndataset.to_file('my_tests.yaml')\n```", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#automatic-schema-creation", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Schema Location", "anchor": "custom-schema-location", "md_text": "```\nfrom pathlib import Path\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\n\ndataset = Dataset[str, str, Any](cases=[Case(inputs='test')])\n\n# Create directories\nPath('data').mkdir(exist_ok=True)\n\n# Custom schema filename (relative to dataset file location)\ndataset.to_file(\n    'data/my_tests.yaml',\n    schema_path='my_schema.json',\n)\n\n# No schema file\ndataset.to_file('my_tests.yaml', schema_path=None)\n```", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#custom-schema-location", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Schema Path Templates", "anchor": "schema-path-templates", "md_text": "Use `{stem}` to reference the dataset filename:\n\n```\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\n\ndataset = Dataset[str, str, Any](cases=[Case(inputs='test')])\n\n# Creates: my_tests.yaml and my_tests.schema.json\ndataset.to_file(\n    'my_tests.yaml',\n    schema_path='{stem}.schema.json',\n)\n```", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#schema-path-templates", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Manual Schema Generation", "anchor": "manual-schema-generation", "md_text": "Generate a schema without saving the dataset:\n\n```\nimport json\nfrom typing import Any\n\nfrom pydantic_evals import Dataset\n\n# Get schema as dictionary for a specific dataset type\nschema = Dataset[str, str, Any].model_json_schema_with_evaluators()\n\n# Save manually\nwith open('custom_schema.json', 'w') as f:\n    json.dump(schema, f, indent=2)\n```", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#manual-schema-generation", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Evaluators", "anchor": "custom-evaluators", "md_text": "Custom evaluators require special handling during serialization and deserialization.", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#custom-evaluators", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Requirements", "anchor": "requirements", "md_text": "Custom evaluators must:\n\n1. Be decorated with `@dataclass`\n2. Inherit from `Evaluator`\n3. Be passed to both `to_file()` and `from_file()`", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#requirements", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Complete Example", "anchor": "complete-example", "md_text": "```\nfrom dataclasses import dataclass\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass CustomThreshold(Evaluator):\n    \"\"\"Check if output length exceeds a threshold.\"\"\"\n\n    min_length: int\n    max_length: int = 100\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        length = len(str(ctx.output))\n        return self.min_length <= length <= self.max_length\n\n\n# Create dataset with custom evaluator\ndataset = Dataset[str, str, Any](\n    cases=[\n        Case(\n            name='test_length',\n            inputs='example',\n            expected_output='long result',\n            evaluators=[\n                CustomThreshold(min_length=5, max_length=20),\n            ],\n        ),\n    ],\n)\n\n# Save with custom evaluator types\ndataset.to_file(\n    'dataset.yaml',\n    custom_evaluator_types=[CustomThreshold],\n)\n```", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#complete-example", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Saved YAML", "anchor": "saved-yaml", "md_text": "```\n# yaml-language-server: $schema=dataset_schema.json\ncases:\n- name: test_length\n  inputs: example\n  expected_output: long result\n  evaluators:\n  - CustomThreshold:\n      min_length: 5\n      max_length: 20\n```", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#saved-yaml", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Loading with Custom Evaluators", "anchor": "loading-with-custom-evaluators", "md_text": "```\nfrom dataclasses import dataclass\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass CustomThreshold(Evaluator):\n    \"\"\"Check if output length exceeds a threshold.\"\"\"\n\n    min_length: int\n    max_length: int = 100\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        length = len(str(ctx.output))\n        return self.min_length <= length <= self.max_length\n\n\n# First create and save the dataset\ndataset = Dataset[str, str, Any](\n    cases=[\n        Case(\n            name='test_length',\n            inputs='example',\n            expected_output='long result',\n            evaluators=[CustomThreshold(min_length=5, max_length=20)],\n        ),\n    ],\n)\ndataset.to_file('dataset.yaml', custom_evaluator_types=[CustomThreshold])\n\n# Load with custom evaluator registry\ndataset = Dataset[str, str, Any].from_file(\n    'dataset.yaml',\n    custom_evaluator_types=[CustomThreshold],\n)\n```\n\nYou must pass `custom_evaluator_types` to **both** `to_file()` and `from_file()`.\n\n* `to_file()`: Includes the evaluator in the JSON schema\n* `from_file()`: Registers the evaluator for deserialization", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#loading-with-custom-evaluators", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluator Serialization Formats", "anchor": "evaluator-serialization-formats", "md_text": "Evaluators can be serialized in three forms:", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#evaluator-serialization-formats", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "1. Name Only (No Parameters)", "anchor": "1-name-only-no-parameters", "md_text": "```\nevaluators:\n- EqualsExpected\n- IsInstance: str  # Using default parameter\n```", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#1-name-only-no-parameters", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "2. Single Parameter (Short Form)", "anchor": "2-single-parameter-short-form", "md_text": "```\nevaluators:\n- IsInstance: str\n- Contains: \"required text\"\n- MaxDuration: 2.0\n```", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#2-single-parameter-short-form", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "3. Multiple Parameters (Dict Form)", "anchor": "3-multiple-parameters-dict-form", "md_text": "```\nevaluators:\n- CustomThreshold:\n    min_length: 5\n    max_length: 20\n- LLMJudge:\n    rubric: \"Response is accurate\"\n    model: \"openai:gpt-4o\"\n    include_input: true\n```", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#3-multiple-parameters-dict-form", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Format Comparison", "anchor": "format-comparison", "md_text": "| Feature | YAML | JSON |\n| --- | --- | --- |\n| Human readable | ✅ Excellent | ⚠️ Good |\n| Comments | ✅ Yes | ❌ No |\n| Compact | ✅ Yes | ⚠️ Verbose |\n| Machine parsing | ✅ Good | ✅ Excellent |\n| IDE support | ✅ Yes | ✅ Yes |\n| Version control | ✅ Clean diffs | ⚠️ Noisy diffs |\n\n**Recommendation**: Use YAML for most cases, JSON for programmatic generation.", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#format-comparison", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Advanced: Evaluator Serialization Name", "anchor": "advanced-evaluator-serialization-name", "md_text": "Customize how your evaluator appears in serialized files:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass VeryLongDescriptiveEvaluatorName(Evaluator):\n    @classmethod\n    def get_serialization_name(cls) -> str:\n        return 'ShortName'\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return True\n```\n\nIn YAML:\n\n```\nevaluators:\n- ShortName  # Instead of VeryLongDescriptiveEvaluatorName\n```", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#advanced-evaluator-serialization-name", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Troubleshooting", "anchor": "troubleshooting", "md_text": "", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#troubleshooting", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Schema Not Found in IDE", "anchor": "schema-not-found-in-ide", "md_text": "**Problem**: YAML file doesn't show autocomplete\n\n**Solutions**:\n\n1. **Check the schema path** in the first line of YAML:\n\n   ```\n   # yaml-language-server: $schema=correct_schema_name.json\n   ```\n2. **Verify schema file exists** in the same directory\n3. **Restart the language server** in your IDE\n4. **Install YAML extension** (VS Code: \"YAML\" by Red Hat)", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#schema-not-found-in-ide", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Evaluator Not Found", "anchor": "custom-evaluator-not-found", "md_text": "**Problem**: `ValueError: Unknown evaluator name: 'CustomEvaluator'`\n\n**Solution**: Pass `custom_evaluator_types` when loading:\n\n```\nfrom dataclasses import dataclass\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass CustomEvaluator(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return True\n\n\n# First create and save with custom evaluator\ndataset = Dataset[str, str, Any](\n    cases=[Case(inputs='test', evaluators=[CustomEvaluator()])],\n)\ndataset.to_file('tests.yaml', custom_evaluator_types=[CustomEvaluator])\n\n# Load with custom evaluator types\ndataset = Dataset[str, str, Any].from_file(\n    'tests.yaml',\n    custom_evaluator_types=[CustomEvaluator],  # Required!\n)\n```", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#custom-evaluator-not-found", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Format Inference Failed", "anchor": "format-inference-failed", "md_text": "**Problem**: `ValueError: Cannot infer format from extension`\n\n**Solution**: Specify format explicitly:\n\n```\nfrom typing import Any\n\nfrom pydantic_evals import Case, Dataset\n\ndataset = Dataset[str, str, Any](cases=[Case(inputs='test')])\n\n# Explicit format for unusual extensions\ndataset.to_file('data.txt', fmt='yaml')\ndataset_loaded = Dataset[str, str, Any].from_file('data.txt', fmt='yaml')\n```", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#format-inference-failed", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Schema Generation Error", "anchor": "schema-generation-error", "md_text": "**Problem**: Custom evaluator causes schema generation to fail\n\n**Solution**: Ensure evaluator is a proper dataclass:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n# ✅ Correct\n@dataclass\nclass MyEvaluator(Evaluator):\n    value: int\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return True\n\n\n# ❌ Wrong: Missing @dataclass\nclass BadEvaluator(Evaluator):\n    def __init__(self, value: int):\n        self.value = value\n\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return True\n```", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#schema-generation-error", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "md_text": "* **[Dataset Management](../dataset-management/index.html)** - Creating and organizing datasets\n* **[Custom Evaluators](../../evaluators/custom/index.html)** - Write custom evaluation logic\n* **[Core Concepts](../../core-concepts/index.html)** - Understand the data model", "url": "https://ai.pydantic.dev/how-to/dataset-serialization/index.html#next-steps", "page": "how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "md_text": "Pydantic Evals uses OpenTelemetry to record traces of the evaluation process. These traces contain all the information from your evaluation reports, plus full tracing from the execution of your task function.\n\nYou can send these traces to any OpenTelemetry-compatible backend, including [Pydantic Logfire](https://logfire.pydantic.dev/docs/guides/web-ui/evals/).", "url": "https://ai.pydantic.dev/how-to/logfire-integration/index.html#overview", "page": "how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Installation", "anchor": "installation", "md_text": "Install the optional logfire dependency:\n\n```\npip install 'pydantic-evals[logfire]'\n```", "url": "https://ai.pydantic.dev/how-to/logfire-integration/index.html#installation", "page": "how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Basic Setup", "anchor": "basic-setup", "md_text": "Configure Logfire before running evaluations:\n\nbasic\\_logfire\\_setup.py\n\n```\nimport logfire\n\nfrom pydantic_evals import Case, Dataset\n\n# Configure Logfire\nlogfire.configure(\n    send_to_logfire='if-token-present',  # (1)!\n)\n\n\n# Your evaluation code\ndef my_task(inputs: str) -> str:\n    return f'result for {inputs}'\n\n\ndataset = Dataset(cases=[Case(name='test', inputs='example')])\nreport = dataset.evaluate_sync(my_task)\n```\n\n1. Sends data to Logfire only if the `LOGFIRE_TOKEN` environment variable is set\n\nThat's it! Your evaluation traces will now appear in the Logfire web UI as long as you have the `LOGFIRE_TOKEN` environment variable set.", "url": "https://ai.pydantic.dev/how-to/logfire-integration/index.html#basic-setup", "page": "how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "What Gets Sent to Logfire", "anchor": "what-gets-sent-to-logfire", "md_text": "When you run an evaluation, Logfire receives:\n\n1. **Evaluation metadata**\n   1. Dataset name\n   2. Number of cases\n   3. Evaluator names\n2. **Per-case data**\n   1. Inputs and outputs\n   2. Expected outputs\n   3. Metadata\n   4. Execution duration\n3. **Evaluation results**\n   1. Scores, assertions, and labels\n   2. Reasons (if included)\n   3. Evaluator failures\n4. **Task execution traces**\n   1. All OpenTelemetry spans from your task function\n   2. Tool calls (for Pydantic AI agents)\n   3. API calls, database queries, etc.", "url": "https://ai.pydantic.dev/how-to/logfire-integration/index.html#what-gets-sent-to-logfire", "page": "how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Viewing Results in Logfire", "anchor": "viewing-results-in-logfire", "md_text": "", "url": "https://ai.pydantic.dev/how-to/logfire-integration/index.html#viewing-results-in-logfire", "page": "how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluation Overview", "anchor": "evaluation-overview", "md_text": "Logfire provides a special table view for evaluation results on the root evaluation span:\n\n[![Logfire Evals Overview](../../logfire-evals-overview.png)](../../logfire-evals-overview.png)\n\nThis view shows:\n\n* Case names\n* Pass/fail status\n* Scores and assertions\n* Execution duration\n* Quick filtering and sorting", "url": "https://ai.pydantic.dev/how-to/logfire-integration/index.html#evaluation-overview", "page": "how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Individual Case Details", "anchor": "individual-case-details", "md_text": "Click any case to see detailed inputs and outputs:\n\n[![Logfire Evals Case](../../logfire-evals-case.png)](../../logfire-evals-case.png)", "url": "https://ai.pydantic.dev/how-to/logfire-integration/index.html#individual-case-details", "page": "how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Full Trace View", "anchor": "full-trace-view", "md_text": "View the complete execution trace including all spans generated during evaluation:\n\n[![Logfire Evals Case Trace](../../logfire-evals-case-trace.png)](../../logfire-evals-case-trace.png)\n\nThis is especially useful for:\n\n* Debugging failed cases\n* Understanding performance bottlenecks\n* Analyzing tool usage patterns\n* Writing span-based evaluators", "url": "https://ai.pydantic.dev/how-to/logfire-integration/index.html#full-trace-view", "page": "how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Analyzing Traces", "anchor": "analyzing-traces", "md_text": "", "url": "https://ai.pydantic.dev/how-to/logfire-integration/index.html#analyzing-traces", "page": "how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Comparing Runs", "anchor": "comparing-runs", "md_text": "Run the same evaluation multiple times and compare in Logfire:\n\n```\nfrom pydantic_evals import Case, Dataset\n\n\ndef original_task(inputs: str) -> str:\n    return f'original result for {inputs}'\n\n\ndef improved_task(inputs: str) -> str:\n    return f'improved result for {inputs}'\n\n\ndataset = Dataset(cases=[Case(name='test', inputs='example')])\n\n# Run 1: Original implementation\nreport1 = dataset.evaluate_sync(original_task)\n\n# Run 2: Improved implementation\nreport2 = dataset.evaluate_sync(improved_task)\n\n# Compare in Logfire by filtering by timestamp or attributes\n```", "url": "https://ai.pydantic.dev/how-to/logfire-integration/index.html#comparing-runs", "page": "how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Debugging Failed Cases", "anchor": "debugging-failed-cases", "md_text": "Find failed cases quickly:\n\n1. Search for `service_name = 'my_service_evals' AND is_exception` (replace with the actual service name you are using)\n2. View the full span tree to see where the failure occurred\n3. Inspect attributes and logs for error messages", "url": "https://ai.pydantic.dev/how-to/logfire-integration/index.html#debugging-failed-cases", "page": "how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Span-Based Evaluation", "anchor": "span-based-evaluation", "md_text": "Logfire integration enables powerful span-based evaluators. See [Span-Based Evaluation](../../evaluators/span-based/index.html) for details.\n\nExample: Verify specific tools were called:\n\n```\nimport logfire\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import HasMatchingSpan\n\nlogfire.configure(send_to_logfire='if-token-present')\n\n\ndef my_agent(inputs: str) -> str:\n    return f'result for {inputs}'\n\n\ndataset = Dataset(\n    cases=[Case(name='test', inputs='example')],\n    evaluators=[\n        HasMatchingSpan(\n            query={'name_contains': 'search_tool'},\n            evaluation_name='used_search',\n        ),\n    ],\n)\n\nreport = dataset.evaluate_sync(my_agent)\n```\n\nThe span tree is available in both:\n\n* Your evaluator code (via `ctx.span_tree`)\n* Logfire UI (visual trace view)", "url": "https://ai.pydantic.dev/how-to/logfire-integration/index.html#span-based-evaluation", "page": "how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Troubleshooting", "anchor": "troubleshooting", "md_text": "", "url": "https://ai.pydantic.dev/how-to/logfire-integration/index.html#troubleshooting", "page": "how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "No Data Appearing in Logfire", "anchor": "no-data-appearing-in-logfire", "md_text": "Check:\n\n1. **Token is set**: `echo $LOGFIRE_TOKEN`\n2. **Configuration is correct**:\n\n   ```\n   import logfire\n\n   logfire.configure(send_to_logfire='always')  # Force sending\n   ```\n3. **Network connectivity**: Check firewall settings\n4. **Project exists**: Verify project name in Logfire UI", "url": "https://ai.pydantic.dev/how-to/logfire-integration/index.html#no-data-appearing-in-logfire", "page": "how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Traces Missing Spans", "anchor": "traces-missing-spans", "md_text": "If some spans are missing:\n\n1. **Ensure logfire is configured before imports**:\n\n   ```\n   import logfire\n\n   logfire.configure()  # Must be first\n   ```\n2. **Check instrumentation**: Ensure your code has enabled all instrumentations you want:\n\n   ```\n   import logfire\n\n   logfire.instrument_pydantic_ai()\n   logfire.instrument_httpx(capture_all=True)\n   ```", "url": "https://ai.pydantic.dev/how-to/logfire-integration/index.html#traces-missing-spans", "page": "how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Best Practices", "anchor": "best-practices", "md_text": "", "url": "https://ai.pydantic.dev/how-to/logfire-integration/index.html#best-practices", "page": "how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "1. Configure Early", "anchor": "1-configure-early", "md_text": "Always configure Logfire before running evaluations:\n\n```\nimport logfire\n\nfrom pydantic_evals import Case, Dataset\n\nlogfire.configure(send_to_logfire='if-token-present')\n\n\n# Now import and run evaluations\ndef task(inputs: str) -> str:\n    return f'result for {inputs}'\n\n\ndataset = Dataset(cases=[Case(name='test', inputs='example')])\ndataset.evaluate_sync(task)\n```", "url": "https://ai.pydantic.dev/how-to/logfire-integration/index.html#1-configure-early", "page": "how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "2. Use Descriptive Service Names And Environments", "anchor": "2-use-descriptive-service-names-and-environments", "md_text": "```\nimport logfire\n\nlogfire.configure(\n    service_name='rag-pipeline-evals',\n    environment='development',\n)\n```", "url": "https://ai.pydantic.dev/how-to/logfire-integration/index.html#2-use-descriptive-service-names-and-environments", "page": "how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "3. Review Periodically", "anchor": "3-review-periodically", "md_text": "* Check Logfire regularly to identify patterns\n* Look for consistently failing cases\n* Analyze performance trends\n* Adjust evaluators based on insights", "url": "https://ai.pydantic.dev/how-to/logfire-integration/index.html#3-review-periodically", "page": "how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "md_text": "* **[Span-Based Evaluation](../../evaluators/span-based/index.html)** - Use OpenTelemetry spans in evaluators\n* **[Logfire Documentation](https://logfire.pydantic.dev/docs/guides/web-ui/evals/)** - Complete Logfire guide\n* **[Metrics & Attributes](../metrics-attributes/index.html)** - Add custom data to traces", "url": "https://ai.pydantic.dev/how-to/logfire-integration/index.html#next-steps", "page": "how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "md_text": "LLM-based systems can experience transient failures:\n\n* Rate limits\n* Network timeouts\n* Temporary API outages\n* Context length errors\n\nPydantic Evals supports retry configuration for both:\n\n* **Task execution** - The function being evaluated\n* **Evaluator execution** - The evaluators themselves", "url": "https://ai.pydantic.dev/how-to/retry-strategies/index.html#overview", "page": "how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Basic Retry Configuration", "anchor": "basic-retry-configuration", "md_text": "Pass a retry configuration to `evaluate()` or `evaluate_sync()` using [Tenacity](https://tenacity.readthedocs.io/) parameters:\n\n```\nfrom tenacity import stop_after_attempt\n\nfrom pydantic_evals import Case, Dataset\n\n\ndef my_function(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test')], evaluators=[])\n\nreport = dataset.evaluate_sync(\n    task=my_function,\n    retry_task={'stop': stop_after_attempt(3)},\n    retry_evaluators={'stop': stop_after_attempt(2)},\n)\n```", "url": "https://ai.pydantic.dev/how-to/retry-strategies/index.html#basic-retry-configuration", "page": "how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Retry Configuration Options", "anchor": "retry-configuration-options", "md_text": "Retry configurations use [Tenacity](https://tenacity.readthedocs.io/) and support the same options as Pydantic AI's [`RetryConfig`](../../retries/index.html#pydantic_ai.retries.RetryConfig):\n\n```\nfrom tenacity import stop_after_attempt, wait_exponential\n\nfrom pydantic_evals import Case, Dataset\n\n\ndef my_function(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test')], evaluators=[])\n\nretry_config = {\n    'stop': stop_after_attempt(3),  # Stop after 3 attempts\n    'wait': wait_exponential(multiplier=1, min=1, max=10),  # Exponential backoff: 1s, 2s, 4s, 8s (capped at 10s)\n    'reraise': True,  # Re-raise the original exception after exhausting retries\n}\n\ndataset.evaluate_sync(\n    task=my_function,\n    retry_task=retry_config,\n)\n```", "url": "https://ai.pydantic.dev/how-to/retry-strategies/index.html#retry-configuration-options", "page": "how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Common Parameters", "anchor": "common-parameters", "md_text": "The retry configuration accepts any parameters from the tenacity `retry` decorator. Common ones include:\n\n| Parameter | Type | Description |\n| --- | --- | --- |\n| `stop` | `StopBaseT` | Stop strategy (e.g., `stop_after_attempt(3)`, `stop_after_delay(60)`) |\n| `wait` | `WaitBaseT` | Wait strategy (e.g., `wait_exponential()`, `wait_fixed(2)`) |\n| `retry` | `RetryBaseT` | Retry condition (e.g., `retry_if_exception_type(TimeoutError)`) |\n| `reraise` | `bool` | Whether to reraise the original exception (default: `False`) |\n| `before_sleep` | `Callable` | Callback before sleeping between retries |\n\nSee the [Tenacity documentation](https://tenacity.readthedocs.io/) for all available options.", "url": "https://ai.pydantic.dev/how-to/retry-strategies/index.html#common-parameters", "page": "how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Task Retries", "anchor": "task-retries", "md_text": "Retry the task function when it fails:\n\n```\nfrom tenacity import stop_after_attempt, wait_exponential\n\nfrom pydantic_evals import Case, Dataset\n\n\nasync def call_llm(inputs: str) -> str:\n    return f'LLM response to: {inputs}'\n\n\nasync def flaky_llm_task(inputs: str) -> str:\n    \"\"\"This might hit rate limits or timeout.\"\"\"\n    response = await call_llm(inputs)\n    return response\n\n\ndataset = Dataset(cases=[Case(inputs='test')])\n\nreport = dataset.evaluate_sync(\n    task=flaky_llm_task,\n    retry_task={\n        'stop': stop_after_attempt(5),  # Try up to 5 times\n        'wait': wait_exponential(multiplier=1, min=1, max=30),  # Exponential backoff, capped at 30s\n        'reraise': True,\n    },\n)\n```", "url": "https://ai.pydantic.dev/how-to/retry-strategies/index.html#task-retries", "page": "how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "When Task Retries Trigger", "anchor": "when-task-retries-trigger", "md_text": "Retries trigger when the task raises an exception:\n\n```\nclass RateLimitError(Exception):\n    pass\n\n\nclass ValidationError(Exception):\n    pass\n\n\nasync def call_api(inputs: str) -> str:\n    return f'API response: {inputs}'\n\n\nasync def my_task(inputs: str) -> str:\n    try:\n        return await call_api(inputs)\n    except RateLimitError:\n        # Will trigger retry\n        raise\n    except ValidationError:\n        # Will also trigger retry\n        raise\n```", "url": "https://ai.pydantic.dev/how-to/retry-strategies/index.html#when-task-retries-trigger", "page": "how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Exponential Backoff", "anchor": "exponential-backoff", "md_text": "When using `wait_exponential()`, delays increase exponentially:\n\n```\nAttempt 1: immediate\nAttempt 2: ~1s delay (multiplier * 2^0)\nAttempt 3: ~2s delay (multiplier * 2^1)\nAttempt 4: ~4s delay (multiplier * 2^2)\nAttempt 5: ~8s delay (multiplier * 2^3, capped at max)\n```\n\nThe actual delay depends on the `multiplier`, `min`, and `max` parameters passed to `wait_exponential()`.", "url": "https://ai.pydantic.dev/how-to/retry-strategies/index.html#exponential-backoff", "page": "how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluator Retries", "anchor": "evaluator-retries", "md_text": "Retry evaluators when they fail:\n\n```\nfrom tenacity import stop_after_attempt, wait_exponential\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\n\ndef my_task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[\n        # LLMJudge might hit rate limits\n        LLMJudge(rubric='Response is accurate'),\n    ],\n)\n\nreport = dataset.evaluate_sync(\n    task=my_task,\n    retry_evaluators={\n        'stop': stop_after_attempt(3),\n        'wait': wait_exponential(multiplier=1, min=0.5, max=10),\n        'reraise': True,\n    },\n)\n```", "url": "https://ai.pydantic.dev/how-to/retry-strategies/index.html#evaluator-retries", "page": "how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "When Evaluator Retries Trigger", "anchor": "when-evaluator-retries-trigger", "md_text": "Retries trigger when an evaluator raises an exception:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\nasync def external_api_call(output: str) -> bool:\n    return len(output) > 0\n\n\n@dataclass\nclass APIEvaluator(Evaluator):\n    async def evaluate(self, ctx: EvaluatorContext) -> bool:\n        # If this raises an exception, retry logic will trigger\n        result = await external_api_call(ctx.output)\n        return result\n```", "url": "https://ai.pydantic.dev/how-to/retry-strategies/index.html#when-evaluator-retries-trigger", "page": "how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluator Failures", "anchor": "evaluator-failures", "md_text": "If an evaluator fails after all retries, it's recorded as an [`EvaluatorFailure`](../../pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorFailure):\n\n```\nfrom tenacity import stop_after_attempt\n\nfrom pydantic_evals import Case, Dataset\n\n\ndef task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test')], evaluators=[])\n\nreport = dataset.evaluate_sync(task, retry_evaluators={'stop': stop_after_attempt(3)})\n\n# Check for evaluator failures\nfor case in report.cases:\n    if case.evaluator_failures:\n        for failure in case.evaluator_failures:\n            print(f'Evaluator {failure.name} failed: {failure.error_message}')\n    #> (No output - no evaluator failures in this case)\n```\n\nView evaluator failures in reports:\n\n```\nfrom pydantic_evals import Case, Dataset\n\n\ndef task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test')], evaluators=[])\nreport = dataset.evaluate_sync(task)\n\nreport.print(include_evaluator_failures=True)\n\"\"\"\n  Evaluation Summary:\n         task\n┏━━━━━━━━━━┳━━━━━━━━━━┓\n┃ Case ID  ┃ Duration ┃\n┡━━━━━━━━━━╇━━━━━━━━━━┩\n│ Case 1   │     10ms │\n├──────────┼──────────┤\n│ Averages │     10ms │\n└──────────┴──────────┘\n\"\"\"\n#>\n#> ✅ case_0                       ━━━━━━━━━━━━━━━━━━━━━━━━━━━ 100% (0/0)\n```", "url": "https://ai.pydantic.dev/how-to/retry-strategies/index.html#evaluator-failures", "page": "how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Combining Task and Evaluator Retries", "anchor": "combining-task-and-evaluator-retries", "md_text": "You can configure both independently:\n\n```\nfrom tenacity import stop_after_attempt, wait_exponential\n\nfrom pydantic_evals import Case, Dataset\n\n\ndef flaky_task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test')], evaluators=[])\n\nreport = dataset.evaluate_sync(\n    task=flaky_task,\n    retry_task={\n        'stop': stop_after_attempt(5),  # Retry task up to 5 times\n        'wait': wait_exponential(multiplier=1, min=1, max=30),\n        'reraise': True,\n    },\n    retry_evaluators={\n        'stop': stop_after_attempt(3),  # Retry evaluators up to 3 times\n        'wait': wait_exponential(multiplier=1, min=0.5, max=10),\n        'reraise': True,\n    },\n)\n```", "url": "https://ai.pydantic.dev/how-to/retry-strategies/index.html#combining-task-and-evaluator-retries", "page": "how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Practical Examples", "anchor": "practical-examples", "md_text": "", "url": "https://ai.pydantic.dev/how-to/retry-strategies/index.html#practical-examples", "page": "how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Rate Limit Handling", "anchor": "rate-limit-handling", "md_text": "```\nfrom tenacity import stop_after_attempt, wait_exponential\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import LLMJudge\n\n\nasync def expensive_llm_call(inputs: str) -> str:\n    return f'LLM response: {inputs}'\n\n\nasync def llm_task(inputs: str) -> str:\n    \"\"\"Task that might hit rate limits.\"\"\"\n    return await expensive_llm_call(inputs)\n\n\ndataset = Dataset(\n    cases=[Case(inputs='test')],\n    evaluators=[\n        LLMJudge(rubric='Quality check'),  # Also might hit rate limits\n    ],\n)\n\n# Generous retries for rate limits\nreport = dataset.evaluate_sync(\n    task=llm_task,\n    retry_task={\n        'stop': stop_after_attempt(10),  # Rate limits can take multiple retries\n        'wait': wait_exponential(multiplier=2, min=2, max=60),  # Start at 2s, exponential up to 60s\n        'reraise': True,\n    },\n    retry_evaluators={\n        'stop': stop_after_attempt(5),\n        'wait': wait_exponential(multiplier=2, min=2, max=30),\n        'reraise': True,\n    },\n)\n```", "url": "https://ai.pydantic.dev/how-to/retry-strategies/index.html#rate-limit-handling", "page": "how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Network Timeout Handling", "anchor": "network-timeout-handling", "md_text": "```\nimport httpx\nfrom tenacity import stop_after_attempt, wait_exponential\n\nfrom pydantic_evals import Case, Dataset\n\n\nasync def api_task(inputs: str) -> str:\n    \"\"\"Task that calls external API which might timeout.\"\"\"\n    async with httpx.AsyncClient(timeout=10.0) as client:\n        response = await client.post('https://api.example.com', json={'input': inputs})\n        return response.text\n\n\ndataset = Dataset(cases=[Case(inputs='test')], evaluators=[])\n\n# Quick retries for network issues\nreport = dataset.evaluate_sync(\n    task=api_task,\n    retry_task={\n        'stop': stop_after_attempt(4),  # A few quick retries\n        'wait': wait_exponential(multiplier=0.5, min=0.5, max=5),  # Fast retry, capped at 5s\n        'reraise': True,\n    },\n)\n```", "url": "https://ai.pydantic.dev/how-to/retry-strategies/index.html#network-timeout-handling", "page": "how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Context Length Handling", "anchor": "context-length-handling", "md_text": "```\nfrom tenacity import stop_after_attempt\n\nfrom pydantic_evals import Case, Dataset\n\n\nclass ContextLengthError(Exception):\n    pass\n\n\nasync def llm_call(inputs: str, max_tokens: int = 8000) -> str:\n    return f'LLM response: {inputs[:100]}'\n\n\nasync def smart_llm_task(inputs: str) -> str:\n    \"\"\"Task that might exceed context length.\"\"\"\n    try:\n        return await llm_call(inputs, max_tokens=8000)\n    except ContextLengthError:\n        # Retry with shorter context\n        truncated_inputs = inputs[:4000]\n        return await llm_call(truncated_inputs, max_tokens=4000)\n\n\ndataset = Dataset(cases=[Case(inputs='test')], evaluators=[])\n\n# Don't retry context length errors (handle in task)\nreport = dataset.evaluate_sync(\n    task=smart_llm_task,\n    retry_task={'stop': stop_after_attempt(1)},  # No retries, we handle it\n)\n```", "url": "https://ai.pydantic.dev/how-to/retry-strategies/index.html#context-length-handling", "page": "how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Retry vs Error Handling", "anchor": "retry-vs-error-handling", "md_text": "**Use retries for:**\n- Transient failures (rate limits, timeouts)\n- Network issues\n- Temporary service outages\n- Recoverable errors\n\n**Use error handling for:**\n- Validation errors\n- Logic errors\n- Permanent failures\n- Expected error conditions\n\n```\nclass RateLimitError(Exception):\n    pass\n\n\nasync def llm_call(inputs: str) -> str:\n    return f'LLM response: {inputs}'\n\n\ndef is_valid(result: str) -> bool:\n    return len(result) > 0\n\n\nasync def smart_task(inputs: str) -> str:\n    \"\"\"Handle expected errors, let retries handle transient failures.\"\"\"\n    try:\n        result = await llm_call(inputs)\n\n        # Validate output (don't retry validation errors)\n        if not is_valid(result):\n            return 'ERROR: Invalid output format'\n\n        return result\n\n    except RateLimitError:\n        # Let retry logic handle this\n        raise\n\n    except ValueError as e:\n        # Don't retry - this is a permanent error\n        return f'ERROR: {e}'\n```", "url": "https://ai.pydantic.dev/how-to/retry-strategies/index.html#retry-vs-error-handling", "page": "how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Troubleshooting", "anchor": "troubleshooting", "md_text": "", "url": "https://ai.pydantic.dev/how-to/retry-strategies/index.html#troubleshooting", "page": "how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "\"Still failing after retries\"", "anchor": "still-failing-after-retries", "md_text": "Increase retry attempts or check if error is retriable:\n\n```\nimport logging\n\nfrom tenacity import stop_after_attempt\n\nfrom pydantic_evals import Case, Dataset\n\n\ndef task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\n# Add logging to see what's failing\nlogging.basicConfig(level=logging.DEBUG)\n\ndataset = Dataset(cases=[Case(inputs='test')], evaluators=[])\n\n# Tenacity logs retry attempts\nreport = dataset.evaluate_sync(task, retry_task={'stop': stop_after_attempt(5)})\n```", "url": "https://ai.pydantic.dev/how-to/retry-strategies/index.html#still-failing-after-retries", "page": "how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "\"Evaluations taking too long\"", "anchor": "evaluations-taking-too-long", "md_text": "Reduce retry attempts or wait times:\n\n```\nfrom tenacity import stop_after_attempt, wait_exponential\n\n# Faster retries\nretry_config = {\n    'stop': stop_after_attempt(3),  # Fewer attempts\n    'wait': wait_exponential(multiplier=0.1, min=0.1, max=2),  # Quick retries, capped at 2s\n    'reraise': True,\n}\n```", "url": "https://ai.pydantic.dev/how-to/retry-strategies/index.html#evaluations-taking-too-long", "page": "how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "\"Hitting rate limits despite retries\"", "anchor": "hitting-rate-limits-despite-retries", "md_text": "Increase delays or use `max_concurrency`:\n\n```\nfrom tenacity import stop_after_attempt, wait_exponential\n\nfrom pydantic_evals import Case, Dataset\n\n\ndef task(inputs: str) -> str:\n    return f'Result: {inputs}'\n\n\ndataset = Dataset(cases=[Case(inputs='test')], evaluators=[])\n\n# Longer delays\nretry_config = {\n    'stop': stop_after_attempt(5),\n    'wait': wait_exponential(multiplier=5, min=5, max=60),  # Start at 5s, exponential up to 60s\n    'reraise': True,\n}\n\n# Also reduce concurrency\nreport = dataset.evaluate_sync(\n    task=task,\n    retry_task=retry_config,\n    max_concurrency=2,  # Only 2 concurrent tasks\n)\n```", "url": "https://ai.pydantic.dev/how-to/retry-strategies/index.html#hitting-rate-limits-despite-retries", "page": "how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "md_text": "* **[Concurrency & Performance](../concurrency/index.html)** - Optimize evaluation performance\n* **[Logfire Integration](../logfire-integration/index.html)** - View retries in Logfire", "url": "https://ai.pydantic.dev/how-to/retry-strategies/index.html#next-steps", "page": "how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "md_text": "* **Broadcasting** - Send the same data to multiple parallel paths\n* **Spreading** - Fan out items from an iterable to parallel paths\n\nBoth create \"forks\" in the execution graph that can later be synchronized with [join nodes](../joins/index.html).", "url": "https://ai.pydantic.dev/beta/parallel/index.html#overview", "page": "beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "Broadcasting", "anchor": "broadcasting", "md_text": "Broadcasting sends identical data to multiple destinations simultaneously:\n\nbasic\\_broadcast.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=list[int])\n\n    @g.step\n    async def source(ctx: StepContext[SimpleState, None, None]) -> int:\n        return 10\n\n    @g.step\n    async def add_one(ctx: StepContext[SimpleState, None, int]) -> int:\n        return ctx.inputs + 1\n\n    @g.step\n    async def add_two(ctx: StepContext[SimpleState, None, int]) -> int:\n        return ctx.inputs + 2\n\n    @g.step\n    async def add_three(ctx: StepContext[SimpleState, None, int]) -> int:\n        return ctx.inputs + 3\n\n    collect = g.join(reduce_list_append, initial_factory=list[int])\n\n    # Broadcasting: send the value from source to all three steps\n    g.add(\n        g.edge_from(g.start_node).to(source),\n        g.edge_from(source).to(add_one, add_two, add_three),\n        g.edge_from(add_one, add_two, add_three).to(collect),\n        g.edge_from(collect).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(sorted(result))\n    #> [11, 12, 13]\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*\n\nAll three steps receive the same input value (`10`) and execute in parallel.", "url": "https://ai.pydantic.dev/beta/parallel/index.html#broadcasting", "page": "beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "Spreading", "anchor": "spreading", "md_text": "Spreading fans out elements from an iterable, processing each element in parallel:\n\nbasic\\_map.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=list[int])\n\n    @g.step\n    async def generate_list(ctx: StepContext[SimpleState, None, None]) -> list[int]:\n        return [1, 2, 3, 4, 5]\n\n    @g.step\n    async def square(ctx: StepContext[SimpleState, None, int]) -> int:\n        return ctx.inputs * ctx.inputs\n\n    collect = g.join(reduce_list_append, initial_factory=list[int])\n\n    # Spreading: each item in the list gets its own parallel execution\n    g.add(\n        g.edge_from(g.start_node).to(generate_list),\n        g.edge_from(generate_list).map().to(square),\n        g.edge_from(square).to(collect),\n        g.edge_from(collect).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(sorted(result))\n    #> [1, 4, 9, 16, 25]\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/parallel/index.html#spreading", "page": "beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "Spreading AsyncIterables", "anchor": "spreading-asynciterables", "md_text": "The `.map()` operation also works with `AsyncIterable` values. When mapping over an async iterable, the graph creates parallel tasks dynamically as values are yielded. This is particularly useful for streaming data or processing data that's being generated on-the-fly:\n\nasync\\_iterable\\_map.py\n\n```\nimport asyncio\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=list[int])\n\n    @g.stream\n    async def stream_numbers(ctx: StepContext[SimpleState, None, None]):\n        \"\"\"Stream numbers with delays to simulate real-time data.\"\"\"\n        for i in range(1, 4):\n            await asyncio.sleep(0.05)  # Simulate delay\n            yield i\n\n    @g.step\n    async def triple(ctx: StepContext[SimpleState, None, int]) -> int:\n        return ctx.inputs * 3\n\n    collect = g.join(reduce_list_append, initial_factory=list[int])\n\n    g.add(\n        g.edge_from(g.start_node).to(stream_numbers),\n        # Map over the async iterable - tasks created as items are yielded\n        g.edge_from(stream_numbers).map().to(triple),\n        g.edge_from(triple).to(collect),\n        g.edge_from(collect).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(sorted(result))\n    #> [3, 6, 9]\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*\n\nThis allows for progressive processing where downstream steps can start working on early results while later results are still being generated.", "url": "https://ai.pydantic.dev/beta/parallel/index.html#spreading-asynciterables", "page": "beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "Using `add_mapping_edge()`", "anchor": "using-addmappingedge", "md_text": "The convenience method [`add_mapping_edge()`](../../pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.add_mapping_edge) provides a simpler syntax:\n\nmapping\\_convenience.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=list[str])\n\n    @g.step\n    async def generate_numbers(ctx: StepContext[SimpleState, None, None]) -> list[int]:\n        return [10, 20, 30]\n\n    @g.step\n    async def stringify(ctx: StepContext[SimpleState, None, int]) -> str:\n        return f'Value: {ctx.inputs}'\n\n    collect = g.join(reduce_list_append, initial_factory=list[str])\n\n    g.add(g.edge_from(g.start_node).to(generate_numbers))\n    g.add_mapping_edge(generate_numbers, stringify)\n    g.add(\n        g.edge_from(stringify).to(collect),\n        g.edge_from(collect).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(sorted(result))\n    #> ['Value: 10', 'Value: 20', 'Value: 30']\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/parallel/index.html#using-addmappingedge", "page": "beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "Empty Iterables", "anchor": "empty-iterables", "md_text": "When mapping an empty iterable, you can specify a `downstream_join_id` to ensure the join still executes:\n\nempty\\_map.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=list[int])\n\n    @g.step\n    async def generate_empty(ctx: StepContext[SimpleState, None, None]) -> list[int]:\n        return []\n\n    @g.step\n    async def double(ctx: StepContext[SimpleState, None, int]) -> int:\n        return ctx.inputs * 2\n\n    collect = g.join(reduce_list_append, initial_factory=list[int])\n\n    g.add(g.edge_from(g.start_node).to(generate_empty))\n    g.add_mapping_edge(generate_empty, double, downstream_join_id=collect.id)\n    g.add(\n        g.edge_from(double).to(collect),\n        g.edge_from(collect).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(result)\n    #> []\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/parallel/index.html#empty-iterables", "page": "beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "Nested Parallel Operations", "anchor": "nested-parallel-operations", "md_text": "You can nest broadcasts and maps for complex parallel patterns:", "url": "https://ai.pydantic.dev/beta/parallel/index.html#nested-parallel-operations", "page": "beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "Spread then Broadcast", "anchor": "spread-then-broadcast", "md_text": "map\\_then\\_broadcast.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=list[int])\n\n    @g.step\n    async def generate_list(ctx: StepContext[SimpleState, None, None]) -> list[int]:\n        return [10, 20]\n\n    @g.step\n    async def add_one(ctx: StepContext[SimpleState, None, int]) -> int:\n        return ctx.inputs + 1\n\n    @g.step\n    async def add_two(ctx: StepContext[SimpleState, None, int]) -> int:\n        return ctx.inputs + 2\n\n    collect = g.join(reduce_list_append, initial_factory=list[int])\n\n    g.add(\n        g.edge_from(g.start_node).to(generate_list),\n        # Spread the list, then broadcast each item to both steps\n        g.edge_from(generate_list).map().to(add_one, add_two),\n        g.edge_from(add_one, add_two).to(collect),\n        g.edge_from(collect).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(sorted(result))\n    #> [11, 12, 21, 22]\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*\n\nThe result contains:\n- From 10: `10+1=11` and `10+2=12`\n- From 20: `20+1=21` and `20+2=22`", "url": "https://ai.pydantic.dev/beta/parallel/index.html#spread-then-broadcast", "page": "beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "Multiple Sequential Spreads", "anchor": "multiple-sequential-spreads", "md_text": "sequential\\_maps.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=list[str])\n\n    @g.step\n    async def generate_pairs(ctx: StepContext[SimpleState, None, None]) -> list[tuple[int, int]]:\n        return [(1, 2), (3, 4)]\n\n    @g.step\n    async def unpack_pair(ctx: StepContext[SimpleState, None, tuple[int, int]]) -> list[int]:\n        return [ctx.inputs[0], ctx.inputs[1]]\n\n    @g.step\n    async def stringify(ctx: StepContext[SimpleState, None, int]) -> str:\n        return f'num:{ctx.inputs}'\n\n    collect = g.join(reduce_list_append, initial_factory=list[str])\n\n    g.add(\n        g.edge_from(g.start_node).to(generate_pairs),\n        # First map: one task per tuple\n        g.edge_from(generate_pairs).map().to(unpack_pair),\n        # Second map: one task per number in each tuple\n        g.edge_from(unpack_pair).map().to(stringify),\n        g.edge_from(stringify).to(collect),\n        g.edge_from(collect).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(sorted(result))\n    #> ['num:1', 'num:2', 'num:3', 'num:4']\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/parallel/index.html#multiple-sequential-spreads", "page": "beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "Edge Labels", "anchor": "edge-labels", "md_text": "Add labels to parallel edges for better documentation:\n\nlabeled\\_parallel.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=list[str])\n\n    @g.step\n    async def generate(ctx: StepContext[SimpleState, None, None]) -> list[int]:\n        return [1, 2, 3]\n\n    @g.step\n    async def process(ctx: StepContext[SimpleState, None, int]) -> str:\n        return f'item-{ctx.inputs}'\n\n    collect = g.join(reduce_list_append, initial_factory=list[str])\n\n    g.add(g.edge_from(g.start_node).to(generate))\n    g.add_mapping_edge(\n        generate,\n        process,\n        pre_map_label='before map',\n        post_map_label='after map',\n    )\n    g.add(\n        g.edge_from(process).to(collect),\n        g.edge_from(collect).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(sorted(result))\n    #> ['item-1', 'item-2', 'item-3']\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/parallel/index.html#edge-labels", "page": "beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "State Sharing in Parallel Execution", "anchor": "state-sharing-in-parallel-execution", "md_text": "All parallel tasks share the same graph state. Be careful with mutations:\n\nparallel\\_state.py\n\n```\nfrom dataclasses import dataclass, field\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass CounterState:\n    values: list[int] = field(default_factory=list)\n\n\nasync def main():\n    g = GraphBuilder(state_type=CounterState, output_type=list[int])\n\n    @g.step\n    async def generate(ctx: StepContext[CounterState, None, None]) -> list[int]:\n        return [1, 2, 3]\n\n    @g.step\n    async def track_and_square(ctx: StepContext[CounterState, None, int]) -> int:\n        # All parallel tasks mutate the same state\n        ctx.state.values.append(ctx.inputs)\n        return ctx.inputs * ctx.inputs\n\n    collect = g.join(reduce_list_append, initial_factory=list[int])\n\n    g.add(\n        g.edge_from(g.start_node).to(generate),\n        g.edge_from(generate).map().to(track_and_square),\n        g.edge_from(track_and_square).to(collect),\n        g.edge_from(collect).to(g.end_node),\n    )\n\n    graph = g.build()\n    state = CounterState()\n    result = await graph.run(state=state)\n\n    print(f'Squared: {sorted(result)}')\n    #> Squared: [1, 4, 9]\n    print(f'Tracked: {sorted(state.values)}')\n    #> Tracked: [1, 2, 3]\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/parallel/index.html#state-sharing-in-parallel-execution", "page": "beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "Edge Transformations", "anchor": "edge-transformations", "md_text": "You can transform data inline as it flows along edges using the `.transform()` method:\n\nedge\\_transform.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=str)\n\n    @g.step\n    async def generate_number(ctx: StepContext[SimpleState, None, None]) -> int:\n        return 42\n\n    @g.step\n    async def format_output(ctx: StepContext[SimpleState, None, str]) -> str:\n        return f'The answer is: {ctx.inputs}'\n\n    # Transform the number to a string inline\n    g.add(\n        g.edge_from(g.start_node).to(generate_number),\n        g.edge_from(generate_number).transform(lambda ctx: str(ctx.inputs * 2)).to(format_output),\n        g.edge_from(format_output).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(result)\n    #> The answer is: 84\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*\n\nThe transform function receives a [`StepContext`](../../pydantic_graph/beta_step/index.html#pydantic_graph.beta.step.StepContext) with the current inputs and has access to state and dependencies. This is useful for:\n\n* Converting data types between incompatible steps\n* Extracting specific fields from complex objects\n* Applying simple computations without creating a full step\n* Adapting data formats during routing\n\nTransforms can be chained and combined with other edge operations like `.map()` and `.label()`:\n\nchained\\_transforms.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=list[str])\n\n    @g.step\n    async def generate_data(ctx: StepContext[SimpleState, None, None]) -> list[dict[str, int]]:\n        return [{'value': 10}, {'value': 20}, {'value': 30}]\n\n    @g.step\n    async def process_number(ctx: StepContext[SimpleState, None, int]) -> str:\n        return f'Processed: {ctx.inputs}'\n\n    collect = g.join(reduce_list_append, initial_factory=list[str])\n\n    g.add(\n        g.edge_from(g.start_node).to(generate_data),\n        # Transform to extract values, then map over them\n        g.edge_from(generate_data)\n        .transform(lambda ctx: [item['value'] for item in ctx.inputs])\n        .label('Extract values')\n        .map()\n        .to(process_number),\n        g.edge_from(process_number).to(collect),\n        g.edge_from(collect).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(sorted(result))\n    #> ['Processed: 10', 'Processed: 20', 'Processed: 30']\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/parallel/index.html#edge-transformations", "page": "beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "md_text": "* Learn about [join nodes](../joins/index.html) for aggregating parallel results\n* Explore [conditional branching](../decisions/index.html) with decision nodes\n* See the [steps documentation](../steps/index.html) for more on step execution", "url": "https://ai.pydantic.dev/beta/parallel/index.html#next-steps", "page": "beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "Creating Steps", "anchor": "creating-steps", "md_text": "Steps are created using the [`@g.step`](../../pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.step) decorator on the [`GraphBuilder`](../../pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder):\n\nbasic\\_step.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\n@dataclass\nclass MyState:\n    counter: int = 0\n\ng = GraphBuilder(state_type=MyState, output_type=int)\n\n@g.step\nasync def increment(ctx: StepContext[MyState, None, None]) -> int:\n    ctx.state.counter += 1\n    return ctx.state.counter\n\ng.add(\n    g.edge_from(g.start_node).to(increment),\n    g.edge_from(increment).to(g.end_node),\n)\n\ngraph = g.build()\n\nasync def main():\n    state = MyState()\n    result = await graph.run(state=state)\n    print(result)\n    #> 1\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/steps/index.html#creating-steps", "page": "beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Step Context", "anchor": "step-context", "md_text": "Every step function receives a [`StepContext`](../../pydantic_graph/beta_step/index.html#pydantic_graph.beta.step.StepContext) as its first parameter. The context provides access to:\n\n* `ctx.state` - The mutable graph state (type: `StateT`)\n* `ctx.deps` - Injected dependencies (type: `DepsT`)\n* `ctx.inputs` - Input data for this step (type: `InputT`)", "url": "https://ai.pydantic.dev/beta/steps/index.html#step-context", "page": "beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Accessing State", "anchor": "accessing-state", "md_text": "State is shared across all steps in a graph and can be freely mutated:\n\nstate\\_access.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\n@dataclass\nclass AppState:\n    messages: list[str]\n\n\nasync def main():\n    g = GraphBuilder(state_type=AppState, output_type=list[str])\n\n    @g.step\n    async def add_hello(ctx: StepContext[AppState, None, None]) -> None:\n        ctx.state.messages.append('Hello')\n\n    @g.step\n    async def add_world(ctx: StepContext[AppState, None, None]) -> None:\n        ctx.state.messages.append('World')\n\n    @g.step\n    async def get_messages(ctx: StepContext[AppState, None, None]) -> list[str]:\n        return ctx.state.messages\n\n    g.add(\n        g.edge_from(g.start_node).to(add_hello),\n        g.edge_from(add_hello).to(add_world),\n        g.edge_from(add_world).to(get_messages),\n        g.edge_from(get_messages).to(g.end_node),\n    )\n\n    graph = g.build()\n    state = AppState(messages=[])\n    result = await graph.run(state=state)\n    print(result)\n    #> ['Hello', 'World']\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/steps/index.html#accessing-state", "page": "beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Working with Inputs", "anchor": "working-with-inputs", "md_text": "Steps can receive and transform input data:\n\nstep\\_inputs.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(\n        state_type=SimpleState,\n        input_type=int,\n        output_type=str,\n    )\n\n    @g.step\n    async def double_it(ctx: StepContext[SimpleState, None, int]) -> int:\n        \"\"\"Double the input value.\"\"\"\n        return ctx.inputs * 2\n\n    @g.step\n    async def stringify(ctx: StepContext[SimpleState, None, int]) -> str:\n        \"\"\"Convert to a formatted string.\"\"\"\n        return f'Result: {ctx.inputs}'\n\n    g.add(\n        g.edge_from(g.start_node).to(double_it),\n        g.edge_from(double_it).to(stringify),\n        g.edge_from(stringify).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState(), inputs=21)\n    print(result)\n    #> Result: 42\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/steps/index.html#working-with-inputs", "page": "beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Dependency Injection", "anchor": "dependency-injection", "md_text": "Steps can access injected dependencies through `ctx.deps`:\n\ndependencies.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\n@dataclass\nclass AppState:\n    pass\n\n\n@dataclass\nclass AppDeps:\n    \"\"\"Dependencies injected into the graph.\"\"\"\n\n    multiplier: int\n\n\nasync def main():\n    g = GraphBuilder(\n        state_type=AppState,\n        deps_type=AppDeps,\n        input_type=int,\n        output_type=int,\n    )\n\n    @g.step\n    async def multiply(ctx: StepContext[AppState, AppDeps, int]) -> int:\n        \"\"\"Multiply input by the injected multiplier.\"\"\"\n        return ctx.inputs * ctx.deps.multiplier\n\n    g.add(\n        g.edge_from(g.start_node).to(multiply),\n        g.edge_from(multiply).to(g.end_node),\n    )\n\n    graph = g.build()\n    deps = AppDeps(multiplier=10)\n    result = await graph.run(state=AppState(), deps=deps, inputs=5)\n    print(result)\n    #> 50\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/steps/index.html#dependency-injection", "page": "beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Customizing Steps", "anchor": "customizing-steps", "md_text": "", "url": "https://ai.pydantic.dev/beta/steps/index.html#customizing-steps", "page": "beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Node IDs", "anchor": "custom-node-ids", "md_text": "By default, step node IDs are inferred from the function name. You can override this:\n\ncustom\\_id.py\n\n```\nfrom pydantic_graph.beta import StepContext\n\nfrom basic_step import MyState, g\n\n\n@g.step(node_id='my_custom_id')\nasync def my_step(ctx: StepContext[MyState, None, None]) -> int:\n    return 42\n\n# The node ID is now 'my_custom_id' instead of 'my_step'\n```", "url": "https://ai.pydantic.dev/beta/steps/index.html#custom-node-ids", "page": "beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Human-Readable Labels", "anchor": "human-readable-labels", "md_text": "Labels provide documentation for diagram generation:\n\nlabels.py\n\n```\nfrom pydantic_graph.beta import StepContext\n\nfrom basic_step import MyState, g\n\n\n@g.step(label='Increment the counter')\nasync def increment(ctx: StepContext[MyState, None, None]) -> int:\n    ctx.state.counter += 1\n    return ctx.state.counter\n\n# Access the label programmatically\nprint(increment.label)\n#> Increment the counter\n```", "url": "https://ai.pydantic.dev/beta/steps/index.html#human-readable-labels", "page": "beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Sequential Steps", "anchor": "sequential-steps", "md_text": "Multiple steps can be chained sequentially:\n\nsequential.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\n@dataclass\nclass MathState:\n    operations: list[str]\n\n\nasync def main():\n    g = GraphBuilder(\n        state_type=MathState,\n        input_type=int,\n        output_type=int,\n    )\n\n    @g.step\n    async def add_five(ctx: StepContext[MathState, None, int]) -> int:\n        ctx.state.operations.append('add 5')\n        return ctx.inputs + 5\n\n    @g.step\n    async def multiply_by_two(ctx: StepContext[MathState, None, int]) -> int:\n        ctx.state.operations.append('multiply by 2')\n        return ctx.inputs * 2\n\n    @g.step\n    async def subtract_three(ctx: StepContext[MathState, None, int]) -> int:\n        ctx.state.operations.append('subtract 3')\n        return ctx.inputs - 3\n\n    # Connect steps sequentially\n    g.add(\n        g.edge_from(g.start_node).to(add_five),\n        g.edge_from(add_five).to(multiply_by_two),\n        g.edge_from(multiply_by_two).to(subtract_three),\n        g.edge_from(subtract_three).to(g.end_node),\n    )\n\n    graph = g.build()\n    state = MathState(operations=[])\n    result = await graph.run(state=state, inputs=10)\n\n    print(f'Result: {result}')\n    #> Result: 27\n    print(f'Operations: {state.operations}')\n    #> Operations: ['add 5', 'multiply by 2', 'subtract 3']\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*\n\nThe computation is: `(10 + 5) * 2 - 3 = 27`", "url": "https://ai.pydantic.dev/beta/steps/index.html#sequential-steps", "page": "beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Streaming Steps", "anchor": "streaming-steps", "md_text": "In addition to regular steps that return a single value, you can create streaming steps that yield multiple values over time using the [`@g.stream`](../../pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.stream) decorator:\n\nstreaming\\_step.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\ng = GraphBuilder(state_type=SimpleState, output_type=list[int])\n\n@g.stream\nasync def generate_stream(ctx: StepContext[SimpleState, None, None]):\n    \"\"\"Stream numbers from 1 to 5.\"\"\"\n    for i in range(1, 6):\n        yield i\n\n@g.step\nasync def square(ctx: StepContext[SimpleState, None, int]) -> int:\n    return ctx.inputs * ctx.inputs\n\ncollect = g.join(reduce_list_append, initial_factory=list[int])\n\ng.add(\n    g.edge_from(g.start_node).to(generate_stream),\n    # The stream output is an AsyncIterable, so we can map over it\n    g.edge_from(generate_stream).map().to(square),\n    g.edge_from(square).to(collect),\n    g.edge_from(collect).to(g.end_node),\n)\n\ngraph = g.build()\n\nasync def main():\n    result = await graph.run(state=SimpleState())\n    print(sorted(result))\n    #> [1, 4, 9, 16, 25]\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/steps/index.html#streaming-steps", "page": "beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "How Streaming Steps Work", "anchor": "how-streaming-steps-work", "md_text": "Streaming steps return an `AsyncIterable` that yields values over time. When you use `.map()` on a streaming step's output, the graph processes each yielded value as it becomes available, creating parallel tasks dynamically. This is particularly useful for:\n\n* Processing data from APIs that stream responses\n* Handling real-time data feeds\n* Progressive processing of large datasets\n* Any scenario where you want to start processing results before all data is available\n\nLike regular steps, streaming steps can also have custom node IDs and labels:\n\nlabeled\\_stream.py\n\n```\nfrom pydantic_graph.beta import StepContext\n\nfrom streaming_step import SimpleState, g\n\n\n@g.stream(node_id='my_stream', label='Generate numbers progressively')\nasync def labeled_stream(ctx: StepContext[SimpleState, None, None]):\n    for i in range(10):\n        yield i\n```", "url": "https://ai.pydantic.dev/beta/steps/index.html#how-streaming-steps-work", "page": "beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Edge Building Convenience Methods", "anchor": "edge-building-convenience-methods", "md_text": "The builder provides helper methods for common edge patterns:", "url": "https://ai.pydantic.dev/beta/steps/index.html#edge-building-convenience-methods", "page": "beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Simple Edges with `add_edge()`", "anchor": "simple-edges-with-addedge", "md_text": "add\\_edge\\_example.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=int)\n\n    @g.step\n    async def step_a(ctx: StepContext[SimpleState, None, None]) -> int:\n        return 10\n\n    @g.step\n    async def step_b(ctx: StepContext[SimpleState, None, int]) -> int:\n        return ctx.inputs + 5\n\n    # Using add_edge() for simple connections\n    g.add_edge(g.start_node, step_a)\n    g.add_edge(step_a, step_b, label='from a to b')\n    g.add_edge(step_b, g.end_node)\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(result)\n    #> 15\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/steps/index.html#simple-edges-with-addedge", "page": "beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Type Safety", "anchor": "type-safety", "md_text": "The beta graph API provides strong type checking through generics. Type parameters on [`StepContext`](../../pydantic_graph/beta_step/index.html#pydantic_graph.beta.step.StepContext) ensure:\n\n* State access is properly typed\n* Dependencies are correctly typed\n* Input/output types match across edges\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\n@dataclass\nclass MyState:\n    pass\n\ng = GraphBuilder(state_type=MyState, output_type=str)\n\n# Type checker will catch mismatches\n@g.step\nasync def expects_int(ctx: StepContext[MyState, None, int]) -> str:\n    return str(ctx.inputs)\n\n@g.step\nasync def returns_str(ctx: StepContext[MyState, None, None]) -> str:\n    return 'hello'\n\n# This would be a type error - expects_int needs int input, but returns_str outputs str\n# g.add(g.edge_from(returns_str).to(expects_int))  # Type error!\n```", "url": "https://ai.pydantic.dev/beta/steps/index.html#type-safety", "page": "beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "md_text": "* Learn about [parallel execution](../parallel/index.html) with broadcasting and mapping\n* Understand [join nodes](../joins/index.html) for aggregating parallel results\n* Explore [conditional branching](../decisions/index.html) with decision nodes", "url": "https://ai.pydantic.dev/beta/steps/index.html#next-steps", "page": "beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "md_text": "When you use [parallel execution](../parallel/index.html) (broadcasting or mapping), you often need to collect and combine the results. Join nodes serve this purpose by:\n\n1. Waiting for all parallel tasks to complete\n2. Aggregating their outputs using a [`ReducerFunction`](../../pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.ReducerFunction)\n3. Passing the aggregated result to the next node", "url": "https://ai.pydantic.dev/beta/joins/index.html#overview", "page": "beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "Creating Joins", "anchor": "creating-joins", "md_text": "Create a join using `GraphBuilder.join` with a reducer function and initial value or factory:\n\nbasic\\_join.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\ng = GraphBuilder(state_type=SimpleState, output_type=list[int])\n\n@g.step\nasync def generate_numbers(ctx: StepContext[SimpleState, None, None]) -> list[int]:\n    return [1, 2, 3, 4, 5]\n\n@g.step\nasync def square(ctx: StepContext[SimpleState, None, int]) -> int:\n    return ctx.inputs * ctx.inputs\n\n# Create a join to collect all squared values\ncollect = g.join(reduce_list_append, initial_factory=list[int])\n\ng.add(\n    g.edge_from(g.start_node).to(generate_numbers),\n    g.edge_from(generate_numbers).map().to(square),\n    g.edge_from(square).to(collect),\n    g.edge_from(collect).to(g.end_node),\n)\n\ngraph = g.build()\n\nasync def main():\n    result = await graph.run(state=SimpleState())\n    print(sorted(result))\n    #> [1, 4, 9, 16, 25]\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/joins/index.html#creating-joins", "page": "beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "Built-in Reducers", "anchor": "built-in-reducers", "md_text": "Pydantic Graph provides several common reducer types out of the box:", "url": "https://ai.pydantic.dev/beta/joins/index.html#built-in-reducers", "page": "beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "`reduce_list_append`", "anchor": "reducelistappend", "md_text": "[`reduce_list_append`](../../pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_list_append) collects all inputs into a list:\n\nlist\\_reducer.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=list[str])\n\n    @g.step\n    async def generate(ctx: StepContext[SimpleState, None, None]) -> list[int]:\n        return [10, 20, 30]\n\n    @g.step\n    async def to_string(ctx: StepContext[SimpleState, None, int]) -> str:\n        return f'value-{ctx.inputs}'\n\n    collect = g.join(reduce_list_append, initial_factory=list[str])\n\n    g.add(\n        g.edge_from(g.start_node).to(generate),\n        g.edge_from(generate).map().to(to_string),\n        g.edge_from(to_string).to(collect),\n        g.edge_from(collect).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(sorted(result))\n    #> ['value-10', 'value-20', 'value-30']\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/joins/index.html#reducelistappend", "page": "beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "`reduce_list_extend`", "anchor": "reducelistextend", "md_text": "[`reduce_list_extend`](../../pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_list_extend) extends a list with an iterable of items:\n\nlist\\_extend\\_reducer.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_extend\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=list[int])\n\n    @g.step\n    async def generate(ctx: StepContext[SimpleState, None, None]) -> list[int]:\n        return [1, 2, 3]\n\n    @g.step\n    async def create_range(ctx: StepContext[SimpleState, None, int]) -> list[int]:\n        \"\"\"Create a range from 0 to the input value.\"\"\"\n        return list(range(ctx.inputs))\n\n    collect = g.join(reduce_list_extend, initial_factory=list[int])\n\n    g.add(\n        g.edge_from(g.start_node).to(generate),\n        g.edge_from(generate).map().to(create_range),\n        g.edge_from(create_range).to(collect),\n        g.edge_from(collect).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(sorted(result))\n    #> [0, 0, 0, 1, 1, 2]\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/joins/index.html#reducelistextend", "page": "beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "`reduce_dict_update`", "anchor": "reducedictupdate", "md_text": "[`reduce_dict_update`](../../pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_dict_update) merges dictionaries together:\n\ndict\\_reducer.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_dict_update\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=dict[str, int])\n\n    @g.step\n    async def generate_keys(ctx: StepContext[SimpleState, None, None]) -> list[str]:\n        return ['apple', 'banana', 'cherry']\n\n    @g.step\n    async def create_entry(ctx: StepContext[SimpleState, None, str]) -> dict[str, int]:\n        return {ctx.inputs: len(ctx.inputs)}\n\n    merge = g.join(reduce_dict_update, initial_factory=dict[str, int])\n\n    g.add(\n        g.edge_from(g.start_node).to(generate_keys),\n        g.edge_from(generate_keys).map().to(create_entry),\n        g.edge_from(create_entry).to(merge),\n        g.edge_from(merge).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    result = {k: result[k] for k in sorted(result)}  # force deterministic ordering\n    print(result)\n    #> {'apple': 5, 'banana': 6, 'cherry': 6}\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/joins/index.html#reducedictupdate", "page": "beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "`reduce_null`", "anchor": "reducenull", "md_text": "[`reduce_null`](../../pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_null) discards all inputs and returns `None`. Useful when you only care about side effects:\n\nnull\\_reducer.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_null\n\n\n@dataclass\nclass CounterState:\n    total: int = 0\n\n\nasync def main():\n    g = GraphBuilder(state_type=CounterState, output_type=int)\n\n    @g.step\n    async def generate(ctx: StepContext[CounterState, None, None]) -> list[int]:\n        return [1, 2, 3, 4, 5]\n\n    @g.step\n    async def accumulate(ctx: StepContext[CounterState, None, int]) -> int:\n        ctx.state.total += ctx.inputs\n        return ctx.inputs\n\n    # We don't care about the outputs, only the side effect on state\n    ignore = g.join(reduce_null, initial=None)\n\n    @g.step\n    async def get_total(ctx: StepContext[CounterState, None, None]) -> int:\n        return ctx.state.total\n\n    g.add(\n        g.edge_from(g.start_node).to(generate),\n        g.edge_from(generate).map().to(accumulate),\n        g.edge_from(accumulate).to(ignore),\n        g.edge_from(ignore).to(get_total),\n        g.edge_from(get_total).to(g.end_node),\n    )\n\n    graph = g.build()\n    state = CounterState()\n    result = await graph.run(state=state)\n    print(result)\n    #> 15\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/joins/index.html#reducenull", "page": "beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "`reduce_sum`", "anchor": "reducesum", "md_text": "[`reduce_sum`](../../pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_sum) sums numeric values:\n\nsum\\_reducer.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_sum\n\n\n@dataclass\nclass SimpleState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=int)\n\n    @g.step\n    async def generate(ctx: StepContext[SimpleState, None, None]) -> list[int]:\n        return [10, 20, 30, 40]\n\n    @g.step\n    async def identity(ctx: StepContext[SimpleState, None, int]) -> int:\n        return ctx.inputs\n\n    sum_join = g.join(reduce_sum, initial=0)\n\n    g.add(\n        g.edge_from(g.start_node).to(generate),\n        g.edge_from(generate).map().to(identity),\n        g.edge_from(identity).to(sum_join),\n        g.edge_from(sum_join).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=SimpleState())\n    print(result)\n    #> 100\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/joins/index.html#reducesum", "page": "beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "`ReduceFirstValue`", "anchor": "reducefirstvalue", "md_text": "[`ReduceFirstValue`](../../pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.ReduceFirstValue) returns the first value it receives and cancels all other parallel tasks. This is useful for \"race\" scenarios where you want the first successful result:\n\nfirst\\_value\\_reducer.py\n\n```\nimport asyncio\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import ReduceFirstValue\n\n\n@dataclass\nclass SimpleState:\n    tasks_completed: int = 0\n\n\nasync def main():\n    g = GraphBuilder(state_type=SimpleState, output_type=str)\n\n    @g.step\n    async def generate(ctx: StepContext[SimpleState, None, None]) -> list[int]:\n        return [1, 12, 13, 14, 15]\n\n    @g.step\n    async def slow_process(ctx: StepContext[SimpleState, None, int]) -> str:\n        \"\"\"Simulate variable processing times.\"\"\"\n        # Simulate different delays\n        await asyncio.sleep(ctx.inputs * 0.1)\n        ctx.state.tasks_completed += 1\n        return f'Result from task {ctx.inputs}'\n\n    # Use ReduceFirstValue to get the first result and cancel the rest\n    first_result = g.join(ReduceFirstValue[str](), initial=None, node_id='first_result')\n\n    g.add(\n        g.edge_from(g.start_node).to(generate),\n        g.edge_from(generate).map().to(slow_process),\n        g.edge_from(slow_process).to(first_result),\n        g.edge_from(first_result).to(g.end_node),\n    )\n\n    graph = g.build()\n    state = SimpleState()\n    result = await graph.run(state=state)\n\n    print(result)\n    #> Result from task 1\n    print(f'Tasks completed: {state.tasks_completed}')\n    #> Tasks completed: 1\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/joins/index.html#reducefirstvalue", "page": "beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Reducers", "anchor": "custom-reducers", "md_text": "Create custom reducers by defining a [`ReducerFunction`](../../pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.ReducerFunction):\n\ncustom\\_reducer.py\n\n```\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\ndef reduce_sum(current: int, inputs: int) -> int:\n    \"\"\"A reducer that sums numbers.\"\"\"\n    return current + inputs\n\n\nasync def main():\n    g = GraphBuilder(output_type=int)\n\n    @g.step\n    async def generate(ctx: StepContext[None, None, None]) -> list[int]:\n        return [5, 10, 15, 20]\n\n    @g.step\n    async def identity(ctx: StepContext[None, None, int]) -> int:\n        return ctx.inputs\n\n    sum_join = g.join(reduce_sum, initial=0)\n\n    g.add(\n        g.edge_from(g.start_node).to(generate),\n        g.edge_from(generate).map().to(identity),\n        g.edge_from(identity).to(sum_join),\n        g.edge_from(sum_join).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run()\n    print(result)\n    #> 50\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/joins/index.html#custom-reducers", "page": "beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "Reducers with State Access", "anchor": "reducers-with-state-access", "md_text": "Reducers can access and modify the graph state:\n\nstateful\\_reducer.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import ReducerContext\n\n\n@dataclass\nclass MetricsState:\n    total_count: int = 0\n    total_sum: int = 0\n\n\n@dataclass\nclass ReducedMetrics:\n    count: int = 0\n    sum: int = 0\n\n\ndef reduce_metrics_sum(ctx: ReducerContext[MetricsState, None], current: ReducedMetrics, inputs: int) -> ReducedMetrics:\n    ctx.state.total_count += 1\n    ctx.state.total_sum += inputs\n    return ReducedMetrics(count=current.count + 1, sum=current.sum + inputs)\n\ndef reduce_metrics_max(current: ReducedMetrics, inputs: ReducedMetrics) -> ReducedMetrics:\n    return ReducedMetrics(count=max(current.count, inputs.count), sum=max(current.sum, inputs.sum))\n\n\nasync def main():\n    g = GraphBuilder(state_type=MetricsState, output_type=dict[str, int])\n\n    @g.step\n    async def generate(ctx: StepContext[object, None, None]) -> list[int]:\n        return [1, 3, 5, 7, 9, 10, 20, 30, 40]\n\n    @g.step\n    async def process_even(ctx: StepContext[MetricsState, None, int]) -> int:\n        return ctx.inputs * 2\n\n    @g.step\n    async def process_odd(ctx: StepContext[MetricsState, None, int]) -> int:\n        return ctx.inputs * 3\n\n    metrics_even = g.join(reduce_metrics_sum, initial_factory=ReducedMetrics, node_id='metrics_even')\n    metrics_odd = g.join(reduce_metrics_sum, initial_factory=ReducedMetrics, node_id='metrics_odd')\n    metrics_max = g.join(reduce_metrics_max, initial_factory=ReducedMetrics, node_id='metrics_max')\n\n    g.add(\n        g.edge_from(g.start_node).to(generate),\n        # Send even and odd numbers to their respective `process` steps\n        g.edge_from(generate).map().to(\n            g.decision()\n            .branch(g.match(int, matches=lambda x: x % 2 == 0).label('even').to(process_even))\n            .branch(g.match(int, matches=lambda x: x % 2 == 1).label('odd').to(process_odd))\n        ),\n        # Reduce metrics for even and odd numbers separately\n        g.edge_from(process_even).to(metrics_even),\n        g.edge_from(process_odd).to(metrics_odd),\n        # Aggregate the max values for each field\n        g.edge_from(metrics_even).to(metrics_max),\n        g.edge_from(metrics_odd).to(metrics_max),\n        # Finish the graph run with the final reduced value\n        g.edge_from(metrics_max).to(g.end_node),\n    )\n\n    graph = g.build()\n    state = MetricsState()\n    result = await graph.run(state=state)\n\n    print(f'Result: {result}')\n    #> Result: ReducedMetrics(count=5, sum=200)\n    print(f'State total_count: {state.total_count}')\n    #> State total_count: 9\n    print(f'State total_sum: {state.total_sum}')\n    #> State total_sum: 275\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/joins/index.html#reducers-with-state-access", "page": "beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "Canceling Sibling Tasks", "anchor": "canceling-sibling-tasks", "md_text": "Reducers with access to [`ReducerContext`](../../pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.ReducerContext) can call [`ctx.cancel_sibling_tasks()`](../../pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.ReducerContext.cancel_sibling_tasks) to cancel all other parallel tasks in the same fork. This is useful for early termination when you've found what you need:\n\ncancel\\_siblings.py\n\n```\nimport asyncio\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import ReducerContext\n\n\n@dataclass\nclass SearchState:\n    searches_completed: int = 0\n\n\ndef reduce_find_match(ctx: ReducerContext[SearchState, None], current: str | None, inputs: str) -> str | None:\n    \"\"\"Return the first input that contains 'target' and cancel remaining tasks.\"\"\"\n    if current is not None:\n        # We already found a match, ignore subsequent inputs\n        return current\n    if 'target' in inputs:\n        # Found a match! Cancel all other parallel tasks\n        ctx.cancel_sibling_tasks()\n        return inputs\n    return None\n\n\nasync def main():\n    g = GraphBuilder(state_type=SearchState, output_type=str | None)\n\n    @g.step\n    async def generate_searches(ctx: StepContext[SearchState, None, None]) -> list[str]:\n        return ['item1', 'item2', 'target_item', 'item4', 'item5']\n\n    @g.step\n    async def search(ctx: StepContext[SearchState, None, str]) -> str:\n        \"\"\"Simulate a slow search operation.\"\"\"\n        # make the search artificially slower for 'item4' and 'item5'\n        search_duration = 0.1 if ctx.inputs not in {'item4', 'item5'} else 1.0\n        await asyncio.sleep(search_duration)\n        ctx.state.searches_completed += 1\n        return ctx.inputs\n\n    find_match = g.join(reduce_find_match, initial=None)\n\n    g.add(\n        g.edge_from(g.start_node).to(generate_searches),\n        g.edge_from(generate_searches).map().to(search),\n        g.edge_from(search).to(find_match),\n        g.edge_from(find_match).to(g.end_node),\n    )\n\n    graph = g.build()\n    state = SearchState()\n    result = await graph.run(state=state)\n\n    print(f'Found: {result}')\n    #> Found: target_item\n    print(f'Searches completed: {state.searches_completed}')\n    #> Searches completed: 3\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*\n\nNote that only 3 searches completed instead of all 5, because the reducer canceled the remaining tasks after finding a match.", "url": "https://ai.pydantic.dev/beta/joins/index.html#canceling-sibling-tasks", "page": "beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "Multiple Joins", "anchor": "multiple-joins", "md_text": "A graph can have multiple independent joins:\n\nmultiple\\_joins.py\n\n```\nfrom dataclasses import dataclass, field\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\nfrom pydantic_graph.beta.join import reduce_list_append\n\n\n@dataclass\nclass MultiState:\n    results: dict[str, list[int]] = field(default_factory=dict)\n\n\nasync def main():\n    g = GraphBuilder(state_type=MultiState, output_type=dict[str, list[int]])\n\n    @g.step\n    async def source_a(ctx: StepContext[MultiState, None, None]) -> list[int]:\n        return [1, 2, 3]\n\n    @g.step\n    async def source_b(ctx: StepContext[MultiState, None, None]) -> list[int]:\n        return [10, 20]\n\n    @g.step\n    async def process_a(ctx: StepContext[MultiState, None, int]) -> int:\n        return ctx.inputs * 2\n\n    @g.step\n    async def process_b(ctx: StepContext[MultiState, None, int]) -> int:\n        return ctx.inputs * 3\n\n    join_a = g.join(reduce_list_append, initial_factory=list[int], node_id='join_a')\n    join_b = g.join(reduce_list_append, initial_factory=list[int], node_id='join_b')\n\n    @g.step\n    async def store_a(ctx: StepContext[MultiState, None, list[int]]) -> None:\n        ctx.state.results['a'] = ctx.inputs\n\n    @g.step\n    async def store_b(ctx: StepContext[MultiState, None, list[int]]) -> None:\n        ctx.state.results['b'] = ctx.inputs\n\n    @g.step\n    async def combine(ctx: StepContext[MultiState, None, None]) -> dict[str, list[int]]:\n        return ctx.state.results\n\n    g.add(\n        g.edge_from(g.start_node).to(source_a, source_b),\n        g.edge_from(source_a).map().to(process_a),\n        g.edge_from(source_b).map().to(process_b),\n        g.edge_from(process_a).to(join_a),\n        g.edge_from(process_b).to(join_b),\n        g.edge_from(join_a).to(store_a),\n        g.edge_from(join_b).to(store_b),\n        g.edge_from(store_a, store_b).to(combine),\n        g.edge_from(combine).to(g.end_node),\n    )\n\n    graph = g.build()\n    state = MultiState()\n    result = await graph.run(state=state)\n\n    print(f\"Group A: {sorted(result['a'])}\")\n    #> Group A: [2, 4, 6]\n    print(f\"Group B: {sorted(result['b'])}\")\n    #> Group B: [30, 60]\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/joins/index.html#multiple-joins", "page": "beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "Customizing Join Nodes", "anchor": "customizing-join-nodes", "md_text": "", "url": "https://ai.pydantic.dev/beta/joins/index.html#customizing-join-nodes", "page": "beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Node IDs", "anchor": "custom-node-ids", "md_text": "Like steps, joins can have custom IDs:\n\njoin\\_custom\\_id.py\n\n```\nfrom pydantic_graph.beta.join import reduce_list_append\n\nfrom basic_join import g\n\nmy_join = g.join(reduce_list_append, initial_factory=list[int], node_id='my_custom_join_id')\n```", "url": "https://ai.pydantic.dev/beta/joins/index.html#custom-node-ids", "page": "beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "How Joins Work", "anchor": "how-joins-work", "md_text": "Internally, the graph tracks which \"fork\" each parallel task belongs to. A join:\n\n1. Identifies its parent fork (the fork that created the parallel paths)\n2. Waits for all tasks from that fork to reach the join\n3. Calls `reduce()` for each incoming value\n4. Calls `finalize()` once all values are received\n5. Passes the finalized result to downstream nodes\n\nThis ensures proper synchronization even with nested parallel operations.", "url": "https://ai.pydantic.dev/beta/joins/index.html#how-joins-work", "page": "beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "md_text": "* Learn about [parallel execution](../parallel/index.html) with broadcasting and mapping\n* Explore [conditional branching](../decisions/index.html) with decision nodes\n* See the [API reference](../../pydantic_graph/beta_join/index.html#pydantic_graph.beta.join) for complete reducer documentation", "url": "https://ai.pydantic.dev/beta/joins/index.html#next-steps", "page": "beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "md_text": "A decision node evaluates incoming data and routes it to different branches based on:\n\n* Type matching (using `isinstance`)\n* Literal value matching\n* Custom predicate functions\n\nThe first matching branch is taken, similar to pattern matching or `if-elif-else` chains.", "url": "https://ai.pydantic.dev/beta/decisions/index.html#overview", "page": "beta/decisions/index.html", "source_site": "pydantic_ai"}
{"title": "Creating Decisions", "anchor": "creating-decisions", "md_text": "Use [`g.decision()`](../../pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.decision) to create a decision node, then add branches with [`g.match()`](../../pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.match):\n\nsimple\\_decision.py\n\n```\nfrom dataclasses import dataclass\nfrom typing import Literal\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext, TypeExpression\n\n\n@dataclass\nclass DecisionState:\n    path_taken: str | None = None\n\n\nasync def main():\n    g = GraphBuilder(state_type=DecisionState, output_type=str)\n\n    @g.step\n    async def choose_path(ctx: StepContext[DecisionState, None, None]) -> Literal['left', 'right']:\n        return 'left'\n\n    @g.step\n    async def left_path(ctx: StepContext[DecisionState, None, object]) -> str:\n        ctx.state.path_taken = 'left'\n        return 'Went left'\n\n    @g.step\n    async def right_path(ctx: StepContext[DecisionState, None, object]) -> str:\n        ctx.state.path_taken = 'right'\n        return 'Went right'\n\n    g.add(\n        g.edge_from(g.start_node).to(choose_path),\n        g.edge_from(choose_path).to(\n            g.decision()\n            .branch(g.match(TypeExpression[Literal['left']]).to(left_path))\n            .branch(g.match(TypeExpression[Literal['right']]).to(right_path))\n        ),\n        g.edge_from(left_path, right_path).to(g.end_node),\n    )\n\n    graph = g.build()\n    state = DecisionState()\n    result = await graph.run(state=state)\n    print(result)\n    #> Went left\n    print(state.path_taken)\n    #> left\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/decisions/index.html#creating-decisions", "page": "beta/decisions/index.html", "source_site": "pydantic_ai"}
{"title": "Type Matching", "anchor": "type-matching", "md_text": "Match by type using regular Python types:\n\ntype\\_matching.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext\n\n\n@dataclass\nclass DecisionState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=DecisionState, output_type=str)\n\n    @g.step\n    async def return_int(ctx: StepContext[DecisionState, None, None]) -> int:\n        return 42\n\n    @g.step\n    async def handle_int(ctx: StepContext[DecisionState, None, int]) -> str:\n        return f'Got int: {ctx.inputs}'\n\n    @g.step\n    async def handle_str(ctx: StepContext[DecisionState, None, str]) -> str:\n        return f'Got str: {ctx.inputs}'\n\n    g.add(\n        g.edge_from(g.start_node).to(return_int),\n        g.edge_from(return_int).to(\n            g.decision()\n            .branch(g.match(int).to(handle_int))\n            .branch(g.match(str).to(handle_str))\n        ),\n        g.edge_from(handle_int, handle_str).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=DecisionState())\n    print(result)\n    #> Got int: 42\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/decisions/index.html#type-matching", "page": "beta/decisions/index.html", "source_site": "pydantic_ai"}
{"title": "Matching Union Types", "anchor": "matching-union-types", "md_text": "For more complex type expressions like unions, you need to use [`TypeExpression`](../../pydantic_graph/beta/index.html#pydantic_graph.beta.TypeExpression) because Python's type system doesn't allow union types to be used directly as runtime values:\n\nunion\\_type\\_matching.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext, TypeExpression\n\n\n@dataclass\nclass DecisionState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=DecisionState, output_type=str)\n\n    @g.step\n    async def return_value(ctx: StepContext[DecisionState, None, None]) -> int | str:\n        \"\"\"Returns either an int or a str.\"\"\"\n        return 42\n\n    @g.step\n    async def handle_number(ctx: StepContext[DecisionState, None, int | float]) -> str:\n        return f'Got number: {ctx.inputs}'\n\n    @g.step\n    async def handle_text(ctx: StepContext[DecisionState, None, str]) -> str:\n        return f'Got text: {ctx.inputs}'\n\n    g.add(\n        g.edge_from(g.start_node).to(return_value),\n        g.edge_from(return_value).to(\n            g.decision()\n            # Use TypeExpression for union types\n            .branch(g.match(TypeExpression[int | float]).to(handle_number))\n            .branch(g.match(str).to(handle_text))\n        ),\n        g.edge_from(handle_number, handle_text).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=DecisionState())\n    print(result)\n    #> Got number: 42\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*\n\n[`TypeExpression`](../../pydantic_graph/beta/index.html#pydantic_graph.beta.TypeExpression) is only necessary for complex type expressions like unions (`int | str`), `Literal`, and other type forms that aren't valid as runtime `type` objects. For simple types like `int`, `str`, or custom classes, you can pass them directly to `g.match()`.\n\nThe `TypeForm` class introduced in [PEP 747](https://peps.python.org/pep-0747/) should eventually eliminate the need for this workaround.", "url": "https://ai.pydantic.dev/beta/decisions/index.html#matching-union-types", "page": "beta/decisions/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Matchers", "anchor": "custom-matchers", "md_text": "Provide custom matching logic with the `matches` parameter:\n\ncustom\\_matcher.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext, TypeExpression\n\n\n@dataclass\nclass DecisionState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=DecisionState, output_type=str)\n\n    @g.step\n    async def return_number(ctx: StepContext[DecisionState, None, None]) -> int:\n        return 7\n\n    @g.step\n    async def even_path(ctx: StepContext[DecisionState, None, int]) -> str:\n        return f'{ctx.inputs} is even'\n\n    @g.step\n    async def odd_path(ctx: StepContext[DecisionState, None, int]) -> str:\n        return f'{ctx.inputs} is odd'\n\n    g.add(\n        g.edge_from(g.start_node).to(return_number),\n        g.edge_from(return_number).to(\n            g.decision()\n            .branch(g.match(TypeExpression[int], matches=lambda x: x % 2 == 0).to(even_path))\n            .branch(g.match(TypeExpression[int], matches=lambda x: x % 2 == 1).to(odd_path))\n        ),\n        g.edge_from(even_path, odd_path).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=DecisionState())\n    print(result)\n    #> 7 is odd\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/decisions/index.html#custom-matchers", "page": "beta/decisions/index.html", "source_site": "pydantic_ai"}
{"title": "Branch Priority", "anchor": "branch-priority", "md_text": "Branches are evaluated in the order they're added. The first matching branch is taken:\n\nbranch\\_priority.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext, TypeExpression\n\n\n@dataclass\nclass DecisionState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=DecisionState, output_type=str)\n\n    @g.step\n    async def return_value(ctx: StepContext[DecisionState, None, None]) -> int:\n        return 10\n\n    @g.step\n    async def branch_a(ctx: StepContext[DecisionState, None, int]) -> str:\n        return 'Branch A'\n\n    @g.step\n    async def branch_b(ctx: StepContext[DecisionState, None, int]) -> str:\n        return 'Branch B'\n\n    g.add(\n        g.edge_from(g.start_node).to(return_value),\n        g.edge_from(return_value).to(\n            g.decision()\n            .branch(g.match(TypeExpression[int], matches=lambda x: x >= 5).to(branch_a))\n            .branch(g.match(TypeExpression[int], matches=lambda x: x >= 0).to(branch_b))\n        ),\n        g.edge_from(branch_a, branch_b).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=DecisionState())\n    print(result)\n    #> Branch A\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*\n\nBoth branches could match `10`, but Branch A is first, so it's taken.", "url": "https://ai.pydantic.dev/beta/decisions/index.html#branch-priority", "page": "beta/decisions/index.html", "source_site": "pydantic_ai"}
{"title": "Catch-All Branches", "anchor": "catch-all-branches", "md_text": "Use `object` or `Any` to create a catch-all branch:\n\ncatch\\_all.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext, TypeExpression\n\n\n@dataclass\nclass DecisionState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=DecisionState, output_type=str)\n\n    @g.step\n    async def return_value(ctx: StepContext[DecisionState, None, None]) -> int:\n        return 100\n\n    @g.step\n    async def catch_all(ctx: StepContext[DecisionState, None, object]) -> str:\n        return f'Caught: {ctx.inputs}'\n\n    g.add(\n        g.edge_from(g.start_node).to(return_value),\n        g.edge_from(return_value).to(g.decision().branch(g.match(TypeExpression[object]).to(catch_all))),\n        g.edge_from(catch_all).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=DecisionState())\n    print(result)\n    #> Caught: 100\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/decisions/index.html#catch-all-branches", "page": "beta/decisions/index.html", "source_site": "pydantic_ai"}
{"title": "Nested Decisions", "anchor": "nested-decisions", "md_text": "Decisions can be nested for complex conditional logic:\n\nnested\\_decisions.py\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext, TypeExpression\n\n\n@dataclass\nclass DecisionState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=DecisionState, output_type=str)\n\n    @g.step\n    async def get_number(ctx: StepContext[DecisionState, None, None]) -> int:\n        return 15\n\n    @g.step\n    async def is_positive(ctx: StepContext[DecisionState, None, int]) -> int:\n        return ctx.inputs\n\n    @g.step\n    async def is_negative(ctx: StepContext[DecisionState, None, int]) -> str:\n        return 'Negative'\n\n    @g.step\n    async def small_positive(ctx: StepContext[DecisionState, None, int]) -> str:\n        return 'Small positive'\n\n    @g.step\n    async def large_positive(ctx: StepContext[DecisionState, None, int]) -> str:\n        return 'Large positive'\n\n    g.add(\n        g.edge_from(g.start_node).to(get_number),\n        g.edge_from(get_number).to(\n            g.decision()\n            .branch(g.match(TypeExpression[int], matches=lambda x: x > 0).to(is_positive))\n            .branch(g.match(TypeExpression[int], matches=lambda x: x <= 0).to(is_negative))\n        ),\n        g.edge_from(is_positive).to(\n            g.decision()\n            .branch(g.match(TypeExpression[int], matches=lambda x: x < 10).to(small_positive))\n            .branch(g.match(TypeExpression[int], matches=lambda x: x >= 10).to(large_positive))\n        ),\n        g.edge_from(is_negative, small_positive, large_positive).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=DecisionState())\n    print(result)\n    #> Large positive\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/decisions/index.html#nested-decisions", "page": "beta/decisions/index.html", "source_site": "pydantic_ai"}
{"title": "Branching with Labels", "anchor": "branching-with-labels", "md_text": "Add labels to branches for documentation and diagram generation:\n\nlabeled\\_branches.py\n\n```\nfrom dataclasses import dataclass\nfrom typing import Literal\n\nfrom pydantic_graph.beta import GraphBuilder, StepContext, TypeExpression\n\n\n@dataclass\nclass DecisionState:\n    pass\n\n\nasync def main():\n    g = GraphBuilder(state_type=DecisionState, output_type=str)\n\n    @g.step\n    async def choose(ctx: StepContext[DecisionState, None, None]) -> Literal['a', 'b']:\n        return 'a'\n\n    @g.step\n    async def path_a(ctx: StepContext[DecisionState, None, object]) -> str:\n        return 'Path A'\n\n    @g.step\n    async def path_b(ctx: StepContext[DecisionState, None, object]) -> str:\n        return 'Path B'\n\n    g.add(\n        g.edge_from(g.start_node).to(choose),\n        g.edge_from(choose).to(\n            g.decision()\n            .branch(g.match(TypeExpression[Literal['a']]).label('Take path A').to(path_a))\n            .branch(g.match(TypeExpression[Literal['b']]).label('Take path B').to(path_b))\n        ),\n        g.edge_from(path_a, path_b).to(g.end_node),\n    )\n\n    graph = g.build()\n    result = await graph.run(state=DecisionState())\n    print(result)\n    #> Path A\n```\n\n*(This example is complete, it can be run \"as is\" — you'll need to add `import asyncio; asyncio.run(main())` to run `main`)*", "url": "https://ai.pydantic.dev/beta/decisions/index.html#branching-with-labels", "page": "beta/decisions/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "md_text": "* Learn about [parallel execution](../parallel/index.html) with broadcasting and mapping\n* Understand [join nodes](../joins/index.html) for aggregating parallel results\n* See the [API reference](../../pydantic_graph/beta_decision/index.html#pydantic_graph.beta.decision) for complete decision documentation", "url": "https://ai.pydantic.dev/beta/decisions/index.html#next-steps", "page": "beta/decisions/index.html", "source_site": "pydantic_ai"}
{"title": "generate\\_dataset `async`", "anchor": "generatedataset-async", "md_text": "```\ngenerate_dataset(\n    *,\n    dataset_type: type[\n        Dataset[InputsT, OutputT, MetadataT]\n    ],\n    path: Path | str | None = None,\n    custom_evaluator_types: Sequence[\n        type[Evaluator[InputsT, OutputT, MetadataT]]\n    ] = (),\n    model: Model | KnownModelName = \"openai:gpt-4o\",\n    n_examples: int = 3,\n    extra_instructions: str | None = None\n) -> Dataset[InputsT, OutputT, MetadataT]\n```\n\nUse an LLM to generate a dataset of test cases, each consisting of input, expected output, and metadata.\n\nThis function creates a properly structured dataset with the specified input, output, and metadata types.\nIt uses an LLM to attempt to generate realistic test cases that conform to the types' schemas.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `path` | `Path | str | None` | Optional path to save the generated dataset. If provided, the dataset will be saved to this location. | `None` |\n| `dataset_type` | `type[Dataset[InputsT, OutputT, MetadataT]]` | The type of dataset to generate, with the desired input, output, and metadata types. | *required* |\n| `custom_evaluator_types` | `Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]]` | Optional sequence of custom evaluator classes to include in the schema. | `()` |\n| `model` | `Model | KnownModelName` | The Pydantic AI model to use for generation. Defaults to 'gpt-4o'. | `'openai:gpt-4o'` |\n| `n_examples` | `int` | Number of examples to generate. Defaults to 3. | `3` |\n| `extra_instructions` | `str | None` | Optional additional instructions to provide to the LLM. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `Dataset[InputsT, OutputT, MetadataT]` | A properly structured Dataset object with generated test cases. |\n\nRaises:\n\n| Type | Description |\n| --- | --- |\n| `ValidationError` | If the LLM's response cannot be parsed as a valid dataset. |\n\nSource code in `pydantic_evals/pydantic_evals/generation.py`", "url": "https://ai.pydantic.dev/pydantic_evals/generation/index.html#generatedataset-async", "page": "pydantic_evals/generation/index.html", "source_site": "pydantic_ai"}
{"title": "generate\\_dataset `async`", "anchor": "generatedataset-async", "md_text": "|  |  |\n| --- | --- |\n| ``` 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 ``` | ``` async def generate_dataset(     *,     dataset_type: type[Dataset[InputsT, OutputT, MetadataT]],     path: Path | str | None = None,     custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),     model: models.Model | models.KnownModelName = 'openai:gpt-4o',     n_examples: int = 3,     extra_instructions: str | None = None, ) -> Dataset[InputsT, OutputT, MetadataT]:     \"\"\"Use an LLM to generate a dataset of test cases, each consisting of input, expected output, and metadata.      This function creates a properly structured dataset with the specified input, output, and metadata types.     It uses an LLM to attempt to generate realistic test cases that conform to the types' schemas.      Args:         path: Optional path to save the generated dataset. If provided, the dataset will be saved to this location.         dataset_type: The type of dataset to generate, with the desired input, output, and metadata types.         custom_evaluator_types: Optional sequence of custom evaluator classes to include in the schema.         model: The Pydantic AI model to use for generation. Defaults to 'gpt-4o'.         n_examples: Number of examples to generate. Defaults to 3.         extra_instructions: Optional additional instructions to provide to the LLM.      Returns:         A properly structured Dataset object with generated test cases.      Raises:         ValidationError: If the LLM's response cannot be parsed as a valid dataset.     \"\"\"     output_schema = dataset_type.model_json_schema_with_evaluators(custom_evaluator_types)      # TODO: Use `output_type=StructuredDict(output_schema)` (and `from_dict` below) once https://github.com/pydantic/pydantic/issues/12145     # is fixed and `StructuredDict` no longer needs to use `InlineDefsJsonSchemaTransformer`.     agent = Agent(         model,         system_prompt=(             f'Generate an object that is in compliance with this JSON schema:\\n{output_schema}\\n\\n'             f'Include {n_examples} example cases.'             ' You must not include any characters in your response before the opening { of the JSON object, or after the closing }.'         ),         output_type=str,         retries=1,     )      result = await agent.run(extra_instructions or 'Please generate the object.')     output = strip_markdown_fences(result.output)     try:         result = dataset_type.from_text(output, fmt='json', custom_evaluator_types=custom_evaluator_types)     except ValidationError as e:  # pragma: no cover         print(f'Raw response from model:\\n{result.output}')         raise e     if path is not None:         result.to_file(path, custom_evaluator_types=custom_evaluator_types)  # pragma: no cover     return result ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/generation/index.html#generatedataset-async", "page": "pydantic_evals/generation/index.html", "source_site": "pydantic_ai"}
{"title": "Case `dataclass`", "anchor": "case-dataclass", "md_text": "Bases: `Generic[InputsT, OutputT, MetadataT]`\n\nA single row of a [`Dataset`](index.html#pydantic_evals.dataset.Dataset).\n\nEach case represents a single test scenario with inputs to test. A case may optionally specify a name, expected\noutputs to compare against, and arbitrary metadata.\n\nCases can also have their own specific evaluators which are run in addition to dataset-level evaluators.\n\nExample:\n\n```\nfrom pydantic_evals import Case\n\ncase = Case(\n    name='Simple addition',\n    inputs={'a': 1, 'b': 2},\n    expected_output=3,\n    metadata={'description': 'Tests basic addition'},\n)\n```\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`\n\n|  |  |\n| --- | --- |\n| ``` 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 ``` | ``` @dataclass(init=False) class Case(Generic[InputsT, OutputT, MetadataT]):     \"\"\"A single row of a [`Dataset`][pydantic_evals.Dataset].      Each case represents a single test scenario with inputs to test. A case may optionally specify a name, expected     outputs to compare against, and arbitrary metadata.      Cases can also have their own specific evaluators which are run in addition to dataset-level evaluators.      Example:     ```python     from pydantic_evals import Case      case = Case(         name='Simple addition',         inputs={'a': 1, 'b': 2},         expected_output=3,         metadata={'description': 'Tests basic addition'},     )     ```     \"\"\"      name: str | None     \"\"\"Name of the case. This is used to identify the case in the report and can be used to filter cases.\"\"\"     inputs: InputsT     \"\"\"Inputs to the task. This is the input to the task that will be evaluated.\"\"\"     metadata: MetadataT | None = None     \"\"\"Metadata to be used in the evaluation.      This can be used to provide additional information about the case to the evaluators.     \"\"\"     expected_output: OutputT | None = None     \"\"\"Expected output of the task. This is the expected output of the task that will be evaluated.\"\"\"     evaluators: list[Evaluator[InputsT, OutputT, MetadataT]] = field(default_factory=list)     \"\"\"Evaluators to be used just on this case.\"\"\"      def __init__(         self,         *,         name: str | None = None,         inputs: InputsT,         metadata: MetadataT | None = None,         expected_output: OutputT | None = None,         evaluators: tuple[Evaluator[InputsT, OutputT, MetadataT], ...] = (),     ):         \"\"\"Initialize a new test case.          Args:             name: Optional name for the case. If not provided, a generic name will be assigned when added to a dataset.             inputs: The inputs to the task being evaluated.             metadata: Optional metadata for the case, which can be used by evaluators.             expected_output: Optional expected output of the task, used for comparison in evaluators.             evaluators: Tuple of evaluators specific to this case. These are in addition to any                 dataset-level evaluators.          \"\"\"         # Note: `evaluators` must be a tuple instead of Sequence due to misbehavior with pyright's generic parameter         # inference if it has type `Sequence`         self.name = name         self.inputs = inputs         self.metadata = metadata         self.expected_output = expected_output         self.evaluators = list(evaluators) ``` |\n\n#### \\_\\_init\\_\\_\n\n```\n__init__(\n    *,\n    name: str | None = None,\n    inputs: InputsT,\n    metadata: MetadataT | None = None,\n    expected_output: OutputT | None = None,\n    evaluators: tuple[\n        Evaluator[InputsT, OutputT, MetadataT], ...\n    ] = ()\n)\n```\n\nInitialize a new test case.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `name` | `str | None` | Optional name for the case. If not provided, a generic name will be assigned when added to a dataset. | `None` |\n| `inputs` | `InputsT` | The inputs to the task being evaluated. | *required* |\n| `metadata` | `MetadataT | None` | Optional metadata for the case, which can be used by evaluators. | `None` |\n| `expected_output` | `OutputT | None` | Optional expected output of the task, used for comparison in evaluators. | `None` |\n| `evaluators` | `tuple[Evaluator[InputsT, OutputT, MetadataT], ...]` | Tuple of evaluators specific to this case. These are in addition to any dataset-level evaluators. | `()` |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`", "url": "https://ai.pydantic.dev/pydantic_evals/dataset/index.html#case-dataclass", "page": "pydantic_evals/dataset/index.html", "source_site": "pydantic_ai"}
{"title": "Case `dataclass`", "anchor": "case-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 ``` | ``` def __init__(     self,     *,     name: str | None = None,     inputs: InputsT,     metadata: MetadataT | None = None,     expected_output: OutputT | None = None,     evaluators: tuple[Evaluator[InputsT, OutputT, MetadataT], ...] = (), ):     \"\"\"Initialize a new test case.      Args:         name: Optional name for the case. If not provided, a generic name will be assigned when added to a dataset.         inputs: The inputs to the task being evaluated.         metadata: Optional metadata for the case, which can be used by evaluators.         expected_output: Optional expected output of the task, used for comparison in evaluators.         evaluators: Tuple of evaluators specific to this case. These are in addition to any             dataset-level evaluators.      \"\"\"     # Note: `evaluators` must be a tuple instead of Sequence due to misbehavior with pyright's generic parameter     # inference if it has type `Sequence`     self.name = name     self.inputs = inputs     self.metadata = metadata     self.expected_output = expected_output     self.evaluators = list(evaluators) ``` |\n\n#### name `instance-attribute`\n\n```\nname: str | None = name\n```\n\nName of the case. This is used to identify the case in the report and can be used to filter cases.\n\n#### inputs `instance-attribute`\n\n```\ninputs: InputsT = inputs\n```\n\nInputs to the task. This is the input to the task that will be evaluated.\n\n#### metadata `class-attribute` `instance-attribute`\n\n```\nmetadata: MetadataT | None = metadata\n```\n\nMetadata to be used in the evaluation.\n\nThis can be used to provide additional information about the case to the evaluators.\n\n#### expected\\_output `class-attribute` `instance-attribute`\n\n```\nexpected_output: OutputT | None = expected_output\n```\n\nExpected output of the task. This is the expected output of the task that will be evaluated.\n\n#### evaluators `class-attribute` `instance-attribute`\n\n```\nevaluators: list[Evaluator[InputsT, OutputT, MetadataT]] = (\n    list(evaluators)\n)\n```\n\nEvaluators to be used just on this case.", "url": "https://ai.pydantic.dev/pydantic_evals/dataset/index.html#case-dataclass", "page": "pydantic_evals/dataset/index.html", "source_site": "pydantic_ai"}
{"title": "Dataset", "anchor": "dataset", "md_text": "Bases: `BaseModel`, `Generic[InputsT, OutputT, MetadataT]`\n\nA dataset of test [cases](index.html#pydantic_evals.dataset.Case).\n\nDatasets allow you to organize a collection of test cases and evaluate them against a task function.\nThey can be loaded from and saved to YAML or JSON files, and can have dataset-level evaluators that\napply to all cases.\n\nExample:\n\n```\n# Create a dataset with two test cases\nfrom dataclasses import dataclass\n\nfrom pydantic_evals import Case, Dataset\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass ExactMatch(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return ctx.output == ctx.expected_output\n\ndataset = Dataset(\n    cases=[\n        Case(name='test1', inputs={'text': 'Hello'}, expected_output='HELLO'),\n        Case(name='test2', inputs={'text': 'World'}, expected_output='WORLD'),\n    ],\n    evaluators=[ExactMatch()],\n)\n\n# Evaluate the dataset against a task function\nasync def uppercase(inputs: dict) -> str:\n    return inputs['text'].upper()\n\nasync def main():\n    report = await dataset.evaluate(uppercase)\n    report.print()\n'''\n   Evaluation Summary: uppercase\n┏━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓\n┃ Case ID  ┃ Assertions ┃ Duration ┃\n┡━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩\n│ test1    │ ✔          │     10ms │\n├──────────┼────────────┼──────────┤\n│ test2    │ ✔          │     10ms │\n├──────────┼────────────┼──────────┤\n│ Averages │ 100.0% ✔   │     10ms │\n└──────────┴────────────┴──────────┘\n'''\n```\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`", "url": "https://ai.pydantic.dev/pydantic_evals/dataset/index.html#dataset", "page": "pydantic_evals/dataset/index.html", "source_site": "pydantic_ai"}
{"title": "Dataset", "anchor": "dataset", "md_text": "|  |  |\n| --- | --- |\n| ``` 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773 774 775 776 777 778 779 780 781 782 783 784 785 786 787 788 789 790 791 792 793 794 795 796 797 798 799 800 801 802 ``` | ``` class Dataset(BaseModel, Generic[InputsT, OutputT, MetadataT], extra='forbid', arbitrary_types_allowed=True):     \"\"\"A dataset of test [cases][pydantic_evals.Case].      Datasets allow you to organize a collection of test cases and evaluate them against a task function.     They can be loaded from and saved to YAML or JSON files, and can have dataset-level evaluators that     apply to all cases.      Example:     ```python     # Create a dataset with two test cases     from dataclasses import dataclass      from pydantic_evals import Case, Dataset     from pydantic_evals.evaluators import Evaluator, EvaluatorContext       @dataclass     class ExactMatch(Evaluator):         def evaluate(self, ctx: EvaluatorContext) -> bool:             return ctx.output == ctx.expected_output      dataset = Dataset(         cases=[             Case(name='test1', inputs={'text': 'Hello'}, expected_output='HELLO'),             Case(name='test2', inputs={'text': 'World'}, expected_output='WORLD'),         ],         evaluators=[ExactMatch()],     )      # Evaluate the dataset against a task function     async def uppercase(inputs: dict) -> str:         return inputs['text'].upper()      async def main():         report = await dataset.evaluate(uppercase)         report.print()     '''        Evaluation Summary: uppercase     ┏━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━┓     ┃ Case ID  ┃ Assertions ┃ Duration ┃     ┡━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━┩     │ test1    │ ✔          │     10ms │     ├──────────┼────────────┼──────────┤     │ test2    │ ✔          │     10ms │     ├──────────┼────────────┼──────────┤     │ Averages │ 100.0% ✔   │     10ms │     └──────────┴────────────┴──────────┘     '''     ```     \"\"\"      name: str | None = None     \"\"\"Optional name of the dataset.\"\"\"     cases: list[Case[InputsT, OutputT, MetadataT]]     \"\"\"List of test cases in the dataset.\"\"\"     evaluators: list[Evaluator[InputsT, OutputT, MetadataT]] = []     \"\"\"List of evaluators to be used on all cases in the dataset.\"\"\"      def __init__(         self,         *,         name: str | None = None,         cases: Sequence[Case[InputsT, OutputT, MetadataT]],         evaluators: Sequence[Evaluator[InputsT, OutputT, MetadataT]] = (),     ):         \"\"\"Initialize a new dataset with test cases and optional evaluators.          Args:             name: Optional name for the dataset.             cases: Sequence of test cases to include in the dataset.             evaluators: Optional sequence of evaluators to apply to all cases in the dataset.         \"\"\"         case_names = set[str]()         for case in cases:             if case.name is None:                 continue             if case.name in case_names:                 raise ValueError(f'Duplicate case name: {case.name!r}')             case_names.add(case.name)          super().__init__(             name=name,             cases=cases,             evaluators=list(evaluators),         )      # TODO in v2: Make everything not required keyword-only     async def evaluate(         self,         task: Callable[[InputsT], Awaitable[OutputT]] | Callable[[InputsT], OutputT],         name: str | None = None,         max_concurrency: int | None = None,         progress: bool = True,         retry_task: RetryConfig | None = None,         retry_evaluators: RetryConfig | None = None,         *,         task_name: str | None = None,     ) -> EvaluationReport[InputsT, OutputT, MetadataT]:         \"\"\"Evaluates the test cases in the dataset using the given task.          This method runs the task on each case in the dataset, applies evaluators,         and collects results into a report. Cases are run concurrently, limited by `max_concurrency` if specified.          Args:             task: The task to evaluate. This should be a callable that takes the inputs of the case                 and returns the output.             name: The name of the experiment being run, this is used to identify the experiment in the report.                 If omitted, the task_name will be used; if that is not specified, the name of the task function is used.             max_concurrency: The maximum number of concurrent evaluations of the task to allow.                 If None, all cases will be evaluated concurrently.             progress: Whether to show a progress bar for the evaluation. Defaults to `True`.             retry_task: Optional retry configuration for the task execution.             retry_evaluators: Optional retry configuration for evaluator execution.             task_name: Optional override to the name of the task being executed, otherwise the name of the task                 function will be used.          Returns:             A report containing the results of the evaluation.         \"\"\"         task_name = task_name or get_unwrapped_function_name(task)         name = name or task_name         total_cases = len(self.cases)         progress_bar = Progress() if progress else None          limiter = anyio.Semaphore(max_concurrency) if max_concurrency is not None else AsyncExitStack()          with (             logfire_span(                 'evaluate {name}',                 name=name,                 task_name=task_name,                 dataset_name=self.name,                 n_cases=len(self.cases),                 **{'gen_ai.operation.name': 'experiment'},  # pyright: ignore[reportArgumentType]             ) as eval_span,             progress_bar or nullcontext(),         ):             task_id = progress_bar.add_task(f'Evaluating {task_name}', total=total_cases) if progress_bar else None              async def _handle_case(case: Case[InputsT, OutputT, MetadataT], report_case_name: str):                 async with limiter:                     result = await _run_task_and_evaluators(                         task, case, report_case_name, self.evaluators, retry_task, retry_evaluators                     )                     if progress_bar and task_id is not None:  # pragma: no branch                         progress_bar.update(task_id, advance=1)                     return result              if (context := eval_span.context) is None:  # pragma: no cover                 trace_id = None                 span_id = None             else:                 trace_id = f'{context.trace_id:032x}'                 span_id = f'{context.span_id:016x}'             cases_and_failures = await task_group_gather(                 [                     lambda case=case, i=i: _handle_case(case, case.name or f'Case {i}')                     for i, case in enumerate(self.cases, 1)                 ]             )             cases: list[ReportCase] = []             failures: list[ReportCaseFailure] = []             for item in cases_and_failures:                 if isinstance(item, ReportCase):                     cases.append(item)                 else:                     failures.append(item)             report = EvaluationReport(                 name=name,                 cases=cases,                 failures=failures,                 span_id=span_id,                 trace_id=trace_id,             )             if (averages := report.averages()) is not None and averages.assertions is not None:                 experiment_metadata = {'n_cases': len(self.cases), 'averages': averages}                 eval_span.set_attribute('logfire.experiment.metadata', experiment_metadata)                 eval_span.set_attribute('assertion_pass_rate', averages.assertions)         return report      def evaluate_sync(         self,         task: Callable[[InputsT], Awaitable[OutputT]] | Callable[[InputsT], OutputT],         name: str | None = None,         max_concurrency: int | None = None,         progress: bool = True,         retry_task: RetryConfig | None = None,         retry_evaluators: RetryConfig | None = None,     ) -> EvaluationReport[InputsT, OutputT, MetadataT]:         \"\"\"Evaluates the test cases in the dataset using the given task.          This is a synchronous wrapper around [`evaluate`][pydantic_evals.Dataset.evaluate] provided for convenience.          Args:             task: The task to evaluate. This should be a callable that takes the inputs of the case                 and returns the output.             name: The name of the task being evaluated, this is used to identify the task in the report.                 If omitted, the name of the task function will be used.             max_concurrency: The maximum number of concurrent evaluations of the task to allow.                 If None, all cases will be evaluated concurrently.             progress: Whether to show a progress bar for the evaluation. Defaults to True.             retry_task: Optional retry configuration for the task execution.             retry_evaluators: Optional retry configuration for evaluator execution.          Returns:             A report containing the results of the evaluation.         \"\"\"         return get_event_loop().run_until_complete(             self.evaluate(                 task,                 task_name=name,                 max_concurrency=max_concurrency,                 progress=progress,                 retry_task=retry_task,                 retry_evaluators=retry_evaluators,             )         )      def add_case(         self,         *,         name: str | None = None,         inputs: InputsT,         metadata: MetadataT | None = None,         expected_output: OutputT | None = None,         evaluators: tuple[Evaluator[InputsT, OutputT, MetadataT], ...] = (),     ) -> None:         \"\"\"Adds a case to the dataset.          This is a convenience method for creating a [`Case`][pydantic_evals.Case] and adding it to the dataset.          Args:             name: Optional name for the case. If not provided, a generic name will be assigned.             inputs: The inputs to the task being evaluated.             metadata: Optional metadata for the case, which can be used by evaluators.             expected_output: The expected output of the task, used for comparison in evaluators.             evaluators: Tuple of evaluators specific to this case, in addition to dataset-level evaluators.         \"\"\"         if name in {case.name for case in self.cases}:             raise ValueError(f'Duplicate case name: {name!r}')          case = Case[InputsT, OutputT, MetadataT](             name=name,             inputs=inputs,             metadata=metadata,             expected_output=expected_output,             evaluators=evaluators,         )         self.cases.append(case)      def add_evaluator(         self,         evaluator: Evaluator[InputsT, OutputT, MetadataT],         specific_case: str | None = None,     ) -> None:         \"\"\"Adds an evaluator to the dataset or a specific case.          Args:             evaluator: The evaluator to add.             specific_case: If provided, the evaluator will only be added to the case with this name.                 If None, the evaluator will be added to all cases in the dataset.          Raises:             ValueError: If `specific_case` is provided but no case with that name exists in the dataset.         \"\"\"         if specific_case is None:             self.evaluators.append(evaluator)         else:             # If this is too slow, we could try to add a case lookup dict.             # Note that if we do that, we'd need to make the cases list private to prevent modification.             added = False             for case in self.cases:                 if case.name == specific_case:                     case.evaluators.append(evaluator)                     added = True             if not added:                 raise ValueError(f'Case {specific_case!r} not found in the dataset')      @classmethod     @functools.cache     def _params(cls) -> tuple[type[InputsT], type[OutputT], type[MetadataT]]:         \"\"\"Get the type parameters for the Dataset class.          Returns:             A tuple of (InputsT, OutputT, MetadataT) types.         \"\"\"         for c in cls.__mro__:             metadata = getattr(c, '__pydantic_generic_metadata__', {})             if len(args := (metadata.get('args', ()) or getattr(c, '__args__', ()))) == 3:  # pragma: no branch                 return args         else:  # pragma: no cover             warnings.warn(                 f'Could not determine the generic parameters for {cls}; using `Any` for each.'                 f' You should explicitly set the generic parameters via `Dataset[MyInputs, MyOutput, MyMetadata]`'                 f' when serializing or deserializing.',                 UserWarning,             )             return Any, Any, Any  # type: ignore      @classmethod     def from_file(         cls,         path: Path | str,         fmt: Literal['yaml', 'json'] | None = None,         custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),     ) -> Self:         \"\"\"Load a dataset from a file.          Args:             path: Path to the file to load.             fmt: Format of the file. If None, the format will be inferred from the file extension.                 Must be either 'yaml' or 'json'.             custom_evaluator_types: Custom evaluator classes to use when deserializing the dataset.                 These are additional evaluators beyond the default ones.          Returns:             A new Dataset instance loaded from the file.          Raises:             ValidationError: If the file cannot be parsed as a valid dataset.             ValueError: If the format cannot be inferred from the file extension.         \"\"\"         path = Path(path)         fmt = cls._infer_fmt(path, fmt)          raw = Path(path).read_text()         try:             return cls.from_text(raw, fmt=fmt, custom_evaluator_types=custom_evaluator_types, default_name=path.stem)         except ValidationError as e:  # pragma: no cover             raise ValueError(f'{path} contains data that does not match the schema for {cls.__name__}:\\n{e}.') from e      @classmethod     def from_text(         cls,         contents: str,         fmt: Literal['yaml', 'json'] = 'yaml',         custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),         *,         default_name: str | None = None,     ) -> Self:         \"\"\"Load a dataset from a string.          Args:             contents: The string content to parse.             fmt: Format of the content. Must be either 'yaml' or 'json'.             custom_evaluator_types: Custom evaluator classes to use when deserializing the dataset.                 These are additional evaluators beyond the default ones.             default_name: Default name of the dataset, to be used if not specified in the serialized contents.          Returns:             A new Dataset instance parsed from the string.          Raises:             ValidationError: If the content cannot be parsed as a valid dataset.         \"\"\"         if fmt == 'yaml':             loaded = yaml.safe_load(contents)             return cls.from_dict(loaded, custom_evaluator_types, default_name=default_name)         else:             dataset_model_type = cls._serialization_type()             dataset_model = dataset_model_type.model_validate_json(contents)             return cls._from_dataset_model(dataset_model, custom_evaluator_types, default_name)      @classmethod     def from_dict(         cls,         data: dict[str, Any],         custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),         *,         default_name: str | None = None,     ) -> Self:         \"\"\"Load a dataset from a dictionary.          Args:             data: Dictionary representation of the dataset.             custom_evaluator_types: Custom evaluator classes to use when deserializing the dataset.                 These are additional evaluators beyond the default ones.             default_name: Default name of the dataset, to be used if not specified in the data.          Returns:             A new Dataset instance created from the dictionary.          Raises:             ValidationError: If the dictionary cannot be converted to a valid dataset.         \"\"\"         dataset_model_type = cls._serialization_type()         dataset_model = dataset_model_type.model_validate(data)         return cls._from_dataset_model(dataset_model, custom_evaluator_types, default_name)      @classmethod     def _from_dataset_model(         cls,         dataset_model: _DatasetModel[InputsT, OutputT, MetadataT],         custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),         default_name: str | None = None,     ) -> Self:         \"\"\"Create a Dataset from a _DatasetModel.          Args:             dataset_model: The _DatasetModel to convert.             custom_evaluator_types: Custom evaluator classes to register for deserialization.             default_name: Default name of the dataset, to be used if the value is `None` in the provided model.          Returns:             A new Dataset instance created from the _DatasetModel.         \"\"\"         registry = _get_registry(custom_evaluator_types)          cases: list[Case[InputsT, OutputT, MetadataT]] = []         errors: list[ValueError] = []         dataset_evaluators: list[Evaluator] = []         for spec in dataset_model.evaluators:             try:                 dataset_evaluator = _load_evaluator_from_registry(registry, None, spec)             except ValueError as e:                 errors.append(e)                 continue             dataset_evaluators.append(dataset_evaluator)          for row in dataset_model.cases:             evaluators: list[Evaluator] = []             for spec in row.evaluators:                 try:                     evaluator = _load_evaluator_from_registry(registry, row.name, spec)                 except ValueError as e:                     errors.append(e)                     continue                 evaluators.append(evaluator)             row = Case[InputsT, OutputT, MetadataT](                 name=row.name,                 inputs=row.inputs,                 metadata=row.metadata,                 expected_output=row.expected_output,             )             row.evaluators = evaluators             cases.append(row)         if errors:             raise ExceptionGroup(f'{len(errors)} error(s) loading evaluators from registry', errors[:3])         result = cls(name=dataset_model.name, cases=cases)         if result.name is None:             result.name = default_name         result.evaluators = dataset_evaluators         return result      def to_file(         self,         path: Path | str,         fmt: Literal['yaml', 'json'] | None = None,         schema_path: Path | str | None = DEFAULT_SCHEMA_PATH_TEMPLATE,         custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),     ):         \"\"\"Save the dataset to a file.          Args:             path: Path to save the dataset to.             fmt: Format to use. If None, the format will be inferred from the file extension.                 Must be either 'yaml' or 'json'.             schema_path: Path to save the JSON schema to. If None, no schema will be saved.                 Can be a string template with {stem} which will be replaced with the dataset filename stem.             custom_evaluator_types: Custom evaluator classes to include in the schema.         \"\"\"         path = Path(path)         fmt = self._infer_fmt(path, fmt)          schema_ref: str | None = None         if schema_path is not None:  # pragma: no branch             if isinstance(schema_path, str):  # pragma: no branch                 schema_path = Path(schema_path.format(stem=path.stem))              if not schema_path.is_absolute():                 schema_ref = str(schema_path)                 schema_path = path.parent / schema_path             elif schema_path.is_relative_to(path):  # pragma: no cover                 schema_ref = str(_get_relative_path_reference(schema_path, path))             else:  # pragma: no cover                 schema_ref = str(schema_path)             self._save_schema(schema_path, custom_evaluator_types)          context: dict[str, Any] = {'use_short_form': True}         if fmt == 'yaml':             dumped_data = self.model_dump(mode='json', by_alias=True, context=context)             content = yaml.dump(dumped_data, sort_keys=False)             if schema_ref:  # pragma: no branch                 yaml_language_server_line = f'{_YAML_SCHEMA_LINE_PREFIX}{schema_ref}'                 content = f'{yaml_language_server_line}\\n{content}'             path.write_text(content)         else:             context['$schema'] = schema_ref             json_data = self.model_dump_json(indent=2, by_alias=True, context=context)             path.write_text(json_data + '\\n')      @classmethod     def model_json_schema_with_evaluators(         cls,         custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),     ) -> dict[str, Any]:         \"\"\"Generate a JSON schema for this dataset type, including evaluator details.          This is useful for generating a schema that can be used to validate YAML-format dataset files.          Args:             custom_evaluator_types: Custom evaluator classes to include in the schema.          Returns:             A dictionary representing the JSON schema.         \"\"\"         # Note: this function could maybe be simplified now that Evaluators are always dataclasses         registry = _get_registry(custom_evaluator_types)          evaluator_schema_types: list[Any] = []         for name, evaluator_class in registry.items():             type_hints = _typing_extra.get_function_type_hints(evaluator_class)             type_hints.pop('return', None)             required_type_hints: dict[str, Any] = {}              for p in inspect.signature(evaluator_class).parameters.values():                 type_hints.setdefault(p.name, Any)                 if p.default is not p.empty:                     type_hints[p.name] = NotRequired[type_hints[p.name]]                 else:                     required_type_hints[p.name] = type_hints[p.name]              def _make_typed_dict(cls_name_prefix: str, fields: dict[str, Any]) -> Any:                 td = TypedDict(f'{cls_name_prefix}_{name}', fields)  # pyright: ignore[reportArgumentType]                 config = ConfigDict(extra='forbid', arbitrary_types_allowed=True)                 # TODO: Replace with pydantic.with_config once pydantic 2.11 is the min supported version                 td.__pydantic_config__ = config  # pyright: ignore[reportAttributeAccessIssue]                 return td              # Shortest form: just the call name             if len(type_hints) == 0 or not required_type_hints:                 evaluator_schema_types.append(Literal[name])              # Short form: can be called with only one parameter             if len(type_hints) == 1:                 [type_hint_type] = type_hints.values()                 evaluator_schema_types.append(_make_typed_dict('short_evaluator', {name: type_hint_type}))             elif len(required_type_hints) == 1:  # pragma: no branch                 [type_hint_type] = required_type_hints.values()                 evaluator_schema_types.append(_make_typed_dict('short_evaluator', {name: type_hint_type}))              # Long form: multiple parameters, possibly required             if len(type_hints) > 1:                 params_td = _make_typed_dict('evaluator_params', type_hints)                 evaluator_schema_types.append(_make_typed_dict('evaluator', {name: params_td}))          in_type, out_type, meta_type = cls._params()          # Note: we shadow the `Case` and `Dataset` class names here to generate a clean JSON schema         class Case(BaseModel, extra='forbid'):  # pyright: ignore[reportUnusedClass]  # this _is_ used below, but pyright doesn't seem to notice..             name: str | None = None             inputs: in_type  # pyright: ignore[reportInvalidTypeForm]             metadata: meta_type | None = None  # pyright: ignore[reportInvalidTypeForm,reportUnknownVariableType]             expected_output: out_type | None = None  # pyright: ignore[reportInvalidTypeForm,reportUnknownVariableType]             if evaluator_schema_types:  # pragma: no branch                 evaluators: list[Union[tuple(evaluator_schema_types)]] = []  # pyright: ignore  # noqa UP007          class Dataset(BaseModel, extra='forbid'):             name: str | None = None             cases: list[Case]             if evaluator_schema_types:  # pragma: no branch                 evaluators: list[Union[tuple(evaluator_schema_types)]] = []  # pyright: ignore  # noqa UP007          json_schema = Dataset.model_json_schema()         # See `_add_json_schema` below, since `$schema` is added to the JSON, it has to be supported in the JSON         json_schema['properties']['$schema'] = {'type': 'string'}         return json_schema      @classmethod     def _save_schema(         cls, path: Path | str, custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = ()     ):         \"\"\"Save the JSON schema for this dataset type to a file.          Args:             path: Path to save the schema to.             custom_evaluator_types: Custom evaluator classes to include in the schema.         \"\"\"         path = Path(path)         json_schema = cls.model_json_schema_with_evaluators(custom_evaluator_types)         schema_content = to_json(json_schema, indent=2).decode() + '\\n'         if not path.exists() or path.read_text() != schema_content:  # pragma: no branch             path.write_text(schema_content)      @classmethod     @functools.cache     def _serialization_type(cls) -> type[_DatasetModel[InputsT, OutputT, MetadataT]]:         \"\"\"Get the serialization type for this dataset class.          Returns:             A _DatasetModel type with the same generic parameters as this Dataset class.         \"\"\"         input_type, output_type, metadata_type = cls._params()         return _DatasetModel[input_type, output_type, metadata_type]      @classmethod     def _infer_fmt(cls, path: Path, fmt: Literal['yaml', 'json'] | None) -> Literal['yaml', 'json']:         \"\"\"Infer the format to use for a file based on its extension.          Args:             path: The path to infer the format for.             fmt: The explicitly provided format, if any.          Returns:             The inferred format ('yaml' or 'json').          Raises:             ValueError: If the format cannot be inferred from the file extension.         \"\"\"         if fmt is not None:             return fmt         suffix = path.suffix.lower()         if suffix in {'.yaml', '.yml'}:             return 'yaml'         elif suffix == '.json':             return 'json'         raise ValueError(             f'Could not infer format for filename {path.name!r}. Use the `fmt` argument to specify the format.'         )      @model_serializer(mode='wrap')     def _add_json_schema(self, nxt: SerializerFunctionWrapHandler, info: SerializationInfo) -> dict[str, Any]:         \"\"\"Add the JSON schema path to the serialized output.          See <https://github.com/json-schema-org/json-schema-spec/issues/828> for context, that seems to be the nearest         there is to a spec for this.         \"\"\"         context = cast(dict[str, Any] | None, info.context)         if isinstance(context, dict) and (schema := context.get('$schema')):             return {'$schema': schema} | nxt(self)         else:             return nxt(self) ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/dataset/index.html#dataset", "page": "pydantic_evals/dataset/index.html", "source_site": "pydantic_ai"}
{"title": "Dataset", "anchor": "dataset", "md_text": "#### name `class-attribute` `instance-attribute`\n\n```\nname: str | None = None\n```\n\nOptional name of the dataset.\n\n#### cases `instance-attribute`\n\n```\ncases: list[Case[InputsT, OutputT, MetadataT]]\n```\n\nList of test cases in the dataset.\n\n#### evaluators `class-attribute` `instance-attribute`\n\n```\nevaluators: list[Evaluator[InputsT, OutputT, MetadataT]] = (\n    []\n)\n```\n\nList of evaluators to be used on all cases in the dataset.\n\n#### \\_\\_init\\_\\_\n\n```\n__init__(\n    *,\n    name: str | None = None,\n    cases: Sequence[Case[InputsT, OutputT, MetadataT]],\n    evaluators: Sequence[\n        Evaluator[InputsT, OutputT, MetadataT]\n    ] = ()\n)\n```\n\nInitialize a new dataset with test cases and optional evaluators.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `name` | `str | None` | Optional name for the dataset. | `None` |\n| `cases` | `Sequence[Case[InputsT, OutputT, MetadataT]]` | Sequence of test cases to include in the dataset. | *required* |\n| `evaluators` | `Sequence[Evaluator[InputsT, OutputT, MetadataT]]` | Optional sequence of evaluators to apply to all cases in the dataset. | `()` |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`\n\n|  |  |\n| --- | --- |\n| ``` 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 ``` | ``` def __init__(     self,     *,     name: str | None = None,     cases: Sequence[Case[InputsT, OutputT, MetadataT]],     evaluators: Sequence[Evaluator[InputsT, OutputT, MetadataT]] = (), ):     \"\"\"Initialize a new dataset with test cases and optional evaluators.      Args:         name: Optional name for the dataset.         cases: Sequence of test cases to include in the dataset.         evaluators: Optional sequence of evaluators to apply to all cases in the dataset.     \"\"\"     case_names = set[str]()     for case in cases:         if case.name is None:             continue         if case.name in case_names:             raise ValueError(f'Duplicate case name: {case.name!r}')         case_names.add(case.name)      super().__init__(         name=name,         cases=cases,         evaluators=list(evaluators),     ) ``` |\n\n#### evaluate `async`\n\n```\nevaluate(\n    task: (\n        Callable[[InputsT], Awaitable[OutputT]]\n        | Callable[[InputsT], OutputT]\n    ),\n    name: str | None = None,\n    max_concurrency: int | None = None,\n    progress: bool = True,\n    retry_task: RetryConfig | None = None,\n    retry_evaluators: RetryConfig | None = None,\n    *,\n    task_name: str | None = None\n) -> EvaluationReport[InputsT, OutputT, MetadataT]\n```\n\nEvaluates the test cases in the dataset using the given task.\n\nThis method runs the task on each case in the dataset, applies evaluators,\nand collects results into a report. Cases are run concurrently, limited by `max_concurrency` if specified.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `task` | `Callable[[InputsT], Awaitable[OutputT]] | Callable[[InputsT], OutputT]` | The task to evaluate. This should be a callable that takes the inputs of the case and returns the output. | *required* |\n| `name` | `str | None` | The name of the experiment being run, this is used to identify the experiment in the report. If omitted, the task\\_name will be used; if that is not specified, the name of the task function is used. | `None` |\n| `max_concurrency` | `int | None` | The maximum number of concurrent evaluations of the task to allow. If None, all cases will be evaluated concurrently. | `None` |\n| `progress` | `bool` | Whether to show a progress bar for the evaluation. Defaults to `True`. | `True` |\n| `retry_task` | `RetryConfig | None` | Optional retry configuration for the task execution. | `None` |\n| `retry_evaluators` | `RetryConfig | None` | Optional retry configuration for evaluator execution. | `None` |\n| `task_name` | `str | None` | Optional override to the name of the task being executed, otherwise the name of the task function will be used. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `EvaluationReport[InputsT, OutputT, MetadataT]` | A report containing the results of the evaluation. |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`", "url": "https://ai.pydantic.dev/pydantic_evals/dataset/index.html#dataset", "page": "pydantic_evals/dataset/index.html", "source_site": "pydantic_ai"}
{"title": "Dataset", "anchor": "dataset", "md_text": "|  |  |\n| --- | --- |\n| ``` 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 ``` | ``` async def evaluate(     self,     task: Callable[[InputsT], Awaitable[OutputT]] | Callable[[InputsT], OutputT],     name: str | None = None,     max_concurrency: int | None = None,     progress: bool = True,     retry_task: RetryConfig | None = None,     retry_evaluators: RetryConfig | None = None,     *,     task_name: str | None = None, ) -> EvaluationReport[InputsT, OutputT, MetadataT]:     \"\"\"Evaluates the test cases in the dataset using the given task.      This method runs the task on each case in the dataset, applies evaluators,     and collects results into a report. Cases are run concurrently, limited by `max_concurrency` if specified.      Args:         task: The task to evaluate. This should be a callable that takes the inputs of the case             and returns the output.         name: The name of the experiment being run, this is used to identify the experiment in the report.             If omitted, the task_name will be used; if that is not specified, the name of the task function is used.         max_concurrency: The maximum number of concurrent evaluations of the task to allow.             If None, all cases will be evaluated concurrently.         progress: Whether to show a progress bar for the evaluation. Defaults to `True`.         retry_task: Optional retry configuration for the task execution.         retry_evaluators: Optional retry configuration for evaluator execution.         task_name: Optional override to the name of the task being executed, otherwise the name of the task             function will be used.      Returns:         A report containing the results of the evaluation.     \"\"\"     task_name = task_name or get_unwrapped_function_name(task)     name = name or task_name     total_cases = len(self.cases)     progress_bar = Progress() if progress else None      limiter = anyio.Semaphore(max_concurrency) if max_concurrency is not None else AsyncExitStack()      with (         logfire_span(             'evaluate {name}',             name=name,             task_name=task_name,             dataset_name=self.name,             n_cases=len(self.cases),             **{'gen_ai.operation.name': 'experiment'},  # pyright: ignore[reportArgumentType]         ) as eval_span,         progress_bar or nullcontext(),     ):         task_id = progress_bar.add_task(f'Evaluating {task_name}', total=total_cases) if progress_bar else None          async def _handle_case(case: Case[InputsT, OutputT, MetadataT], report_case_name: str):             async with limiter:                 result = await _run_task_and_evaluators(                     task, case, report_case_name, self.evaluators, retry_task, retry_evaluators                 )                 if progress_bar and task_id is not None:  # pragma: no branch                     progress_bar.update(task_id, advance=1)                 return result          if (context := eval_span.context) is None:  # pragma: no cover             trace_id = None             span_id = None         else:             trace_id = f'{context.trace_id:032x}'             span_id = f'{context.span_id:016x}'         cases_and_failures = await task_group_gather(             [                 lambda case=case, i=i: _handle_case(case, case.name or f'Case {i}')                 for i, case in enumerate(self.cases, 1)             ]         )         cases: list[ReportCase] = []         failures: list[ReportCaseFailure] = []         for item in cases_and_failures:             if isinstance(item, ReportCase):                 cases.append(item)             else:                 failures.append(item)         report = EvaluationReport(             name=name,             cases=cases,             failures=failures,             span_id=span_id,             trace_id=trace_id,         )         if (averages := report.averages()) is not None and averages.assertions is not None:             experiment_metadata = {'n_cases': len(self.cases), 'averages': averages}             eval_span.set_attribute('logfire.experiment.metadata', experiment_metadata)             eval_span.set_attribute('assertion_pass_rate', averages.assertions)     return report ``` |\n\n#### evaluate\\_sync\n\n```\nevaluate_sync(\n    task: (\n        Callable[[InputsT], Awaitable[OutputT]]\n        | Callable[[InputsT], OutputT]\n    ),\n    name: str | None = None,\n    max_concurrency: int | None = None,\n    progress: bool = True,\n    retry_task: RetryConfig | None = None,\n    retry_evaluators: RetryConfig | None = None,\n) -> EvaluationReport[InputsT, OutputT, MetadataT]\n```", "url": "https://ai.pydantic.dev/pydantic_evals/dataset/index.html#dataset", "page": "pydantic_evals/dataset/index.html", "source_site": "pydantic_ai"}
{"title": "Dataset", "anchor": "dataset", "md_text": "Evaluates the test cases in the dataset using the given task.\n\nThis is a synchronous wrapper around [`evaluate`](index.html#pydantic_evals.dataset.Dataset.evaluate) provided for convenience.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `task` | `Callable[[InputsT], Awaitable[OutputT]] | Callable[[InputsT], OutputT]` | The task to evaluate. This should be a callable that takes the inputs of the case and returns the output. | *required* |\n| `name` | `str | None` | The name of the task being evaluated, this is used to identify the task in the report. If omitted, the name of the task function will be used. | `None` |\n| `max_concurrency` | `int | None` | The maximum number of concurrent evaluations of the task to allow. If None, all cases will be evaluated concurrently. | `None` |\n| `progress` | `bool` | Whether to show a progress bar for the evaluation. Defaults to True. | `True` |\n| `retry_task` | `RetryConfig | None` | Optional retry configuration for the task execution. | `None` |\n| `retry_evaluators` | `RetryConfig | None` | Optional retry configuration for evaluator execution. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `EvaluationReport[InputsT, OutputT, MetadataT]` | A report containing the results of the evaluation. |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`\n\n|  |  |\n| --- | --- |\n| ``` 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 ``` | ``` def evaluate_sync(     self,     task: Callable[[InputsT], Awaitable[OutputT]] | Callable[[InputsT], OutputT],     name: str | None = None,     max_concurrency: int | None = None,     progress: bool = True,     retry_task: RetryConfig | None = None,     retry_evaluators: RetryConfig | None = None, ) -> EvaluationReport[InputsT, OutputT, MetadataT]:     \"\"\"Evaluates the test cases in the dataset using the given task.      This is a synchronous wrapper around [`evaluate`][pydantic_evals.Dataset.evaluate] provided for convenience.      Args:         task: The task to evaluate. This should be a callable that takes the inputs of the case             and returns the output.         name: The name of the task being evaluated, this is used to identify the task in the report.             If omitted, the name of the task function will be used.         max_concurrency: The maximum number of concurrent evaluations of the task to allow.             If None, all cases will be evaluated concurrently.         progress: Whether to show a progress bar for the evaluation. Defaults to True.         retry_task: Optional retry configuration for the task execution.         retry_evaluators: Optional retry configuration for evaluator execution.      Returns:         A report containing the results of the evaluation.     \"\"\"     return get_event_loop().run_until_complete(         self.evaluate(             task,             task_name=name,             max_concurrency=max_concurrency,             progress=progress,             retry_task=retry_task,             retry_evaluators=retry_evaluators,         )     ) ``` |\n\n#### add\\_case\n\n```\nadd_case(\n    *,\n    name: str | None = None,\n    inputs: InputsT,\n    metadata: MetadataT | None = None,\n    expected_output: OutputT | None = None,\n    evaluators: tuple[\n        Evaluator[InputsT, OutputT, MetadataT], ...\n    ] = ()\n) -> None\n```\n\nAdds a case to the dataset.\n\nThis is a convenience method for creating a [`Case`](index.html#pydantic_evals.dataset.Case) and adding it to the dataset.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `name` | `str | None` | Optional name for the case. If not provided, a generic name will be assigned. | `None` |\n| `inputs` | `InputsT` | The inputs to the task being evaluated. | *required* |\n| `metadata` | `MetadataT | None` | Optional metadata for the case, which can be used by evaluators. | `None` |\n| `expected_output` | `OutputT | None` | The expected output of the task, used for comparison in evaluators. | `None` |\n| `evaluators` | `tuple[Evaluator[InputsT, OutputT, MetadataT], ...]` | Tuple of evaluators specific to this case, in addition to dataset-level evaluators. | `()` |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`", "url": "https://ai.pydantic.dev/pydantic_evals/dataset/index.html#dataset", "page": "pydantic_evals/dataset/index.html", "source_site": "pydantic_ai"}
{"title": "Dataset", "anchor": "dataset", "md_text": "|  |  |\n| --- | --- |\n| ``` 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 ``` | ``` def add_case(     self,     *,     name: str | None = None,     inputs: InputsT,     metadata: MetadataT | None = None,     expected_output: OutputT | None = None,     evaluators: tuple[Evaluator[InputsT, OutputT, MetadataT], ...] = (), ) -> None:     \"\"\"Adds a case to the dataset.      This is a convenience method for creating a [`Case`][pydantic_evals.Case] and adding it to the dataset.      Args:         name: Optional name for the case. If not provided, a generic name will be assigned.         inputs: The inputs to the task being evaluated.         metadata: Optional metadata for the case, which can be used by evaluators.         expected_output: The expected output of the task, used for comparison in evaluators.         evaluators: Tuple of evaluators specific to this case, in addition to dataset-level evaluators.     \"\"\"     if name in {case.name for case in self.cases}:         raise ValueError(f'Duplicate case name: {name!r}')      case = Case[InputsT, OutputT, MetadataT](         name=name,         inputs=inputs,         metadata=metadata,         expected_output=expected_output,         evaluators=evaluators,     )     self.cases.append(case) ``` |\n\n#### add\\_evaluator\n\n```\nadd_evaluator(\n    evaluator: Evaluator[InputsT, OutputT, MetadataT],\n    specific_case: str | None = None,\n) -> None\n```\n\nAdds an evaluator to the dataset or a specific case.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `evaluator` | `Evaluator[InputsT, OutputT, MetadataT]` | The evaluator to add. | *required* |\n| `specific_case` | `str | None` | If provided, the evaluator will only be added to the case with this name. If None, the evaluator will be added to all cases in the dataset. | `None` |\n\nRaises:\n\n| Type | Description |\n| --- | --- |\n| `ValueError` | If `specific_case` is provided but no case with that name exists in the dataset. |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`\n\n|  |  |\n| --- | --- |\n| ``` 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 ``` | ``` def add_evaluator(     self,     evaluator: Evaluator[InputsT, OutputT, MetadataT],     specific_case: str | None = None, ) -> None:     \"\"\"Adds an evaluator to the dataset or a specific case.      Args:         evaluator: The evaluator to add.         specific_case: If provided, the evaluator will only be added to the case with this name.             If None, the evaluator will be added to all cases in the dataset.      Raises:         ValueError: If `specific_case` is provided but no case with that name exists in the dataset.     \"\"\"     if specific_case is None:         self.evaluators.append(evaluator)     else:         # If this is too slow, we could try to add a case lookup dict.         # Note that if we do that, we'd need to make the cases list private to prevent modification.         added = False         for case in self.cases:             if case.name == specific_case:                 case.evaluators.append(evaluator)                 added = True         if not added:             raise ValueError(f'Case {specific_case!r} not found in the dataset') ``` |\n\n#### from\\_file `classmethod`\n\n```\nfrom_file(\n    path: Path | str,\n    fmt: Literal[\"yaml\", \"json\"] | None = None,\n    custom_evaluator_types: Sequence[\n        type[Evaluator[InputsT, OutputT, MetadataT]]\n    ] = (),\n) -> Self\n```\n\nLoad a dataset from a file.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `path` | `Path | str` | Path to the file to load. | *required* |\n| `fmt` | `Literal['yaml', 'json'] | None` | Format of the file. If None, the format will be inferred from the file extension. Must be either 'yaml' or 'json'. | `None` |\n| `custom_evaluator_types` | `Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]]` | Custom evaluator classes to use when deserializing the dataset. These are additional evaluators beyond the default ones. | `()` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `Self` | A new Dataset instance loaded from the file. |\n\nRaises:\n\n| Type | Description |\n| --- | --- |\n| `ValidationError` | If the file cannot be parsed as a valid dataset. |\n| `ValueError` | If the format cannot be inferred from the file extension. |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`", "url": "https://ai.pydantic.dev/pydantic_evals/dataset/index.html#dataset", "page": "pydantic_evals/dataset/index.html", "source_site": "pydantic_ai"}
{"title": "Dataset", "anchor": "dataset", "md_text": "|  |  |\n| --- | --- |\n| ``` 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 ``` | ``` @classmethod def from_file(     cls,     path: Path | str,     fmt: Literal['yaml', 'json'] | None = None,     custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (), ) -> Self:     \"\"\"Load a dataset from a file.      Args:         path: Path to the file to load.         fmt: Format of the file. If None, the format will be inferred from the file extension.             Must be either 'yaml' or 'json'.         custom_evaluator_types: Custom evaluator classes to use when deserializing the dataset.             These are additional evaluators beyond the default ones.      Returns:         A new Dataset instance loaded from the file.      Raises:         ValidationError: If the file cannot be parsed as a valid dataset.         ValueError: If the format cannot be inferred from the file extension.     \"\"\"     path = Path(path)     fmt = cls._infer_fmt(path, fmt)      raw = Path(path).read_text()     try:         return cls.from_text(raw, fmt=fmt, custom_evaluator_types=custom_evaluator_types, default_name=path.stem)     except ValidationError as e:  # pragma: no cover         raise ValueError(f'{path} contains data that does not match the schema for {cls.__name__}:\\n{e}.') from e ``` |\n\n#### from\\_text `classmethod`\n\n```\nfrom_text(\n    contents: str,\n    fmt: Literal[\"yaml\", \"json\"] = \"yaml\",\n    custom_evaluator_types: Sequence[\n        type[Evaluator[InputsT, OutputT, MetadataT]]\n    ] = (),\n    *,\n    default_name: str | None = None\n) -> Self\n```\n\nLoad a dataset from a string.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `contents` | `str` | The string content to parse. | *required* |\n| `fmt` | `Literal['yaml', 'json']` | Format of the content. Must be either 'yaml' or 'json'. | `'yaml'` |\n| `custom_evaluator_types` | `Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]]` | Custom evaluator classes to use when deserializing the dataset. These are additional evaluators beyond the default ones. | `()` |\n| `default_name` | `str | None` | Default name of the dataset, to be used if not specified in the serialized contents. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `Self` | A new Dataset instance parsed from the string. |\n\nRaises:\n\n| Type | Description |\n| --- | --- |\n| `ValidationError` | If the content cannot be parsed as a valid dataset. |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`\n\n|  |  |\n| --- | --- |\n| ``` 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 ``` | ``` @classmethod def from_text(     cls,     contents: str,     fmt: Literal['yaml', 'json'] = 'yaml',     custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),     *,     default_name: str | None = None, ) -> Self:     \"\"\"Load a dataset from a string.      Args:         contents: The string content to parse.         fmt: Format of the content. Must be either 'yaml' or 'json'.         custom_evaluator_types: Custom evaluator classes to use when deserializing the dataset.             These are additional evaluators beyond the default ones.         default_name: Default name of the dataset, to be used if not specified in the serialized contents.      Returns:         A new Dataset instance parsed from the string.      Raises:         ValidationError: If the content cannot be parsed as a valid dataset.     \"\"\"     if fmt == 'yaml':         loaded = yaml.safe_load(contents)         return cls.from_dict(loaded, custom_evaluator_types, default_name=default_name)     else:         dataset_model_type = cls._serialization_type()         dataset_model = dataset_model_type.model_validate_json(contents)         return cls._from_dataset_model(dataset_model, custom_evaluator_types, default_name) ``` |\n\n#### from\\_dict `classmethod`\n\n```\nfrom_dict(\n    data: dict[str, Any],\n    custom_evaluator_types: Sequence[\n        type[Evaluator[InputsT, OutputT, MetadataT]]\n    ] = (),\n    *,\n    default_name: str | None = None\n) -> Self\n```\n\nLoad a dataset from a dictionary.\n\nParameters:", "url": "https://ai.pydantic.dev/pydantic_evals/dataset/index.html#dataset", "page": "pydantic_evals/dataset/index.html", "source_site": "pydantic_ai"}
{"title": "Dataset", "anchor": "dataset", "md_text": "| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `data` | `dict[str, Any]` | Dictionary representation of the dataset. | *required* |\n| `custom_evaluator_types` | `Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]]` | Custom evaluator classes to use when deserializing the dataset. These are additional evaluators beyond the default ones. | `()` |\n| `default_name` | `str | None` | Default name of the dataset, to be used if not specified in the data. | `None` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `Self` | A new Dataset instance created from the dictionary. |\n\nRaises:\n\n| Type | Description |\n| --- | --- |\n| `ValidationError` | If the dictionary cannot be converted to a valid dataset. |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`\n\n|  |  |\n| --- | --- |\n| ``` 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 ``` | ``` @classmethod def from_dict(     cls,     data: dict[str, Any],     custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (),     *,     default_name: str | None = None, ) -> Self:     \"\"\"Load a dataset from a dictionary.      Args:         data: Dictionary representation of the dataset.         custom_evaluator_types: Custom evaluator classes to use when deserializing the dataset.             These are additional evaluators beyond the default ones.         default_name: Default name of the dataset, to be used if not specified in the data.      Returns:         A new Dataset instance created from the dictionary.      Raises:         ValidationError: If the dictionary cannot be converted to a valid dataset.     \"\"\"     dataset_model_type = cls._serialization_type()     dataset_model = dataset_model_type.model_validate(data)     return cls._from_dataset_model(dataset_model, custom_evaluator_types, default_name) ``` |\n\n#### to\\_file\n\n```\nto_file(\n    path: Path | str,\n    fmt: Literal[\"yaml\", \"json\"] | None = None,\n    schema_path: (\n        Path | str | None\n    ) = DEFAULT_SCHEMA_PATH_TEMPLATE,\n    custom_evaluator_types: Sequence[\n        type[Evaluator[InputsT, OutputT, MetadataT]]\n    ] = (),\n)\n```\n\nSave the dataset to a file.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `path` | `Path | str` | Path to save the dataset to. | *required* |\n| `fmt` | `Literal['yaml', 'json'] | None` | Format to use. If None, the format will be inferred from the file extension. Must be either 'yaml' or 'json'. | `None` |\n| `schema_path` | `Path | str | None` | Path to save the JSON schema to. If None, no schema will be saved. Can be a string template with {stem} which will be replaced with the dataset filename stem. | `DEFAULT_SCHEMA_PATH_TEMPLATE` |\n| `custom_evaluator_types` | `Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]]` | Custom evaluator classes to include in the schema. | `()` |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`", "url": "https://ai.pydantic.dev/pydantic_evals/dataset/index.html#dataset", "page": "pydantic_evals/dataset/index.html", "source_site": "pydantic_ai"}
{"title": "Dataset", "anchor": "dataset", "md_text": "|  |  |\n| --- | --- |\n| ``` 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 ``` | ``` def to_file(     self,     path: Path | str,     fmt: Literal['yaml', 'json'] | None = None,     schema_path: Path | str | None = DEFAULT_SCHEMA_PATH_TEMPLATE,     custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (), ):     \"\"\"Save the dataset to a file.      Args:         path: Path to save the dataset to.         fmt: Format to use. If None, the format will be inferred from the file extension.             Must be either 'yaml' or 'json'.         schema_path: Path to save the JSON schema to. If None, no schema will be saved.             Can be a string template with {stem} which will be replaced with the dataset filename stem.         custom_evaluator_types: Custom evaluator classes to include in the schema.     \"\"\"     path = Path(path)     fmt = self._infer_fmt(path, fmt)      schema_ref: str | None = None     if schema_path is not None:  # pragma: no branch         if isinstance(schema_path, str):  # pragma: no branch             schema_path = Path(schema_path.format(stem=path.stem))          if not schema_path.is_absolute():             schema_ref = str(schema_path)             schema_path = path.parent / schema_path         elif schema_path.is_relative_to(path):  # pragma: no cover             schema_ref = str(_get_relative_path_reference(schema_path, path))         else:  # pragma: no cover             schema_ref = str(schema_path)         self._save_schema(schema_path, custom_evaluator_types)      context: dict[str, Any] = {'use_short_form': True}     if fmt == 'yaml':         dumped_data = self.model_dump(mode='json', by_alias=True, context=context)         content = yaml.dump(dumped_data, sort_keys=False)         if schema_ref:  # pragma: no branch             yaml_language_server_line = f'{_YAML_SCHEMA_LINE_PREFIX}{schema_ref}'             content = f'{yaml_language_server_line}\\n{content}'         path.write_text(content)     else:         context['$schema'] = schema_ref         json_data = self.model_dump_json(indent=2, by_alias=True, context=context)         path.write_text(json_data + '\\n') ``` |\n\n#### model\\_json\\_schema\\_with\\_evaluators `classmethod`\n\n```\nmodel_json_schema_with_evaluators(\n    custom_evaluator_types: Sequence[\n        type[Evaluator[InputsT, OutputT, MetadataT]]\n    ] = (),\n) -> dict[str, Any]\n```\n\nGenerate a JSON schema for this dataset type, including evaluator details.\n\nThis is useful for generating a schema that can be used to validate YAML-format dataset files.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `custom_evaluator_types` | `Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]]` | Custom evaluator classes to include in the schema. | `()` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `dict[str, Any]` | A dictionary representing the JSON schema. |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`", "url": "https://ai.pydantic.dev/pydantic_evals/dataset/index.html#dataset", "page": "pydantic_evals/dataset/index.html", "source_site": "pydantic_ai"}
{"title": "Dataset", "anchor": "dataset", "md_text": "|  |  |\n| --- | --- |\n| ``` 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 ``` | ``` @classmethod def model_json_schema_with_evaluators(     cls,     custom_evaluator_types: Sequence[type[Evaluator[InputsT, OutputT, MetadataT]]] = (), ) -> dict[str, Any]:     \"\"\"Generate a JSON schema for this dataset type, including evaluator details.      This is useful for generating a schema that can be used to validate YAML-format dataset files.      Args:         custom_evaluator_types: Custom evaluator classes to include in the schema.      Returns:         A dictionary representing the JSON schema.     \"\"\"     # Note: this function could maybe be simplified now that Evaluators are always dataclasses     registry = _get_registry(custom_evaluator_types)      evaluator_schema_types: list[Any] = []     for name, evaluator_class in registry.items():         type_hints = _typing_extra.get_function_type_hints(evaluator_class)         type_hints.pop('return', None)         required_type_hints: dict[str, Any] = {}          for p in inspect.signature(evaluator_class).parameters.values():             type_hints.setdefault(p.name, Any)             if p.default is not p.empty:                 type_hints[p.name] = NotRequired[type_hints[p.name]]             else:                 required_type_hints[p.name] = type_hints[p.name]          def _make_typed_dict(cls_name_prefix: str, fields: dict[str, Any]) -> Any:             td = TypedDict(f'{cls_name_prefix}_{name}', fields)  # pyright: ignore[reportArgumentType]             config = ConfigDict(extra='forbid', arbitrary_types_allowed=True)             # TODO: Replace with pydantic.with_config once pydantic 2.11 is the min supported version             td.__pydantic_config__ = config  # pyright: ignore[reportAttributeAccessIssue]             return td          # Shortest form: just the call name         if len(type_hints) == 0 or not required_type_hints:             evaluator_schema_types.append(Literal[name])          # Short form: can be called with only one parameter         if len(type_hints) == 1:             [type_hint_type] = type_hints.values()             evaluator_schema_types.append(_make_typed_dict('short_evaluator', {name: type_hint_type}))         elif len(required_type_hints) == 1:  # pragma: no branch             [type_hint_type] = required_type_hints.values()             evaluator_schema_types.append(_make_typed_dict('short_evaluator', {name: type_hint_type}))          # Long form: multiple parameters, possibly required         if len(type_hints) > 1:             params_td = _make_typed_dict('evaluator_params', type_hints)             evaluator_schema_types.append(_make_typed_dict('evaluator', {name: params_td}))      in_type, out_type, meta_type = cls._params()      # Note: we shadow the `Case` and `Dataset` class names here to generate a clean JSON schema     class Case(BaseModel, extra='forbid'):  # pyright: ignore[reportUnusedClass]  # this _is_ used below, but pyright doesn't seem to notice..         name: str | None = None         inputs: in_type  # pyright: ignore[reportInvalidTypeForm]         metadata: meta_type | None = None  # pyright: ignore[reportInvalidTypeForm,reportUnknownVariableType]         expected_output: out_type | None = None  # pyright: ignore[reportInvalidTypeForm,reportUnknownVariableType]         if evaluator_schema_types:  # pragma: no branch             evaluators: list[Union[tuple(evaluator_schema_types)]] = []  # pyright: ignore  # noqa UP007      class Dataset(BaseModel, extra='forbid'):         name: str | None = None         cases: list[Case]         if evaluator_schema_types:  # pragma: no branch             evaluators: list[Union[tuple(evaluator_schema_types)]] = []  # pyright: ignore  # noqa UP007      json_schema = Dataset.model_json_schema()     # See `_add_json_schema` below, since `$schema` is added to the JSON, it has to be supported in the JSON     json_schema['properties']['$schema'] = {'type': 'string'}     return json_schema ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/dataset/index.html#dataset", "page": "pydantic_evals/dataset/index.html", "source_site": "pydantic_ai"}
{"title": "set\\_eval\\_attribute", "anchor": "setevalattribute", "md_text": "```\nset_eval_attribute(name: str, value: Any) -> None\n```\n\nSet an attribute on the current task run.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `name` | `str` | The name of the attribute. | *required* |\n| `value` | `Any` | The value of the attribute. | *required* |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`\n\n|  |  |\n| --- | --- |\n| ``` 1095 1096 1097 1098 1099 1100 1101 1102 1103 1104 ``` | ``` def set_eval_attribute(name: str, value: Any) -> None:     \"\"\"Set an attribute on the current task run.      Args:         name: The name of the attribute.         value: The value of the attribute.     \"\"\"     current_case = _CURRENT_TASK_RUN.get()     if current_case is not None:  # pragma: no branch         current_case.record_attribute(name, value) ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/dataset/index.html#setevalattribute", "page": "pydantic_evals/dataset/index.html", "source_site": "pydantic_ai"}
{"title": "increment\\_eval\\_metric", "anchor": "incrementevalmetric", "md_text": "```\nincrement_eval_metric(\n    name: str, amount: int | float\n) -> None\n```\n\nIncrement a metric on the current task run.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `name` | `str` | The name of the metric. | *required* |\n| `amount` | `int | float` | The amount to increment by. | *required* |\n\nSource code in `pydantic_evals/pydantic_evals/dataset.py`\n\n|  |  |\n| --- | --- |\n| ``` 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 ``` | ``` def increment_eval_metric(name: str, amount: int | float) -> None:     \"\"\"Increment a metric on the current task run.      Args:         name: The name of the metric.         amount: The amount to increment by.     \"\"\"     current_case = _CURRENT_TASK_RUN.get()     if current_case is not None:  # pragma: no branch         current_case.increment_metric(name, amount) ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/dataset/index.html#incrementevalmetric", "page": "pydantic_evals/dataset/index.html", "source_site": "pydantic_ai"}
{"title": "SpanNode `dataclass`", "anchor": "spannode-dataclass", "md_text": "A node in the span tree; provides references to parents/children for easy traversal and queries.\n\nSource code in `pydantic_evals/pydantic_evals/otel/span_tree.py`", "url": "https://ai.pydantic.dev/pydantic_evals/otel/index.html#spannode-dataclass", "page": "pydantic_evals/otel/index.html", "source_site": "pydantic_ai"}
{"title": "SpanNode `dataclass`", "anchor": "spannode-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ```  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 ``` | ``` @dataclass(repr=False, kw_only=True) class SpanNode:     \"\"\"A node in the span tree; provides references to parents/children for easy traversal and queries.\"\"\"      name: str     trace_id: int     span_id: int     parent_span_id: int | None     start_timestamp: datetime     end_timestamp: datetime     attributes: dict[str, AttributeValue]      @property     def duration(self) -> timedelta:         \"\"\"Return the span's duration as a timedelta, or None if start/end not set.\"\"\"         return self.end_timestamp - self.start_timestamp      @property     def children(self) -> list[SpanNode]:         return list(self.children_by_id.values())      @property     def descendants(self) -> list[SpanNode]:         \"\"\"Return all descendants of this node in DFS order.\"\"\"         return self.find_descendants(lambda _: True)      @property     def ancestors(self) -> list[SpanNode]:         \"\"\"Return all ancestors of this node.\"\"\"         return self.find_ancestors(lambda _: True)      @property     def node_key(self) -> str:         return f'{self.trace_id:032x}:{self.span_id:016x}'      @property     def parent_node_key(self) -> str | None:         return None if self.parent_span_id is None else f'{self.trace_id:032x}:{self.parent_span_id:016x}'      # -------------------------------------------------------------------------     # Construction     # -------------------------------------------------------------------------     def __post_init__(self):         self.parent: SpanNode | None = None         self.children_by_id: dict[str, SpanNode] = {}      @staticmethod     def from_readable_span(span: ReadableSpan) -> SpanNode:         assert span.context is not None, 'Span has no context'         assert span.start_time is not None, 'Span has no start time'         assert span.end_time is not None, 'Span has no end time'         return SpanNode(             name=span.name,             trace_id=span.context.trace_id,             span_id=span.context.span_id,             parent_span_id=span.parent.span_id if span.parent else None,             start_timestamp=datetime.fromtimestamp(span.start_time / 1e9, tz=timezone.utc),             end_timestamp=datetime.fromtimestamp(span.end_time / 1e9, tz=timezone.utc),             attributes=dict(span.attributes or {}),         )      def add_child(self, child: SpanNode) -> None:         \"\"\"Attach a child node to this node's list of children.\"\"\"         assert child.trace_id == self.trace_id, f\"traces don't match: {child.trace_id:032x} != {self.trace_id:032x}\"         assert child.parent_span_id == self.span_id, (             f'parent span mismatch: {child.parent_span_id:016x} != {self.span_id:016x}'         )         self.children_by_id[child.node_key] = child         child.parent = self      # -------------------------------------------------------------------------     # Child queries     # -------------------------------------------------------------------------     def find_children(self, predicate: SpanQuery | SpanPredicate) -> list[SpanNode]:         \"\"\"Return all immediate children that satisfy the given predicate.\"\"\"         return list(self._filter_children(predicate))      def first_child(self, predicate: SpanQuery | SpanPredicate) -> SpanNode | None:         \"\"\"Return the first immediate child that satisfies the given predicate, or None if none match.\"\"\"         return next(self._filter_children(predicate), None)      def any_child(self, predicate: SpanQuery | SpanPredicate) -> bool:         \"\"\"Returns True if there is at least one child that satisfies the predicate.\"\"\"         return self.first_child(predicate) is not None      def _filter_children(self, predicate: SpanQuery | SpanPredicate) -> Iterator[SpanNode]:         return (child for child in self.children if child.matches(predicate))      # -------------------------------------------------------------------------     # Descendant queries (DFS)     # -------------------------------------------------------------------------     def find_descendants(         self, predicate: SpanQuery | SpanPredicate, stop_recursing_when: SpanQuery | SpanPredicate | None = None     ) -> list[SpanNode]:         \"\"\"Return all descendant nodes that satisfy the given predicate in DFS order.\"\"\"         return list(self._filter_descendants(predicate, stop_recursing_when))      def first_descendant(         self, predicate: SpanQuery | SpanPredicate, stop_recursing_when: SpanQuery | SpanPredicate | None = None     ) -> SpanNode | None:         \"\"\"DFS: Return the first descendant (in DFS order) that satisfies the given predicate, or `None` if none match.\"\"\"         return next(self._filter_descendants(predicate, stop_recursing_when), None)      def any_descendant(         self, predicate: SpanQuery | SpanPredicate, stop_recursing_when: SpanQuery | SpanPredicate | None = None     ) -> bool:         \"\"\"Returns `True` if there is at least one descendant that satisfies the predicate.\"\"\"         return self.first_descendant(predicate, stop_recursing_when) is not None      def _filter_descendants(         self, predicate: SpanQuery | SpanPredicate, stop_recursing_when: SpanQuery | SpanPredicate | None     ) -> Iterator[SpanNode]:         stack = list(self.children)         while stack:             node = stack.pop()             if node.matches(predicate):                 yield node             if stop_recursing_when is not None and node.matches(stop_recursing_when):                 continue             stack.extend(node.children)      # -------------------------------------------------------------------------     # Ancestor queries (DFS \"up\" the chain)     # -------------------------------------------------------------------------     def find_ancestors(         self, predicate: SpanQuery | SpanPredicate, stop_recursing_when: SpanQuery | SpanPredicate | None = None     ) -> list[SpanNode]:         \"\"\"Return all ancestors that satisfy the given predicate.\"\"\"         return list(self._filter_ancestors(predicate, stop_recursing_when))      def first_ancestor(         self, predicate: SpanQuery | SpanPredicate, stop_recursing_when: SpanQuery | SpanPredicate | None = None     ) -> SpanNode | None:         \"\"\"Return the closest ancestor that satisfies the given predicate, or `None` if none match.\"\"\"         return next(self._filter_ancestors(predicate, stop_recursing_when), None)      def any_ancestor(         self, predicate: SpanQuery | SpanPredicate, stop_recursing_when: SpanQuery | SpanPredicate | None = None     ) -> bool:         \"\"\"Returns True if any ancestor satisfies the predicate.\"\"\"         return self.first_ancestor(predicate, stop_recursing_when) is not None      def _filter_ancestors(         self, predicate: SpanQuery | SpanPredicate, stop_recursing_when: SpanQuery | SpanPredicate | None     ) -> Iterator[SpanNode]:         node = self.parent         while node:             if node.matches(predicate):                 yield node             if stop_recursing_when is not None and node.matches(stop_recursing_when):                 break             node = node.parent      # -------------------------------------------------------------------------     # Query matching     # -------------------------------------------------------------------------     def matches(self, query: SpanQuery | SpanPredicate) -> bool:         \"\"\"Check if the span node matches the query conditions or predicate.\"\"\"         if callable(query):             return query(self)          return self._matches_query(query)      def _matches_query(self, query: SpanQuery) -> bool:  # noqa C901         \"\"\"Check if the span matches the query conditions.\"\"\"         # Logical combinations         if or_ := query.get('or_'):             if len(query) > 1:                 raise ValueError(\"Cannot combine 'or_' conditions with other conditions at the same level\")             return any(self._matches_query(q) for q in or_)         if not_ := query.get('not_'):             if self._matches_query(not_):                 return False         if and_ := query.get('and_'):             results = [self._matches_query(q) for q in and_]             if not all(results):                 return False         # At this point, all existing ANDs and no existing ORs have passed, so it comes down to this condition          # Name conditions         if (name_equals := query.get('name_equals')) and self.name != name_equals:             return False         if (name_contains := query.get('name_contains')) and name_contains not in self.name:             return False         if (name_matches_regex := query.get('name_matches_regex')) and not re.match(name_matches_regex, self.name):             return False          # Attribute conditions         if (has_attributes := query.get('has_attributes')) and not all(             self.attributes.get(key) == value for key, value in has_attributes.items()         ):             return False         if (has_attributes_keys := query.get('has_attribute_keys')) and not all(             key in self.attributes for key in has_attributes_keys         ):             return False          # Timing conditions         if (min_duration := query.get('min_duration')) is not None:             if not isinstance(min_duration, timedelta):                 min_duration = timedelta(seconds=min_duration)             if self.duration < min_duration:                 return False         if (max_duration := query.get('max_duration')) is not None:             if not isinstance(max_duration, timedelta):                 max_duration = timedelta(seconds=max_duration)             if self.duration > max_duration:                 return False          # Children conditions         if (min_child_count := query.get('min_child_count')) and len(self.children) < min_child_count:             return False         if (max_child_count := query.get('max_child_count')) and len(self.children) > max_child_count:             return False         if (some_child_has := query.get('some_child_has')) and not any(             child._matches_query(some_child_has) for child in self.children         ):             return False         if (all_children_have := query.get('all_children_have')) and not all(             child._matches_query(all_children_have) for child in self.children         ):             return False         if (no_child_has := query.get('no_child_has')) and any(             child._matches_query(no_child_has) for child in self.children         ):             return False          # Descendant conditions         # The following local functions with cache decorators are used to avoid repeatedly evaluating these properties         @cache         def descendants():             return self.descendants          @cache         def pruned_descendants():             stop_recursing_when = query.get('stop_recursing_when')             return (                 self._filter_descendants(lambda _: True, stop_recursing_when) if stop_recursing_when else descendants()             )          if (min_descendant_count := query.get('min_descendant_count')) and len(descendants()) < min_descendant_count:             return False         if (max_descendant_count := query.get('max_descendant_count')) and len(descendants()) > max_descendant_count:             return False         if (some_descendant_has := query.get('some_descendant_has')) and not any(             descendant._matches_query(some_descendant_has) for descendant in pruned_descendants()         ):             return False         if (all_descendants_have := query.get('all_descendants_have')) and not all(             descendant._matches_query(all_descendants_have) for descendant in pruned_descendants()         ):             return False         if (no_descendant_has := query.get('no_descendant_has')) and any(             descendant._matches_query(no_descendant_has) for descendant in pruned_descendants()         ):             return False          # Ancestor conditions         # The following local functions with cache decorators are used to avoid repeatedly evaluating these properties         @cache         def ancestors():             return self.ancestors          @cache         def pruned_ancestors():             stop_recursing_when = query.get('stop_recursing_when')             return self._filter_ancestors(lambda _: True, stop_recursing_when) if stop_recursing_when else ancestors()          if (min_depth := query.get('min_depth')) and len(ancestors()) < min_depth:             return False         if (max_depth := query.get('max_depth')) and len(ancestors()) > max_depth:             return False         if (some_ancestor_has := query.get('some_ancestor_has')) and not any(             ancestor._matches_query(some_ancestor_has) for ancestor in pruned_ancestors()         ):             return False         if (all_ancestors_have := query.get('all_ancestors_have')) and not all(             ancestor._matches_query(all_ancestors_have) for ancestor in pruned_ancestors()         ):             return False         if (no_ancestor_has := query.get('no_ancestor_has')) and any(             ancestor._matches_query(no_ancestor_has) for ancestor in pruned_ancestors()         ):             return False          return True      # -------------------------------------------------------------------------     # String representation     # -------------------------------------------------------------------------     def repr_xml(         self,         include_children: bool = True,         include_trace_id: bool = False,         include_span_id: bool = False,         include_start_timestamp: bool = False,         include_duration: bool = False,     ) -> str:         \"\"\"Return an XML-like string representation of the node.          Optionally includes children, trace_id, span_id, start_timestamp, and duration.         \"\"\"         first_line_parts = [f'<SpanNode name={self.name!r}']         if include_trace_id:             first_line_parts.append(f\"trace_id='{self.trace_id:032x}'\")         if include_span_id:             first_line_parts.append(f\"span_id='{self.span_id:016x}'\")         if include_start_timestamp:             first_line_parts.append(f'start_timestamp={self.start_timestamp.isoformat()!r}')         if include_duration:             first_line_parts.append(f\"duration='{self.duration}'\")          extra_lines: list[str] = []         if include_children and self.children:             first_line_parts.append('>')             for child in self.children:                 extra_lines.append(                     indent(                         child.repr_xml(                             include_children=include_children,                             include_trace_id=include_trace_id,                             include_span_id=include_span_id,                             include_start_timestamp=include_start_timestamp,                             include_duration=include_duration,                         ),                         '  ',                     )                 )             extra_lines.append('</SpanNode>')         else:             if self.children:                 first_line_parts.append('children=...')             first_line_parts.append('/>')         return '\\n'.join([' '.join(first_line_parts), *extra_lines])      def __str__(self) -> str:         if self.children:             return f\"<SpanNode name={self.name!r} span_id='{self.span_id:016x}'>...</SpanNode>\"         else:             return f\"<SpanNode name={self.name!r} span_id='{self.span_id:016x}' />\"      def __repr__(self) -> str:         return self.repr_xml() ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/otel/index.html#spannode-dataclass", "page": "pydantic_evals/otel/index.html", "source_site": "pydantic_ai"}
{"title": "SpanNode `dataclass`", "anchor": "spannode-dataclass", "md_text": "#### duration `property`\n\n```\nduration: timedelta\n```\n\nReturn the span's duration as a timedelta, or None if start/end not set.\n\n#### descendants `property`\n\n```\ndescendants: list[SpanNode]\n```\n\nReturn all descendants of this node in DFS order.\n\n#### ancestors `property`\n\n```\nancestors: list[SpanNode]\n```\n\nReturn all ancestors of this node.\n\n#### add\\_child\n\n```\nadd_child(child: SpanNode) -> None\n```\n\nAttach a child node to this node's list of children.\n\nSource code in `pydantic_evals/pydantic_evals/otel/span_tree.py`\n\n|  |  |\n| --- | --- |\n| ``` 142 143 144 145 146 147 148 149 ``` | ``` def add_child(self, child: SpanNode) -> None:     \"\"\"Attach a child node to this node's list of children.\"\"\"     assert child.trace_id == self.trace_id, f\"traces don't match: {child.trace_id:032x} != {self.trace_id:032x}\"     assert child.parent_span_id == self.span_id, (         f'parent span mismatch: {child.parent_span_id:016x} != {self.span_id:016x}'     )     self.children_by_id[child.node_key] = child     child.parent = self ``` |\n\n#### find\\_children\n\n```\nfind_children(\n    predicate: SpanQuery | SpanPredicate,\n) -> list[SpanNode]\n```\n\nReturn all immediate children that satisfy the given predicate.\n\nSource code in `pydantic_evals/pydantic_evals/otel/span_tree.py`\n\n|  |  |\n| --- | --- |\n| ``` 154 155 156 ``` | ``` def find_children(self, predicate: SpanQuery | SpanPredicate) -> list[SpanNode]:     \"\"\"Return all immediate children that satisfy the given predicate.\"\"\"     return list(self._filter_children(predicate)) ``` |\n\n#### first\\_child\n\n```\nfirst_child(\n    predicate: SpanQuery | SpanPredicate,\n) -> SpanNode | None\n```\n\nReturn the first immediate child that satisfies the given predicate, or None if none match.\n\nSource code in `pydantic_evals/pydantic_evals/otel/span_tree.py`\n\n|  |  |\n| --- | --- |\n| ``` 158 159 160 ``` | ``` def first_child(self, predicate: SpanQuery | SpanPredicate) -> SpanNode | None:     \"\"\"Return the first immediate child that satisfies the given predicate, or None if none match.\"\"\"     return next(self._filter_children(predicate), None) ``` |\n\n#### any\\_child\n\n```\nany_child(predicate: SpanQuery | SpanPredicate) -> bool\n```\n\nReturns True if there is at least one child that satisfies the predicate.\n\nSource code in `pydantic_evals/pydantic_evals/otel/span_tree.py`\n\n|  |  |\n| --- | --- |\n| ``` 162 163 164 ``` | ``` def any_child(self, predicate: SpanQuery | SpanPredicate) -> bool:     \"\"\"Returns True if there is at least one child that satisfies the predicate.\"\"\"     return self.first_child(predicate) is not None ``` |\n\n#### find\\_descendants\n\n```\nfind_descendants(\n    predicate: SpanQuery | SpanPredicate,\n    stop_recursing_when: (\n        SpanQuery | SpanPredicate | None\n    ) = None,\n) -> list[SpanNode]\n```\n\nReturn all descendant nodes that satisfy the given predicate in DFS order.\n\nSource code in `pydantic_evals/pydantic_evals/otel/span_tree.py`\n\n|  |  |\n| --- | --- |\n| ``` 172 173 174 175 176 ``` | ``` def find_descendants(     self, predicate: SpanQuery | SpanPredicate, stop_recursing_when: SpanQuery | SpanPredicate | None = None ) -> list[SpanNode]:     \"\"\"Return all descendant nodes that satisfy the given predicate in DFS order.\"\"\"     return list(self._filter_descendants(predicate, stop_recursing_when)) ``` |\n\n#### first\\_descendant\n\n```\nfirst_descendant(\n    predicate: SpanQuery | SpanPredicate,\n    stop_recursing_when: (\n        SpanQuery | SpanPredicate | None\n    ) = None,\n) -> SpanNode | None\n```\n\nDFS: Return the first descendant (in DFS order) that satisfies the given predicate, or `None` if none match.\n\nSource code in `pydantic_evals/pydantic_evals/otel/span_tree.py`\n\n|  |  |\n| --- | --- |\n| ``` 178 179 180 181 182 ``` | ``` def first_descendant(     self, predicate: SpanQuery | SpanPredicate, stop_recursing_when: SpanQuery | SpanPredicate | None = None ) -> SpanNode | None:     \"\"\"DFS: Return the first descendant (in DFS order) that satisfies the given predicate, or `None` if none match.\"\"\"     return next(self._filter_descendants(predicate, stop_recursing_when), None) ``` |\n\n#### any\\_descendant\n\n```\nany_descendant(\n    predicate: SpanQuery | SpanPredicate,\n    stop_recursing_when: (\n        SpanQuery | SpanPredicate | None\n    ) = None,\n) -> bool\n```\n\nReturns `True` if there is at least one descendant that satisfies the predicate.\n\nSource code in `pydantic_evals/pydantic_evals/otel/span_tree.py`", "url": "https://ai.pydantic.dev/pydantic_evals/otel/index.html#spannode-dataclass", "page": "pydantic_evals/otel/index.html", "source_site": "pydantic_ai"}
{"title": "SpanNode `dataclass`", "anchor": "spannode-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 184 185 186 187 188 ``` | ``` def any_descendant(     self, predicate: SpanQuery | SpanPredicate, stop_recursing_when: SpanQuery | SpanPredicate | None = None ) -> bool:     \"\"\"Returns `True` if there is at least one descendant that satisfies the predicate.\"\"\"     return self.first_descendant(predicate, stop_recursing_when) is not None ``` |\n\n#### find\\_ancestors\n\n```\nfind_ancestors(\n    predicate: SpanQuery | SpanPredicate,\n    stop_recursing_when: (\n        SpanQuery | SpanPredicate | None\n    ) = None,\n) -> list[SpanNode]\n```\n\nReturn all ancestors that satisfy the given predicate.\n\nSource code in `pydantic_evals/pydantic_evals/otel/span_tree.py`\n\n|  |  |\n| --- | --- |\n| ``` 205 206 207 208 209 ``` | ``` def find_ancestors(     self, predicate: SpanQuery | SpanPredicate, stop_recursing_when: SpanQuery | SpanPredicate | None = None ) -> list[SpanNode]:     \"\"\"Return all ancestors that satisfy the given predicate.\"\"\"     return list(self._filter_ancestors(predicate, stop_recursing_when)) ``` |\n\n#### first\\_ancestor\n\n```\nfirst_ancestor(\n    predicate: SpanQuery | SpanPredicate,\n    stop_recursing_when: (\n        SpanQuery | SpanPredicate | None\n    ) = None,\n) -> SpanNode | None\n```\n\nReturn the closest ancestor that satisfies the given predicate, or `None` if none match.\n\nSource code in `pydantic_evals/pydantic_evals/otel/span_tree.py`\n\n|  |  |\n| --- | --- |\n| ``` 211 212 213 214 215 ``` | ``` def first_ancestor(     self, predicate: SpanQuery | SpanPredicate, stop_recursing_when: SpanQuery | SpanPredicate | None = None ) -> SpanNode | None:     \"\"\"Return the closest ancestor that satisfies the given predicate, or `None` if none match.\"\"\"     return next(self._filter_ancestors(predicate, stop_recursing_when), None) ``` |\n\n#### any\\_ancestor\n\n```\nany_ancestor(\n    predicate: SpanQuery | SpanPredicate,\n    stop_recursing_when: (\n        SpanQuery | SpanPredicate | None\n    ) = None,\n) -> bool\n```\n\nReturns True if any ancestor satisfies the predicate.\n\nSource code in `pydantic_evals/pydantic_evals/otel/span_tree.py`\n\n|  |  |\n| --- | --- |\n| ``` 217 218 219 220 221 ``` | ``` def any_ancestor(     self, predicate: SpanQuery | SpanPredicate, stop_recursing_when: SpanQuery | SpanPredicate | None = None ) -> bool:     \"\"\"Returns True if any ancestor satisfies the predicate.\"\"\"     return self.first_ancestor(predicate, stop_recursing_when) is not None ``` |\n\n#### matches\n\n```\nmatches(query: SpanQuery | SpanPredicate) -> bool\n```\n\nCheck if the span node matches the query conditions or predicate.\n\nSource code in `pydantic_evals/pydantic_evals/otel/span_tree.py`\n\n|  |  |\n| --- | --- |\n| ``` 237 238 239 240 241 242 ``` | ``` def matches(self, query: SpanQuery | SpanPredicate) -> bool:     \"\"\"Check if the span node matches the query conditions or predicate.\"\"\"     if callable(query):         return query(self)      return self._matches_query(query) ``` |\n\n#### repr\\_xml\n\n```\nrepr_xml(\n    include_children: bool = True,\n    include_trace_id: bool = False,\n    include_span_id: bool = False,\n    include_start_timestamp: bool = False,\n    include_duration: bool = False,\n) -> str\n```\n\nReturn an XML-like string representation of the node.\n\nOptionally includes children, trace\\_id, span\\_id, start\\_timestamp, and duration.\n\nSource code in `pydantic_evals/pydantic_evals/otel/span_tree.py`", "url": "https://ai.pydantic.dev/pydantic_evals/otel/index.html#spannode-dataclass", "page": "pydantic_evals/otel/index.html", "source_site": "pydantic_ai"}
{"title": "SpanNode `dataclass`", "anchor": "spannode-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 ``` | ``` def repr_xml(     self,     include_children: bool = True,     include_trace_id: bool = False,     include_span_id: bool = False,     include_start_timestamp: bool = False,     include_duration: bool = False, ) -> str:     \"\"\"Return an XML-like string representation of the node.      Optionally includes children, trace_id, span_id, start_timestamp, and duration.     \"\"\"     first_line_parts = [f'<SpanNode name={self.name!r}']     if include_trace_id:         first_line_parts.append(f\"trace_id='{self.trace_id:032x}'\")     if include_span_id:         first_line_parts.append(f\"span_id='{self.span_id:016x}'\")     if include_start_timestamp:         first_line_parts.append(f'start_timestamp={self.start_timestamp.isoformat()!r}')     if include_duration:         first_line_parts.append(f\"duration='{self.duration}'\")      extra_lines: list[str] = []     if include_children and self.children:         first_line_parts.append('>')         for child in self.children:             extra_lines.append(                 indent(                     child.repr_xml(                         include_children=include_children,                         include_trace_id=include_trace_id,                         include_span_id=include_span_id,                         include_start_timestamp=include_start_timestamp,                         include_duration=include_duration,                     ),                     '  ',                 )             )         extra_lines.append('</SpanNode>')     else:         if self.children:             first_line_parts.append('children=...')         first_line_parts.append('/>')     return '\\n'.join([' '.join(first_line_parts), *extra_lines]) ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/otel/index.html#spannode-dataclass", "page": "pydantic_evals/otel/index.html", "source_site": "pydantic_ai"}
{"title": "SpanQuery", "anchor": "spanquery", "md_text": "Bases: `TypedDict`\n\nA serializable query for filtering SpanNodes based on various conditions.\n\nAll fields are optional and combined with AND logic by default.\n\nSource code in `pydantic_evals/pydantic_evals/otel/span_tree.py`\n\n|  |  |\n| --- | --- |\n| ``` 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 ``` | ``` class SpanQuery(TypedDict, total=False):     \"\"\"A serializable query for filtering SpanNodes based on various conditions.      All fields are optional and combined with AND logic by default.     \"\"\"      # These fields are ordered to match the implementation of SpanNode.matches_query for easy review.     # * Individual span conditions come first because these are generally the cheapest to evaluate     # * Logical combinations come next because they may just be combinations of individual span conditions     # * Related-span conditions come last because they may require the most work to evaluate      # Individual span conditions     ## Name conditions     name_equals: str     name_contains: str     name_matches_regex: str  # regex pattern      ## Attribute conditions     has_attributes: dict[str, Any]     has_attribute_keys: list[str]      ## Timing conditions     min_duration: timedelta | float     max_duration: timedelta | float      # Logical combinations of conditions     not_: SpanQuery     and_: list[SpanQuery]     or_: list[SpanQuery]      # Child conditions     min_child_count: int     max_child_count: int     some_child_has: SpanQuery     all_children_have: SpanQuery     no_child_has: SpanQuery      # Recursive conditions     stop_recursing_when: SpanQuery     \"\"\"If present, stop recursing through ancestors or descendants at nodes that match this condition.\"\"\"      ## Descendant conditions     min_descendant_count: int     max_descendant_count: int     some_descendant_has: SpanQuery     all_descendants_have: SpanQuery     no_descendant_has: SpanQuery      ## Ancestor conditions     min_depth: int  # depth is equivalent to ancestor count; roots have depth 0     max_depth: int     some_ancestor_has: SpanQuery     all_ancestors_have: SpanQuery     no_ancestor_has: SpanQuery ``` |\n\n#### stop\\_recursing\\_when `instance-attribute`\n\n```\nstop_recursing_when: SpanQuery\n```\n\nIf present, stop recursing through ancestors or descendants at nodes that match this condition.", "url": "https://ai.pydantic.dev/pydantic_evals/otel/index.html#spanquery", "page": "pydantic_evals/otel/index.html", "source_site": "pydantic_ai"}
{"title": "SpanTree `dataclass`", "anchor": "spantree-dataclass", "md_text": "A container that builds a hierarchy of SpanNode objects from a list of finished spans.\n\nYou can then search or iterate the tree to make your assertions (using DFS for traversal).\n\nSource code in `pydantic_evals/pydantic_evals/otel/span_tree.py`\n\n|  |  |\n| --- | --- |\n| ``` 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 ``` | ``` @dataclass(repr=False, kw_only=True) class SpanTree:     \"\"\"A container that builds a hierarchy of SpanNode objects from a list of finished spans.      You can then search or iterate the tree to make your assertions (using DFS for traversal).     \"\"\"      roots: list[SpanNode] = field(default_factory=list)     nodes_by_id: dict[str, SpanNode] = field(default_factory=dict)      # -------------------------------------------------------------------------     # Construction     # -------------------------------------------------------------------------     def __post_init__(self):         self._rebuild_tree()      def add_spans(self, spans: list[SpanNode]) -> None:         \"\"\"Add a list of spans to the tree, rebuilding the tree structure.\"\"\"         for span in spans:             self.nodes_by_id[span.node_key] = span         self._rebuild_tree()      def add_readable_spans(self, readable_spans: list[ReadableSpan]):         self.add_spans([SpanNode.from_readable_span(span) for span in readable_spans])      def _rebuild_tree(self):         # Ensure spans are ordered by start_timestamp so that roots and children end up in the right order         nodes = list(self.nodes_by_id.values())         nodes.sort(key=lambda node: node.start_timestamp or datetime.min)         self.nodes_by_id = {node.node_key: node for node in nodes}          # Build the parent/child relationships         for node in self.nodes_by_id.values():             parent_node_key = node.parent_node_key             if parent_node_key is not None:                 parent_node = self.nodes_by_id.get(parent_node_key)                 if parent_node is not None:                     parent_node.add_child(node)          # Determine the roots         # A node is a \"root\" if its parent is None or if its parent's span_id is not in the current set of spans.         self.roots = []         for node in self.nodes_by_id.values():             parent_node_key = node.parent_node_key             if parent_node_key is None or parent_node_key not in self.nodes_by_id:                 self.roots.append(node)      # -------------------------------------------------------------------------     # Node filtering and iteration     # -------------------------------------------------------------------------     def find(self, predicate: SpanQuery | SpanPredicate) -> list[SpanNode]:         \"\"\"Find all nodes in the entire tree that match the predicate, scanning from each root in DFS order.\"\"\"         return list(self._filter(predicate))      def first(self, predicate: SpanQuery | SpanPredicate) -> SpanNode | None:         \"\"\"Find the first node that matches a predicate, scanning from each root in DFS order. Returns `None` if not found.\"\"\"         return next(self._filter(predicate), None)      def any(self, predicate: SpanQuery | SpanPredicate) -> bool:         \"\"\"Returns True if any node in the tree matches the predicate.\"\"\"         return self.first(predicate) is not None      def _filter(self, predicate: SpanQuery | SpanPredicate) -> Iterator[SpanNode]:         for node in self:             if node.matches(predicate):                 yield node      def __iter__(self) -> Iterator[SpanNode]:         \"\"\"Return an iterator over all nodes in the tree.\"\"\"         return iter(self.nodes_by_id.values())      # -------------------------------------------------------------------------     # String representation     # -------------------------------------------------------------------------     def repr_xml(         self,         include_children: bool = True,         include_trace_id: bool = False,         include_span_id: bool = False,         include_start_timestamp: bool = False,         include_duration: bool = False,     ) -> str:         \"\"\"Return an XML-like string representation of the tree, optionally including children, trace_id, span_id, duration, and timestamps.\"\"\"         if not self.roots:             return '<SpanTree />'         repr_parts = [             '<SpanTree>',             *[                 indent(                     root.repr_xml(                         include_children=include_children,                         include_trace_id=include_trace_id,                         include_span_id=include_span_id,                         include_start_timestamp=include_start_timestamp,                         include_duration=include_duration,                     ),                     '  ',                 )                 for root in self.roots             ],             '</SpanTree>',         ]         return '\\n'.join(repr_parts)      def __str__(self):         return f'<SpanTree num_roots={len(self.roots)} total_spans={len(self.nodes_by_id)} />'      def __repr__(self):         return self.repr_xml() ``` |\n\n#### add\\_spans", "url": "https://ai.pydantic.dev/pydantic_evals/otel/index.html#spantree-dataclass", "page": "pydantic_evals/otel/index.html", "source_site": "pydantic_ai"}
{"title": "SpanTree `dataclass`", "anchor": "spantree-dataclass", "md_text": "```\nadd_spans(spans: list[SpanNode]) -> None\n```\n\nAdd a list of spans to the tree, rebuilding the tree structure.\n\nSource code in `pydantic_evals/pydantic_evals/otel/span_tree.py`\n\n|  |  |\n| --- | --- |\n| ``` 445 446 447 448 449 ``` | ``` def add_spans(self, spans: list[SpanNode]) -> None:     \"\"\"Add a list of spans to the tree, rebuilding the tree structure.\"\"\"     for span in spans:         self.nodes_by_id[span.node_key] = span     self._rebuild_tree() ``` |\n\n#### find\n\n```\nfind(\n    predicate: SpanQuery | SpanPredicate,\n) -> list[SpanNode]\n```\n\nFind all nodes in the entire tree that match the predicate, scanning from each root in DFS order.\n\nSource code in `pydantic_evals/pydantic_evals/otel/span_tree.py`\n\n|  |  |\n| --- | --- |\n| ``` 479 480 481 ``` | ``` def find(self, predicate: SpanQuery | SpanPredicate) -> list[SpanNode]:     \"\"\"Find all nodes in the entire tree that match the predicate, scanning from each root in DFS order.\"\"\"     return list(self._filter(predicate)) ``` |\n\n#### first\n\n```\nfirst(\n    predicate: SpanQuery | SpanPredicate,\n) -> SpanNode | None\n```\n\nFind the first node that matches a predicate, scanning from each root in DFS order. Returns `None` if not found.\n\nSource code in `pydantic_evals/pydantic_evals/otel/span_tree.py`\n\n|  |  |\n| --- | --- |\n| ``` 483 484 485 ``` | ``` def first(self, predicate: SpanQuery | SpanPredicate) -> SpanNode | None:     \"\"\"Find the first node that matches a predicate, scanning from each root in DFS order. Returns `None` if not found.\"\"\"     return next(self._filter(predicate), None) ``` |\n\n#### any\n\n```\nany(predicate: SpanQuery | SpanPredicate) -> bool\n```\n\nReturns True if any node in the tree matches the predicate.\n\nSource code in `pydantic_evals/pydantic_evals/otel/span_tree.py`\n\n|  |  |\n| --- | --- |\n| ``` 487 488 489 ``` | ``` def any(self, predicate: SpanQuery | SpanPredicate) -> bool:     \"\"\"Returns True if any node in the tree matches the predicate.\"\"\"     return self.first(predicate) is not None ``` |\n\n#### \\_\\_iter\\_\\_\n\n```\n__iter__() -> Iterator[SpanNode]\n```\n\nReturn an iterator over all nodes in the tree.\n\nSource code in `pydantic_evals/pydantic_evals/otel/span_tree.py`\n\n|  |  |\n| --- | --- |\n| ``` 496 497 498 ``` | ``` def __iter__(self) -> Iterator[SpanNode]:     \"\"\"Return an iterator over all nodes in the tree.\"\"\"     return iter(self.nodes_by_id.values()) ``` |\n\n#### repr\\_xml\n\n```\nrepr_xml(\n    include_children: bool = True,\n    include_trace_id: bool = False,\n    include_span_id: bool = False,\n    include_start_timestamp: bool = False,\n    include_duration: bool = False,\n) -> str\n```\n\nReturn an XML-like string representation of the tree, optionally including children, trace\\_id, span\\_id, duration, and timestamps.\n\nSource code in `pydantic_evals/pydantic_evals/otel/span_tree.py`\n\n|  |  |\n| --- | --- |\n| ``` 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 ``` | ``` def repr_xml(     self,     include_children: bool = True,     include_trace_id: bool = False,     include_span_id: bool = False,     include_start_timestamp: bool = False,     include_duration: bool = False, ) -> str:     \"\"\"Return an XML-like string representation of the tree, optionally including children, trace_id, span_id, duration, and timestamps.\"\"\"     if not self.roots:         return '<SpanTree />'     repr_parts = [         '<SpanTree>',         *[             indent(                 root.repr_xml(                     include_children=include_children,                     include_trace_id=include_trace_id,                     include_span_id=include_span_id,                     include_start_timestamp=include_start_timestamp,                     include_duration=include_duration,                 ),                 '  ',             )             for root in self.roots         ],         '</SpanTree>',     ]     return '\\n'.join(repr_parts) ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/otel/index.html#spantree-dataclass", "page": "pydantic_evals/otel/index.html", "source_site": "pydantic_ai"}
{"title": "Contains `dataclass`", "anchor": "contains-dataclass", "md_text": "Bases: `Evaluator[object, object, object]`\n\nCheck if the output contains the expected output.\n\nFor strings, checks if expected\\_output is a substring of output.\nFor lists/tuples, checks if expected\\_output is in output.\nFor dicts, checks if all key-value pairs in expected\\_output are in output.\n\nNote: case\\_sensitive only applies when both the value and output are strings.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/common.py`\n\n|  |  |\n| --- | --- |\n| ```  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 ``` | ``` @dataclass(repr=False) class Contains(Evaluator[object, object, object]):     \"\"\"Check if the output contains the expected output.      For strings, checks if expected_output is a substring of output.     For lists/tuples, checks if expected_output is in output.     For dicts, checks if all key-value pairs in expected_output are in output.      Note: case_sensitive only applies when both the value and output are strings.     \"\"\"      value: Any     case_sensitive: bool = True     as_strings: bool = False     evaluation_name: str | None = field(default=None)      def evaluate(         self,         ctx: EvaluatorContext[object, object, object],     ) -> EvaluationReason:         # Convert objects to strings if requested         failure_reason: str | None = None         as_strings = self.as_strings or (isinstance(self.value, str) and isinstance(ctx.output, str))         if as_strings:             output_str = str(ctx.output)             expected_str = str(self.value)              if not self.case_sensitive:                 output_str = output_str.lower()                 expected_str = expected_str.lower()              failure_reason: str | None = None             if expected_str not in output_str:                 output_trunc = _truncated_repr(output_str, max_length=100)                 expected_trunc = _truncated_repr(expected_str, max_length=100)                 failure_reason = f'Output string {output_trunc} does not contain expected string {expected_trunc}'             return EvaluationReason(value=failure_reason is None, reason=failure_reason)          try:             # Handle different collection types             if isinstance(ctx.output, dict):                 if isinstance(self.value, dict):                     # Cast to Any to avoid type checking issues                     output_dict = cast(dict[Any, Any], ctx.output)  # pyright: ignore[reportUnknownMemberType]                     expected_dict = cast(dict[Any, Any], self.value)  # pyright: ignore[reportUnknownMemberType]                     for k in expected_dict:                         if k not in output_dict:                             k_trunc = _truncated_repr(k, max_length=30)                             failure_reason = f'Output dictionary does not contain expected key {k_trunc}'                             break                         elif output_dict[k] != expected_dict[k]:                             k_trunc = _truncated_repr(k, max_length=30)                             output_v_trunc = _truncated_repr(output_dict[k], max_length=100)                             expected_v_trunc = _truncated_repr(expected_dict[k], max_length=100)                             failure_reason = f'Output dictionary has different value for key {k_trunc}: {output_v_trunc} != {expected_v_trunc}'                             break                 else:                     if self.value not in ctx.output:  # pyright: ignore[reportUnknownMemberType]                         output_trunc = _truncated_repr(ctx.output, max_length=200)  # pyright: ignore[reportUnknownMemberType]                         failure_reason = f'Output {output_trunc} does not contain provided value as a key'             elif self.value not in ctx.output:  # pyright: ignore[reportOperatorIssue]  # will be handled by except block                 output_trunc = _truncated_repr(ctx.output, max_length=200)                 failure_reason = f'Output {output_trunc} does not contain provided value'         except (TypeError, ValueError) as e:             failure_reason = f'Containment check failed: {e}'          return EvaluationReason(value=failure_reason is None, reason=failure_reason) ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#contains-dataclass", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "Equals `dataclass`", "anchor": "equals-dataclass", "md_text": "Bases: `Evaluator[object, object, object]`\n\nCheck if the output exactly equals the provided value.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/common.py`\n\n|  |  |\n| --- | --- |\n| ``` 28 29 30 31 32 33 34 35 36 ``` | ``` @dataclass(repr=False) class Equals(Evaluator[object, object, object]):     \"\"\"Check if the output exactly equals the provided value.\"\"\"      value: Any     evaluation_name: str | None = field(default=None)      def evaluate(self, ctx: EvaluatorContext[object, object, object]) -> bool:         return ctx.output == self.value ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#equals-dataclass", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "EqualsExpected `dataclass`", "anchor": "equalsexpected-dataclass", "md_text": "Bases: `Evaluator[object, object, object]`\n\nCheck if the output exactly equals the expected output.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/common.py`\n\n|  |  |\n| --- | --- |\n| ``` 39 40 41 42 43 44 45 46 47 48 ``` | ``` @dataclass(repr=False) class EqualsExpected(Evaluator[object, object, object]):     \"\"\"Check if the output exactly equals the expected output.\"\"\"      evaluation_name: str | None = field(default=None)      def evaluate(self, ctx: EvaluatorContext[object, object, object]) -> bool | dict[str, bool]:         if ctx.expected_output is None:             return {}  # Only compare if expected output is provided         return ctx.output == ctx.expected_output ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#equalsexpected-dataclass", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "HasMatchingSpan `dataclass`", "anchor": "hasmatchingspan-dataclass", "md_text": "Bases: `Evaluator[object, object, object]`\n\nCheck if the span tree contains a span that matches the specified query.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/common.py`\n\n|  |  |\n| --- | --- |\n| ``` 256 257 258 259 260 261 262 263 264 265 266 267 ``` | ``` @dataclass(repr=False) class HasMatchingSpan(Evaluator[object, object, object]):     \"\"\"Check if the span tree contains a span that matches the specified query.\"\"\"      query: SpanQuery     evaluation_name: str | None = field(default=None)      def evaluate(         self,         ctx: EvaluatorContext[object, object, object],     ) -> bool:         return ctx.span_tree.any(self.query) ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#hasmatchingspan-dataclass", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "IsInstance `dataclass`", "anchor": "isinstance-dataclass", "md_text": "Bases: `Evaluator[object, object, object]`\n\nCheck if the output is an instance of a type with the given name.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/common.py`\n\n|  |  |\n| --- | --- |\n| ``` 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 ``` | ``` @dataclass(repr=False) class IsInstance(Evaluator[object, object, object]):     \"\"\"Check if the output is an instance of a type with the given name.\"\"\"      type_name: str     evaluation_name: str | None = field(default=None)      def evaluate(self, ctx: EvaluatorContext[object, object, object]) -> EvaluationReason:         output = ctx.output         for cls in type(output).__mro__:             if cls.__name__ == self.type_name or cls.__qualname__ == self.type_name:                 return EvaluationReason(value=True)          reason = f'output is of type {type(output).__name__}'         if type(output).__qualname__ != type(output).__name__:             reason += f' (qualname: {type(output).__qualname__})'         return EvaluationReason(value=False, reason=reason) ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#isinstance-dataclass", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "LLMJudge `dataclass`", "anchor": "llmjudge-dataclass", "md_text": "Bases: `Evaluator[object, object, object]`\n\nJudge whether the output of a language model meets the criteria of a provided rubric.\n\nIf you do not specify a model, it uses the default model for judging. This starts as 'openai:gpt-4o', but can be\noverridden by calling [`set_default_judge_model`](index.html#pydantic_evals.evaluators.llm_as_a_judge.set_default_judge_model).\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/common.py`\n\n|  |  |\n| --- | --- |\n| ``` 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 ``` | ``` @dataclass(repr=False) class LLMJudge(Evaluator[object, object, object]):     \"\"\"Judge whether the output of a language model meets the criteria of a provided rubric.      If you do not specify a model, it uses the default model for judging. This starts as 'openai:gpt-4o', but can be     overridden by calling [`set_default_judge_model`][pydantic_evals.evaluators.llm_as_a_judge.set_default_judge_model].     \"\"\"      rubric: str     model: models.Model | models.KnownModelName | None = None     include_input: bool = False     include_expected_output: bool = False     model_settings: ModelSettings | None = None     score: OutputConfig | Literal[False] = False     assertion: OutputConfig | Literal[False] = field(default_factory=lambda: OutputConfig(include_reason=True))      async def evaluate(         self,         ctx: EvaluatorContext[object, object, object],     ) -> EvaluatorOutput:         if self.include_input:             if self.include_expected_output:                 from .llm_as_a_judge import judge_input_output_expected                  grading_output = await judge_input_output_expected(                     ctx.inputs, ctx.output, ctx.expected_output, self.rubric, self.model, self.model_settings                 )             else:                 from .llm_as_a_judge import judge_input_output                  grading_output = await judge_input_output(                     ctx.inputs, ctx.output, self.rubric, self.model, self.model_settings                 )         else:             if self.include_expected_output:                 from .llm_as_a_judge import judge_output_expected                  grading_output = await judge_output_expected(                     ctx.output, ctx.expected_output, self.rubric, self.model, self.model_settings                 )             else:                 from .llm_as_a_judge import judge_output                  grading_output = await judge_output(ctx.output, self.rubric, self.model, self.model_settings)          output: dict[str, EvaluationScalar | EvaluationReason] = {}         include_both = self.score is not False and self.assertion is not False         evaluation_name = self.get_default_evaluation_name()          if self.score is not False:             default_name = f'{evaluation_name}_score' if include_both else evaluation_name             _update_combined_output(output, grading_output.score, grading_output.reason, self.score, default_name)          if self.assertion is not False:             default_name = f'{evaluation_name}_pass' if include_both else evaluation_name             _update_combined_output(output, grading_output.pass_, grading_output.reason, self.assertion, default_name)          return output      def build_serialization_arguments(self):         result = super().build_serialization_arguments()         # always serialize the model as a string when present; use its name if it's a KnownModelName         if (model := result.get('model')) and isinstance(model, models.Model):  # pragma: no branch             result['model'] = f'{model.system}:{model.model_name}'          # Note: this may lead to confusion if you try to serialize-then-deserialize with a custom model.         # I expect that is rare enough to be worth not solving yet, but common enough that we probably will want to         # solve it eventually. I'm imagining some kind of model registry, but don't want to work out the details yet.         return result ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#llmjudge-dataclass", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "MaxDuration `dataclass`", "anchor": "maxduration-dataclass", "md_text": "Bases: `Evaluator[object, object, object]`\n\nCheck if the execution time is under the specified maximum.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/common.py`\n\n|  |  |\n| --- | --- |\n| ``` 150 151 152 153 154 155 156 157 158 159 160 161 ``` | ``` @dataclass(repr=False) class MaxDuration(Evaluator[object, object, object]):     \"\"\"Check if the execution time is under the specified maximum.\"\"\"      seconds: float | timedelta      def evaluate(self, ctx: EvaluatorContext[object, object, object]) -> bool:         duration = timedelta(seconds=ctx.duration)         seconds = self.seconds         if not isinstance(seconds, timedelta):             seconds = timedelta(seconds=seconds)         return duration <= seconds ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#maxduration-dataclass", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "OutputConfig", "anchor": "outputconfig", "md_text": "Bases: `TypedDict`\n\nConfiguration for the score and assertion outputs of the LLMJudge evaluator.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/common.py`\n\n|  |  |\n| --- | --- |\n| ``` 164 165 166 167 168 ``` | ``` class OutputConfig(TypedDict, total=False):     \"\"\"Configuration for the score and assertion outputs of the LLMJudge evaluator.\"\"\"      evaluation_name: str     include_reason: bool ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#outputconfig", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluatorContext `dataclass`", "anchor": "evaluatorcontext-dataclass", "md_text": "Bases: `Generic[InputsT, OutputT, MetadataT]`\n\nContext for evaluating a task execution.\n\nAn instance of this class is the sole input to all Evaluators. It contains all the information\nneeded to evaluate the task execution, including inputs, outputs, metadata, and telemetry data.\n\nEvaluators use this context to access the task inputs, actual output, expected output, and other\ninformation when evaluating the result of the task execution.\n\nExample:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass ExactMatch(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        # Use the context to access task inputs, outputs, and expected outputs\n        return ctx.output == ctx.expected_output\n```\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/context.py`\n\n|  |  |\n| --- | --- |\n| ```  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 ``` | ``` @dataclass(kw_only=True) class EvaluatorContext(Generic[InputsT, OutputT, MetadataT]):     \"\"\"Context for evaluating a task execution.      An instance of this class is the sole input to all Evaluators. It contains all the information     needed to evaluate the task execution, including inputs, outputs, metadata, and telemetry data.      Evaluators use this context to access the task inputs, actual output, expected output, and other     information when evaluating the result of the task execution.      Example:     ```python     from dataclasses import dataclass      from pydantic_evals.evaluators import Evaluator, EvaluatorContext       @dataclass     class ExactMatch(Evaluator):         def evaluate(self, ctx: EvaluatorContext) -> bool:             # Use the context to access task inputs, outputs, and expected outputs             return ctx.output == ctx.expected_output     ```     \"\"\"      name: str | None     \"\"\"The name of the case.\"\"\"     inputs: InputsT     \"\"\"The inputs provided to the task for this case.\"\"\"     metadata: MetadataT | None     \"\"\"Metadata associated with the case, if provided. May be None if no metadata was specified.\"\"\"     expected_output: OutputT | None     \"\"\"The expected output for the case, if provided. May be None if no expected output was specified.\"\"\"      output: OutputT     \"\"\"The actual output produced by the task for this case.\"\"\"     duration: float     \"\"\"The duration of the task run for this case.\"\"\"     _span_tree: SpanTree | SpanTreeRecordingError = field(repr=False)     \"\"\"The span tree for the task run for this case.      This will be `None` if `logfire.configure` has not been called.     \"\"\"      attributes: dict[str, Any]     \"\"\"Attributes associated with the task run for this case.      These can be set by calling `pydantic_evals.dataset.set_eval_attribute` in any code executed     during the evaluation task.\"\"\"     metrics: dict[str, int | float]     \"\"\"Metrics associated with the task run for this case.      These can be set by calling `pydantic_evals.dataset.increment_eval_metric` in any code executed     during the evaluation task.\"\"\"      @property     def span_tree(self) -> SpanTree:         \"\"\"Get the `SpanTree` for this task execution.          The span tree is a graph where each node corresponds to an OpenTelemetry span recorded during the task         execution, including timing information and any custom spans created during execution.          Returns:             The span tree for the task execution.          Raises:             SpanTreeRecordingError: If spans were not captured during execution of the task, e.g. due to not having                 the necessary dependencies installed.         \"\"\"         if isinstance(self._span_tree, SpanTreeRecordingError):             # In this case, there was a reason we couldn't record the SpanTree. We raise that now             raise self._span_tree         return self._span_tree ``` |\n\n#### name `instance-attribute`\n\n```\nname: str | None\n```\n\nThe name of the case.\n\n#### inputs `instance-attribute`\n\n```\ninputs: InputsT\n```\n\nThe inputs provided to the task for this case.\n\n#### metadata `instance-attribute`\n\n```\nmetadata: MetadataT | None\n```\n\nMetadata associated with the case, if provided. May be None if no metadata was specified.\n\n#### expected\\_output `instance-attribute`\n\n```\nexpected_output: OutputT | None\n```\n\nThe expected output for the case, if provided. May be None if no expected output was specified.\n\n#### output `instance-attribute`\n\n```\noutput: OutputT\n```\n\nThe actual output produced by the task for this case.\n\n#### duration `instance-attribute`\n\n```\nduration: float\n```", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#evaluatorcontext-dataclass", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluatorContext `dataclass`", "anchor": "evaluatorcontext-dataclass", "md_text": "The duration of the task run for this case.\n\n#### attributes `instance-attribute`\n\n```\nattributes: dict[str, Any]\n```\n\nAttributes associated with the task run for this case.\n\nThese can be set by calling `pydantic_evals.dataset.set_eval_attribute` in any code executed\nduring the evaluation task.\n\n#### metrics `instance-attribute`\n\n```\nmetrics: dict[str, int | float]\n```\n\nMetrics associated with the task run for this case.\n\nThese can be set by calling `pydantic_evals.dataset.increment_eval_metric` in any code executed\nduring the evaluation task.\n\n#### span\\_tree `property`\n\n```\nspan_tree: SpanTree\n```\n\nGet the `SpanTree` for this task execution.\n\nThe span tree is a graph where each node corresponds to an OpenTelemetry span recorded during the task\nexecution, including timing information and any custom spans created during execution.\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `SpanTree` | The span tree for the task execution. |\n\nRaises:\n\n| Type | Description |\n| --- | --- |\n| `SpanTreeRecordingError` | If spans were not captured during execution of the task, e.g. due to not having the necessary dependencies installed. |", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#evaluatorcontext-dataclass", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluationReason `dataclass`", "anchor": "evaluationreason-dataclass", "md_text": "The result of running an evaluator with an optional explanation.\n\nContains a scalar value and an optional \"reason\" explaining the value.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `value` | `EvaluationScalar` | The scalar result of the evaluation (boolean, integer, float, or string). | *required* |\n| `reason` | `str | None` | An optional explanation of the evaluation result. | `None` |\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`\n\n|  |  |\n| --- | --- |\n| ``` 40 41 42 43 44 45 46 47 48 49 50 51 52 ``` | ``` @dataclass class EvaluationReason:     \"\"\"The result of running an evaluator with an optional explanation.      Contains a scalar value and an optional \"reason\" explaining the value.      Args:         value: The scalar result of the evaluation (boolean, integer, float, or string).         reason: An optional explanation of the evaluation result.     \"\"\"      value: EvaluationScalar     reason: str | None = None ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#evaluationreason-dataclass", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluationResult `dataclass`", "anchor": "evaluationresult-dataclass", "md_text": "Bases: `Generic[EvaluationScalarT]`\n\nThe details of an individual evaluation result.\n\nContains the name, value, reason, and source evaluator for a single evaluation.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `name` | `str` | The name of the evaluation. | *required* |\n| `value` | `EvaluationScalarT` | The scalar result of the evaluation. | *required* |\n| `reason` | `str | None` | An optional explanation of the evaluation result. | *required* |\n| `source` | `EvaluatorSpec` | The spec of the evaluator that produced this result. | *required* |\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`\n\n|  |  |\n| --- | --- |\n| ```  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 ``` | ``` @dataclass class EvaluationResult(Generic[EvaluationScalarT]):     \"\"\"The details of an individual evaluation result.      Contains the name, value, reason, and source evaluator for a single evaluation.      Args:         name: The name of the evaluation.         value: The scalar result of the evaluation.         reason: An optional explanation of the evaluation result.         source: The spec of the evaluator that produced this result.     \"\"\"      name: str     value: EvaluationScalarT     reason: str | None     source: EvaluatorSpec      def downcast(self, *value_types: type[T]) -> EvaluationResult[T] | None:         \"\"\"Attempt to downcast this result to a more specific type.          Args:             *value_types: The types to check the value against.          Returns:             A downcast version of this result if the value is an instance of one of the given types,             otherwise None.         \"\"\"         # Check if value matches any of the target types, handling bool as a special case         for value_type in value_types:             if isinstance(self.value, value_type):                 # Only match bool with explicit bool type                 if isinstance(self.value, bool) and value_type is not bool:                     continue                 return cast(EvaluationResult[T], self)         return None ``` |\n\n#### downcast\n\n```\ndowncast(\n    *value_types: type[T],\n) -> EvaluationResult[T] | None\n```\n\nAttempt to downcast this result to a more specific type.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `*value_types` | `type[T]` | The types to check the value against. | `()` |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `EvaluationResult[T] | None` | A downcast version of this result if the value is an instance of one of the given types, |\n| `EvaluationResult[T] | None` | otherwise None. |\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`\n\n|  |  |\n| --- | --- |\n| ```  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 ``` | ``` def downcast(self, *value_types: type[T]) -> EvaluationResult[T] | None:     \"\"\"Attempt to downcast this result to a more specific type.      Args:         *value_types: The types to check the value against.      Returns:         A downcast version of this result if the value is an instance of one of the given types,         otherwise None.     \"\"\"     # Check if value matches any of the target types, handling bool as a special case     for value_type in value_types:         if isinstance(self.value, value_type):             # Only match bool with explicit bool type             if isinstance(self.value, bool) and value_type is not bool:                 continue             return cast(EvaluationResult[T], self)     return None ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#evaluationresult-dataclass", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluator `dataclass`", "anchor": "evaluator-dataclass", "md_text": "Bases: `Generic[InputsT, OutputT, MetadataT]`\n\nBase class for all evaluators.\n\nEvaluators can assess the performance of a task in a variety of ways, as a function of the EvaluatorContext.\n\nSubclasses must implement the `evaluate` method. Note it can be defined with either `def` or `async def`.\n\nExample:\n\n```\nfrom dataclasses import dataclass\n\nfrom pydantic_evals.evaluators import Evaluator, EvaluatorContext\n\n\n@dataclass\nclass ExactMatch(Evaluator):\n    def evaluate(self, ctx: EvaluatorContext) -> bool:\n        return ctx.output == ctx.expected_output\n```\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#evaluator-dataclass", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluator `dataclass`", "anchor": "evaluator-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 ``` | ``` @dataclass(repr=False) class Evaluator(Generic[InputsT, OutputT, MetadataT], metaclass=_StrictABCMeta):     \"\"\"Base class for all evaluators.      Evaluators can assess the performance of a task in a variety of ways, as a function of the EvaluatorContext.      Subclasses must implement the `evaluate` method. Note it can be defined with either `def` or `async def`.      Example:     ```python     from dataclasses import dataclass      from pydantic_evals.evaluators import Evaluator, EvaluatorContext       @dataclass     class ExactMatch(Evaluator):         def evaluate(self, ctx: EvaluatorContext) -> bool:             return ctx.output == ctx.expected_output     ```     \"\"\"      __pydantic_config__ = ConfigDict(arbitrary_types_allowed=True)      @classmethod     def get_serialization_name(cls) -> str:         \"\"\"Return the 'name' of this Evaluator to use during serialization.          Returns:             The name of the Evaluator, which is typically the class name.         \"\"\"         return cls.__name__      @classmethod     @deprecated('`name` has been renamed, use `get_serialization_name` instead.')     def name(cls) -> str:         \"\"\"`name` has been renamed, use `get_serialization_name` instead.\"\"\"         return cls.get_serialization_name()      def get_default_evaluation_name(self) -> str:         \"\"\"Return the default name to use in reports for the output of this evaluator.          By default, if the evaluator has an attribute called `evaluation_name` of type string, that will be used.         Otherwise, the serialization name of the evaluator (which is usually the class name) will be used.          This can be overridden to get a more descriptive name in evaluation reports, e.g. using instance information.          Note that evaluators that return a mapping of results will always use the keys of that mapping as the names         of the associated evaluation results.         \"\"\"         evaluation_name = getattr(self, 'evaluation_name', None)         if isinstance(evaluation_name, str):             # If the evaluator has an attribute `name` of type string, use that             return evaluation_name          return self.get_serialization_name()      @abstractmethod     def evaluate(         self, ctx: EvaluatorContext[InputsT, OutputT, MetadataT]     ) -> EvaluatorOutput | Awaitable[EvaluatorOutput]:  # pragma: no cover         \"\"\"Evaluate the task output in the given context.          This is the main evaluation method that subclasses must implement. It can be either synchronous         or asynchronous, returning either an EvaluatorOutput directly or an Awaitable[EvaluatorOutput].          Args:             ctx: The context containing the inputs, outputs, and metadata for evaluation.          Returns:             The evaluation result, which can be a scalar value, an EvaluationReason, or a mapping             of evaluation names to either of those. Can be returned either synchronously or as an             awaitable for asynchronous evaluation.         \"\"\"         raise NotImplementedError('You must implement `evaluate`.')      def evaluate_sync(self, ctx: EvaluatorContext[InputsT, OutputT, MetadataT]) -> EvaluatorOutput:         \"\"\"Run the evaluator synchronously, handling both sync and async implementations.          This method ensures synchronous execution by running any async evaluate implementation         to completion using run_until_complete.          Args:             ctx: The context containing the inputs, outputs, and metadata for evaluation.          Returns:             The evaluation result, which can be a scalar value, an EvaluationReason, or a mapping             of evaluation names to either of those.         \"\"\"         output = self.evaluate(ctx)         if inspect.iscoroutine(output):  # pragma: no cover             return get_event_loop().run_until_complete(output)         else:             return cast(EvaluatorOutput, output)      async def evaluate_async(self, ctx: EvaluatorContext[InputsT, OutputT, MetadataT]) -> EvaluatorOutput:         \"\"\"Run the evaluator asynchronously, handling both sync and async implementations.          This method ensures asynchronous execution by properly awaiting any async evaluate         implementation. For synchronous implementations, it returns the result directly.          Args:             ctx: The context containing the inputs, outputs, and metadata for evaluation.          Returns:             The evaluation result, which can be a scalar value, an EvaluationReason, or a mapping             of evaluation names to either of those.         \"\"\"         # Note: If self.evaluate is synchronous, but you need to prevent this from blocking, override this method with:         # return await anyio.to_thread.run_sync(self.evaluate, ctx)         output = self.evaluate(ctx)         if inspect.iscoroutine(output):             return await output         else:             return cast(EvaluatorOutput, output)      @model_serializer(mode='plain')     def serialize(self, info: SerializationInfo) -> Any:         \"\"\"Serialize this Evaluator to a JSON-serializable form.          Returns:             A JSON-serializable representation of this evaluator as an EvaluatorSpec.         \"\"\"         return to_jsonable_python(             self.as_spec(),             context=info.context,             serialize_unknown=True,         )      def as_spec(self) -> EvaluatorSpec:         raw_arguments = self.build_serialization_arguments()          arguments: None | tuple[Any,] | dict[str, Any]         if len(raw_arguments) == 0:             arguments = None         elif len(raw_arguments) == 1:             arguments = (next(iter(raw_arguments.values())),)         else:             arguments = raw_arguments          return EvaluatorSpec(name=self.get_serialization_name(), arguments=arguments)      def build_serialization_arguments(self) -> dict[str, Any]:         \"\"\"Build the arguments for serialization.          Evaluators are serialized for inclusion as the \"source\" in an `EvaluationResult`.         If you want to modify how the evaluator is serialized for that or other purposes, you can override this method.          Returns:             A dictionary of arguments to be used during serialization.         \"\"\"         raw_arguments: dict[str, Any] = {}         for field in fields(self):             value = getattr(self, field.name)             # always exclude defaults:             if field.default is not MISSING:                 if value == field.default:                     continue             if field.default_factory is not MISSING:                 if value == field.default_factory():  # pragma: no branch                     continue             raw_arguments[field.name] = value         return raw_arguments      __repr__ = _utils.dataclasses_no_defaults_repr ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#evaluator-dataclass", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluator `dataclass`", "anchor": "evaluator-dataclass", "md_text": "#### get\\_serialization\\_name `classmethod`\n\n```\nget_serialization_name() -> str\n```\n\nReturn the 'name' of this Evaluator to use during serialization.\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `str` | The name of the Evaluator, which is typically the class name. |\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`\n\n|  |  |\n| --- | --- |\n| ``` 162 163 164 165 166 167 168 169 ``` | ``` @classmethod def get_serialization_name(cls) -> str:     \"\"\"Return the 'name' of this Evaluator to use during serialization.      Returns:         The name of the Evaluator, which is typically the class name.     \"\"\"     return cls.__name__ ``` |\n\n#### name `classmethod` `deprecated`\n\n```\nname() -> str\n```\n\nDeprecated\n\n`name` has been renamed, use `get_serialization_name` instead.\n\n`name` has been renamed, use `get_serialization_name` instead.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`\n\n|  |  |\n| --- | --- |\n| ``` 171 172 173 174 175 ``` | ``` @classmethod @deprecated('`name` has been renamed, use `get_serialization_name` instead.') def name(cls) -> str:     \"\"\"`name` has been renamed, use `get_serialization_name` instead.\"\"\"     return cls.get_serialization_name() ``` |\n\n#### get\\_default\\_evaluation\\_name\n\n```\nget_default_evaluation_name() -> str\n```\n\nReturn the default name to use in reports for the output of this evaluator.\n\nBy default, if the evaluator has an attribute called `evaluation_name` of type string, that will be used.\nOtherwise, the serialization name of the evaluator (which is usually the class name) will be used.\n\nThis can be overridden to get a more descriptive name in evaluation reports, e.g. using instance information.\n\nNote that evaluators that return a mapping of results will always use the keys of that mapping as the names\nof the associated evaluation results.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`\n\n|  |  |\n| --- | --- |\n| ``` 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 ``` | ``` def get_default_evaluation_name(self) -> str:     \"\"\"Return the default name to use in reports for the output of this evaluator.      By default, if the evaluator has an attribute called `evaluation_name` of type string, that will be used.     Otherwise, the serialization name of the evaluator (which is usually the class name) will be used.      This can be overridden to get a more descriptive name in evaluation reports, e.g. using instance information.      Note that evaluators that return a mapping of results will always use the keys of that mapping as the names     of the associated evaluation results.     \"\"\"     evaluation_name = getattr(self, 'evaluation_name', None)     if isinstance(evaluation_name, str):         # If the evaluator has an attribute `name` of type string, use that         return evaluation_name      return self.get_serialization_name() ``` |\n\n#### evaluate `abstractmethod`\n\n```\nevaluate(\n    ctx: EvaluatorContext[InputsT, OutputT, MetadataT],\n) -> EvaluatorOutput | Awaitable[EvaluatorOutput]\n```\n\nEvaluate the task output in the given context.\n\nThis is the main evaluation method that subclasses must implement. It can be either synchronous\nor asynchronous, returning either an EvaluatorOutput directly or an Awaitable[EvaluatorOutput].\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `ctx` | `EvaluatorContext[InputsT, OutputT, MetadataT]` | The context containing the inputs, outputs, and metadata for evaluation. | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `EvaluatorOutput | Awaitable[EvaluatorOutput]` | The evaluation result, which can be a scalar value, an EvaluationReason, or a mapping |\n| `EvaluatorOutput | Awaitable[EvaluatorOutput]` | of evaluation names to either of those. Can be returned either synchronously or as an |\n| `EvaluatorOutput | Awaitable[EvaluatorOutput]` | awaitable for asynchronous evaluation. |\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`\n\n|  |  |\n| --- | --- |\n| ``` 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 ``` | ``` @abstractmethod def evaluate(     self, ctx: EvaluatorContext[InputsT, OutputT, MetadataT] ) -> EvaluatorOutput | Awaitable[EvaluatorOutput]:  # pragma: no cover     \"\"\"Evaluate the task output in the given context.      This is the main evaluation method that subclasses must implement. It can be either synchronous     or asynchronous, returning either an EvaluatorOutput directly or an Awaitable[EvaluatorOutput].      Args:         ctx: The context containing the inputs, outputs, and metadata for evaluation.      Returns:         The evaluation result, which can be a scalar value, an EvaluationReason, or a mapping         of evaluation names to either of those. Can be returned either synchronously or as an         awaitable for asynchronous evaluation.     \"\"\"     raise NotImplementedError('You must implement `evaluate`.') ``` |\n\n#### evaluate\\_sync", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#evaluator-dataclass", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluator `dataclass`", "anchor": "evaluator-dataclass", "md_text": "```\nevaluate_sync(\n    ctx: EvaluatorContext[InputsT, OutputT, MetadataT],\n) -> EvaluatorOutput\n```\n\nRun the evaluator synchronously, handling both sync and async implementations.\n\nThis method ensures synchronous execution by running any async evaluate implementation\nto completion using run\\_until\\_complete.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `ctx` | `EvaluatorContext[InputsT, OutputT, MetadataT]` | The context containing the inputs, outputs, and metadata for evaluation. | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `EvaluatorOutput` | The evaluation result, which can be a scalar value, an EvaluationReason, or a mapping |\n| `EvaluatorOutput` | of evaluation names to either of those. |\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`\n\n|  |  |\n| --- | --- |\n| ``` 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 ``` | ``` def evaluate_sync(self, ctx: EvaluatorContext[InputsT, OutputT, MetadataT]) -> EvaluatorOutput:     \"\"\"Run the evaluator synchronously, handling both sync and async implementations.      This method ensures synchronous execution by running any async evaluate implementation     to completion using run_until_complete.      Args:         ctx: The context containing the inputs, outputs, and metadata for evaluation.      Returns:         The evaluation result, which can be a scalar value, an EvaluationReason, or a mapping         of evaluation names to either of those.     \"\"\"     output = self.evaluate(ctx)     if inspect.iscoroutine(output):  # pragma: no cover         return get_event_loop().run_until_complete(output)     else:         return cast(EvaluatorOutput, output) ``` |\n\n#### evaluate\\_async `async`\n\n```\nevaluate_async(\n    ctx: EvaluatorContext[InputsT, OutputT, MetadataT],\n) -> EvaluatorOutput\n```\n\nRun the evaluator asynchronously, handling both sync and async implementations.\n\nThis method ensures asynchronous execution by properly awaiting any async evaluate\nimplementation. For synchronous implementations, it returns the result directly.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `ctx` | `EvaluatorContext[InputsT, OutputT, MetadataT]` | The context containing the inputs, outputs, and metadata for evaluation. | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `EvaluatorOutput` | The evaluation result, which can be a scalar value, an EvaluationReason, or a mapping |\n| `EvaluatorOutput` | of evaluation names to either of those. |\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`\n\n|  |  |\n| --- | --- |\n| ``` 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 ``` | ``` async def evaluate_async(self, ctx: EvaluatorContext[InputsT, OutputT, MetadataT]) -> EvaluatorOutput:     \"\"\"Run the evaluator asynchronously, handling both sync and async implementations.      This method ensures asynchronous execution by properly awaiting any async evaluate     implementation. For synchronous implementations, it returns the result directly.      Args:         ctx: The context containing the inputs, outputs, and metadata for evaluation.      Returns:         The evaluation result, which can be a scalar value, an EvaluationReason, or a mapping         of evaluation names to either of those.     \"\"\"     # Note: If self.evaluate is synchronous, but you need to prevent this from blocking, override this method with:     # return await anyio.to_thread.run_sync(self.evaluate, ctx)     output = self.evaluate(ctx)     if inspect.iscoroutine(output):         return await output     else:         return cast(EvaluatorOutput, output) ``` |\n\n#### serialize\n\n```\nserialize(info: SerializationInfo) -> Any\n```\n\nSerialize this Evaluator to a JSON-serializable form.\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `Any` | A JSON-serializable representation of this evaluator as an EvaluatorSpec. |\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`\n\n|  |  |\n| --- | --- |\n| ``` 254 255 256 257 258 259 260 261 262 263 264 265 ``` | ``` @model_serializer(mode='plain') def serialize(self, info: SerializationInfo) -> Any:     \"\"\"Serialize this Evaluator to a JSON-serializable form.      Returns:         A JSON-serializable representation of this evaluator as an EvaluatorSpec.     \"\"\"     return to_jsonable_python(         self.as_spec(),         context=info.context,         serialize_unknown=True,     ) ``` |\n\n#### build\\_serialization\\_arguments\n\n```\nbuild_serialization_arguments() -> dict[str, Any]\n```\n\nBuild the arguments for serialization.\n\nEvaluators are serialized for inclusion as the \"source\" in an `EvaluationResult`.\nIf you want to modify how the evaluator is serialized for that or other purposes, you can override this method.\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `dict[str, Any]` | A dictionary of arguments to be used during serialization. |\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#evaluator-dataclass", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluator `dataclass`", "anchor": "evaluator-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 ``` | ``` def build_serialization_arguments(self) -> dict[str, Any]:     \"\"\"Build the arguments for serialization.      Evaluators are serialized for inclusion as the \"source\" in an `EvaluationResult`.     If you want to modify how the evaluator is serialized for that or other purposes, you can override this method.      Returns:         A dictionary of arguments to be used during serialization.     \"\"\"     raw_arguments: dict[str, Any] = {}     for field in fields(self):         value = getattr(self, field.name)         # always exclude defaults:         if field.default is not MISSING:             if value == field.default:                 continue         if field.default_factory is not MISSING:             if value == field.default_factory():  # pragma: no branch                 continue         raw_arguments[field.name] = value     return raw_arguments ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#evaluator-dataclass", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluatorFailure `dataclass`", "anchor": "evaluatorfailure-dataclass", "md_text": "Represents a failure raised during the execution of an evaluator.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/evaluator.py`\n\n|  |  |\n| --- | --- |\n| ``` 104 105 106 107 108 109 110 111 ``` | ``` @dataclass class EvaluatorFailure:     \"\"\"Represents a failure raised during the execution of an evaluator.\"\"\"      name: str     error_message: str     error_stacktrace: str     source: EvaluatorSpec ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#evaluatorfailure-dataclass", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluatorOutput `module-attribute`", "anchor": "evaluatoroutput-module-attribute", "md_text": "```\nEvaluatorOutput = (\n    EvaluationScalar\n    | EvaluationReason\n    | Mapping[str, EvaluationScalar | EvaluationReason]\n)\n```\n\nType for the output of an evaluator, which can be a scalar, an EvaluationReason, or a mapping of names to either.", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#evaluatoroutput-module-attribute", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluatorSpec", "anchor": "evaluatorspec", "md_text": "Bases: `BaseModel`\n\nThe specification of an evaluator to be run.\n\nThis class is used to represent evaluators in a serializable format, supporting various\nshort forms for convenience when defining evaluators in YAML or JSON dataset files.\n\nIn particular, each of the following forms is supported for specifying an evaluator with name `MyEvaluator`:\n\\* `'MyEvaluator'` - Just the (string) name of the Evaluator subclass is used if its `__init__` takes no arguments\n\\* `{'MyEvaluator': first_arg}` - A single argument is passed as the first positional argument to `MyEvaluator.__init__`\n\\* `{'MyEvaluator': {k1: v1, k2: v2}}` - Multiple kwargs are passed to `MyEvaluator.__init__`\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/spec.py`\n\n|  |  |\n| --- | --- |\n| ```  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 ``` | ``` class EvaluatorSpec(BaseModel):     \"\"\"The specification of an evaluator to be run.      This class is used to represent evaluators in a serializable format, supporting various     short forms for convenience when defining evaluators in YAML or JSON dataset files.      In particular, each of the following forms is supported for specifying an evaluator with name `MyEvaluator`:     * `'MyEvaluator'` - Just the (string) name of the Evaluator subclass is used if its `__init__` takes no arguments     * `{'MyEvaluator': first_arg}` - A single argument is passed as the first positional argument to `MyEvaluator.__init__`     * `{'MyEvaluator': {k1: v1, k2: v2}}` - Multiple kwargs are passed to `MyEvaluator.__init__`     \"\"\"      name: str     \"\"\"The name of the evaluator class; should be the value returned by `EvaluatorClass.get_serialization_name()`\"\"\"      arguments: None | tuple[Any] | dict[str, Any]     \"\"\"The arguments to pass to the evaluator's constructor.      Can be None (no arguments), a tuple (a single positional argument), or a dict (keyword arguments).     \"\"\"      @property     def args(self) -> tuple[Any, ...]:         \"\"\"Get the positional arguments for the evaluator.          Returns:             A tuple of positional arguments if arguments is a tuple, otherwise an empty tuple.         \"\"\"         if isinstance(self.arguments, tuple):             return self.arguments         return ()      @property     def kwargs(self) -> dict[str, Any]:         \"\"\"Get the keyword arguments for the evaluator.          Returns:             A dictionary of keyword arguments if arguments is a dict, otherwise an empty dict.         \"\"\"         if isinstance(self.arguments, dict):             return self.arguments         return {}      @model_validator(mode='wrap')     @classmethod     def deserialize(cls, value: Any, handler: ModelWrapValidatorHandler[EvaluatorSpec]) -> EvaluatorSpec:         \"\"\"Deserialize an EvaluatorSpec from various formats.          This validator handles the various short forms of evaluator specifications,         converting them to a consistent EvaluatorSpec instance.          Args:             value: The value to deserialize.             handler: The validator handler.          Returns:             The deserialized EvaluatorSpec.          Raises:             ValidationError: If the value cannot be deserialized.         \"\"\"         try:             result = handler(value)             return result         except ValidationError as exc:             try:                 deserialized = _SerializedEvaluatorSpec.model_validate(value)             except ValidationError:                 raise exc  # raise the original error             return deserialized.to_evaluator_spec()      @model_serializer(mode='wrap')     def serialize(self, handler: SerializerFunctionWrapHandler, info: SerializationInfo) -> Any:         \"\"\"Serialize using the appropriate short-form if possible.          Returns:             The serialized evaluator specification, using the shortest form possible:             - Just the name if there are no arguments             - {name: first_arg} if there's a single positional argument             - {name: {kwargs}} if there are multiple (keyword) arguments         \"\"\"         if isinstance(info.context, dict) and info.context.get('use_short_form'):  # pyright: ignore[reportUnknownMemberType]             if self.arguments is None:                 return self.name             elif isinstance(self.arguments, tuple):                 return {self.name: self.arguments[0]}             else:                 return {self.name: self.arguments}         else:             return handler(self) ``` |\n\n#### name `instance-attribute`\n\n```\nname: str\n```", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#evaluatorspec", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluatorSpec", "anchor": "evaluatorspec", "md_text": "The name of the evaluator class; should be the value returned by `EvaluatorClass.get_serialization_name()`\n\n#### arguments `instance-attribute`\n\n```\narguments: None | tuple[Any] | dict[str, Any]\n```\n\nThe arguments to pass to the evaluator's constructor.\n\nCan be None (no arguments), a tuple (a single positional argument), or a dict (keyword arguments).\n\n#### args `property`\n\n```\nargs: tuple[Any, ...]\n```\n\nGet the positional arguments for the evaluator.\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `tuple[Any, ...]` | A tuple of positional arguments if arguments is a tuple, otherwise an empty tuple. |\n\n#### kwargs `property`\n\n```\nkwargs: dict[str, Any]\n```\n\nGet the keyword arguments for the evaluator.\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `dict[str, Any]` | A dictionary of keyword arguments if arguments is a dict, otherwise an empty dict. |\n\n#### deserialize `classmethod`\n\n```\ndeserialize(\n    value: Any,\n    handler: ModelWrapValidatorHandler[EvaluatorSpec],\n) -> EvaluatorSpec\n```\n\nDeserialize an EvaluatorSpec from various formats.\n\nThis validator handles the various short forms of evaluator specifications,\nconverting them to a consistent EvaluatorSpec instance.\n\nParameters:\n\n| Name | Type | Description | Default |\n| --- | --- | --- | --- |\n| `value` | `Any` | The value to deserialize. | *required* |\n| `handler` | `ModelWrapValidatorHandler[EvaluatorSpec]` | The validator handler. | *required* |\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `EvaluatorSpec` | The deserialized EvaluatorSpec. |\n\nRaises:\n\n| Type | Description |\n| --- | --- |\n| `ValidationError` | If the value cannot be deserialized. |\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/spec.py`\n\n|  |  |\n| --- | --- |\n| ``` 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 ``` | ``` @model_validator(mode='wrap') @classmethod def deserialize(cls, value: Any, handler: ModelWrapValidatorHandler[EvaluatorSpec]) -> EvaluatorSpec:     \"\"\"Deserialize an EvaluatorSpec from various formats.      This validator handles the various short forms of evaluator specifications,     converting them to a consistent EvaluatorSpec instance.      Args:         value: The value to deserialize.         handler: The validator handler.      Returns:         The deserialized EvaluatorSpec.      Raises:         ValidationError: If the value cannot be deserialized.     \"\"\"     try:         result = handler(value)         return result     except ValidationError as exc:         try:             deserialized = _SerializedEvaluatorSpec.model_validate(value)         except ValidationError:             raise exc  # raise the original error         return deserialized.to_evaluator_spec() ``` |\n\n#### serialize\n\n```\nserialize(\n    handler: SerializerFunctionWrapHandler,\n    info: SerializationInfo,\n) -> Any\n```\n\nSerialize using the appropriate short-form if possible.\n\nReturns:\n\n| Type | Description |\n| --- | --- |\n| `Any` | The serialized evaluator specification, using the shortest form possible: |\n| `Any` | * Just the name if there are no arguments |\n| `Any` | * {name: first\\_arg} if there's a single positional argument |\n| `Any` | * {name: {kwargs}} if there are multiple (keyword) arguments |\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/spec.py`\n\n|  |  |\n| --- | --- |\n| ```  94  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 112 ``` | ``` @model_serializer(mode='wrap') def serialize(self, handler: SerializerFunctionWrapHandler, info: SerializationInfo) -> Any:     \"\"\"Serialize using the appropriate short-form if possible.      Returns:         The serialized evaluator specification, using the shortest form possible:         - Just the name if there are no arguments         - {name: first_arg} if there's a single positional argument         - {name: {kwargs}} if there are multiple (keyword) arguments     \"\"\"     if isinstance(info.context, dict) and info.context.get('use_short_form'):  # pyright: ignore[reportUnknownMemberType]         if self.arguments is None:             return self.name         elif isinstance(self.arguments, tuple):             return {self.name: self.arguments[0]}         else:             return {self.name: self.arguments}     else:         return handler(self) ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#evaluatorspec", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "GradingOutput", "anchor": "gradingoutput", "md_text": "Bases: `BaseModel`\n\nThe output of a grading operation.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py`\n\n|  |  |\n| --- | --- |\n| ``` 26 27 28 29 30 31 ``` | ``` class GradingOutput(BaseModel, populate_by_name=True):     \"\"\"The output of a grading operation.\"\"\"      reason: str     pass_: bool = Field(validation_alias='pass', serialization_alias='pass')     score: float ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#gradingoutput", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "judge\\_output `async`", "anchor": "judgeoutput-async", "md_text": "```\njudge_output(\n    output: Any,\n    rubric: str,\n    model: Model | KnownModelName | None = None,\n    model_settings: ModelSettings | None = None,\n) -> GradingOutput\n```\n\nJudge the output of a model based on a rubric.\n\nIf the model is not specified, a default model is used. The default model starts as 'openai:gpt-4o',\nbut this can be changed using the `set_default_judge_model` function.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py`\n\n|  |  |\n| --- | --- |\n| ``` 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 ``` | ``` async def judge_output(     output: Any,     rubric: str,     model: models.Model | models.KnownModelName | None = None,     model_settings: ModelSettings | None = None, ) -> GradingOutput:     \"\"\"Judge the output of a model based on a rubric.      If the model is not specified, a default model is used. The default model starts as 'openai:gpt-4o',     but this can be changed using the `set_default_judge_model` function.     \"\"\"     user_prompt = _build_prompt(output=output, rubric=rubric)     return (         await _judge_output_agent.run(user_prompt, model=model or _default_model, model_settings=model_settings)     ).output ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#judgeoutput-async", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "judge\\_input\\_output `async`", "anchor": "judgeinputoutput-async", "md_text": "```\njudge_input_output(\n    inputs: Any,\n    output: Any,\n    rubric: str,\n    model: Model | KnownModelName | None = None,\n    model_settings: ModelSettings | None = None,\n) -> GradingOutput\n```\n\nJudge the output of a model based on the inputs and a rubric.\n\nIf the model is not specified, a default model is used. The default model starts as 'openai:gpt-4o',\nbut this can be changed using the `set_default_judge_model` function.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py`\n\n|  |  |\n| --- | --- |\n| ```  95  96  97  98  99 100 101 102 103 104 105 106 107 108 109 110 111 ``` | ``` async def judge_input_output(     inputs: Any,     output: Any,     rubric: str,     model: models.Model | models.KnownModelName | None = None,     model_settings: ModelSettings | None = None, ) -> GradingOutput:     \"\"\"Judge the output of a model based on the inputs and a rubric.      If the model is not specified, a default model is used. The default model starts as 'openai:gpt-4o',     but this can be changed using the `set_default_judge_model` function.     \"\"\"     user_prompt = _build_prompt(inputs=inputs, output=output, rubric=rubric)      return (         await _judge_input_output_agent.run(user_prompt, model=model or _default_model, model_settings=model_settings)     ).output ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#judgeinputoutput-async", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "judge\\_input\\_output\\_expected `async`", "anchor": "judgeinputoutputexpected-async", "md_text": "```\njudge_input_output_expected(\n    inputs: Any,\n    output: Any,\n    expected_output: Any,\n    rubric: str,\n    model: Model | KnownModelName | None = None,\n    model_settings: ModelSettings | None = None,\n) -> GradingOutput\n```\n\nJudge the output of a model based on the inputs and a rubric.\n\nIf the model is not specified, a default model is used. The default model starts as 'openai:gpt-4o',\nbut this can be changed using the `set_default_judge_model` function.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py`\n\n|  |  |\n| --- | --- |\n| ``` 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 ``` | ``` async def judge_input_output_expected(     inputs: Any,     output: Any,     expected_output: Any,     rubric: str,     model: models.Model | models.KnownModelName | None = None,     model_settings: ModelSettings | None = None, ) -> GradingOutput:     \"\"\"Judge the output of a model based on the inputs and a rubric.      If the model is not specified, a default model is used. The default model starts as 'openai:gpt-4o',     but this can be changed using the `set_default_judge_model` function.     \"\"\"     user_prompt = _build_prompt(inputs=inputs, output=output, rubric=rubric, expected_output=expected_output)      return (         await _judge_input_output_expected_agent.run(             user_prompt, model=model or _default_model, model_settings=model_settings         )     ).output ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#judgeinputoutputexpected-async", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "judge\\_output\\_expected `async`", "anchor": "judgeoutputexpected-async", "md_text": "```\njudge_output_expected(\n    output: Any,\n    expected_output: Any,\n    rubric: str,\n    model: Model | KnownModelName | None = None,\n    model_settings: ModelSettings | None = None,\n) -> GradingOutput\n```\n\nJudge the output of a model based on the expected output, output, and a rubric.\n\nIf the model is not specified, a default model is used. The default model starts as 'openai:gpt-4o',\nbut this can be changed using the `set_default_judge_model` function.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py`\n\n|  |  |\n| --- | --- |\n| ``` 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 ``` | ``` async def judge_output_expected(     output: Any,     expected_output: Any,     rubric: str,     model: models.Model | models.KnownModelName | None = None,     model_settings: ModelSettings | None = None, ) -> GradingOutput:     \"\"\"Judge the output of a model based on the expected output, output, and a rubric.      If the model is not specified, a default model is used. The default model starts as 'openai:gpt-4o',     but this can be changed using the `set_default_judge_model` function.     \"\"\"     user_prompt = _build_prompt(output=output, rubric=rubric, expected_output=expected_output)     return (         await _judge_output_expected_agent.run(             user_prompt, model=model or _default_model, model_settings=model_settings         )     ).output ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#judgeoutputexpected-async", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "set\\_default\\_judge\\_model", "anchor": "setdefaultjudgemodel", "md_text": "```\nset_default_judge_model(\n    model: Model | KnownModelName,\n) -> None\n```\n\nSet the default model used for judging.\n\nThis model is used if `None` is passed to the `model` argument of `judge_output` and `judge_input_output`.\n\nSource code in `pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py`\n\n|  |  |\n| --- | --- |\n| ``` 204 205 206 207 208 209 210 ``` | ``` def set_default_judge_model(model: models.Model | models.KnownModelName) -> None:     \"\"\"Set the default model used for judging.      This model is used if `None` is passed to the `model` argument of `judge_output` and `judge_input_output`.     \"\"\"     global _default_model     _default_model = model ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/evaluators/index.html#setdefaultjudgemodel", "page": "pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "ReportCase `dataclass`", "anchor": "reportcase-dataclass", "md_text": "Bases: `Generic[InputsT, OutputT, MetadataT]`\n\nA single case in an evaluation report.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 ``` | ``` @dataclass(kw_only=True) class ReportCase(Generic[InputsT, OutputT, MetadataT]):     \"\"\"A single case in an evaluation report.\"\"\"      name: str     \"\"\"The name of the [case][pydantic_evals.Case].\"\"\"     inputs: InputsT     \"\"\"The inputs to the task, from [`Case.inputs`][pydantic_evals.Case.inputs].\"\"\"     metadata: MetadataT | None     \"\"\"Any metadata associated with the case, from [`Case.metadata`][pydantic_evals.Case.metadata].\"\"\"     expected_output: OutputT | None     \"\"\"The expected output of the task, from [`Case.expected_output`][pydantic_evals.Case.expected_output].\"\"\"     output: OutputT     \"\"\"The output of the task execution.\"\"\"      metrics: dict[str, float | int]     attributes: dict[str, Any]      scores: dict[str, EvaluationResult[int | float]]     labels: dict[str, EvaluationResult[str]]     assertions: dict[str, EvaluationResult[bool]]      task_duration: float     total_duration: float  # includes evaluator execution time      trace_id: str | None = None     \"\"\"The trace ID of the case span.\"\"\"     span_id: str | None = None     \"\"\"The span ID of the case span.\"\"\"      evaluator_failures: list[EvaluatorFailure] = field(default_factory=list) ``` |\n\n#### name `instance-attribute`\n\n```\nname: str\n```\n\nThe name of the [case](../dataset/index.html#pydantic_evals.dataset.Case).\n\n#### inputs `instance-attribute`\n\n```\ninputs: InputsT\n```\n\nThe inputs to the task, from [`Case.inputs`](../dataset/index.html#pydantic_evals.dataset.Case.inputs).\n\n#### metadata `instance-attribute`\n\n```\nmetadata: MetadataT | None\n```\n\nAny metadata associated with the case, from [`Case.metadata`](../dataset/index.html#pydantic_evals.dataset.Case.metadata).\n\n#### expected\\_output `instance-attribute`\n\n```\nexpected_output: OutputT | None\n```\n\nThe expected output of the task, from [`Case.expected_output`](../dataset/index.html#pydantic_evals.dataset.Case.expected_output).\n\n#### output `instance-attribute`\n\n```\noutput: OutputT\n```\n\nThe output of the task execution.\n\n#### trace\\_id `class-attribute` `instance-attribute`\n\n```\ntrace_id: str | None = None\n```\n\nThe trace ID of the case span.\n\n#### span\\_id `class-attribute` `instance-attribute`\n\n```\nspan_id: str | None = None\n```\n\nThe span ID of the case span.", "url": "https://ai.pydantic.dev/pydantic_evals/reporting/index.html#reportcase-dataclass", "page": "pydantic_evals/reporting/index.html", "source_site": "pydantic_ai"}
{"title": "ReportCaseFailure `dataclass`", "anchor": "reportcasefailure-dataclass", "md_text": "Bases: `Generic[InputsT, OutputT, MetadataT]`\n\nA single case in an evaluation report that failed due to an error during task execution.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`\n\n|  |  |\n| --- | --- |\n| ```  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 100 101 102 103 ``` | ``` @dataclass(kw_only=True) class ReportCaseFailure(Generic[InputsT, OutputT, MetadataT]):     \"\"\"A single case in an evaluation report that failed due to an error during task execution.\"\"\"      name: str     \"\"\"The name of the [case][pydantic_evals.Case].\"\"\"     inputs: InputsT     \"\"\"The inputs to the task, from [`Case.inputs`][pydantic_evals.Case.inputs].\"\"\"     metadata: MetadataT | None     \"\"\"Any metadata associated with the case, from [`Case.metadata`][pydantic_evals.Case.metadata].\"\"\"     expected_output: OutputT | None     \"\"\"The expected output of the task, from [`Case.expected_output`][pydantic_evals.Case.expected_output].\"\"\"      error_message: str     \"\"\"The message of the exception that caused the failure.\"\"\"     error_stacktrace: str     \"\"\"The stacktrace of the exception that caused the failure.\"\"\"      trace_id: str | None = None     \"\"\"The trace ID of the case span.\"\"\"     span_id: str | None = None     \"\"\"The span ID of the case span.\"\"\" ``` |\n\n#### name `instance-attribute`\n\n```\nname: str\n```\n\nThe name of the [case](../dataset/index.html#pydantic_evals.dataset.Case).\n\n#### inputs `instance-attribute`\n\n```\ninputs: InputsT\n```\n\nThe inputs to the task, from [`Case.inputs`](../dataset/index.html#pydantic_evals.dataset.Case.inputs).\n\n#### metadata `instance-attribute`\n\n```\nmetadata: MetadataT | None\n```\n\nAny metadata associated with the case, from [`Case.metadata`](../dataset/index.html#pydantic_evals.dataset.Case.metadata).\n\n#### expected\\_output `instance-attribute`\n\n```\nexpected_output: OutputT | None\n```\n\nThe expected output of the task, from [`Case.expected_output`](../dataset/index.html#pydantic_evals.dataset.Case.expected_output).\n\n#### error\\_message `instance-attribute`\n\n```\nerror_message: str\n```\n\nThe message of the exception that caused the failure.\n\n#### error\\_stacktrace `instance-attribute`\n\n```\nerror_stacktrace: str\n```\n\nThe stacktrace of the exception that caused the failure.\n\n#### trace\\_id `class-attribute` `instance-attribute`\n\n```\ntrace_id: str | None = None\n```\n\nThe trace ID of the case span.\n\n#### span\\_id `class-attribute` `instance-attribute`\n\n```\nspan_id: str | None = None\n```\n\nThe span ID of the case span.", "url": "https://ai.pydantic.dev/pydantic_evals/reporting/index.html#reportcasefailure-dataclass", "page": "pydantic_evals/reporting/index.html", "source_site": "pydantic_ai"}
{"title": "ReportCaseAggregate", "anchor": "reportcaseaggregate", "md_text": "Bases: `BaseModel`\n\nA synthetic case that summarizes a set of cases.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 ``` | ``` class ReportCaseAggregate(BaseModel):     \"\"\"A synthetic case that summarizes a set of cases.\"\"\"      name: str      scores: dict[str, float | int]     labels: dict[str, dict[str, float]]     metrics: dict[str, float | int]     assertions: float | None     task_duration: float     total_duration: float      @staticmethod     def average(cases: list[ReportCase]) -> ReportCaseAggregate:         \"\"\"Produce a synthetic \"summary\" case by averaging quantitative attributes.\"\"\"         num_cases = len(cases)         if num_cases == 0:             return ReportCaseAggregate(                 name='Averages',                 scores={},                 labels={},                 metrics={},                 assertions=None,                 task_duration=0.0,                 total_duration=0.0,             )          def _scores_averages(scores_by_name: list[dict[str, int | float | bool]]) -> dict[str, float]:             counts_by_name: dict[str, int] = defaultdict(int)             sums_by_name: dict[str, float] = defaultdict(float)             for sbn in scores_by_name:                 for name, score in sbn.items():                     counts_by_name[name] += 1                     sums_by_name[name] += score             return {name: sums_by_name[name] / counts_by_name[name] for name in sums_by_name}          def _labels_averages(labels_by_name: list[dict[str, str]]) -> dict[str, dict[str, float]]:             counts_by_name: dict[str, int] = defaultdict(int)             sums_by_name: dict[str, dict[str, float]] = defaultdict(lambda: defaultdict(float))             for lbn in labels_by_name:                 for name, label in lbn.items():                     counts_by_name[name] += 1                     sums_by_name[name][label] += 1             return {                 name: {value: count / counts_by_name[name] for value, count in sums_by_name[name].items()}                 for name in sums_by_name             }          average_task_duration = sum(case.task_duration for case in cases) / num_cases         average_total_duration = sum(case.total_duration for case in cases) / num_cases          # average_assertions: dict[str, float] = _scores_averages([{k: v.value for k, v in case.scores.items()} for case in cases])         average_scores: dict[str, float] = _scores_averages(             [{k: v.value for k, v in case.scores.items()} for case in cases]         )         average_labels: dict[str, dict[str, float]] = _labels_averages(             [{k: v.value for k, v in case.labels.items()} for case in cases]         )         average_metrics: dict[str, float] = _scores_averages([case.metrics for case in cases])          average_assertions: float | None = None         n_assertions = sum(len(case.assertions) for case in cases)         if n_assertions > 0:             n_passing = sum(1 for case in cases for assertion in case.assertions.values() if assertion.value)             average_assertions = n_passing / n_assertions          return ReportCaseAggregate(             name='Averages',             scores=average_scores,             labels=average_labels,             metrics=average_metrics,             assertions=average_assertions,             task_duration=average_task_duration,             total_duration=average_total_duration,         ) ``` |\n\n#### average `staticmethod`\n\n```\naverage(cases: list[ReportCase]) -> ReportCaseAggregate\n```\n\nProduce a synthetic \"summary\" case by averaging quantitative attributes.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`", "url": "https://ai.pydantic.dev/pydantic_evals/reporting/index.html#reportcaseaggregate", "page": "pydantic_evals/reporting/index.html", "source_site": "pydantic_ai"}
{"title": "ReportCaseAggregate", "anchor": "reportcaseaggregate", "md_text": "|  |  |\n| --- | --- |\n| ``` 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 ``` | ``` @staticmethod def average(cases: list[ReportCase]) -> ReportCaseAggregate:     \"\"\"Produce a synthetic \"summary\" case by averaging quantitative attributes.\"\"\"     num_cases = len(cases)     if num_cases == 0:         return ReportCaseAggregate(             name='Averages',             scores={},             labels={},             metrics={},             assertions=None,             task_duration=0.0,             total_duration=0.0,         )      def _scores_averages(scores_by_name: list[dict[str, int | float | bool]]) -> dict[str, float]:         counts_by_name: dict[str, int] = defaultdict(int)         sums_by_name: dict[str, float] = defaultdict(float)         for sbn in scores_by_name:             for name, score in sbn.items():                 counts_by_name[name] += 1                 sums_by_name[name] += score         return {name: sums_by_name[name] / counts_by_name[name] for name in sums_by_name}      def _labels_averages(labels_by_name: list[dict[str, str]]) -> dict[str, dict[str, float]]:         counts_by_name: dict[str, int] = defaultdict(int)         sums_by_name: dict[str, dict[str, float]] = defaultdict(lambda: defaultdict(float))         for lbn in labels_by_name:             for name, label in lbn.items():                 counts_by_name[name] += 1                 sums_by_name[name][label] += 1         return {             name: {value: count / counts_by_name[name] for value, count in sums_by_name[name].items()}             for name in sums_by_name         }      average_task_duration = sum(case.task_duration for case in cases) / num_cases     average_total_duration = sum(case.total_duration for case in cases) / num_cases      # average_assertions: dict[str, float] = _scores_averages([{k: v.value for k, v in case.scores.items()} for case in cases])     average_scores: dict[str, float] = _scores_averages(         [{k: v.value for k, v in case.scores.items()} for case in cases]     )     average_labels: dict[str, dict[str, float]] = _labels_averages(         [{k: v.value for k, v in case.labels.items()} for case in cases]     )     average_metrics: dict[str, float] = _scores_averages([case.metrics for case in cases])      average_assertions: float | None = None     n_assertions = sum(len(case.assertions) for case in cases)     if n_assertions > 0:         n_passing = sum(1 for case in cases for assertion in case.assertions.values() if assertion.value)         average_assertions = n_passing / n_assertions      return ReportCaseAggregate(         name='Averages',         scores=average_scores,         labels=average_labels,         metrics=average_metrics,         assertions=average_assertions,         task_duration=average_task_duration,         total_duration=average_total_duration,     ) ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/reporting/index.html#reportcaseaggregate", "page": "pydantic_evals/reporting/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluationReport `dataclass`", "anchor": "evaluationreport-dataclass", "md_text": "Bases: `Generic[InputsT, OutputT, MetadataT]`\n\nA report of the results of evaluating a model on a set of cases.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`", "url": "https://ai.pydantic.dev/pydantic_evals/reporting/index.html#evaluationreport-dataclass", "page": "pydantic_evals/reporting/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluationReport `dataclass`", "anchor": "evaluationreport-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 ``` | ``` @dataclass(kw_only=True) class EvaluationReport(Generic[InputsT, OutputT, MetadataT]):     \"\"\"A report of the results of evaluating a model on a set of cases.\"\"\"      name: str     \"\"\"The name of the report.\"\"\"      cases: list[ReportCase[InputsT, OutputT, MetadataT]]     \"\"\"The cases in the report.\"\"\"     failures: list[ReportCaseFailure[InputsT, OutputT, MetadataT]] = field(default_factory=list)     \"\"\"The failures in the report. These are cases where task execution raised an exception.\"\"\"      trace_id: str | None = None     \"\"\"The trace ID of the evaluation.\"\"\"     span_id: str | None = None     \"\"\"The span ID of the evaluation.\"\"\"      def averages(self) -> ReportCaseAggregate | None:         if self.cases:             return ReportCaseAggregate.average(self.cases)         return None      def render(         self,         width: int | None = None,         baseline: EvaluationReport[InputsT, OutputT, MetadataT] | None = None,         *,         include_input: bool = False,         include_metadata: bool = False,         include_expected_output: bool = False,         include_output: bool = False,         include_durations: bool = True,         include_total_duration: bool = False,         include_removed_cases: bool = False,         include_averages: bool = True,         include_errors: bool = True,         include_error_stacktrace: bool = False,         include_evaluator_failures: bool = True,         input_config: RenderValueConfig | None = None,         metadata_config: RenderValueConfig | None = None,         output_config: RenderValueConfig | None = None,         score_configs: dict[str, RenderNumberConfig] | None = None,         label_configs: dict[str, RenderValueConfig] | None = None,         metric_configs: dict[str, RenderNumberConfig] | None = None,         duration_config: RenderNumberConfig | None = None,         include_reasons: bool = False,     ) -> str:  # pragma: no cover         \"\"\"Render this report to a nicely-formatted string, optionally comparing it to a baseline report.          If you want more control over the output, use `console_table` instead and pass it to `rich.Console.print`.         \"\"\"         io_file = StringIO()         console = Console(width=width, file=io_file)         self.print(             width=width,             baseline=baseline,             console=console,             include_input=include_input,             include_metadata=include_metadata,             include_expected_output=include_expected_output,             include_output=include_output,             include_durations=include_durations,             include_total_duration=include_total_duration,             include_removed_cases=include_removed_cases,             include_averages=include_averages,             include_errors=include_errors,             include_error_stacktrace=include_error_stacktrace,             include_evaluator_failures=include_evaluator_failures,             input_config=input_config,             metadata_config=metadata_config,             output_config=output_config,             score_configs=score_configs,             label_configs=label_configs,             metric_configs=metric_configs,             duration_config=duration_config,             include_reasons=include_reasons,         )         Console(file=io_file)         return io_file.getvalue()      def print(         self,         width: int | None = None,         baseline: EvaluationReport[InputsT, OutputT, MetadataT] | None = None,         *,         console: Console | None = None,         include_input: bool = False,         include_metadata: bool = False,         include_expected_output: bool = False,         include_output: bool = False,         include_durations: bool = True,         include_total_duration: bool = False,         include_removed_cases: bool = False,         include_averages: bool = True,         include_errors: bool = True,         include_error_stacktrace: bool = False,         include_evaluator_failures: bool = True,         input_config: RenderValueConfig | None = None,         metadata_config: RenderValueConfig | None = None,         output_config: RenderValueConfig | None = None,         score_configs: dict[str, RenderNumberConfig] | None = None,         label_configs: dict[str, RenderValueConfig] | None = None,         metric_configs: dict[str, RenderNumberConfig] | None = None,         duration_config: RenderNumberConfig | None = None,         include_reasons: bool = False,     ) -> None:         \"\"\"Print this report to the console, optionally comparing it to a baseline report.          If you want more control over the output, use `console_table` instead and pass it to `rich.Console.print`.         \"\"\"         if console is None:  # pragma: no branch             console = Console(width=width)          table = self.console_table(             baseline=baseline,             include_input=include_input,             include_metadata=include_metadata,             include_expected_output=include_expected_output,             include_output=include_output,             include_durations=include_durations,             include_total_duration=include_total_duration,             include_removed_cases=include_removed_cases,             include_averages=include_averages,             include_evaluator_failures=include_evaluator_failures,             input_config=input_config,             metadata_config=metadata_config,             output_config=output_config,             score_configs=score_configs,             label_configs=label_configs,             metric_configs=metric_configs,             duration_config=duration_config,             include_reasons=include_reasons,         )         console.print(table)         if include_errors and self.failures:  # pragma: no cover             failures_table = self.failures_table(                 include_input=include_input,                 include_metadata=include_metadata,                 include_expected_output=include_expected_output,                 include_error_message=True,                 include_error_stacktrace=include_error_stacktrace,                 input_config=input_config,                 metadata_config=metadata_config,             )             console.print(failures_table, style='red')      def console_table(         self,         baseline: EvaluationReport[InputsT, OutputT, MetadataT] | None = None,         *,         include_input: bool = False,         include_metadata: bool = False,         include_expected_output: bool = False,         include_output: bool = False,         include_durations: bool = True,         include_total_duration: bool = False,         include_removed_cases: bool = False,         include_averages: bool = True,         include_evaluator_failures: bool = True,         input_config: RenderValueConfig | None = None,         metadata_config: RenderValueConfig | None = None,         output_config: RenderValueConfig | None = None,         score_configs: dict[str, RenderNumberConfig] | None = None,         label_configs: dict[str, RenderValueConfig] | None = None,         metric_configs: dict[str, RenderNumberConfig] | None = None,         duration_config: RenderNumberConfig | None = None,         include_reasons: bool = False,     ) -> Table:         \"\"\"Return a table containing the data from this report, or the diff between this report and a baseline report.          Optionally include input and output details.         \"\"\"         renderer = EvaluationRenderer(             include_input=include_input,             include_metadata=include_metadata,             include_expected_output=include_expected_output,             include_output=include_output,             include_durations=include_durations,             include_total_duration=include_total_duration,             include_removed_cases=include_removed_cases,             include_averages=include_averages,             include_error_message=False,             include_error_stacktrace=False,             include_evaluator_failures=include_evaluator_failures,             input_config={**_DEFAULT_VALUE_CONFIG, **(input_config or {})},             metadata_config={**_DEFAULT_VALUE_CONFIG, **(metadata_config or {})},             output_config=output_config or _DEFAULT_VALUE_CONFIG,             score_configs=score_configs or {},             label_configs=label_configs or {},             metric_configs=metric_configs or {},             duration_config=duration_config or _DEFAULT_DURATION_CONFIG,             include_reasons=include_reasons,         )         if baseline is None:             return renderer.build_table(self)         else:  # pragma: no cover             return renderer.build_diff_table(self, baseline)      def failures_table(         self,         *,         include_input: bool = False,         include_metadata: bool = False,         include_expected_output: bool = False,         include_error_message: bool = True,         include_error_stacktrace: bool = True,         input_config: RenderValueConfig | None = None,         metadata_config: RenderValueConfig | None = None,     ) -> Table:         \"\"\"Return a table containing the failures in this report.\"\"\"         renderer = EvaluationRenderer(             include_input=include_input,             include_metadata=include_metadata,             include_expected_output=include_expected_output,             include_output=False,             include_durations=False,             include_total_duration=False,             include_removed_cases=False,             include_averages=False,             input_config={**_DEFAULT_VALUE_CONFIG, **(input_config or {})},             metadata_config={**_DEFAULT_VALUE_CONFIG, **(metadata_config or {})},             output_config=_DEFAULT_VALUE_CONFIG,             score_configs={},             label_configs={},             metric_configs={},             duration_config=_DEFAULT_DURATION_CONFIG,             include_reasons=False,             include_error_message=include_error_message,             include_error_stacktrace=include_error_stacktrace,             include_evaluator_failures=False,  # Not applicable for failures table         )         return renderer.build_failures_table(self)      def __str__(self) -> str:  # pragma: lax no cover         \"\"\"Return a string representation of the report.\"\"\"         return self.render() ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/reporting/index.html#evaluationreport-dataclass", "page": "pydantic_evals/reporting/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluationReport `dataclass`", "anchor": "evaluationreport-dataclass", "md_text": "#### name `instance-attribute`\n\n```\nname: str\n```\n\nThe name of the report.\n\n#### cases `instance-attribute`\n\n```\ncases: list[ReportCase[InputsT, OutputT, MetadataT]]\n```\n\nThe cases in the report.\n\n#### failures `class-attribute` `instance-attribute`\n\n```\nfailures: list[\n    ReportCaseFailure[InputsT, OutputT, MetadataT]\n] = field(default_factory=list)\n```\n\nThe failures in the report. These are cases where task execution raised an exception.\n\n#### trace\\_id `class-attribute` `instance-attribute`\n\n```\ntrace_id: str | None = None\n```\n\nThe trace ID of the evaluation.\n\n#### span\\_id `class-attribute` `instance-attribute`\n\n```\nspan_id: str | None = None\n```\n\nThe span ID of the evaluation.\n\n#### render\n\n```\nrender(\n    width: int | None = None,\n    baseline: (\n        EvaluationReport[InputsT, OutputT, MetadataT] | None\n    ) = None,\n    *,\n    include_input: bool = False,\n    include_metadata: bool = False,\n    include_expected_output: bool = False,\n    include_output: bool = False,\n    include_durations: bool = True,\n    include_total_duration: bool = False,\n    include_removed_cases: bool = False,\n    include_averages: bool = True,\n    include_errors: bool = True,\n    include_error_stacktrace: bool = False,\n    include_evaluator_failures: bool = True,\n    input_config: RenderValueConfig | None = None,\n    metadata_config: RenderValueConfig | None = None,\n    output_config: RenderValueConfig | None = None,\n    score_configs: (\n        dict[str, RenderNumberConfig] | None\n    ) = None,\n    label_configs: (\n        dict[str, RenderValueConfig] | None\n    ) = None,\n    metric_configs: (\n        dict[str, RenderNumberConfig] | None\n    ) = None,\n    duration_config: RenderNumberConfig | None = None,\n    include_reasons: bool = False\n) -> str\n```\n\nRender this report to a nicely-formatted string, optionally comparing it to a baseline report.\n\nIf you want more control over the output, use `console_table` instead and pass it to `rich.Console.print`.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 ``` | ``` def render(     self,     width: int | None = None,     baseline: EvaluationReport[InputsT, OutputT, MetadataT] | None = None,     *,     include_input: bool = False,     include_metadata: bool = False,     include_expected_output: bool = False,     include_output: bool = False,     include_durations: bool = True,     include_total_duration: bool = False,     include_removed_cases: bool = False,     include_averages: bool = True,     include_errors: bool = True,     include_error_stacktrace: bool = False,     include_evaluator_failures: bool = True,     input_config: RenderValueConfig | None = None,     metadata_config: RenderValueConfig | None = None,     output_config: RenderValueConfig | None = None,     score_configs: dict[str, RenderNumberConfig] | None = None,     label_configs: dict[str, RenderValueConfig] | None = None,     metric_configs: dict[str, RenderNumberConfig] | None = None,     duration_config: RenderNumberConfig | None = None,     include_reasons: bool = False, ) -> str:  # pragma: no cover     \"\"\"Render this report to a nicely-formatted string, optionally comparing it to a baseline report.      If you want more control over the output, use `console_table` instead and pass it to `rich.Console.print`.     \"\"\"     io_file = StringIO()     console = Console(width=width, file=io_file)     self.print(         width=width,         baseline=baseline,         console=console,         include_input=include_input,         include_metadata=include_metadata,         include_expected_output=include_expected_output,         include_output=include_output,         include_durations=include_durations,         include_total_duration=include_total_duration,         include_removed_cases=include_removed_cases,         include_averages=include_averages,         include_errors=include_errors,         include_error_stacktrace=include_error_stacktrace,         include_evaluator_failures=include_evaluator_failures,         input_config=input_config,         metadata_config=metadata_config,         output_config=output_config,         score_configs=score_configs,         label_configs=label_configs,         metric_configs=metric_configs,         duration_config=duration_config,         include_reasons=include_reasons,     )     Console(file=io_file)     return io_file.getvalue() ``` |\n\n#### print", "url": "https://ai.pydantic.dev/pydantic_evals/reporting/index.html#evaluationreport-dataclass", "page": "pydantic_evals/reporting/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluationReport `dataclass`", "anchor": "evaluationreport-dataclass", "md_text": "```\nprint(\n    width: int | None = None,\n    baseline: (\n        EvaluationReport[InputsT, OutputT, MetadataT] | None\n    ) = None,\n    *,\n    console: Console | None = None,\n    include_input: bool = False,\n    include_metadata: bool = False,\n    include_expected_output: bool = False,\n    include_output: bool = False,\n    include_durations: bool = True,\n    include_total_duration: bool = False,\n    include_removed_cases: bool = False,\n    include_averages: bool = True,\n    include_errors: bool = True,\n    include_error_stacktrace: bool = False,\n    include_evaluator_failures: bool = True,\n    input_config: RenderValueConfig | None = None,\n    metadata_config: RenderValueConfig | None = None,\n    output_config: RenderValueConfig | None = None,\n    score_configs: (\n        dict[str, RenderNumberConfig] | None\n    ) = None,\n    label_configs: (\n        dict[str, RenderValueConfig] | None\n    ) = None,\n    metric_configs: (\n        dict[str, RenderNumberConfig] | None\n    ) = None,\n    duration_config: RenderNumberConfig | None = None,\n    include_reasons: bool = False\n) -> None\n```\n\nPrint this report to the console, optionally comparing it to a baseline report.\n\nIf you want more control over the output, use `console_table` instead and pass it to `rich.Console.print`.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 ``` | ``` def print(     self,     width: int | None = None,     baseline: EvaluationReport[InputsT, OutputT, MetadataT] | None = None,     *,     console: Console | None = None,     include_input: bool = False,     include_metadata: bool = False,     include_expected_output: bool = False,     include_output: bool = False,     include_durations: bool = True,     include_total_duration: bool = False,     include_removed_cases: bool = False,     include_averages: bool = True,     include_errors: bool = True,     include_error_stacktrace: bool = False,     include_evaluator_failures: bool = True,     input_config: RenderValueConfig | None = None,     metadata_config: RenderValueConfig | None = None,     output_config: RenderValueConfig | None = None,     score_configs: dict[str, RenderNumberConfig] | None = None,     label_configs: dict[str, RenderValueConfig] | None = None,     metric_configs: dict[str, RenderNumberConfig] | None = None,     duration_config: RenderNumberConfig | None = None,     include_reasons: bool = False, ) -> None:     \"\"\"Print this report to the console, optionally comparing it to a baseline report.      If you want more control over the output, use `console_table` instead and pass it to `rich.Console.print`.     \"\"\"     if console is None:  # pragma: no branch         console = Console(width=width)      table = self.console_table(         baseline=baseline,         include_input=include_input,         include_metadata=include_metadata,         include_expected_output=include_expected_output,         include_output=include_output,         include_durations=include_durations,         include_total_duration=include_total_duration,         include_removed_cases=include_removed_cases,         include_averages=include_averages,         include_evaluator_failures=include_evaluator_failures,         input_config=input_config,         metadata_config=metadata_config,         output_config=output_config,         score_configs=score_configs,         label_configs=label_configs,         metric_configs=metric_configs,         duration_config=duration_config,         include_reasons=include_reasons,     )     console.print(table)     if include_errors and self.failures:  # pragma: no cover         failures_table = self.failures_table(             include_input=include_input,             include_metadata=include_metadata,             include_expected_output=include_expected_output,             include_error_message=True,             include_error_stacktrace=include_error_stacktrace,             input_config=input_config,             metadata_config=metadata_config,         )         console.print(failures_table, style='red') ``` |\n\n#### console\\_table", "url": "https://ai.pydantic.dev/pydantic_evals/reporting/index.html#evaluationreport-dataclass", "page": "pydantic_evals/reporting/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluationReport `dataclass`", "anchor": "evaluationreport-dataclass", "md_text": "```\nconsole_table(\n    baseline: (\n        EvaluationReport[InputsT, OutputT, MetadataT] | None\n    ) = None,\n    *,\n    include_input: bool = False,\n    include_metadata: bool = False,\n    include_expected_output: bool = False,\n    include_output: bool = False,\n    include_durations: bool = True,\n    include_total_duration: bool = False,\n    include_removed_cases: bool = False,\n    include_averages: bool = True,\n    include_evaluator_failures: bool = True,\n    input_config: RenderValueConfig | None = None,\n    metadata_config: RenderValueConfig | None = None,\n    output_config: RenderValueConfig | None = None,\n    score_configs: (\n        dict[str, RenderNumberConfig] | None\n    ) = None,\n    label_configs: (\n        dict[str, RenderValueConfig] | None\n    ) = None,\n    metric_configs: (\n        dict[str, RenderNumberConfig] | None\n    ) = None,\n    duration_config: RenderNumberConfig | None = None,\n    include_reasons: bool = False\n) -> Table\n```\n\nReturn a table containing the data from this report, or the diff between this report and a baseline report.\n\nOptionally include input and output details.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 ``` | ``` def console_table(     self,     baseline: EvaluationReport[InputsT, OutputT, MetadataT] | None = None,     *,     include_input: bool = False,     include_metadata: bool = False,     include_expected_output: bool = False,     include_output: bool = False,     include_durations: bool = True,     include_total_duration: bool = False,     include_removed_cases: bool = False,     include_averages: bool = True,     include_evaluator_failures: bool = True,     input_config: RenderValueConfig | None = None,     metadata_config: RenderValueConfig | None = None,     output_config: RenderValueConfig | None = None,     score_configs: dict[str, RenderNumberConfig] | None = None,     label_configs: dict[str, RenderValueConfig] | None = None,     metric_configs: dict[str, RenderNumberConfig] | None = None,     duration_config: RenderNumberConfig | None = None,     include_reasons: bool = False, ) -> Table:     \"\"\"Return a table containing the data from this report, or the diff between this report and a baseline report.      Optionally include input and output details.     \"\"\"     renderer = EvaluationRenderer(         include_input=include_input,         include_metadata=include_metadata,         include_expected_output=include_expected_output,         include_output=include_output,         include_durations=include_durations,         include_total_duration=include_total_duration,         include_removed_cases=include_removed_cases,         include_averages=include_averages,         include_error_message=False,         include_error_stacktrace=False,         include_evaluator_failures=include_evaluator_failures,         input_config={**_DEFAULT_VALUE_CONFIG, **(input_config or {})},         metadata_config={**_DEFAULT_VALUE_CONFIG, **(metadata_config or {})},         output_config=output_config or _DEFAULT_VALUE_CONFIG,         score_configs=score_configs or {},         label_configs=label_configs or {},         metric_configs=metric_configs or {},         duration_config=duration_config or _DEFAULT_DURATION_CONFIG,         include_reasons=include_reasons,     )     if baseline is None:         return renderer.build_table(self)     else:  # pragma: no cover         return renderer.build_diff_table(self, baseline) ``` |\n\n#### failures\\_table\n\n```\nfailures_table(\n    *,\n    include_input: bool = False,\n    include_metadata: bool = False,\n    include_expected_output: bool = False,\n    include_error_message: bool = True,\n    include_error_stacktrace: bool = True,\n    input_config: RenderValueConfig | None = None,\n    metadata_config: RenderValueConfig | None = None\n) -> Table\n```\n\nReturn a table containing the failures in this report.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`", "url": "https://ai.pydantic.dev/pydantic_evals/reporting/index.html#evaluationreport-dataclass", "page": "pydantic_evals/reporting/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluationReport `dataclass`", "anchor": "evaluationreport-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 ``` | ``` def failures_table(     self,     *,     include_input: bool = False,     include_metadata: bool = False,     include_expected_output: bool = False,     include_error_message: bool = True,     include_error_stacktrace: bool = True,     input_config: RenderValueConfig | None = None,     metadata_config: RenderValueConfig | None = None, ) -> Table:     \"\"\"Return a table containing the failures in this report.\"\"\"     renderer = EvaluationRenderer(         include_input=include_input,         include_metadata=include_metadata,         include_expected_output=include_expected_output,         include_output=False,         include_durations=False,         include_total_duration=False,         include_removed_cases=False,         include_averages=False,         input_config={**_DEFAULT_VALUE_CONFIG, **(input_config or {})},         metadata_config={**_DEFAULT_VALUE_CONFIG, **(metadata_config or {})},         output_config=_DEFAULT_VALUE_CONFIG,         score_configs={},         label_configs={},         metric_configs={},         duration_config=_DEFAULT_DURATION_CONFIG,         include_reasons=False,         include_error_message=include_error_message,         include_error_stacktrace=include_error_stacktrace,         include_evaluator_failures=False,  # Not applicable for failures table     )     return renderer.build_failures_table(self) ``` |\n\n#### \\_\\_str\\_\\_\n\n```\n__str__() -> str\n```\n\nReturn a string representation of the report.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 420 421 422 ``` | ``` def __str__(self) -> str:  # pragma: lax no cover     \"\"\"Return a string representation of the report.\"\"\"     return self.render() ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/reporting/index.html#evaluationreport-dataclass", "page": "pydantic_evals/reporting/index.html", "source_site": "pydantic_ai"}
{"title": "RenderValueConfig", "anchor": "rendervalueconfig", "md_text": "Bases: `TypedDict`\n\nA configuration for rendering a values in an Evaluation report.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 428 429 430 431 432 433 434 ``` | ``` class RenderValueConfig(TypedDict, total=False):     \"\"\"A configuration for rendering a values in an Evaluation report.\"\"\"      value_formatter: str | Callable[[Any], str]     diff_checker: Callable[[Any, Any], bool] | None     diff_formatter: Callable[[Any, Any], str | None] | None     diff_style: str ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/reporting/index.html#rendervalueconfig", "page": "pydantic_evals/reporting/index.html", "source_site": "pydantic_ai"}
{"title": "RenderNumberConfig", "anchor": "rendernumberconfig", "md_text": "Bases: `TypedDict`\n\nA configuration for rendering a particular score or metric in an Evaluation report.\n\nSee the implementation of `_RenderNumber` for more clarity on how these parameters affect the rendering.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`\n\n|  |  |\n| --- | --- |\n| ``` 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 ``` | ``` class RenderNumberConfig(TypedDict, total=False):     \"\"\"A configuration for rendering a particular score or metric in an Evaluation report.      See the implementation of `_RenderNumber` for more clarity on how these parameters affect the rendering.     \"\"\"      value_formatter: str | Callable[[float | int], str]     \"\"\"The logic to use for formatting values.      * If not provided, format as ints if all values are ints, otherwise at least one decimal place and at least four significant figures.     * You can also use a custom string format spec, e.g. '{:.3f}'     * You can also use a custom function, e.g. lambda x: f'{x:.3f}'     \"\"\"     diff_formatter: str | Callable[[float | int, float | int], str | None] | None     \"\"\"The logic to use for formatting details about the diff.      The strings produced by the value_formatter will always be included in the reports, but the diff_formatter is     used to produce additional text about the difference between the old and new values, such as the absolute or     relative difference.      * If not provided, format as ints if all values are ints, otherwise at least one decimal place and at least four         significant figures, and will include the percentage change.     * You can also use a custom string format spec, e.g. '{:+.3f}'     * You can also use a custom function, e.g. lambda x: f'{x:+.3f}'.         If this function returns None, no extra diff text will be added.     * You can also use None to never generate extra diff text.     \"\"\"     diff_atol: float     \"\"\"The absolute tolerance for considering a difference \"significant\".      A difference is \"significant\" if `abs(new - old) < self.diff_atol + self.diff_rtol * abs(old)`.      If a difference is not significant, it will not have the diff styles applied. Note that we still show     both the rendered before and after values in the diff any time they differ, even if the difference is not     significant. (If the rendered values are exactly the same, we only show the value once.)      If not provided, use 1e-6.     \"\"\"     diff_rtol: float     \"\"\"The relative tolerance for considering a difference \"significant\".      See the description of `diff_atol` for more details about what makes a difference \"significant\".      If not provided, use 0.001 if all values are ints, otherwise 0.05.     \"\"\"     diff_increase_style: str     \"\"\"The style to apply to diffed values that have a significant increase.      See the description of `diff_atol` for more details about what makes a difference \"significant\".      If not provided, use green for scores and red for metrics. You can also use arbitrary `rich` styles, such as \"bold red\".     \"\"\"     diff_decrease_style: str     \"\"\"The style to apply to diffed values that have significant decrease.      See the description of `diff_atol` for more details about what makes a difference \"significant\".      If not provided, use red for scores and green for metrics. You can also use arbitrary `rich` styles, such as \"bold red\".     \"\"\" ``` |\n\n#### value\\_formatter `instance-attribute`\n\n```\nvalue_formatter: str | Callable[[float | int], str]\n```\n\nThe logic to use for formatting values.\n\n* If not provided, format as ints if all values are ints, otherwise at least one decimal place and at least four significant figures.\n* You can also use a custom string format spec, e.g. '{:.3f}'\n* You can also use a custom function, e.g. lambda x: f'{x:.3f}'\n\n#### diff\\_formatter `instance-attribute`\n\n```\ndiff_formatter: (\n    str\n    | Callable[[float | int, float | int], str | None]\n    | None\n)\n```\n\nThe logic to use for formatting details about the diff.\n\nThe strings produced by the value\\_formatter will always be included in the reports, but the diff\\_formatter is\nused to produce additional text about the difference between the old and new values, such as the absolute or\nrelative difference.\n\n* If not provided, format as ints if all values are ints, otherwise at least one decimal place and at least four\n  significant figures, and will include the percentage change.\n* You can also use a custom string format spec, e.g. '{:+.3f}'\n* You can also use a custom function, e.g. lambda x: f'{x:+.3f}'.\n  If this function returns None, no extra diff text will be added.\n* You can also use None to never generate extra diff text.\n\n#### diff\\_atol `instance-attribute`\n\n```\ndiff_atol: float\n```", "url": "https://ai.pydantic.dev/pydantic_evals/reporting/index.html#rendernumberconfig", "page": "pydantic_evals/reporting/index.html", "source_site": "pydantic_ai"}
{"title": "RenderNumberConfig", "anchor": "rendernumberconfig", "md_text": "The absolute tolerance for considering a difference \"significant\".\n\nA difference is \"significant\" if `abs(new - old) < self.diff_atol + self.diff_rtol * abs(old)`.\n\nIf a difference is not significant, it will not have the diff styles applied. Note that we still show\nboth the rendered before and after values in the diff any time they differ, even if the difference is not\nsignificant. (If the rendered values are exactly the same, we only show the value once.)\n\nIf not provided, use 1e-6.\n\n#### diff\\_rtol `instance-attribute`\n\n```\ndiff_rtol: float\n```\n\nThe relative tolerance for considering a difference \"significant\".\n\nSee the description of `diff_atol` for more details about what makes a difference \"significant\".\n\nIf not provided, use 0.001 if all values are ints, otherwise 0.05.\n\n#### diff\\_increase\\_style `instance-attribute`\n\n```\ndiff_increase_style: str\n```\n\nThe style to apply to diffed values that have a significant increase.\n\nSee the description of `diff_atol` for more details about what makes a difference \"significant\".\n\nIf not provided, use green for scores and red for metrics. You can also use arbitrary `rich` styles, such as \"bold red\".\n\n#### diff\\_decrease\\_style `instance-attribute`\n\n```\ndiff_decrease_style: str\n```\n\nThe style to apply to diffed values that have significant decrease.\n\nSee the description of `diff_atol` for more details about what makes a difference \"significant\".\n\nIf not provided, use red for scores and green for metrics. You can also use arbitrary `rich` styles, such as \"bold red\".", "url": "https://ai.pydantic.dev/pydantic_evals/reporting/index.html#rendernumberconfig", "page": "pydantic_evals/reporting/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluationRenderer `dataclass`", "anchor": "evaluationrenderer-dataclass", "md_text": "A class for rendering an EvalReport or the diff between two EvalReports.\n\nSource code in `pydantic_evals/pydantic_evals/reporting/__init__.py`", "url": "https://ai.pydantic.dev/pydantic_evals/reporting/index.html#evaluationrenderer-dataclass", "page": "pydantic_evals/reporting/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluationRenderer `dataclass`", "anchor": "evaluationrenderer-dataclass", "md_text": "|  |  |\n| --- | --- |\n| ``` 1096 1097 1098 1099 1100 1101 1102 1103 1104 1105 1106 1107 1108 1109 1110 1111 1112 1113 1114 1115 1116 1117 1118 1119 1120 1121 1122 1123 1124 1125 1126 1127 1128 1129 1130 1131 1132 1133 1134 1135 1136 1137 1138 1139 1140 1141 1142 1143 1144 1145 1146 1147 1148 1149 1150 1151 1152 1153 1154 1155 1156 1157 1158 1159 1160 1161 1162 1163 1164 1165 1166 1167 1168 1169 1170 1171 1172 1173 1174 1175 1176 1177 1178 1179 1180 1181 1182 1183 1184 1185 1186 1187 1188 1189 1190 1191 1192 1193 1194 1195 1196 1197 1198 1199 1200 1201 1202 1203 1204 1205 1206 1207 1208 1209 1210 1211 1212 1213 1214 1215 1216 1217 1218 1219 1220 1221 1222 1223 1224 1225 1226 1227 1228 1229 1230 1231 1232 1233 1234 1235 1236 1237 1238 1239 1240 1241 1242 1243 1244 1245 1246 1247 1248 1249 1250 1251 1252 1253 1254 1255 1256 1257 1258 1259 1260 1261 1262 1263 1264 1265 1266 1267 1268 1269 1270 1271 1272 1273 1274 1275 1276 1277 1278 1279 1280 1281 1282 1283 1284 1285 1286 1287 1288 1289 1290 1291 1292 1293 1294 1295 1296 1297 1298 1299 1300 1301 1302 1303 1304 1305 1306 1307 1308 1309 1310 1311 1312 1313 1314 1315 ``` | ``` @dataclass(kw_only=True) class EvaluationRenderer:     \"\"\"A class for rendering an EvalReport or the diff between two EvalReports.\"\"\"      # Columns to include     include_input: bool     include_metadata: bool     include_expected_output: bool     include_output: bool     include_durations: bool     include_total_duration: bool      # Rows to include     include_removed_cases: bool     include_averages: bool      input_config: RenderValueConfig     metadata_config: RenderValueConfig     output_config: RenderValueConfig     score_configs: dict[str, RenderNumberConfig]     label_configs: dict[str, RenderValueConfig]     metric_configs: dict[str, RenderNumberConfig]     duration_config: RenderNumberConfig      # Data to include     include_reasons: bool  # only applies to reports, not to diffs      include_error_message: bool     include_error_stacktrace: bool     include_evaluator_failures: bool      def include_scores(self, report: EvaluationReport, baseline: EvaluationReport | None = None):         return any(case.scores for case in self._all_cases(report, baseline))      def include_labels(self, report: EvaluationReport, baseline: EvaluationReport | None = None):         return any(case.labels for case in self._all_cases(report, baseline))      def include_metrics(self, report: EvaluationReport, baseline: EvaluationReport | None = None):         return any(case.metrics for case in self._all_cases(report, baseline))      def include_assertions(self, report: EvaluationReport, baseline: EvaluationReport | None = None):         return any(case.assertions for case in self._all_cases(report, baseline))      def include_evaluator_failures_column(self, report: EvaluationReport, baseline: EvaluationReport | None = None):         return self.include_evaluator_failures and any(             case.evaluator_failures for case in self._all_cases(report, baseline)         )      def _all_cases(self, report: EvaluationReport, baseline: EvaluationReport | None) -> list[ReportCase]:         if not baseline:             return report.cases         else:             return report.cases + self._baseline_cases_to_include(report, baseline)      def _baseline_cases_to_include(self, report: EvaluationReport, baseline: EvaluationReport) -> list[ReportCase]:         if self.include_removed_cases:             return baseline.cases         report_case_names = {case.name for case in report.cases}         return [case for case in baseline.cases if case.name in report_case_names]      def _get_case_renderer(         self, report: EvaluationReport, baseline: EvaluationReport | None = None     ) -> ReportCaseRenderer:         input_renderer = _ValueRenderer.from_config(self.input_config)         metadata_renderer = _ValueRenderer.from_config(self.metadata_config)         output_renderer = _ValueRenderer.from_config(self.output_config)         score_renderers = self._infer_score_renderers(report, baseline)         label_renderers = self._infer_label_renderers(report, baseline)         metric_renderers = self._infer_metric_renderers(report, baseline)         duration_renderer = _NumberRenderer.infer_from_config(             self.duration_config, 'duration', [x.task_duration for x in self._all_cases(report, baseline)]         )          return ReportCaseRenderer(             include_input=self.include_input,             include_metadata=self.include_metadata,             include_expected_output=self.include_expected_output,             include_output=self.include_output,             include_scores=self.include_scores(report, baseline),             include_labels=self.include_labels(report, baseline),             include_metrics=self.include_metrics(report, baseline),             include_assertions=self.include_assertions(report, baseline),             include_reasons=self.include_reasons,             include_durations=self.include_durations,             include_total_duration=self.include_total_duration,             include_error_message=self.include_error_message,             include_error_stacktrace=self.include_error_stacktrace,             include_evaluator_failures=self.include_evaluator_failures_column(report, baseline),             input_renderer=input_renderer,             metadata_renderer=metadata_renderer,             output_renderer=output_renderer,             score_renderers=score_renderers,             label_renderers=label_renderers,             metric_renderers=metric_renderers,             duration_renderer=duration_renderer,         )      def build_table(self, report: EvaluationReport) -> Table:         case_renderer = self._get_case_renderer(report)         table = case_renderer.build_base_table(f'Evaluation Summary: {report.name}')         for case in report.cases:             table.add_row(*case_renderer.build_row(case))          if self.include_averages:  # pragma: no branch             average = report.averages()             if average:  # pragma: no branch                 table.add_row(*case_renderer.build_aggregate_row(average))          return table      def build_diff_table(self, report: EvaluationReport, baseline: EvaluationReport) -> Table:         report_cases = report.cases         baseline_cases = self._baseline_cases_to_include(report, baseline)          report_cases_by_id = {case.name: case for case in report_cases}         baseline_cases_by_id = {case.name: case for case in baseline_cases}          diff_cases: list[tuple[ReportCase, ReportCase]] = []         removed_cases: list[ReportCase] = []         added_cases: list[ReportCase] = []          for case_id in sorted(set(baseline_cases_by_id.keys()) | set(report_cases_by_id.keys())):             maybe_baseline_case = baseline_cases_by_id.get(case_id)             maybe_report_case = report_cases_by_id.get(case_id)             if maybe_baseline_case and maybe_report_case:                 diff_cases.append((maybe_baseline_case, maybe_report_case))             elif maybe_baseline_case:                 removed_cases.append(maybe_baseline_case)             elif maybe_report_case:                 added_cases.append(maybe_report_case)             else:  # pragma: no cover                 assert False, 'This should be unreachable'          case_renderer = self._get_case_renderer(report, baseline)         diff_name = baseline.name if baseline.name == report.name else f'{baseline.name} → {report.name}'         table = case_renderer.build_base_table(f'Evaluation Diff: {diff_name}')         for baseline_case, new_case in diff_cases:             table.add_row(*case_renderer.build_diff_row(new_case, baseline_case))         for case in added_cases:             row = case_renderer.build_row(case)             row[0] = f'[green]+ Added Case[/]\\n{row[0]}'             table.add_row(*row)         for case in removed_cases:             row = case_renderer.build_row(case)             row[0] = f'[red]- Removed Case[/]\\n{row[0]}'             table.add_row(*row)          if self.include_averages:  # pragma: no branch             report_average = ReportCaseAggregate.average(report_cases)             baseline_average = ReportCaseAggregate.average(baseline_cases)             table.add_row(*case_renderer.build_diff_aggregate_row(report_average, baseline_average))          return table      def build_failures_table(self, report: EvaluationReport) -> Table:         case_renderer = self._get_case_renderer(report)         table = case_renderer.build_failures_table('Case Failures')         for case in report.failures:             table.add_row(*case_renderer.build_failure_row(case))          return table      def _infer_score_renderers(         self, report: EvaluationReport, baseline: EvaluationReport | None     ) -> dict[str, _NumberRenderer]:         all_cases = self._all_cases(report, baseline)          values_by_name: dict[str, list[float | int]] = {}         for case in all_cases:             for k, score in case.scores.items():                 values_by_name.setdefault(k, []).append(score.value)          all_renderers: dict[str, _NumberRenderer] = {}         for name, values in values_by_name.items():             merged_config = _DEFAULT_NUMBER_CONFIG.copy()             merged_config.update(self.score_configs.get(name, {}))             all_renderers[name] = _NumberRenderer.infer_from_config(merged_config, 'score', values)         return all_renderers      def _infer_label_renderers(         self, report: EvaluationReport, baseline: EvaluationReport | None     ) -> dict[str, _ValueRenderer]:         all_cases = self._all_cases(report, baseline)         all_names: set[str] = set()         for case in all_cases:             for k in case.labels:                 all_names.add(k)          all_renderers: dict[str, _ValueRenderer] = {}         for name in all_names:             merged_config = _DEFAULT_VALUE_CONFIG.copy()             merged_config.update(self.label_configs.get(name, {}))             all_renderers[name] = _ValueRenderer.from_config(merged_config)         return all_renderers      def _infer_metric_renderers(         self, report: EvaluationReport, baseline: EvaluationReport | None     ) -> dict[str, _NumberRenderer]:         all_cases = self._all_cases(report, baseline)          values_by_name: dict[str, list[float | int]] = {}         for case in all_cases:             for k, v in case.metrics.items():                 values_by_name.setdefault(k, []).append(v)          all_renderers: dict[str, _NumberRenderer] = {}         for name, values in values_by_name.items():             merged_config = _DEFAULT_NUMBER_CONFIG.copy()             merged_config.update(self.metric_configs.get(name, {}))             all_renderers[name] = _NumberRenderer.infer_from_config(merged_config, 'metric', values)         return all_renderers      def _infer_duration_renderer(         self, report: EvaluationReport, baseline: EvaluationReport | None     ) -> _NumberRenderer:  # pragma: no cover         all_cases = self._all_cases(report, baseline)         all_durations = [x.task_duration for x in all_cases]         if self.include_total_duration:             all_durations += [x.total_duration for x in all_cases]         return _NumberRenderer.infer_from_config(self.duration_config, 'duration', all_durations) ``` |", "url": "https://ai.pydantic.dev/pydantic_evals/reporting/index.html#evaluationrenderer-dataclass", "page": "pydantic_evals/reporting/index.html", "source_site": "pydantic_ai"}
