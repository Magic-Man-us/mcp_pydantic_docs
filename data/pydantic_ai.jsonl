{"title": "Agent2Agent (A2A) Protocol", "anchor": "agent2agent-a2a-protocol", "heading_level": 1, "md_text": "The [Agent2Agent (A2A) Protocol](https://google.github.io/A2A/) is an open standard introduced by Google that enables communication and interoperability between AI agents, regardless of the framework or vendor they are built on. At Pydantic, we built the [FastA2A](index.html#fasta2a) library to make it easier to implement the A2A protocol in Python. We also built a convenience method that expose Pydantic AI agents as A2A servers - let's have a quick look at how to use it: agent\\_to\\_a2a.py *You can run the example with uvicorn agent_to_a2a:app --host 0.0.0.0 --port 8000 * This will expose the agent as an A2A server, and you can start sending requests to it. See more about [exposing Pydantic AI agents as A2A servers](index.html#pydantic-ai-agent-to-a2a-server). ## FastA2A **FastA2A** is an agentic framework agnostic implementation of the A2A protocol in Python. The library is designed to be used with any agentic framework, and is **not exclusive to Pydantic AI**. ### Design **FastA2A** is built on top of [Starlette](https://www.starlette.io), which means it's fully compatible with any ASGI server. Given the nature of the A2A protocol, it's important to understand the design before using it, as a developer you'll need to provide some components: * [ Storage ](../api/fasta2a/index.html#fasta2a.Storage): to save and load tasks, as well as store context for conversations * [ Broker ](../api/fasta2a/index.html#fasta2a.Broker): to schedule tasks * [ Worker ](../api/fasta2a/index.html#fasta2a.Worker): to execute tasks Let's have a look at how those components fit together: FastA2A allows you to bring your own [ Storage ](../api/fasta2a/index.html#fasta2a.Storage), [ Broker ](../api/fasta2a/index.html#fasta2a.Broker) and [ Worker ](../api/fasta2a/index.html#fasta2a.Worker). #### Understanding Tasks and Context In the A2A protocol: * **Task**: Represents one complete execution of an agent. When a client sends a message to the agent, a new task is created. The agent runs until completion (or failure), and this entire execution is considered one task. The final output is stored as a task artifact. * **Context**: Represents a conversation thread that can span multiple tasks. The A2A protocol uses a context_id to maintain conversation continuity: * When a new message is sent without a context_id , the server generates a new one * Subsequent messages can include the same context_id to continue the conversation * All tasks sharing the same context_id have access to the complete message history #### Storage Architecture The [ Storage ](../api/fasta2a/index.html#fasta2a.Storage) component serves two purposes: 1. **Task Storage**: Stores tasks in A2A protocol format, including their status, artifacts, and message history 2. **Context Storage**: Stores conversation context in a format optimized for the specific agent implementation This design allows for agents to store rich internal state (e.g., tool calls, reasoning traces) as well as store task-specific A2A-formatted messages and artifacts. For example, a Pydantic AI agent might store its complete internal message format (including tool calls and responses) in the context storage, while storing only the A2A-compliant messages in the task history. ### Installation FastA2A is available on PyPI as [ fasta2a ](https://pypi.org/project/fasta2a/) so installation is as simple as: pipuv The only dependencies are: * [starlette](https://www.starlette.io): to expose the A2A server as an [ASGI application](https://asgi.readthedocs.io/en/latest/) * [pydantic](https://pydantic.dev): to validate the request/response messages * [opentelemetry-api](https://opentelemetry-python.readthedocs.io/en/latest): to provide tracing capabilities You can install Pydantic AI with the a2a extra to include **FastA2A**: pipuv ### Pydantic AI Agent to A2A Server To expose a Pydantic AI agent as an A2A server, you can use the to_a2a method: agent\\_to\\_a2a.py Since app is an ASGI application, it can be used with any ASGI server. Since the goal of to_a2a is to be a convenience method, it accepts the same arguments as the [ FastA2A ](../api/fasta2a/index.html#fasta2a.FastA2A) constructor. When using to_a2a() , Pydantic AI automatically: * Stores the complete conversation history (including tool calls and responses) in the context storage * Ensures that subsequent messages with the same context_id have access to the full conversation history * Persists agent results as A2A artifacts: * String results become TextPart artifacts and also appear in the message history * Structured data (Pydantic models, dataclasses, tuples, etc.) become DataPart artifacts with the data wrapped as {\"result\": <your_data>} * Artifacts include metadata with type information and JSON schema when available", "url": "https://ai.pydantic.dev/a2a/index.html#agent2agent-a2a-protocol", "page": "a2a/index.html", "source_site": "pydantic_ai"}
{"title": "FastA2A", "anchor": "fasta2a", "heading_level": 2, "md_text": "**FastA2A** is an agentic framework agnostic implementation of the A2A protocol in Python. The library is designed to be used with any agentic framework, and is **not exclusive to Pydantic AI**. ### Design **FastA2A** is built on top of [Starlette](https://www.starlette.io), which means it's fully compatible with any ASGI server. Given the nature of the A2A protocol, it's important to understand the design before using it, as a developer you'll need to provide some components: * [ Storage ](../api/fasta2a/index.html#fasta2a.Storage): to save and load tasks, as well as store context for conversations * [ Broker ](../api/fasta2a/index.html#fasta2a.Broker): to schedule tasks * [ Worker ](../api/fasta2a/index.html#fasta2a.Worker): to execute tasks Let's have a look at how those components fit together: FastA2A allows you to bring your own [ Storage ](../api/fasta2a/index.html#fasta2a.Storage), [ Broker ](../api/fasta2a/index.html#fasta2a.Broker) and [ Worker ](../api/fasta2a/index.html#fasta2a.Worker). #### Understanding Tasks and Context In the A2A protocol: * **Task**: Represents one complete execution of an agent. When a client sends a message to the agent, a new task is created. The agent runs until completion (or failure), and this entire execution is considered one task. The final output is stored as a task artifact. * **Context**: Represents a conversation thread that can span multiple tasks. The A2A protocol uses a context_id to maintain conversation continuity: * When a new message is sent without a context_id , the server generates a new one * Subsequent messages can include the same context_id to continue the conversation * All tasks sharing the same context_id have access to the complete message history #### Storage Architecture The [ Storage ](../api/fasta2a/index.html#fasta2a.Storage) component serves two purposes: 1. **Task Storage**: Stores tasks in A2A protocol format, including their status, artifacts, and message history 2. **Context Storage**: Stores conversation context in a format optimized for the specific agent implementation This design allows for agents to store rich internal state (e.g., tool calls, reasoning traces) as well as store task-specific A2A-formatted messages and artifacts. For example, a Pydantic AI agent might store its complete internal message format (including tool calls and responses) in the context storage, while storing only the A2A-compliant messages in the task history. ### Installation FastA2A is available on PyPI as [ fasta2a ](https://pypi.org/project/fasta2a/) so installation is as simple as: pipuv The only dependencies are: * [starlette](https://www.starlette.io): to expose the A2A server as an [ASGI application](https://asgi.readthedocs.io/en/latest/) * [pydantic](https://pydantic.dev): to validate the request/response messages * [opentelemetry-api](https://opentelemetry-python.readthedocs.io/en/latest): to provide tracing capabilities You can install Pydantic AI with the a2a extra to include **FastA2A**: pipuv ### Pydantic AI Agent to A2A Server To expose a Pydantic AI agent as an A2A server, you can use the to_a2a method: agent\\_to\\_a2a.py Since app is an ASGI application, it can be used with any ASGI server. Since the goal of to_a2a is to be a convenience method, it accepts the same arguments as the [ FastA2A ](../api/fasta2a/index.html#fasta2a.FastA2A) constructor. When using to_a2a() , Pydantic AI automatically: * Stores the complete conversation history (including tool calls and responses) in the context storage * Ensures that subsequent messages with the same context_id have access to the full conversation history * Persists agent results as A2A artifacts: * String results become TextPart artifacts and also appear in the message history * Structured data (Pydantic models, dataclasses, tuples, etc.) become DataPart artifacts with the data wrapped as {\"result\": <your_data>} * Artifacts include metadata with type information and JSON schema when available", "url": "https://ai.pydantic.dev/a2a/index.html#fasta2a", "page": "a2a/index.html", "source_site": "pydantic_ai"}
{"title": "Design", "anchor": "design", "heading_level": 3, "md_text": "**FastA2A** is built on top of [Starlette](https://www.starlette.io), which means it's fully compatible with any ASGI server. Given the nature of the A2A protocol, it's important to understand the design before using it, as a developer you'll need to provide some components: * [ Storage ](../api/fasta2a/index.html#fasta2a.Storage): to save and load tasks, as well as store context for conversations * [ Broker ](../api/fasta2a/index.html#fasta2a.Broker): to schedule tasks * [ Worker ](../api/fasta2a/index.html#fasta2a.Worker): to execute tasks Let's have a look at how those components fit together: FastA2A allows you to bring your own [ Storage ](../api/fasta2a/index.html#fasta2a.Storage), [ Broker ](../api/fasta2a/index.html#fasta2a.Broker) and [ Worker ](../api/fasta2a/index.html#fasta2a.Worker). #### Understanding Tasks and Context In the A2A protocol: * **Task**: Represents one complete execution of an agent. When a client sends a message to the agent, a new task is created. The agent runs until completion (or failure), and this entire execution is considered one task. The final output is stored as a task artifact. * **Context**: Represents a conversation thread that can span multiple tasks. The A2A protocol uses a context_id to maintain conversation continuity: * When a new message is sent without a context_id , the server generates a new one * Subsequent messages can include the same context_id to continue the conversation * All tasks sharing the same context_id have access to the complete message history #### Storage Architecture The [ Storage ](../api/fasta2a/index.html#fasta2a.Storage) component serves two purposes: 1. **Task Storage**: Stores tasks in A2A protocol format, including their status, artifacts, and message history 2. **Context Storage**: Stores conversation context in a format optimized for the specific agent implementation This design allows for agents to store rich internal state (e.g., tool calls, reasoning traces) as well as store task-specific A2A-formatted messages and artifacts. For example, a Pydantic AI agent might store its complete internal message format (including tool calls and responses) in the context storage, while storing only the A2A-compliant messages in the task history.", "url": "https://ai.pydantic.dev/a2a/index.html#design", "page": "a2a/index.html", "source_site": "pydantic_ai"}
{"title": "Installation", "anchor": "installation", "heading_level": 3, "md_text": "FastA2A is available on PyPI as [ fasta2a ](https://pypi.org/project/fasta2a/) so installation is as simple as: pipuv The only dependencies are: * [starlette](https://www.starlette.io): to expose the A2A server as an [ASGI application](https://asgi.readthedocs.io/en/latest/) * [pydantic](https://pydantic.dev): to validate the request/response messages * [opentelemetry-api](https://opentelemetry-python.readthedocs.io/en/latest): to provide tracing capabilities You can install Pydantic AI with the a2a extra to include **FastA2A**: pipuv", "url": "https://ai.pydantic.dev/a2a/index.html#installation", "page": "a2a/index.html", "source_site": "pydantic_ai"}
{"title": "Pydantic AI Agent to A2A Server", "anchor": "pydantic-ai-agent-to-a2a-server", "heading_level": 3, "md_text": "To expose a Pydantic AI agent as an A2A server, you can use the to_a2a method: agent\\_to\\_a2a.py Since app is an ASGI application, it can be used with any ASGI server. Since the goal of to_a2a is to be a convenience method, it accepts the same arguments as the [ FastA2A ](../api/fasta2a/index.html#fasta2a.FastA2A) constructor. When using to_a2a() , Pydantic AI automatically: * Stores the complete conversation history (including tool calls and responses) in the context storage * Ensures that subsequent messages with the same context_id have access to the full conversation history * Persists agent results as A2A artifacts: * String results become TextPart artifacts and also appear in the message history * Structured data (Pydantic models, dataclasses, tuples, etc.) become DataPart artifacts with the data wrapped as {\"result\": <your_data>} * Artifacts include metadata with type information and JSON schema when available", "url": "https://ai.pydantic.dev/a2a/index.html#pydantic-ai-agent-to-a2a-server", "page": "a2a/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.common_tools", "anchor": "pydantic_aicommon_tools", "heading_level": 1, "md_text": "### duckduckgo\\_search\\_tool Creates a DuckDuckGo search tool. Parameters: Source code in pydantic_ai_slim/pydantic_ai/common_tools/duckduckgo.py ### tavily\\_search\\_tool Creates a Tavily search tool. Parameters: Source code in pydantic_ai_slim/pydantic_ai/common_tools/tavily.py", "url": "https://ai.pydantic.dev/api/common_tools/index.html#pydantic_aicommon_tools", "page": "api/common_tools/index.html", "source_site": "pydantic_ai"}
{"title": "duckduckgo_search_tool", "anchor": "pydantic_ai.common_tools.duckduckgo.duckduckgo_search_tool", "heading_level": 3, "md_text": "Creates a DuckDuckGo search tool. Parameters: Source code in pydantic_ai_slim/pydantic_ai/common_tools/duckduckgo.py", "url": "https://ai.pydantic.dev/api/common_tools/index.html#pydantic_ai.common_tools.duckduckgo.duckduckgo_search_tool", "page": "api/common_tools/index.html", "source_site": "pydantic_ai"}
{"title": "tavily_search_tool", "anchor": "pydantic_ai.common_tools.tavily.tavily_search_tool", "heading_level": 3, "md_text": "Creates a Tavily search tool. Parameters: Source code in pydantic_ai_slim/pydantic_ai/common_tools/tavily.py", "url": "https://ai.pydantic.dev/api/common_tools/index.html#pydantic_ai.common_tools.tavily.tavily_search_tool", "page": "api/common_tools/index.html", "source_site": "pydantic_ai"}
{"title": "Agent User Interaction (AG-UI) Protocol", "anchor": "agent-user-interaction-ag-ui-protocol", "heading_level": 1, "md_text": "The [Agent User Interaction (AG-UI) Protocol](https://docs.ag-ui.com/introduction) is an open standard introduced by the [CopilotKit](https://webflow.copilotkit.ai/blog/introducing-ag-ui-the-protocol-where-agents-meet-users) team that standardises how frontend applications communicate with AI agents, with support for streaming, frontend tools, shared state, and custom events. Note The AG-UI integration was originally built by the team at [Rocket Science](https://www.rocketscience.gg/) and contributed in collaboration with the Pydantic AI and CopilotKit teams. Thanks Rocket Science! ## Installation The only dependencies are: * [ag-ui-protocol](https://docs.ag-ui.com/introduction): to provide the AG-UI types and encoder. * [starlette](https://www.starlette.io): to handle [ASGI](https://asgi.readthedocs.io/en/latest/) requests from a framework like FastAPI. You can install Pydantic AI with the ag-ui extra to ensure you have all the required AG-UI dependencies: pipuv To run the examples you'll also need: * [uvicorn](https://www.uvicorn.org/) or another ASGI compatible server pipuv ## Usage There are three ways to run a Pydantic AI agent based on AG-UI run input with streamed AG-UI events as output, from most to least flexible. If you're using a Starlette-based web framework like FastAPI, you'll typically want to use the second method. 1. [ run_ag_ui() ](../api/ag_ui/index.html#pydantic_ai.ag_ui.run_ag_ui) takes an agent and an AG-UI [ RunAgentInput ](https://docs.ag-ui.com/sdk/python/core/types#runagentinput) object, and returns a stream of AG-UI events encoded as strings. It also takes optional [ Agent.iter() ](../api/agent/index.html#pydantic_ai.agent.Agent.iter) arguments including deps . Use this if you're using a web framework not based on Starlette (e.g. Django or Flask) or want to modify the input or output some way. 2. [ handle_ag_ui_request() ](../api/ag_ui/index.html#pydantic_ai.ag_ui.handle_ag_ui_request) takes an agent and a Starlette request (e.g. from FastAPI) coming from an AG-UI frontend, and returns a streaming Starlette response of AG-UI events that you can return directly from your endpoint. It also takes optional [ Agent.iter() ](../api/agent/index.html#pydantic_ai.agent.Agent.iter) arguments including deps , that you can vary for each request (e.g. based on the authenticated user). 3. [ Agent.to_ag_ui() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.to_ag_ui) returns an ASGI application that handles every AG-UI request by running the agent. It also takes optional [ Agent.iter() ](../api/agent/index.html#pydantic_ai.agent.Agent.iter) arguments including deps , but these will be the same for each request, with the exception of the AG-UI state that's injected as described under [state management](index.html#state-management). This ASGI app can be [mounted](https://fastapi.tiangolo.com/advanced/sub-applications/) at a given path in an existing FastAPI app. ### Handle run input and output directly This example uses [ run_ag_ui() ](../api/ag_ui/index.html#pydantic_ai.ag_ui.run_ag_ui) and performs its own request parsing and response generation. This can be modified to work with any web framework. run\\_ag\\_ui.py Since app is an ASGI application, it can be used with any ASGI server: This will expose the agent as an AG-UI server, and your frontend can start sending requests to it. ### Handle a Starlette request This example uses [ handle_ag_ui_request() ](../api/ag_ui/index.html#pydantic_ai.ag_ui.run_ag_ui) to directly handle a FastAPI request and return a response. Something analogous to this will work with any Starlette-based web framework. handle\\_ag\\_ui\\_request.py Since app is an ASGI application, it can be used with any ASGI server: This will expose the agent as an AG-UI server, and your frontend can start sending requests to it. ### Stand-alone ASGI app This example uses [ Agent.to_ag_ui() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.to_ag_ui) to turn the agent into a stand-alone ASGI application: agent\\_to\\_ag\\_ui.py Since app is an ASGI application, it can be used with any ASGI server: This will expose the agent as an AG-UI server, and your frontend can start sending requests to it. ## Design The Pydantic AI AG-UI integration supports all features of the spec: * [Events](https://docs.ag-ui.com/concepts/events) * [Messages](https://docs.ag-ui.com/concepts/messages) * [State Management](https://docs.ag-ui.com/concepts/state) * [Tools](https://docs.ag-ui.com/concepts/tools) The integration receives messages in the form of a [ RunAgentInput ](https://docs.ag-ui.com/sdk/python/core/types#runagentinput) object that describes the details of the requested agent run including message history, state, and available tools. These are converted to Pydantic AI types and passed to the agent's run method. Events from the agent, including tool calls, are converted to AG-UI events and streamed back to the caller as Server-Sent Events (SSE). A user request may require multiple round trips between client UI and Pydantic AI server, depending on the tools and events needed. ## Features ### State management The integration provides full support for [AG-UI state management](https://docs.ag-ui.com/concepts/state), which enables real-time synchronization between agents and frontend applications. In the example below we have document state which is shared between the UI and server using the [ StateDeps ](../api/ag_ui/index.html#pydantic_ai.ag_ui.StateDeps) [dependencies type](../dependencies/index.html) that can be used to automatically validate state contained in [ RunAgentInput.state ](https://docs.ag-ui.com/sdk/js/core/types#runagentinput) using a Pydantic BaseModel specified as a generic parameter. Custom dependencies type with AG-UI state If you want to use your own dependencies type to hold AG-UI state as well as other things, it needs to implements the [ StateHandler ](../api/ag_ui/index.html#pydantic_ai.ag_ui.StateHandler) protocol, meaning it needs to be a [dataclass](https://docs.python.org/3/library/dataclasses.html) with a non-optional state field. This lets Pydantic AI ensure that state is properly isolated between requests by building a new dependencies object each time. If the state field's type is a Pydantic BaseModel subclass, the raw state dictionary on the request is automatically validated. If not, you can validate the raw value yourself in your dependencies dataclass's __post_init__ method. ag\\_ui\\_state.py Since app is an ASGI application, it can be used with any ASGI server: ### Tools AG-UI frontend tools are seamlessly provided to the Pydantic AI agent, enabling rich user experiences with frontend user interfaces. ### Events Pydantic AI tools can send [AG-UI events](https://docs.ag-ui.com/concepts/events) simply by returning a [ ToolReturn ](../tools-advanced/index.html#advanced-tool-returns) object with a [ BaseEvent ](https://docs.ag-ui.com/sdk/python/core/events#baseevent) (or a list of events) as metadata , which allows for custom events and state updates. ag\\_ui\\_tool\\_events.py Since app is an ASGI application, it can be used with any ASGI server: ## Examples For more examples of how to use [ to_ag_ui() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.to_ag_ui) see [ pydantic_ai_examples.ag_ui ](https://github.com/pydantic/pydantic-ai/tree/main/examples/pydantic_ai_examples/ag_ui), which includes a server for use with the [AG-UI Dojo](https://docs.ag-ui.com/tutorials/debugging#the-ag-ui-dojo).", "url": "https://ai.pydantic.dev/ag-ui/index.html#agent-user-interaction-ag-ui-protocol", "page": "ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Installation", "anchor": "installation", "heading_level": 2, "md_text": "The only dependencies are: * [ag-ui-protocol](https://docs.ag-ui.com/introduction): to provide the AG-UI types and encoder. * [starlette](https://www.starlette.io): to handle [ASGI](https://asgi.readthedocs.io/en/latest/) requests from a framework like FastAPI. You can install Pydantic AI with the ag-ui extra to ensure you have all the required AG-UI dependencies: pipuv To run the examples you'll also need: * [uvicorn](https://www.uvicorn.org/) or another ASGI compatible server pipuv", "url": "https://ai.pydantic.dev/ag-ui/index.html#installation", "page": "ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 2, "md_text": "There are three ways to run a Pydantic AI agent based on AG-UI run input with streamed AG-UI events as output, from most to least flexible. If you're using a Starlette-based web framework like FastAPI, you'll typically want to use the second method. 1. [ run_ag_ui() ](../api/ag_ui/index.html#pydantic_ai.ag_ui.run_ag_ui) takes an agent and an AG-UI [ RunAgentInput ](https://docs.ag-ui.com/sdk/python/core/types#runagentinput) object, and returns a stream of AG-UI events encoded as strings. It also takes optional [ Agent.iter() ](../api/agent/index.html#pydantic_ai.agent.Agent.iter) arguments including deps . Use this if you're using a web framework not based on Starlette (e.g. Django or Flask) or want to modify the input or output some way. 2. [ handle_ag_ui_request() ](../api/ag_ui/index.html#pydantic_ai.ag_ui.handle_ag_ui_request) takes an agent and a Starlette request (e.g. from FastAPI) coming from an AG-UI frontend, and returns a streaming Starlette response of AG-UI events that you can return directly from your endpoint. It also takes optional [ Agent.iter() ](../api/agent/index.html#pydantic_ai.agent.Agent.iter) arguments including deps , that you can vary for each request (e.g. based on the authenticated user). 3. [ Agent.to_ag_ui() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.to_ag_ui) returns an ASGI application that handles every AG-UI request by running the agent. It also takes optional [ Agent.iter() ](../api/agent/index.html#pydantic_ai.agent.Agent.iter) arguments including deps , but these will be the same for each request, with the exception of the AG-UI state that's injected as described under [state management](index.html#state-management). This ASGI app can be [mounted](https://fastapi.tiangolo.com/advanced/sub-applications/) at a given path in an existing FastAPI app. ### Handle run input and output directly This example uses [ run_ag_ui() ](../api/ag_ui/index.html#pydantic_ai.ag_ui.run_ag_ui) and performs its own request parsing and response generation. This can be modified to work with any web framework. run\\_ag\\_ui.py Since app is an ASGI application, it can be used with any ASGI server: This will expose the agent as an AG-UI server, and your frontend can start sending requests to it. ### Handle a Starlette request This example uses [ handle_ag_ui_request() ](../api/ag_ui/index.html#pydantic_ai.ag_ui.run_ag_ui) to directly handle a FastAPI request and return a response. Something analogous to this will work with any Starlette-based web framework. handle\\_ag\\_ui\\_request.py Since app is an ASGI application, it can be used with any ASGI server: This will expose the agent as an AG-UI server, and your frontend can start sending requests to it. ### Stand-alone ASGI app This example uses [ Agent.to_ag_ui() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.to_ag_ui) to turn the agent into a stand-alone ASGI application: agent\\_to\\_ag\\_ui.py Since app is an ASGI application, it can be used with any ASGI server: This will expose the agent as an AG-UI server, and your frontend can start sending requests to it.", "url": "https://ai.pydantic.dev/ag-ui/index.html#usage", "page": "ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Handle run input and output directly", "anchor": "handle-run-input-and-output-directly", "heading_level": 3, "md_text": "This example uses [ run_ag_ui() ](../api/ag_ui/index.html#pydantic_ai.ag_ui.run_ag_ui) and performs its own request parsing and response generation. This can be modified to work with any web framework. run\\_ag\\_ui.py Since app is an ASGI application, it can be used with any ASGI server: This will expose the agent as an AG-UI server, and your frontend can start sending requests to it.", "url": "https://ai.pydantic.dev/ag-ui/index.html#handle-run-input-and-output-directly", "page": "ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Handle a Starlette request", "anchor": "handle-a-starlette-request", "heading_level": 3, "md_text": "This example uses [ handle_ag_ui_request() ](../api/ag_ui/index.html#pydantic_ai.ag_ui.run_ag_ui) to directly handle a FastAPI request and return a response. Something analogous to this will work with any Starlette-based web framework. handle\\_ag\\_ui\\_request.py Since app is an ASGI application, it can be used with any ASGI server: This will expose the agent as an AG-UI server, and your frontend can start sending requests to it.", "url": "https://ai.pydantic.dev/ag-ui/index.html#handle-a-starlette-request", "page": "ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Stand-alone ASGI app", "anchor": "stand-alone-asgi-app", "heading_level": 3, "md_text": "This example uses [ Agent.to_ag_ui() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.to_ag_ui) to turn the agent into a stand-alone ASGI application: agent\\_to\\_ag\\_ui.py Since app is an ASGI application, it can be used with any ASGI server: This will expose the agent as an AG-UI server, and your frontend can start sending requests to it.", "url": "https://ai.pydantic.dev/ag-ui/index.html#stand-alone-asgi-app", "page": "ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Design", "anchor": "design", "heading_level": 2, "md_text": "The Pydantic AI AG-UI integration supports all features of the spec: * [Events](https://docs.ag-ui.com/concepts/events) * [Messages](https://docs.ag-ui.com/concepts/messages) * [State Management](https://docs.ag-ui.com/concepts/state) * [Tools](https://docs.ag-ui.com/concepts/tools) The integration receives messages in the form of a [ RunAgentInput ](https://docs.ag-ui.com/sdk/python/core/types#runagentinput) object that describes the details of the requested agent run including message history, state, and available tools. These are converted to Pydantic AI types and passed to the agent's run method. Events from the agent, including tool calls, are converted to AG-UI events and streamed back to the caller as Server-Sent Events (SSE). A user request may require multiple round trips between client UI and Pydantic AI server, depending on the tools and events needed.", "url": "https://ai.pydantic.dev/ag-ui/index.html#design", "page": "ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Features", "anchor": "features", "heading_level": 2, "md_text": "### State management The integration provides full support for [AG-UI state management](https://docs.ag-ui.com/concepts/state), which enables real-time synchronization between agents and frontend applications. In the example below we have document state which is shared between the UI and server using the [ StateDeps ](../api/ag_ui/index.html#pydantic_ai.ag_ui.StateDeps) [dependencies type](../dependencies/index.html) that can be used to automatically validate state contained in [ RunAgentInput.state ](https://docs.ag-ui.com/sdk/js/core/types#runagentinput) using a Pydantic BaseModel specified as a generic parameter. Custom dependencies type with AG-UI state If you want to use your own dependencies type to hold AG-UI state as well as other things, it needs to implements the [ StateHandler ](../api/ag_ui/index.html#pydantic_ai.ag_ui.StateHandler) protocol, meaning it needs to be a [dataclass](https://docs.python.org/3/library/dataclasses.html) with a non-optional state field. This lets Pydantic AI ensure that state is properly isolated between requests by building a new dependencies object each time. If the state field's type is a Pydantic BaseModel subclass, the raw state dictionary on the request is automatically validated. If not, you can validate the raw value yourself in your dependencies dataclass's __post_init__ method. ag\\_ui\\_state.py Since app is an ASGI application, it can be used with any ASGI server: ### Tools AG-UI frontend tools are seamlessly provided to the Pydantic AI agent, enabling rich user experiences with frontend user interfaces. ### Events Pydantic AI tools can send [AG-UI events](https://docs.ag-ui.com/concepts/events) simply by returning a [ ToolReturn ](../tools-advanced/index.html#advanced-tool-returns) object with a [ BaseEvent ](https://docs.ag-ui.com/sdk/python/core/events#baseevent) (or a list of events) as metadata , which allows for custom events and state updates. ag\\_ui\\_tool\\_events.py Since app is an ASGI application, it can be used with any ASGI server:", "url": "https://ai.pydantic.dev/ag-ui/index.html#features", "page": "ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "State management", "anchor": "state-management", "heading_level": 3, "md_text": "The integration provides full support for [AG-UI state management](https://docs.ag-ui.com/concepts/state), which enables real-time synchronization between agents and frontend applications. In the example below we have document state which is shared between the UI and server using the [ StateDeps ](../api/ag_ui/index.html#pydantic_ai.ag_ui.StateDeps) [dependencies type](../dependencies/index.html) that can be used to automatically validate state contained in [ RunAgentInput.state ](https://docs.ag-ui.com/sdk/js/core/types#runagentinput) using a Pydantic BaseModel specified as a generic parameter. Custom dependencies type with AG-UI state If you want to use your own dependencies type to hold AG-UI state as well as other things, it needs to implements the [ StateHandler ](../api/ag_ui/index.html#pydantic_ai.ag_ui.StateHandler) protocol, meaning it needs to be a [dataclass](https://docs.python.org/3/library/dataclasses.html) with a non-optional state field. This lets Pydantic AI ensure that state is properly isolated between requests by building a new dependencies object each time. If the state field's type is a Pydantic BaseModel subclass, the raw state dictionary on the request is automatically validated. If not, you can validate the raw value yourself in your dependencies dataclass's __post_init__ method. ag\\_ui\\_state.py Since app is an ASGI application, it can be used with any ASGI server:", "url": "https://ai.pydantic.dev/ag-ui/index.html#state-management", "page": "ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Tools", "anchor": "tools", "heading_level": 3, "md_text": "AG-UI frontend tools are seamlessly provided to the Pydantic AI agent, enabling rich user experiences with frontend user interfaces.", "url": "https://ai.pydantic.dev/ag-ui/index.html#tools", "page": "ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Events", "anchor": "events", "heading_level": 3, "md_text": "Pydantic AI tools can send [AG-UI events](https://docs.ag-ui.com/concepts/events) simply by returning a [ ToolReturn ](../tools-advanced/index.html#advanced-tool-returns) object with a [ BaseEvent ](https://docs.ag-ui.com/sdk/python/core/events#baseevent) (or a list of events) as metadata , which allows for custom events and state updates. ag\\_ui\\_tool\\_events.py Since app is an ASGI application, it can be used with any ASGI server:", "url": "https://ai.pydantic.dev/ag-ui/index.html#events", "page": "ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Examples", "anchor": "examples", "heading_level": 2, "md_text": "For more examples of how to use [ to_ag_ui() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.to_ag_ui) see [ pydantic_ai_examples.ag_ui ](https://github.com/pydantic/pydantic-ai/tree/main/examples/pydantic_ai_examples/ag_ui), which includes a server for use with the [AG-UI Dojo](https://docs.ag-ui.com/tutorials/debugging#the-ag-ui-dojo).", "url": "https://ai.pydantic.dev/ag-ui/index.html#examples", "page": "ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.builtin_tools", "anchor": "pydantic_aibuiltin_tools", "heading_level": 1, "md_text": "### AbstractBuiltinTool dataclass Bases: ABC A builtin tool that can be used by an agent. This class is abstract and cannot be instantiated directly. The builtin tools are passed to the model as part of the ModelRequestParameters . Source code in pydantic_ai_slim/pydantic_ai/builtin_tools.py #### kind class-attribute instance-attribute Built-in tool identifier, this should be available on all built-in tools as a discriminator. #### unique\\_id property A unique identifier for the builtin tool. If multiple instances of the same builtin tool can be passed to the model, subclasses should override this property to allow them to be distinguished. ### WebSearchTool dataclass Bases: AbstractBuiltinTool A builtin tool that allows your agent to search the web for information. The parameters that PydanticAI passes depend on the model, as some parameters may not be supported by certain models. Supported by: * Anthropic * OpenAI Responses * Groq * Google Source code in pydantic_ai_slim/pydantic_ai/builtin_tools.py #### search\\_context\\_size class-attribute instance-attribute The search_context_size parameter controls how much context is retrieved from the web to help the tool formulate a response. Supported by: * OpenAI Responses #### user\\_location class-attribute instance-attribute The user_location parameter allows you to localize search results based on a user's location. Supported by: * Anthropic * OpenAI Responses #### blocked\\_domains class-attribute instance-attribute If provided, these domains will never appear in results. With Anthropic, you can only use one of blocked_domains or allowed_domains , not both. Supported by: * Anthropic, see <https://docs.anthropic.com/en/docs/build-with-claude/tool-use/web-search-tool#domain-filtering> * Groq, see <https://console.groq.com/docs/agentic-tooling#search-settings> #### allowed\\_domains class-attribute instance-attribute If provided, only these domains will be included in results. With Anthropic, you can only use one of blocked_domains or allowed_domains , not both. Supported by: * Anthropic, see <https://docs.anthropic.com/en/docs/build-with-claude/tool-use/web-search-tool#domain-filtering> * Groq, see <https://console.groq.com/docs/agentic-tooling#search-settings> #### max\\_uses class-attribute instance-attribute If provided, the tool will stop searching the web after the given number of uses. Supported by: * Anthropic #### kind class-attribute instance-attribute The kind of tool. ### WebSearchUserLocation Bases: TypedDict Allows you to localize search results based on a user's location. Supported by: * Anthropic * OpenAI Responses Source code in pydantic_ai_slim/pydantic_ai/builtin_tools.py #### city instance-attribute The city where the user is located. #### country instance-attribute The country where the user is located. For OpenAI, this must be a 2-letter country code (e.g., 'US', 'GB'). #### region instance-attribute The region or state where the user is located. #### timezone instance-attribute The timezone of the user's location. ### CodeExecutionTool dataclass Bases: AbstractBuiltinTool A builtin tool that allows your agent to execute code. Supported by: * Anthropic * OpenAI Responses * Google Source code in pydantic_ai_slim/pydantic_ai/builtin_tools.py #### kind class-attribute instance-attribute The kind of tool. ### UrlContextTool dataclass Bases: AbstractBuiltinTool Allows your agent to access contents from URLs. Supported by: * Google Source code in pydantic_ai_slim/pydantic_ai/builtin_tools.py #### kind class-attribute instance-attribute The kind of tool. ### ImageGenerationTool dataclass Bases: AbstractBuiltinTool A builtin tool that allows your agent to generate images. Supported by: * OpenAI Responses * Google Source code in pydantic_ai_slim/pydantic_ai/builtin_tools.py #### background class-attribute instance-attribute Background type for the generated image. Supported by: * OpenAI Responses. 'transparent' is only supported for 'png' and 'webp' output formats. #### input\\_fidelity class-attribute instance-attribute Control how much effort the model will exert to match the style and features, especially facial features, of input images. Supported by: * OpenAI Responses. Default: 'low'. #### moderation class-attribute instance-attribute Moderation level for the generated image. Supported by: * OpenAI Responses #### output\\_compression class-attribute instance-attribute Compression level for the output image. Supported by: * OpenAI Responses. Only supported for 'png' and 'webp' output formats. #### output\\_format class-attribute instance-attribute The output format of the generated image. Supported by: * OpenAI Responses. Default: 'png'. #### partial\\_images class-attribute instance-attribute Number of partial images to generate in streaming mode. Supported by: * OpenAI Responses. Supports 0 to 3. #### quality class-attribute instance-attribute The quality of the generated image. Supported by: * OpenAI Responses #### size class-attribute instance-attribute The size of the generated image. Supported by: * OpenAI Responses #### kind class-attribute instance-attribute The kind of tool. ### MemoryTool dataclass Bases: AbstractBuiltinTool A builtin tool that allows your agent to use memory. Supported by: * Anthropic Source code in pydantic_ai_slim/pydantic_ai/builtin_tools.py #### kind class-attribute instance-attribute The kind of tool. ### MCPServerTool dataclass Bases: AbstractBuiltinTool A builtin tool that allows your agent to use MCP servers. Supported by: * OpenAI Responses * Anthropic Source code in pydantic_ai_slim/pydantic_ai/builtin_tools.py #### id instance-attribute A unique identifier for the MCP server. #### url instance-attribute The URL of the MCP server to use. For OpenAI Responses, it is possible to use connector_id by providing it as x-openai-connector:<connector_id> . #### authorization\\_token class-attribute instance-attribute Authorization header to use when making requests to the MCP server. Supported by: * OpenAI Responses * Anthropic #### description class-attribute instance-attribute A description of the MCP server. Supported by: * OpenAI Responses #### allowed\\_tools class-attribute instance-attribute A list of tools that the MCP server can use. Supported by: * OpenAI Responses * Anthropic #### headers class-attribute instance-attribute Optional HTTP headers to send to the MCP server. Use for authentication or other purposes. Supported by: * OpenAI Responses", "url": "https://ai.pydantic.dev/api/builtin_tools/index.html#pydantic_aibuiltin_tools", "page": "api/builtin_tools/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractBuiltinTool dataclass", "anchor": "pydantic_ai.builtin_tools.AbstractBuiltinTool", "heading_level": 3, "md_text": "Bases: ABC A builtin tool that can be used by an agent. This class is abstract and cannot be instantiated directly. The builtin tools are passed to the model as part of the ModelRequestParameters . Source code in pydantic_ai_slim/pydantic_ai/builtin_tools.py #### kind class-attribute instance-attribute Built-in tool identifier, this should be available on all built-in tools as a discriminator. #### unique\\_id property A unique identifier for the builtin tool. If multiple instances of the same builtin tool can be passed to the model, subclasses should override this property to allow them to be distinguished.", "url": "https://ai.pydantic.dev/api/builtin_tools/index.html#pydantic_ai.builtin_tools.AbstractBuiltinTool", "page": "api/builtin_tools/index.html", "source_site": "pydantic_ai"}
{"title": "WebSearchTool dataclass", "anchor": "pydantic_ai.builtin_tools.WebSearchTool", "heading_level": 3, "md_text": "Bases: AbstractBuiltinTool A builtin tool that allows your agent to search the web for information. The parameters that PydanticAI passes depend on the model, as some parameters may not be supported by certain models. Supported by: * Anthropic * OpenAI Responses * Groq * Google Source code in pydantic_ai_slim/pydantic_ai/builtin_tools.py #### search\\_context\\_size class-attribute instance-attribute The search_context_size parameter controls how much context is retrieved from the web to help the tool formulate a response. Supported by: * OpenAI Responses #### user\\_location class-attribute instance-attribute The user_location parameter allows you to localize search results based on a user's location. Supported by: * Anthropic * OpenAI Responses #### blocked\\_domains class-attribute instance-attribute If provided, these domains will never appear in results. With Anthropic, you can only use one of blocked_domains or allowed_domains , not both. Supported by: * Anthropic, see <https://docs.anthropic.com/en/docs/build-with-claude/tool-use/web-search-tool#domain-filtering> * Groq, see <https://console.groq.com/docs/agentic-tooling#search-settings> #### allowed\\_domains class-attribute instance-attribute If provided, only these domains will be included in results. With Anthropic, you can only use one of blocked_domains or allowed_domains , not both. Supported by: * Anthropic, see <https://docs.anthropic.com/en/docs/build-with-claude/tool-use/web-search-tool#domain-filtering> * Groq, see <https://console.groq.com/docs/agentic-tooling#search-settings> #### max\\_uses class-attribute instance-attribute If provided, the tool will stop searching the web after the given number of uses. Supported by: * Anthropic #### kind class-attribute instance-attribute The kind of tool.", "url": "https://ai.pydantic.dev/api/builtin_tools/index.html#pydantic_ai.builtin_tools.WebSearchTool", "page": "api/builtin_tools/index.html", "source_site": "pydantic_ai"}
{"title": "WebSearchUserLocation", "anchor": "pydantic_ai.builtin_tools.WebSearchUserLocation", "heading_level": 3, "md_text": "Bases: TypedDict Allows you to localize search results based on a user's location. Supported by: * Anthropic * OpenAI Responses Source code in pydantic_ai_slim/pydantic_ai/builtin_tools.py #### city instance-attribute The city where the user is located. #### country instance-attribute The country where the user is located. For OpenAI, this must be a 2-letter country code (e.g., 'US', 'GB'). #### region instance-attribute The region or state where the user is located. #### timezone instance-attribute The timezone of the user's location.", "url": "https://ai.pydantic.dev/api/builtin_tools/index.html#pydantic_ai.builtin_tools.WebSearchUserLocation", "page": "api/builtin_tools/index.html", "source_site": "pydantic_ai"}
{"title": "CodeExecutionTool dataclass", "anchor": "pydantic_ai.builtin_tools.CodeExecutionTool", "heading_level": 3, "md_text": "Bases: AbstractBuiltinTool A builtin tool that allows your agent to execute code. Supported by: * Anthropic * OpenAI Responses * Google Source code in pydantic_ai_slim/pydantic_ai/builtin_tools.py #### kind class-attribute instance-attribute The kind of tool.", "url": "https://ai.pydantic.dev/api/builtin_tools/index.html#pydantic_ai.builtin_tools.CodeExecutionTool", "page": "api/builtin_tools/index.html", "source_site": "pydantic_ai"}
{"title": "UrlContextTool dataclass", "anchor": "pydantic_ai.builtin_tools.UrlContextTool", "heading_level": 3, "md_text": "Bases: AbstractBuiltinTool Allows your agent to access contents from URLs. Supported by: * Google Source code in pydantic_ai_slim/pydantic_ai/builtin_tools.py #### kind class-attribute instance-attribute The kind of tool.", "url": "https://ai.pydantic.dev/api/builtin_tools/index.html#pydantic_ai.builtin_tools.UrlContextTool", "page": "api/builtin_tools/index.html", "source_site": "pydantic_ai"}
{"title": "ImageGenerationTool dataclass", "anchor": "pydantic_ai.builtin_tools.ImageGenerationTool", "heading_level": 3, "md_text": "Bases: AbstractBuiltinTool A builtin tool that allows your agent to generate images. Supported by: * OpenAI Responses * Google Source code in pydantic_ai_slim/pydantic_ai/builtin_tools.py #### background class-attribute instance-attribute Background type for the generated image. Supported by: * OpenAI Responses. 'transparent' is only supported for 'png' and 'webp' output formats. #### input\\_fidelity class-attribute instance-attribute Control how much effort the model will exert to match the style and features, especially facial features, of input images. Supported by: * OpenAI Responses. Default: 'low'. #### moderation class-attribute instance-attribute Moderation level for the generated image. Supported by: * OpenAI Responses #### output\\_compression class-attribute instance-attribute Compression level for the output image. Supported by: * OpenAI Responses. Only supported for 'png' and 'webp' output formats. #### output\\_format class-attribute instance-attribute The output format of the generated image. Supported by: * OpenAI Responses. Default: 'png'. #### partial\\_images class-attribute instance-attribute Number of partial images to generate in streaming mode. Supported by: * OpenAI Responses. Supports 0 to 3. #### quality class-attribute instance-attribute The quality of the generated image. Supported by: * OpenAI Responses #### size class-attribute instance-attribute The size of the generated image. Supported by: * OpenAI Responses #### kind class-attribute instance-attribute The kind of tool.", "url": "https://ai.pydantic.dev/api/builtin_tools/index.html#pydantic_ai.builtin_tools.ImageGenerationTool", "page": "api/builtin_tools/index.html", "source_site": "pydantic_ai"}
{"title": "MemoryTool dataclass", "anchor": "pydantic_ai.builtin_tools.MemoryTool", "heading_level": 3, "md_text": "Bases: AbstractBuiltinTool A builtin tool that allows your agent to use memory. Supported by: * Anthropic Source code in pydantic_ai_slim/pydantic_ai/builtin_tools.py #### kind class-attribute instance-attribute The kind of tool.", "url": "https://ai.pydantic.dev/api/builtin_tools/index.html#pydantic_ai.builtin_tools.MemoryTool", "page": "api/builtin_tools/index.html", "source_site": "pydantic_ai"}
{"title": "MCPServerTool dataclass", "anchor": "pydantic_ai.builtin_tools.MCPServerTool", "heading_level": 3, "md_text": "Bases: AbstractBuiltinTool A builtin tool that allows your agent to use MCP servers. Supported by: * OpenAI Responses * Anthropic Source code in pydantic_ai_slim/pydantic_ai/builtin_tools.py #### id instance-attribute A unique identifier for the MCP server. #### url instance-attribute The URL of the MCP server to use. For OpenAI Responses, it is possible to use connector_id by providing it as x-openai-connector:<connector_id> . #### authorization\\_token class-attribute instance-attribute Authorization header to use when making requests to the MCP server. Supported by: * OpenAI Responses * Anthropic #### description class-attribute instance-attribute A description of the MCP server. Supported by: * OpenAI Responses #### allowed\\_tools class-attribute instance-attribute A list of tools that the MCP server can use. Supported by: * OpenAI Responses * Anthropic #### headers class-attribute instance-attribute Optional HTTP headers to send to the MCP server. Use for authentication or other purposes. Supported by: * OpenAI Responses", "url": "https://ai.pydantic.dev/api/builtin_tools/index.html#pydantic_ai.builtin_tools.MCPServerTool", "page": "api/builtin_tools/index.html", "source_site": "pydantic_ai"}
{"title": "Introduction", "anchor": "introduction", "heading_level": 2, "md_text": "Agents are Pydantic AI's primary interface for interacting with LLMs. In some use cases a single Agent will control an entire application or component, but multiple agents can also interact to embody more complex workflows. The [ Agent ](../api/agent/index.html#pydantic_ai.agent.Agent) class has full API documentation, but conceptually you can think of an agent as a container for: In typing terms, agents are generic in their dependency and output types, e.g., an agent which required dependencies of type Foobar and produced outputs of type list[str] would have type Agent[Foobar, list[str]] . In practice, you shouldn't need to care about this, it should just mean your IDE can tell you when you have the right type, and if you choose to use [static type checking](index.html#static-type-checking) it should work well with Pydantic AI. Here's a toy example of an agent that simulates a roulette wheel: roulette\\_wheel.py 1. Create an agent, which expects an integer dependency and produces a boolean output. This agent will have type Agent[int, bool] . 2. Define a tool that checks if the square is a winner. Here [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext) is parameterized with the dependency type int ; if you got the dependency type wrong you'd get a typing error. 3. In reality, you might want to use a random number here e.g. random.randint(0, 36) . 4. result.output will be a boolean indicating if the square is a winner. Pydantic performs the output validation, and it'll be typed as a bool since its type is derived from the output_type generic parameter of the agent. Agents are designed for reuse, like FastAPI Apps Agents are intended to be instantiated once (frequently as module globals) and reused throughout your application, similar to a small [FastAPI](https://fastapi.tiangolo.com/reference/fastapi/#fastapi.FastAPI) app or an [APIRouter](https://fastapi.tiangolo.com/reference/apirouter/#fastapi.APIRouter).", "url": "https://ai.pydantic.dev/agents/index.html#introduction", "page": "agents/index.html", "source_site": "pydantic_ai"}
{"title": "Running Agents", "anchor": "running-agents", "heading_level": 2, "md_text": "There are five ways to run an agent: 1. [ agent.run() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run)  an async function which returns a [ RunResult ](../api/agent/index.html#pydantic_ai.agent.AgentRunResult) containing a completed response. 2. [ agent.run_sync() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_sync)  a plain, synchronous function which returns a [ RunResult ](../api/agent/index.html#pydantic_ai.agent.AgentRunResult) containing a completed response (internally, this just calls loop.run_until_complete(self.run()) ). 3. [ agent.run_stream() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream)  an async context manager which returns a [ StreamedRunResult ](../api/result/index.html#pydantic_ai.result.StreamedRunResult), which contains methods to stream text and structured output as an async iterable. 4. [ agent.run_stream_events() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream_events)  a function which returns an async iterable of [ AgentStreamEvent s](../api/messages/index.html#pydantic_ai.messages.AgentStreamEvent) and a [ AgentRunResultEvent ](../api/run/index.html#pydantic_ai.run.AgentRunResultEvent) containing the final run result. 5. [ agent.iter() ](../api/agent/index.html#pydantic_ai.agent.Agent.iter)  a context manager which returns an [ AgentRun ](../api/agent/index.html#pydantic_ai.agent.AgentRun), an async iterable over the nodes of the agent's underlying [ Graph ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph). Here's a simple example demonstrating the first four: run\\_agent.py *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* You can also pass messages from previous runs to continue a conversation or provide context, as described in [Messages and Chat History](../message-history/index.html). ### Streaming Events and Final Output As shown in the example above, [ run_stream() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream) makes it easy to stream the agent's final output as it comes in. It also takes an optional event_stream_handler argument that you can use to gain insight into what is happening during the run before the final output is produced. The example below shows how to stream events and text output. You can also [stream structured output](../output/index.html#streaming-structured-output). Note As the run_stream() method will consider the first output matching the [output type](../output/index.html#structured-output) to be the final output, it will stop running the agent graph and will not execute any tool calls made by the model after this \"final\" output. If you want to always run the agent graph to completion and stream all events from the model's streaming response and the agent's execution of tools, use [ agent.run_stream_events() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream_events) or [ agent.iter() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.iter) instead, as described in the following sections. run\\_stream\\_event\\_stream\\_handler.py ### Streaming All Events Like agent.run_stream() , [ agent.run() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream) takes an optional event_stream_handler argument that lets you stream all events from the model's streaming response and the agent's execution of tools. Unlike run_stream() , it always runs the agent graph to completion even if text was received ahead of tool calls that looked like it could've been the final result. For convenience, a [ agent.run_stream_events() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream_events) method is also available as a wrapper around run(event_stream_handler=...) , which returns an async iterable of [ AgentStreamEvent s](../api/messages/index.html#pydantic_ai.messages.AgentStreamEvent) and a [ AgentRunResultEvent ](../api/run/index.html#pydantic_ai.run.AgentRunResultEvent) containing the final run result. Note As they return raw events as they come in, the run_stream_events() and run(event_stream_handler=...) methods require you to piece together the streamed text and structured output yourself from the PartStartEvent and subsequent PartDeltaEvent s. To get the best of both worlds, at the expense of some additional complexity, you can use [ agent.iter() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.iter) as described in the next section, which lets you [iterate over the agent graph](index.html#iterating-over-an-agents-graph) and [stream both events and output](index.html#streaming-all-events-and-output) at every step. run\\_events.py *(This example is complete, it can be run \"as is\")* ### Iterating Over an Agent's Graph Under the hood, each Agent in Pydantic AI uses **pydantic-graph** to manage its execution flow. **pydantic-graph** is a generic, type-centric library for building and running finite state machines in Python. It doesn't actually depend on Pydantic AI  you can use it standalone for workflows that have nothing to do with GenAI  but Pydantic AI makes use of it to orchestrate the handling of model requests and model responses in an agent's run. In many scenarios, you don't need to worry about pydantic-graph at all; calling agent.run(...) simply traverses the underlying graph from start to finish. However, if you need deeper insight or control  for example to inject your own logic at specific stages  Pydantic AI exposes the lower-level iteration process via [ Agent.iter ](../api/agent/index.html#pydantic_ai.agent.Agent.iter). This method returns an [ AgentRun ](../api/agent/index.html#pydantic_ai.agent.AgentRun), which you can async-iterate over, or manually drive node-by-node via the [ next ](../api/agent/index.html#pydantic_ai.agent.AgentRun.next) method. Once the agent's graph returns an [ End ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.End), you have the final result along with a detailed history of all steps. #### async for iteration Here's an example of using async for with iter to record each node the agent executes: agent\\_iter\\_async\\_for.py *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* * The AgentRun is an async iterator that yields each node ( BaseNode or End ) in the flow. * The run ends when an End node is returned. #### Using .next(...) manually You can also drive the iteration manually by passing the node you want to run next to the AgentRun.next(...) method. This allows you to inspect or modify the node before it executes or skip nodes based on your own logic, and to catch errors in next() more easily: agent\\_iter\\_next.py 1. We start by grabbing the first node that will be run in the agent's graph. 2. The agent run is finished once an End node has been produced; instances of End cannot be passed to next . 3. When you call await agent_run.next(node) , it executes that node in the agent's graph, updates the run's history, and returns the *next* node to run. 4. You could also inspect or mutate the new node here as needed. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* #### Accessing usage and final output You can retrieve usage statistics (tokens, requests, etc.) at any time from the [ AgentRun ](../api/agent/index.html#pydantic_ai.agent.AgentRun) object via agent_run.usage() . This method returns a [ RunUsage ](../api/usage/index.html#pydantic_ai.usage.RunUsage) object containing the usage data. Once the run finishes, agent_run.result becomes a [ AgentRunResult ](../api/agent/index.html#pydantic_ai.agent.AgentRunResult) object containing the final output (and related metadata). #### Streaming All Events and Output Here is an example of streaming an agent run in combination with async for iteration: streaming\\_iter.py *(This example is complete, it can be run \"as is\")* ### Additional Configuration #### Usage Limits Pydantic AI offers a [ UsageLimits ](../api/usage/index.html#pydantic_ai.usage.UsageLimits) structure to help you limit your usage (tokens, requests, and tool calls) on model runs. You can apply these settings by passing the usage_limits argument to the run{_sync,_stream} functions. Consider the following example, where we limit the number of response tokens: Restricting the number of requests can be useful in preventing infinite loops or excessive tool calling: 1. This tool has the ability to retry 5 times before erroring, simulating a tool that might get stuck in a loop. 2. This run will error after 3 requests, preventing the infinite tool calling. ##### Capping tool calls If you need a limit on the number of successful tool invocations within a single run, use tool_calls_limit : Note * Usage limits are especially relevant if you've registered many tools. Use request_limit to bound the number of model turns, and tool_calls_limit to cap the number of successful tool executions within a run. * The tool_calls_limit is checked before executing tool calls. If the model returns parallel tool calls that would exceed the limit, no tools will be executed. #### Model (Run) Settings Pydantic AI offers a [ settings.ModelSettings ](../api/settings/index.html#pydantic_ai.settings.ModelSettings) structure to help you fine tune your requests. This structure allows you to configure common parameters that influence the model's behavior, such as temperature , max_tokens , timeout , and more. There are three ways to apply these settings, with a clear precedence order: 1. **Model-level defaults** - Set when creating a model instance via the settings parameter. These serve as the base defaults for that model. 2. **Agent-level defaults** - Set during [ Agent ](../api/agent/index.html#pydantic_ai.agent.Agent) initialization via the model_settings argument. These are merged with model defaults, with agent settings taking precedence. 3. **Run-time overrides** - Passed to run{_sync,_stream} functions via the model_settings argument. These have the highest priority and are merged with the combined agent and model defaults. For example, if you'd like to set the temperature setting to 0.0 to ensure less random behavior, you can do the following: The final request uses temperature=0.0 (run-time), max_tokens=500 (from model), demonstrating how settings merge with run-time taking precedence. Model Settings Support Model-level settings are supported by all concrete model implementations (OpenAI, Anthropic, Google, etc.). Wrapper models like FallbackModel , WrapperModel , and InstrumentedModel don't have their own settings - they use the settings of their underlying models. ### Model specific settings If you wish to further customize model behavior, you can use a subclass of [ ModelSettings ](../api/settings/index.html#pydantic_ai.settings.ModelSettings), like [ GoogleModelSettings ](../api/models/google/index.html#pydantic_ai.models.google.GoogleModelSettings), associated with your model of choice. For example: 1. This error is raised because the safety thresholds were exceeded.", "url": "https://ai.pydantic.dev/agents/index.html#running-agents", "page": "agents/index.html", "source_site": "pydantic_ai"}
{"title": "Streaming Events and Final Output", "anchor": "streaming-events-and-final-output", "heading_level": 3, "md_text": "As shown in the example above, [ run_stream() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream) makes it easy to stream the agent's final output as it comes in. It also takes an optional event_stream_handler argument that you can use to gain insight into what is happening during the run before the final output is produced. The example below shows how to stream events and text output. You can also [stream structured output](../output/index.html#streaming-structured-output). Note As the run_stream() method will consider the first output matching the [output type](../output/index.html#structured-output) to be the final output, it will stop running the agent graph and will not execute any tool calls made by the model after this \"final\" output. If you want to always run the agent graph to completion and stream all events from the model's streaming response and the agent's execution of tools, use [ agent.run_stream_events() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream_events) or [ agent.iter() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.iter) instead, as described in the following sections. run\\_stream\\_event\\_stream\\_handler.py", "url": "https://ai.pydantic.dev/agents/index.html#streaming-events-and-final-output", "page": "agents/index.html", "source_site": "pydantic_ai"}
{"title": "Streaming All Events", "anchor": "streaming-all-events", "heading_level": 3, "md_text": "Like agent.run_stream() , [ agent.run() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream) takes an optional event_stream_handler argument that lets you stream all events from the model's streaming response and the agent's execution of tools. Unlike run_stream() , it always runs the agent graph to completion even if text was received ahead of tool calls that looked like it could've been the final result. For convenience, a [ agent.run_stream_events() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream_events) method is also available as a wrapper around run(event_stream_handler=...) , which returns an async iterable of [ AgentStreamEvent s](../api/messages/index.html#pydantic_ai.messages.AgentStreamEvent) and a [ AgentRunResultEvent ](../api/run/index.html#pydantic_ai.run.AgentRunResultEvent) containing the final run result. Note As they return raw events as they come in, the run_stream_events() and run(event_stream_handler=...) methods require you to piece together the streamed text and structured output yourself from the PartStartEvent and subsequent PartDeltaEvent s. To get the best of both worlds, at the expense of some additional complexity, you can use [ agent.iter() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.iter) as described in the next section, which lets you [iterate over the agent graph](index.html#iterating-over-an-agents-graph) and [stream both events and output](index.html#streaming-all-events-and-output) at every step. run\\_events.py *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/agents/index.html#streaming-all-events", "page": "agents/index.html", "source_site": "pydantic_ai"}
{"title": "Iterating Over an Agent's Graph", "anchor": "iterating-over-an-agents-graph", "heading_level": 3, "md_text": "Under the hood, each Agent in Pydantic AI uses **pydantic-graph** to manage its execution flow. **pydantic-graph** is a generic, type-centric library for building and running finite state machines in Python. It doesn't actually depend on Pydantic AI  you can use it standalone for workflows that have nothing to do with GenAI  but Pydantic AI makes use of it to orchestrate the handling of model requests and model responses in an agent's run. In many scenarios, you don't need to worry about pydantic-graph at all; calling agent.run(...) simply traverses the underlying graph from start to finish. However, if you need deeper insight or control  for example to inject your own logic at specific stages  Pydantic AI exposes the lower-level iteration process via [ Agent.iter ](../api/agent/index.html#pydantic_ai.agent.Agent.iter). This method returns an [ AgentRun ](../api/agent/index.html#pydantic_ai.agent.AgentRun), which you can async-iterate over, or manually drive node-by-node via the [ next ](../api/agent/index.html#pydantic_ai.agent.AgentRun.next) method. Once the agent's graph returns an [ End ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.End), you have the final result along with a detailed history of all steps. #### async for iteration Here's an example of using async for with iter to record each node the agent executes: agent\\_iter\\_async\\_for.py *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* * The AgentRun is an async iterator that yields each node ( BaseNode or End ) in the flow. * The run ends when an End node is returned. #### Using .next(...) manually You can also drive the iteration manually by passing the node you want to run next to the AgentRun.next(...) method. This allows you to inspect or modify the node before it executes or skip nodes based on your own logic, and to catch errors in next() more easily: agent\\_iter\\_next.py 1. We start by grabbing the first node that will be run in the agent's graph. 2. The agent run is finished once an End node has been produced; instances of End cannot be passed to next . 3. When you call await agent_run.next(node) , it executes that node in the agent's graph, updates the run's history, and returns the *next* node to run. 4. You could also inspect or mutate the new node here as needed. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* #### Accessing usage and final output You can retrieve usage statistics (tokens, requests, etc.) at any time from the [ AgentRun ](../api/agent/index.html#pydantic_ai.agent.AgentRun) object via agent_run.usage() . This method returns a [ RunUsage ](../api/usage/index.html#pydantic_ai.usage.RunUsage) object containing the usage data. Once the run finishes, agent_run.result becomes a [ AgentRunResult ](../api/agent/index.html#pydantic_ai.agent.AgentRunResult) object containing the final output (and related metadata). #### Streaming All Events and Output Here is an example of streaming an agent run in combination with async for iteration: streaming\\_iter.py *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/agents/index.html#iterating-over-an-agents-graph", "page": "agents/index.html", "source_site": "pydantic_ai"}
{"title": "Additional Configuration", "anchor": "additional-configuration", "heading_level": 3, "md_text": "#### Usage Limits Pydantic AI offers a [ UsageLimits ](../api/usage/index.html#pydantic_ai.usage.UsageLimits) structure to help you limit your usage (tokens, requests, and tool calls) on model runs. You can apply these settings by passing the usage_limits argument to the run{_sync,_stream} functions. Consider the following example, where we limit the number of response tokens: Restricting the number of requests can be useful in preventing infinite loops or excessive tool calling: 1. This tool has the ability to retry 5 times before erroring, simulating a tool that might get stuck in a loop. 2. This run will error after 3 requests, preventing the infinite tool calling. ##### Capping tool calls If you need a limit on the number of successful tool invocations within a single run, use tool_calls_limit : Note * Usage limits are especially relevant if you've registered many tools. Use request_limit to bound the number of model turns, and tool_calls_limit to cap the number of successful tool executions within a run. * The tool_calls_limit is checked before executing tool calls. If the model returns parallel tool calls that would exceed the limit, no tools will be executed. #### Model (Run) Settings Pydantic AI offers a [ settings.ModelSettings ](../api/settings/index.html#pydantic_ai.settings.ModelSettings) structure to help you fine tune your requests. This structure allows you to configure common parameters that influence the model's behavior, such as temperature , max_tokens , timeout , and more. There are three ways to apply these settings, with a clear precedence order: 1. **Model-level defaults** - Set when creating a model instance via the settings parameter. These serve as the base defaults for that model. 2. **Agent-level defaults** - Set during [ Agent ](../api/agent/index.html#pydantic_ai.agent.Agent) initialization via the model_settings argument. These are merged with model defaults, with agent settings taking precedence. 3. **Run-time overrides** - Passed to run{_sync,_stream} functions via the model_settings argument. These have the highest priority and are merged with the combined agent and model defaults. For example, if you'd like to set the temperature setting to 0.0 to ensure less random behavior, you can do the following: The final request uses temperature=0.0 (run-time), max_tokens=500 (from model), demonstrating how settings merge with run-time taking precedence. Model Settings Support Model-level settings are supported by all concrete model implementations (OpenAI, Anthropic, Google, etc.). Wrapper models like FallbackModel , WrapperModel , and InstrumentedModel don't have their own settings - they use the settings of their underlying models.", "url": "https://ai.pydantic.dev/agents/index.html#additional-configuration", "page": "agents/index.html", "source_site": "pydantic_ai"}
{"title": "Model specific settings", "anchor": "model-specific-settings", "heading_level": 3, "md_text": "If you wish to further customize model behavior, you can use a subclass of [ ModelSettings ](../api/settings/index.html#pydantic_ai.settings.ModelSettings), like [ GoogleModelSettings ](../api/models/google/index.html#pydantic_ai.models.google.GoogleModelSettings), associated with your model of choice. For example: 1. This error is raised because the safety thresholds were exceeded.", "url": "https://ai.pydantic.dev/agents/index.html#model-specific-settings", "page": "agents/index.html", "source_site": "pydantic_ai"}
{"title": "Runs vs. Conversations", "anchor": "runs-vs-conversations", "heading_level": 2, "md_text": "An agent **run** might represent an entire conversation  there's no limit to how many messages can be exchanged in a single run. However, a **conversation** might also be composed of multiple runs, especially if you need to maintain state between separate interactions or API calls. Here's an example of a conversation comprised of multiple runs: conversation\\_example.py 1. Continue the conversation; without message_history the model would not know who \"his\" was referring to. *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/agents/index.html#runs-vs-conversations", "page": "agents/index.html", "source_site": "pydantic_ai"}
{"title": "Type safe by design", "anchor": "static-type-checking", "heading_level": 2, "md_text": "Pydantic AI is designed to work well with static type checkers, like mypy and pyright. Typing is (somewhat) optional Pydantic AI is designed to make type checking as useful as possible for you if you choose to use it, but you don't have to use types everywhere all the time. That said, because Pydantic AI uses Pydantic, and Pydantic uses type hints as the definition for schema and validation, some types (specifically type hints on parameters to tools, and the output_type arguments to [ Agent ](../api/agent/index.html#pydantic_ai.agent.Agent)) are used at runtime. We (the library developers) have messed up if type hints are confusing you more than helping you, if you find this, please create an [issue](https://github.com/pydantic/pydantic-ai/issues) explaining what's annoying you! In particular, agents are generic in both the type of their dependencies and the type of the outputs they return, so you can use the type hints to ensure you're using the right types. Consider the following script with type mistakes: type\\_mistakes.py 1. The agent is defined as expecting an instance of User as deps . 2. But here add_user_name is defined as taking a str as the dependency, not a User . 3. Since the agent is defined as returning a bool , this will raise a type error since foobar expects bytes . Running mypy on this will give the following output: Running pyright would identify the same issues.", "url": "https://ai.pydantic.dev/agents/index.html#static-type-checking", "page": "agents/index.html", "source_site": "pydantic_ai"}
{"title": "System Prompts", "anchor": "system-prompts", "heading_level": 2, "md_text": "System prompts might seem simple at first glance since they're just strings (or sequences of strings that are concatenated), but crafting the right system prompt is key to getting the model to behave as you want. Tip For most use cases, you should use instructions instead of \"system prompts\". If you know what you are doing though and want to preserve system prompt messages in the message history sent to the LLM in subsequent completions requests, you can achieve this using the system_prompt argument/decorator. See the section below on [Instructions](index.html#instructions) for more information. Generally, system prompts fall into two categories: 1. **Static system prompts**: These are known when writing the code and can be defined via the system_prompt parameter of the [ Agent constructor](../api/agent/index.html#pydantic_ai.agent.Agent.__init__). 2. **Dynamic system prompts**: These depend in some way on context that isn't known until runtime, and should be defined via functions decorated with [ @agent.system_prompt ](../api/agent/index.html#pydantic_ai.agent.Agent.system_prompt). You can add both to a single agent; they're appended in the order they're defined at runtime. Here's an example using both types of system prompts: system\\_prompts.py 1. The agent expects a string dependency. 2. Static system prompt defined at agent creation time. 3. Dynamic system prompt defined via a decorator with [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext), this is called just after run_sync , not when the agent is created, so can benefit from runtime information like the dependencies used on that run. 4. Another dynamic system prompt, system prompts don't have to have the RunContext parameter. *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/agents/index.html#system-prompts", "page": "agents/index.html", "source_site": "pydantic_ai"}
{"title": "Instructions", "anchor": "instructions", "heading_level": 2, "md_text": "Instructions are similar to system prompts. The main difference is that when an explicit message_history is provided in a call to Agent.run and similar methods, *instructions* from any existing messages in the history are not included in the request to the model  only the instructions of the *current* agent are included. You should use: * instructions when you want your request to the model to only include system prompts for the *current* agent * system_prompt when you want your request to the model to *retain* the system prompts used in previous requests (possibly made using other agents) In general, we recommend using instructions instead of system_prompt unless you have a specific reason to use system_prompt . Instructions, like system prompts, fall into two categories: 1. **Static instructions**: These are known when writing the code and can be defined via the instructions parameter of the [ Agent constructor](../api/agent/index.html#pydantic_ai.agent.Agent.__init__). 2. **Dynamic instructions**: These rely on context that is only available at runtime and should be defined using functions decorated with [ @agent.instructions ](../api/agent/index.html#pydantic_ai.agent.Agent.instructions). Unlike dynamic system prompts, which may be reused when message_history is present, dynamic instructions are always reevaluated. Both static and dynamic instructions can be added to a single agent, and they are appended in the order they are defined at runtime. Here's an example using both types of instructions: instructions.py 1. The agent expects a string dependency. 2. Static instructions defined at agent creation time. 3. Dynamic instructions defined via a decorator with [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext), this is called just after run_sync , not when the agent is created, so can benefit from runtime information like the dependencies used on that run. 4. Another dynamic instruction, instructions don't have to have the RunContext parameter. *(This example is complete, it can be run \"as is\")* Note that returning an empty string will result in no instruction message added.", "url": "https://ai.pydantic.dev/agents/index.html#instructions", "page": "agents/index.html", "source_site": "pydantic_ai"}
{"title": "Reflection and self-correction", "anchor": "reflection-and-self-correction", "heading_level": 2, "md_text": "Validation errors from both function tool parameter validation and [structured output validation](../output/index.html#structured-output) can be passed back to the model with a request to retry. You can also raise [ ModelRetry ](../api/exceptions/index.html#pydantic_ai.exceptions.ModelRetry) from within a [tool](../tools/index.html) or [output function](../output/index.html#output-functions) to tell the model it should retry generating a response. * The default retry count is **1** but can be altered for the [entire agent](../api/agent/index.html#pydantic_ai.agent.Agent.__init__), a [specific tool](../api/agent/index.html#pydantic_ai.agent.Agent.tool), or [outputs](../api/agent/index.html#pydantic_ai.agent.Agent.__init__). * You can access the current retry count from within a tool or output function via [ ctx.retry ](../api/tools/index.html#pydantic_ai.tools.RunContext). Here's an example: tool\\_retry.py", "url": "https://ai.pydantic.dev/agents/index.html#reflection-and-self-correction", "page": "agents/index.html", "source_site": "pydantic_ai"}
{"title": "Model errors", "anchor": "model-errors", "heading_level": 2, "md_text": "If models behave unexpectedly (e.g., the retry limit is exceeded, or their API returns 503 ), agent runs will raise [ UnexpectedModelBehavior ](../api/exceptions/index.html#pydantic_ai.exceptions.UnexpectedModelBehavior). In these cases, [ capture_run_messages ](../api/agent/index.html#pydantic_ai.agent.capture_run_messages) can be used to access the messages exchanged during the run to help diagnose the issue. agent\\_model\\_errors.py 1. Define a tool that will raise ModelRetry repeatedly in this case. 2. [ capture_run_messages ](../api/agent/index.html#pydantic_ai.agent.capture_run_messages) is used to capture the messages exchanged during the run. *(This example is complete, it can be run \"as is\")* Note If you call [ run ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run), [ run_sync ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_sync), or [ run_stream ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream) more than once within a single capture_run_messages context, messages will represent the messages exchanged during the first call only.", "url": "https://ai.pydantic.dev/agents/index.html#model-errors", "page": "agents/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.ag_ui", "anchor": "pydantic_aiag_ui", "heading_level": 1, "md_text": "Provides an AG-UI protocol adapter for the Pydantic AI agent. This package provides seamless integration between pydantic-ai agents and ag-ui for building interactive AI applications with streaming event-based communication. ### SSE\\_CONTENT\\_TYPE module-attribute Content type header value for Server-Sent Events (SSE). ### OnCompleteFunc module-attribute Callback function type that receives the AgentRunResult of the completed run. Can be sync or async. ### AGUIApp Bases: Generic[AgentDepsT, OutputDataT] , Starlette ASGI application for running Pydantic AI agents with AG-UI protocol support. Source code in pydantic_ai_slim/pydantic_ai/ag_ui.py #### \\_\\_init\\_\\_ An ASGI application that handles every AG-UI request by running the agent. Note that the deps will be the same for each request, with the exception of the AG-UI state that's injected into the state field of a deps object that implements the [ StateHandler ](index.html#pydantic_ai.ag_ui.StateHandler) protocol. To provide different deps for each request (e.g. based on the authenticated user), use [ pydantic_ai.ag_ui.run_ag_ui ](index.html#pydantic_ai.ag_ui.run_ag_ui) or [ pydantic_ai.ag_ui.handle_ag_ui_request ](index.html#pydantic_ai.ag_ui.handle_ag_ui_request) instead. Parameters: Source code in pydantic_ai_slim/pydantic_ai/ag_ui.py ### handle\\_ag\\_ui\\_request async Handle an AG-UI request by running the agent and returning a streaming response. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/ag_ui.py ### run\\_ag\\_ui async Run the agent with the AG-UI run input and stream AG-UI protocol events. Parameters: Yields: Source code in pydantic_ai_slim/pydantic_ai/ag_ui.py ### StateHandler Bases: Protocol Protocol for state handlers in agent runs. Requires the class to be a dataclass with a state field. Source code in pydantic_ai_slim/pydantic_ai/ag_ui.py #### state property writable Get the current state of the agent run. ### StateDeps dataclass Bases: Generic[StateT] Provides AG-UI state management. This class is used to manage the state of an agent run. It allows setting the state of the agent run with a specific type of state model, which must be a subclass of BaseModel . The state is set using the state setter by the Adapter when the run starts. Implements the StateHandler protocol. Source code in pydantic_ai_slim/pydantic_ai/ag_ui.py", "url": "https://ai.pydantic.dev/api/ag_ui/index.html#pydantic_aiag_ui", "page": "api/ag_ui/index.html", "source_site": "pydantic_ai"}
{"title": "SSE_CONTENT_TYPE module-attribute", "anchor": "pydantic_ai.ag_ui.SSE_CONTENT_TYPE", "heading_level": 3, "md_text": "Content type header value for Server-Sent Events (SSE).", "url": "https://ai.pydantic.dev/api/ag_ui/index.html#pydantic_ai.ag_ui.SSE_CONTENT_TYPE", "page": "api/ag_ui/index.html", "source_site": "pydantic_ai"}
{"title": "OnCompleteFunc module-attribute", "anchor": "pydantic_ai.ag_ui.OnCompleteFunc", "heading_level": 3, "md_text": "Callback function type that receives the AgentRunResult of the completed run. Can be sync or async.", "url": "https://ai.pydantic.dev/api/ag_ui/index.html#pydantic_ai.ag_ui.OnCompleteFunc", "page": "api/ag_ui/index.html", "source_site": "pydantic_ai"}
{"title": "AGUIApp", "anchor": "pydantic_ai.ag_ui.AGUIApp", "heading_level": 3, "md_text": "Bases: Generic[AgentDepsT, OutputDataT] , Starlette ASGI application for running Pydantic AI agents with AG-UI protocol support. Source code in pydantic_ai_slim/pydantic_ai/ag_ui.py #### \\_\\_init\\_\\_ An ASGI application that handles every AG-UI request by running the agent. Note that the deps will be the same for each request, with the exception of the AG-UI state that's injected into the state field of a deps object that implements the [ StateHandler ](index.html#pydantic_ai.ag_ui.StateHandler) protocol. To provide different deps for each request (e.g. based on the authenticated user), use [ pydantic_ai.ag_ui.run_ag_ui ](index.html#pydantic_ai.ag_ui.run_ag_ui) or [ pydantic_ai.ag_ui.handle_ag_ui_request ](index.html#pydantic_ai.ag_ui.handle_ag_ui_request) instead. Parameters: Source code in pydantic_ai_slim/pydantic_ai/ag_ui.py", "url": "https://ai.pydantic.dev/api/ag_ui/index.html#pydantic_ai.ag_ui.AGUIApp", "page": "api/ag_ui/index.html", "source_site": "pydantic_ai"}
{"title": "handle_ag_ui_request async", "anchor": "pydantic_ai.ag_ui.handle_ag_ui_request", "heading_level": 3, "md_text": "Handle an AG-UI request by running the agent and returning a streaming response. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/ag_ui.py", "url": "https://ai.pydantic.dev/api/ag_ui/index.html#pydantic_ai.ag_ui.handle_ag_ui_request", "page": "api/ag_ui/index.html", "source_site": "pydantic_ai"}
{"title": "run_ag_ui async", "anchor": "pydantic_ai.ag_ui.run_ag_ui", "heading_level": 3, "md_text": "Run the agent with the AG-UI run input and stream AG-UI protocol events. Parameters: Yields: Source code in pydantic_ai_slim/pydantic_ai/ag_ui.py", "url": "https://ai.pydantic.dev/api/ag_ui/index.html#pydantic_ai.ag_ui.run_ag_ui", "page": "api/ag_ui/index.html", "source_site": "pydantic_ai"}
{"title": "StateHandler", "anchor": "pydantic_ai.ag_ui.StateHandler", "heading_level": 3, "md_text": "Bases: Protocol Protocol for state handlers in agent runs. Requires the class to be a dataclass with a state field. Source code in pydantic_ai_slim/pydantic_ai/ag_ui.py #### state property writable Get the current state of the agent run.", "url": "https://ai.pydantic.dev/api/ag_ui/index.html#pydantic_ai.ag_ui.StateHandler", "page": "api/ag_ui/index.html", "source_site": "pydantic_ai"}
{"title": "StateDeps dataclass", "anchor": "pydantic_ai.ag_ui.StateDeps", "heading_level": 3, "md_text": "Bases: Generic[StateT] Provides AG-UI state management. This class is used to manage the state of an agent run. It allows setting the state of the agent run with a specific type of state model, which must be a subclass of BaseModel . The state is set using the state setter by the Adapter when the run starts. Implements the StateHandler protocol. Source code in pydantic_ai_slim/pydantic_ai/ag_ui.py", "url": "https://ai.pydantic.dev/api/ag_ui/index.html#pydantic_ai.ag_ui.StateDeps", "page": "api/ag_ui/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.direct", "anchor": "pydantic_aidirect", "heading_level": 1, "md_text": "Methods for making imperative requests to language models with minimal abstraction. These methods allow you to make requests to LLMs where the only abstraction is input and output schema translation so you can use all models with the same API. These methods are thin wrappers around [ Model ](../models/base/index.html#pydantic_ai.models.Model) implementations. ### model\\_request async Make a non-streamed request to a model. model\\_request\\_example.py 1. See [ ModelRequest.user_text_prompt ](../messages/index.html#pydantic_ai.messages.ModelRequest.user_text_prompt) for details. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/direct.py ### model\\_request\\_sync Make a Synchronous, non-streamed request to a model. This is a convenience method that wraps [ model_request ](index.html#pydantic_ai.direct.model_request) with loop.run_until_complete(...) . You therefore can't use this method inside async code or if there's an active event loop. model\\_request\\_sync\\_example.py 1. See [ ModelRequest.user_text_prompt ](../messages/index.html#pydantic_ai.messages.ModelRequest.user_text_prompt) for details. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/direct.py ### model\\_request\\_stream Make a streamed async request to a model. model\\_request\\_stream\\_example.py 1. See [ ModelRequest.user_text_prompt ](../messages/index.html#pydantic_ai.messages.ModelRequest.user_text_prompt) for details. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/direct.py ### model\\_request\\_stream\\_sync Make a streamed synchronous request to a model. This is the synchronous version of [ model_request_stream ](index.html#pydantic_ai.direct.model_request_stream). It uses threading to run the asynchronous stream in the background while providing a synchronous iterator interface. model\\_request\\_stream\\_sync\\_example.py Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/direct.py ### StreamedResponseSync dataclass Synchronous wrapper to async streaming responses by running the async producer in a background thread and providing a synchronous iterator. This class must be used as a context manager with the with statement. Source code in pydantic_ai_slim/pydantic_ai/direct.py #### \\_\\_iter\\_\\_ Stream the response as an iterable of [ ModelResponseStreamEvent ](../messages/index.html#pydantic_ai.messages.ModelResponseStreamEvent)s. Source code in pydantic_ai_slim/pydantic_ai/direct.py #### get Build a ModelResponse from the data received from the stream so far. Source code in pydantic_ai_slim/pydantic_ai/direct.py #### response property Get the current state of the response. #### usage Get the usage of the response so far. Source code in pydantic_ai_slim/pydantic_ai/direct.py #### model\\_name property Get the model name of the response. #### timestamp property Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/direct/index.html#pydantic_aidirect", "page": "api/direct/index.html", "source_site": "pydantic_ai"}
{"title": "model_request async", "anchor": "pydantic_ai.direct.model_request", "heading_level": 3, "md_text": "Make a non-streamed request to a model. model\\_request\\_example.py 1. See [ ModelRequest.user_text_prompt ](../messages/index.html#pydantic_ai.messages.ModelRequest.user_text_prompt) for details. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/direct.py", "url": "https://ai.pydantic.dev/api/direct/index.html#pydantic_ai.direct.model_request", "page": "api/direct/index.html", "source_site": "pydantic_ai"}
{"title": "model_request_sync", "anchor": "pydantic_ai.direct.model_request_sync", "heading_level": 3, "md_text": "Make a Synchronous, non-streamed request to a model. This is a convenience method that wraps [ model_request ](index.html#pydantic_ai.direct.model_request) with loop.run_until_complete(...) . You therefore can't use this method inside async code or if there's an active event loop. model\\_request\\_sync\\_example.py 1. See [ ModelRequest.user_text_prompt ](../messages/index.html#pydantic_ai.messages.ModelRequest.user_text_prompt) for details. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/direct.py", "url": "https://ai.pydantic.dev/api/direct/index.html#pydantic_ai.direct.model_request_sync", "page": "api/direct/index.html", "source_site": "pydantic_ai"}
{"title": "model_request_stream", "anchor": "pydantic_ai.direct.model_request_stream", "heading_level": 3, "md_text": "Make a streamed async request to a model. model\\_request\\_stream\\_example.py 1. See [ ModelRequest.user_text_prompt ](../messages/index.html#pydantic_ai.messages.ModelRequest.user_text_prompt) for details. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/direct.py", "url": "https://ai.pydantic.dev/api/direct/index.html#pydantic_ai.direct.model_request_stream", "page": "api/direct/index.html", "source_site": "pydantic_ai"}
{"title": "model_request_stream_sync", "anchor": "pydantic_ai.direct.model_request_stream_sync", "heading_level": 3, "md_text": "Make a streamed synchronous request to a model. This is the synchronous version of [ model_request_stream ](index.html#pydantic_ai.direct.model_request_stream). It uses threading to run the asynchronous stream in the background while providing a synchronous iterator interface. model\\_request\\_stream\\_sync\\_example.py Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/direct.py", "url": "https://ai.pydantic.dev/api/direct/index.html#pydantic_ai.direct.model_request_stream_sync", "page": "api/direct/index.html", "source_site": "pydantic_ai"}
{"title": "StreamedResponseSync dataclass", "anchor": "pydantic_ai.direct.StreamedResponseSync", "heading_level": 3, "md_text": "Synchronous wrapper to async streaming responses by running the async producer in a background thread and providing a synchronous iterator. This class must be used as a context manager with the with statement. Source code in pydantic_ai_slim/pydantic_ai/direct.py #### \\_\\_iter\\_\\_ Stream the response as an iterable of [ ModelResponseStreamEvent ](../messages/index.html#pydantic_ai.messages.ModelResponseStreamEvent)s. Source code in pydantic_ai_slim/pydantic_ai/direct.py #### get Build a ModelResponse from the data received from the stream so far. Source code in pydantic_ai_slim/pydantic_ai/direct.py #### response property Get the current state of the response. #### usage Get the usage of the response so far. Source code in pydantic_ai_slim/pydantic_ai/direct.py #### model\\_name property Get the model name of the response. #### timestamp property Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/direct/index.html#pydantic_ai.direct.StreamedResponseSync", "page": "api/direct/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.exceptions", "anchor": "pydantic_aiexceptions", "heading_level": 1, "md_text": "### ModelRetry Bases: Exception Exception to raise when a tool function should be retried. The agent will return the message to the model and ask it to try calling the function/tool again. Source code in pydantic_ai_slim/pydantic_ai/exceptions.py #### message instance-attribute The message to return to the model. #### \\_\\_get\\_pydantic\\_core\\_schema\\_\\_ classmethod Pydantic core schema to allow ModelRetry to be (de)serialized. Source code in pydantic_ai_slim/pydantic_ai/exceptions.py ### CallDeferred Bases: Exception Exception to raise when a tool call should be deferred. See [tools docs](../../deferred-tools/index.html#deferred-tools) for more information. Source code in pydantic_ai_slim/pydantic_ai/exceptions.py ### ApprovalRequired Bases: Exception Exception to raise when a tool call requires human-in-the-loop approval. See [tools docs](../../deferred-tools/index.html#human-in-the-loop-tool-approval) for more information. Source code in pydantic_ai_slim/pydantic_ai/exceptions.py ### UserError Bases: RuntimeError Error caused by a usage mistake by the application developer  You! Source code in pydantic_ai_slim/pydantic_ai/exceptions.py #### message instance-attribute Description of the mistake. ### AgentRunError Bases: RuntimeError Base class for errors occurring during an agent run. Source code in pydantic_ai_slim/pydantic_ai/exceptions.py #### message instance-attribute The error message. ### UsageLimitExceeded Bases: AgentRunError Error raised when a Model's usage exceeds the specified limits. Source code in pydantic_ai_slim/pydantic_ai/exceptions.py ### UnexpectedModelBehavior Bases: AgentRunError Error caused by unexpected Model behavior, e.g. an unexpected response code. Source code in pydantic_ai_slim/pydantic_ai/exceptions.py #### message instance-attribute Description of the unexpected behavior. #### body instance-attribute The body of the response, if available. ### ModelHTTPError Bases: AgentRunError Raised when an model provider response has a status code of 4xx or 5xx. Source code in pydantic_ai_slim/pydantic_ai/exceptions.py #### message instance-attribute The error message with the status code and response body, if available. #### status\\_code instance-attribute The HTTP status code returned by the API. #### model\\_name instance-attribute The name of the model associated with the error. #### body instance-attribute The body of the response, if available. ### FallbackExceptionGroup Bases: ExceptionGroup[Any] A group of exceptions that can be raised when all fallback models fail. Source code in pydantic_ai_slim/pydantic_ai/exceptions.py ### IncompleteToolCall Bases: UnexpectedModelBehavior Error raised when a model stops due to token limit while emitting a tool call. Source code in pydantic_ai_slim/pydantic_ai/exceptions.py", "url": "https://ai.pydantic.dev/api/exceptions/index.html#pydantic_aiexceptions", "page": "api/exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "ModelRetry", "anchor": "pydantic_ai.exceptions.ModelRetry", "heading_level": 3, "md_text": "Bases: Exception Exception to raise when a tool function should be retried. The agent will return the message to the model and ask it to try calling the function/tool again. Source code in pydantic_ai_slim/pydantic_ai/exceptions.py #### message instance-attribute The message to return to the model. #### \\_\\_get\\_pydantic\\_core\\_schema\\_\\_ classmethod Pydantic core schema to allow ModelRetry to be (de)serialized. Source code in pydantic_ai_slim/pydantic_ai/exceptions.py", "url": "https://ai.pydantic.dev/api/exceptions/index.html#pydantic_ai.exceptions.ModelRetry", "page": "api/exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "CallDeferred", "anchor": "pydantic_ai.exceptions.CallDeferred", "heading_level": 3, "md_text": "Bases: Exception Exception to raise when a tool call should be deferred. See [tools docs](../../deferred-tools/index.html#deferred-tools) for more information. Source code in pydantic_ai_slim/pydantic_ai/exceptions.py", "url": "https://ai.pydantic.dev/api/exceptions/index.html#pydantic_ai.exceptions.CallDeferred", "page": "api/exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "ApprovalRequired", "anchor": "pydantic_ai.exceptions.ApprovalRequired", "heading_level": 3, "md_text": "Bases: Exception Exception to raise when a tool call requires human-in-the-loop approval. See [tools docs](../../deferred-tools/index.html#human-in-the-loop-tool-approval) for more information. Source code in pydantic_ai_slim/pydantic_ai/exceptions.py", "url": "https://ai.pydantic.dev/api/exceptions/index.html#pydantic_ai.exceptions.ApprovalRequired", "page": "api/exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "UserError", "anchor": "pydantic_ai.exceptions.UserError", "heading_level": 3, "md_text": "Bases: RuntimeError Error caused by a usage mistake by the application developer  You! Source code in pydantic_ai_slim/pydantic_ai/exceptions.py #### message instance-attribute Description of the mistake.", "url": "https://ai.pydantic.dev/api/exceptions/index.html#pydantic_ai.exceptions.UserError", "page": "api/exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "AgentRunError", "anchor": "pydantic_ai.exceptions.AgentRunError", "heading_level": 3, "md_text": "Bases: RuntimeError Base class for errors occurring during an agent run. Source code in pydantic_ai_slim/pydantic_ai/exceptions.py #### message instance-attribute The error message.", "url": "https://ai.pydantic.dev/api/exceptions/index.html#pydantic_ai.exceptions.AgentRunError", "page": "api/exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "UsageLimitExceeded", "anchor": "pydantic_ai.exceptions.UsageLimitExceeded", "heading_level": 3, "md_text": "Bases: AgentRunError Error raised when a Model's usage exceeds the specified limits. Source code in pydantic_ai_slim/pydantic_ai/exceptions.py", "url": "https://ai.pydantic.dev/api/exceptions/index.html#pydantic_ai.exceptions.UsageLimitExceeded", "page": "api/exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "UnexpectedModelBehavior", "anchor": "pydantic_ai.exceptions.UnexpectedModelBehavior", "heading_level": 3, "md_text": "Bases: AgentRunError Error caused by unexpected Model behavior, e.g. an unexpected response code. Source code in pydantic_ai_slim/pydantic_ai/exceptions.py #### message instance-attribute Description of the unexpected behavior. #### body instance-attribute The body of the response, if available.", "url": "https://ai.pydantic.dev/api/exceptions/index.html#pydantic_ai.exceptions.UnexpectedModelBehavior", "page": "api/exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "ModelHTTPError", "anchor": "pydantic_ai.exceptions.ModelHTTPError", "heading_level": 3, "md_text": "Bases: AgentRunError Raised when an model provider response has a status code of 4xx or 5xx. Source code in pydantic_ai_slim/pydantic_ai/exceptions.py #### message instance-attribute The error message with the status code and response body, if available. #### status\\_code instance-attribute The HTTP status code returned by the API. #### model\\_name instance-attribute The name of the model associated with the error. #### body instance-attribute The body of the response, if available.", "url": "https://ai.pydantic.dev/api/exceptions/index.html#pydantic_ai.exceptions.ModelHTTPError", "page": "api/exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "FallbackExceptionGroup", "anchor": "pydantic_ai.exceptions.FallbackExceptionGroup", "heading_level": 3, "md_text": "Bases: ExceptionGroup[Any] A group of exceptions that can be raised when all fallback models fail. Source code in pydantic_ai_slim/pydantic_ai/exceptions.py", "url": "https://ai.pydantic.dev/api/exceptions/index.html#pydantic_ai.exceptions.FallbackExceptionGroup", "page": "api/exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "IncompleteToolCall", "anchor": "pydantic_ai.exceptions.IncompleteToolCall", "heading_level": 3, "md_text": "Bases: UnexpectedModelBehavior Error raised when a model stops due to token limit while emitting a tool call. Source code in pydantic_ai_slim/pydantic_ai/exceptions.py", "url": "https://ai.pydantic.dev/api/exceptions/index.html#pydantic_ai.exceptions.IncompleteToolCall", "page": "api/exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.ext", "anchor": "pydantic_aiext", "heading_level": 1, "md_text": "### tool\\_from\\_langchain Creates a Pydantic AI tool proxy from a LangChain tool. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/ext/langchain.py ### LangChainToolset Bases: FunctionToolset A toolset that wraps LangChain tools. Source code in pydantic_ai_slim/pydantic_ai/ext/langchain.py ### tool\\_from\\_aci Creates a Pydantic AI tool proxy from an ACI.dev function. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/ext/aci.py ### ACIToolset Bases: FunctionToolset A toolset that wraps ACI.dev tools. Source code in pydantic_ai_slim/pydantic_ai/ext/aci.py", "url": "https://ai.pydantic.dev/api/ext/index.html#pydantic_aiext", "page": "api/ext/index.html", "source_site": "pydantic_ai"}
{"title": "tool_from_langchain", "anchor": "pydantic_ai.ext.langchain.tool_from_langchain", "heading_level": 3, "md_text": "Creates a Pydantic AI tool proxy from a LangChain tool. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/ext/langchain.py", "url": "https://ai.pydantic.dev/api/ext/index.html#pydantic_ai.ext.langchain.tool_from_langchain", "page": "api/ext/index.html", "source_site": "pydantic_ai"}
{"title": "LangChainToolset", "anchor": "pydantic_ai.ext.langchain.LangChainToolset", "heading_level": 3, "md_text": "Bases: FunctionToolset A toolset that wraps LangChain tools. Source code in pydantic_ai_slim/pydantic_ai/ext/langchain.py", "url": "https://ai.pydantic.dev/api/ext/index.html#pydantic_ai.ext.langchain.LangChainToolset", "page": "api/ext/index.html", "source_site": "pydantic_ai"}
{"title": "tool_from_aci", "anchor": "pydantic_ai.ext.aci.tool_from_aci", "heading_level": 3, "md_text": "Creates a Pydantic AI tool proxy from an ACI.dev function. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/ext/aci.py", "url": "https://ai.pydantic.dev/api/ext/index.html#pydantic_ai.ext.aci.tool_from_aci", "page": "api/ext/index.html", "source_site": "pydantic_ai"}
{"title": "ACIToolset", "anchor": "pydantic_ai.ext.aci.ACIToolset", "heading_level": 3, "md_text": "Bases: FunctionToolset A toolset that wraps ACI.dev tools. Source code in pydantic_ai_slim/pydantic_ai/ext/aci.py", "url": "https://ai.pydantic.dev/api/ext/index.html#pydantic_ai.ext.aci.ACIToolset", "page": "api/ext/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.format_prompt", "anchor": "pydantic_aiformat_prompt", "heading_level": 1, "md_text": "### format\\_as\\_xml Format a Python object as XML. This is useful since LLMs often find it easier to read semi-structured data (e.g. examples) as XML, rather than JSON etc. Supports: str , bytes , bytearray , bool , int , float , date , datetime , time , timedelta , Enum , Mapping , Iterable , dataclass , and BaseModel . Parameters: Returns: Example: format\\_as\\_xml\\_example.py Source code in pydantic_ai_slim/pydantic_ai/format_prompt.py", "url": "https://ai.pydantic.dev/api/format_prompt/index.html#pydantic_aiformat_prompt", "page": "api/format_prompt/index.html", "source_site": "pydantic_ai"}
{"title": "format_as_xml", "anchor": "pydantic_ai.format_prompt.format_as_xml", "heading_level": 3, "md_text": "Format a Python object as XML. This is useful since LLMs often find it easier to read semi-structured data (e.g. examples) as XML, rather than JSON etc. Supports: str , bytes , bytearray , bool , int , float , date , datetime , time , timedelta , Enum , Mapping , Iterable , dataclass , and BaseModel . Parameters: Returns: Example: format\\_as\\_xml\\_example.py Source code in pydantic_ai_slim/pydantic_ai/format_prompt.py", "url": "https://ai.pydantic.dev/api/format_prompt/index.html#pydantic_ai.format_prompt.format_as_xml", "page": "api/format_prompt/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.mcp", "anchor": "pydantic_aimcp", "heading_level": 1, "md_text": "### MCPServer Bases: AbstractToolset[Any] , ABC Base class for attaching agents to MCP servers. See <https://modelcontextprotocol.io> for more information. Source code in pydantic_ai_slim/pydantic_ai/mcp.py #### tool\\_prefix instance-attribute A prefix to add to all tools that are registered with the server. If not empty, will include a trailing underscore( _ ). e.g. if tool_prefix='foo' , then a tool named bar will be registered as foo_bar #### log\\_level instance-attribute The log level to set when connecting to the server, if any. See <https://modelcontextprotocol.io/specification/2025-03-26/server/utilities/logging#logging> for more details. If None , no log level will be set. #### log\\_handler instance-attribute A handler for logging messages from the server. #### timeout instance-attribute The timeout in seconds to wait for the client to initialize. #### read\\_timeout instance-attribute Maximum time in seconds to wait for new messages before timing out. This timeout applies to the long-lived connection after it's established. If no new messages are received within this time, the connection will be considered stale and may be closed. Defaults to 5 minutes (300 seconds). #### process\\_tool\\_call instance-attribute Hook to customize tool calling and optionally pass extra metadata. #### allow\\_sampling instance-attribute Whether to allow MCP sampling through this client. #### sampling\\_model instance-attribute The model to use for sampling. #### max\\_retries instance-attribute The maximum number of times to retry a tool call. #### elicitation\\_callback class-attribute instance-attribute Callback function to handle elicitation requests from the server. #### client\\_streams abstractmethod async Create the streams for the MCP server. Source code in pydantic_ai_slim/pydantic_ai/mcp.py #### server\\_info property Access the information send by the MCP server during initialization. #### list\\_tools async Retrieve tools that are currently active on the server. Note: - We don't cache tools as they might change. - We also don't subscribe to the server to avoid complexity. Source code in pydantic_ai_slim/pydantic_ai/mcp.py #### direct\\_call\\_tool async Call a tool on the server. Parameters: Returns: Raises: Source code in pydantic_ai_slim/pydantic_ai/mcp.py #### \\_\\_aenter\\_\\_ async Enter the MCP server context. This will initialize the connection to the server. If this server is an [ MCPServerStdio ](index.html#pydantic_ai.mcp.MCPServerStdio), the server will first be started as a subprocess. This is a no-op if the MCP server has already been entered. Source code in pydantic_ai_slim/pydantic_ai/mcp.py #### is\\_running property Check if the MCP server is running. ### MCPServerStdio Bases: MCPServer Runs an MCP server in a subprocess and communicates with it over stdin/stdout. This class implements the stdio transport from the MCP specification. See <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio> for more information. Note Using this class as an async context manager will start the server as a subprocess when entering the context, and stop it when exiting the context. Example: 1. See [MCP Run Python](https://github.com/pydantic/mcp-run-python) for more information. Source code in pydantic_ai_slim/pydantic_ai/mcp.py #### \\_\\_init\\_\\_ Build a new MCP server. Parameters: Source code in pydantic_ai_slim/pydantic_ai/mcp.py #### command instance-attribute The command to run. #### args instance-attribute The arguments to pass to the command. #### env instance-attribute The environment variables the CLI server will have access to. By default the subprocess will not inherit any environment variables from the parent process. If you want to inherit the environment variables from the parent process, use env=os.environ . #### cwd instance-attribute The working directory to use when spawning the process. ### MCPServerSSE Bases: _MCPServerHTTP An MCP server that connects over streamable HTTP connections. This class implements the SSE transport from the MCP specification. See <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse> for more information. Note Using this class as an async context manager will create a new pool of HTTP connections to connect to a server which should already be running. Example: Source code in pydantic_ai_slim/pydantic_ai/mcp.py ### MCPServerHTTP deprecated Bases: MCPServerSSE Deprecated The MCPServerHTTP class is deprecated, use MCPServerSSE instead. An MCP server that connects over HTTP using the old SSE transport. This class implements the SSE transport from the MCP specification. See <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse> for more information. Note Using this class as an async context manager will create a new pool of HTTP connections to connect to a server which should already be running. Example: Source code in pydantic_ai_slim/pydantic_ai/mcp.py ### MCPServerStreamableHTTP Bases: _MCPServerHTTP An MCP server that connects over HTTP using the Streamable HTTP transport. This class implements the Streamable HTTP transport from the MCP specification. See <https://modelcontextprotocol.io/introduction#streamable-http> for more information. Note Using this class as an async context manager will create a new pool of HTTP connections to connect to a server which should already be running. Example: Source code in pydantic_ai_slim/pydantic_ai/mcp.py ### load\\_mcp\\_servers Load MCP servers from a configuration file. Parameters: Returns: Raises: Source code in pydantic_ai_slim/pydantic_ai/mcp.py", "url": "https://ai.pydantic.dev/api/mcp/index.html#pydantic_aimcp", "page": "api/mcp/index.html", "source_site": "pydantic_ai"}
{"title": "MCPServer", "anchor": "pydantic_ai.mcp.MCPServer", "heading_level": 3, "md_text": "Bases: AbstractToolset[Any] , ABC Base class for attaching agents to MCP servers. See <https://modelcontextprotocol.io> for more information. Source code in pydantic_ai_slim/pydantic_ai/mcp.py #### tool\\_prefix instance-attribute A prefix to add to all tools that are registered with the server. If not empty, will include a trailing underscore( _ ). e.g. if tool_prefix='foo' , then a tool named bar will be registered as foo_bar #### log\\_level instance-attribute The log level to set when connecting to the server, if any. See <https://modelcontextprotocol.io/specification/2025-03-26/server/utilities/logging#logging> for more details. If None , no log level will be set. #### log\\_handler instance-attribute A handler for logging messages from the server. #### timeout instance-attribute The timeout in seconds to wait for the client to initialize. #### read\\_timeout instance-attribute Maximum time in seconds to wait for new messages before timing out. This timeout applies to the long-lived connection after it's established. If no new messages are received within this time, the connection will be considered stale and may be closed. Defaults to 5 minutes (300 seconds). #### process\\_tool\\_call instance-attribute Hook to customize tool calling and optionally pass extra metadata. #### allow\\_sampling instance-attribute Whether to allow MCP sampling through this client. #### sampling\\_model instance-attribute The model to use for sampling. #### max\\_retries instance-attribute The maximum number of times to retry a tool call. #### elicitation\\_callback class-attribute instance-attribute Callback function to handle elicitation requests from the server. #### client\\_streams abstractmethod async Create the streams for the MCP server. Source code in pydantic_ai_slim/pydantic_ai/mcp.py #### server\\_info property Access the information send by the MCP server during initialization. #### list\\_tools async Retrieve tools that are currently active on the server. Note: - We don't cache tools as they might change. - We also don't subscribe to the server to avoid complexity. Source code in pydantic_ai_slim/pydantic_ai/mcp.py #### direct\\_call\\_tool async Call a tool on the server. Parameters: Returns: Raises: Source code in pydantic_ai_slim/pydantic_ai/mcp.py #### \\_\\_aenter\\_\\_ async Enter the MCP server context. This will initialize the connection to the server. If this server is an [ MCPServerStdio ](index.html#pydantic_ai.mcp.MCPServerStdio), the server will first be started as a subprocess. This is a no-op if the MCP server has already been entered. Source code in pydantic_ai_slim/pydantic_ai/mcp.py #### is\\_running property Check if the MCP server is running.", "url": "https://ai.pydantic.dev/api/mcp/index.html#pydantic_ai.mcp.MCPServer", "page": "api/mcp/index.html", "source_site": "pydantic_ai"}
{"title": "MCPServerStdio", "anchor": "pydantic_ai.mcp.MCPServerStdio", "heading_level": 3, "md_text": "Bases: MCPServer Runs an MCP server in a subprocess and communicates with it over stdin/stdout. This class implements the stdio transport from the MCP specification. See <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio> for more information. Note Using this class as an async context manager will start the server as a subprocess when entering the context, and stop it when exiting the context. Example: 1. See [MCP Run Python](https://github.com/pydantic/mcp-run-python) for more information. Source code in pydantic_ai_slim/pydantic_ai/mcp.py #### \\_\\_init\\_\\_ Build a new MCP server. Parameters: Source code in pydantic_ai_slim/pydantic_ai/mcp.py #### command instance-attribute The command to run. #### args instance-attribute The arguments to pass to the command. #### env instance-attribute The environment variables the CLI server will have access to. By default the subprocess will not inherit any environment variables from the parent process. If you want to inherit the environment variables from the parent process, use env=os.environ . #### cwd instance-attribute The working directory to use when spawning the process.", "url": "https://ai.pydantic.dev/api/mcp/index.html#pydantic_ai.mcp.MCPServerStdio", "page": "api/mcp/index.html", "source_site": "pydantic_ai"}
{"title": "MCPServerSSE", "anchor": "pydantic_ai.mcp.MCPServerSSE", "heading_level": 3, "md_text": "Bases: _MCPServerHTTP An MCP server that connects over streamable HTTP connections. This class implements the SSE transport from the MCP specification. See <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse> for more information. Note Using this class as an async context manager will create a new pool of HTTP connections to connect to a server which should already be running. Example: Source code in pydantic_ai_slim/pydantic_ai/mcp.py", "url": "https://ai.pydantic.dev/api/mcp/index.html#pydantic_ai.mcp.MCPServerSSE", "page": "api/mcp/index.html", "source_site": "pydantic_ai"}
{"title": "MCPServerHTTP deprecated", "anchor": "pydantic_ai.mcp.MCPServerHTTP", "heading_level": 3, "md_text": "Bases: MCPServerSSE Deprecated The MCPServerHTTP class is deprecated, use MCPServerSSE instead. An MCP server that connects over HTTP using the old SSE transport. This class implements the SSE transport from the MCP specification. See <https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse> for more information. Note Using this class as an async context manager will create a new pool of HTTP connections to connect to a server which should already be running. Example: Source code in pydantic_ai_slim/pydantic_ai/mcp.py", "url": "https://ai.pydantic.dev/api/mcp/index.html#pydantic_ai.mcp.MCPServerHTTP", "page": "api/mcp/index.html", "source_site": "pydantic_ai"}
{"title": "MCPServerStreamableHTTP", "anchor": "pydantic_ai.mcp.MCPServerStreamableHTTP", "heading_level": 3, "md_text": "Bases: _MCPServerHTTP An MCP server that connects over HTTP using the Streamable HTTP transport. This class implements the Streamable HTTP transport from the MCP specification. See <https://modelcontextprotocol.io/introduction#streamable-http> for more information. Note Using this class as an async context manager will create a new pool of HTTP connections to connect to a server which should already be running. Example: Source code in pydantic_ai_slim/pydantic_ai/mcp.py", "url": "https://ai.pydantic.dev/api/mcp/index.html#pydantic_ai.mcp.MCPServerStreamableHTTP", "page": "api/mcp/index.html", "source_site": "pydantic_ai"}
{"title": "load_mcp_servers", "anchor": "pydantic_ai.mcp.load_mcp_servers", "heading_level": 3, "md_text": "Load MCP servers from a configuration file. Parameters: Returns: Raises: Source code in pydantic_ai_slim/pydantic_ai/mcp.py", "url": "https://ai.pydantic.dev/api/mcp/index.html#pydantic_ai.mcp.load_mcp_servers", "page": "api/mcp/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.models.anthropic", "anchor": "pydantic_aimodelsanthropic", "heading_level": 1, "md_text": "## Setup For details on how to set up authentication with this model, see [model configuration for Anthropic](../../../models/anthropic/index.html). ### LatestAnthropicModelNames module-attribute Latest Anthropic models. ### AnthropicModelName module-attribute Possible Anthropic model names. Since Anthropic supports a variety of date-stamped models, we explicitly list the latest models but allow any name in the type hints. See [the Anthropic docs](https://docs.anthropic.com/en/docs/about-claude/models) for a full list. ### AnthropicModelSettings Bases: ModelSettings Settings used for an Anthropic model request. Source code in pydantic_ai_slim/pydantic_ai/models/anthropic.py #### anthropic\\_metadata instance-attribute An object describing metadata about the request. Contains user_id , an external identifier for the user who is associated with the request. #### anthropic\\_thinking instance-attribute Determine whether the model should generate a thinking block. See [the Anthropic docs](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking) for more information. ### AnthropicModel dataclass Bases: Model A model that uses the Anthropic API. Internally, this uses the [Anthropic Python client](https://github.com/anthropics/anthropic-sdk-python) to interact with the API. Apart from __init__ , all methods are private or match those of the base class. Source code in pydantic_ai_slim/pydantic_ai/models/anthropic.py #### \\_\\_init\\_\\_ Initialize an Anthropic model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/anthropic.py #### model\\_name property The model name. #### system property The model provider. ### AnthropicStreamedResponse dataclass Bases: StreamedResponse Implementation of StreamedResponse for Anthropic models. Source code in pydantic_ai_slim/pydantic_ai/models/anthropic.py #### model\\_name property Get the model name of the response. #### provider\\_name property Get the provider name. #### timestamp property Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/models/anthropic/index.html#pydantic_aimodelsanthropic", "page": "api/models/anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "Setup", "anchor": "setup", "heading_level": 2, "md_text": "For details on how to set up authentication with this model, see [model configuration for Anthropic](../../../models/anthropic/index.html). ### LatestAnthropicModelNames module-attribute Latest Anthropic models. ### AnthropicModelName module-attribute Possible Anthropic model names. Since Anthropic supports a variety of date-stamped models, we explicitly list the latest models but allow any name in the type hints. See [the Anthropic docs](https://docs.anthropic.com/en/docs/about-claude/models) for a full list. ### AnthropicModelSettings Bases: ModelSettings Settings used for an Anthropic model request. Source code in pydantic_ai_slim/pydantic_ai/models/anthropic.py #### anthropic\\_metadata instance-attribute An object describing metadata about the request. Contains user_id , an external identifier for the user who is associated with the request. #### anthropic\\_thinking instance-attribute Determine whether the model should generate a thinking block. See [the Anthropic docs](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking) for more information. ### AnthropicModel dataclass Bases: Model A model that uses the Anthropic API. Internally, this uses the [Anthropic Python client](https://github.com/anthropics/anthropic-sdk-python) to interact with the API. Apart from __init__ , all methods are private or match those of the base class. Source code in pydantic_ai_slim/pydantic_ai/models/anthropic.py #### \\_\\_init\\_\\_ Initialize an Anthropic model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/anthropic.py #### model\\_name property The model name. #### system property The model provider. ### AnthropicStreamedResponse dataclass Bases: StreamedResponse Implementation of StreamedResponse for Anthropic models. Source code in pydantic_ai_slim/pydantic_ai/models/anthropic.py #### model\\_name property Get the model name of the response. #### provider\\_name property Get the provider name. #### timestamp property Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/models/anthropic/index.html#setup", "page": "api/models/anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "LatestAnthropicModelNames module-attribute", "anchor": "pydantic_ai.models.anthropic.LatestAnthropicModelNames", "heading_level": 3, "md_text": "Latest Anthropic models.", "url": "https://ai.pydantic.dev/api/models/anthropic/index.html#pydantic_ai.models.anthropic.LatestAnthropicModelNames", "page": "api/models/anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "AnthropicModelName module-attribute", "anchor": "pydantic_ai.models.anthropic.AnthropicModelName", "heading_level": 3, "md_text": "Possible Anthropic model names. Since Anthropic supports a variety of date-stamped models, we explicitly list the latest models but allow any name in the type hints. See [the Anthropic docs](https://docs.anthropic.com/en/docs/about-claude/models) for a full list.", "url": "https://ai.pydantic.dev/api/models/anthropic/index.html#pydantic_ai.models.anthropic.AnthropicModelName", "page": "api/models/anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "AnthropicModelSettings", "anchor": "pydantic_ai.models.anthropic.AnthropicModelSettings", "heading_level": 3, "md_text": "Bases: ModelSettings Settings used for an Anthropic model request. Source code in pydantic_ai_slim/pydantic_ai/models/anthropic.py #### anthropic\\_metadata instance-attribute An object describing metadata about the request. Contains user_id , an external identifier for the user who is associated with the request. #### anthropic\\_thinking instance-attribute Determine whether the model should generate a thinking block. See [the Anthropic docs](https://docs.anthropic.com/en/docs/build-with-claude/extended-thinking) for more information.", "url": "https://ai.pydantic.dev/api/models/anthropic/index.html#pydantic_ai.models.anthropic.AnthropicModelSettings", "page": "api/models/anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "AnthropicModel dataclass", "anchor": "pydantic_ai.models.anthropic.AnthropicModel", "heading_level": 3, "md_text": "Bases: Model A model that uses the Anthropic API. Internally, this uses the [Anthropic Python client](https://github.com/anthropics/anthropic-sdk-python) to interact with the API. Apart from __init__ , all methods are private or match those of the base class. Source code in pydantic_ai_slim/pydantic_ai/models/anthropic.py #### \\_\\_init\\_\\_ Initialize an Anthropic model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/anthropic.py #### model\\_name property The model name. #### system property The model provider.", "url": "https://ai.pydantic.dev/api/models/anthropic/index.html#pydantic_ai.models.anthropic.AnthropicModel", "page": "api/models/anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "AnthropicStreamedResponse dataclass", "anchor": "pydantic_ai.models.anthropic.AnthropicStreamedResponse", "heading_level": 3, "md_text": "Bases: StreamedResponse Implementation of StreamedResponse for Anthropic models. Source code in pydantic_ai_slim/pydantic_ai/models/anthropic.py #### model\\_name property Get the model name of the response. #### provider\\_name property Get the provider name. #### timestamp property Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/models/anthropic/index.html#pydantic_ai.models.anthropic.AnthropicStreamedResponse", "page": "api/models/anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "fasta2a", "anchor": "fasta2a_1", "heading_level": 1, "md_text": "### FastA2A Bases: Starlette The main class for the FastA2A library. Source code in .venv/lib/python3.12/site-packages/fasta2a/applications.py ### Broker dataclass Bases: ABC The broker class is in charge of scheduling the tasks. The HTTP server uses the broker to schedule tasks. The simple implementation is the InMemoryBroker , which is the broker that runs the tasks in the same process as the HTTP server. That said, this class can be extended to support remote workers. Source code in .venv/lib/python3.12/site-packages/fasta2a/broker.py #### run\\_task abstractmethod async Send a task to be executed by the worker. Source code in .venv/lib/python3.12/site-packages/fasta2a/broker.py #### cancel\\_task abstractmethod async Cancel a task. Source code in .venv/lib/python3.12/site-packages/fasta2a/broker.py #### receive\\_task\\_operations abstractmethod Receive task operations from the broker. On a multi-worker setup, the broker will need to round-robin the task operations between the workers. Source code in .venv/lib/python3.12/site-packages/fasta2a/broker.py ### Skill Bases: TypedDict Skills are a unit of capability that an agent can perform. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### id instance-attribute A unique identifier for the skill. #### name instance-attribute Human readable name of the skill. #### description instance-attribute A human-readable description of the skill. It will be used by the client or a human as a hint to understand the skill. #### tags instance-attribute Set of tag-words describing classes of capabilities for this specific skill. Examples: \"cooking\", \"customer support\", \"billing\". #### examples instance-attribute The set of example scenarios that the skill can perform. Will be used by the client as a hint to understand how the skill can be used. (e.g. \"I need a recipe for bread\") #### input\\_modes instance-attribute Supported mime types for input data. #### output\\_modes instance-attribute Supported mime types for output data. ### Storage Bases: ABC , Generic[ContextT] A storage to retrieve and save tasks, as well as retrieve and save context. The storage serves two purposes: 1. Task storage: Stores tasks in A2A protocol format with their status, artifacts, and message history 2. Context storage: Stores conversation context in a format optimized for the specific agent implementation Source code in .venv/lib/python3.12/site-packages/fasta2a/storage.py #### load\\_task abstractmethod async Load a task from storage. If the task is not found, return None. Source code in .venv/lib/python3.12/site-packages/fasta2a/storage.py #### submit\\_task abstractmethod async Submit a task to storage. Source code in .venv/lib/python3.12/site-packages/fasta2a/storage.py #### update\\_task abstractmethod async Update the state of a task. Appends artifacts and messages, if specified. Source code in .venv/lib/python3.12/site-packages/fasta2a/storage.py #### load\\_context abstractmethod async Retrieve the stored context given the context_id . Source code in .venv/lib/python3.12/site-packages/fasta2a/storage.py #### update\\_context abstractmethod async Updates the context for a context_id . Implementing agent can decide what to store in context. Source code in .venv/lib/python3.12/site-packages/fasta2a/storage.py ### Worker dataclass Bases: ABC , Generic[ContextT] A worker is responsible for executing tasks. Source code in .venv/lib/python3.12/site-packages/fasta2a/worker.py #### run async Run the worker. It connects to the broker, and it makes itself available to receive commands. Source code in .venv/lib/python3.12/site-packages/fasta2a/worker.py This module contains the schema for the agent card. ### AgentCard Bases: TypedDict The card that describes an agent. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### name instance-attribute Human readable name of the agent e.g. \"Recipe Agent\". #### description instance-attribute A human-readable description of the agent. Used to assist users and other agents in understanding what the agent can do. (e.g. \"Agent that helps users with recipes and cooking.\") #### url instance-attribute A URL to the address the agent is hosted at. #### version instance-attribute The version of the agent - format is up to the provider. (e.g. \"1.0.0\") #### protocol\\_version instance-attribute The version of the A2A protocol this agent supports. #### provider instance-attribute The service provider of the agent. #### documentation\\_url instance-attribute A URL to documentation for the agent. #### icon\\_url instance-attribute A URL to an icon for the agent. #### preferred\\_transport instance-attribute The transport of the preferred endpoint. If empty, defaults to JSONRPC. #### additional\\_interfaces instance-attribute Announcement of additional supported transports. #### capabilities instance-attribute The capabilities of the agent. #### security instance-attribute Security requirements for contacting the agent. #### security\\_schemes instance-attribute Security scheme definitions. #### default\\_input\\_modes instance-attribute Supported mime types for input data. #### default\\_output\\_modes instance-attribute Supported mime types for output data. ### AgentProvider Bases: TypedDict The service provider of the agent. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py ### AgentCapabilities Bases: TypedDict The capabilities of the agent. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### streaming instance-attribute Whether the agent supports streaming. #### push\\_notifications instance-attribute Whether the agent can notify updates to client. #### state\\_transition\\_history instance-attribute Whether the agent exposes status change history for tasks. ### HttpSecurityScheme Bases: TypedDict HTTP security scheme. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### scheme instance-attribute The name of the HTTP Authorization scheme. #### bearer\\_format instance-attribute A hint to the client to identify how the bearer token is formatted. #### description instance-attribute Description of this security scheme. ### ApiKeySecurityScheme Bases: TypedDict API Key security scheme. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### name instance-attribute The name of the header, query or cookie parameter to be used. #### in\\_ instance-attribute The location of the API key. #### description instance-attribute Description of this security scheme. ### OAuth2SecurityScheme Bases: TypedDict OAuth2 security scheme. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### flows instance-attribute An object containing configuration information for the flow types supported. #### description instance-attribute Description of this security scheme. ### OpenIdConnectSecurityScheme Bases: TypedDict OpenID Connect security scheme. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### open\\_id\\_connect\\_url instance-attribute OpenId Connect URL to discover OAuth2 configuration values. #### description instance-attribute Description of this security scheme. ### SecurityScheme module-attribute A security scheme for authentication. ### AgentInterface Bases: TypedDict An interface that the agent supports. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### transport instance-attribute The transport protocol (e.g., 'jsonrpc', 'websocket'). #### url instance-attribute The URL endpoint for this transport. #### description instance-attribute Description of this interface. ### AgentExtension Bases: TypedDict A declaration of an extension supported by an Agent. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### uri instance-attribute The URI of the extension. #### description instance-attribute A description of how this agent uses this extension. #### required instance-attribute Whether the client must follow specific requirements of the extension. #### params instance-attribute Optional configuration for the extension. ### Skill Bases: TypedDict Skills are a unit of capability that an agent can perform. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### id instance-attribute A unique identifier for the skill. #### name instance-attribute Human readable name of the skill. #### description instance-attribute A human-readable description of the skill. It will be used by the client or a human as a hint to understand the skill. #### tags instance-attribute Set of tag-words describing classes of capabilities for this specific skill. Examples: \"cooking\", \"customer support\", \"billing\". #### examples instance-attribute The set of example scenarios that the skill can perform. Will be used by the client as a hint to understand how the skill can be used. (e.g. \"I need a recipe for bread\") #### input\\_modes instance-attribute Supported mime types for input data. #### output\\_modes instance-attribute Supported mime types for output data. ### Artifact Bases: TypedDict Agents generate Artifacts as an end result of a Task. Artifacts are immutable, can be named, and can have multiple parts. A streaming response can append parts to existing Artifacts. A single Task can generate many Artifacts. For example, \"create a webpage\" could create separate HTML and image Artifacts. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### artifact\\_id instance-attribute Unique identifier for the artifact. #### name instance-attribute The name of the artifact. #### description instance-attribute A description of the artifact. #### parts instance-attribute The parts that make up the artifact. #### metadata instance-attribute Metadata about the artifact. #### extensions instance-attribute Array of extensions. #### append instance-attribute Whether to append this artifact to an existing one. #### last\\_chunk instance-attribute Whether this is the last chunk of the artifact. ### PushNotificationConfig Bases: TypedDict Configuration for push notifications. A2A supports a secure notification mechanism whereby an agent can notify a client of an update outside of a connected session via a PushNotificationService. Within and across enterprises, it is critical that the agent verifies the identity of the notification service, authenticates itself with the service, and presents an identifier that ties the notification to the executing Task. The target server of the PushNotificationService should be considered a separate service, and is not guaranteed (or even expected) to be the client directly. This PushNotificationService is responsible for authenticating and authorizing the agent and for proxying the verified notification to the appropriate endpoint (which could be anything from a pub/sub queue, to an email inbox or other service, etc). For contrived scenarios with isolated client-agent pairs (e.g. local service mesh in a contained VPC, etc.) or isolated environments without enterprise security concerns, the client may choose to simply open a port and act as its own PushNotificationService. Any enterprise implementation will likely have a centralized service that authenticates the remote agents with trusted notification credentials and can handle online/offline scenarios. (This should be thought of similarly to a mobile Push Notification Service). Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### id instance-attribute Server-assigned identifier. #### url instance-attribute The URL to send push notifications to. #### token instance-attribute Token unique to this task/session. #### authentication instance-attribute Authentication details for push notifications. ### TaskPushNotificationConfig Bases: TypedDict Configuration for task push notifications. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### id instance-attribute The task id. #### push\\_notification\\_config instance-attribute The push notification configuration. ### Message Bases: TypedDict A Message contains any content that is not an Artifact. This can include things like agent thoughts, user context, instructions, errors, status, or metadata. All content from a client comes in the form of a Message. Agents send Messages to communicate status or to provide instructions (whereas generated results are sent as Artifacts). A Message can have multiple parts to denote different pieces of content. For example, a user request could include a textual description from a user and then multiple files used as context from the client. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### role instance-attribute The role of the message. #### parts instance-attribute The parts of the message. #### kind instance-attribute Event type. #### metadata instance-attribute Metadata about the message. #### message\\_id instance-attribute Identifier created by the message creator. #### context\\_id instance-attribute The context the message is associated with. #### task\\_id instance-attribute Identifier of task the message is related to. #### reference\\_task\\_ids instance-attribute Array of task IDs this message references. #### extensions instance-attribute Array of extensions. ### TextPart Bases: _BasePart A part that contains text. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### kind instance-attribute The kind of the part. #### text instance-attribute The text of the part. ### FileWithBytes Bases: TypedDict File with base64 encoded data. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### bytes instance-attribute The base64 encoded content of the file. #### mime\\_type instance-attribute Optional mime type for the file. ### FileWithUri Bases: TypedDict File with URI reference. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### uri instance-attribute The URI of the file. #### mime\\_type instance-attribute The mime type of the file. ### FilePart Bases: _BasePart A part that contains a file. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### kind instance-attribute The kind of the part. #### file instance-attribute The file content - either bytes or URI. ### DataPart Bases: _BasePart A part that contains structured data. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### kind instance-attribute The kind of the part. #### data instance-attribute The data of the part. ### Part module-attribute A fully formed piece of content exchanged between a client and a remote agent as part of a Message or an Artifact. Each Part has its own content type and metadata. ### TaskState module-attribute The possible states of a task. ### TaskStatus Bases: TypedDict Status and accompanying message for a task. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### state instance-attribute The current state of the task. #### message instance-attribute Additional status updates for client. #### timestamp instance-attribute ISO datetime value of when the status was updated. ### Task Bases: TypedDict A Task is a stateful entity that allows Clients and Remote Agents to achieve a specific outcome. Clients and Remote Agents exchange Messages within a Task. Remote Agents generate results as Artifacts. A Task is always created by a Client and the status is always determined by the Remote Agent. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### id instance-attribute Unique identifier for the task. #### context\\_id instance-attribute The context the task is associated with. #### kind instance-attribute Event type. #### status instance-attribute Current status of the task. #### history instance-attribute Optional history of messages. #### artifacts instance-attribute Collection of artifacts created by the agent. #### metadata instance-attribute Extension metadata. ### TaskStatusUpdateEvent Bases: TypedDict Sent by server during message/stream requests. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### task\\_id instance-attribute The id of the task. #### context\\_id instance-attribute The context the task is associated with. #### kind instance-attribute Event type. #### status instance-attribute The status of the task. #### final instance-attribute Indicates the end of the event stream. #### metadata instance-attribute Extension metadata. ### TaskArtifactUpdateEvent Bases: TypedDict Sent by server during message/stream requests. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### task\\_id instance-attribute The id of the task. #### context\\_id instance-attribute The context the task is associated with. #### kind instance-attribute Event type identification. #### artifact instance-attribute The artifact that was updated. #### append instance-attribute Whether to append to existing artifact (true) or replace (false). #### last\\_chunk instance-attribute Indicates this is the final chunk of the artifact. #### metadata instance-attribute Extension metadata. ### TaskIdParams Bases: TypedDict Parameters for a task id. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py ### TaskQueryParams Bases: TaskIdParams Query parameters for a task. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### history\\_length instance-attribute Number of recent messages to be retrieved. ### MessageSendConfiguration Bases: TypedDict Configuration for the send message request. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### accepted\\_output\\_modes instance-attribute Accepted output modalities by the client. #### blocking instance-attribute If the server should treat the client as a blocking request. #### history\\_length instance-attribute Number of recent messages to be retrieved. #### push\\_notification\\_config instance-attribute Where the server should send notifications when disconnected. ### MessageSendParams Bases: TypedDict Parameters for message/send method. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### configuration instance-attribute Send message configuration. #### message instance-attribute The message being sent to the server. #### metadata instance-attribute Extension metadata. ### TaskSendParams Bases: TypedDict Internal parameters for task execution within the framework. Note: This is not part of the A2A protocol - it's used internally for broker/worker communication. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### id instance-attribute The id of the task. #### context\\_id instance-attribute The context id for the task. #### message instance-attribute The message to process. #### history\\_length instance-attribute Number of recent messages to be retrieved. #### metadata instance-attribute Extension metadata. ### ListTaskPushNotificationConfigParams Bases: TypedDict Parameters for getting list of pushNotificationConfigurations associated with a Task. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### id instance-attribute Task id. #### metadata instance-attribute Extension metadata. ### DeleteTaskPushNotificationConfigParams Bases: TypedDict Parameters for removing pushNotificationConfiguration associated with a Task. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### id instance-attribute Task id. #### push\\_notification\\_config\\_id instance-attribute The push notification config id to delete. #### metadata instance-attribute Extension metadata. ### JSONRPCMessage Bases: TypedDict A JSON RPC message. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### jsonrpc instance-attribute The JSON RPC version. #### id instance-attribute The request id. ### JSONRPCRequest Bases: JSONRPCMessage , Generic[Method, Params] A JSON RPC request. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### method instance-attribute The method to call. #### params instance-attribute The parameters to pass to the method. ### JSONRPCError Bases: TypedDict , Generic[CodeT, MessageT] A JSON RPC error. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py ### JSONRPCResponse Bases: JSONRPCMessage , Generic[ResultT, ErrorT] A JSON RPC response. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py ### JSONParseError module-attribute A JSON RPC error for a parse error. ### InvalidRequestError module-attribute A JSON RPC error for an invalid request. ### MethodNotFoundError module-attribute A JSON RPC error for a method not found. ### InvalidParamsError module-attribute A JSON RPC error for invalid parameters. ### InternalError module-attribute A JSON RPC error for an internal error. ### TaskNotFoundError module-attribute A JSON RPC error for a task not found. ### TaskNotCancelableError module-attribute A JSON RPC error for a task not cancelable. ### PushNotificationNotSupportedError module-attribute A JSON RPC error for a push notification not supported. ### UnsupportedOperationError module-attribute A JSON RPC error for an unsupported operation. ### ContentTypeNotSupportedError module-attribute A JSON RPC error for incompatible content types. ### InvalidAgentResponseError module-attribute A JSON RPC error for invalid agent response. ### SendMessageRequest module-attribute A JSON RPC request to send a message. ### SendMessageResponse module-attribute A JSON RPC response to send a message. ### StreamMessageRequest module-attribute A JSON RPC request to stream a message. ### GetTaskRequest module-attribute A JSON RPC request to get a task. ### GetTaskResponse module-attribute A JSON RPC response to get a task. ### CancelTaskRequest module-attribute A JSON RPC request to cancel a task. ### CancelTaskResponse module-attribute A JSON RPC response to cancel a task. ### SetTaskPushNotificationRequest module-attribute A JSON RPC request to set a task push notification. ### SetTaskPushNotificationResponse module-attribute A JSON RPC response to set a task push notification. ### GetTaskPushNotificationRequest module-attribute A JSON RPC request to get a task push notification. ### GetTaskPushNotificationResponse module-attribute A JSON RPC response to get a task push notification. ### ResubscribeTaskRequest module-attribute A JSON RPC request to resubscribe to a task. ### ListTaskPushNotificationConfigRequest module-attribute A JSON RPC request to list task push notification configs. ### DeleteTaskPushNotificationConfigRequest module-attribute A JSON RPC request to delete a task push notification config. ### A2ARequest module-attribute A JSON RPC request to the A2A server. ### A2AResponse module-attribute A JSON RPC response from the A2A server. ### A2AClient A client for the A2A protocol. Source code in .venv/lib/python3.12/site-packages/fasta2a/client.py #### send\\_message async Send a message using the A2A protocol. Returns a JSON-RPC response containing either a result (Task) or an error. Source code in .venv/lib/python3.12/site-packages/fasta2a/client.py ### UnexpectedResponseError Bases: Exception An error raised when an unexpected response is received from the server. Source code in .venv/lib/python3.12/site-packages/fasta2a/client.py", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a_1", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "FastA2A", "anchor": "fasta2a.FastA2A", "heading_level": 3, "md_text": "Bases: Starlette The main class for the FastA2A library. Source code in .venv/lib/python3.12/site-packages/fasta2a/applications.py", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.FastA2A", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "Broker dataclass", "anchor": "fasta2a.Broker", "heading_level": 3, "md_text": "Bases: ABC The broker class is in charge of scheduling the tasks. The HTTP server uses the broker to schedule tasks. The simple implementation is the InMemoryBroker , which is the broker that runs the tasks in the same process as the HTTP server. That said, this class can be extended to support remote workers. Source code in .venv/lib/python3.12/site-packages/fasta2a/broker.py #### run\\_task abstractmethod async Send a task to be executed by the worker. Source code in .venv/lib/python3.12/site-packages/fasta2a/broker.py #### cancel\\_task abstractmethod async Cancel a task. Source code in .venv/lib/python3.12/site-packages/fasta2a/broker.py #### receive\\_task\\_operations abstractmethod Receive task operations from the broker. On a multi-worker setup, the broker will need to round-robin the task operations between the workers. Source code in .venv/lib/python3.12/site-packages/fasta2a/broker.py", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.Broker", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "Skill", "anchor": "fasta2a.Skill", "heading_level": 3, "md_text": "Bases: TypedDict Skills are a unit of capability that an agent can perform. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### id instance-attribute A unique identifier for the skill. #### name instance-attribute Human readable name of the skill. #### description instance-attribute A human-readable description of the skill. It will be used by the client or a human as a hint to understand the skill. #### tags instance-attribute Set of tag-words describing classes of capabilities for this specific skill. Examples: \"cooking\", \"customer support\", \"billing\". #### examples instance-attribute The set of example scenarios that the skill can perform. Will be used by the client as a hint to understand how the skill can be used. (e.g. \"I need a recipe for bread\") #### input\\_modes instance-attribute Supported mime types for input data. #### output\\_modes instance-attribute Supported mime types for output data.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.Skill", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "Storage", "anchor": "fasta2a.Storage", "heading_level": 3, "md_text": "Bases: ABC , Generic[ContextT] A storage to retrieve and save tasks, as well as retrieve and save context. The storage serves two purposes: 1. Task storage: Stores tasks in A2A protocol format with their status, artifacts, and message history 2. Context storage: Stores conversation context in a format optimized for the specific agent implementation Source code in .venv/lib/python3.12/site-packages/fasta2a/storage.py #### load\\_task abstractmethod async Load a task from storage. If the task is not found, return None. Source code in .venv/lib/python3.12/site-packages/fasta2a/storage.py #### submit\\_task abstractmethod async Submit a task to storage. Source code in .venv/lib/python3.12/site-packages/fasta2a/storage.py #### update\\_task abstractmethod async Update the state of a task. Appends artifacts and messages, if specified. Source code in .venv/lib/python3.12/site-packages/fasta2a/storage.py #### load\\_context abstractmethod async Retrieve the stored context given the context_id . Source code in .venv/lib/python3.12/site-packages/fasta2a/storage.py #### update\\_context abstractmethod async Updates the context for a context_id . Implementing agent can decide what to store in context. Source code in .venv/lib/python3.12/site-packages/fasta2a/storage.py", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.Storage", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "Worker dataclass", "anchor": "fasta2a.Worker", "heading_level": 3, "md_text": "Bases: ABC , Generic[ContextT] A worker is responsible for executing tasks. Source code in .venv/lib/python3.12/site-packages/fasta2a/worker.py #### run async Run the worker. It connects to the broker, and it makes itself available to receive commands. Source code in .venv/lib/python3.12/site-packages/fasta2a/worker.py", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.Worker", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "AgentCard", "anchor": "fasta2a.schema.AgentCard", "heading_level": 3, "md_text": "Bases: TypedDict The card that describes an agent. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### name instance-attribute Human readable name of the agent e.g. \"Recipe Agent\". #### description instance-attribute A human-readable description of the agent. Used to assist users and other agents in understanding what the agent can do. (e.g. \"Agent that helps users with recipes and cooking.\") #### url instance-attribute A URL to the address the agent is hosted at. #### version instance-attribute The version of the agent - format is up to the provider. (e.g. \"1.0.0\") #### protocol\\_version instance-attribute The version of the A2A protocol this agent supports. #### provider instance-attribute The service provider of the agent. #### documentation\\_url instance-attribute A URL to documentation for the agent. #### icon\\_url instance-attribute A URL to an icon for the agent. #### preferred\\_transport instance-attribute The transport of the preferred endpoint. If empty, defaults to JSONRPC. #### additional\\_interfaces instance-attribute Announcement of additional supported transports. #### capabilities instance-attribute The capabilities of the agent. #### security instance-attribute Security requirements for contacting the agent. #### security\\_schemes instance-attribute Security scheme definitions. #### default\\_input\\_modes instance-attribute Supported mime types for input data. #### default\\_output\\_modes instance-attribute Supported mime types for output data.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.AgentCard", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "AgentProvider", "anchor": "fasta2a.schema.AgentProvider", "heading_level": 3, "md_text": "Bases: TypedDict The service provider of the agent. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.AgentProvider", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "AgentCapabilities", "anchor": "fasta2a.schema.AgentCapabilities", "heading_level": 3, "md_text": "Bases: TypedDict The capabilities of the agent. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### streaming instance-attribute Whether the agent supports streaming. #### push\\_notifications instance-attribute Whether the agent can notify updates to client. #### state\\_transition\\_history instance-attribute Whether the agent exposes status change history for tasks.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.AgentCapabilities", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "HttpSecurityScheme", "anchor": "fasta2a.schema.HttpSecurityScheme", "heading_level": 3, "md_text": "Bases: TypedDict HTTP security scheme. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### scheme instance-attribute The name of the HTTP Authorization scheme. #### bearer\\_format instance-attribute A hint to the client to identify how the bearer token is formatted. #### description instance-attribute Description of this security scheme.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.HttpSecurityScheme", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "ApiKeySecurityScheme", "anchor": "fasta2a.schema.ApiKeySecurityScheme", "heading_level": 3, "md_text": "Bases: TypedDict API Key security scheme. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### name instance-attribute The name of the header, query or cookie parameter to be used. #### in\\_ instance-attribute The location of the API key. #### description instance-attribute Description of this security scheme.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.ApiKeySecurityScheme", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "OAuth2SecurityScheme", "anchor": "fasta2a.schema.OAuth2SecurityScheme", "heading_level": 3, "md_text": "Bases: TypedDict OAuth2 security scheme. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### flows instance-attribute An object containing configuration information for the flow types supported. #### description instance-attribute Description of this security scheme.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.OAuth2SecurityScheme", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "OpenIdConnectSecurityScheme", "anchor": "fasta2a.schema.OpenIdConnectSecurityScheme", "heading_level": 3, "md_text": "Bases: TypedDict OpenID Connect security scheme. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### open\\_id\\_connect\\_url instance-attribute OpenId Connect URL to discover OAuth2 configuration values. #### description instance-attribute Description of this security scheme.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.OpenIdConnectSecurityScheme", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "SecurityScheme module-attribute", "anchor": "fasta2a.schema.SecurityScheme", "heading_level": 3, "md_text": "A security scheme for authentication.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.SecurityScheme", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "AgentInterface", "anchor": "fasta2a.schema.AgentInterface", "heading_level": 3, "md_text": "Bases: TypedDict An interface that the agent supports. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### transport instance-attribute The transport protocol (e.g., 'jsonrpc', 'websocket'). #### url instance-attribute The URL endpoint for this transport. #### description instance-attribute Description of this interface.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.AgentInterface", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "AgentExtension", "anchor": "fasta2a.schema.AgentExtension", "heading_level": 3, "md_text": "Bases: TypedDict A declaration of an extension supported by an Agent. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### uri instance-attribute The URI of the extension. #### description instance-attribute A description of how this agent uses this extension. #### required instance-attribute Whether the client must follow specific requirements of the extension. #### params instance-attribute Optional configuration for the extension.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.AgentExtension", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "Skill", "anchor": "fasta2a.schema.Skill", "heading_level": 3, "md_text": "Bases: TypedDict Skills are a unit of capability that an agent can perform. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### id instance-attribute A unique identifier for the skill. #### name instance-attribute Human readable name of the skill. #### description instance-attribute A human-readable description of the skill. It will be used by the client or a human as a hint to understand the skill. #### tags instance-attribute Set of tag-words describing classes of capabilities for this specific skill. Examples: \"cooking\", \"customer support\", \"billing\". #### examples instance-attribute The set of example scenarios that the skill can perform. Will be used by the client as a hint to understand how the skill can be used. (e.g. \"I need a recipe for bread\") #### input\\_modes instance-attribute Supported mime types for input data. #### output\\_modes instance-attribute Supported mime types for output data.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.Skill", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "Artifact", "anchor": "fasta2a.schema.Artifact", "heading_level": 3, "md_text": "Bases: TypedDict Agents generate Artifacts as an end result of a Task. Artifacts are immutable, can be named, and can have multiple parts. A streaming response can append parts to existing Artifacts. A single Task can generate many Artifacts. For example, \"create a webpage\" could create separate HTML and image Artifacts. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### artifact\\_id instance-attribute Unique identifier for the artifact. #### name instance-attribute The name of the artifact. #### description instance-attribute A description of the artifact. #### parts instance-attribute The parts that make up the artifact. #### metadata instance-attribute Metadata about the artifact. #### extensions instance-attribute Array of extensions. #### append instance-attribute Whether to append this artifact to an existing one. #### last\\_chunk instance-attribute Whether this is the last chunk of the artifact.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.Artifact", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "PushNotificationConfig", "anchor": "fasta2a.schema.PushNotificationConfig", "heading_level": 3, "md_text": "Bases: TypedDict Configuration for push notifications. A2A supports a secure notification mechanism whereby an agent can notify a client of an update outside of a connected session via a PushNotificationService. Within and across enterprises, it is critical that the agent verifies the identity of the notification service, authenticates itself with the service, and presents an identifier that ties the notification to the executing Task. The target server of the PushNotificationService should be considered a separate service, and is not guaranteed (or even expected) to be the client directly. This PushNotificationService is responsible for authenticating and authorizing the agent and for proxying the verified notification to the appropriate endpoint (which could be anything from a pub/sub queue, to an email inbox or other service, etc). For contrived scenarios with isolated client-agent pairs (e.g. local service mesh in a contained VPC, etc.) or isolated environments without enterprise security concerns, the client may choose to simply open a port and act as its own PushNotificationService. Any enterprise implementation will likely have a centralized service that authenticates the remote agents with trusted notification credentials and can handle online/offline scenarios. (This should be thought of similarly to a mobile Push Notification Service). Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### id instance-attribute Server-assigned identifier. #### url instance-attribute The URL to send push notifications to. #### token instance-attribute Token unique to this task/session. #### authentication instance-attribute Authentication details for push notifications.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.PushNotificationConfig", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "TaskPushNotificationConfig", "anchor": "fasta2a.schema.TaskPushNotificationConfig", "heading_level": 3, "md_text": "Bases: TypedDict Configuration for task push notifications. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### id instance-attribute The task id. #### push\\_notification\\_config instance-attribute The push notification configuration.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.TaskPushNotificationConfig", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "Message", "anchor": "fasta2a.schema.Message", "heading_level": 3, "md_text": "Bases: TypedDict A Message contains any content that is not an Artifact. This can include things like agent thoughts, user context, instructions, errors, status, or metadata. All content from a client comes in the form of a Message. Agents send Messages to communicate status or to provide instructions (whereas generated results are sent as Artifacts). A Message can have multiple parts to denote different pieces of content. For example, a user request could include a textual description from a user and then multiple files used as context from the client. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### role instance-attribute The role of the message. #### parts instance-attribute The parts of the message. #### kind instance-attribute Event type. #### metadata instance-attribute Metadata about the message. #### message\\_id instance-attribute Identifier created by the message creator. #### context\\_id instance-attribute The context the message is associated with. #### task\\_id instance-attribute Identifier of task the message is related to. #### reference\\_task\\_ids instance-attribute Array of task IDs this message references. #### extensions instance-attribute Array of extensions.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.Message", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "TextPart", "anchor": "fasta2a.schema.TextPart", "heading_level": 3, "md_text": "Bases: _BasePart A part that contains text. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### kind instance-attribute The kind of the part. #### text instance-attribute The text of the part.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.TextPart", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "FileWithBytes", "anchor": "fasta2a.schema.FileWithBytes", "heading_level": 3, "md_text": "Bases: TypedDict File with base64 encoded data. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### bytes instance-attribute The base64 encoded content of the file. #### mime\\_type instance-attribute Optional mime type for the file.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.FileWithBytes", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "FileWithUri", "anchor": "fasta2a.schema.FileWithUri", "heading_level": 3, "md_text": "Bases: TypedDict File with URI reference. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### uri instance-attribute The URI of the file. #### mime\\_type instance-attribute The mime type of the file.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.FileWithUri", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "FilePart", "anchor": "fasta2a.schema.FilePart", "heading_level": 3, "md_text": "Bases: _BasePart A part that contains a file. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### kind instance-attribute The kind of the part. #### file instance-attribute The file content - either bytes or URI.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.FilePart", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "DataPart", "anchor": "fasta2a.schema.DataPart", "heading_level": 3, "md_text": "Bases: _BasePart A part that contains structured data. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### kind instance-attribute The kind of the part. #### data instance-attribute The data of the part.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.DataPart", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "Part module-attribute", "anchor": "fasta2a.schema.Part", "heading_level": 3, "md_text": "A fully formed piece of content exchanged between a client and a remote agent as part of a Message or an Artifact. Each Part has its own content type and metadata.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.Part", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "TaskState module-attribute", "anchor": "fasta2a.schema.TaskState", "heading_level": 3, "md_text": "The possible states of a task.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.TaskState", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "TaskStatus", "anchor": "fasta2a.schema.TaskStatus", "heading_level": 3, "md_text": "Bases: TypedDict Status and accompanying message for a task. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### state instance-attribute The current state of the task. #### message instance-attribute Additional status updates for client. #### timestamp instance-attribute ISO datetime value of when the status was updated.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.TaskStatus", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "Task", "anchor": "fasta2a.schema.Task", "heading_level": 3, "md_text": "Bases: TypedDict A Task is a stateful entity that allows Clients and Remote Agents to achieve a specific outcome. Clients and Remote Agents exchange Messages within a Task. Remote Agents generate results as Artifacts. A Task is always created by a Client and the status is always determined by the Remote Agent. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### id instance-attribute Unique identifier for the task. #### context\\_id instance-attribute The context the task is associated with. #### kind instance-attribute Event type. #### status instance-attribute Current status of the task. #### history instance-attribute Optional history of messages. #### artifacts instance-attribute Collection of artifacts created by the agent. #### metadata instance-attribute Extension metadata.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.Task", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "TaskStatusUpdateEvent", "anchor": "fasta2a.schema.TaskStatusUpdateEvent", "heading_level": 3, "md_text": "Bases: TypedDict Sent by server during message/stream requests. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### task\\_id instance-attribute The id of the task. #### context\\_id instance-attribute The context the task is associated with. #### kind instance-attribute Event type. #### status instance-attribute The status of the task. #### final instance-attribute Indicates the end of the event stream. #### metadata instance-attribute Extension metadata.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.TaskStatusUpdateEvent", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "TaskArtifactUpdateEvent", "anchor": "fasta2a.schema.TaskArtifactUpdateEvent", "heading_level": 3, "md_text": "Bases: TypedDict Sent by server during message/stream requests. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### task\\_id instance-attribute The id of the task. #### context\\_id instance-attribute The context the task is associated with. #### kind instance-attribute Event type identification. #### artifact instance-attribute The artifact that was updated. #### append instance-attribute Whether to append to existing artifact (true) or replace (false). #### last\\_chunk instance-attribute Indicates this is the final chunk of the artifact. #### metadata instance-attribute Extension metadata.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.TaskArtifactUpdateEvent", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "TaskIdParams", "anchor": "fasta2a.schema.TaskIdParams", "heading_level": 3, "md_text": "Bases: TypedDict Parameters for a task id. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.TaskIdParams", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "TaskQueryParams", "anchor": "fasta2a.schema.TaskQueryParams", "heading_level": 3, "md_text": "Bases: TaskIdParams Query parameters for a task. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### history\\_length instance-attribute Number of recent messages to be retrieved.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.TaskQueryParams", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "MessageSendConfiguration", "anchor": "fasta2a.schema.MessageSendConfiguration", "heading_level": 3, "md_text": "Bases: TypedDict Configuration for the send message request. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### accepted\\_output\\_modes instance-attribute Accepted output modalities by the client. #### blocking instance-attribute If the server should treat the client as a blocking request. #### history\\_length instance-attribute Number of recent messages to be retrieved. #### push\\_notification\\_config instance-attribute Where the server should send notifications when disconnected.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.MessageSendConfiguration", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "MessageSendParams", "anchor": "fasta2a.schema.MessageSendParams", "heading_level": 3, "md_text": "Bases: TypedDict Parameters for message/send method. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### configuration instance-attribute Send message configuration. #### message instance-attribute The message being sent to the server. #### metadata instance-attribute Extension metadata.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.MessageSendParams", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "TaskSendParams", "anchor": "fasta2a.schema.TaskSendParams", "heading_level": 3, "md_text": "Bases: TypedDict Internal parameters for task execution within the framework. Note: This is not part of the A2A protocol - it's used internally for broker/worker communication. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### id instance-attribute The id of the task. #### context\\_id instance-attribute The context id for the task. #### message instance-attribute The message to process. #### history\\_length instance-attribute Number of recent messages to be retrieved. #### metadata instance-attribute Extension metadata.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.TaskSendParams", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "ListTaskPushNotificationConfigParams", "anchor": "fasta2a.schema.ListTaskPushNotificationConfigParams", "heading_level": 3, "md_text": "Bases: TypedDict Parameters for getting list of pushNotificationConfigurations associated with a Task. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### id instance-attribute Task id. #### metadata instance-attribute Extension metadata.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.ListTaskPushNotificationConfigParams", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "DeleteTaskPushNotificationConfigParams", "anchor": "fasta2a.schema.DeleteTaskPushNotificationConfigParams", "heading_level": 3, "md_text": "Bases: TypedDict Parameters for removing pushNotificationConfiguration associated with a Task. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### id instance-attribute Task id. #### push\\_notification\\_config\\_id instance-attribute The push notification config id to delete. #### metadata instance-attribute Extension metadata.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.DeleteTaskPushNotificationConfigParams", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "JSONRPCMessage", "anchor": "fasta2a.schema.JSONRPCMessage", "heading_level": 3, "md_text": "Bases: TypedDict A JSON RPC message. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### jsonrpc instance-attribute The JSON RPC version. #### id instance-attribute The request id.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.JSONRPCMessage", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "JSONRPCRequest", "anchor": "fasta2a.schema.JSONRPCRequest", "heading_level": 3, "md_text": "Bases: JSONRPCMessage , Generic[Method, Params] A JSON RPC request. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py #### method instance-attribute The method to call. #### params instance-attribute The parameters to pass to the method.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.JSONRPCRequest", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "JSONRPCError", "anchor": "fasta2a.schema.JSONRPCError", "heading_level": 3, "md_text": "Bases: TypedDict , Generic[CodeT, MessageT] A JSON RPC error. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.JSONRPCError", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "JSONRPCResponse", "anchor": "fasta2a.schema.JSONRPCResponse", "heading_level": 3, "md_text": "Bases: JSONRPCMessage , Generic[ResultT, ErrorT] A JSON RPC response. Source code in .venv/lib/python3.12/site-packages/fasta2a/schema.py", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.JSONRPCResponse", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "JSONParseError module-attribute", "anchor": "fasta2a.schema.JSONParseError", "heading_level": 3, "md_text": "A JSON RPC error for a parse error.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.JSONParseError", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "InvalidRequestError module-attribute", "anchor": "fasta2a.schema.InvalidRequestError", "heading_level": 3, "md_text": "A JSON RPC error for an invalid request.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.InvalidRequestError", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "MethodNotFoundError module-attribute", "anchor": "fasta2a.schema.MethodNotFoundError", "heading_level": 3, "md_text": "A JSON RPC error for a method not found.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.MethodNotFoundError", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "InvalidParamsError module-attribute", "anchor": "fasta2a.schema.InvalidParamsError", "heading_level": 3, "md_text": "A JSON RPC error for invalid parameters.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.InvalidParamsError", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "InternalError module-attribute", "anchor": "fasta2a.schema.InternalError", "heading_level": 3, "md_text": "A JSON RPC error for an internal error.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.InternalError", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "TaskNotFoundError module-attribute", "anchor": "fasta2a.schema.TaskNotFoundError", "heading_level": 3, "md_text": "A JSON RPC error for a task not found.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.TaskNotFoundError", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "TaskNotCancelableError module-attribute", "anchor": "fasta2a.schema.TaskNotCancelableError", "heading_level": 3, "md_text": "A JSON RPC error for a task not cancelable.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.TaskNotCancelableError", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "PushNotificationNotSupportedError module-attribute", "anchor": "fasta2a.schema.PushNotificationNotSupportedError", "heading_level": 3, "md_text": "A JSON RPC error for a push notification not supported.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.PushNotificationNotSupportedError", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "UnsupportedOperationError module-attribute", "anchor": "fasta2a.schema.UnsupportedOperationError", "heading_level": 3, "md_text": "A JSON RPC error for an unsupported operation.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.UnsupportedOperationError", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "ContentTypeNotSupportedError module-attribute", "anchor": "fasta2a.schema.ContentTypeNotSupportedError", "heading_level": 3, "md_text": "A JSON RPC error for incompatible content types.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.ContentTypeNotSupportedError", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "InvalidAgentResponseError module-attribute", "anchor": "fasta2a.schema.InvalidAgentResponseError", "heading_level": 3, "md_text": "A JSON RPC error for invalid agent response.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.InvalidAgentResponseError", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "SendMessageRequest module-attribute", "anchor": "fasta2a.schema.SendMessageRequest", "heading_level": 3, "md_text": "A JSON RPC request to send a message.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.SendMessageRequest", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "SendMessageResponse module-attribute", "anchor": "fasta2a.schema.SendMessageResponse", "heading_level": 3, "md_text": "A JSON RPC response to send a message.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.SendMessageResponse", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "StreamMessageRequest module-attribute", "anchor": "fasta2a.schema.StreamMessageRequest", "heading_level": 3, "md_text": "A JSON RPC request to stream a message.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.StreamMessageRequest", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "GetTaskRequest module-attribute", "anchor": "fasta2a.schema.GetTaskRequest", "heading_level": 3, "md_text": "A JSON RPC request to get a task.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.GetTaskRequest", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "GetTaskResponse module-attribute", "anchor": "fasta2a.schema.GetTaskResponse", "heading_level": 3, "md_text": "A JSON RPC response to get a task.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.GetTaskResponse", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "CancelTaskRequest module-attribute", "anchor": "fasta2a.schema.CancelTaskRequest", "heading_level": 3, "md_text": "A JSON RPC request to cancel a task.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.CancelTaskRequest", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "CancelTaskResponse module-attribute", "anchor": "fasta2a.schema.CancelTaskResponse", "heading_level": 3, "md_text": "A JSON RPC response to cancel a task.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.CancelTaskResponse", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "SetTaskPushNotificationRequest module-attribute", "anchor": "fasta2a.schema.SetTaskPushNotificationRequest", "heading_level": 3, "md_text": "A JSON RPC request to set a task push notification.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.SetTaskPushNotificationRequest", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "SetTaskPushNotificationResponse module-attribute", "anchor": "fasta2a.schema.SetTaskPushNotificationResponse", "heading_level": 3, "md_text": "A JSON RPC response to set a task push notification.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.SetTaskPushNotificationResponse", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "GetTaskPushNotificationRequest module-attribute", "anchor": "fasta2a.schema.GetTaskPushNotificationRequest", "heading_level": 3, "md_text": "A JSON RPC request to get a task push notification.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.GetTaskPushNotificationRequest", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "GetTaskPushNotificationResponse module-attribute", "anchor": "fasta2a.schema.GetTaskPushNotificationResponse", "heading_level": 3, "md_text": "A JSON RPC response to get a task push notification.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.GetTaskPushNotificationResponse", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "ResubscribeTaskRequest module-attribute", "anchor": "fasta2a.schema.ResubscribeTaskRequest", "heading_level": 3, "md_text": "A JSON RPC request to resubscribe to a task.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.ResubscribeTaskRequest", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "ListTaskPushNotificationConfigRequest module-attribute", "anchor": "fasta2a.schema.ListTaskPushNotificationConfigRequest", "heading_level": 3, "md_text": "A JSON RPC request to list task push notification configs.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.ListTaskPushNotificationConfigRequest", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "DeleteTaskPushNotificationConfigRequest module-attribute", "anchor": "fasta2a.schema.DeleteTaskPushNotificationConfigRequest", "heading_level": 3, "md_text": "A JSON RPC request to delete a task push notification config.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.DeleteTaskPushNotificationConfigRequest", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "A2ARequest module-attribute", "anchor": "fasta2a.schema.A2ARequest", "heading_level": 3, "md_text": "A JSON RPC request to the A2A server.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.A2ARequest", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "A2AResponse module-attribute", "anchor": "fasta2a.schema.A2AResponse", "heading_level": 3, "md_text": "A JSON RPC response from the A2A server.", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.schema.A2AResponse", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "A2AClient", "anchor": "fasta2a.client.A2AClient", "heading_level": 3, "md_text": "A client for the A2A protocol. Source code in .venv/lib/python3.12/site-packages/fasta2a/client.py #### send\\_message async Send a message using the A2A protocol. Returns a JSON-RPC response containing either a result (Task) or an error. Source code in .venv/lib/python3.12/site-packages/fasta2a/client.py", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.client.A2AClient", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "UnexpectedResponseError", "anchor": "fasta2a.client.UnexpectedResponseError", "heading_level": 3, "md_text": "Bases: Exception An error raised when an unexpected response is received from the server. Source code in .venv/lib/python3.12/site-packages/fasta2a/client.py", "url": "https://ai.pydantic.dev/api/fasta2a/index.html#fasta2a.client.UnexpectedResponseError", "page": "api/fasta2a/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.models", "anchor": "pydantic_aimodels", "heading_level": 1, "md_text": "Logic related to making requests to an LLM. The aim here is to make a common interface for different LLMs, so that the rest of the code can be agnostic to the specific LLM being used. ### KnownModelName module-attribute Known model names that can be used with the model parameter of [ Agent ](../../agent/index.html#pydantic_ai.agent.Agent). KnownModelName is provided as a concise way to specify a model. ### ModelRequestParameters dataclass Configuration for an agent's request to a model, specifically related to tools and output handling. Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py ### Model Bases: ABC Abstract class for a model. Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py #### \\_\\_init\\_\\_ Initialize the model with optional settings and profile. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py #### settings property Get the model settings. #### request abstractmethod async Make a request to the model. Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py #### count\\_tokens async Make a request to the model for counting tokens. Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py #### request\\_stream async Make a request to the model and return a streaming response. Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py #### customize\\_request\\_parameters Customize the request parameters for the model. This method can be overridden by subclasses to modify the request parameters before sending them to the model. In particular, this method can be used to make modifications to the generated tool JSON schemas if necessary for vendor/model-specific reasons. Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py #### prepare\\_request Prepare request inputs before they are passed to the provider. This merges the given model_settings with the model's own settings attribute and ensures customize_request_parameters is applied to the resolved [ ModelRequestParameters ](index.html#pydantic_ai.models.ModelRequestParameters). Subclasses can override this method if they need to customize the preparation flow further, but most implementations should simply call self.prepare_request(...) at the start of their request (and related) methods. Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py #### model\\_name abstractmethod property The model name. #### profile cached property The model profile. #### system abstractmethod property The model provider, ex: openai. Use to populate the gen_ai.system OpenTelemetry semantic convention attribute, so should use well-known values listed in https://opentelemetry.io/docs/specs/semconv/attributes-registry/gen-ai/#gen-ai-system when applicable. #### base\\_url property The base URL for the provider API, if available. ### StreamedResponse dataclass Bases: ABC Streamed response from an LLM when calling a tool. Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py #### \\_\\_aiter\\_\\_ Stream the response as an async iterable of [ ModelResponseStreamEvent ](../../messages/index.html#pydantic_ai.messages.ModelResponseStreamEvent)s. This proxies the _event_iterator() and emits all events, while also checking for matches on the result schema and emitting a [ FinalResultEvent ](../../messages/index.html#pydantic_ai.messages.FinalResultEvent) if/when the first match is found. Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py #### get Build a [ ModelResponse ](../../messages/index.html#pydantic_ai.messages.ModelResponse) from the data received from the stream so far. Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py #### usage Get the usage of the response so far. This will not be the final usage until the stream is exhausted. Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py #### model\\_name abstractmethod property Get the model name of the response. #### provider\\_name abstractmethod property Get the provider name. #### timestamp abstractmethod property Get the timestamp of the response. ### ALLOW\\_MODEL\\_REQUESTS module-attribute Whether to allow requests to models. This global setting allows you to disable request to most models, e.g. to make sure you don't accidentally make costly requests to a model during tests. The testing models [ TestModel ](../test/index.html#pydantic_ai.models.test.TestModel) and [ FunctionModel ](../function/index.html#pydantic_ai.models.function.FunctionModel) are no affected by this setting. ### check\\_allow\\_model\\_requests Check if model requests are allowed. If you're defining your own models that have costs or latency associated with their use, you should call this in [ Model.request ](index.html#pydantic_ai.models.Model.request) and [ Model.request_stream ](index.html#pydantic_ai.models.Model.request_stream). Raises: Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py ### override\\_allow\\_model\\_requests Context manager to temporarily override [ ALLOW_MODEL_REQUESTS ](index.html#pydantic_ai.models.ALLOW_MODEL_REQUESTS). Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py", "url": "https://ai.pydantic.dev/api/models/base/index.html#pydantic_aimodels", "page": "api/models/base/index.html", "source_site": "pydantic_ai"}
{"title": "KnownModelName module-attribute", "anchor": "pydantic_ai.models.KnownModelName", "heading_level": 3, "md_text": "Known model names that can be used with the model parameter of [ Agent ](../../agent/index.html#pydantic_ai.agent.Agent). KnownModelName is provided as a concise way to specify a model.", "url": "https://ai.pydantic.dev/api/models/base/index.html#pydantic_ai.models.KnownModelName", "page": "api/models/base/index.html", "source_site": "pydantic_ai"}
{"title": "ModelRequestParameters dataclass", "anchor": "pydantic_ai.models.ModelRequestParameters", "heading_level": 3, "md_text": "Configuration for an agent's request to a model, specifically related to tools and output handling. Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py", "url": "https://ai.pydantic.dev/api/models/base/index.html#pydantic_ai.models.ModelRequestParameters", "page": "api/models/base/index.html", "source_site": "pydantic_ai"}
{"title": "Model", "anchor": "pydantic_ai.models.Model", "heading_level": 3, "md_text": "Bases: ABC Abstract class for a model. Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py #### \\_\\_init\\_\\_ Initialize the model with optional settings and profile. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py #### settings property Get the model settings. #### request abstractmethod async Make a request to the model. Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py #### count\\_tokens async Make a request to the model for counting tokens. Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py #### request\\_stream async Make a request to the model and return a streaming response. Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py #### customize\\_request\\_parameters Customize the request parameters for the model. This method can be overridden by subclasses to modify the request parameters before sending them to the model. In particular, this method can be used to make modifications to the generated tool JSON schemas if necessary for vendor/model-specific reasons. Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py #### prepare\\_request Prepare request inputs before they are passed to the provider. This merges the given model_settings with the model's own settings attribute and ensures customize_request_parameters is applied to the resolved [ ModelRequestParameters ](index.html#pydantic_ai.models.ModelRequestParameters). Subclasses can override this method if they need to customize the preparation flow further, but most implementations should simply call self.prepare_request(...) at the start of their request (and related) methods. Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py #### model\\_name abstractmethod property The model name. #### profile cached property The model profile. #### system abstractmethod property The model provider, ex: openai. Use to populate the gen_ai.system OpenTelemetry semantic convention attribute, so should use well-known values listed in https://opentelemetry.io/docs/specs/semconv/attributes-registry/gen-ai/#gen-ai-system when applicable. #### base\\_url property The base URL for the provider API, if available.", "url": "https://ai.pydantic.dev/api/models/base/index.html#pydantic_ai.models.Model", "page": "api/models/base/index.html", "source_site": "pydantic_ai"}
{"title": "StreamedResponse dataclass", "anchor": "pydantic_ai.models.StreamedResponse", "heading_level": 3, "md_text": "Bases: ABC Streamed response from an LLM when calling a tool. Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py #### \\_\\_aiter\\_\\_ Stream the response as an async iterable of [ ModelResponseStreamEvent ](../../messages/index.html#pydantic_ai.messages.ModelResponseStreamEvent)s. This proxies the _event_iterator() and emits all events, while also checking for matches on the result schema and emitting a [ FinalResultEvent ](../../messages/index.html#pydantic_ai.messages.FinalResultEvent) if/when the first match is found. Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py #### get Build a [ ModelResponse ](../../messages/index.html#pydantic_ai.messages.ModelResponse) from the data received from the stream so far. Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py #### usage Get the usage of the response so far. This will not be the final usage until the stream is exhausted. Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py #### model\\_name abstractmethod property Get the model name of the response. #### provider\\_name abstractmethod property Get the provider name. #### timestamp abstractmethod property Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/models/base/index.html#pydantic_ai.models.StreamedResponse", "page": "api/models/base/index.html", "source_site": "pydantic_ai"}
{"title": "ALLOW_MODEL_REQUESTS module-attribute", "anchor": "pydantic_ai.models.ALLOW_MODEL_REQUESTS", "heading_level": 3, "md_text": "Whether to allow requests to models. This global setting allows you to disable request to most models, e.g. to make sure you don't accidentally make costly requests to a model during tests. The testing models [ TestModel ](../test/index.html#pydantic_ai.models.test.TestModel) and [ FunctionModel ](../function/index.html#pydantic_ai.models.function.FunctionModel) are no affected by this setting.", "url": "https://ai.pydantic.dev/api/models/base/index.html#pydantic_ai.models.ALLOW_MODEL_REQUESTS", "page": "api/models/base/index.html", "source_site": "pydantic_ai"}
{"title": "check_allow_model_requests", "anchor": "pydantic_ai.models.check_allow_model_requests", "heading_level": 3, "md_text": "Check if model requests are allowed. If you're defining your own models that have costs or latency associated with their use, you should call this in [ Model.request ](index.html#pydantic_ai.models.Model.request) and [ Model.request_stream ](index.html#pydantic_ai.models.Model.request_stream). Raises: Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py", "url": "https://ai.pydantic.dev/api/models/base/index.html#pydantic_ai.models.check_allow_model_requests", "page": "api/models/base/index.html", "source_site": "pydantic_ai"}
{"title": "override_allow_model_requests", "anchor": "pydantic_ai.models.override_allow_model_requests", "heading_level": 3, "md_text": "Context manager to temporarily override [ ALLOW_MODEL_REQUESTS ](index.html#pydantic_ai.models.ALLOW_MODEL_REQUESTS). Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/__init__.py", "url": "https://ai.pydantic.dev/api/models/base/index.html#pydantic_ai.models.override_allow_model_requests", "page": "api/models/base/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.models.cohere", "anchor": "pydantic_aimodelscohere", "heading_level": 1, "md_text": "## Setup For details on how to set up authentication with this model, see [model configuration for Cohere](../../../models/cohere/index.html). ### LatestCohereModelNames module-attribute Latest Cohere models. ### CohereModelName module-attribute Possible Cohere model names. Since Cohere supports a variety of date-stamped models, we explicitly list the latest models but allow any name in the type hints. See [Cohere's docs](https://docs.cohere.com/v2/docs/models) for a list of all available models. ### CohereModelSettings Bases: ModelSettings Settings used for a Cohere model request. Source code in pydantic_ai_slim/pydantic_ai/models/cohere.py ### CohereModel dataclass Bases: Model A model that uses the Cohere API. Internally, this uses the [Cohere Python client](https://github.com/cohere-ai/cohere-python) to interact with the API. Apart from __init__ , all methods are private or match those of the base class. Source code in pydantic_ai_slim/pydantic_ai/models/cohere.py #### \\_\\_init\\_\\_ Initialize an Cohere model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/cohere.py #### model\\_name property The model name. #### system property The model provider.", "url": "https://ai.pydantic.dev/api/models/cohere/index.html#pydantic_aimodelscohere", "page": "api/models/cohere/index.html", "source_site": "pydantic_ai"}
{"title": "Setup", "anchor": "setup", "heading_level": 2, "md_text": "For details on how to set up authentication with this model, see [model configuration for Cohere](../../../models/cohere/index.html). ### LatestCohereModelNames module-attribute Latest Cohere models. ### CohereModelName module-attribute Possible Cohere model names. Since Cohere supports a variety of date-stamped models, we explicitly list the latest models but allow any name in the type hints. See [Cohere's docs](https://docs.cohere.com/v2/docs/models) for a list of all available models. ### CohereModelSettings Bases: ModelSettings Settings used for a Cohere model request. Source code in pydantic_ai_slim/pydantic_ai/models/cohere.py ### CohereModel dataclass Bases: Model A model that uses the Cohere API. Internally, this uses the [Cohere Python client](https://github.com/cohere-ai/cohere-python) to interact with the API. Apart from __init__ , all methods are private or match those of the base class. Source code in pydantic_ai_slim/pydantic_ai/models/cohere.py #### \\_\\_init\\_\\_ Initialize an Cohere model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/cohere.py #### model\\_name property The model name. #### system property The model provider.", "url": "https://ai.pydantic.dev/api/models/cohere/index.html#setup", "page": "api/models/cohere/index.html", "source_site": "pydantic_ai"}
{"title": "LatestCohereModelNames module-attribute", "anchor": "pydantic_ai.models.cohere.LatestCohereModelNames", "heading_level": 3, "md_text": "Latest Cohere models.", "url": "https://ai.pydantic.dev/api/models/cohere/index.html#pydantic_ai.models.cohere.LatestCohereModelNames", "page": "api/models/cohere/index.html", "source_site": "pydantic_ai"}
{"title": "CohereModelName module-attribute", "anchor": "pydantic_ai.models.cohere.CohereModelName", "heading_level": 3, "md_text": "Possible Cohere model names. Since Cohere supports a variety of date-stamped models, we explicitly list the latest models but allow any name in the type hints. See [Cohere's docs](https://docs.cohere.com/v2/docs/models) for a list of all available models.", "url": "https://ai.pydantic.dev/api/models/cohere/index.html#pydantic_ai.models.cohere.CohereModelName", "page": "api/models/cohere/index.html", "source_site": "pydantic_ai"}
{"title": "CohereModelSettings", "anchor": "pydantic_ai.models.cohere.CohereModelSettings", "heading_level": 3, "md_text": "Bases: ModelSettings Settings used for a Cohere model request. Source code in pydantic_ai_slim/pydantic_ai/models/cohere.py", "url": "https://ai.pydantic.dev/api/models/cohere/index.html#pydantic_ai.models.cohere.CohereModelSettings", "page": "api/models/cohere/index.html", "source_site": "pydantic_ai"}
{"title": "CohereModel dataclass", "anchor": "pydantic_ai.models.cohere.CohereModel", "heading_level": 3, "md_text": "Bases: Model A model that uses the Cohere API. Internally, this uses the [Cohere Python client](https://github.com/cohere-ai/cohere-python) to interact with the API. Apart from __init__ , all methods are private or match those of the base class. Source code in pydantic_ai_slim/pydantic_ai/models/cohere.py #### \\_\\_init\\_\\_ Initialize an Cohere model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/cohere.py #### model\\_name property The model name. #### system property The model provider.", "url": "https://ai.pydantic.dev/api/models/cohere/index.html#pydantic_ai.models.cohere.CohereModel", "page": "api/models/cohere/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.models.fallback", "anchor": "pydantic_aimodelsfallback", "heading_level": 1, "md_text": "### FallbackModel dataclass Bases: Model A model that uses one or more fallback models upon failure. Apart from __init__ , all methods are private or match those of the base class. Source code in pydantic_ai_slim/pydantic_ai/models/fallback.py #### \\_\\_init\\_\\_ Initialize a fallback model instance. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/fallback.py #### model\\_name property The model name. #### request async Try each model in sequence until one succeeds. In case of failure, raise a FallbackExceptionGroup with all exceptions. Source code in pydantic_ai_slim/pydantic_ai/models/fallback.py #### request\\_stream async Try each model in sequence until one succeeds. Source code in pydantic_ai_slim/pydantic_ai/models/fallback.py", "url": "https://ai.pydantic.dev/api/models/fallback/index.html#pydantic_aimodelsfallback", "page": "api/models/fallback/index.html", "source_site": "pydantic_ai"}
{"title": "FallbackModel dataclass", "anchor": "pydantic_ai.models.fallback.FallbackModel", "heading_level": 3, "md_text": "Bases: Model A model that uses one or more fallback models upon failure. Apart from __init__ , all methods are private or match those of the base class. Source code in pydantic_ai_slim/pydantic_ai/models/fallback.py #### \\_\\_init\\_\\_ Initialize a fallback model instance. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/fallback.py #### model\\_name property The model name. #### request async Try each model in sequence until one succeeds. In case of failure, raise a FallbackExceptionGroup with all exceptions. Source code in pydantic_ai_slim/pydantic_ai/models/fallback.py #### request\\_stream async Try each model in sequence until one succeeds. Source code in pydantic_ai_slim/pydantic_ai/models/fallback.py", "url": "https://ai.pydantic.dev/api/models/fallback/index.html#pydantic_ai.models.fallback.FallbackModel", "page": "api/models/fallback/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.models.bedrock", "anchor": "pydantic_aimodelsbedrock", "heading_level": 1, "md_text": "## Setup For details on how to set up authentication with this model, see [model configuration for Bedrock](../../../models/bedrock/index.html). ### LatestBedrockModelNames module-attribute Latest Bedrock models. ### BedrockModelName module-attribute Possible Bedrock model names. Since Bedrock supports a variety of date-stamped models, we explicitly list the latest models but allow any name in the type hints. See [the Bedrock docs](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html) for a full list. ### BedrockModelSettings Bases: ModelSettings Settings for Bedrock models. See [the Bedrock Converse API docs](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html#API_runtime_Converse_RequestSyntax) for a full list. See [the boto3 implementation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html) of the Bedrock Converse API. Source code in pydantic_ai_slim/pydantic_ai/models/bedrock.py #### bedrock\\_guardrail\\_config instance-attribute Content moderation and safety settings for Bedrock API requests. See more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_GuardrailConfiguration.html>. #### bedrock\\_performance\\_configuration instance-attribute Performance optimization settings for model inference. See more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_PerformanceConfiguration.html>. #### bedrock\\_request\\_metadata instance-attribute Additional metadata to attach to Bedrock API requests. See more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html#API_runtime_Converse_RequestSyntax>. #### bedrock\\_additional\\_model\\_response\\_fields\\_paths instance-attribute JSON paths to extract additional fields from model responses. See more about it on <https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html>. #### bedrock\\_prompt\\_variables instance-attribute Variables for substitution into prompt templates. See more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_PromptVariableValues.html>. #### bedrock\\_additional\\_model\\_requests\\_fields instance-attribute Additional model-specific parameters to include in requests. See more about it on <https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html>. ### BedrockConverseModel dataclass Bases: Model A model that uses the Bedrock Converse API. Source code in pydantic_ai_slim/pydantic_ai/models/bedrock.py #### \\_\\_init\\_\\_ Initialize a Bedrock model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/bedrock.py #### model\\_name property The model name. #### system property The model provider. ### BedrockStreamedResponse dataclass Bases: StreamedResponse Implementation of StreamedResponse for Bedrock models. Source code in pydantic_ai_slim/pydantic_ai/models/bedrock.py #### model\\_name property Get the model name of the response. #### provider\\_name property Get the provider name.", "url": "https://ai.pydantic.dev/api/models/bedrock/index.html#pydantic_aimodelsbedrock", "page": "api/models/bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "Setup", "anchor": "setup", "heading_level": 2, "md_text": "For details on how to set up authentication with this model, see [model configuration for Bedrock](../../../models/bedrock/index.html). ### LatestBedrockModelNames module-attribute Latest Bedrock models. ### BedrockModelName module-attribute Possible Bedrock model names. Since Bedrock supports a variety of date-stamped models, we explicitly list the latest models but allow any name in the type hints. See [the Bedrock docs](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html) for a full list. ### BedrockModelSettings Bases: ModelSettings Settings for Bedrock models. See [the Bedrock Converse API docs](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html#API_runtime_Converse_RequestSyntax) for a full list. See [the boto3 implementation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html) of the Bedrock Converse API. Source code in pydantic_ai_slim/pydantic_ai/models/bedrock.py #### bedrock\\_guardrail\\_config instance-attribute Content moderation and safety settings for Bedrock API requests. See more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_GuardrailConfiguration.html>. #### bedrock\\_performance\\_configuration instance-attribute Performance optimization settings for model inference. See more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_PerformanceConfiguration.html>. #### bedrock\\_request\\_metadata instance-attribute Additional metadata to attach to Bedrock API requests. See more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html#API_runtime_Converse_RequestSyntax>. #### bedrock\\_additional\\_model\\_response\\_fields\\_paths instance-attribute JSON paths to extract additional fields from model responses. See more about it on <https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html>. #### bedrock\\_prompt\\_variables instance-attribute Variables for substitution into prompt templates. See more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_PromptVariableValues.html>. #### bedrock\\_additional\\_model\\_requests\\_fields instance-attribute Additional model-specific parameters to include in requests. See more about it on <https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html>. ### BedrockConverseModel dataclass Bases: Model A model that uses the Bedrock Converse API. Source code in pydantic_ai_slim/pydantic_ai/models/bedrock.py #### \\_\\_init\\_\\_ Initialize a Bedrock model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/bedrock.py #### model\\_name property The model name. #### system property The model provider. ### BedrockStreamedResponse dataclass Bases: StreamedResponse Implementation of StreamedResponse for Bedrock models. Source code in pydantic_ai_slim/pydantic_ai/models/bedrock.py #### model\\_name property Get the model name of the response. #### provider\\_name property Get the provider name.", "url": "https://ai.pydantic.dev/api/models/bedrock/index.html#setup", "page": "api/models/bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "LatestBedrockModelNames module-attribute", "anchor": "pydantic_ai.models.bedrock.LatestBedrockModelNames", "heading_level": 3, "md_text": "Latest Bedrock models.", "url": "https://ai.pydantic.dev/api/models/bedrock/index.html#pydantic_ai.models.bedrock.LatestBedrockModelNames", "page": "api/models/bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "BedrockModelName module-attribute", "anchor": "pydantic_ai.models.bedrock.BedrockModelName", "heading_level": 3, "md_text": "Possible Bedrock model names. Since Bedrock supports a variety of date-stamped models, we explicitly list the latest models but allow any name in the type hints. See [the Bedrock docs](https://docs.aws.amazon.com/bedrock/latest/userguide/models-supported.html) for a full list.", "url": "https://ai.pydantic.dev/api/models/bedrock/index.html#pydantic_ai.models.bedrock.BedrockModelName", "page": "api/models/bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "BedrockModelSettings", "anchor": "pydantic_ai.models.bedrock.BedrockModelSettings", "heading_level": 3, "md_text": "Bases: ModelSettings Settings for Bedrock models. See [the Bedrock Converse API docs](https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html#API_runtime_Converse_RequestSyntax) for a full list. See [the boto3 implementation](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html) of the Bedrock Converse API. Source code in pydantic_ai_slim/pydantic_ai/models/bedrock.py #### bedrock\\_guardrail\\_config instance-attribute Content moderation and safety settings for Bedrock API requests. See more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_GuardrailConfiguration.html>. #### bedrock\\_performance\\_configuration instance-attribute Performance optimization settings for model inference. See more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_PerformanceConfiguration.html>. #### bedrock\\_request\\_metadata instance-attribute Additional metadata to attach to Bedrock API requests. See more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_Converse.html#API_runtime_Converse_RequestSyntax>. #### bedrock\\_additional\\_model\\_response\\_fields\\_paths instance-attribute JSON paths to extract additional fields from model responses. See more about it on <https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html>. #### bedrock\\_prompt\\_variables instance-attribute Variables for substitution into prompt templates. See more about it on <https://docs.aws.amazon.com/bedrock/latest/APIReference/API_runtime_PromptVariableValues.html>. #### bedrock\\_additional\\_model\\_requests\\_fields instance-attribute Additional model-specific parameters to include in requests. See more about it on <https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters.html>.", "url": "https://ai.pydantic.dev/api/models/bedrock/index.html#pydantic_ai.models.bedrock.BedrockModelSettings", "page": "api/models/bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "BedrockConverseModel dataclass", "anchor": "pydantic_ai.models.bedrock.BedrockConverseModel", "heading_level": 3, "md_text": "Bases: Model A model that uses the Bedrock Converse API. Source code in pydantic_ai_slim/pydantic_ai/models/bedrock.py #### \\_\\_init\\_\\_ Initialize a Bedrock model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/bedrock.py #### model\\_name property The model name. #### system property The model provider.", "url": "https://ai.pydantic.dev/api/models/bedrock/index.html#pydantic_ai.models.bedrock.BedrockConverseModel", "page": "api/models/bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "BedrockStreamedResponse dataclass", "anchor": "pydantic_ai.models.bedrock.BedrockStreamedResponse", "heading_level": 3, "md_text": "Bases: StreamedResponse Implementation of StreamedResponse for Bedrock models. Source code in pydantic_ai_slim/pydantic_ai/models/bedrock.py #### model\\_name property Get the model name of the response. #### provider\\_name property Get the provider name.", "url": "https://ai.pydantic.dev/api/models/bedrock/index.html#pydantic_ai.models.bedrock.BedrockStreamedResponse", "page": "api/models/bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.models.function", "anchor": "pydantic_aimodelsfunction", "heading_level": 1, "md_text": "A model controlled by a local function. [ FunctionModel ](index.html#pydantic_ai.models.function.FunctionModel) is similar to [ TestModel ](../test/index.html), but allows greater control over the model's behavior. Its primary use case is for more advanced unit testing than is possible with TestModel . Here's a minimal example: function\\_model\\_usage.py See [Unit testing with FunctionModel ](../../../testing/index.html#unit-testing-with-functionmodel) for detailed documentation. ### FunctionModel dataclass Bases: Model A model controlled by a local function. Apart from __init__ , all methods are private or match those of the base class. Source code in pydantic_ai_slim/pydantic_ai/models/function.py #### \\_\\_init\\_\\_ Initialize a FunctionModel . Either function or stream_function must be provided, providing both is allowed. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/function.py #### model\\_name property The model name. #### system property The system / model provider. ### AgentInfo dataclass Information about an agent. This is passed as the second to functions used within [ FunctionModel ](index.html#pydantic_ai.models.function.FunctionModel). Source code in pydantic_ai_slim/pydantic_ai/models/function.py #### function\\_tools instance-attribute The function tools available on this agent. These are the tools registered via the [ tool ](../../agent/index.html#pydantic_ai.agent.Agent.tool) and [ tool_plain ](../../agent/index.html#pydantic_ai.agent.Agent.tool_plain) decorators. #### allow\\_text\\_output instance-attribute Whether a plain text output is allowed. #### output\\_tools instance-attribute The tools that can called to produce the final output of the run. #### model\\_settings instance-attribute The model settings passed to the run call. ### DeltaToolCall dataclass Incremental change to a tool call. Used to describe a chunk when streaming structured responses. Source code in pydantic_ai_slim/pydantic_ai/models/function.py #### name class-attribute instance-attribute Incremental change to the name of the tool. #### json\\_args class-attribute instance-attribute Incremental change to the arguments as JSON #### tool\\_call\\_id class-attribute instance-attribute Incremental change to the tool call ID. ### DeltaThinkingPart dataclass Incremental change to a thinking part. Used to describe a chunk when streaming thinking responses. Source code in pydantic_ai_slim/pydantic_ai/models/function.py #### content class-attribute instance-attribute Incremental change to the thinking content. #### signature class-attribute instance-attribute Incremental change to the thinking signature. ### DeltaToolCalls module-attribute A mapping of tool call IDs to incremental changes. ### DeltaThinkingCalls module-attribute A mapping of thinking call IDs to incremental changes. ### FunctionDef module-attribute A function used to generate a non-streamed response. ### StreamFunctionDef module-attribute A function used to generate a streamed response. While this is defined as having return type of AsyncIterator[str DeltaToolCalls DeltaThinkingCalls BuiltinTools] , it should really be considered as AsyncIterator[str] AsyncIterator[DeltaToolCalls] AsyncIterator[DeltaThinkingCalls] , E.g. you need to yield all text, all DeltaToolCalls , all DeltaThinkingCalls , or all BuiltinToolCallsReturns , not mix them. ### FunctionStreamedResponse dataclass Bases: StreamedResponse Implementation of StreamedResponse for [FunctionModel](index.html#pydantic_ai.models.function.FunctionModel). Source code in pydantic_ai_slim/pydantic_ai/models/function.py #### model\\_name property Get the model name of the response. #### provider\\_name property Get the provider name. #### timestamp property Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/models/function/index.html#pydantic_aimodelsfunction", "page": "api/models/function/index.html", "source_site": "pydantic_ai"}
{"title": "FunctionModel dataclass", "anchor": "pydantic_ai.models.function.FunctionModel", "heading_level": 3, "md_text": "Bases: Model A model controlled by a local function. Apart from __init__ , all methods are private or match those of the base class. Source code in pydantic_ai_slim/pydantic_ai/models/function.py #### \\_\\_init\\_\\_ Initialize a FunctionModel . Either function or stream_function must be provided, providing both is allowed. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/function.py #### model\\_name property The model name. #### system property The system / model provider.", "url": "https://ai.pydantic.dev/api/models/function/index.html#pydantic_ai.models.function.FunctionModel", "page": "api/models/function/index.html", "source_site": "pydantic_ai"}
{"title": "AgentInfo dataclass", "anchor": "pydantic_ai.models.function.AgentInfo", "heading_level": 3, "md_text": "Information about an agent. This is passed as the second to functions used within [ FunctionModel ](index.html#pydantic_ai.models.function.FunctionModel). Source code in pydantic_ai_slim/pydantic_ai/models/function.py #### function\\_tools instance-attribute The function tools available on this agent. These are the tools registered via the [ tool ](../../agent/index.html#pydantic_ai.agent.Agent.tool) and [ tool_plain ](../../agent/index.html#pydantic_ai.agent.Agent.tool_plain) decorators. #### allow\\_text\\_output instance-attribute Whether a plain text output is allowed. #### output\\_tools instance-attribute The tools that can called to produce the final output of the run. #### model\\_settings instance-attribute The model settings passed to the run call.", "url": "https://ai.pydantic.dev/api/models/function/index.html#pydantic_ai.models.function.AgentInfo", "page": "api/models/function/index.html", "source_site": "pydantic_ai"}
{"title": "DeltaToolCall dataclass", "anchor": "pydantic_ai.models.function.DeltaToolCall", "heading_level": 3, "md_text": "Incremental change to a tool call. Used to describe a chunk when streaming structured responses. Source code in pydantic_ai_slim/pydantic_ai/models/function.py #### name class-attribute instance-attribute Incremental change to the name of the tool. #### json\\_args class-attribute instance-attribute Incremental change to the arguments as JSON #### tool\\_call\\_id class-attribute instance-attribute Incremental change to the tool call ID.", "url": "https://ai.pydantic.dev/api/models/function/index.html#pydantic_ai.models.function.DeltaToolCall", "page": "api/models/function/index.html", "source_site": "pydantic_ai"}
{"title": "DeltaThinkingPart dataclass", "anchor": "pydantic_ai.models.function.DeltaThinkingPart", "heading_level": 3, "md_text": "Incremental change to a thinking part. Used to describe a chunk when streaming thinking responses. Source code in pydantic_ai_slim/pydantic_ai/models/function.py #### content class-attribute instance-attribute Incremental change to the thinking content. #### signature class-attribute instance-attribute Incremental change to the thinking signature.", "url": "https://ai.pydantic.dev/api/models/function/index.html#pydantic_ai.models.function.DeltaThinkingPart", "page": "api/models/function/index.html", "source_site": "pydantic_ai"}
{"title": "DeltaToolCalls module-attribute", "anchor": "pydantic_ai.models.function.DeltaToolCalls", "heading_level": 3, "md_text": "A mapping of tool call IDs to incremental changes.", "url": "https://ai.pydantic.dev/api/models/function/index.html#pydantic_ai.models.function.DeltaToolCalls", "page": "api/models/function/index.html", "source_site": "pydantic_ai"}
{"title": "DeltaThinkingCalls module-attribute", "anchor": "pydantic_ai.models.function.DeltaThinkingCalls", "heading_level": 3, "md_text": "A mapping of thinking call IDs to incremental changes.", "url": "https://ai.pydantic.dev/api/models/function/index.html#pydantic_ai.models.function.DeltaThinkingCalls", "page": "api/models/function/index.html", "source_site": "pydantic_ai"}
{"title": "FunctionDef module-attribute", "anchor": "pydantic_ai.models.function.FunctionDef", "heading_level": 3, "md_text": "A function used to generate a non-streamed response.", "url": "https://ai.pydantic.dev/api/models/function/index.html#pydantic_ai.models.function.FunctionDef", "page": "api/models/function/index.html", "source_site": "pydantic_ai"}
{"title": "StreamFunctionDef module-attribute", "anchor": "pydantic_ai.models.function.StreamFunctionDef", "heading_level": 3, "md_text": "A function used to generate a streamed response. While this is defined as having return type of AsyncIterator[str DeltaToolCalls DeltaThinkingCalls BuiltinTools] , it should really be considered as AsyncIterator[str] AsyncIterator[DeltaToolCalls] AsyncIterator[DeltaThinkingCalls] , E.g. you need to yield all text, all DeltaToolCalls , all DeltaThinkingCalls , or all BuiltinToolCallsReturns , not mix them.", "url": "https://ai.pydantic.dev/api/models/function/index.html#pydantic_ai.models.function.StreamFunctionDef", "page": "api/models/function/index.html", "source_site": "pydantic_ai"}
{"title": "FunctionStreamedResponse dataclass", "anchor": "pydantic_ai.models.function.FunctionStreamedResponse", "heading_level": 3, "md_text": "Bases: StreamedResponse Implementation of StreamedResponse for [FunctionModel](index.html#pydantic_ai.models.function.FunctionModel). Source code in pydantic_ai_slim/pydantic_ai/models/function.py #### model\\_name property Get the model name of the response. #### provider\\_name property Get the provider name. #### timestamp property Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/models/function/index.html#pydantic_ai.models.function.FunctionStreamedResponse", "page": "api/models/function/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.messages", "anchor": "pydantic_aimessages", "heading_level": 1, "md_text": "The structure of [ ModelMessage ](index.html#pydantic_ai.messages.ModelMessage) can be shown as a graph: ### FinishReason module-attribute Reason the model finished generating the response, normalized to OpenTelemetry values. ### SystemPromptPart dataclass A system prompt, generally written by the application developer. This gives the model context and guidance on how to respond. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### content instance-attribute The content of the prompt. #### timestamp class-attribute instance-attribute The timestamp of the prompt. #### dynamic\\_ref class-attribute instance-attribute The ref of the dynamic system prompt function that generated this part. Only set if system prompt is dynamic, see [ system_prompt ](../agent/index.html#pydantic_ai.agent.Agent.system_prompt) for more information. #### part\\_kind class-attribute instance-attribute Part type identifier, this is available on all parts as a discriminator. ### FileUrl dataclass Bases: ABC Abstract base class for any URL-based file. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### url instance-attribute The URL of the file. #### force\\_download class-attribute instance-attribute For OpenAI and Google APIs it: * If True, the file is downloaded and the data is sent to the model as bytes. * If False, the URL is sent directly to the model and no download is performed. #### vendor\\_metadata class-attribute instance-attribute Vendor-specific metadata for the file. Supported by: - GoogleModel : VideoUrl.vendor_metadata is used as video_metadata : https://ai.google.dev/gemini-api/docs/video-understanding#customize-video-processing - OpenAIChatModel , OpenAIResponsesModel : ImageUrl.vendor_metadata['detail'] is used as detail setting for images #### media\\_type property Return the media type of the file, based on the URL or the provided media_type . #### identifier property The identifier of the file, such as a unique ID. This identifier can be provided to the model in a message to allow it to refer to this file in a tool call argument, and the tool can look up the file in question by iterating over the message history and finding the matching FileUrl . This identifier is only automatically passed to the model when the FileUrl is returned by a tool. If you're passing the FileUrl as a user message, it's up to you to include a separate text part with the identifier, e.g. \"This is file :\" preceding the FileUrl . It's also included in inline-text delimiters for providers that require inlining text documents, so the model can distinguish multiple files. #### format abstractmethod property The file format. ### VideoUrl dataclass Bases: FileUrl A URL to a video. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### url instance-attribute The URL of the video. #### kind class-attribute instance-attribute Type identifier, this is available on all parts as a discriminator. #### is\\_youtube property True if the URL has a YouTube domain. #### format property The file format of the video. The choice of supported formats were based on the Bedrock Converse API. Other APIs don't require to use a format. ### AudioUrl dataclass Bases: FileUrl A URL to an audio file. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### url instance-attribute The URL of the audio file. #### kind class-attribute instance-attribute Type identifier, this is available on all parts as a discriminator. #### format property The file format of the audio file. ### ImageUrl dataclass Bases: FileUrl A URL to an image. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### url instance-attribute The URL of the image. #### kind class-attribute instance-attribute Type identifier, this is available on all parts as a discriminator. #### format property The file format of the image. The choice of supported formats were based on the Bedrock Converse API. Other APIs don't require to use a format. ### DocumentUrl dataclass Bases: FileUrl The URL of the document. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### url instance-attribute The URL of the document. #### kind class-attribute instance-attribute Type identifier, this is available on all parts as a discriminator. #### format property The file format of the document. The choice of supported formats were based on the Bedrock Converse API. Other APIs don't require to use a format. ### BinaryContent dataclass Binary content, e.g. an audio or image file. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### data instance-attribute The binary data. #### media\\_type instance-attribute The media type of the binary data. #### vendor\\_metadata class-attribute instance-attribute Vendor-specific metadata for the file. Supported by: - GoogleModel : BinaryContent.vendor_metadata is used as video_metadata : https://ai.google.dev/gemini-api/docs/video-understanding#customize-video-processing - OpenAIChatModel , OpenAIResponsesModel : BinaryContent.vendor_metadata['detail'] is used as detail setting for images #### kind class-attribute instance-attribute Type identifier, this is available on all parts as a discriminator. #### narrow\\_type staticmethod Narrow the type of the BinaryContent to BinaryImage if it's an image. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### from\\_data\\_uri classmethod Create a BinaryContent from a data URI. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### identifier property Identifier for the binary content, such as a unique ID. This identifier can be provided to the model in a message to allow it to refer to this file in a tool call argument, and the tool can look up the file in question by iterating over the message history and finding the matching BinaryContent . This identifier is only automatically passed to the model when the BinaryContent is returned by a tool. If you're passing the BinaryContent as a user message, it's up to you to include a separate text part with the identifier, e.g. \"This is file :\" preceding the BinaryContent . It's also included in inline-text delimiters for providers that require inlining text documents, so the model can distinguish multiple files. #### data\\_uri property Convert the BinaryContent to a data URI. #### is\\_audio property Return True if the media type is an audio type. #### is\\_image property Return True if the media type is an image type. #### is\\_video property Return True if the media type is a video type. #### is\\_document property Return True if the media type is a document type. #### format property The file format of the binary content. ### BinaryImage Bases: BinaryContent Binary content that's guaranteed to be an image. Source code in pydantic_ai_slim/pydantic_ai/messages.py ### ToolReturn dataclass A structured return value for tools that need to provide both a return value and custom content to the model. This class allows tools to return complex responses that include: - A return value for actual tool return - Custom content (including multi-modal content) to be sent to the model as a UserPromptPart - Optional metadata for application use Source code in pydantic_ai_slim/pydantic_ai/messages.py #### return\\_value instance-attribute The return value to be used in the tool response. #### content class-attribute instance-attribute The content to be sent to the model as a UserPromptPart. #### metadata class-attribute instance-attribute Additional data that can be accessed programmatically by the application but is not sent to the LLM. ### UserPromptPart dataclass A user prompt, generally written by the end user. Content comes from the user_prompt parameter of [ Agent.run ](../agent/index.html#pydantic_ai.agent.AbstractAgent.run), [ Agent.run_sync ](../agent/index.html#pydantic_ai.agent.AbstractAgent.run_sync), and [ Agent.run_stream ](../agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream). Source code in pydantic_ai_slim/pydantic_ai/messages.py #### content instance-attribute The content of the prompt. #### timestamp class-attribute instance-attribute The timestamp of the prompt. #### part\\_kind class-attribute instance-attribute Part type identifier, this is available on all parts as a discriminator. ### BaseToolReturnPart dataclass Base class for tool return parts. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### tool\\_name instance-attribute The name of the \"tool\" was called. #### content instance-attribute The return value. #### tool\\_call\\_id class-attribute instance-attribute The tool call identifier, this is used by some models including OpenAI. In case the tool call id is not provided by the model, Pydantic AI will generate a random one. #### metadata class-attribute instance-attribute Additional data that can be accessed programmatically by the application but is not sent to the LLM. #### timestamp class-attribute instance-attribute The timestamp, when the tool returned. #### model\\_response\\_str Return a string representation of the content for the model. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### model\\_response\\_object Return a dictionary representation of the content, wrapping non-dict types appropriately. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### has\\_content Return True if the tool return has content. Source code in pydantic_ai_slim/pydantic_ai/messages.py ### ToolReturnPart dataclass Bases: BaseToolReturnPart A tool return message, this encodes the result of running a tool. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### part\\_kind class-attribute instance-attribute Part type identifier, this is available on all parts as a discriminator. ### BuiltinToolReturnPart dataclass Bases: BaseToolReturnPart A tool return message from a built-in tool. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### provider\\_name class-attribute instance-attribute The name of the provider that generated the response. #### part\\_kind class-attribute instance-attribute Part type identifier, this is available on all parts as a discriminator. ### RetryPromptPart dataclass A message back to a model asking it to try again. This can be sent for a number of reasons: * Pydantic validation of tool arguments failed, here content is derived from a Pydantic [ ValidationError ](https://docs.pydantic.dev/latest/api/pydantic_core/#pydantic_core.ValidationError) * a tool raised a [ ModelRetry ](../exceptions/index.html#pydantic_ai.exceptions.ModelRetry) exception * no tool was found for the tool name * the model returned plain text when a structured response was expected * Pydantic validation of a structured response failed, here content is derived from a Pydantic [ ValidationError ](https://docs.pydantic.dev/latest/api/pydantic_core/#pydantic_core.ValidationError) * an output validator raised a [ ModelRetry ](../exceptions/index.html#pydantic_ai.exceptions.ModelRetry) exception Source code in pydantic_ai_slim/pydantic_ai/messages.py #### content instance-attribute Details of why and how the model should retry. If the retry was triggered by a [ ValidationError ](https://docs.pydantic.dev/latest/api/pydantic_core/#pydantic_core.ValidationError), this will be a list of error details. #### tool\\_name class-attribute instance-attribute The name of the tool that was called, if any. #### tool\\_call\\_id class-attribute instance-attribute The tool call identifier, this is used by some models including OpenAI. In case the tool call id is not provided by the model, Pydantic AI will generate a random one. #### timestamp class-attribute instance-attribute The timestamp, when the retry was triggered. #### part\\_kind class-attribute instance-attribute Part type identifier, this is available on all parts as a discriminator. #### model\\_response Return a string message describing why the retry is requested. Source code in pydantic_ai_slim/pydantic_ai/messages.py ### ModelRequestPart module-attribute A message part sent by Pydantic AI to a model. ### ModelRequest dataclass A request generated by Pydantic AI and sent to a model, e.g. a message from the Pydantic AI app to the model. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### parts instance-attribute The parts of the user message. #### instructions class-attribute instance-attribute The instructions for the model. #### kind class-attribute instance-attribute Message type identifier, this is available on all parts as a discriminator. #### user\\_text\\_prompt classmethod Create a ModelRequest with a single user prompt as text. Source code in pydantic_ai_slim/pydantic_ai/messages.py ### TextPart dataclass A plain text response from a model. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### content instance-attribute The text content of the response. #### id class-attribute instance-attribute An optional identifier of the text part. #### part\\_kind class-attribute instance-attribute Part type identifier, this is available on all parts as a discriminator. #### has\\_content Return True if the text content is non-empty. Source code in pydantic_ai_slim/pydantic_ai/messages.py ### ThinkingPart dataclass A thinking response from a model. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### content instance-attribute The thinking content of the response. #### id class-attribute instance-attribute The identifier of the thinking part. #### signature class-attribute instance-attribute The signature of the thinking. Supported by: * Anthropic (corresponds to the signature field) * Bedrock (corresponds to the signature field) * Google (corresponds to the thought_signature field) * OpenAI (corresponds to the encrypted_content field) #### provider\\_name class-attribute instance-attribute The name of the provider that generated the response. Signatures are only sent back to the same provider. #### part\\_kind class-attribute instance-attribute Part type identifier, this is available on all parts as a discriminator. #### has\\_content Return True if the thinking content is non-empty. Source code in pydantic_ai_slim/pydantic_ai/messages.py ### FilePart dataclass A file response from a model. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### content instance-attribute The file content of the response. #### id class-attribute instance-attribute The identifier of the file part. #### provider\\_name class-attribute instance-attribute The name of the provider that generated the response. #### part\\_kind class-attribute instance-attribute Part type identifier, this is available on all parts as a discriminator. #### has\\_content Return True if the file content is non-empty. Source code in pydantic_ai_slim/pydantic_ai/messages.py ### BaseToolCallPart dataclass A tool call from a model. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### tool\\_name instance-attribute The name of the tool to call. #### args class-attribute instance-attribute The arguments to pass to the tool. This is stored either as a JSON string or a Python dictionary depending on how data was received. #### tool\\_call\\_id class-attribute instance-attribute The tool call identifier, this is used by some models including OpenAI. In case the tool call id is not provided by the model, Pydantic AI will generate a random one. #### id class-attribute instance-attribute An optional identifier of the tool call part, separate from the tool call ID. This is used by some APIs like OpenAI Responses. #### args\\_as\\_dict Return the arguments as a Python dictionary. This is just for convenience with models that require dicts as input. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### args\\_as\\_json\\_str Return the arguments as a JSON string. This is just for convenience with models that require JSON strings as input. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### has\\_content Return True if the arguments contain any data. Source code in pydantic_ai_slim/pydantic_ai/messages.py ### ToolCallPart dataclass Bases: BaseToolCallPart A tool call from a model. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### part\\_kind class-attribute instance-attribute Part type identifier, this is available on all parts as a discriminator. ### BuiltinToolCallPart dataclass Bases: BaseToolCallPart A tool call to a built-in tool. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### provider\\_name class-attribute instance-attribute The name of the provider that generated the response. Built-in tool calls are only sent back to the same provider. #### part\\_kind class-attribute instance-attribute Part type identifier, this is available on all parts as a discriminator. ### ModelResponsePart module-attribute A message part returned by a model. ### ModelResponse dataclass A response from a model, e.g. a message from the model to the Pydantic AI app. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### parts instance-attribute The parts of the model message. #### usage class-attribute instance-attribute Usage information for the request. This has a default to make tests easier, and to support loading old messages where usage will be missing. #### model\\_name class-attribute instance-attribute The name of the model that generated the response. #### timestamp class-attribute instance-attribute The timestamp of the response. If the model provides a timestamp in the response (as OpenAI does) that will be used. #### kind class-attribute instance-attribute Message type identifier, this is available on all parts as a discriminator. #### provider\\_name class-attribute instance-attribute The name of the LLM provider that generated the response. #### provider\\_details class-attribute instance-attribute Additional provider-specific details in a serializable format. This allows storing selected vendor-specific data that isn't mapped to standard ModelResponse fields. For OpenAI models, this may include 'logprobs', 'finish\\_reason', etc. #### provider\\_response\\_id class-attribute instance-attribute request ID as specified by the model provider. This can be used to track the specific request to the model. #### finish\\_reason class-attribute instance-attribute Reason the model finished generating the response, normalized to OpenTelemetry values. #### text property Get the text in the response. #### thinking property Get the thinking in the response. #### files property Get the files in the response. #### images property Get the images in the response. #### tool\\_calls property Get the tool calls in the response. #### builtin\\_tool\\_calls property Get the builtin tool calls and results in the response. #### price deprecated Deprecated price is deprecated, use cost instead Source code in pydantic_ai_slim/pydantic_ai/messages.py #### cost Calculate the cost of the usage. Uses [ genai-prices ](https://github.com/pydantic/genai-prices). Source code in pydantic_ai_slim/pydantic_ai/messages.py #### otel\\_events Return OpenTelemetry events for the response. Source code in pydantic_ai_slim/pydantic_ai/messages.py ### ModelMessage module-attribute Any message sent to or returned by a model. ### ModelMessagesTypeAdapter module-attribute Pydantic [ TypeAdapter ](https://docs.pydantic.dev/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter) for (de)serializing messages. ### TextPartDelta dataclass A partial update (delta) for a TextPart to append new text content. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### content\\_delta instance-attribute The incremental text content to add to the existing TextPart content. #### part\\_delta\\_kind class-attribute instance-attribute Part delta type identifier, used as a discriminator. #### apply Apply this text delta to an existing TextPart . Parameters: Returns: Raises: Source code in pydantic_ai_slim/pydantic_ai/messages.py ### ThinkingPartDelta dataclass A partial update (delta) for a ThinkingPart to append new thinking content. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### content\\_delta class-attribute instance-attribute The incremental thinking content to add to the existing ThinkingPart content. #### signature\\_delta class-attribute instance-attribute Optional signature delta. Note this is never treated as a delta  it can replace None. #### provider\\_name class-attribute instance-attribute Optional provider name for the thinking part. Signatures are only sent back to the same provider. #### part\\_delta\\_kind class-attribute instance-attribute Part delta type identifier, used as a discriminator. #### apply Apply this thinking delta to an existing ThinkingPart . Parameters: Returns: Raises: Source code in pydantic_ai_slim/pydantic_ai/messages.py ### ToolCallPartDelta dataclass A partial update (delta) for a ToolCallPart to modify tool name, arguments, or tool call ID. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### tool\\_name\\_delta class-attribute instance-attribute Incremental text to add to the existing tool name, if any. #### args\\_delta class-attribute instance-attribute Incremental data to add to the tool arguments. If this is a string, it will be appended to existing JSON arguments. If this is a dict, it will be merged with existing dict arguments. #### tool\\_call\\_id class-attribute instance-attribute Optional tool call identifier, this is used by some models including OpenAI. Note this is never treated as a delta  it can replace None, but otherwise if a non-matching value is provided an error will be raised. #### part\\_delta\\_kind class-attribute instance-attribute Part delta type identifier, used as a discriminator. #### as\\_part Convert this delta to a fully formed ToolCallPart if possible, otherwise return None . Returns: Source code in pydantic_ai_slim/pydantic_ai/messages.py #### apply Apply this delta to a part or delta, returning a new part or delta with the changes applied. Parameters: Returns: Raises: Source code in pydantic_ai_slim/pydantic_ai/messages.py ### ModelResponsePartDelta module-attribute A partial update (delta) for any model response part. ### PartStartEvent dataclass An event indicating that a new part has started. If multiple PartStartEvent s are received with the same index, the new one should fully replace the old one. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### index instance-attribute The index of the part within the overall response parts list. #### part instance-attribute The newly started ModelResponsePart . #### event\\_kind class-attribute instance-attribute Event type identifier, used as a discriminator. ### PartDeltaEvent dataclass An event indicating a delta update for an existing part. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### index instance-attribute The index of the part within the overall response parts list. #### delta instance-attribute The delta to apply to the specified part. #### event\\_kind class-attribute instance-attribute Event type identifier, used as a discriminator. ### FinalResultEvent dataclass An event indicating the response to the current model request matches the output schema and will produce a result. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### tool\\_name instance-attribute The name of the output tool that was called. None if the result is from text content and not from a tool. #### tool\\_call\\_id instance-attribute The tool call ID, if any, that this result is associated with. #### event\\_kind class-attribute instance-attribute Event type identifier, used as a discriminator. ### ModelResponseStreamEvent module-attribute An event in the model response stream, starting a new part, applying a delta to an existing one, or indicating the final result. ### FunctionToolCallEvent dataclass An event indicating the start to a call to a function tool. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### part instance-attribute The (function) tool call to make. #### event\\_kind class-attribute instance-attribute Event type identifier, used as a discriminator. #### tool\\_call\\_id property An ID used for matching details about the call to its result. #### call\\_id property An ID used for matching details about the call to its result. ### FunctionToolResultEvent dataclass An event indicating the result of a function tool call. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### result instance-attribute The result of the call to the function tool. #### content class-attribute instance-attribute The content that will be sent to the model as a UserPromptPart following the result. #### event\\_kind class-attribute instance-attribute Event type identifier, used as a discriminator. #### tool\\_call\\_id property An ID used to match the result to its original call. ### BuiltinToolCallEvent dataclass deprecated Deprecated BuiltinToolCallEvent is deprecated, look for PartStartEvent and PartDeltaEvent with BuiltinToolCallPart instead. An event indicating the start to a call to a built-in tool. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### part instance-attribute The built-in tool call to make. #### event\\_kind class-attribute instance-attribute Event type identifier, used as a discriminator. ### BuiltinToolResultEvent dataclass deprecated Deprecated BuiltinToolResultEvent is deprecated, look for PartStartEvent and PartDeltaEvent with BuiltinToolReturnPart instead. An event indicating the result of a built-in tool call. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### result instance-attribute The result of the call to the built-in tool. #### event\\_kind class-attribute instance-attribute Event type identifier, used as a discriminator. ### HandleResponseEvent module-attribute An event yielded when handling a model response, indicating tool calls and results. ### AgentStreamEvent module-attribute An event in the agent stream: model response stream events and response-handling events.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_aimessages", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "FinishReason module-attribute", "anchor": "pydantic_ai.messages.FinishReason", "heading_level": 3, "md_text": "Reason the model finished generating the response, normalized to OpenTelemetry values.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.FinishReason", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "SystemPromptPart dataclass", "anchor": "pydantic_ai.messages.SystemPromptPart", "heading_level": 3, "md_text": "A system prompt, generally written by the application developer. This gives the model context and guidance on how to respond. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### content instance-attribute The content of the prompt. #### timestamp class-attribute instance-attribute The timestamp of the prompt. #### dynamic\\_ref class-attribute instance-attribute The ref of the dynamic system prompt function that generated this part. Only set if system prompt is dynamic, see [ system_prompt ](../agent/index.html#pydantic_ai.agent.Agent.system_prompt) for more information. #### part\\_kind class-attribute instance-attribute Part type identifier, this is available on all parts as a discriminator.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.SystemPromptPart", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "FileUrl dataclass", "anchor": "pydantic_ai.messages.FileUrl", "heading_level": 3, "md_text": "Bases: ABC Abstract base class for any URL-based file. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### url instance-attribute The URL of the file. #### force\\_download class-attribute instance-attribute For OpenAI and Google APIs it: * If True, the file is downloaded and the data is sent to the model as bytes. * If False, the URL is sent directly to the model and no download is performed. #### vendor\\_metadata class-attribute instance-attribute Vendor-specific metadata for the file. Supported by: - GoogleModel : VideoUrl.vendor_metadata is used as video_metadata : https://ai.google.dev/gemini-api/docs/video-understanding#customize-video-processing - OpenAIChatModel , OpenAIResponsesModel : ImageUrl.vendor_metadata['detail'] is used as detail setting for images #### media\\_type property Return the media type of the file, based on the URL or the provided media_type . #### identifier property The identifier of the file, such as a unique ID. This identifier can be provided to the model in a message to allow it to refer to this file in a tool call argument, and the tool can look up the file in question by iterating over the message history and finding the matching FileUrl . This identifier is only automatically passed to the model when the FileUrl is returned by a tool. If you're passing the FileUrl as a user message, it's up to you to include a separate text part with the identifier, e.g. \"This is file :\" preceding the FileUrl . It's also included in inline-text delimiters for providers that require inlining text documents, so the model can distinguish multiple files. #### format abstractmethod property The file format.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.FileUrl", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "VideoUrl dataclass", "anchor": "pydantic_ai.messages.VideoUrl", "heading_level": 3, "md_text": "Bases: FileUrl A URL to a video. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### url instance-attribute The URL of the video. #### kind class-attribute instance-attribute Type identifier, this is available on all parts as a discriminator. #### is\\_youtube property True if the URL has a YouTube domain. #### format property The file format of the video. The choice of supported formats were based on the Bedrock Converse API. Other APIs don't require to use a format.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.VideoUrl", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "AudioUrl dataclass", "anchor": "pydantic_ai.messages.AudioUrl", "heading_level": 3, "md_text": "Bases: FileUrl A URL to an audio file. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### url instance-attribute The URL of the audio file. #### kind class-attribute instance-attribute Type identifier, this is available on all parts as a discriminator. #### format property The file format of the audio file.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.AudioUrl", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "ImageUrl dataclass", "anchor": "pydantic_ai.messages.ImageUrl", "heading_level": 3, "md_text": "Bases: FileUrl A URL to an image. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### url instance-attribute The URL of the image. #### kind class-attribute instance-attribute Type identifier, this is available on all parts as a discriminator. #### format property The file format of the image. The choice of supported formats were based on the Bedrock Converse API. Other APIs don't require to use a format.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.ImageUrl", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "DocumentUrl dataclass", "anchor": "pydantic_ai.messages.DocumentUrl", "heading_level": 3, "md_text": "Bases: FileUrl The URL of the document. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### url instance-attribute The URL of the document. #### kind class-attribute instance-attribute Type identifier, this is available on all parts as a discriminator. #### format property The file format of the document. The choice of supported formats were based on the Bedrock Converse API. Other APIs don't require to use a format.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.DocumentUrl", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "BinaryContent dataclass", "anchor": "pydantic_ai.messages.BinaryContent", "heading_level": 3, "md_text": "Binary content, e.g. an audio or image file. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### data instance-attribute The binary data. #### media\\_type instance-attribute The media type of the binary data. #### vendor\\_metadata class-attribute instance-attribute Vendor-specific metadata for the file. Supported by: - GoogleModel : BinaryContent.vendor_metadata is used as video_metadata : https://ai.google.dev/gemini-api/docs/video-understanding#customize-video-processing - OpenAIChatModel , OpenAIResponsesModel : BinaryContent.vendor_metadata['detail'] is used as detail setting for images #### kind class-attribute instance-attribute Type identifier, this is available on all parts as a discriminator. #### narrow\\_type staticmethod Narrow the type of the BinaryContent to BinaryImage if it's an image. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### from\\_data\\_uri classmethod Create a BinaryContent from a data URI. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### identifier property Identifier for the binary content, such as a unique ID. This identifier can be provided to the model in a message to allow it to refer to this file in a tool call argument, and the tool can look up the file in question by iterating over the message history and finding the matching BinaryContent . This identifier is only automatically passed to the model when the BinaryContent is returned by a tool. If you're passing the BinaryContent as a user message, it's up to you to include a separate text part with the identifier, e.g. \"This is file :\" preceding the BinaryContent . It's also included in inline-text delimiters for providers that require inlining text documents, so the model can distinguish multiple files. #### data\\_uri property Convert the BinaryContent to a data URI. #### is\\_audio property Return True if the media type is an audio type. #### is\\_image property Return True if the media type is an image type. #### is\\_video property Return True if the media type is a video type. #### is\\_document property Return True if the media type is a document type. #### format property The file format of the binary content.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.BinaryContent", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "BinaryImage", "anchor": "pydantic_ai.messages.BinaryImage", "heading_level": 3, "md_text": "Bases: BinaryContent Binary content that's guaranteed to be an image. Source code in pydantic_ai_slim/pydantic_ai/messages.py", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.BinaryImage", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "ToolReturn dataclass", "anchor": "pydantic_ai.messages.ToolReturn", "heading_level": 3, "md_text": "A structured return value for tools that need to provide both a return value and custom content to the model. This class allows tools to return complex responses that include: - A return value for actual tool return - Custom content (including multi-modal content) to be sent to the model as a UserPromptPart - Optional metadata for application use Source code in pydantic_ai_slim/pydantic_ai/messages.py #### return\\_value instance-attribute The return value to be used in the tool response. #### content class-attribute instance-attribute The content to be sent to the model as a UserPromptPart. #### metadata class-attribute instance-attribute Additional data that can be accessed programmatically by the application but is not sent to the LLM.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.ToolReturn", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "UserPromptPart dataclass", "anchor": "pydantic_ai.messages.UserPromptPart", "heading_level": 3, "md_text": "A user prompt, generally written by the end user. Content comes from the user_prompt parameter of [ Agent.run ](../agent/index.html#pydantic_ai.agent.AbstractAgent.run), [ Agent.run_sync ](../agent/index.html#pydantic_ai.agent.AbstractAgent.run_sync), and [ Agent.run_stream ](../agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream). Source code in pydantic_ai_slim/pydantic_ai/messages.py #### content instance-attribute The content of the prompt. #### timestamp class-attribute instance-attribute The timestamp of the prompt. #### part\\_kind class-attribute instance-attribute Part type identifier, this is available on all parts as a discriminator.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.UserPromptPart", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "BaseToolReturnPart dataclass", "anchor": "pydantic_ai.messages.BaseToolReturnPart", "heading_level": 3, "md_text": "Base class for tool return parts. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### tool\\_name instance-attribute The name of the \"tool\" was called. #### content instance-attribute The return value. #### tool\\_call\\_id class-attribute instance-attribute The tool call identifier, this is used by some models including OpenAI. In case the tool call id is not provided by the model, Pydantic AI will generate a random one. #### metadata class-attribute instance-attribute Additional data that can be accessed programmatically by the application but is not sent to the LLM. #### timestamp class-attribute instance-attribute The timestamp, when the tool returned. #### model\\_response\\_str Return a string representation of the content for the model. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### model\\_response\\_object Return a dictionary representation of the content, wrapping non-dict types appropriately. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### has\\_content Return True if the tool return has content. Source code in pydantic_ai_slim/pydantic_ai/messages.py", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.BaseToolReturnPart", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "ToolReturnPart dataclass", "anchor": "pydantic_ai.messages.ToolReturnPart", "heading_level": 3, "md_text": "Bases: BaseToolReturnPart A tool return message, this encodes the result of running a tool. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### part\\_kind class-attribute instance-attribute Part type identifier, this is available on all parts as a discriminator.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.ToolReturnPart", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "BuiltinToolReturnPart dataclass", "anchor": "pydantic_ai.messages.BuiltinToolReturnPart", "heading_level": 3, "md_text": "Bases: BaseToolReturnPart A tool return message from a built-in tool. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### provider\\_name class-attribute instance-attribute The name of the provider that generated the response. #### part\\_kind class-attribute instance-attribute Part type identifier, this is available on all parts as a discriminator.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.BuiltinToolReturnPart", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "RetryPromptPart dataclass", "anchor": "pydantic_ai.messages.RetryPromptPart", "heading_level": 3, "md_text": "A message back to a model asking it to try again. This can be sent for a number of reasons: * Pydantic validation of tool arguments failed, here content is derived from a Pydantic [ ValidationError ](https://docs.pydantic.dev/latest/api/pydantic_core/#pydantic_core.ValidationError) * a tool raised a [ ModelRetry ](../exceptions/index.html#pydantic_ai.exceptions.ModelRetry) exception * no tool was found for the tool name * the model returned plain text when a structured response was expected * Pydantic validation of a structured response failed, here content is derived from a Pydantic [ ValidationError ](https://docs.pydantic.dev/latest/api/pydantic_core/#pydantic_core.ValidationError) * an output validator raised a [ ModelRetry ](../exceptions/index.html#pydantic_ai.exceptions.ModelRetry) exception Source code in pydantic_ai_slim/pydantic_ai/messages.py #### content instance-attribute Details of why and how the model should retry. If the retry was triggered by a [ ValidationError ](https://docs.pydantic.dev/latest/api/pydantic_core/#pydantic_core.ValidationError), this will be a list of error details. #### tool\\_name class-attribute instance-attribute The name of the tool that was called, if any. #### tool\\_call\\_id class-attribute instance-attribute The tool call identifier, this is used by some models including OpenAI. In case the tool call id is not provided by the model, Pydantic AI will generate a random one. #### timestamp class-attribute instance-attribute The timestamp, when the retry was triggered. #### part\\_kind class-attribute instance-attribute Part type identifier, this is available on all parts as a discriminator. #### model\\_response Return a string message describing why the retry is requested. Source code in pydantic_ai_slim/pydantic_ai/messages.py", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.RetryPromptPart", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "ModelRequestPart module-attribute", "anchor": "pydantic_ai.messages.ModelRequestPart", "heading_level": 3, "md_text": "A message part sent by Pydantic AI to a model.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.ModelRequestPart", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "ModelRequest dataclass", "anchor": "pydantic_ai.messages.ModelRequest", "heading_level": 3, "md_text": "A request generated by Pydantic AI and sent to a model, e.g. a message from the Pydantic AI app to the model. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### parts instance-attribute The parts of the user message. #### instructions class-attribute instance-attribute The instructions for the model. #### kind class-attribute instance-attribute Message type identifier, this is available on all parts as a discriminator. #### user\\_text\\_prompt classmethod Create a ModelRequest with a single user prompt as text. Source code in pydantic_ai_slim/pydantic_ai/messages.py", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.ModelRequest", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "TextPart dataclass", "anchor": "pydantic_ai.messages.TextPart", "heading_level": 3, "md_text": "A plain text response from a model. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### content instance-attribute The text content of the response. #### id class-attribute instance-attribute An optional identifier of the text part. #### part\\_kind class-attribute instance-attribute Part type identifier, this is available on all parts as a discriminator. #### has\\_content Return True if the text content is non-empty. Source code in pydantic_ai_slim/pydantic_ai/messages.py", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.TextPart", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "ThinkingPart dataclass", "anchor": "pydantic_ai.messages.ThinkingPart", "heading_level": 3, "md_text": "A thinking response from a model. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### content instance-attribute The thinking content of the response. #### id class-attribute instance-attribute The identifier of the thinking part. #### signature class-attribute instance-attribute The signature of the thinking. Supported by: * Anthropic (corresponds to the signature field) * Bedrock (corresponds to the signature field) * Google (corresponds to the thought_signature field) * OpenAI (corresponds to the encrypted_content field) #### provider\\_name class-attribute instance-attribute The name of the provider that generated the response. Signatures are only sent back to the same provider. #### part\\_kind class-attribute instance-attribute Part type identifier, this is available on all parts as a discriminator. #### has\\_content Return True if the thinking content is non-empty. Source code in pydantic_ai_slim/pydantic_ai/messages.py", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.ThinkingPart", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "FilePart dataclass", "anchor": "pydantic_ai.messages.FilePart", "heading_level": 3, "md_text": "A file response from a model. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### content instance-attribute The file content of the response. #### id class-attribute instance-attribute The identifier of the file part. #### provider\\_name class-attribute instance-attribute The name of the provider that generated the response. #### part\\_kind class-attribute instance-attribute Part type identifier, this is available on all parts as a discriminator. #### has\\_content Return True if the file content is non-empty. Source code in pydantic_ai_slim/pydantic_ai/messages.py", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.FilePart", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "BaseToolCallPart dataclass", "anchor": "pydantic_ai.messages.BaseToolCallPart", "heading_level": 3, "md_text": "A tool call from a model. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### tool\\_name instance-attribute The name of the tool to call. #### args class-attribute instance-attribute The arguments to pass to the tool. This is stored either as a JSON string or a Python dictionary depending on how data was received. #### tool\\_call\\_id class-attribute instance-attribute The tool call identifier, this is used by some models including OpenAI. In case the tool call id is not provided by the model, Pydantic AI will generate a random one. #### id class-attribute instance-attribute An optional identifier of the tool call part, separate from the tool call ID. This is used by some APIs like OpenAI Responses. #### args\\_as\\_dict Return the arguments as a Python dictionary. This is just for convenience with models that require dicts as input. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### args\\_as\\_json\\_str Return the arguments as a JSON string. This is just for convenience with models that require JSON strings as input. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### has\\_content Return True if the arguments contain any data. Source code in pydantic_ai_slim/pydantic_ai/messages.py", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.BaseToolCallPart", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "ToolCallPart dataclass", "anchor": "pydantic_ai.messages.ToolCallPart", "heading_level": 3, "md_text": "Bases: BaseToolCallPart A tool call from a model. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### part\\_kind class-attribute instance-attribute Part type identifier, this is available on all parts as a discriminator.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.ToolCallPart", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "BuiltinToolCallPart dataclass", "anchor": "pydantic_ai.messages.BuiltinToolCallPart", "heading_level": 3, "md_text": "Bases: BaseToolCallPart A tool call to a built-in tool. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### provider\\_name class-attribute instance-attribute The name of the provider that generated the response. Built-in tool calls are only sent back to the same provider. #### part\\_kind class-attribute instance-attribute Part type identifier, this is available on all parts as a discriminator.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.BuiltinToolCallPart", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "ModelResponsePart module-attribute", "anchor": "pydantic_ai.messages.ModelResponsePart", "heading_level": 3, "md_text": "A message part returned by a model.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.ModelResponsePart", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "ModelResponse dataclass", "anchor": "pydantic_ai.messages.ModelResponse", "heading_level": 3, "md_text": "A response from a model, e.g. a message from the model to the Pydantic AI app. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### parts instance-attribute The parts of the model message. #### usage class-attribute instance-attribute Usage information for the request. This has a default to make tests easier, and to support loading old messages where usage will be missing. #### model\\_name class-attribute instance-attribute The name of the model that generated the response. #### timestamp class-attribute instance-attribute The timestamp of the response. If the model provides a timestamp in the response (as OpenAI does) that will be used. #### kind class-attribute instance-attribute Message type identifier, this is available on all parts as a discriminator. #### provider\\_name class-attribute instance-attribute The name of the LLM provider that generated the response. #### provider\\_details class-attribute instance-attribute Additional provider-specific details in a serializable format. This allows storing selected vendor-specific data that isn't mapped to standard ModelResponse fields. For OpenAI models, this may include 'logprobs', 'finish\\_reason', etc. #### provider\\_response\\_id class-attribute instance-attribute request ID as specified by the model provider. This can be used to track the specific request to the model. #### finish\\_reason class-attribute instance-attribute Reason the model finished generating the response, normalized to OpenTelemetry values. #### text property Get the text in the response. #### thinking property Get the thinking in the response. #### files property Get the files in the response. #### images property Get the images in the response. #### tool\\_calls property Get the tool calls in the response. #### builtin\\_tool\\_calls property Get the builtin tool calls and results in the response. #### price deprecated Deprecated price is deprecated, use cost instead Source code in pydantic_ai_slim/pydantic_ai/messages.py #### cost Calculate the cost of the usage. Uses [ genai-prices ](https://github.com/pydantic/genai-prices). Source code in pydantic_ai_slim/pydantic_ai/messages.py #### otel\\_events Return OpenTelemetry events for the response. Source code in pydantic_ai_slim/pydantic_ai/messages.py", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.ModelResponse", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "ModelMessage module-attribute", "anchor": "pydantic_ai.messages.ModelMessage", "heading_level": 3, "md_text": "Any message sent to or returned by a model.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.ModelMessage", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "ModelMessagesTypeAdapter module-attribute", "anchor": "pydantic_ai.messages.ModelMessagesTypeAdapter", "heading_level": 3, "md_text": "Pydantic [ TypeAdapter ](https://docs.pydantic.dev/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter) for (de)serializing messages.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.ModelMessagesTypeAdapter", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "TextPartDelta dataclass", "anchor": "pydantic_ai.messages.TextPartDelta", "heading_level": 3, "md_text": "A partial update (delta) for a TextPart to append new text content. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### content\\_delta instance-attribute The incremental text content to add to the existing TextPart content. #### part\\_delta\\_kind class-attribute instance-attribute Part delta type identifier, used as a discriminator. #### apply Apply this text delta to an existing TextPart . Parameters: Returns: Raises: Source code in pydantic_ai_slim/pydantic_ai/messages.py", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.TextPartDelta", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "ThinkingPartDelta dataclass", "anchor": "pydantic_ai.messages.ThinkingPartDelta", "heading_level": 3, "md_text": "A partial update (delta) for a ThinkingPart to append new thinking content. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### content\\_delta class-attribute instance-attribute The incremental thinking content to add to the existing ThinkingPart content. #### signature\\_delta class-attribute instance-attribute Optional signature delta. Note this is never treated as a delta  it can replace None. #### provider\\_name class-attribute instance-attribute Optional provider name for the thinking part. Signatures are only sent back to the same provider. #### part\\_delta\\_kind class-attribute instance-attribute Part delta type identifier, used as a discriminator. #### apply Apply this thinking delta to an existing ThinkingPart . Parameters: Returns: Raises: Source code in pydantic_ai_slim/pydantic_ai/messages.py", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.ThinkingPartDelta", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "ToolCallPartDelta dataclass", "anchor": "pydantic_ai.messages.ToolCallPartDelta", "heading_level": 3, "md_text": "A partial update (delta) for a ToolCallPart to modify tool name, arguments, or tool call ID. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### tool\\_name\\_delta class-attribute instance-attribute Incremental text to add to the existing tool name, if any. #### args\\_delta class-attribute instance-attribute Incremental data to add to the tool arguments. If this is a string, it will be appended to existing JSON arguments. If this is a dict, it will be merged with existing dict arguments. #### tool\\_call\\_id class-attribute instance-attribute Optional tool call identifier, this is used by some models including OpenAI. Note this is never treated as a delta  it can replace None, but otherwise if a non-matching value is provided an error will be raised. #### part\\_delta\\_kind class-attribute instance-attribute Part delta type identifier, used as a discriminator. #### as\\_part Convert this delta to a fully formed ToolCallPart if possible, otherwise return None . Returns: Source code in pydantic_ai_slim/pydantic_ai/messages.py #### apply Apply this delta to a part or delta, returning a new part or delta with the changes applied. Parameters: Returns: Raises: Source code in pydantic_ai_slim/pydantic_ai/messages.py", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.ToolCallPartDelta", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "ModelResponsePartDelta module-attribute", "anchor": "pydantic_ai.messages.ModelResponsePartDelta", "heading_level": 3, "md_text": "A partial update (delta) for any model response part.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.ModelResponsePartDelta", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "PartStartEvent dataclass", "anchor": "pydantic_ai.messages.PartStartEvent", "heading_level": 3, "md_text": "An event indicating that a new part has started. If multiple PartStartEvent s are received with the same index, the new one should fully replace the old one. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### index instance-attribute The index of the part within the overall response parts list. #### part instance-attribute The newly started ModelResponsePart . #### event\\_kind class-attribute instance-attribute Event type identifier, used as a discriminator.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.PartStartEvent", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "PartDeltaEvent dataclass", "anchor": "pydantic_ai.messages.PartDeltaEvent", "heading_level": 3, "md_text": "An event indicating a delta update for an existing part. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### index instance-attribute The index of the part within the overall response parts list. #### delta instance-attribute The delta to apply to the specified part. #### event\\_kind class-attribute instance-attribute Event type identifier, used as a discriminator.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.PartDeltaEvent", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "FinalResultEvent dataclass", "anchor": "pydantic_ai.messages.FinalResultEvent", "heading_level": 3, "md_text": "An event indicating the response to the current model request matches the output schema and will produce a result. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### tool\\_name instance-attribute The name of the output tool that was called. None if the result is from text content and not from a tool. #### tool\\_call\\_id instance-attribute The tool call ID, if any, that this result is associated with. #### event\\_kind class-attribute instance-attribute Event type identifier, used as a discriminator.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.FinalResultEvent", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "ModelResponseStreamEvent module-attribute", "anchor": "pydantic_ai.messages.ModelResponseStreamEvent", "heading_level": 3, "md_text": "An event in the model response stream, starting a new part, applying a delta to an existing one, or indicating the final result.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.ModelResponseStreamEvent", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "FunctionToolCallEvent dataclass", "anchor": "pydantic_ai.messages.FunctionToolCallEvent", "heading_level": 3, "md_text": "An event indicating the start to a call to a function tool. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### part instance-attribute The (function) tool call to make. #### event\\_kind class-attribute instance-attribute Event type identifier, used as a discriminator. #### tool\\_call\\_id property An ID used for matching details about the call to its result. #### call\\_id property An ID used for matching details about the call to its result.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.FunctionToolCallEvent", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "FunctionToolResultEvent dataclass", "anchor": "pydantic_ai.messages.FunctionToolResultEvent", "heading_level": 3, "md_text": "An event indicating the result of a function tool call. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### result instance-attribute The result of the call to the function tool. #### content class-attribute instance-attribute The content that will be sent to the model as a UserPromptPart following the result. #### event\\_kind class-attribute instance-attribute Event type identifier, used as a discriminator. #### tool\\_call\\_id property An ID used to match the result to its original call.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.FunctionToolResultEvent", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "BuiltinToolCallEvent dataclass deprecated", "anchor": "pydantic_ai.messages.BuiltinToolCallEvent", "heading_level": 3, "md_text": "Deprecated BuiltinToolCallEvent is deprecated, look for PartStartEvent and PartDeltaEvent with BuiltinToolCallPart instead. An event indicating the start to a call to a built-in tool. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### part instance-attribute The built-in tool call to make. #### event\\_kind class-attribute instance-attribute Event type identifier, used as a discriminator.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.BuiltinToolCallEvent", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "BuiltinToolResultEvent dataclass deprecated", "anchor": "pydantic_ai.messages.BuiltinToolResultEvent", "heading_level": 3, "md_text": "Deprecated BuiltinToolResultEvent is deprecated, look for PartStartEvent and PartDeltaEvent with BuiltinToolReturnPart instead. An event indicating the result of a built-in tool call. Source code in pydantic_ai_slim/pydantic_ai/messages.py #### result instance-attribute The result of the call to the built-in tool. #### event\\_kind class-attribute instance-attribute Event type identifier, used as a discriminator.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.BuiltinToolResultEvent", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "HandleResponseEvent module-attribute", "anchor": "pydantic_ai.messages.HandleResponseEvent", "heading_level": 3, "md_text": "An event yielded when handling a model response, indicating tool calls and results.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.HandleResponseEvent", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "AgentStreamEvent module-attribute", "anchor": "pydantic_ai.messages.AgentStreamEvent", "heading_level": 3, "md_text": "An event in the agent stream: model response stream events and response-handling events.", "url": "https://ai.pydantic.dev/api/messages/index.html#pydantic_ai.messages.AgentStreamEvent", "page": "api/messages/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.models.groq", "anchor": "pydantic_aimodelsgroq", "heading_level": 1, "md_text": "## Setup For details on how to set up authentication with this model, see [model configuration for Groq](../../../models/groq/index.html). ### ProductionGroqModelNames module-attribute Production Groq models from <https://console.groq.com/docs/models#production-models>. ### PreviewGroqModelNames module-attribute Preview Groq models from <https://console.groq.com/docs/models#preview-models>. ### GroqModelName module-attribute Possible Groq model names. Since Groq supports a variety of models and the list changes frequencly, we explicitly list the named models as of 2025-03-31 but allow any name in the type hints. See <https://console.groq.com/docs/models> for an up to date date list of models and more details. ### GroqModelSettings Bases: ModelSettings Settings used for a Groq model request. Source code in pydantic_ai_slim/pydantic_ai/models/groq.py #### groq\\_reasoning\\_format instance-attribute The format of the reasoning output. See [the Groq docs](https://console.groq.com/docs/reasoning#reasoning-format) for more details. ### GroqModel dataclass Bases: Model A model that uses the Groq API. Internally, this uses the [Groq Python client](https://github.com/groq/groq-python) to interact with the API. Apart from __init__ , all methods are private or match those of the base class. Source code in pydantic_ai_slim/pydantic_ai/models/groq.py #### \\_\\_init\\_\\_ Initialize a Groq model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/groq.py #### model\\_name property The model name. #### system property The model provider. ### GroqStreamedResponse dataclass Bases: StreamedResponse Implementation of StreamedResponse for Groq models. Source code in pydantic_ai_slim/pydantic_ai/models/groq.py #### model\\_name property Get the model name of the response. #### provider\\_name property Get the provider name. #### timestamp property Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/models/groq/index.html#pydantic_aimodelsgroq", "page": "api/models/groq/index.html", "source_site": "pydantic_ai"}
{"title": "Setup", "anchor": "setup", "heading_level": 2, "md_text": "For details on how to set up authentication with this model, see [model configuration for Groq](../../../models/groq/index.html). ### ProductionGroqModelNames module-attribute Production Groq models from <https://console.groq.com/docs/models#production-models>. ### PreviewGroqModelNames module-attribute Preview Groq models from <https://console.groq.com/docs/models#preview-models>. ### GroqModelName module-attribute Possible Groq model names. Since Groq supports a variety of models and the list changes frequencly, we explicitly list the named models as of 2025-03-31 but allow any name in the type hints. See <https://console.groq.com/docs/models> for an up to date date list of models and more details. ### GroqModelSettings Bases: ModelSettings Settings used for a Groq model request. Source code in pydantic_ai_slim/pydantic_ai/models/groq.py #### groq\\_reasoning\\_format instance-attribute The format of the reasoning output. See [the Groq docs](https://console.groq.com/docs/reasoning#reasoning-format) for more details. ### GroqModel dataclass Bases: Model A model that uses the Groq API. Internally, this uses the [Groq Python client](https://github.com/groq/groq-python) to interact with the API. Apart from __init__ , all methods are private or match those of the base class. Source code in pydantic_ai_slim/pydantic_ai/models/groq.py #### \\_\\_init\\_\\_ Initialize a Groq model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/groq.py #### model\\_name property The model name. #### system property The model provider. ### GroqStreamedResponse dataclass Bases: StreamedResponse Implementation of StreamedResponse for Groq models. Source code in pydantic_ai_slim/pydantic_ai/models/groq.py #### model\\_name property Get the model name of the response. #### provider\\_name property Get the provider name. #### timestamp property Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/models/groq/index.html#setup", "page": "api/models/groq/index.html", "source_site": "pydantic_ai"}
{"title": "ProductionGroqModelNames module-attribute", "anchor": "pydantic_ai.models.groq.ProductionGroqModelNames", "heading_level": 3, "md_text": "Production Groq models from <https://console.groq.com/docs/models#production-models>.", "url": "https://ai.pydantic.dev/api/models/groq/index.html#pydantic_ai.models.groq.ProductionGroqModelNames", "page": "api/models/groq/index.html", "source_site": "pydantic_ai"}
{"title": "PreviewGroqModelNames module-attribute", "anchor": "pydantic_ai.models.groq.PreviewGroqModelNames", "heading_level": 3, "md_text": "Preview Groq models from <https://console.groq.com/docs/models#preview-models>.", "url": "https://ai.pydantic.dev/api/models/groq/index.html#pydantic_ai.models.groq.PreviewGroqModelNames", "page": "api/models/groq/index.html", "source_site": "pydantic_ai"}
{"title": "GroqModelName module-attribute", "anchor": "pydantic_ai.models.groq.GroqModelName", "heading_level": 3, "md_text": "Possible Groq model names. Since Groq supports a variety of models and the list changes frequencly, we explicitly list the named models as of 2025-03-31 but allow any name in the type hints. See <https://console.groq.com/docs/models> for an up to date date list of models and more details.", "url": "https://ai.pydantic.dev/api/models/groq/index.html#pydantic_ai.models.groq.GroqModelName", "page": "api/models/groq/index.html", "source_site": "pydantic_ai"}
{"title": "GroqModelSettings", "anchor": "pydantic_ai.models.groq.GroqModelSettings", "heading_level": 3, "md_text": "Bases: ModelSettings Settings used for a Groq model request. Source code in pydantic_ai_slim/pydantic_ai/models/groq.py #### groq\\_reasoning\\_format instance-attribute The format of the reasoning output. See [the Groq docs](https://console.groq.com/docs/reasoning#reasoning-format) for more details.", "url": "https://ai.pydantic.dev/api/models/groq/index.html#pydantic_ai.models.groq.GroqModelSettings", "page": "api/models/groq/index.html", "source_site": "pydantic_ai"}
{"title": "GroqModel dataclass", "anchor": "pydantic_ai.models.groq.GroqModel", "heading_level": 3, "md_text": "Bases: Model A model that uses the Groq API. Internally, this uses the [Groq Python client](https://github.com/groq/groq-python) to interact with the API. Apart from __init__ , all methods are private or match those of the base class. Source code in pydantic_ai_slim/pydantic_ai/models/groq.py #### \\_\\_init\\_\\_ Initialize a Groq model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/groq.py #### model\\_name property The model name. #### system property The model provider.", "url": "https://ai.pydantic.dev/api/models/groq/index.html#pydantic_ai.models.groq.GroqModel", "page": "api/models/groq/index.html", "source_site": "pydantic_ai"}
{"title": "GroqStreamedResponse dataclass", "anchor": "pydantic_ai.models.groq.GroqStreamedResponse", "heading_level": 3, "md_text": "Bases: StreamedResponse Implementation of StreamedResponse for Groq models. Source code in pydantic_ai_slim/pydantic_ai/models/groq.py #### model\\_name property Get the model name of the response. #### provider\\_name property Get the provider name. #### timestamp property Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/models/groq/index.html#pydantic_ai.models.groq.GroqStreamedResponse", "page": "api/models/groq/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.models.mcp_sampling", "anchor": "pydantic_aimodelsmcp_sampling", "heading_level": 1, "md_text": "### MCPSamplingModelSettings Bases: ModelSettings Settings used for an MCP Sampling model request. Source code in pydantic_ai_slim/pydantic_ai/models/mcp_sampling.py #### mcp\\_model\\_preferences instance-attribute Model preferences to use for MCP Sampling. ### MCPSamplingModel dataclass Bases: Model A model that uses MCP Sampling. [MCP Sampling](https://modelcontextprotocol.io/docs/concepts/sampling) allows an MCP server to make requests to a model by calling back to the MCP client that connected to it. Source code in pydantic_ai_slim/pydantic_ai/models/mcp_sampling.py #### session instance-attribute The MCP server session to use for sampling. #### default\\_max\\_tokens class-attribute instance-attribute Default max tokens to use if not set in [ ModelSettings ](../../settings/index.html#pydantic_ai.settings.ModelSettings.max_tokens). Max tokens is a required parameter for MCP Sampling, but optional on [ ModelSettings ](../../settings/index.html#pydantic_ai.settings.ModelSettings), so this value is used as fallback. #### model\\_name property The model name. Since the model name isn't known until the request is made, this property always returns 'mcp-sampling' . #### system property The system / model provider, returns 'MCP' .", "url": "https://ai.pydantic.dev/api/models/mcp-sampling/index.html#pydantic_aimodelsmcp_sampling", "page": "api/models/mcp-sampling/index.html", "source_site": "pydantic_ai"}
{"title": "MCPSamplingModelSettings", "anchor": "pydantic_ai.models.mcp_sampling.MCPSamplingModelSettings", "heading_level": 3, "md_text": "Bases: ModelSettings Settings used for an MCP Sampling model request. Source code in pydantic_ai_slim/pydantic_ai/models/mcp_sampling.py #### mcp\\_model\\_preferences instance-attribute Model preferences to use for MCP Sampling.", "url": "https://ai.pydantic.dev/api/models/mcp-sampling/index.html#pydantic_ai.models.mcp_sampling.MCPSamplingModelSettings", "page": "api/models/mcp-sampling/index.html", "source_site": "pydantic_ai"}
{"title": "MCPSamplingModel dataclass", "anchor": "pydantic_ai.models.mcp_sampling.MCPSamplingModel", "heading_level": 3, "md_text": "Bases: Model A model that uses MCP Sampling. [MCP Sampling](https://modelcontextprotocol.io/docs/concepts/sampling) allows an MCP server to make requests to a model by calling back to the MCP client that connected to it. Source code in pydantic_ai_slim/pydantic_ai/models/mcp_sampling.py #### session instance-attribute The MCP server session to use for sampling. #### default\\_max\\_tokens class-attribute instance-attribute Default max tokens to use if not set in [ ModelSettings ](../../settings/index.html#pydantic_ai.settings.ModelSettings.max_tokens). Max tokens is a required parameter for MCP Sampling, but optional on [ ModelSettings ](../../settings/index.html#pydantic_ai.settings.ModelSettings), so this value is used as fallback. #### model\\_name property The model name. Since the model name isn't known until the request is made, this property always returns 'mcp-sampling' . #### system property The system / model provider, returns 'MCP' .", "url": "https://ai.pydantic.dev/api/models/mcp-sampling/index.html#pydantic_ai.models.mcp_sampling.MCPSamplingModel", "page": "api/models/mcp-sampling/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.models.google", "anchor": "pydantic_aimodelsgoogle", "heading_level": 1, "md_text": "Interface that uses the [ google-genai ](https://pypi.org/project/google-genai/) package under the hood to access Google's Gemini models via both the Generative Language API and Vertex AI. ## Setup For details on how to set up authentication with this model, see [model configuration for Google](../../../models/google/index.html). ### LatestGoogleModelNames module-attribute Latest Gemini models. ### GoogleModelName module-attribute Possible Gemini model names. Since Gemini supports a variety of date-stamped models, we explicitly list the latest models but allow any name in the type hints. See [the Gemini API docs](https://ai.google.dev/gemini-api/docs/models/gemini#model-variations) for a full list. ### GoogleModelSettings Bases: ModelSettings Settings used for a Gemini model request. Source code in pydantic_ai_slim/pydantic_ai/models/google.py #### google\\_safety\\_settings instance-attribute The safety settings to use for the model. See <https://ai.google.dev/gemini-api/docs/safety-settings> for more information. #### google\\_thinking\\_config instance-attribute The thinking configuration to use for the model. See <https://ai.google.dev/gemini-api/docs/thinking> for more information. #### google\\_labels instance-attribute User-defined metadata to break down billed charges. Only supported by the Vertex AI API. See the [Gemini API docs](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/add-labels-to-api-calls) for use cases and limitations. #### google\\_video\\_resolution instance-attribute The video resolution to use for the model. See <https://ai.google.dev/api/generate-content#MediaResolution> for more information. #### google\\_cached\\_content instance-attribute The name of the cached content to use for the model. See <https://ai.google.dev/gemini-api/docs/caching> for more information. ### GoogleModel dataclass Bases: Model A model that uses Gemini via generativelanguage.googleapis.com API. This is implemented from scratch rather than using a dedicated SDK, good API documentation is available [here](https://ai.google.dev/api). Apart from __init__ , all methods are private or match those of the base class. Source code in pydantic_ai_slim/pydantic_ai/models/google.py #### \\_\\_init\\_\\_ Initialize a Gemini model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/google.py #### model\\_name property The model name. #### system property The model provider. ### GeminiStreamedResponse dataclass Bases: StreamedResponse Implementation of StreamedResponse for the Gemini model. Source code in pydantic_ai_slim/pydantic_ai/models/google.py #### model\\_name property Get the model name of the response. #### provider\\_name property Get the provider name. #### timestamp property Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/models/google/index.html#pydantic_aimodelsgoogle", "page": "api/models/google/index.html", "source_site": "pydantic_ai"}
{"title": "Setup", "anchor": "setup", "heading_level": 2, "md_text": "For details on how to set up authentication with this model, see [model configuration for Google](../../../models/google/index.html). ### LatestGoogleModelNames module-attribute Latest Gemini models. ### GoogleModelName module-attribute Possible Gemini model names. Since Gemini supports a variety of date-stamped models, we explicitly list the latest models but allow any name in the type hints. See [the Gemini API docs](https://ai.google.dev/gemini-api/docs/models/gemini#model-variations) for a full list. ### GoogleModelSettings Bases: ModelSettings Settings used for a Gemini model request. Source code in pydantic_ai_slim/pydantic_ai/models/google.py #### google\\_safety\\_settings instance-attribute The safety settings to use for the model. See <https://ai.google.dev/gemini-api/docs/safety-settings> for more information. #### google\\_thinking\\_config instance-attribute The thinking configuration to use for the model. See <https://ai.google.dev/gemini-api/docs/thinking> for more information. #### google\\_labels instance-attribute User-defined metadata to break down billed charges. Only supported by the Vertex AI API. See the [Gemini API docs](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/add-labels-to-api-calls) for use cases and limitations. #### google\\_video\\_resolution instance-attribute The video resolution to use for the model. See <https://ai.google.dev/api/generate-content#MediaResolution> for more information. #### google\\_cached\\_content instance-attribute The name of the cached content to use for the model. See <https://ai.google.dev/gemini-api/docs/caching> for more information. ### GoogleModel dataclass Bases: Model A model that uses Gemini via generativelanguage.googleapis.com API. This is implemented from scratch rather than using a dedicated SDK, good API documentation is available [here](https://ai.google.dev/api). Apart from __init__ , all methods are private or match those of the base class. Source code in pydantic_ai_slim/pydantic_ai/models/google.py #### \\_\\_init\\_\\_ Initialize a Gemini model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/google.py #### model\\_name property The model name. #### system property The model provider. ### GeminiStreamedResponse dataclass Bases: StreamedResponse Implementation of StreamedResponse for the Gemini model. Source code in pydantic_ai_slim/pydantic_ai/models/google.py #### model\\_name property Get the model name of the response. #### provider\\_name property Get the provider name. #### timestamp property Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/models/google/index.html#setup", "page": "api/models/google/index.html", "source_site": "pydantic_ai"}
{"title": "LatestGoogleModelNames module-attribute", "anchor": "pydantic_ai.models.google.LatestGoogleModelNames", "heading_level": 3, "md_text": "Latest Gemini models.", "url": "https://ai.pydantic.dev/api/models/google/index.html#pydantic_ai.models.google.LatestGoogleModelNames", "page": "api/models/google/index.html", "source_site": "pydantic_ai"}
{"title": "GoogleModelName module-attribute", "anchor": "pydantic_ai.models.google.GoogleModelName", "heading_level": 3, "md_text": "Possible Gemini model names. Since Gemini supports a variety of date-stamped models, we explicitly list the latest models but allow any name in the type hints. See [the Gemini API docs](https://ai.google.dev/gemini-api/docs/models/gemini#model-variations) for a full list.", "url": "https://ai.pydantic.dev/api/models/google/index.html#pydantic_ai.models.google.GoogleModelName", "page": "api/models/google/index.html", "source_site": "pydantic_ai"}
{"title": "GoogleModelSettings", "anchor": "pydantic_ai.models.google.GoogleModelSettings", "heading_level": 3, "md_text": "Bases: ModelSettings Settings used for a Gemini model request. Source code in pydantic_ai_slim/pydantic_ai/models/google.py #### google\\_safety\\_settings instance-attribute The safety settings to use for the model. See <https://ai.google.dev/gemini-api/docs/safety-settings> for more information. #### google\\_thinking\\_config instance-attribute The thinking configuration to use for the model. See <https://ai.google.dev/gemini-api/docs/thinking> for more information. #### google\\_labels instance-attribute User-defined metadata to break down billed charges. Only supported by the Vertex AI API. See the [Gemini API docs](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/add-labels-to-api-calls) for use cases and limitations. #### google\\_video\\_resolution instance-attribute The video resolution to use for the model. See <https://ai.google.dev/api/generate-content#MediaResolution> for more information. #### google\\_cached\\_content instance-attribute The name of the cached content to use for the model. See <https://ai.google.dev/gemini-api/docs/caching> for more information.", "url": "https://ai.pydantic.dev/api/models/google/index.html#pydantic_ai.models.google.GoogleModelSettings", "page": "api/models/google/index.html", "source_site": "pydantic_ai"}
{"title": "GoogleModel dataclass", "anchor": "pydantic_ai.models.google.GoogleModel", "heading_level": 3, "md_text": "Bases: Model A model that uses Gemini via generativelanguage.googleapis.com API. This is implemented from scratch rather than using a dedicated SDK, good API documentation is available [here](https://ai.google.dev/api). Apart from __init__ , all methods are private or match those of the base class. Source code in pydantic_ai_slim/pydantic_ai/models/google.py #### \\_\\_init\\_\\_ Initialize a Gemini model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/google.py #### model\\_name property The model name. #### system property The model provider.", "url": "https://ai.pydantic.dev/api/models/google/index.html#pydantic_ai.models.google.GoogleModel", "page": "api/models/google/index.html", "source_site": "pydantic_ai"}
{"title": "GeminiStreamedResponse dataclass", "anchor": "pydantic_ai.models.google.GeminiStreamedResponse", "heading_level": 3, "md_text": "Bases: StreamedResponse Implementation of StreamedResponse for the Gemini model. Source code in pydantic_ai_slim/pydantic_ai/models/google.py #### model\\_name property Get the model name of the response. #### provider\\_name property Get the provider name. #### timestamp property Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/models/google/index.html#pydantic_ai.models.google.GeminiStreamedResponse", "page": "api/models/google/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.models.huggingface", "anchor": "pydantic_aimodelshuggingface", "heading_level": 1, "md_text": "## Setup For details on how to set up authentication with this model, see [model configuration for Hugging Face](../../../models/huggingface/index.html). ### HuggingFaceModelSettings Bases: ModelSettings Settings used for a Hugging Face model request. Source code in pydantic_ai_slim/pydantic_ai/models/huggingface.py ### HuggingFaceModel dataclass Bases: Model A model that uses Hugging Face Inference Providers. Internally, this uses the [HF Python client](https://github.com/huggingface/huggingface_hub) to interact with the API. Apart from __init__ , all methods are private or match those of the base class. Source code in pydantic_ai_slim/pydantic_ai/models/huggingface.py #### \\_\\_init\\_\\_ Initialize a Hugging Face model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/huggingface.py #### model\\_name property The model name. #### system property The system / model provider.", "url": "https://ai.pydantic.dev/api/models/huggingface/index.html#pydantic_aimodelshuggingface", "page": "api/models/huggingface/index.html", "source_site": "pydantic_ai"}
{"title": "Setup", "anchor": "setup", "heading_level": 2, "md_text": "For details on how to set up authentication with this model, see [model configuration for Hugging Face](../../../models/huggingface/index.html). ### HuggingFaceModelSettings Bases: ModelSettings Settings used for a Hugging Face model request. Source code in pydantic_ai_slim/pydantic_ai/models/huggingface.py ### HuggingFaceModel dataclass Bases: Model A model that uses Hugging Face Inference Providers. Internally, this uses the [HF Python client](https://github.com/huggingface/huggingface_hub) to interact with the API. Apart from __init__ , all methods are private or match those of the base class. Source code in pydantic_ai_slim/pydantic_ai/models/huggingface.py #### \\_\\_init\\_\\_ Initialize a Hugging Face model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/huggingface.py #### model\\_name property The model name. #### system property The system / model provider.", "url": "https://ai.pydantic.dev/api/models/huggingface/index.html#setup", "page": "api/models/huggingface/index.html", "source_site": "pydantic_ai"}
{"title": "HuggingFaceModelSettings", "anchor": "pydantic_ai.models.huggingface.HuggingFaceModelSettings", "heading_level": 3, "md_text": "Bases: ModelSettings Settings used for a Hugging Face model request. Source code in pydantic_ai_slim/pydantic_ai/models/huggingface.py", "url": "https://ai.pydantic.dev/api/models/huggingface/index.html#pydantic_ai.models.huggingface.HuggingFaceModelSettings", "page": "api/models/huggingface/index.html", "source_site": "pydantic_ai"}
{"title": "HuggingFaceModel dataclass", "anchor": "pydantic_ai.models.huggingface.HuggingFaceModel", "heading_level": 3, "md_text": "Bases: Model A model that uses Hugging Face Inference Providers. Internally, this uses the [HF Python client](https://github.com/huggingface/huggingface_hub) to interact with the API. Apart from __init__ , all methods are private or match those of the base class. Source code in pydantic_ai_slim/pydantic_ai/models/huggingface.py #### \\_\\_init\\_\\_ Initialize a Hugging Face model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/huggingface.py #### model\\_name property The model name. #### system property The system / model provider.", "url": "https://ai.pydantic.dev/api/models/huggingface/index.html#pydantic_ai.models.huggingface.HuggingFaceModel", "page": "api/models/huggingface/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.models.instrumented", "anchor": "pydantic_aimodelsinstrumented", "heading_level": 1, "md_text": "### instrument\\_model Instrument a model with OpenTelemetry/logfire. Source code in pydantic_ai_slim/pydantic_ai/models/instrumented.py ### InstrumentationSettings dataclass Options for instrumenting models and agents with OpenTelemetry. Used in: * Agent(instrument=...) * [ Agent.instrument_all() ](../../agent/index.html#pydantic_ai.agent.Agent.instrument_all) * [ InstrumentedModel ](index.html#pydantic_ai.models.instrumented.InstrumentedModel) See the [Debugging and Monitoring guide](../../../logfire/index.html) for more info. Source code in pydantic_ai_slim/pydantic_ai/models/instrumented.py #### \\_\\_init\\_\\_ Create instrumentation options. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/instrumented.py #### messages\\_to\\_otel\\_events Convert a list of model messages to OpenTelemetry events. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/models/instrumented.py ### InstrumentedModel dataclass Bases: WrapperModel Model which wraps another model so that requests are instrumented with OpenTelemetry. See the [Debugging and Monitoring guide](../../../logfire/index.html) for more info. Source code in pydantic_ai_slim/pydantic_ai/models/instrumented.py #### instrumentation\\_settings instance-attribute Instrumentation settings for this model.", "url": "https://ai.pydantic.dev/api/models/instrumented/index.html#pydantic_aimodelsinstrumented", "page": "api/models/instrumented/index.html", "source_site": "pydantic_ai"}
{"title": "instrument_model", "anchor": "pydantic_ai.models.instrumented.instrument_model", "heading_level": 3, "md_text": "Instrument a model with OpenTelemetry/logfire. Source code in pydantic_ai_slim/pydantic_ai/models/instrumented.py", "url": "https://ai.pydantic.dev/api/models/instrumented/index.html#pydantic_ai.models.instrumented.instrument_model", "page": "api/models/instrumented/index.html", "source_site": "pydantic_ai"}
{"title": "InstrumentationSettings dataclass", "anchor": "pydantic_ai.models.instrumented.InstrumentationSettings", "heading_level": 3, "md_text": "Options for instrumenting models and agents with OpenTelemetry. Used in: * Agent(instrument=...) * [ Agent.instrument_all() ](../../agent/index.html#pydantic_ai.agent.Agent.instrument_all) * [ InstrumentedModel ](index.html#pydantic_ai.models.instrumented.InstrumentedModel) See the [Debugging and Monitoring guide](../../../logfire/index.html) for more info. Source code in pydantic_ai_slim/pydantic_ai/models/instrumented.py #### \\_\\_init\\_\\_ Create instrumentation options. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/instrumented.py #### messages\\_to\\_otel\\_events Convert a list of model messages to OpenTelemetry events. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/models/instrumented.py", "url": "https://ai.pydantic.dev/api/models/instrumented/index.html#pydantic_ai.models.instrumented.InstrumentationSettings", "page": "api/models/instrumented/index.html", "source_site": "pydantic_ai"}
{"title": "InstrumentedModel dataclass", "anchor": "pydantic_ai.models.instrumented.InstrumentedModel", "heading_level": 3, "md_text": "Bases: WrapperModel Model which wraps another model so that requests are instrumented with OpenTelemetry. See the [Debugging and Monitoring guide](../../../logfire/index.html) for more info. Source code in pydantic_ai_slim/pydantic_ai/models/instrumented.py #### instrumentation\\_settings instance-attribute Instrumentation settings for this model.", "url": "https://ai.pydantic.dev/api/models/instrumented/index.html#pydantic_ai.models.instrumented.InstrumentedModel", "page": "api/models/instrumented/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.models.wrapper", "anchor": "pydantic_aimodelswrapper", "heading_level": 1, "md_text": "### WrapperModel dataclass Bases: Model Model which wraps another model. Does nothing on its own, used as a base class. Source code in pydantic_ai_slim/pydantic_ai/models/wrapper.py #### wrapped instance-attribute The underlying model being wrapped. #### settings property Get the settings from the wrapped model.", "url": "https://ai.pydantic.dev/api/models/wrapper/index.html#pydantic_aimodelswrapper", "page": "api/models/wrapper/index.html", "source_site": "pydantic_ai"}
{"title": "WrapperModel dataclass", "anchor": "pydantic_ai.models.wrapper.WrapperModel", "heading_level": 3, "md_text": "Bases: Model Model which wraps another model. Does nothing on its own, used as a base class. Source code in pydantic_ai_slim/pydantic_ai/models/wrapper.py #### wrapped instance-attribute The underlying model being wrapped. #### settings property Get the settings from the wrapped model.", "url": "https://ai.pydantic.dev/api/models/wrapper/index.html#pydantic_ai.models.wrapper.WrapperModel", "page": "api/models/wrapper/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.models.test", "anchor": "pydantic_aimodelstest", "heading_level": 1, "md_text": "Utility model for quickly testing apps built with Pydantic AI. Here's a minimal example: test\\_model\\_usage.py See [Unit testing with TestModel ](../../../testing/index.html#unit-testing-with-testmodel) for detailed documentation. ### TestModel dataclass Bases: Model A model specifically for testing purposes. This will (by default) call all tools in the agent, then return a tool response if possible, otherwise a plain response. How useful this model is will vary significantly. Apart from __init__ derived by the dataclass decorator, all methods are private or match those of the base class. Source code in pydantic_ai_slim/pydantic_ai/models/test.py #### \\_\\_init\\_\\_ Initialize TestModel with optional settings and profile. Source code in pydantic_ai_slim/pydantic_ai/models/test.py #### call\\_tools class-attribute instance-attribute List of tools to call. If 'all' , all tools will be called. #### custom\\_output\\_text class-attribute instance-attribute If set, this text is returned as the final output. #### custom\\_output\\_args class-attribute instance-attribute If set, these args will be passed to the output tool. #### seed class-attribute instance-attribute Seed for generating random data. #### last\\_model\\_request\\_parameters class-attribute instance-attribute The last ModelRequestParameters passed to the model in a request. The ModelRequestParameters contains information about the function and output tools available during request handling. This is set when a request is made, so will reflect the function tools from the last step of the last run. #### model\\_name property The model name. #### system property The model provider. ### TestStreamedResponse dataclass Bases: StreamedResponse A structured response that streams test data. Source code in pydantic_ai_slim/pydantic_ai/models/test.py #### model\\_name property Get the model name of the response. #### provider\\_name property Get the provider name. #### timestamp property Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/models/test/index.html#pydantic_aimodelstest", "page": "api/models/test/index.html", "source_site": "pydantic_ai"}
{"title": "TestModel dataclass", "anchor": "pydantic_ai.models.test.TestModel", "heading_level": 3, "md_text": "Bases: Model A model specifically for testing purposes. This will (by default) call all tools in the agent, then return a tool response if possible, otherwise a plain response. How useful this model is will vary significantly. Apart from __init__ derived by the dataclass decorator, all methods are private or match those of the base class. Source code in pydantic_ai_slim/pydantic_ai/models/test.py #### \\_\\_init\\_\\_ Initialize TestModel with optional settings and profile. Source code in pydantic_ai_slim/pydantic_ai/models/test.py #### call\\_tools class-attribute instance-attribute List of tools to call. If 'all' , all tools will be called. #### custom\\_output\\_text class-attribute instance-attribute If set, this text is returned as the final output. #### custom\\_output\\_args class-attribute instance-attribute If set, these args will be passed to the output tool. #### seed class-attribute instance-attribute Seed for generating random data. #### last\\_model\\_request\\_parameters class-attribute instance-attribute The last ModelRequestParameters passed to the model in a request. The ModelRequestParameters contains information about the function and output tools available during request handling. This is set when a request is made, so will reflect the function tools from the last step of the last run. #### model\\_name property The model name. #### system property The model provider.", "url": "https://ai.pydantic.dev/api/models/test/index.html#pydantic_ai.models.test.TestModel", "page": "api/models/test/index.html", "source_site": "pydantic_ai"}
{"title": "TestStreamedResponse dataclass", "anchor": "pydantic_ai.models.test.TestStreamedResponse", "heading_level": 3, "md_text": "Bases: StreamedResponse A structured response that streams test data. Source code in pydantic_ai_slim/pydantic_ai/models/test.py #### model\\_name property Get the model name of the response. #### provider\\_name property Get the provider name. #### timestamp property Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/models/test/index.html#pydantic_ai.models.test.TestStreamedResponse", "page": "api/models/test/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.output", "anchor": "pydantic_aioutput", "heading_level": 1, "md_text": "### OutputDataT module-attribute Covariant type variable for the output data type of a run. ### ToolOutput dataclass Bases: Generic[OutputDataT] Marker class to use a tool for output and optionally customize the tool. Example: tool\\_output.py Source code in pydantic_ai_slim/pydantic_ai/output.py #### output instance-attribute An output type or function. #### name instance-attribute The name of the tool that will be passed to the model. If not specified and only one output is provided, final_result will be used. If multiple outputs are provided, the name of the output type or function will be added to the tool name. #### description instance-attribute The description of the tool that will be passed to the model. If not specified, the docstring of the output type or function will be used. #### max\\_retries instance-attribute The maximum number of retries for the tool. #### strict instance-attribute Whether to use strict mode for the tool. ### NativeOutput dataclass Bases: Generic[OutputDataT] Marker class to use the model's native structured outputs functionality for outputs and optionally customize the name and description. Example: native\\_output.py Source code in pydantic_ai_slim/pydantic_ai/output.py #### outputs instance-attribute The output types or functions. #### name instance-attribute The name of the structured output that will be passed to the model. If not specified and only one output is provided, the name of the output type or function will be used. #### description instance-attribute The description of the structured output that will be passed to the model. If not specified and only one output is provided, the docstring of the output type or function will be used. #### strict instance-attribute Whether to use strict mode for the output, if the model supports it. ### PromptedOutput dataclass Bases: Generic[OutputDataT] Marker class to use a prompt to tell the model what to output and optionally customize the prompt. Example: prompted\\_output.py Source code in pydantic_ai_slim/pydantic_ai/output.py #### outputs instance-attribute The output types or functions. #### name instance-attribute The name of the structured output that will be passed to the model. If not specified and only one output is provided, the name of the output type or function will be used. #### description instance-attribute The description that will be passed to the model. If not specified and only one output is provided, the docstring of the output type or function will be used. #### template instance-attribute Template for the prompt passed to the model. The '{schema}' placeholder will be replaced with the output JSON schema. If not specified, the default template specified on the model's profile will be used. ### TextOutput dataclass Bases: Generic[OutputDataT] Marker class to use text output for an output function taking a string argument. Example: Source code in pydantic_ai_slim/pydantic_ai/output.py #### output\\_function instance-attribute The function that will be called to process the model's plain text output. The function must take a single string argument. ### StructuredDict Returns a dict[str, Any] subclass with a JSON schema attached that will be used for structured output. Parameters: Example: structured\\_dict.py Source code in pydantic_ai_slim/pydantic_ai/output.py ### DeferredToolRequests dataclass Tool calls that require approval or external execution. This can be used as an agent's output_type and will be used as the output of the agent run if the model called any deferred tools. Results can be passed to the next agent run using a [ DeferredToolResults ](../tools/index.html#pydantic_ai.tools.DeferredToolResults) object with the same tool call IDs. See [deferred tools docs](../../deferred-tools/index.html#deferred-tools) for more information. Source code in pydantic_ai_slim/pydantic_ai/tools.py #### calls class-attribute instance-attribute Tool calls that require external execution. #### approvals class-attribute instance-attribute Tool calls that require human-in-the-loop approval.", "url": "https://ai.pydantic.dev/api/output/index.html#pydantic_aioutput", "page": "api/output/index.html", "source_site": "pydantic_ai"}
{"title": "OutputDataT module-attribute", "anchor": "pydantic_ai.output.OutputDataT", "heading_level": 3, "md_text": "Covariant type variable for the output data type of a run.", "url": "https://ai.pydantic.dev/api/output/index.html#pydantic_ai.output.OutputDataT", "page": "api/output/index.html", "source_site": "pydantic_ai"}
{"title": "ToolOutput dataclass", "anchor": "pydantic_ai.output.ToolOutput", "heading_level": 3, "md_text": "Bases: Generic[OutputDataT] Marker class to use a tool for output and optionally customize the tool. Example: tool\\_output.py Source code in pydantic_ai_slim/pydantic_ai/output.py #### output instance-attribute An output type or function. #### name instance-attribute The name of the tool that will be passed to the model. If not specified and only one output is provided, final_result will be used. If multiple outputs are provided, the name of the output type or function will be added to the tool name. #### description instance-attribute The description of the tool that will be passed to the model. If not specified, the docstring of the output type or function will be used. #### max\\_retries instance-attribute The maximum number of retries for the tool. #### strict instance-attribute Whether to use strict mode for the tool.", "url": "https://ai.pydantic.dev/api/output/index.html#pydantic_ai.output.ToolOutput", "page": "api/output/index.html", "source_site": "pydantic_ai"}
{"title": "NativeOutput dataclass", "anchor": "pydantic_ai.output.NativeOutput", "heading_level": 3, "md_text": "Bases: Generic[OutputDataT] Marker class to use the model's native structured outputs functionality for outputs and optionally customize the name and description. Example: native\\_output.py Source code in pydantic_ai_slim/pydantic_ai/output.py #### outputs instance-attribute The output types or functions. #### name instance-attribute The name of the structured output that will be passed to the model. If not specified and only one output is provided, the name of the output type or function will be used. #### description instance-attribute The description of the structured output that will be passed to the model. If not specified and only one output is provided, the docstring of the output type or function will be used. #### strict instance-attribute Whether to use strict mode for the output, if the model supports it.", "url": "https://ai.pydantic.dev/api/output/index.html#pydantic_ai.output.NativeOutput", "page": "api/output/index.html", "source_site": "pydantic_ai"}
{"title": "PromptedOutput dataclass", "anchor": "pydantic_ai.output.PromptedOutput", "heading_level": 3, "md_text": "Bases: Generic[OutputDataT] Marker class to use a prompt to tell the model what to output and optionally customize the prompt. Example: prompted\\_output.py Source code in pydantic_ai_slim/pydantic_ai/output.py #### outputs instance-attribute The output types or functions. #### name instance-attribute The name of the structured output that will be passed to the model. If not specified and only one output is provided, the name of the output type or function will be used. #### description instance-attribute The description that will be passed to the model. If not specified and only one output is provided, the docstring of the output type or function will be used. #### template instance-attribute Template for the prompt passed to the model. The '{schema}' placeholder will be replaced with the output JSON schema. If not specified, the default template specified on the model's profile will be used.", "url": "https://ai.pydantic.dev/api/output/index.html#pydantic_ai.output.PromptedOutput", "page": "api/output/index.html", "source_site": "pydantic_ai"}
{"title": "TextOutput dataclass", "anchor": "pydantic_ai.output.TextOutput", "heading_level": 3, "md_text": "Bases: Generic[OutputDataT] Marker class to use text output for an output function taking a string argument. Example: Source code in pydantic_ai_slim/pydantic_ai/output.py #### output\\_function instance-attribute The function that will be called to process the model's plain text output. The function must take a single string argument.", "url": "https://ai.pydantic.dev/api/output/index.html#pydantic_ai.output.TextOutput", "page": "api/output/index.html", "source_site": "pydantic_ai"}
{"title": "StructuredDict", "anchor": "pydantic_ai.output.StructuredDict", "heading_level": 3, "md_text": "Returns a dict[str, Any] subclass with a JSON schema attached that will be used for structured output. Parameters: Example: structured\\_dict.py Source code in pydantic_ai_slim/pydantic_ai/output.py", "url": "https://ai.pydantic.dev/api/output/index.html#pydantic_ai.output.StructuredDict", "page": "api/output/index.html", "source_site": "pydantic_ai"}
{"title": "DeferredToolRequests dataclass", "anchor": "pydantic_ai.output.DeferredToolRequests", "heading_level": 3, "md_text": "Tool calls that require approval or external execution. This can be used as an agent's output_type and will be used as the output of the agent run if the model called any deferred tools. Results can be passed to the next agent run using a [ DeferredToolResults ](../tools/index.html#pydantic_ai.tools.DeferredToolResults) object with the same tool call IDs. See [deferred tools docs](../../deferred-tools/index.html#deferred-tools) for more information. Source code in pydantic_ai_slim/pydantic_ai/tools.py #### calls class-attribute instance-attribute Tool calls that require external execution. #### approvals class-attribute instance-attribute Tool calls that require human-in-the-loop approval.", "url": "https://ai.pydantic.dev/api/output/index.html#pydantic_ai.output.DeferredToolRequests", "page": "api/output/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.models.mistral", "anchor": "pydantic_aimodelsmistral", "heading_level": 1, "md_text": "## Setup For details on how to set up authentication with this model, see [model configuration for Mistral](../../../models/mistral/index.html). ### LatestMistralModelNames module-attribute Latest Mistral models. ### MistralModelName module-attribute Possible Mistral model names. Since Mistral supports a variety of date-stamped models, we explicitly list the most popular models but allow any name in the type hints. Since [the Mistral docs](https://docs.mistral.ai/getting-started/models/models_overview/) for a full list. ### MistralModelSettings Bases: ModelSettings Settings used for a Mistral model request. Source code in pydantic_ai_slim/pydantic_ai/models/mistral.py ### MistralModel dataclass Bases: Model A model that uses Mistral. Internally, this uses the [Mistral Python client](https://github.com/mistralai/client-python) to interact with the API. [API Documentation](https://docs.mistral.ai/) Source code in pydantic_ai_slim/pydantic_ai/models/mistral.py #### \\_\\_init\\_\\_ \\n{schema}\\n Initialize a Mistral model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/mistral.py #### model\\_name property The model name. #### system property The model provider. #### request async Make a non-streaming request to the model from Pydantic AI call. Source code in pydantic_ai_slim/pydantic_ai/models/mistral.py #### request\\_stream async Make a streaming request to the model from Pydantic AI call. Source code in pydantic_ai_slim/pydantic_ai/models/mistral.py ### MistralStreamedResponse dataclass Bases: StreamedResponse Implementation of StreamedResponse for Mistral models. Source code in pydantic_ai_slim/pydantic_ai/models/mistral.py #### model\\_name property Get the model name of the response. #### provider\\_name property Get the provider name. #### timestamp property Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/models/mistral/index.html#pydantic_aimodelsmistral", "page": "api/models/mistral/index.html", "source_site": "pydantic_ai"}
{"title": "Setup", "anchor": "setup", "heading_level": 2, "md_text": "For details on how to set up authentication with this model, see [model configuration for Mistral](../../../models/mistral/index.html). ### LatestMistralModelNames module-attribute Latest Mistral models. ### MistralModelName module-attribute Possible Mistral model names. Since Mistral supports a variety of date-stamped models, we explicitly list the most popular models but allow any name in the type hints. Since [the Mistral docs](https://docs.mistral.ai/getting-started/models/models_overview/) for a full list. ### MistralModelSettings Bases: ModelSettings Settings used for a Mistral model request. Source code in pydantic_ai_slim/pydantic_ai/models/mistral.py ### MistralModel dataclass Bases: Model A model that uses Mistral. Internally, this uses the [Mistral Python client](https://github.com/mistralai/client-python) to interact with the API. [API Documentation](https://docs.mistral.ai/) Source code in pydantic_ai_slim/pydantic_ai/models/mistral.py #### \\_\\_init\\_\\_ \\n{schema}\\n Initialize a Mistral model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/mistral.py #### model\\_name property The model name. #### system property The model provider. #### request async Make a non-streaming request to the model from Pydantic AI call. Source code in pydantic_ai_slim/pydantic_ai/models/mistral.py #### request\\_stream async Make a streaming request to the model from Pydantic AI call. Source code in pydantic_ai_slim/pydantic_ai/models/mistral.py ### MistralStreamedResponse dataclass Bases: StreamedResponse Implementation of StreamedResponse for Mistral models. Source code in pydantic_ai_slim/pydantic_ai/models/mistral.py #### model\\_name property Get the model name of the response. #### provider\\_name property Get the provider name. #### timestamp property Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/models/mistral/index.html#setup", "page": "api/models/mistral/index.html", "source_site": "pydantic_ai"}
{"title": "LatestMistralModelNames module-attribute", "anchor": "pydantic_ai.models.mistral.LatestMistralModelNames", "heading_level": 3, "md_text": "Latest Mistral models.", "url": "https://ai.pydantic.dev/api/models/mistral/index.html#pydantic_ai.models.mistral.LatestMistralModelNames", "page": "api/models/mistral/index.html", "source_site": "pydantic_ai"}
{"title": "MistralModelName module-attribute", "anchor": "pydantic_ai.models.mistral.MistralModelName", "heading_level": 3, "md_text": "Possible Mistral model names. Since Mistral supports a variety of date-stamped models, we explicitly list the most popular models but allow any name in the type hints. Since [the Mistral docs](https://docs.mistral.ai/getting-started/models/models_overview/) for a full list.", "url": "https://ai.pydantic.dev/api/models/mistral/index.html#pydantic_ai.models.mistral.MistralModelName", "page": "api/models/mistral/index.html", "source_site": "pydantic_ai"}
{"title": "MistralModelSettings", "anchor": "pydantic_ai.models.mistral.MistralModelSettings", "heading_level": 3, "md_text": "Bases: ModelSettings Settings used for a Mistral model request. Source code in pydantic_ai_slim/pydantic_ai/models/mistral.py", "url": "https://ai.pydantic.dev/api/models/mistral/index.html#pydantic_ai.models.mistral.MistralModelSettings", "page": "api/models/mistral/index.html", "source_site": "pydantic_ai"}
{"title": "MistralModel dataclass", "anchor": "pydantic_ai.models.mistral.MistralModel", "heading_level": 3, "md_text": "Bases: Model A model that uses Mistral. Internally, this uses the [Mistral Python client](https://github.com/mistralai/client-python) to interact with the API. [API Documentation](https://docs.mistral.ai/) Source code in pydantic_ai_slim/pydantic_ai/models/mistral.py #### \\_\\_init\\_\\_ \\n{schema}\\n Initialize a Mistral model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/mistral.py #### model\\_name property The model name. #### system property The model provider. #### request async Make a non-streaming request to the model from Pydantic AI call. Source code in pydantic_ai_slim/pydantic_ai/models/mistral.py #### request\\_stream async Make a streaming request to the model from Pydantic AI call. Source code in pydantic_ai_slim/pydantic_ai/models/mistral.py", "url": "https://ai.pydantic.dev/api/models/mistral/index.html#pydantic_ai.models.mistral.MistralModel", "page": "api/models/mistral/index.html", "source_site": "pydantic_ai"}
{"title": "MistralStreamedResponse dataclass", "anchor": "pydantic_ai.models.mistral.MistralStreamedResponse", "heading_level": 3, "md_text": "Bases: StreamedResponse Implementation of StreamedResponse for Mistral models. Source code in pydantic_ai_slim/pydantic_ai/models/mistral.py #### model\\_name property Get the model name of the response. #### provider\\_name property Get the provider name. #### timestamp property Get the timestamp of the response.", "url": "https://ai.pydantic.dev/api/models/mistral/index.html#pydantic_ai.models.mistral.MistralStreamedResponse", "page": "api/models/mistral/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.profiles", "anchor": "pydantic_aiprofiles", "heading_level": 1, "md_text": "Describes how requests to and responses from specific models or families of models need to be constructed and processed to get the best results, independent of the model and provider classes used. Source code in pydantic_ai_slim/pydantic_ai/profiles/__init__.py ### supports\\_tools class-attribute instance-attribute Whether the model supports tools. ### supports\\_json\\_schema\\_output class-attribute instance-attribute Whether the model supports JSON schema output. ### supports\\_json\\_object\\_output class-attribute instance-attribute Whether the model supports JSON object output. ### supports\\_image\\_output class-attribute instance-attribute Whether the model supports image output. ### default\\_structured\\_output\\_mode class-attribute instance-attribute The default structured output mode to use for the model. ### prompted\\_output\\_template class-attribute instance-attribute The instructions template to use for prompted structured output. The '{schema}' placeholder will be replaced with the JSON schema for the output. ### json\\_schema\\_transformer class-attribute instance-attribute The transformer to use to make JSON schemas for tools and structured output compatible with the model. ### thinking\\_tags class-attribute instance-attribute The tags used to indicate thinking parts in the model's output. Defaults to ('', ''). ### ignore\\_streamed\\_leading\\_whitespace class-attribute instance-attribute Whether to ignore leading whitespace when streaming a response. or an empty text part ahead of tool calls (e.g. Ollama + Qwen3), which we don't want to end up treating as a final result when using run\\_stream with str a valid output\\_type . ### from\\_profile classmethod Build a ModelProfile subclass instance from a ModelProfile instance. Source code in pydantic_ai_slim/pydantic_ai/profiles/__init__.py ### update Update this ModelProfile (subclass) instance with the non-default values from another ModelProfile instance. Source code in pydantic_ai_slim/pydantic_ai/profiles/__init__.py ### OpenAIModelProfile dataclass Bases: ModelProfile Profile for models used with OpenAIChatModel . ALL FIELDS MUST BE openai_ PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS. Source code in pydantic_ai_slim/pydantic_ai/profiles/openai.py #### openai\\_supports\\_strict\\_tool\\_definition class-attribute instance-attribute This can be set by a provider or user if the OpenAI-\"compatible\" API doesn't support strict tool definitions. #### openai\\_supports\\_sampling\\_settings class-attribute instance-attribute Turn off to don't send sampling settings like temperature and top_p to models that don't support them, like OpenAI's o-series reasoning models. #### openai\\_unsupported\\_model\\_settings class-attribute instance-attribute A list of model settings that are not supported by this model. #### openai\\_supports\\_tool\\_choice\\_required class-attribute instance-attribute Whether the provider accepts the value tool_choice='required' in the request payload. #### openai\\_system\\_prompt\\_role class-attribute instance-attribute The role to use for the system prompt message. If not provided, defaults to 'system' . #### openai\\_chat\\_supports\\_web\\_search class-attribute instance-attribute Whether the model supports web search in Chat Completions API. #### openai\\_supports\\_encrypted\\_reasoning\\_content class-attribute instance-attribute Whether the model supports including encrypted reasoning content in the response. #### openai\\_responses\\_requires\\_function\\_call\\_status\\_none class-attribute instance-attribute Whether the Responses API requires the status field on function tool calls to be None . This is required by vLLM Responses API versions before https://github.com/vllm-project/vllm/pull/26706. See https://github.com/pydantic/pydantic-ai/issues/3245 for more details. ### openai\\_model\\_profile Get the model profile for an OpenAI model. Source code in pydantic_ai_slim/pydantic_ai/profiles/openai.py ### OpenAIJsonSchemaTransformer dataclass Bases: JsonSchemaTransformer Recursively handle the schema to make it compatible with OpenAI strict mode. See https://platform.openai.com/docs/guides/function-calling?api-mode=responses#strict-mode for more details, but this basically just requires: \\* additionalProperties must be set to false for each object in the parameters \\* all fields in properties must be marked as required Source code in pydantic_ai_slim/pydantic_ai/profiles/openai.py ### anthropic\\_model\\_profile Get the model profile for an Anthropic model. Source code in pydantic_ai_slim/pydantic_ai/profiles/anthropic.py ### google\\_model\\_profile Get the model profile for a Google model. Source code in pydantic_ai_slim/pydantic_ai/profiles/google.py ### GoogleJsonSchemaTransformer Bases: JsonSchemaTransformer Transforms the JSON Schema from Pydantic to be suitable for Gemini. Gemini which [supports](https://ai.google.dev/gemini-api/docs/function-calling#function_declarations) a subset of OpenAPI v3.0.3. Specifically: \\* gemini doesn't allow the title keyword to be set \\* gemini doesn't allow $defs  we need to inline the definitions where possible Source code in pydantic_ai_slim/pydantic_ai/profiles/google.py ### meta\\_model\\_profile Get the model profile for a Meta model. Source code in pydantic_ai_slim/pydantic_ai/profiles/meta.py ### amazon\\_model\\_profile Get the model profile for an Amazon model. Source code in pydantic_ai_slim/pydantic_ai/profiles/amazon.py ### deepseek\\_model\\_profile Get the model profile for a DeepSeek model. Source code in pydantic_ai_slim/pydantic_ai/profiles/deepseek.py ### grok\\_model\\_profile Get the model profile for a Grok model. Source code in pydantic_ai_slim/pydantic_ai/profiles/grok.py ### mistral\\_model\\_profile Get the model profile for a Mistral model. Source code in pydantic_ai_slim/pydantic_ai/profiles/mistral.py ### qwen\\_model\\_profile Get the model profile for a Qwen model. Source code in pydantic_ai_slim/pydantic_ai/profiles/qwen.py", "url": "https://ai.pydantic.dev/api/profiles/index.html#pydantic_aiprofiles", "page": "api/profiles/index.html", "source_site": "pydantic_ai"}
{"title": "supports_tools class-attribute instance-attribute", "anchor": "pydantic_ai.profiles.ModelProfile.supports_tools", "heading_level": 3, "md_text": "Whether the model supports tools.", "url": "https://ai.pydantic.dev/api/profiles/index.html#pydantic_ai.profiles.ModelProfile.supports_tools", "page": "api/profiles/index.html", "source_site": "pydantic_ai"}
{"title": "supports_json_schema_output class-attribute instance-attribute", "anchor": "pydantic_ai.profiles.ModelProfile.supports_json_schema_output", "heading_level": 3, "md_text": "Whether the model supports JSON schema output.", "url": "https://ai.pydantic.dev/api/profiles/index.html#pydantic_ai.profiles.ModelProfile.supports_json_schema_output", "page": "api/profiles/index.html", "source_site": "pydantic_ai"}
{"title": "supports_json_object_output class-attribute instance-attribute", "anchor": "pydantic_ai.profiles.ModelProfile.supports_json_object_output", "heading_level": 3, "md_text": "Whether the model supports JSON object output.", "url": "https://ai.pydantic.dev/api/profiles/index.html#pydantic_ai.profiles.ModelProfile.supports_json_object_output", "page": "api/profiles/index.html", "source_site": "pydantic_ai"}
{"title": "supports_image_output class-attribute instance-attribute", "anchor": "pydantic_ai.profiles.ModelProfile.supports_image_output", "heading_level": 3, "md_text": "Whether the model supports image output.", "url": "https://ai.pydantic.dev/api/profiles/index.html#pydantic_ai.profiles.ModelProfile.supports_image_output", "page": "api/profiles/index.html", "source_site": "pydantic_ai"}
{"title": "default_structured_output_mode class-attribute instance-attribute", "anchor": "pydantic_ai.profiles.ModelProfile.default_structured_output_mode", "heading_level": 3, "md_text": "The default structured output mode to use for the model.", "url": "https://ai.pydantic.dev/api/profiles/index.html#pydantic_ai.profiles.ModelProfile.default_structured_output_mode", "page": "api/profiles/index.html", "source_site": "pydantic_ai"}
{"title": "prompted_output_template class-attribute instance-attribute", "anchor": "pydantic_ai.profiles.ModelProfile.prompted_output_template", "heading_level": 3, "md_text": "The instructions template to use for prompted structured output. The '{schema}' placeholder will be replaced with the JSON schema for the output.", "url": "https://ai.pydantic.dev/api/profiles/index.html#pydantic_ai.profiles.ModelProfile.prompted_output_template", "page": "api/profiles/index.html", "source_site": "pydantic_ai"}
{"title": "json_schema_transformer class-attribute instance-attribute", "anchor": "pydantic_ai.profiles.ModelProfile.json_schema_transformer", "heading_level": 3, "md_text": "The transformer to use to make JSON schemas for tools and structured output compatible with the model.", "url": "https://ai.pydantic.dev/api/profiles/index.html#pydantic_ai.profiles.ModelProfile.json_schema_transformer", "page": "api/profiles/index.html", "source_site": "pydantic_ai"}
{"title": "thinking_tags class-attribute instance-attribute", "anchor": "pydantic_ai.profiles.ModelProfile.thinking_tags", "heading_level": 3, "md_text": "The tags used to indicate thinking parts in the model's output. Defaults to ('', '').", "url": "https://ai.pydantic.dev/api/profiles/index.html#pydantic_ai.profiles.ModelProfile.thinking_tags", "page": "api/profiles/index.html", "source_site": "pydantic_ai"}
{"title": "ignore_streamed_leading_whitespace class-attribute instance-attribute", "anchor": "pydantic_ai.profiles.ModelProfile.ignore_streamed_leading_whitespace", "heading_level": 3, "md_text": "Whether to ignore leading whitespace when streaming a response. or an empty text part ahead of tool calls (e.g. Ollama + Qwen3), which we don't want to end up treating as a final result when using run\\_stream with str a valid output\\_type .", "url": "https://ai.pydantic.dev/api/profiles/index.html#pydantic_ai.profiles.ModelProfile.ignore_streamed_leading_whitespace", "page": "api/profiles/index.html", "source_site": "pydantic_ai"}
{"title": "from_profile classmethod", "anchor": "pydantic_ai.profiles.ModelProfile.from_profile", "heading_level": 3, "md_text": "Build a ModelProfile subclass instance from a ModelProfile instance. Source code in pydantic_ai_slim/pydantic_ai/profiles/__init__.py", "url": "https://ai.pydantic.dev/api/profiles/index.html#pydantic_ai.profiles.ModelProfile.from_profile", "page": "api/profiles/index.html", "source_site": "pydantic_ai"}
{"title": "update", "anchor": "pydantic_ai.profiles.ModelProfile.update", "heading_level": 3, "md_text": "Update this ModelProfile (subclass) instance with the non-default values from another ModelProfile instance. Source code in pydantic_ai_slim/pydantic_ai/profiles/__init__.py", "url": "https://ai.pydantic.dev/api/profiles/index.html#pydantic_ai.profiles.ModelProfile.update", "page": "api/profiles/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIModelProfile dataclass", "anchor": "pydantic_ai.profiles.openai.OpenAIModelProfile", "heading_level": 3, "md_text": "Bases: ModelProfile Profile for models used with OpenAIChatModel . ALL FIELDS MUST BE openai_ PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS. Source code in pydantic_ai_slim/pydantic_ai/profiles/openai.py #### openai\\_supports\\_strict\\_tool\\_definition class-attribute instance-attribute This can be set by a provider or user if the OpenAI-\"compatible\" API doesn't support strict tool definitions. #### openai\\_supports\\_sampling\\_settings class-attribute instance-attribute Turn off to don't send sampling settings like temperature and top_p to models that don't support them, like OpenAI's o-series reasoning models. #### openai\\_unsupported\\_model\\_settings class-attribute instance-attribute A list of model settings that are not supported by this model. #### openai\\_supports\\_tool\\_choice\\_required class-attribute instance-attribute Whether the provider accepts the value tool_choice='required' in the request payload. #### openai\\_system\\_prompt\\_role class-attribute instance-attribute The role to use for the system prompt message. If not provided, defaults to 'system' . #### openai\\_chat\\_supports\\_web\\_search class-attribute instance-attribute Whether the model supports web search in Chat Completions API. #### openai\\_supports\\_encrypted\\_reasoning\\_content class-attribute instance-attribute Whether the model supports including encrypted reasoning content in the response. #### openai\\_responses\\_requires\\_function\\_call\\_status\\_none class-attribute instance-attribute Whether the Responses API requires the status field on function tool calls to be None . This is required by vLLM Responses API versions before https://github.com/vllm-project/vllm/pull/26706. See https://github.com/pydantic/pydantic-ai/issues/3245 for more details.", "url": "https://ai.pydantic.dev/api/profiles/index.html#pydantic_ai.profiles.openai.OpenAIModelProfile", "page": "api/profiles/index.html", "source_site": "pydantic_ai"}
{"title": "openai_model_profile", "anchor": "pydantic_ai.profiles.openai.openai_model_profile", "heading_level": 3, "md_text": "Get the model profile for an OpenAI model. Source code in pydantic_ai_slim/pydantic_ai/profiles/openai.py", "url": "https://ai.pydantic.dev/api/profiles/index.html#pydantic_ai.profiles.openai.openai_model_profile", "page": "api/profiles/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIJsonSchemaTransformer dataclass", "anchor": "pydantic_ai.profiles.openai.OpenAIJsonSchemaTransformer", "heading_level": 3, "md_text": "Bases: JsonSchemaTransformer Recursively handle the schema to make it compatible with OpenAI strict mode. See https://platform.openai.com/docs/guides/function-calling?api-mode=responses#strict-mode for more details, but this basically just requires: \\* additionalProperties must be set to false for each object in the parameters \\* all fields in properties must be marked as required Source code in pydantic_ai_slim/pydantic_ai/profiles/openai.py", "url": "https://ai.pydantic.dev/api/profiles/index.html#pydantic_ai.profiles.openai.OpenAIJsonSchemaTransformer", "page": "api/profiles/index.html", "source_site": "pydantic_ai"}
{"title": "anthropic_model_profile", "anchor": "pydantic_ai.profiles.anthropic.anthropic_model_profile", "heading_level": 3, "md_text": "Get the model profile for an Anthropic model. Source code in pydantic_ai_slim/pydantic_ai/profiles/anthropic.py", "url": "https://ai.pydantic.dev/api/profiles/index.html#pydantic_ai.profiles.anthropic.anthropic_model_profile", "page": "api/profiles/index.html", "source_site": "pydantic_ai"}
{"title": "google_model_profile", "anchor": "pydantic_ai.profiles.google.google_model_profile", "heading_level": 3, "md_text": "Get the model profile for a Google model. Source code in pydantic_ai_slim/pydantic_ai/profiles/google.py", "url": "https://ai.pydantic.dev/api/profiles/index.html#pydantic_ai.profiles.google.google_model_profile", "page": "api/profiles/index.html", "source_site": "pydantic_ai"}
{"title": "GoogleJsonSchemaTransformer", "anchor": "pydantic_ai.profiles.google.GoogleJsonSchemaTransformer", "heading_level": 3, "md_text": "Bases: JsonSchemaTransformer Transforms the JSON Schema from Pydantic to be suitable for Gemini. Gemini which [supports](https://ai.google.dev/gemini-api/docs/function-calling#function_declarations) a subset of OpenAPI v3.0.3. Specifically: \\* gemini doesn't allow the title keyword to be set \\* gemini doesn't allow $defs  we need to inline the definitions where possible Source code in pydantic_ai_slim/pydantic_ai/profiles/google.py", "url": "https://ai.pydantic.dev/api/profiles/index.html#pydantic_ai.profiles.google.GoogleJsonSchemaTransformer", "page": "api/profiles/index.html", "source_site": "pydantic_ai"}
{"title": "meta_model_profile", "anchor": "pydantic_ai.profiles.meta.meta_model_profile", "heading_level": 3, "md_text": "Get the model profile for a Meta model. Source code in pydantic_ai_slim/pydantic_ai/profiles/meta.py", "url": "https://ai.pydantic.dev/api/profiles/index.html#pydantic_ai.profiles.meta.meta_model_profile", "page": "api/profiles/index.html", "source_site": "pydantic_ai"}
{"title": "amazon_model_profile", "anchor": "pydantic_ai.profiles.amazon.amazon_model_profile", "heading_level": 3, "md_text": "Get the model profile for an Amazon model. Source code in pydantic_ai_slim/pydantic_ai/profiles/amazon.py", "url": "https://ai.pydantic.dev/api/profiles/index.html#pydantic_ai.profiles.amazon.amazon_model_profile", "page": "api/profiles/index.html", "source_site": "pydantic_ai"}
{"title": "deepseek_model_profile", "anchor": "pydantic_ai.profiles.deepseek.deepseek_model_profile", "heading_level": 3, "md_text": "Get the model profile for a DeepSeek model. Source code in pydantic_ai_slim/pydantic_ai/profiles/deepseek.py", "url": "https://ai.pydantic.dev/api/profiles/index.html#pydantic_ai.profiles.deepseek.deepseek_model_profile", "page": "api/profiles/index.html", "source_site": "pydantic_ai"}
{"title": "grok_model_profile", "anchor": "pydantic_ai.profiles.grok.grok_model_profile", "heading_level": 3, "md_text": "Get the model profile for a Grok model. Source code in pydantic_ai_slim/pydantic_ai/profiles/grok.py", "url": "https://ai.pydantic.dev/api/profiles/index.html#pydantic_ai.profiles.grok.grok_model_profile", "page": "api/profiles/index.html", "source_site": "pydantic_ai"}
{"title": "mistral_model_profile", "anchor": "pydantic_ai.profiles.mistral.mistral_model_profile", "heading_level": 3, "md_text": "Get the model profile for a Mistral model. Source code in pydantic_ai_slim/pydantic_ai/profiles/mistral.py", "url": "https://ai.pydantic.dev/api/profiles/index.html#pydantic_ai.profiles.mistral.mistral_model_profile", "page": "api/profiles/index.html", "source_site": "pydantic_ai"}
{"title": "qwen_model_profile", "anchor": "pydantic_ai.profiles.qwen.qwen_model_profile", "heading_level": 3, "md_text": "Get the model profile for a Qwen model. Source code in pydantic_ai_slim/pydantic_ai/profiles/qwen.py", "url": "https://ai.pydantic.dev/api/profiles/index.html#pydantic_ai.profiles.qwen.qwen_model_profile", "page": "api/profiles/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_evals.evaluators", "anchor": "pydantic_evalsevaluators", "heading_level": 1, "md_text": "### Contains dataclass Bases: Evaluator[object, object, object] Check if the output contains the expected output. For strings, checks if expected\\_output is a substring of output. For lists/tuples, checks if expected\\_output is in output. For dicts, checks if all key-value pairs in expected\\_output are in output. Note: case\\_sensitive only applies when both the value and output are strings. Source code in pydantic_evals/pydantic_evals/evaluators/common.py ### Equals dataclass Bases: Evaluator[object, object, object] Check if the output exactly equals the provided value. Source code in pydantic_evals/pydantic_evals/evaluators/common.py ### EqualsExpected dataclass Bases: Evaluator[object, object, object] Check if the output exactly equals the expected output. Source code in pydantic_evals/pydantic_evals/evaluators/common.py ### HasMatchingSpan dataclass Bases: Evaluator[object, object, object] Check if the span tree contains a span that matches the specified query. Source code in pydantic_evals/pydantic_evals/evaluators/common.py ### IsInstance dataclass Bases: Evaluator[object, object, object] Check if the output is an instance of a type with the given name. Source code in pydantic_evals/pydantic_evals/evaluators/common.py ### LLMJudge dataclass Bases: Evaluator[object, object, object] Judge whether the output of a language model meets the criteria of a provided rubric. If you do not specify a model, it uses the default model for judging. This starts as 'openai:gpt-4o', but can be overridden by calling [ set_default_judge_model ](index.html#pydantic_evals.evaluators.llm_as_a_judge.set_default_judge_model). Source code in pydantic_evals/pydantic_evals/evaluators/common.py ### MaxDuration dataclass Bases: Evaluator[object, object, object] Check if the execution time is under the specified maximum. Source code in pydantic_evals/pydantic_evals/evaluators/common.py ### OutputConfig Bases: TypedDict Configuration for the score and assertion outputs of the LLMJudge evaluator. Source code in pydantic_evals/pydantic_evals/evaluators/common.py ### EvaluatorContext dataclass Bases: Generic[InputsT, OutputT, MetadataT] Context for evaluating a task execution. An instance of this class is the sole input to all Evaluators. It contains all the information needed to evaluate the task execution, including inputs, outputs, metadata, and telemetry data. Evaluators use this context to access the task inputs, actual output, expected output, and other information when evaluating the result of the task execution. Example: Source code in pydantic_evals/pydantic_evals/evaluators/context.py #### name instance-attribute The name of the case. #### inputs instance-attribute The inputs provided to the task for this case. #### metadata instance-attribute Metadata associated with the case, if provided. May be None if no metadata was specified. #### expected\\_output instance-attribute The expected output for the case, if provided. May be None if no expected output was specified. #### output instance-attribute The actual output produced by the task for this case. #### duration instance-attribute The duration of the task run for this case. #### attributes instance-attribute Attributes associated with the task run for this case. These can be set by calling pydantic_evals.dataset.set_eval_attribute in any code executed during the evaluation task. #### metrics instance-attribute Metrics associated with the task run for this case. These can be set by calling pydantic_evals.dataset.increment_eval_metric in any code executed during the evaluation task. #### span\\_tree property Get the SpanTree for this task execution. The span tree is a graph where each node corresponds to an OpenTelemetry span recorded during the task execution, including timing information and any custom spans created during execution. Returns: Raises: ### EvaluationReason dataclass The result of running an evaluator with an optional explanation. Contains a scalar value and an optional \"reason\" explaining the value. Parameters: Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py ### EvaluationResult dataclass Bases: Generic[EvaluationScalarT] The details of an individual evaluation result. Contains the name, value, reason, and source evaluator for a single evaluation. Parameters: Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py #### downcast Attempt to downcast this result to a more specific type. Parameters: Returns: Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py ### Evaluator dataclass Bases: Generic[InputsT, OutputT, MetadataT] Base class for all evaluators. Evaluators can assess the performance of a task in a variety of ways, as a function of the EvaluatorContext. Subclasses must implement the evaluate method. Note it can be defined with either def or async def . Example: Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py #### get\\_serialization\\_name classmethod Return the 'name' of this Evaluator to use during serialization. Returns: Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py #### name classmethod deprecated Deprecated name has been renamed, use get_serialization_name instead. name has been renamed, use get_serialization_name instead. Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py #### get\\_default\\_evaluation\\_name Return the default name to use in reports for the output of this evaluator. By default, if the evaluator has an attribute called evaluation_name of type string, that will be used. Otherwise, the serialization name of the evaluator (which is usually the class name) will be used. This can be overridden to get a more descriptive name in evaluation reports, e.g. using instance information. Note that evaluators that return a mapping of results will always use the keys of that mapping as the names of the associated evaluation results. Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py #### evaluate abstractmethod Evaluate the task output in the given context. This is the main evaluation method that subclasses must implement. It can be either synchronous or asynchronous, returning either an EvaluatorOutput directly or an Awaitable[EvaluatorOutput]. Parameters: Returns: Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py #### evaluate\\_sync Run the evaluator synchronously, handling both sync and async implementations. This method ensures synchronous execution by running any async evaluate implementation to completion using run\\_until\\_complete. Parameters: Returns: Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py #### evaluate\\_async async Run the evaluator asynchronously, handling both sync and async implementations. This method ensures asynchronous execution by properly awaiting any async evaluate implementation. For synchronous implementations, it returns the result directly. Parameters: Returns: Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py #### serialize Serialize this Evaluator to a JSON-serializable form. Returns: Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py #### build\\_serialization\\_arguments Build the arguments for serialization. Evaluators are serialized for inclusion as the \"source\" in an EvaluationResult . If you want to modify how the evaluator is serialized for that or other purposes, you can override this method. Returns: Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py ### EvaluatorFailure dataclass Represents a failure raised during the execution of an evaluator. Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py ### EvaluatorOutput module-attribute Type for the output of an evaluator, which can be a scalar, an EvaluationReason, or a mapping of names to either. ### EvaluatorSpec Bases: BaseModel The specification of an evaluator to be run. This class is used to represent evaluators in a serializable format, supporting various short forms for convenience when defining evaluators in YAML or JSON dataset files. In particular, each of the following forms is supported for specifying an evaluator with name MyEvaluator : \\* 'MyEvaluator' - Just the (string) name of the Evaluator subclass is used if its __init__ takes no arguments \\* {'MyEvaluator': first_arg} - A single argument is passed as the first positional argument to MyEvaluator.__init__ \\* {'MyEvaluator': {k1: v1, k2: v2}} - Multiple kwargs are passed to MyEvaluator.__init__ Source code in pydantic_evals/pydantic_evals/evaluators/spec.py #### name instance-attribute The name of the evaluator class; should be the value returned by EvaluatorClass.get_serialization_name() #### arguments instance-attribute The arguments to pass to the evaluator's constructor. Can be None (no arguments), a tuple (a single positional argument), or a dict (keyword arguments). #### args property Get the positional arguments for the evaluator. Returns: #### kwargs property Get the keyword arguments for the evaluator. Returns: #### deserialize classmethod Deserialize an EvaluatorSpec from various formats. This validator handles the various short forms of evaluator specifications, converting them to a consistent EvaluatorSpec instance. Parameters: Returns: Raises: Source code in pydantic_evals/pydantic_evals/evaluators/spec.py #### serialize Serialize using the appropriate short-form if possible. Returns: Source code in pydantic_evals/pydantic_evals/evaluators/spec.py ### GradingOutput Bases: BaseModel The output of a grading operation. Source code in pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py ### judge\\_output async Judge the output of a model based on a rubric. If the model is not specified, a default model is used. The default model starts as 'openai:gpt-4o', but this can be changed using the set_default_judge_model function. Source code in pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py ### judge\\_input\\_output async Judge the output of a model based on the inputs and a rubric. If the model is not specified, a default model is used. The default model starts as 'openai:gpt-4o', but this can be changed using the set_default_judge_model function. Source code in pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py ### judge\\_input\\_output\\_expected async Judge the output of a model based on the inputs and a rubric. If the model is not specified, a default model is used. The default model starts as 'openai:gpt-4o', but this can be changed using the set_default_judge_model function. Source code in pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py ### judge\\_output\\_expected async Judge the output of a model based on the expected output, output, and a rubric. If the model is not specified, a default model is used. The default model starts as 'openai:gpt-4o', but this can be changed using the set_default_judge_model function. Source code in pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py ### set\\_default\\_judge\\_model Set the default model used for judging. This model is used if None is passed to the model argument of judge_output and judge_input_output . Source code in pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/evaluators/index.html#pydantic_evalsevaluators", "page": "api/pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "Contains dataclass", "anchor": "pydantic_evals.evaluators.Contains", "heading_level": 3, "md_text": "Bases: Evaluator[object, object, object] Check if the output contains the expected output. For strings, checks if expected\\_output is a substring of output. For lists/tuples, checks if expected\\_output is in output. For dicts, checks if all key-value pairs in expected\\_output are in output. Note: case\\_sensitive only applies when both the value and output are strings. Source code in pydantic_evals/pydantic_evals/evaluators/common.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.Contains", "page": "api/pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "Equals dataclass", "anchor": "pydantic_evals.evaluators.Equals", "heading_level": 3, "md_text": "Bases: Evaluator[object, object, object] Check if the output exactly equals the provided value. Source code in pydantic_evals/pydantic_evals/evaluators/common.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.Equals", "page": "api/pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "EqualsExpected dataclass", "anchor": "pydantic_evals.evaluators.EqualsExpected", "heading_level": 3, "md_text": "Bases: Evaluator[object, object, object] Check if the output exactly equals the expected output. Source code in pydantic_evals/pydantic_evals/evaluators/common.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EqualsExpected", "page": "api/pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "HasMatchingSpan dataclass", "anchor": "pydantic_evals.evaluators.HasMatchingSpan", "heading_level": 3, "md_text": "Bases: Evaluator[object, object, object] Check if the span tree contains a span that matches the specified query. Source code in pydantic_evals/pydantic_evals/evaluators/common.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.HasMatchingSpan", "page": "api/pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "IsInstance dataclass", "anchor": "pydantic_evals.evaluators.IsInstance", "heading_level": 3, "md_text": "Bases: Evaluator[object, object, object] Check if the output is an instance of a type with the given name. Source code in pydantic_evals/pydantic_evals/evaluators/common.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.IsInstance", "page": "api/pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "LLMJudge dataclass", "anchor": "pydantic_evals.evaluators.LLMJudge", "heading_level": 3, "md_text": "Bases: Evaluator[object, object, object] Judge whether the output of a language model meets the criteria of a provided rubric. If you do not specify a model, it uses the default model for judging. This starts as 'openai:gpt-4o', but can be overridden by calling [ set_default_judge_model ](index.html#pydantic_evals.evaluators.llm_as_a_judge.set_default_judge_model). Source code in pydantic_evals/pydantic_evals/evaluators/common.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.LLMJudge", "page": "api/pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "MaxDuration dataclass", "anchor": "pydantic_evals.evaluators.MaxDuration", "heading_level": 3, "md_text": "Bases: Evaluator[object, object, object] Check if the execution time is under the specified maximum. Source code in pydantic_evals/pydantic_evals/evaluators/common.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.MaxDuration", "page": "api/pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "OutputConfig", "anchor": "pydantic_evals.evaluators.OutputConfig", "heading_level": 3, "md_text": "Bases: TypedDict Configuration for the score and assertion outputs of the LLMJudge evaluator. Source code in pydantic_evals/pydantic_evals/evaluators/common.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.OutputConfig", "page": "api/pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluatorContext dataclass", "anchor": "pydantic_evals.evaluators.EvaluatorContext", "heading_level": 3, "md_text": "Bases: Generic[InputsT, OutputT, MetadataT] Context for evaluating a task execution. An instance of this class is the sole input to all Evaluators. It contains all the information needed to evaluate the task execution, including inputs, outputs, metadata, and telemetry data. Evaluators use this context to access the task inputs, actual output, expected output, and other information when evaluating the result of the task execution. Example: Source code in pydantic_evals/pydantic_evals/evaluators/context.py #### name instance-attribute The name of the case. #### inputs instance-attribute The inputs provided to the task for this case. #### metadata instance-attribute Metadata associated with the case, if provided. May be None if no metadata was specified. #### expected\\_output instance-attribute The expected output for the case, if provided. May be None if no expected output was specified. #### output instance-attribute The actual output produced by the task for this case. #### duration instance-attribute The duration of the task run for this case. #### attributes instance-attribute Attributes associated with the task run for this case. These can be set by calling pydantic_evals.dataset.set_eval_attribute in any code executed during the evaluation task. #### metrics instance-attribute Metrics associated with the task run for this case. These can be set by calling pydantic_evals.dataset.increment_eval_metric in any code executed during the evaluation task. #### span\\_tree property Get the SpanTree for this task execution. The span tree is a graph where each node corresponds to an OpenTelemetry span recorded during the task execution, including timing information and any custom spans created during execution. Returns: Raises:", "url": "https://ai.pydantic.dev/api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorContext", "page": "api/pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluationReason dataclass", "anchor": "pydantic_evals.evaluators.EvaluationReason", "heading_level": 3, "md_text": "The result of running an evaluator with an optional explanation. Contains a scalar value and an optional \"reason\" explaining the value. Parameters: Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluationReason", "page": "api/pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluationResult dataclass", "anchor": "pydantic_evals.evaluators.EvaluationResult", "heading_level": 3, "md_text": "Bases: Generic[EvaluationScalarT] The details of an individual evaluation result. Contains the name, value, reason, and source evaluator for a single evaluation. Parameters: Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py #### downcast Attempt to downcast this result to a more specific type. Parameters: Returns: Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluationResult", "page": "api/pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluator dataclass", "anchor": "pydantic_evals.evaluators.Evaluator", "heading_level": 3, "md_text": "Bases: Generic[InputsT, OutputT, MetadataT] Base class for all evaluators. Evaluators can assess the performance of a task in a variety of ways, as a function of the EvaluatorContext. Subclasses must implement the evaluate method. Note it can be defined with either def or async def . Example: Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py #### get\\_serialization\\_name classmethod Return the 'name' of this Evaluator to use during serialization. Returns: Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py #### name classmethod deprecated Deprecated name has been renamed, use get_serialization_name instead. name has been renamed, use get_serialization_name instead. Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py #### get\\_default\\_evaluation\\_name Return the default name to use in reports for the output of this evaluator. By default, if the evaluator has an attribute called evaluation_name of type string, that will be used. Otherwise, the serialization name of the evaluator (which is usually the class name) will be used. This can be overridden to get a more descriptive name in evaluation reports, e.g. using instance information. Note that evaluators that return a mapping of results will always use the keys of that mapping as the names of the associated evaluation results. Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py #### evaluate abstractmethod Evaluate the task output in the given context. This is the main evaluation method that subclasses must implement. It can be either synchronous or asynchronous, returning either an EvaluatorOutput directly or an Awaitable[EvaluatorOutput]. Parameters: Returns: Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py #### evaluate\\_sync Run the evaluator synchronously, handling both sync and async implementations. This method ensures synchronous execution by running any async evaluate implementation to completion using run\\_until\\_complete. Parameters: Returns: Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py #### evaluate\\_async async Run the evaluator asynchronously, handling both sync and async implementations. This method ensures asynchronous execution by properly awaiting any async evaluate implementation. For synchronous implementations, it returns the result directly. Parameters: Returns: Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py #### serialize Serialize this Evaluator to a JSON-serializable form. Returns: Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py #### build\\_serialization\\_arguments Build the arguments for serialization. Evaluators are serialized for inclusion as the \"source\" in an EvaluationResult . If you want to modify how the evaluator is serialized for that or other purposes, you can override this method. Returns: Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.Evaluator", "page": "api/pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluatorFailure dataclass", "anchor": "pydantic_evals.evaluators.EvaluatorFailure", "heading_level": 3, "md_text": "Represents a failure raised during the execution of an evaluator. Source code in pydantic_evals/pydantic_evals/evaluators/evaluator.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorFailure", "page": "api/pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluatorOutput module-attribute", "anchor": "pydantic_evals.evaluators.EvaluatorOutput", "heading_level": 3, "md_text": "Type for the output of an evaluator, which can be a scalar, an EvaluationReason, or a mapping of names to either.", "url": "https://ai.pydantic.dev/api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorOutput", "page": "api/pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluatorSpec", "anchor": "pydantic_evals.evaluators.EvaluatorSpec", "heading_level": 3, "md_text": "Bases: BaseModel The specification of an evaluator to be run. This class is used to represent evaluators in a serializable format, supporting various short forms for convenience when defining evaluators in YAML or JSON dataset files. In particular, each of the following forms is supported for specifying an evaluator with name MyEvaluator : \\* 'MyEvaluator' - Just the (string) name of the Evaluator subclass is used if its __init__ takes no arguments \\* {'MyEvaluator': first_arg} - A single argument is passed as the first positional argument to MyEvaluator.__init__ \\* {'MyEvaluator': {k1: v1, k2: v2}} - Multiple kwargs are passed to MyEvaluator.__init__ Source code in pydantic_evals/pydantic_evals/evaluators/spec.py #### name instance-attribute The name of the evaluator class; should be the value returned by EvaluatorClass.get_serialization_name() #### arguments instance-attribute The arguments to pass to the evaluator's constructor. Can be None (no arguments), a tuple (a single positional argument), or a dict (keyword arguments). #### args property Get the positional arguments for the evaluator. Returns: #### kwargs property Get the keyword arguments for the evaluator. Returns: #### deserialize classmethod Deserialize an EvaluatorSpec from various formats. This validator handles the various short forms of evaluator specifications, converting them to a consistent EvaluatorSpec instance. Parameters: Returns: Raises: Source code in pydantic_evals/pydantic_evals/evaluators/spec.py #### serialize Serialize using the appropriate short-form if possible. Returns: Source code in pydantic_evals/pydantic_evals/evaluators/spec.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorSpec", "page": "api/pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "GradingOutput", "anchor": "pydantic_evals.evaluators.llm_as_a_judge.GradingOutput", "heading_level": 3, "md_text": "Bases: BaseModel The output of a grading operation. Source code in pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.llm_as_a_judge.GradingOutput", "page": "api/pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "judge_output async", "anchor": "pydantic_evals.evaluators.llm_as_a_judge.judge_output", "heading_level": 3, "md_text": "Judge the output of a model based on a rubric. If the model is not specified, a default model is used. The default model starts as 'openai:gpt-4o', but this can be changed using the set_default_judge_model function. Source code in pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.llm_as_a_judge.judge_output", "page": "api/pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "judge_input_output async", "anchor": "pydantic_evals.evaluators.llm_as_a_judge.judge_input_output", "heading_level": 3, "md_text": "Judge the output of a model based on the inputs and a rubric. If the model is not specified, a default model is used. The default model starts as 'openai:gpt-4o', but this can be changed using the set_default_judge_model function. Source code in pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.llm_as_a_judge.judge_input_output", "page": "api/pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "judge_input_output_expected async", "anchor": "pydantic_evals.evaluators.llm_as_a_judge.judge_input_output_expected", "heading_level": 3, "md_text": "Judge the output of a model based on the inputs and a rubric. If the model is not specified, a default model is used. The default model starts as 'openai:gpt-4o', but this can be changed using the set_default_judge_model function. Source code in pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.llm_as_a_judge.judge_input_output_expected", "page": "api/pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "judge_output_expected async", "anchor": "pydantic_evals.evaluators.llm_as_a_judge.judge_output_expected", "heading_level": 3, "md_text": "Judge the output of a model based on the expected output, output, and a rubric. If the model is not specified, a default model is used. The default model starts as 'openai:gpt-4o', but this can be changed using the set_default_judge_model function. Source code in pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.llm_as_a_judge.judge_output_expected", "page": "api/pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "set_default_judge_model", "anchor": "pydantic_evals.evaluators.llm_as_a_judge.set_default_judge_model", "heading_level": 3, "md_text": "Set the default model used for judging. This model is used if None is passed to the model argument of judge_output and judge_input_output . Source code in pydantic_evals/pydantic_evals/evaluators/llm_as_a_judge.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.llm_as_a_judge.set_default_judge_model", "page": "api/pydantic_evals/evaluators/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_evals.generation", "anchor": "pydantic_evalsgeneration", "heading_level": 1, "md_text": "Utilities for generating example datasets for pydantic\\_evals. This module provides functions for generating sample datasets for testing and examples, using LLMs to create realistic test data with proper structure. ### generate\\_dataset async Use an LLM to generate a dataset of test cases, each consisting of input, expected output, and metadata. This function creates a properly structured dataset with the specified input, output, and metadata types. It uses an LLM to attempt to generate realistic test cases that conform to the types' schemas. Parameters: Returns: Raises: Source code in pydantic_evals/pydantic_evals/generation.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/generation/index.html#pydantic_evalsgeneration", "page": "api/pydantic_evals/generation/index.html", "source_site": "pydantic_ai"}
{"title": "generate_dataset async", "anchor": "pydantic_evals.generation.generate_dataset", "heading_level": 3, "md_text": "Use an LLM to generate a dataset of test cases, each consisting of input, expected output, and metadata. This function creates a properly structured dataset with the specified input, output, and metadata types. It uses an LLM to attempt to generate realistic test cases that conform to the types' schemas. Parameters: Returns: Raises: Source code in pydantic_evals/pydantic_evals/generation.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/generation/index.html#pydantic_evals.generation.generate_dataset", "page": "api/pydantic_evals/generation/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.models.openai", "anchor": "pydantic_aimodelsopenai", "heading_level": 1, "md_text": "## Setup For details on how to set up authentication with this model, see [model configuration for OpenAI](../../../models/openai/index.html). ### OpenAIModelName module-attribute Possible OpenAI model names. Since OpenAI supports a variety of date-stamped models, we explicitly list the latest models but allow any name in the type hints. See [the OpenAI docs](https://platform.openai.com/docs/models) for a full list. Using this more broad type for the model name instead of the ChatModel definition allows this model to be used more easily with other model types (ie, Ollama, Deepseek). ### OpenAIChatModelSettings Bases: ModelSettings Settings used for an OpenAI model request. Source code in pydantic_ai_slim/pydantic_ai/models/openai.py #### openai\\_reasoning\\_effort instance-attribute Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are low , medium , and high . Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response. #### openai\\_logprobs instance-attribute Include log probabilities in the response. #### openai\\_top\\_logprobs instance-attribute Include log probabilities of the top n tokens in the response. #### openai\\_user instance-attribute A unique identifier representing the end-user, which can help OpenAI monitor and detect abuse. See [OpenAI's safety best practices](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids) for more details. #### openai\\_service\\_tier instance-attribute The service tier to use for the model request. Currently supported values are auto , default , flex , and priority . For more information, see [OpenAI's service tiers documentation](https://platform.openai.com/docs/api-reference/chat/object#chat/object-service_tier). #### openai\\_prediction instance-attribute Enables [predictive outputs](https://platform.openai.com/docs/guides/predicted-outputs). This feature is currently only supported for some OpenAI models. ### OpenAIModelSettings deprecated Bases: OpenAIChatModelSettings Deprecated Use OpenAIChatModelSettings instead. Deprecated alias for OpenAIChatModelSettings . Source code in pydantic_ai_slim/pydantic_ai/models/openai.py ### OpenAIResponsesModelSettings Bases: OpenAIChatModelSettings Settings used for an OpenAI Responses model request. ALL FIELDS MUST BE openai_ PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS. Source code in pydantic_ai_slim/pydantic_ai/models/openai.py #### openai\\_builtin\\_tools instance-attribute The provided OpenAI built-in tools to use. See [OpenAI's built-in tools](https://platform.openai.com/docs/guides/tools?api-mode=responses) for more details. #### openai\\_reasoning\\_generate\\_summary instance-attribute Deprecated alias for openai_reasoning_summary . #### openai\\_reasoning\\_summary instance-attribute A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process. One of concise or detailed . Check the [OpenAI Reasoning documentation](https://platform.openai.com/docs/guides/reasoning?api-mode=responses#reasoning-summaries) for more details. #### openai\\_send\\_reasoning\\_ids instance-attribute Whether to send the unique IDs of reasoning, text, and function call parts from the message history to the model. Enabled by default for reasoning models. This can result in errors like \"Item 'rs_123' of type 'reasoning' was provided without its required following item.\" if the message history you're sending does not match exactly what was received from the Responses API in a previous response, for example if you're using a [history processor](../../../message-history/index.html#processing-message-history). In that case, you'll want to disable this. #### openai\\_truncation instance-attribute The truncation strategy to use for the model response. It can be either: - disabled (default): If a model response will exceed the context window size for a model, the request will fail with a 400 error. - auto : If the context of this response and previous ones exceeds the model's context window size, the model will truncate the response to fit the context window by dropping input items in the middle of the conversation. #### openai\\_text\\_verbosity instance-attribute Constrains the verbosity of the model's text response. Lower values will result in more concise responses, while higher values will result in more verbose responses. Currently supported values are low , medium , and high . #### openai\\_previous\\_response\\_id instance-attribute The ID of a previous response from the model to use as the starting point for a continued conversation. When set to 'auto' , the request automatically uses the most recent provider_response_id from the message history and omits earlier messages. This enables the model to use server-side conversation state and faithfully reference previous reasoning. See the [OpenAI Responses API documentation](https://platform.openai.com/docs/guides/reasoning#keeping-reasoning-items-in-context) for more information. #### openai\\_include\\_code\\_execution\\_outputs instance-attribute Whether to include the code execution results in the response. Corresponds to the code_interpreter_call.outputs value of the include parameter in the Responses API. #### openai\\_include\\_web\\_search\\_sources instance-attribute Whether to include the web search results in the response. Corresponds to the web_search_call.action.sources value of the include parameter in the Responses API. ### OpenAIChatModel dataclass Bases: Model A model that uses the OpenAI API. Internally, this uses the [OpenAI Python client](https://github.com/openai/openai-python) to interact with the API. Apart from __init__ , all methods are private or match those of the base class. Source code in pydantic_ai_slim/pydantic_ai/models/openai.py #### \\_\\_init\\_\\_ Initialize an OpenAI model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/openai.py #### model\\_name property The model name. #### system property The model provider. ### OpenAIModel dataclass deprecated Bases: OpenAIChatModel Deprecated OpenAIModel was renamed to OpenAIChatModel to clearly distinguish it from OpenAIResponsesModel which uses OpenAI's newer Responses API. Use that unless you're using an OpenAI Chat Completions-compatible API, or require a feature that the Responses API doesn't support yet like audio. Deprecated alias for OpenAIChatModel . Source code in pydantic_ai_slim/pydantic_ai/models/openai.py ### OpenAIResponsesModel dataclass Bases: Model A model that uses the OpenAI Responses API. The [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses) is the new API for OpenAI models. If you are interested in the differences between the Responses API and the Chat Completions API, see the [OpenAI API docs](https://platform.openai.com/docs/guides/responses-vs-chat-completions). Source code in pydantic_ai_slim/pydantic_ai/models/openai.py #### \\_\\_init\\_\\_ Initialize an OpenAI Responses model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/openai.py #### model\\_name property The model name. #### system property The model provider.", "url": "https://ai.pydantic.dev/api/models/openai/index.html#pydantic_aimodelsopenai", "page": "api/models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "Setup", "anchor": "setup", "heading_level": 2, "md_text": "For details on how to set up authentication with this model, see [model configuration for OpenAI](../../../models/openai/index.html). ### OpenAIModelName module-attribute Possible OpenAI model names. Since OpenAI supports a variety of date-stamped models, we explicitly list the latest models but allow any name in the type hints. See [the OpenAI docs](https://platform.openai.com/docs/models) for a full list. Using this more broad type for the model name instead of the ChatModel definition allows this model to be used more easily with other model types (ie, Ollama, Deepseek). ### OpenAIChatModelSettings Bases: ModelSettings Settings used for an OpenAI model request. Source code in pydantic_ai_slim/pydantic_ai/models/openai.py #### openai\\_reasoning\\_effort instance-attribute Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are low , medium , and high . Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response. #### openai\\_logprobs instance-attribute Include log probabilities in the response. #### openai\\_top\\_logprobs instance-attribute Include log probabilities of the top n tokens in the response. #### openai\\_user instance-attribute A unique identifier representing the end-user, which can help OpenAI monitor and detect abuse. See [OpenAI's safety best practices](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids) for more details. #### openai\\_service\\_tier instance-attribute The service tier to use for the model request. Currently supported values are auto , default , flex , and priority . For more information, see [OpenAI's service tiers documentation](https://platform.openai.com/docs/api-reference/chat/object#chat/object-service_tier). #### openai\\_prediction instance-attribute Enables [predictive outputs](https://platform.openai.com/docs/guides/predicted-outputs). This feature is currently only supported for some OpenAI models. ### OpenAIModelSettings deprecated Bases: OpenAIChatModelSettings Deprecated Use OpenAIChatModelSettings instead. Deprecated alias for OpenAIChatModelSettings . Source code in pydantic_ai_slim/pydantic_ai/models/openai.py ### OpenAIResponsesModelSettings Bases: OpenAIChatModelSettings Settings used for an OpenAI Responses model request. ALL FIELDS MUST BE openai_ PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS. Source code in pydantic_ai_slim/pydantic_ai/models/openai.py #### openai\\_builtin\\_tools instance-attribute The provided OpenAI built-in tools to use. See [OpenAI's built-in tools](https://platform.openai.com/docs/guides/tools?api-mode=responses) for more details. #### openai\\_reasoning\\_generate\\_summary instance-attribute Deprecated alias for openai_reasoning_summary . #### openai\\_reasoning\\_summary instance-attribute A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process. One of concise or detailed . Check the [OpenAI Reasoning documentation](https://platform.openai.com/docs/guides/reasoning?api-mode=responses#reasoning-summaries) for more details. #### openai\\_send\\_reasoning\\_ids instance-attribute Whether to send the unique IDs of reasoning, text, and function call parts from the message history to the model. Enabled by default for reasoning models. This can result in errors like \"Item 'rs_123' of type 'reasoning' was provided without its required following item.\" if the message history you're sending does not match exactly what was received from the Responses API in a previous response, for example if you're using a [history processor](../../../message-history/index.html#processing-message-history). In that case, you'll want to disable this. #### openai\\_truncation instance-attribute The truncation strategy to use for the model response. It can be either: - disabled (default): If a model response will exceed the context window size for a model, the request will fail with a 400 error. - auto : If the context of this response and previous ones exceeds the model's context window size, the model will truncate the response to fit the context window by dropping input items in the middle of the conversation. #### openai\\_text\\_verbosity instance-attribute Constrains the verbosity of the model's text response. Lower values will result in more concise responses, while higher values will result in more verbose responses. Currently supported values are low , medium , and high . #### openai\\_previous\\_response\\_id instance-attribute The ID of a previous response from the model to use as the starting point for a continued conversation. When set to 'auto' , the request automatically uses the most recent provider_response_id from the message history and omits earlier messages. This enables the model to use server-side conversation state and faithfully reference previous reasoning. See the [OpenAI Responses API documentation](https://platform.openai.com/docs/guides/reasoning#keeping-reasoning-items-in-context) for more information. #### openai\\_include\\_code\\_execution\\_outputs instance-attribute Whether to include the code execution results in the response. Corresponds to the code_interpreter_call.outputs value of the include parameter in the Responses API. #### openai\\_include\\_web\\_search\\_sources instance-attribute Whether to include the web search results in the response. Corresponds to the web_search_call.action.sources value of the include parameter in the Responses API. ### OpenAIChatModel dataclass Bases: Model A model that uses the OpenAI API. Internally, this uses the [OpenAI Python client](https://github.com/openai/openai-python) to interact with the API. Apart from __init__ , all methods are private or match those of the base class. Source code in pydantic_ai_slim/pydantic_ai/models/openai.py #### \\_\\_init\\_\\_ Initialize an OpenAI model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/openai.py #### model\\_name property The model name. #### system property The model provider. ### OpenAIModel dataclass deprecated Bases: OpenAIChatModel Deprecated OpenAIModel was renamed to OpenAIChatModel to clearly distinguish it from OpenAIResponsesModel which uses OpenAI's newer Responses API. Use that unless you're using an OpenAI Chat Completions-compatible API, or require a feature that the Responses API doesn't support yet like audio. Deprecated alias for OpenAIChatModel . Source code in pydantic_ai_slim/pydantic_ai/models/openai.py ### OpenAIResponsesModel dataclass Bases: Model A model that uses the OpenAI Responses API. The [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses) is the new API for OpenAI models. If you are interested in the differences between the Responses API and the Chat Completions API, see the [OpenAI API docs](https://platform.openai.com/docs/guides/responses-vs-chat-completions). Source code in pydantic_ai_slim/pydantic_ai/models/openai.py #### \\_\\_init\\_\\_ Initialize an OpenAI Responses model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/openai.py #### model\\_name property The model name. #### system property The model provider.", "url": "https://ai.pydantic.dev/api/models/openai/index.html#setup", "page": "api/models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIModelName module-attribute", "anchor": "pydantic_ai.models.openai.OpenAIModelName", "heading_level": 3, "md_text": "Possible OpenAI model names. Since OpenAI supports a variety of date-stamped models, we explicitly list the latest models but allow any name in the type hints. See [the OpenAI docs](https://platform.openai.com/docs/models) for a full list. Using this more broad type for the model name instead of the ChatModel definition allows this model to be used more easily with other model types (ie, Ollama, Deepseek).", "url": "https://ai.pydantic.dev/api/models/openai/index.html#pydantic_ai.models.openai.OpenAIModelName", "page": "api/models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIChatModelSettings", "anchor": "pydantic_ai.models.openai.OpenAIChatModelSettings", "heading_level": 3, "md_text": "Bases: ModelSettings Settings used for an OpenAI model request. Source code in pydantic_ai_slim/pydantic_ai/models/openai.py #### openai\\_reasoning\\_effort instance-attribute Constrains effort on reasoning for [reasoning models](https://platform.openai.com/docs/guides/reasoning). Currently supported values are low , medium , and high . Reducing reasoning effort can result in faster responses and fewer tokens used on reasoning in a response. #### openai\\_logprobs instance-attribute Include log probabilities in the response. #### openai\\_top\\_logprobs instance-attribute Include log probabilities of the top n tokens in the response. #### openai\\_user instance-attribute A unique identifier representing the end-user, which can help OpenAI monitor and detect abuse. See [OpenAI's safety best practices](https://platform.openai.com/docs/guides/safety-best-practices#end-user-ids) for more details. #### openai\\_service\\_tier instance-attribute The service tier to use for the model request. Currently supported values are auto , default , flex , and priority . For more information, see [OpenAI's service tiers documentation](https://platform.openai.com/docs/api-reference/chat/object#chat/object-service_tier). #### openai\\_prediction instance-attribute Enables [predictive outputs](https://platform.openai.com/docs/guides/predicted-outputs). This feature is currently only supported for some OpenAI models.", "url": "https://ai.pydantic.dev/api/models/openai/index.html#pydantic_ai.models.openai.OpenAIChatModelSettings", "page": "api/models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIModelSettings deprecated", "anchor": "pydantic_ai.models.openai.OpenAIModelSettings", "heading_level": 3, "md_text": "Bases: OpenAIChatModelSettings Deprecated Use OpenAIChatModelSettings instead. Deprecated alias for OpenAIChatModelSettings . Source code in pydantic_ai_slim/pydantic_ai/models/openai.py", "url": "https://ai.pydantic.dev/api/models/openai/index.html#pydantic_ai.models.openai.OpenAIModelSettings", "page": "api/models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIResponsesModelSettings", "anchor": "pydantic_ai.models.openai.OpenAIResponsesModelSettings", "heading_level": 3, "md_text": "Bases: OpenAIChatModelSettings Settings used for an OpenAI Responses model request. ALL FIELDS MUST BE openai_ PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS. Source code in pydantic_ai_slim/pydantic_ai/models/openai.py #### openai\\_builtin\\_tools instance-attribute The provided OpenAI built-in tools to use. See [OpenAI's built-in tools](https://platform.openai.com/docs/guides/tools?api-mode=responses) for more details. #### openai\\_reasoning\\_generate\\_summary instance-attribute Deprecated alias for openai_reasoning_summary . #### openai\\_reasoning\\_summary instance-attribute A summary of the reasoning performed by the model. This can be useful for debugging and understanding the model's reasoning process. One of concise or detailed . Check the [OpenAI Reasoning documentation](https://platform.openai.com/docs/guides/reasoning?api-mode=responses#reasoning-summaries) for more details. #### openai\\_send\\_reasoning\\_ids instance-attribute Whether to send the unique IDs of reasoning, text, and function call parts from the message history to the model. Enabled by default for reasoning models. This can result in errors like \"Item 'rs_123' of type 'reasoning' was provided without its required following item.\" if the message history you're sending does not match exactly what was received from the Responses API in a previous response, for example if you're using a [history processor](../../../message-history/index.html#processing-message-history). In that case, you'll want to disable this. #### openai\\_truncation instance-attribute The truncation strategy to use for the model response. It can be either: - disabled (default): If a model response will exceed the context window size for a model, the request will fail with a 400 error. - auto : If the context of this response and previous ones exceeds the model's context window size, the model will truncate the response to fit the context window by dropping input items in the middle of the conversation. #### openai\\_text\\_verbosity instance-attribute Constrains the verbosity of the model's text response. Lower values will result in more concise responses, while higher values will result in more verbose responses. Currently supported values are low , medium , and high . #### openai\\_previous\\_response\\_id instance-attribute The ID of a previous response from the model to use as the starting point for a continued conversation. When set to 'auto' , the request automatically uses the most recent provider_response_id from the message history and omits earlier messages. This enables the model to use server-side conversation state and faithfully reference previous reasoning. See the [OpenAI Responses API documentation](https://platform.openai.com/docs/guides/reasoning#keeping-reasoning-items-in-context) for more information. #### openai\\_include\\_code\\_execution\\_outputs instance-attribute Whether to include the code execution results in the response. Corresponds to the code_interpreter_call.outputs value of the include parameter in the Responses API. #### openai\\_include\\_web\\_search\\_sources instance-attribute Whether to include the web search results in the response. Corresponds to the web_search_call.action.sources value of the include parameter in the Responses API.", "url": "https://ai.pydantic.dev/api/models/openai/index.html#pydantic_ai.models.openai.OpenAIResponsesModelSettings", "page": "api/models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIChatModel dataclass", "anchor": "pydantic_ai.models.openai.OpenAIChatModel", "heading_level": 3, "md_text": "Bases: Model A model that uses the OpenAI API. Internally, this uses the [OpenAI Python client](https://github.com/openai/openai-python) to interact with the API. Apart from __init__ , all methods are private or match those of the base class. Source code in pydantic_ai_slim/pydantic_ai/models/openai.py #### \\_\\_init\\_\\_ Initialize an OpenAI model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/openai.py #### model\\_name property The model name. #### system property The model provider.", "url": "https://ai.pydantic.dev/api/models/openai/index.html#pydantic_ai.models.openai.OpenAIChatModel", "page": "api/models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIModel dataclass deprecated", "anchor": "pydantic_ai.models.openai.OpenAIModel", "heading_level": 3, "md_text": "Bases: OpenAIChatModel Deprecated OpenAIModel was renamed to OpenAIChatModel to clearly distinguish it from OpenAIResponsesModel which uses OpenAI's newer Responses API. Use that unless you're using an OpenAI Chat Completions-compatible API, or require a feature that the Responses API doesn't support yet like audio. Deprecated alias for OpenAIChatModel . Source code in pydantic_ai_slim/pydantic_ai/models/openai.py", "url": "https://ai.pydantic.dev/api/models/openai/index.html#pydantic_ai.models.openai.OpenAIModel", "page": "api/models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIResponsesModel dataclass", "anchor": "pydantic_ai.models.openai.OpenAIResponsesModel", "heading_level": 3, "md_text": "Bases: Model A model that uses the OpenAI Responses API. The [OpenAI Responses API](https://platform.openai.com/docs/api-reference/responses) is the new API for OpenAI models. If you are interested in the differences between the Responses API and the Chat Completions API, see the [OpenAI API docs](https://platform.openai.com/docs/guides/responses-vs-chat-completions). Source code in pydantic_ai_slim/pydantic_ai/models/openai.py #### \\_\\_init\\_\\_ Initialize an OpenAI Responses model. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/openai.py #### model\\_name property The model name. #### system property The model provider.", "url": "https://ai.pydantic.dev/api/models/openai/index.html#pydantic_ai.models.openai.OpenAIResponsesModel", "page": "api/models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_evals.dataset", "anchor": "pydantic_evalsdataset", "heading_level": 1, "md_text": "Dataset management for pydantic evals. This module provides functionality for creating, loading, saving, and evaluating datasets of test cases. Each case must have inputs, and can optionally have a name, expected output, metadata, and case-specific evaluators. Datasets can be loaded from and saved to YAML or JSON files, and can be evaluated against a task function to produce an evaluation report. ### Case dataclass Bases: Generic[InputsT, OutputT, MetadataT] A single row of a [ Dataset ](index.html#pydantic_evals.dataset.Dataset). Each case represents a single test scenario with inputs to test. A case may optionally specify a name, expected outputs to compare against, and arbitrary metadata. Cases can also have their own specific evaluators which are run in addition to dataset-level evaluators. Example: Source code in pydantic_evals/pydantic_evals/dataset.py #### \\_\\_init\\_\\_ Initialize a new test case. Parameters: Source code in pydantic_evals/pydantic_evals/dataset.py #### name instance-attribute Name of the case. This is used to identify the case in the report and can be used to filter cases. #### inputs instance-attribute Inputs to the task. This is the input to the task that will be evaluated. #### metadata class-attribute instance-attribute Metadata to be used in the evaluation. This can be used to provide additional information about the case to the evaluators. #### expected\\_output class-attribute instance-attribute Expected output of the task. This is the expected output of the task that will be evaluated. #### evaluators class-attribute instance-attribute Evaluators to be used just on this case. ### Dataset Bases: BaseModel , Generic[InputsT, OutputT, MetadataT] A dataset of test [cases](index.html#pydantic_evals.dataset.Case). Datasets allow you to organize a collection of test cases and evaluate them against a task function. They can be loaded from and saved to YAML or JSON files, and can have dataset-level evaluators that apply to all cases. Example: Source code in pydantic_evals/pydantic_evals/dataset.py #### name class-attribute instance-attribute Optional name of the dataset. #### cases instance-attribute List of test cases in the dataset. #### evaluators class-attribute instance-attribute List of evaluators to be used on all cases in the dataset. #### \\_\\_init\\_\\_ Initialize a new dataset with test cases and optional evaluators. Parameters: Source code in pydantic_evals/pydantic_evals/dataset.py #### evaluate async Evaluates the test cases in the dataset using the given task. This method runs the task on each case in the dataset, applies evaluators, and collects results into a report. Cases are run concurrently, limited by max_concurrency if specified. Parameters: Returns: Source code in pydantic_evals/pydantic_evals/dataset.py #### evaluate\\_sync Evaluates the test cases in the dataset using the given task. This is a synchronous wrapper around [ evaluate ](index.html#pydantic_evals.dataset.Dataset.evaluate) provided for convenience. Parameters: Returns: Source code in pydantic_evals/pydantic_evals/dataset.py #### add\\_case Adds a case to the dataset. This is a convenience method for creating a [ Case ](index.html#pydantic_evals.dataset.Case) and adding it to the dataset. Parameters: Source code in pydantic_evals/pydantic_evals/dataset.py #### add\\_evaluator Adds an evaluator to the dataset or a specific case. Parameters: Raises: Source code in pydantic_evals/pydantic_evals/dataset.py #### from\\_file classmethod Load a dataset from a file. Parameters: Returns: Raises: Source code in pydantic_evals/pydantic_evals/dataset.py #### from\\_text classmethod Load a dataset from a string. Parameters: Returns: Raises: Source code in pydantic_evals/pydantic_evals/dataset.py #### from\\_dict classmethod Load a dataset from a dictionary. Parameters: Returns: Raises: Source code in pydantic_evals/pydantic_evals/dataset.py #### to\\_file Save the dataset to a file. Parameters: Source code in pydantic_evals/pydantic_evals/dataset.py #### model\\_json\\_schema\\_with\\_evaluators classmethod Generate a JSON schema for this dataset type, including evaluator details. This is useful for generating a schema that can be used to validate YAML-format dataset files. Parameters: Returns: Source code in pydantic_evals/pydantic_evals/dataset.py ### set\\_eval\\_attribute Set an attribute on the current task run. Parameters: Source code in pydantic_evals/pydantic_evals/dataset.py ### increment\\_eval\\_metric Increment a metric on the current task run. Parameters: Source code in pydantic_evals/pydantic_evals/dataset.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/dataset/index.html#pydantic_evalsdataset", "page": "api/pydantic_evals/dataset/index.html", "source_site": "pydantic_ai"}
{"title": "Case dataclass", "anchor": "pydantic_evals.dataset.Case", "heading_level": 3, "md_text": "Bases: Generic[InputsT, OutputT, MetadataT] A single row of a [ Dataset ](index.html#pydantic_evals.dataset.Dataset). Each case represents a single test scenario with inputs to test. A case may optionally specify a name, expected outputs to compare against, and arbitrary metadata. Cases can also have their own specific evaluators which are run in addition to dataset-level evaluators. Example: Source code in pydantic_evals/pydantic_evals/dataset.py #### \\_\\_init\\_\\_ Initialize a new test case. Parameters: Source code in pydantic_evals/pydantic_evals/dataset.py #### name instance-attribute Name of the case. This is used to identify the case in the report and can be used to filter cases. #### inputs instance-attribute Inputs to the task. This is the input to the task that will be evaluated. #### metadata class-attribute instance-attribute Metadata to be used in the evaluation. This can be used to provide additional information about the case to the evaluators. #### expected\\_output class-attribute instance-attribute Expected output of the task. This is the expected output of the task that will be evaluated. #### evaluators class-attribute instance-attribute Evaluators to be used just on this case.", "url": "https://ai.pydantic.dev/api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Case", "page": "api/pydantic_evals/dataset/index.html", "source_site": "pydantic_ai"}
{"title": "Dataset", "anchor": "pydantic_evals.dataset.Dataset", "heading_level": 3, "md_text": "Bases: BaseModel , Generic[InputsT, OutputT, MetadataT] A dataset of test [cases](index.html#pydantic_evals.dataset.Case). Datasets allow you to organize a collection of test cases and evaluate them against a task function. They can be loaded from and saved to YAML or JSON files, and can have dataset-level evaluators that apply to all cases. Example: Source code in pydantic_evals/pydantic_evals/dataset.py #### name class-attribute instance-attribute Optional name of the dataset. #### cases instance-attribute List of test cases in the dataset. #### evaluators class-attribute instance-attribute List of evaluators to be used on all cases in the dataset. #### \\_\\_init\\_\\_ Initialize a new dataset with test cases and optional evaluators. Parameters: Source code in pydantic_evals/pydantic_evals/dataset.py #### evaluate async Evaluates the test cases in the dataset using the given task. This method runs the task on each case in the dataset, applies evaluators, and collects results into a report. Cases are run concurrently, limited by max_concurrency if specified. Parameters: Returns: Source code in pydantic_evals/pydantic_evals/dataset.py #### evaluate\\_sync Evaluates the test cases in the dataset using the given task. This is a synchronous wrapper around [ evaluate ](index.html#pydantic_evals.dataset.Dataset.evaluate) provided for convenience. Parameters: Returns: Source code in pydantic_evals/pydantic_evals/dataset.py #### add\\_case Adds a case to the dataset. This is a convenience method for creating a [ Case ](index.html#pydantic_evals.dataset.Case) and adding it to the dataset. Parameters: Source code in pydantic_evals/pydantic_evals/dataset.py #### add\\_evaluator Adds an evaluator to the dataset or a specific case. Parameters: Raises: Source code in pydantic_evals/pydantic_evals/dataset.py #### from\\_file classmethod Load a dataset from a file. Parameters: Returns: Raises: Source code in pydantic_evals/pydantic_evals/dataset.py #### from\\_text classmethod Load a dataset from a string. Parameters: Returns: Raises: Source code in pydantic_evals/pydantic_evals/dataset.py #### from\\_dict classmethod Load a dataset from a dictionary. Parameters: Returns: Raises: Source code in pydantic_evals/pydantic_evals/dataset.py #### to\\_file Save the dataset to a file. Parameters: Source code in pydantic_evals/pydantic_evals/dataset.py #### model\\_json\\_schema\\_with\\_evaluators classmethod Generate a JSON schema for this dataset type, including evaluator details. This is useful for generating a schema that can be used to validate YAML-format dataset files. Parameters: Returns: Source code in pydantic_evals/pydantic_evals/dataset.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset", "page": "api/pydantic_evals/dataset/index.html", "source_site": "pydantic_ai"}
{"title": "set_eval_attribute", "anchor": "pydantic_evals.dataset.set_eval_attribute", "heading_level": 3, "md_text": "Set an attribute on the current task run. Parameters: Source code in pydantic_evals/pydantic_evals/dataset.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.set_eval_attribute", "page": "api/pydantic_evals/dataset/index.html", "source_site": "pydantic_ai"}
{"title": "increment_eval_metric", "anchor": "pydantic_evals.dataset.increment_eval_metric", "heading_level": 3, "md_text": "Increment a metric on the current task run. Parameters: Source code in pydantic_evals/pydantic_evals/dataset.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.increment_eval_metric", "page": "api/pydantic_evals/dataset/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_evals.otel", "anchor": "pydantic_evalsotel", "heading_level": 1, "md_text": "### SpanNode dataclass A node in the span tree; provides references to parents/children for easy traversal and queries. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### duration property Return the span's duration as a timedelta, or None if start/end not set. #### descendants property Return all descendants of this node in DFS order. #### ancestors property Return all ancestors of this node. #### add\\_child Attach a child node to this node's list of children. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### find\\_children Return all immediate children that satisfy the given predicate. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### first\\_child Return the first immediate child that satisfies the given predicate, or None if none match. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### any\\_child Returns True if there is at least one child that satisfies the predicate. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### find\\_descendants Return all descendant nodes that satisfy the given predicate in DFS order. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### first\\_descendant DFS: Return the first descendant (in DFS order) that satisfies the given predicate, or None if none match. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### any\\_descendant Returns True if there is at least one descendant that satisfies the predicate. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### find\\_ancestors Return all ancestors that satisfy the given predicate. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### first\\_ancestor Return the closest ancestor that satisfies the given predicate, or None if none match. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### any\\_ancestor Returns True if any ancestor satisfies the predicate. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### matches Check if the span node matches the query conditions or predicate. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### repr\\_xml Return an XML-like string representation of the node. Optionally includes children, trace\\_id, span\\_id, start\\_timestamp, and duration. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py ### SpanQuery Bases: TypedDict A serializable query for filtering SpanNodes based on various conditions. All fields are optional and combined with AND logic by default. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### stop\\_recursing\\_when instance-attribute If present, stop recursing through ancestors or descendants at nodes that match this condition. ### SpanTree dataclass A container that builds a hierarchy of SpanNode objects from a list of finished spans. You can then search or iterate the tree to make your assertions (using DFS for traversal). Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### add\\_spans Add a list of spans to the tree, rebuilding the tree structure. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### find Find all nodes in the entire tree that match the predicate, scanning from each root in DFS order. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### first Find the first node that matches a predicate, scanning from each root in DFS order. Returns None if not found. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### any Returns True if any node in the tree matches the predicate. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### \\_\\_iter\\_\\_ Return an iterator over all nodes in the tree. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### repr\\_xml Return an XML-like string representation of the tree, optionally including children, trace\\_id, span\\_id, duration, and timestamps. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/otel/index.html#pydantic_evalsotel", "page": "api/pydantic_evals/otel/index.html", "source_site": "pydantic_ai"}
{"title": "SpanNode dataclass", "anchor": "pydantic_evals.otel.SpanNode", "heading_level": 3, "md_text": "A node in the span tree; provides references to parents/children for easy traversal and queries. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### duration property Return the span's duration as a timedelta, or None if start/end not set. #### descendants property Return all descendants of this node in DFS order. #### ancestors property Return all ancestors of this node. #### add\\_child Attach a child node to this node's list of children. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### find\\_children Return all immediate children that satisfy the given predicate. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### first\\_child Return the first immediate child that satisfies the given predicate, or None if none match. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### any\\_child Returns True if there is at least one child that satisfies the predicate. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### find\\_descendants Return all descendant nodes that satisfy the given predicate in DFS order. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### first\\_descendant DFS: Return the first descendant (in DFS order) that satisfies the given predicate, or None if none match. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### any\\_descendant Returns True if there is at least one descendant that satisfies the predicate. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### find\\_ancestors Return all ancestors that satisfy the given predicate. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### first\\_ancestor Return the closest ancestor that satisfies the given predicate, or None if none match. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### any\\_ancestor Returns True if any ancestor satisfies the predicate. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### matches Check if the span node matches the query conditions or predicate. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### repr\\_xml Return an XML-like string representation of the node. Optionally includes children, trace\\_id, span\\_id, start\\_timestamp, and duration. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/otel/index.html#pydantic_evals.otel.SpanNode", "page": "api/pydantic_evals/otel/index.html", "source_site": "pydantic_ai"}
{"title": "SpanQuery", "anchor": "pydantic_evals.otel.SpanQuery", "heading_level": 3, "md_text": "Bases: TypedDict A serializable query for filtering SpanNodes based on various conditions. All fields are optional and combined with AND logic by default. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### stop\\_recursing\\_when instance-attribute If present, stop recursing through ancestors or descendants at nodes that match this condition.", "url": "https://ai.pydantic.dev/api/pydantic_evals/otel/index.html#pydantic_evals.otel.SpanQuery", "page": "api/pydantic_evals/otel/index.html", "source_site": "pydantic_ai"}
{"title": "SpanTree dataclass", "anchor": "pydantic_evals.otel.SpanTree", "heading_level": 3, "md_text": "A container that builds a hierarchy of SpanNode objects from a list of finished spans. You can then search or iterate the tree to make your assertions (using DFS for traversal). Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### add\\_spans Add a list of spans to the tree, rebuilding the tree structure. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### find Find all nodes in the entire tree that match the predicate, scanning from each root in DFS order. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### first Find the first node that matches a predicate, scanning from each root in DFS order. Returns None if not found. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### any Returns True if any node in the tree matches the predicate. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### \\_\\_iter\\_\\_ Return an iterator over all nodes in the tree. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py #### repr\\_xml Return an XML-like string representation of the tree, optionally including children, trace\\_id, span\\_id, duration, and timestamps. Source code in pydantic_evals/pydantic_evals/otel/span_tree.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/otel/index.html#pydantic_evals.otel.SpanTree", "page": "api/pydantic_evals/otel/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.providers", "anchor": "pydantic_aiproviders", "heading_level": 1, "md_text": "Bases: ABC , Generic[InterfaceClient] Abstract class for a provider. The provider is in charge of providing an authenticated client to the API. Each provider only supports a specific interface. A interface can be supported by multiple providers. For example, the OpenAIChatModel interface can be supported by the OpenAIProvider and the DeepSeekProvider . Source code in pydantic_ai_slim/pydantic_ai/providers/__init__.py ### name abstractmethod property The provider name. ### base\\_url abstractmethod property The base URL for the provider API. ### client abstractmethod property The client for the provider. ### model\\_profile The model profile for the named model, if available. Source code in pydantic_ai_slim/pydantic_ai/providers/__init__.py ### GoogleProvider Bases: Provider[Client] Provider for Google. Source code in pydantic_ai_slim/pydantic_ai/providers/google.py #### \\_\\_init\\_\\_ Create a new Google provider. Parameters: Source code in pydantic_ai_slim/pydantic_ai/providers/google.py ### VertexAILocation module-attribute Regions available for Vertex AI. More details [here](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations#genai-locations). ### OpenAIProvider Bases: Provider[AsyncOpenAI] Provider for OpenAI API. Source code in pydantic_ai_slim/pydantic_ai/providers/openai.py #### \\_\\_init\\_\\_ Create a new OpenAI provider. Parameters: Source code in pydantic_ai_slim/pydantic_ai/providers/openai.py ### DeepSeekProvider Bases: Provider[AsyncOpenAI] Provider for DeepSeek API. Source code in pydantic_ai_slim/pydantic_ai/providers/deepseek.py ### BedrockModelProfile dataclass Bases: ModelProfile Profile for models used with BedrockModel. ALL FIELDS MUST BE bedrock_ PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS. Source code in pydantic_ai_slim/pydantic_ai/providers/bedrock.py ### bedrock\\_amazon\\_model\\_profile Get the model profile for an Amazon model used via Bedrock. Source code in pydantic_ai_slim/pydantic_ai/providers/bedrock.py ### bedrock\\_deepseek\\_model\\_profile Get the model profile for a DeepSeek model used via Bedrock. Source code in pydantic_ai_slim/pydantic_ai/providers/bedrock.py ### BedrockProvider Bases: Provider[BaseClient] Provider for AWS Bedrock. Source code in pydantic_ai_slim/pydantic_ai/providers/bedrock.py #### \\_\\_init\\_\\_ Initialize the Bedrock provider. Parameters: Source code in pydantic_ai_slim/pydantic_ai/providers/bedrock.py ### groq\\_moonshotai\\_model\\_profile Get the model profile for an MoonshotAI model used with the Groq provider. Source code in pydantic_ai_slim/pydantic_ai/providers/groq.py ### meta\\_groq\\_model\\_profile Get the model profile for a Meta model used with the Groq provider. Source code in pydantic_ai_slim/pydantic_ai/providers/groq.py ### GroqProvider Bases: Provider[AsyncGroq] Provider for Groq API. Source code in pydantic_ai_slim/pydantic_ai/providers/groq.py #### \\_\\_init\\_\\_ Create a new Groq provider. Parameters: Source code in pydantic_ai_slim/pydantic_ai/providers/groq.py ### AzureProvider Bases: Provider[AsyncOpenAI] Provider for Azure OpenAI API. See <https://azure.microsoft.com/en-us/products/ai-foundry> for more information. Source code in pydantic_ai_slim/pydantic_ai/providers/azure.py #### \\_\\_init\\_\\_ Create a new Azure provider. Parameters: Source code in pydantic_ai_slim/pydantic_ai/providers/azure.py ### CohereProvider Bases: Provider[AsyncClientV2] Provider for Cohere API. Source code in pydantic_ai_slim/pydantic_ai/providers/cohere.py #### \\_\\_init\\_\\_ Create a new Cohere provider. Parameters: Source code in pydantic_ai_slim/pydantic_ai/providers/cohere.py Bases: Provider[AsyncOpenAI] Provider for Cerebras API. Source code in pydantic_ai_slim/pydantic_ai/providers/cerebras.py Bases: Provider[Mistral] Provider for Mistral API. Source code in pydantic_ai_slim/pydantic_ai/providers/mistral.py ### \\_\\_init\\_\\_ Create a new Mistral provider. Parameters: Source code in pydantic_ai_slim/pydantic_ai/providers/mistral.py Bases: Provider[AsyncOpenAI] Provider for Fireworks AI API. Source code in pydantic_ai_slim/pydantic_ai/providers/fireworks.py Bases: Provider[AsyncOpenAI] Provider for Grok API. Source code in pydantic_ai_slim/pydantic_ai/providers/grok.py Bases: Provider[AsyncOpenAI] Provider for Together AI API. Source code in pydantic_ai_slim/pydantic_ai/providers/together.py Bases: Provider[AsyncOpenAI] Provider for Heroku API. Source code in pydantic_ai_slim/pydantic_ai/providers/heroku.py Bases: Provider[AsyncOpenAI] Provider for GitHub Models API. GitHub Models provides access to various AI models through an OpenAI-compatible API. See <https://docs.github.com/en/github-models> for more information. Source code in pydantic_ai_slim/pydantic_ai/providers/github.py ### \\_\\_init\\_\\_ Create a new GitHub Models provider. Parameters: Source code in pydantic_ai_slim/pydantic_ai/providers/github.py Bases: Provider[AsyncOpenAI] Provider for OpenRouter API. Source code in pydantic_ai_slim/pydantic_ai/providers/openrouter.py Bases: Provider[AsyncOpenAI] Provider for Vercel AI Gateway API. Source code in pydantic_ai_slim/pydantic_ai/providers/vercel.py Bases: Provider[AsyncInferenceClient] Provider for Hugging Face. Source code in pydantic_ai_slim/pydantic_ai/providers/huggingface.py ### \\_\\_init\\_\\_ Create a new Hugging Face provider. Parameters: Source code in pydantic_ai_slim/pydantic_ai/providers/huggingface.py Bases: Provider[AsyncOpenAI] Provider for MoonshotAI platform (Kimi models). Source code in pydantic_ai_slim/pydantic_ai/providers/moonshotai.py Bases: Provider[AsyncOpenAI] Provider for local or remote Ollama API. Source code in pydantic_ai_slim/pydantic_ai/providers/ollama.py ### \\_\\_init\\_\\_ Create a new Ollama provider. Parameters: Source code in pydantic_ai_slim/pydantic_ai/providers/ollama.py Bases: Provider[AsyncOpenAI] Provider for LiteLLM API. Source code in pydantic_ai_slim/pydantic_ai/providers/litellm.py ### \\_\\_init\\_\\_ Initialize a LiteLLM provider. Parameters: Source code in pydantic_ai_slim/pydantic_ai/providers/litellm.py Bases: Provider[AsyncOpenAI] Provider for Nebius AI Studio API. Source code in pydantic_ai_slim/pydantic_ai/providers/nebius.py Bases: Provider[AsyncOpenAI] Provider for OVHcloud AI Endpoints. Source code in pydantic_ai_slim/pydantic_ai/providers/ovhcloud.py", "url": "https://ai.pydantic.dev/api/providers/index.html#pydantic_aiproviders", "page": "api/providers/index.html", "source_site": "pydantic_ai"}
{"title": "name abstractmethod property", "anchor": "pydantic_ai.providers.Provider.name", "heading_level": 3, "md_text": "The provider name.", "url": "https://ai.pydantic.dev/api/providers/index.html#pydantic_ai.providers.Provider.name", "page": "api/providers/index.html", "source_site": "pydantic_ai"}
{"title": "base_url abstractmethod property", "anchor": "pydantic_ai.providers.Provider.base_url", "heading_level": 3, "md_text": "The base URL for the provider API.", "url": "https://ai.pydantic.dev/api/providers/index.html#pydantic_ai.providers.Provider.base_url", "page": "api/providers/index.html", "source_site": "pydantic_ai"}
{"title": "client abstractmethod property", "anchor": "pydantic_ai.providers.Provider.client", "heading_level": 3, "md_text": "The client for the provider.", "url": "https://ai.pydantic.dev/api/providers/index.html#pydantic_ai.providers.Provider.client", "page": "api/providers/index.html", "source_site": "pydantic_ai"}
{"title": "model_profile", "anchor": "pydantic_ai.providers.Provider.model_profile", "heading_level": 3, "md_text": "The model profile for the named model, if available. Source code in pydantic_ai_slim/pydantic_ai/providers/__init__.py", "url": "https://ai.pydantic.dev/api/providers/index.html#pydantic_ai.providers.Provider.model_profile", "page": "api/providers/index.html", "source_site": "pydantic_ai"}
{"title": "GoogleProvider", "anchor": "pydantic_ai.providers.google.GoogleProvider", "heading_level": 3, "md_text": "Bases: Provider[Client] Provider for Google. Source code in pydantic_ai_slim/pydantic_ai/providers/google.py #### \\_\\_init\\_\\_ Create a new Google provider. Parameters: Source code in pydantic_ai_slim/pydantic_ai/providers/google.py", "url": "https://ai.pydantic.dev/api/providers/index.html#pydantic_ai.providers.google.GoogleProvider", "page": "api/providers/index.html", "source_site": "pydantic_ai"}
{"title": "VertexAILocation module-attribute", "anchor": "pydantic_ai.providers.google.VertexAILocation", "heading_level": 3, "md_text": "Regions available for Vertex AI. More details [here](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/locations#genai-locations).", "url": "https://ai.pydantic.dev/api/providers/index.html#pydantic_ai.providers.google.VertexAILocation", "page": "api/providers/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAIProvider", "anchor": "pydantic_ai.providers.openai.OpenAIProvider", "heading_level": 3, "md_text": "Bases: Provider[AsyncOpenAI] Provider for OpenAI API. Source code in pydantic_ai_slim/pydantic_ai/providers/openai.py #### \\_\\_init\\_\\_ Create a new OpenAI provider. Parameters: Source code in pydantic_ai_slim/pydantic_ai/providers/openai.py", "url": "https://ai.pydantic.dev/api/providers/index.html#pydantic_ai.providers.openai.OpenAIProvider", "page": "api/providers/index.html", "source_site": "pydantic_ai"}
{"title": "DeepSeekProvider", "anchor": "pydantic_ai.providers.deepseek.DeepSeekProvider", "heading_level": 3, "md_text": "Bases: Provider[AsyncOpenAI] Provider for DeepSeek API. Source code in pydantic_ai_slim/pydantic_ai/providers/deepseek.py", "url": "https://ai.pydantic.dev/api/providers/index.html#pydantic_ai.providers.deepseek.DeepSeekProvider", "page": "api/providers/index.html", "source_site": "pydantic_ai"}
{"title": "BedrockModelProfile dataclass", "anchor": "pydantic_ai.providers.bedrock.BedrockModelProfile", "heading_level": 3, "md_text": "Bases: ModelProfile Profile for models used with BedrockModel. ALL FIELDS MUST BE bedrock_ PREFIXED SO YOU CAN MERGE THEM WITH OTHER MODELS. Source code in pydantic_ai_slim/pydantic_ai/providers/bedrock.py", "url": "https://ai.pydantic.dev/api/providers/index.html#pydantic_ai.providers.bedrock.BedrockModelProfile", "page": "api/providers/index.html", "source_site": "pydantic_ai"}
{"title": "bedrock_amazon_model_profile", "anchor": "pydantic_ai.providers.bedrock.bedrock_amazon_model_profile", "heading_level": 3, "md_text": "Get the model profile for an Amazon model used via Bedrock. Source code in pydantic_ai_slim/pydantic_ai/providers/bedrock.py", "url": "https://ai.pydantic.dev/api/providers/index.html#pydantic_ai.providers.bedrock.bedrock_amazon_model_profile", "page": "api/providers/index.html", "source_site": "pydantic_ai"}
{"title": "bedrock_deepseek_model_profile", "anchor": "pydantic_ai.providers.bedrock.bedrock_deepseek_model_profile", "heading_level": 3, "md_text": "Get the model profile for a DeepSeek model used via Bedrock. Source code in pydantic_ai_slim/pydantic_ai/providers/bedrock.py", "url": "https://ai.pydantic.dev/api/providers/index.html#pydantic_ai.providers.bedrock.bedrock_deepseek_model_profile", "page": "api/providers/index.html", "source_site": "pydantic_ai"}
{"title": "BedrockProvider", "anchor": "pydantic_ai.providers.bedrock.BedrockProvider", "heading_level": 3, "md_text": "Bases: Provider[BaseClient] Provider for AWS Bedrock. Source code in pydantic_ai_slim/pydantic_ai/providers/bedrock.py #### \\_\\_init\\_\\_ Initialize the Bedrock provider. Parameters: Source code in pydantic_ai_slim/pydantic_ai/providers/bedrock.py", "url": "https://ai.pydantic.dev/api/providers/index.html#pydantic_ai.providers.bedrock.BedrockProvider", "page": "api/providers/index.html", "source_site": "pydantic_ai"}
{"title": "groq_moonshotai_model_profile", "anchor": "pydantic_ai.providers.groq.groq_moonshotai_model_profile", "heading_level": 3, "md_text": "Get the model profile for an MoonshotAI model used with the Groq provider. Source code in pydantic_ai_slim/pydantic_ai/providers/groq.py", "url": "https://ai.pydantic.dev/api/providers/index.html#pydantic_ai.providers.groq.groq_moonshotai_model_profile", "page": "api/providers/index.html", "source_site": "pydantic_ai"}
{"title": "meta_groq_model_profile", "anchor": "pydantic_ai.providers.groq.meta_groq_model_profile", "heading_level": 3, "md_text": "Get the model profile for a Meta model used with the Groq provider. Source code in pydantic_ai_slim/pydantic_ai/providers/groq.py", "url": "https://ai.pydantic.dev/api/providers/index.html#pydantic_ai.providers.groq.meta_groq_model_profile", "page": "api/providers/index.html", "source_site": "pydantic_ai"}
{"title": "GroqProvider", "anchor": "pydantic_ai.providers.groq.GroqProvider", "heading_level": 3, "md_text": "Bases: Provider[AsyncGroq] Provider for Groq API. Source code in pydantic_ai_slim/pydantic_ai/providers/groq.py #### \\_\\_init\\_\\_ Create a new Groq provider. Parameters: Source code in pydantic_ai_slim/pydantic_ai/providers/groq.py", "url": "https://ai.pydantic.dev/api/providers/index.html#pydantic_ai.providers.groq.GroqProvider", "page": "api/providers/index.html", "source_site": "pydantic_ai"}
{"title": "AzureProvider", "anchor": "pydantic_ai.providers.azure.AzureProvider", "heading_level": 3, "md_text": "Bases: Provider[AsyncOpenAI] Provider for Azure OpenAI API. See <https://azure.microsoft.com/en-us/products/ai-foundry> for more information. Source code in pydantic_ai_slim/pydantic_ai/providers/azure.py #### \\_\\_init\\_\\_ Create a new Azure provider. Parameters: Source code in pydantic_ai_slim/pydantic_ai/providers/azure.py", "url": "https://ai.pydantic.dev/api/providers/index.html#pydantic_ai.providers.azure.AzureProvider", "page": "api/providers/index.html", "source_site": "pydantic_ai"}
{"title": "CohereProvider", "anchor": "pydantic_ai.providers.cohere.CohereProvider", "heading_level": 3, "md_text": "Bases: Provider[AsyncClientV2] Provider for Cohere API. Source code in pydantic_ai_slim/pydantic_ai/providers/cohere.py #### \\_\\_init\\_\\_ Create a new Cohere provider. Parameters: Source code in pydantic_ai_slim/pydantic_ai/providers/cohere.py", "url": "https://ai.pydantic.dev/api/providers/index.html#pydantic_ai.providers.cohere.CohereProvider", "page": "api/providers/index.html", "source_site": "pydantic_ai"}
{"title": "__init__", "anchor": "pydantic_ai.providers.mistral.MistralProvider.__init__", "heading_level": 3, "md_text": "Create a new Mistral provider. Parameters: Source code in pydantic_ai_slim/pydantic_ai/providers/mistral.py", "url": "https://ai.pydantic.dev/api/providers/index.html#pydantic_ai.providers.mistral.MistralProvider.__init__", "page": "api/providers/index.html", "source_site": "pydantic_ai"}
{"title": "__init__", "anchor": "pydantic_ai.providers.github.GitHubProvider.__init__", "heading_level": 3, "md_text": "Create a new GitHub Models provider. Parameters: Source code in pydantic_ai_slim/pydantic_ai/providers/github.py", "url": "https://ai.pydantic.dev/api/providers/index.html#pydantic_ai.providers.github.GitHubProvider.__init__", "page": "api/providers/index.html", "source_site": "pydantic_ai"}
{"title": "__init__", "anchor": "pydantic_ai.providers.huggingface.HuggingFaceProvider.__init__", "heading_level": 3, "md_text": "Create a new Hugging Face provider. Parameters: Source code in pydantic_ai_slim/pydantic_ai/providers/huggingface.py", "url": "https://ai.pydantic.dev/api/providers/index.html#pydantic_ai.providers.huggingface.HuggingFaceProvider.__init__", "page": "api/providers/index.html", "source_site": "pydantic_ai"}
{"title": "__init__", "anchor": "pydantic_ai.providers.ollama.OllamaProvider.__init__", "heading_level": 3, "md_text": "Create a new Ollama provider. Parameters: Source code in pydantic_ai_slim/pydantic_ai/providers/ollama.py", "url": "https://ai.pydantic.dev/api/providers/index.html#pydantic_ai.providers.ollama.OllamaProvider.__init__", "page": "api/providers/index.html", "source_site": "pydantic_ai"}
{"title": "__init__", "anchor": "pydantic_ai.providers.litellm.LiteLLMProvider.__init__", "heading_level": 3, "md_text": "Initialize a LiteLLM provider. Parameters: Source code in pydantic_ai_slim/pydantic_ai/providers/litellm.py", "url": "https://ai.pydantic.dev/api/providers/index.html#pydantic_ai.providers.litellm.LiteLLMProvider.__init__", "page": "api/providers/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_graph.beta.decision", "anchor": "pydantic_graphbetadecision", "heading_level": 1, "md_text": "Decision node implementation for conditional branching in graph execution. This module provides the Decision node type and related classes for implementing conditional branching logic in parallel control flow graphs. Decision nodes allow the graph to choose different execution paths based on runtime conditions. ### StateT module-attribute Type variable for graph state. ### DepsT module-attribute Type variable for graph dependencies. ### HandledT module-attribute Type variable used to track types handled by the branches of a Decision. ### T module-attribute Generic type variable. ### Decision dataclass Bases: Generic[StateT, DepsT, HandledT] Decision node for conditional branching in graph execution. A Decision node evaluates conditions and routes execution to different branches based on the input data type or custom matching logic. Source code in pydantic_graph/pydantic_graph/beta/decision.py #### id instance-attribute Unique identifier for this decision node. #### branches instance-attribute List of branches that can be taken from this decision. #### note instance-attribute Optional documentation note for this decision. #### branch Add a new branch to this decision. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/decision.py ### SourceT module-attribute Type variable for source data for a DecisionBranch. ### DecisionBranch dataclass Bases: Generic[SourceT] Represents a single branch within a decision node. Each branch defines the conditions under which it should be taken and the path to follow when those conditions are met. Note: with the current design, it is actually *critical* that this class is invariant in SourceT for the sake of type-checking that inputs to a Decision are actually handled. See the # type: ignore comment in tests.graph.beta.test_graph_edge_cases.test_decision_no_matching_branch for an example of how this works. Source code in pydantic_graph/pydantic_graph/beta/decision.py #### source instance-attribute The expected type of data for this branch. This is necessary for exhaustiveness-checking when handling the inputs to a decision node. #### matches instance-attribute An optional predicate function used to determine whether input data matches this branch. If None , default logic is used which attempts to check the value for type-compatibility with the source type: \\* If source is Any or object , the branch will always match \\* If source is a Literal type, this branch will match if the value is one of the parametrizing literal values \\* If source is any other type, the value will be checked for matching using isinstance Inputs are tested against each branch of a decision node in order, and the path of the first matching branch is used to handle the input value. #### path instance-attribute The execution path to follow when an input value matches this branch of a decision node. This can include transforming, mapping, and broadcasting the output before sending to the next node or nodes. The path can also include position-aware labels which are used when generating mermaid diagrams. #### destinations instance-attribute The destination nodes that can be referenced by DestinationMarker in the path. ### OutputT module-attribute Type variable for the output data of a node. ### NewOutputT module-attribute Type variable for transformed output. ### DecisionBranchBuilder dataclass Bases: Generic[StateT, DepsT, OutputT, SourceT, HandledT] Builder for constructing decision branches with fluent API. This builder provides methods to configure branches with destinations, forks, and transformations in a type-safe manner. Instances of this class should be created using [ GraphBuilder.match ](../beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder), not created directly. Source code in pydantic_graph/pydantic_graph/beta/decision.py #### to Set the destination(s) for this branch. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/decision.py #### broadcast Broadcast this decision branch into multiple destinations. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/decision.py #### transform Apply a transformation to the branch's output. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/decision.py #### map Spread the branch's output. To do this, the current output must be iterable, and any subsequent steps in the path being built for this branch will be applied to each item of the current output in parallel. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/decision.py #### label Apply a label to the branch at the current point in the path being built. These labels are only used in generated mermaid diagrams. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/decision.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_decision/index.html#pydantic_graphbetadecision", "page": "api/pydantic_graph/beta_decision/index.html", "source_site": "pydantic_ai"}
{"title": "StateT module-attribute", "anchor": "pydantic_graph.beta.decision.StateT", "heading_level": 3, "md_text": "Type variable for graph state.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_decision/index.html#pydantic_graph.beta.decision.StateT", "page": "api/pydantic_graph/beta_decision/index.html", "source_site": "pydantic_ai"}
{"title": "DepsT module-attribute", "anchor": "pydantic_graph.beta.decision.DepsT", "heading_level": 3, "md_text": "Type variable for graph dependencies.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_decision/index.html#pydantic_graph.beta.decision.DepsT", "page": "api/pydantic_graph/beta_decision/index.html", "source_site": "pydantic_ai"}
{"title": "HandledT module-attribute", "anchor": "pydantic_graph.beta.decision.HandledT", "heading_level": 3, "md_text": "Type variable used to track types handled by the branches of a Decision.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_decision/index.html#pydantic_graph.beta.decision.HandledT", "page": "api/pydantic_graph/beta_decision/index.html", "source_site": "pydantic_ai"}
{"title": "T module-attribute", "anchor": "pydantic_graph.beta.decision.T", "heading_level": 3, "md_text": "Generic type variable.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_decision/index.html#pydantic_graph.beta.decision.T", "page": "api/pydantic_graph/beta_decision/index.html", "source_site": "pydantic_ai"}
{"title": "Decision dataclass", "anchor": "pydantic_graph.beta.decision.Decision", "heading_level": 3, "md_text": "Bases: Generic[StateT, DepsT, HandledT] Decision node for conditional branching in graph execution. A Decision node evaluates conditions and routes execution to different branches based on the input data type or custom matching logic. Source code in pydantic_graph/pydantic_graph/beta/decision.py #### id instance-attribute Unique identifier for this decision node. #### branches instance-attribute List of branches that can be taken from this decision. #### note instance-attribute Optional documentation note for this decision. #### branch Add a new branch to this decision. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/decision.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_decision/index.html#pydantic_graph.beta.decision.Decision", "page": "api/pydantic_graph/beta_decision/index.html", "source_site": "pydantic_ai"}
{"title": "SourceT module-attribute", "anchor": "pydantic_graph.beta.decision.SourceT", "heading_level": 3, "md_text": "Type variable for source data for a DecisionBranch.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_decision/index.html#pydantic_graph.beta.decision.SourceT", "page": "api/pydantic_graph/beta_decision/index.html", "source_site": "pydantic_ai"}
{"title": "DecisionBranch dataclass", "anchor": "pydantic_graph.beta.decision.DecisionBranch", "heading_level": 3, "md_text": "Bases: Generic[SourceT] Represents a single branch within a decision node. Each branch defines the conditions under which it should be taken and the path to follow when those conditions are met. Note: with the current design, it is actually *critical* that this class is invariant in SourceT for the sake of type-checking that inputs to a Decision are actually handled. See the # type: ignore comment in tests.graph.beta.test_graph_edge_cases.test_decision_no_matching_branch for an example of how this works. Source code in pydantic_graph/pydantic_graph/beta/decision.py #### source instance-attribute The expected type of data for this branch. This is necessary for exhaustiveness-checking when handling the inputs to a decision node. #### matches instance-attribute An optional predicate function used to determine whether input data matches this branch. If None , default logic is used which attempts to check the value for type-compatibility with the source type: \\* If source is Any or object , the branch will always match \\* If source is a Literal type, this branch will match if the value is one of the parametrizing literal values \\* If source is any other type, the value will be checked for matching using isinstance Inputs are tested against each branch of a decision node in order, and the path of the first matching branch is used to handle the input value. #### path instance-attribute The execution path to follow when an input value matches this branch of a decision node. This can include transforming, mapping, and broadcasting the output before sending to the next node or nodes. The path can also include position-aware labels which are used when generating mermaid diagrams. #### destinations instance-attribute The destination nodes that can be referenced by DestinationMarker in the path.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_decision/index.html#pydantic_graph.beta.decision.DecisionBranch", "page": "api/pydantic_graph/beta_decision/index.html", "source_site": "pydantic_ai"}
{"title": "OutputT module-attribute", "anchor": "pydantic_graph.beta.decision.OutputT", "heading_level": 3, "md_text": "Type variable for the output data of a node.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_decision/index.html#pydantic_graph.beta.decision.OutputT", "page": "api/pydantic_graph/beta_decision/index.html", "source_site": "pydantic_ai"}
{"title": "NewOutputT module-attribute", "anchor": "pydantic_graph.beta.decision.NewOutputT", "heading_level": 3, "md_text": "Type variable for transformed output.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_decision/index.html#pydantic_graph.beta.decision.NewOutputT", "page": "api/pydantic_graph/beta_decision/index.html", "source_site": "pydantic_ai"}
{"title": "DecisionBranchBuilder dataclass", "anchor": "pydantic_graph.beta.decision.DecisionBranchBuilder", "heading_level": 3, "md_text": "Bases: Generic[StateT, DepsT, OutputT, SourceT, HandledT] Builder for constructing decision branches with fluent API. This builder provides methods to configure branches with destinations, forks, and transformations in a type-safe manner. Instances of this class should be created using [ GraphBuilder.match ](../beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder), not created directly. Source code in pydantic_graph/pydantic_graph/beta/decision.py #### to Set the destination(s) for this branch. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/decision.py #### broadcast Broadcast this decision branch into multiple destinations. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/decision.py #### transform Apply a transformation to the branch's output. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/decision.py #### map Spread the branch's output. To do this, the current output must be iterable, and any subsequent steps in the path being built for this branch will be applied to each item of the current output in parallel. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/decision.py #### label Apply a label to the branch at the current point in the path being built. These labels are only used in generated mermaid diagrams. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/decision.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_decision/index.html#pydantic_graph.beta.decision.DecisionBranchBuilder", "page": "api/pydantic_graph/beta_decision/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_evals.reporting", "anchor": "pydantic_evalsreporting", "heading_level": 1, "md_text": "### ReportCase dataclass Bases: Generic[InputsT, OutputT, MetadataT] A single case in an evaluation report. Source code in pydantic_evals/pydantic_evals/reporting/__init__.py #### name instance-attribute The name of the [case](../dataset/index.html#pydantic_evals.dataset.Case). #### inputs instance-attribute The inputs to the task, from [ Case.inputs ](../dataset/index.html#pydantic_evals.dataset.Case.inputs). #### metadata instance-attribute Any metadata associated with the case, from [ Case.metadata ](../dataset/index.html#pydantic_evals.dataset.Case.metadata). #### expected\\_output instance-attribute The expected output of the task, from [ Case.expected_output ](../dataset/index.html#pydantic_evals.dataset.Case.expected_output). #### output instance-attribute The output of the task execution. #### trace\\_id class-attribute instance-attribute The trace ID of the case span. #### span\\_id class-attribute instance-attribute The span ID of the case span. ### ReportCaseFailure dataclass Bases: Generic[InputsT, OutputT, MetadataT] A single case in an evaluation report that failed due to an error during task execution. Source code in pydantic_evals/pydantic_evals/reporting/__init__.py #### name instance-attribute The name of the [case](../dataset/index.html#pydantic_evals.dataset.Case). #### inputs instance-attribute The inputs to the task, from [ Case.inputs ](../dataset/index.html#pydantic_evals.dataset.Case.inputs). #### metadata instance-attribute Any metadata associated with the case, from [ Case.metadata ](../dataset/index.html#pydantic_evals.dataset.Case.metadata). #### expected\\_output instance-attribute The expected output of the task, from [ Case.expected_output ](../dataset/index.html#pydantic_evals.dataset.Case.expected_output). #### error\\_message instance-attribute The message of the exception that caused the failure. #### error\\_stacktrace instance-attribute The stacktrace of the exception that caused the failure. #### trace\\_id class-attribute instance-attribute The trace ID of the case span. #### span\\_id class-attribute instance-attribute The span ID of the case span. ### ReportCaseAggregate Bases: BaseModel A synthetic case that summarizes a set of cases. Source code in pydantic_evals/pydantic_evals/reporting/__init__.py #### average staticmethod Produce a synthetic \"summary\" case by averaging quantitative attributes. Source code in pydantic_evals/pydantic_evals/reporting/__init__.py ### EvaluationReport dataclass Bases: Generic[InputsT, OutputT, MetadataT] A report of the results of evaluating a model on a set of cases. Source code in pydantic_evals/pydantic_evals/reporting/__init__.py #### name instance-attribute The name of the report. #### cases instance-attribute The cases in the report. #### failures class-attribute instance-attribute The failures in the report. These are cases where task execution raised an exception. #### trace\\_id class-attribute instance-attribute The trace ID of the evaluation. #### span\\_id class-attribute instance-attribute The span ID of the evaluation. #### render Render this report to a nicely-formatted string, optionally comparing it to a baseline report. If you want more control over the output, use console_table instead and pass it to rich.Console.print . Source code in pydantic_evals/pydantic_evals/reporting/__init__.py #### print Print this report to the console, optionally comparing it to a baseline report. If you want more control over the output, use console_table instead and pass it to rich.Console.print . Source code in pydantic_evals/pydantic_evals/reporting/__init__.py #### console\\_table Return a table containing the data from this report, or the diff between this report and a baseline report. Optionally include input and output details. Source code in pydantic_evals/pydantic_evals/reporting/__init__.py #### failures\\_table Return a table containing the failures in this report. Source code in pydantic_evals/pydantic_evals/reporting/__init__.py #### \\_\\_str\\_\\_ Return a string representation of the report. Source code in pydantic_evals/pydantic_evals/reporting/__init__.py ### RenderValueConfig Bases: TypedDict A configuration for rendering a values in an Evaluation report. Source code in pydantic_evals/pydantic_evals/reporting/__init__.py ### RenderNumberConfig Bases: TypedDict A configuration for rendering a particular score or metric in an Evaluation report. See the implementation of _RenderNumber for more clarity on how these parameters affect the rendering. Source code in pydantic_evals/pydantic_evals/reporting/__init__.py #### value\\_formatter instance-attribute The logic to use for formatting values. * If not provided, format as ints if all values are ints, otherwise at least one decimal place and at least four significant figures. * You can also use a custom string format spec, e.g. '{:.3f}' * You can also use a custom function, e.g. lambda x: f'{x:.3f}' #### diff\\_formatter instance-attribute The logic to use for formatting details about the diff. The strings produced by the value\\_formatter will always be included in the reports, but the diff\\_formatter is used to produce additional text about the difference between the old and new values, such as the absolute or relative difference. * If not provided, format as ints if all values are ints, otherwise at least one decimal place and at least four significant figures, and will include the percentage change. * You can also use a custom string format spec, e.g. '{:+.3f}' * You can also use a custom function, e.g. lambda x: f'{x:+.3f}'. If this function returns None, no extra diff text will be added. * You can also use None to never generate extra diff text. #### diff\\_atol instance-attribute The absolute tolerance for considering a difference \"significant\". A difference is \"significant\" if abs(new - old) < self.diff_atol + self.diff_rtol * abs(old) . If a difference is not significant, it will not have the diff styles applied. Note that we still show both the rendered before and after values in the diff any time they differ, even if the difference is not significant. (If the rendered values are exactly the same, we only show the value once.) If not provided, use 1e-6. #### diff\\_rtol instance-attribute The relative tolerance for considering a difference \"significant\". See the description of diff_atol for more details about what makes a difference \"significant\". If not provided, use 0.001 if all values are ints, otherwise 0.05. #### diff\\_increase\\_style instance-attribute The style to apply to diffed values that have a significant increase. See the description of diff_atol for more details about what makes a difference \"significant\". If not provided, use green for scores and red for metrics. You can also use arbitrary rich styles, such as \"bold red\". #### diff\\_decrease\\_style instance-attribute The style to apply to diffed values that have significant decrease. See the description of diff_atol for more details about what makes a difference \"significant\". If not provided, use red for scores and green for metrics. You can also use arbitrary rich styles, such as \"bold red\". ### EvaluationRenderer dataclass A class for rendering an EvalReport or the diff between two EvalReports. Source code in pydantic_evals/pydantic_evals/reporting/__init__.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/reporting/index.html#pydantic_evalsreporting", "page": "api/pydantic_evals/reporting/index.html", "source_site": "pydantic_ai"}
{"title": "ReportCase dataclass", "anchor": "pydantic_evals.reporting.ReportCase", "heading_level": 3, "md_text": "Bases: Generic[InputsT, OutputT, MetadataT] A single case in an evaluation report. Source code in pydantic_evals/pydantic_evals/reporting/__init__.py #### name instance-attribute The name of the [case](../dataset/index.html#pydantic_evals.dataset.Case). #### inputs instance-attribute The inputs to the task, from [ Case.inputs ](../dataset/index.html#pydantic_evals.dataset.Case.inputs). #### metadata instance-attribute Any metadata associated with the case, from [ Case.metadata ](../dataset/index.html#pydantic_evals.dataset.Case.metadata). #### expected\\_output instance-attribute The expected output of the task, from [ Case.expected_output ](../dataset/index.html#pydantic_evals.dataset.Case.expected_output). #### output instance-attribute The output of the task execution. #### trace\\_id class-attribute instance-attribute The trace ID of the case span. #### span\\_id class-attribute instance-attribute The span ID of the case span.", "url": "https://ai.pydantic.dev/api/pydantic_evals/reporting/index.html#pydantic_evals.reporting.ReportCase", "page": "api/pydantic_evals/reporting/index.html", "source_site": "pydantic_ai"}
{"title": "ReportCaseFailure dataclass", "anchor": "pydantic_evals.reporting.ReportCaseFailure", "heading_level": 3, "md_text": "Bases: Generic[InputsT, OutputT, MetadataT] A single case in an evaluation report that failed due to an error during task execution. Source code in pydantic_evals/pydantic_evals/reporting/__init__.py #### name instance-attribute The name of the [case](../dataset/index.html#pydantic_evals.dataset.Case). #### inputs instance-attribute The inputs to the task, from [ Case.inputs ](../dataset/index.html#pydantic_evals.dataset.Case.inputs). #### metadata instance-attribute Any metadata associated with the case, from [ Case.metadata ](../dataset/index.html#pydantic_evals.dataset.Case.metadata). #### expected\\_output instance-attribute The expected output of the task, from [ Case.expected_output ](../dataset/index.html#pydantic_evals.dataset.Case.expected_output). #### error\\_message instance-attribute The message of the exception that caused the failure. #### error\\_stacktrace instance-attribute The stacktrace of the exception that caused the failure. #### trace\\_id class-attribute instance-attribute The trace ID of the case span. #### span\\_id class-attribute instance-attribute The span ID of the case span.", "url": "https://ai.pydantic.dev/api/pydantic_evals/reporting/index.html#pydantic_evals.reporting.ReportCaseFailure", "page": "api/pydantic_evals/reporting/index.html", "source_site": "pydantic_ai"}
{"title": "ReportCaseAggregate", "anchor": "pydantic_evals.reporting.ReportCaseAggregate", "heading_level": 3, "md_text": "Bases: BaseModel A synthetic case that summarizes a set of cases. Source code in pydantic_evals/pydantic_evals/reporting/__init__.py #### average staticmethod Produce a synthetic \"summary\" case by averaging quantitative attributes. Source code in pydantic_evals/pydantic_evals/reporting/__init__.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/reporting/index.html#pydantic_evals.reporting.ReportCaseAggregate", "page": "api/pydantic_evals/reporting/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluationReport dataclass", "anchor": "pydantic_evals.reporting.EvaluationReport", "heading_level": 3, "md_text": "Bases: Generic[InputsT, OutputT, MetadataT] A report of the results of evaluating a model on a set of cases. Source code in pydantic_evals/pydantic_evals/reporting/__init__.py #### name instance-attribute The name of the report. #### cases instance-attribute The cases in the report. #### failures class-attribute instance-attribute The failures in the report. These are cases where task execution raised an exception. #### trace\\_id class-attribute instance-attribute The trace ID of the evaluation. #### span\\_id class-attribute instance-attribute The span ID of the evaluation. #### render Render this report to a nicely-formatted string, optionally comparing it to a baseline report. If you want more control over the output, use console_table instead and pass it to rich.Console.print . Source code in pydantic_evals/pydantic_evals/reporting/__init__.py #### print Print this report to the console, optionally comparing it to a baseline report. If you want more control over the output, use console_table instead and pass it to rich.Console.print . Source code in pydantic_evals/pydantic_evals/reporting/__init__.py #### console\\_table Return a table containing the data from this report, or the diff between this report and a baseline report. Optionally include input and output details. Source code in pydantic_evals/pydantic_evals/reporting/__init__.py #### failures\\_table Return a table containing the failures in this report. Source code in pydantic_evals/pydantic_evals/reporting/__init__.py #### \\_\\_str\\_\\_ Return a string representation of the report. Source code in pydantic_evals/pydantic_evals/reporting/__init__.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/reporting/index.html#pydantic_evals.reporting.EvaluationReport", "page": "api/pydantic_evals/reporting/index.html", "source_site": "pydantic_ai"}
{"title": "RenderValueConfig", "anchor": "pydantic_evals.reporting.RenderValueConfig", "heading_level": 3, "md_text": "Bases: TypedDict A configuration for rendering a values in an Evaluation report. Source code in pydantic_evals/pydantic_evals/reporting/__init__.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/reporting/index.html#pydantic_evals.reporting.RenderValueConfig", "page": "api/pydantic_evals/reporting/index.html", "source_site": "pydantic_ai"}
{"title": "RenderNumberConfig", "anchor": "pydantic_evals.reporting.RenderNumberConfig", "heading_level": 3, "md_text": "Bases: TypedDict A configuration for rendering a particular score or metric in an Evaluation report. See the implementation of _RenderNumber for more clarity on how these parameters affect the rendering. Source code in pydantic_evals/pydantic_evals/reporting/__init__.py #### value\\_formatter instance-attribute The logic to use for formatting values. * If not provided, format as ints if all values are ints, otherwise at least one decimal place and at least four significant figures. * You can also use a custom string format spec, e.g. '{:.3f}' * You can also use a custom function, e.g. lambda x: f'{x:.3f}' #### diff\\_formatter instance-attribute The logic to use for formatting details about the diff. The strings produced by the value\\_formatter will always be included in the reports, but the diff\\_formatter is used to produce additional text about the difference between the old and new values, such as the absolute or relative difference. * If not provided, format as ints if all values are ints, otherwise at least one decimal place and at least four significant figures, and will include the percentage change. * You can also use a custom string format spec, e.g. '{:+.3f}' * You can also use a custom function, e.g. lambda x: f'{x:+.3f}'. If this function returns None, no extra diff text will be added. * You can also use None to never generate extra diff text. #### diff\\_atol instance-attribute The absolute tolerance for considering a difference \"significant\". A difference is \"significant\" if abs(new - old) < self.diff_atol + self.diff_rtol * abs(old) . If a difference is not significant, it will not have the diff styles applied. Note that we still show both the rendered before and after values in the diff any time they differ, even if the difference is not significant. (If the rendered values are exactly the same, we only show the value once.) If not provided, use 1e-6. #### diff\\_rtol instance-attribute The relative tolerance for considering a difference \"significant\". See the description of diff_atol for more details about what makes a difference \"significant\". If not provided, use 0.001 if all values are ints, otherwise 0.05. #### diff\\_increase\\_style instance-attribute The style to apply to diffed values that have a significant increase. See the description of diff_atol for more details about what makes a difference \"significant\". If not provided, use green for scores and red for metrics. You can also use arbitrary rich styles, such as \"bold red\". #### diff\\_decrease\\_style instance-attribute The style to apply to diffed values that have significant decrease. See the description of diff_atol for more details about what makes a difference \"significant\". If not provided, use red for scores and green for metrics. You can also use arbitrary rich styles, such as \"bold red\".", "url": "https://ai.pydantic.dev/api/pydantic_evals/reporting/index.html#pydantic_evals.reporting.RenderNumberConfig", "page": "api/pydantic_evals/reporting/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluationRenderer dataclass", "anchor": "pydantic_evals.reporting.EvaluationRenderer", "heading_level": 3, "md_text": "A class for rendering an EvalReport or the diff between two EvalReports. Source code in pydantic_evals/pydantic_evals/reporting/__init__.py", "url": "https://ai.pydantic.dev/api/pydantic_evals/reporting/index.html#pydantic_evals.reporting.EvaluationRenderer", "page": "api/pydantic_evals/reporting/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.agent", "anchor": "pydantic_aiagent", "heading_level": 1, "md_text": "### Agent dataclass Bases: AbstractAgent[AgentDepsT, OutputDataT] Class for defining \"agents\" - a way to have a specific type of \"conversation\" with an LLM. Agents are generic in the dependency type they take [ AgentDepsT ](../tools/index.html#pydantic_ai.tools.AgentDepsT) and the output type they return, [ OutputDataT ](../output/index.html#pydantic_ai.output.OutputDataT). By default, if neither generic parameter is customised, agents have type Agent[None, str] . Minimal usage example: Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### \\_\\_init\\_\\_ Create an agent. Parameters: Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### end\\_strategy instance-attribute Strategy for handling tool calls when a final result is found. #### model\\_settings instance-attribute Optional model request settings to use for this agents's runs, by default. Note, if model_settings is provided by run , run_sync , or run_stream , those settings will be merged with this value, with the runtime argument taking priority. #### instrument instance-attribute Options to automatically instrument with OpenTelemetry. #### instrument\\_all staticmethod Set the instrumentation options for all agents where instrument is not set. Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### model property writable The default model configured for this agent. #### name property writable The name of the agent, used for logging. If None , we try to infer the agent name from the call frame when the agent is first run. #### deps\\_type property The type of dependencies used by the agent. #### output\\_type property The type of data output by agent runs, used to validate the data returned by the model, defaults to str . #### event\\_stream\\_handler property Optional handler for events from the model's streaming response and the agent's execution of tools. #### iter async A contextmanager which can be used to iterate over the agent graph's nodes as they are executed. This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an AgentRun object. The AgentRun can be used to async-iterate over the nodes of the graph as they are executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the stream of events coming from the execution of tools. The AgentRun also provides methods to access the full message history, new messages, and usage statistics, and the final result of the run once it has completed. For more details, see the documentation of AgentRun . Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### override Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions. This is particularly useful when testing. You can find an example of this [here](../../testing/index.html#overriding-model-via-pytest-fixtures). Parameters: Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### instructions Decorator to register an instructions function. Optionally takes [ RunContext ](../tools/index.html#pydantic_ai.tools.RunContext) as its only argument. Can decorate a sync or async functions. The decorator can be used bare ( agent.instructions ). Overloads for every possible signature of instructions are included so the decorator doesn't obscure the type of the function. Example: Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### system\\_prompt Decorator to register a system prompt function. Optionally takes [ RunContext ](../tools/index.html#pydantic_ai.tools.RunContext) as its only argument. Can decorate a sync or async functions. The decorator can be used either bare ( agent.system_prompt ) or as a function call ( agent.system_prompt(...) ), see the examples below. Overloads for every possible signature of system_prompt are included so the decorator doesn't obscure the type of the function, see tests/typed_agent.py for tests. Parameters: Example: Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### output\\_validator Decorator to register an output validator function. Optionally takes [ RunContext ](../tools/index.html#pydantic_ai.tools.RunContext) as its first argument. Can decorate a sync or async functions. Overloads for every possible signature of output_validator are included so the decorator doesn't obscure the type of the function, see tests/typed_agent.py for tests. Example: Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### tool Decorator to register a tool function which takes [ RunContext ](../tools/index.html#pydantic_ai.tools.RunContext) as its first argument. Can decorate a sync or async functions. The docstring is inspected to extract both the tool description and description of each parameter, [learn more](../../tools/index.html#function-tools-and-schema). We can't add overloads for every possible signature of tool, since the return type is a recursive union so the signature of functions decorated with @agent.tool is obscured. Example: Parameters: Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### tool\\_plain Decorator to register a tool function which DOES NOT take RunContext as an argument. Can decorate a sync or async functions. The docstring is inspected to extract both the tool description and description of each parameter, [learn more](../../tools/index.html#function-tools-and-schema). We can't add overloads for every possible signature of tool, since the return type is a recursive union so the signature of functions decorated with @agent.tool is obscured. Example: Parameters: Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### toolset Decorator to register a toolset function which takes [ RunContext ](../tools/index.html#pydantic_ai.tools.RunContext) as its only argument. Can decorate a sync or async functions. The decorator can be used bare ( agent.toolset ). Example: Parameters: Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### toolsets property All toolsets registered on the agent, including a function toolset holding tools that were registered on the agent directly. Output tools are not included. #### \\_\\_aenter\\_\\_ async Enter the agent context. This will start all [ MCPServerStdio s](../mcp/index.html#pydantic_ai.mcp.MCPServerStdio) registered as toolsets so they are ready to be used. This is a no-op if the agent has already been entered. Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### set\\_mcp\\_sampling\\_model Set the sampling model on all MCP servers registered with the agent. If no sampling model is provided, the agent's model will be used. Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### run\\_mcp\\_servers async deprecated Deprecated run_mcp_servers is deprecated, use async with agent: instead. If you need to set a sampling model on all MCP servers, use agent.set_mcp_sampling_model() . Run [ MCPServerStdio s](../mcp/index.html#pydantic_ai.mcp.MCPServerStdio) so they can be used by the agent. Deprecated: use [ async with agent ](index.html#pydantic_ai.agent.Agent.__aenter__) instead. If you need to set a sampling model on all MCP servers, use [ agent.set_mcp_sampling_model() ](index.html#pydantic_ai.agent.Agent.set_mcp_sampling_model). Returns: a context manager to start and shutdown the servers. Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py ### AbstractAgent Bases: Generic[AgentDepsT, OutputDataT] , ABC Abstract superclass for [ Agent ](index.html#pydantic_ai.agent.Agent), [ WrapperAgent ](index.html#pydantic_ai.agent.WrapperAgent), and your own custom agent implementations. Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### model abstractmethod property The default model configured for this agent. #### name abstractmethod property writable The name of the agent, used for logging. If None , we try to infer the agent name from the call frame when the agent is first run. #### deps\\_type abstractmethod property The type of dependencies used by the agent. #### output\\_type abstractmethod property The type of data output by agent runs, used to validate the data returned by the model, defaults to str . #### event\\_stream\\_handler abstractmethod property Optional handler for events from the model's streaming response and the agent's execution of tools. #### toolsets abstractmethod property All toolsets registered on the agent. Output tools are not included. #### run async Run the agent with a user prompt in async mode. This method builds an internal agent graph (using system prompts, tools and output schemas) and then runs the graph to completion. The result of the run is returned. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### run\\_sync Synchronously run the agent with a user prompt. This is a convenience method that wraps [ self.run ](index.html#pydantic_ai.agent.AbstractAgent.run) with loop.run_until_complete(...) . You therefore can't use this method inside async code or if there's an active event loop. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### run\\_stream async Run the agent with a user prompt in async streaming mode. This method builds an internal agent graph (using system prompts, tools and output schemas) and then runs the graph until the model produces output matching the output_type , for example text or structured data. At this point, a streaming run result object is yielded from which you can stream the output as it comes in, and -- once this output has completed streaming -- get the complete output, message history, and usage. As this method will consider the first output matching the output_type to be the final output, it will stop running the agent graph and will not execute any tool calls made by the model after this \"final\" output. If you want to always run the agent graph to completion and stream events and output at the same time, use [ agent.run() ](index.html#pydantic_ai.agent.AbstractAgent.run) with an event_stream_handler or [ agent.iter() ](index.html#pydantic_ai.agent.AbstractAgent.iter) instead. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### run\\_stream\\_events Run the agent with a user prompt in async mode and stream events from the run. This is a convenience method that wraps [ self.run ](index.html#pydantic_ai.agent.AbstractAgent.run) and uses the event_stream_handler kwarg to get a stream of events from the run. Example: Arguments are the same as for [ self.run ](index.html#pydantic_ai.agent.AbstractAgent.run), except that event_stream_handler is now allowed. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### iter abstractmethod async A contextmanager which can be used to iterate over the agent graph's nodes as they are executed. This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an AgentRun object. The AgentRun can be used to async-iterate over the nodes of the graph as they are executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the stream of events coming from the execution of tools. The AgentRun also provides methods to access the full message history, new messages, and usage statistics, and the final result of the run once it has completed. For more details, see the documentation of AgentRun . Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### override abstractmethod Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions. This is particularly useful when testing. You can find an example of this [here](../../testing/index.html#overriding-model-via-pytest-fixtures). Parameters: Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### sequential\\_tool\\_calls staticmethod Run tool calls sequentially during the context. Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### is\\_model\\_request\\_node staticmethod Check if the node is a ModelRequestNode , narrowing the type if it is. This method preserves the generic parameters while narrowing the type, unlike a direct call to isinstance . Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### is\\_call\\_tools\\_node staticmethod Check if the node is a CallToolsNode , narrowing the type if it is. This method preserves the generic parameters while narrowing the type, unlike a direct call to isinstance . Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### is\\_user\\_prompt\\_node staticmethod Check if the node is a UserPromptNode , narrowing the type if it is. This method preserves the generic parameters while narrowing the type, unlike a direct call to isinstance . Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### is\\_end\\_node staticmethod Check if the node is a End , narrowing the type if it is. This method preserves the generic parameters while narrowing the type, unlike a direct call to isinstance . Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### to\\_ag\\_ui Returns an ASGI application that handles every AG-UI request by running the agent. Note that the deps will be the same for each request, with the exception of the AG-UI state that's injected into the state field of a deps object that implements the [ StateHandler ](../ag_ui/index.html#pydantic_ai.ag_ui.StateHandler) protocol. To provide different deps for each request (e.g. based on the authenticated user), use [ pydantic_ai.ag_ui.run_ag_ui ](../ag_ui/index.html#pydantic_ai.ag_ui.run_ag_ui) or [ pydantic_ai.ag_ui.handle_ag_ui_request ](../ag_ui/index.html#pydantic_ai.ag_ui.handle_ag_ui_request) instead. Example: The app is an ASGI application that can be used with any ASGI server. To run the application, you can use the following command: See [AG-UI docs](../../ag-ui/index.html) for more information. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### to\\_a2a Convert the agent to a FastA2A application. Example: The app is an ASGI application that can be used with any ASGI server. To run the application, you can use the following command: Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### to\\_cli async Run the agent in a CLI chat interface. Parameters: Example: agent\\_to\\_cli.py Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### to\\_cli\\_sync Run the agent in a CLI chat interface with the non-async interface. Parameters: agent\\_to\\_cli\\_sync.py Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py ### WrapperAgent Bases: AbstractAgent[AgentDepsT, OutputDataT] Agent which wraps another agent. Does nothing on its own, used as a base class. Source code in pydantic_ai_slim/pydantic_ai/agent/wrapper.py #### iter async A contextmanager which can be used to iterate over the agent graph's nodes as they are executed. This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an AgentRun object. The AgentRun can be used to async-iterate over the nodes of the graph as they are executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the stream of events coming from the execution of tools. The AgentRun also provides methods to access the full message history, new messages, and usage statistics, and the final result of the run once it has completed. For more details, see the documentation of AgentRun . Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/agent/wrapper.py #### override Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions. This is particularly useful when testing. You can find an example of this [here](../../testing/index.html#overriding-model-via-pytest-fixtures). Parameters: Source code in pydantic_ai_slim/pydantic_ai/agent/wrapper.py ### AgentRun dataclass Bases: Generic[AgentDepsT, OutputDataT] A stateful, async-iterable run of an [ Agent ](index.html#pydantic_ai.agent.Agent). You generally obtain an AgentRun instance by calling async with my_agent.iter(...) as agent_run: . Once you have an instance, you can use it to iterate through the run's nodes as they execute. When an [ End ](../pydantic_graph/nodes/index.html#pydantic_graph.nodes.End) is reached, the run finishes and [ result ](index.html#pydantic_ai.agent.AgentRun.result) becomes available. Example: You can also manually drive the iteration using the [ next ](index.html#pydantic_ai.agent.AgentRun.next) method for more granular control. Source code in pydantic_ai_slim/pydantic_ai/run.py #### ctx property The current context of the agent run. #### next\\_node property The next node that will be run in the agent graph. This is the next node that will be used during async iteration, or if a node is not passed to self.next(...) . #### result property The final result of the run if it has ended, otherwise None . Once the run returns an [ End ](../pydantic_graph/nodes/index.html#pydantic_graph.nodes.End) node, result is populated with an [ AgentRunResult ](index.html#pydantic_ai.agent.AgentRunResult). #### \\_\\_aiter\\_\\_ Provide async-iteration over the nodes in the agent run. Source code in pydantic_ai_slim/pydantic_ai/run.py #### \\_\\_anext\\_\\_ async Advance to the next node automatically based on the last returned node. Source code in pydantic_ai_slim/pydantic_ai/run.py #### next async Manually drive the agent run by passing in the node you want to run next. This lets you inspect or mutate the node before continuing execution, or skip certain nodes under dynamic conditions. The agent run should be stopped when you return an [ End ](../pydantic_graph/nodes/index.html#pydantic_graph.nodes.End) node. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/run.py #### usage Get usage statistics for the run so far, including token usage, model requests, and so on. Source code in pydantic_ai_slim/pydantic_ai/run.py ### AgentRunResult dataclass Bases: Generic[OutputDataT] The final result of an agent run. Source code in pydantic_ai_slim/pydantic_ai/run.py #### output instance-attribute The output data from the agent run. #### all\\_messages Return the history of \\_messages. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/run.py #### all\\_messages\\_json Return all messages from [ all_messages ](index.html#pydantic_ai.agent.AgentRunResult.all_messages) as JSON bytes. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/run.py #### new\\_messages Return new messages associated with this run. Messages from older runs are excluded. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/run.py #### new\\_messages\\_json Return new messages from [ new_messages ](index.html#pydantic_ai.agent.AgentRunResult.new_messages) as JSON bytes. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/run.py #### response property Return the last response from the message history. #### usage Return the usage of the whole run. Source code in pydantic_ai_slim/pydantic_ai/run.py #### timestamp Return the timestamp of last response. Source code in pydantic_ai_slim/pydantic_ai/run.py ### EndStrategy module-attribute The strategy for handling multiple tool calls when a final result is found. * 'early' : Stop processing other tool calls once a final result is found * 'exhaustive' : Process all tool calls even after finding a final result ### RunOutputDataT module-attribute Type variable for the result data of a run where output_type was customized on the run call. ### capture\\_run\\_messages Context manager to access the messages used in a [ run ](index.html#pydantic_ai.agent.AbstractAgent.run), [ run_sync ](index.html#pydantic_ai.agent.AbstractAgent.run_sync), or [ run_stream ](index.html#pydantic_ai.agent.AbstractAgent.run_stream) call. Useful when a run may raise an exception, see [model errors](../../agents/index.html#model-errors) for more information. Examples: Note If you call run , run_sync , or run_stream more than once within a single capture_run_messages context, messages will represent the messages exchanged during the first call only. Source code in pydantic_ai_slim/pydantic_ai/_agent_graph.py ### InstrumentationSettings dataclass Options for instrumenting models and agents with OpenTelemetry. Used in: * Agent(instrument=...) * [ Agent.instrument_all() ](index.html#pydantic_ai.agent.Agent.instrument_all) * [ InstrumentedModel ](../models/instrumented/index.html#pydantic_ai.models.instrumented.InstrumentedModel) See the [Debugging and Monitoring guide](../../logfire/index.html) for more info. Source code in pydantic_ai_slim/pydantic_ai/models/instrumented.py #### \\_\\_init\\_\\_ Create instrumentation options. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/instrumented.py #### messages\\_to\\_otel\\_events Convert a list of model messages to OpenTelemetry events. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/models/instrumented.py ### EventStreamHandler module-attribute A function that receives agent [ RunContext ](../tools/index.html#pydantic_ai.tools.RunContext) and an async iterable of events from the model's streaming response and the agent's execution of tools.", "url": "https://ai.pydantic.dev/api/agent/index.html#pydantic_aiagent", "page": "api/agent/index.html", "source_site": "pydantic_ai"}
{"title": "Agent dataclass", "anchor": "pydantic_ai.agent.Agent", "heading_level": 3, "md_text": "Bases: AbstractAgent[AgentDepsT, OutputDataT] Class for defining \"agents\" - a way to have a specific type of \"conversation\" with an LLM. Agents are generic in the dependency type they take [ AgentDepsT ](../tools/index.html#pydantic_ai.tools.AgentDepsT) and the output type they return, [ OutputDataT ](../output/index.html#pydantic_ai.output.OutputDataT). By default, if neither generic parameter is customised, agents have type Agent[None, str] . Minimal usage example: Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### \\_\\_init\\_\\_ Create an agent. Parameters: Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### end\\_strategy instance-attribute Strategy for handling tool calls when a final result is found. #### model\\_settings instance-attribute Optional model request settings to use for this agents's runs, by default. Note, if model_settings is provided by run , run_sync , or run_stream , those settings will be merged with this value, with the runtime argument taking priority. #### instrument instance-attribute Options to automatically instrument with OpenTelemetry. #### instrument\\_all staticmethod Set the instrumentation options for all agents where instrument is not set. Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### model property writable The default model configured for this agent. #### name property writable The name of the agent, used for logging. If None , we try to infer the agent name from the call frame when the agent is first run. #### deps\\_type property The type of dependencies used by the agent. #### output\\_type property The type of data output by agent runs, used to validate the data returned by the model, defaults to str . #### event\\_stream\\_handler property Optional handler for events from the model's streaming response and the agent's execution of tools. #### iter async A contextmanager which can be used to iterate over the agent graph's nodes as they are executed. This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an AgentRun object. The AgentRun can be used to async-iterate over the nodes of the graph as they are executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the stream of events coming from the execution of tools. The AgentRun also provides methods to access the full message history, new messages, and usage statistics, and the final result of the run once it has completed. For more details, see the documentation of AgentRun . Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### override Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions. This is particularly useful when testing. You can find an example of this [here](../../testing/index.html#overriding-model-via-pytest-fixtures). Parameters: Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### instructions Decorator to register an instructions function. Optionally takes [ RunContext ](../tools/index.html#pydantic_ai.tools.RunContext) as its only argument. Can decorate a sync or async functions. The decorator can be used bare ( agent.instructions ). Overloads for every possible signature of instructions are included so the decorator doesn't obscure the type of the function. Example: Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### system\\_prompt Decorator to register a system prompt function. Optionally takes [ RunContext ](../tools/index.html#pydantic_ai.tools.RunContext) as its only argument. Can decorate a sync or async functions. The decorator can be used either bare ( agent.system_prompt ) or as a function call ( agent.system_prompt(...) ), see the examples below. Overloads for every possible signature of system_prompt are included so the decorator doesn't obscure the type of the function, see tests/typed_agent.py for tests. Parameters: Example: Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### output\\_validator Decorator to register an output validator function. Optionally takes [ RunContext ](../tools/index.html#pydantic_ai.tools.RunContext) as its first argument. Can decorate a sync or async functions. Overloads for every possible signature of output_validator are included so the decorator doesn't obscure the type of the function, see tests/typed_agent.py for tests. Example: Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### tool Decorator to register a tool function which takes [ RunContext ](../tools/index.html#pydantic_ai.tools.RunContext) as its first argument. Can decorate a sync or async functions. The docstring is inspected to extract both the tool description and description of each parameter, [learn more](../../tools/index.html#function-tools-and-schema). We can't add overloads for every possible signature of tool, since the return type is a recursive union so the signature of functions decorated with @agent.tool is obscured. Example: Parameters: Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### tool\\_plain Decorator to register a tool function which DOES NOT take RunContext as an argument. Can decorate a sync or async functions. The docstring is inspected to extract both the tool description and description of each parameter, [learn more](../../tools/index.html#function-tools-and-schema). We can't add overloads for every possible signature of tool, since the return type is a recursive union so the signature of functions decorated with @agent.tool is obscured. Example: Parameters: Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### toolset Decorator to register a toolset function which takes [ RunContext ](../tools/index.html#pydantic_ai.tools.RunContext) as its only argument. Can decorate a sync or async functions. The decorator can be used bare ( agent.toolset ). Example: Parameters: Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### toolsets property All toolsets registered on the agent, including a function toolset holding tools that were registered on the agent directly. Output tools are not included. #### \\_\\_aenter\\_\\_ async Enter the agent context. This will start all [ MCPServerStdio s](../mcp/index.html#pydantic_ai.mcp.MCPServerStdio) registered as toolsets so they are ready to be used. This is a no-op if the agent has already been entered. Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### set\\_mcp\\_sampling\\_model Set the sampling model on all MCP servers registered with the agent. If no sampling model is provided, the agent's model will be used. Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py #### run\\_mcp\\_servers async deprecated Deprecated run_mcp_servers is deprecated, use async with agent: instead. If you need to set a sampling model on all MCP servers, use agent.set_mcp_sampling_model() . Run [ MCPServerStdio s](../mcp/index.html#pydantic_ai.mcp.MCPServerStdio) so they can be used by the agent. Deprecated: use [ async with agent ](index.html#pydantic_ai.agent.Agent.__aenter__) instead. If you need to set a sampling model on all MCP servers, use [ agent.set_mcp_sampling_model() ](index.html#pydantic_ai.agent.Agent.set_mcp_sampling_model). Returns: a context manager to start and shutdown the servers. Source code in pydantic_ai_slim/pydantic_ai/agent/__init__.py", "url": "https://ai.pydantic.dev/api/agent/index.html#pydantic_ai.agent.Agent", "page": "api/agent/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractAgent", "anchor": "pydantic_ai.agent.AbstractAgent", "heading_level": 3, "md_text": "Bases: Generic[AgentDepsT, OutputDataT] , ABC Abstract superclass for [ Agent ](index.html#pydantic_ai.agent.Agent), [ WrapperAgent ](index.html#pydantic_ai.agent.WrapperAgent), and your own custom agent implementations. Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### model abstractmethod property The default model configured for this agent. #### name abstractmethod property writable The name of the agent, used for logging. If None , we try to infer the agent name from the call frame when the agent is first run. #### deps\\_type abstractmethod property The type of dependencies used by the agent. #### output\\_type abstractmethod property The type of data output by agent runs, used to validate the data returned by the model, defaults to str . #### event\\_stream\\_handler abstractmethod property Optional handler for events from the model's streaming response and the agent's execution of tools. #### toolsets abstractmethod property All toolsets registered on the agent. Output tools are not included. #### run async Run the agent with a user prompt in async mode. This method builds an internal agent graph (using system prompts, tools and output schemas) and then runs the graph to completion. The result of the run is returned. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### run\\_sync Synchronously run the agent with a user prompt. This is a convenience method that wraps [ self.run ](index.html#pydantic_ai.agent.AbstractAgent.run) with loop.run_until_complete(...) . You therefore can't use this method inside async code or if there's an active event loop. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### run\\_stream async Run the agent with a user prompt in async streaming mode. This method builds an internal agent graph (using system prompts, tools and output schemas) and then runs the graph until the model produces output matching the output_type , for example text or structured data. At this point, a streaming run result object is yielded from which you can stream the output as it comes in, and -- once this output has completed streaming -- get the complete output, message history, and usage. As this method will consider the first output matching the output_type to be the final output, it will stop running the agent graph and will not execute any tool calls made by the model after this \"final\" output. If you want to always run the agent graph to completion and stream events and output at the same time, use [ agent.run() ](index.html#pydantic_ai.agent.AbstractAgent.run) with an event_stream_handler or [ agent.iter() ](index.html#pydantic_ai.agent.AbstractAgent.iter) instead. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### run\\_stream\\_events Run the agent with a user prompt in async mode and stream events from the run. This is a convenience method that wraps [ self.run ](index.html#pydantic_ai.agent.AbstractAgent.run) and uses the event_stream_handler kwarg to get a stream of events from the run. Example: Arguments are the same as for [ self.run ](index.html#pydantic_ai.agent.AbstractAgent.run), except that event_stream_handler is now allowed. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### iter abstractmethod async A contextmanager which can be used to iterate over the agent graph's nodes as they are executed. This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an AgentRun object. The AgentRun can be used to async-iterate over the nodes of the graph as they are executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the stream of events coming from the execution of tools. The AgentRun also provides methods to access the full message history, new messages, and usage statistics, and the final result of the run once it has completed. For more details, see the documentation of AgentRun . Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### override abstractmethod Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions. This is particularly useful when testing. You can find an example of this [here](../../testing/index.html#overriding-model-via-pytest-fixtures). Parameters: Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### sequential\\_tool\\_calls staticmethod Run tool calls sequentially during the context. Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### is\\_model\\_request\\_node staticmethod Check if the node is a ModelRequestNode , narrowing the type if it is. This method preserves the generic parameters while narrowing the type, unlike a direct call to isinstance . Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### is\\_call\\_tools\\_node staticmethod Check if the node is a CallToolsNode , narrowing the type if it is. This method preserves the generic parameters while narrowing the type, unlike a direct call to isinstance . Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### is\\_user\\_prompt\\_node staticmethod Check if the node is a UserPromptNode , narrowing the type if it is. This method preserves the generic parameters while narrowing the type, unlike a direct call to isinstance . Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### is\\_end\\_node staticmethod Check if the node is a End , narrowing the type if it is. This method preserves the generic parameters while narrowing the type, unlike a direct call to isinstance . Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### to\\_ag\\_ui Returns an ASGI application that handles every AG-UI request by running the agent. Note that the deps will be the same for each request, with the exception of the AG-UI state that's injected into the state field of a deps object that implements the [ StateHandler ](../ag_ui/index.html#pydantic_ai.ag_ui.StateHandler) protocol. To provide different deps for each request (e.g. based on the authenticated user), use [ pydantic_ai.ag_ui.run_ag_ui ](../ag_ui/index.html#pydantic_ai.ag_ui.run_ag_ui) or [ pydantic_ai.ag_ui.handle_ag_ui_request ](../ag_ui/index.html#pydantic_ai.ag_ui.handle_ag_ui_request) instead. Example: The app is an ASGI application that can be used with any ASGI server. To run the application, you can use the following command: See [AG-UI docs](../../ag-ui/index.html) for more information. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### to\\_a2a Convert the agent to a FastA2A application. Example: The app is an ASGI application that can be used with any ASGI server. To run the application, you can use the following command: Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### to\\_cli async Run the agent in a CLI chat interface. Parameters: Example: agent\\_to\\_cli.py Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py #### to\\_cli\\_sync Run the agent in a CLI chat interface with the non-async interface. Parameters: agent\\_to\\_cli\\_sync.py Source code in pydantic_ai_slim/pydantic_ai/agent/abstract.py", "url": "https://ai.pydantic.dev/api/agent/index.html#pydantic_ai.agent.AbstractAgent", "page": "api/agent/index.html", "source_site": "pydantic_ai"}
{"title": "WrapperAgent", "anchor": "pydantic_ai.agent.WrapperAgent", "heading_level": 3, "md_text": "Bases: AbstractAgent[AgentDepsT, OutputDataT] Agent which wraps another agent. Does nothing on its own, used as a base class. Source code in pydantic_ai_slim/pydantic_ai/agent/wrapper.py #### iter async A contextmanager which can be used to iterate over the agent graph's nodes as they are executed. This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an AgentRun object. The AgentRun can be used to async-iterate over the nodes of the graph as they are executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the stream of events coming from the execution of tools. The AgentRun also provides methods to access the full message history, new messages, and usage statistics, and the final result of the run once it has completed. For more details, see the documentation of AgentRun . Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/agent/wrapper.py #### override Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions. This is particularly useful when testing. You can find an example of this [here](../../testing/index.html#overriding-model-via-pytest-fixtures). Parameters: Source code in pydantic_ai_slim/pydantic_ai/agent/wrapper.py", "url": "https://ai.pydantic.dev/api/agent/index.html#pydantic_ai.agent.WrapperAgent", "page": "api/agent/index.html", "source_site": "pydantic_ai"}
{"title": "AgentRun dataclass", "anchor": "pydantic_ai.agent.AgentRun", "heading_level": 3, "md_text": "Bases: Generic[AgentDepsT, OutputDataT] A stateful, async-iterable run of an [ Agent ](index.html#pydantic_ai.agent.Agent). You generally obtain an AgentRun instance by calling async with my_agent.iter(...) as agent_run: . Once you have an instance, you can use it to iterate through the run's nodes as they execute. When an [ End ](../pydantic_graph/nodes/index.html#pydantic_graph.nodes.End) is reached, the run finishes and [ result ](index.html#pydantic_ai.agent.AgentRun.result) becomes available. Example: You can also manually drive the iteration using the [ next ](index.html#pydantic_ai.agent.AgentRun.next) method for more granular control. Source code in pydantic_ai_slim/pydantic_ai/run.py #### ctx property The current context of the agent run. #### next\\_node property The next node that will be run in the agent graph. This is the next node that will be used during async iteration, or if a node is not passed to self.next(...) . #### result property The final result of the run if it has ended, otherwise None . Once the run returns an [ End ](../pydantic_graph/nodes/index.html#pydantic_graph.nodes.End) node, result is populated with an [ AgentRunResult ](index.html#pydantic_ai.agent.AgentRunResult). #### \\_\\_aiter\\_\\_ Provide async-iteration over the nodes in the agent run. Source code in pydantic_ai_slim/pydantic_ai/run.py #### \\_\\_anext\\_\\_ async Advance to the next node automatically based on the last returned node. Source code in pydantic_ai_slim/pydantic_ai/run.py #### next async Manually drive the agent run by passing in the node you want to run next. This lets you inspect or mutate the node before continuing execution, or skip certain nodes under dynamic conditions. The agent run should be stopped when you return an [ End ](../pydantic_graph/nodes/index.html#pydantic_graph.nodes.End) node. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/run.py #### usage Get usage statistics for the run so far, including token usage, model requests, and so on. Source code in pydantic_ai_slim/pydantic_ai/run.py", "url": "https://ai.pydantic.dev/api/agent/index.html#pydantic_ai.agent.AgentRun", "page": "api/agent/index.html", "source_site": "pydantic_ai"}
{"title": "AgentRunResult dataclass", "anchor": "pydantic_ai.agent.AgentRunResult", "heading_level": 3, "md_text": "Bases: Generic[OutputDataT] The final result of an agent run. Source code in pydantic_ai_slim/pydantic_ai/run.py #### output instance-attribute The output data from the agent run. #### all\\_messages Return the history of \\_messages. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/run.py #### all\\_messages\\_json Return all messages from [ all_messages ](index.html#pydantic_ai.agent.AgentRunResult.all_messages) as JSON bytes. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/run.py #### new\\_messages Return new messages associated with this run. Messages from older runs are excluded. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/run.py #### new\\_messages\\_json Return new messages from [ new_messages ](index.html#pydantic_ai.agent.AgentRunResult.new_messages) as JSON bytes. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/run.py #### response property Return the last response from the message history. #### usage Return the usage of the whole run. Source code in pydantic_ai_slim/pydantic_ai/run.py #### timestamp Return the timestamp of last response. Source code in pydantic_ai_slim/pydantic_ai/run.py", "url": "https://ai.pydantic.dev/api/agent/index.html#pydantic_ai.agent.AgentRunResult", "page": "api/agent/index.html", "source_site": "pydantic_ai"}
{"title": "EndStrategy module-attribute", "anchor": "pydantic_ai.agent.EndStrategy", "heading_level": 3, "md_text": "The strategy for handling multiple tool calls when a final result is found. * 'early' : Stop processing other tool calls once a final result is found * 'exhaustive' : Process all tool calls even after finding a final result", "url": "https://ai.pydantic.dev/api/agent/index.html#pydantic_ai.agent.EndStrategy", "page": "api/agent/index.html", "source_site": "pydantic_ai"}
{"title": "RunOutputDataT module-attribute", "anchor": "pydantic_ai.agent.RunOutputDataT", "heading_level": 3, "md_text": "Type variable for the result data of a run where output_type was customized on the run call.", "url": "https://ai.pydantic.dev/api/agent/index.html#pydantic_ai.agent.RunOutputDataT", "page": "api/agent/index.html", "source_site": "pydantic_ai"}
{"title": "capture_run_messages", "anchor": "pydantic_ai.agent.capture_run_messages", "heading_level": 3, "md_text": "Context manager to access the messages used in a [ run ](index.html#pydantic_ai.agent.AbstractAgent.run), [ run_sync ](index.html#pydantic_ai.agent.AbstractAgent.run_sync), or [ run_stream ](index.html#pydantic_ai.agent.AbstractAgent.run_stream) call. Useful when a run may raise an exception, see [model errors](../../agents/index.html#model-errors) for more information. Examples: Note If you call run , run_sync , or run_stream more than once within a single capture_run_messages context, messages will represent the messages exchanged during the first call only. Source code in pydantic_ai_slim/pydantic_ai/_agent_graph.py", "url": "https://ai.pydantic.dev/api/agent/index.html#pydantic_ai.agent.capture_run_messages", "page": "api/agent/index.html", "source_site": "pydantic_ai"}
{"title": "InstrumentationSettings dataclass", "anchor": "pydantic_ai.agent.InstrumentationSettings", "heading_level": 3, "md_text": "Options for instrumenting models and agents with OpenTelemetry. Used in: * Agent(instrument=...) * [ Agent.instrument_all() ](index.html#pydantic_ai.agent.Agent.instrument_all) * [ InstrumentedModel ](../models/instrumented/index.html#pydantic_ai.models.instrumented.InstrumentedModel) See the [Debugging and Monitoring guide](../../logfire/index.html) for more info. Source code in pydantic_ai_slim/pydantic_ai/models/instrumented.py #### \\_\\_init\\_\\_ Create instrumentation options. Parameters: Source code in pydantic_ai_slim/pydantic_ai/models/instrumented.py #### messages\\_to\\_otel\\_events Convert a list of model messages to OpenTelemetry events. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/models/instrumented.py", "url": "https://ai.pydantic.dev/api/agent/index.html#pydantic_ai.agent.InstrumentationSettings", "page": "api/agent/index.html", "source_site": "pydantic_ai"}
{"title": "EventStreamHandler module-attribute", "anchor": "pydantic_ai.agent.EventStreamHandler", "heading_level": 3, "md_text": "A function that receives agent [ RunContext ](../tools/index.html#pydantic_ai.tools.RunContext) and an async iterable of events from the model's streaming response and the agent's execution of tools.", "url": "https://ai.pydantic.dev/api/agent/index.html#pydantic_ai.agent.EventStreamHandler", "page": "api/agent/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.durable_exec", "anchor": "pydantic_aidurable_exec", "heading_level": 1, "md_text": "### TemporalAgent Bases: WrapperAgent[AgentDepsT, OutputDataT] Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py #### \\_\\_init\\_\\_ Wrap an agent to enable it to be used inside a Temporal workflow, by automatically offloading model requests, tool calls, and MCP server communication to Temporal activities. After wrapping, the original agent can still be used as normal outside of the Temporal workflow, but any changes to its model or toolsets after wrapping will not be reflected in the durable agent. Parameters: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py #### run async Run the agent with a user prompt in async mode. This method builds an internal agent graph (using system prompts, tools and result schemas) and then runs the graph to completion. The result of the run is returned. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py #### run\\_sync Synchronously run the agent with a user prompt. This is a convenience method that wraps [ self.run ](../agent/index.html#pydantic_ai.agent.AbstractAgent.run) with loop.run_until_complete(...) . You therefore can't use this method inside async code or if there's an active event loop. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py #### run\\_stream async Run the agent with a user prompt in async mode, returning a streamed response. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py #### run\\_stream\\_events Run the agent with a user prompt in async mode and stream events from the run. This is a convenience method that wraps [ self.run ](../agent/index.html#pydantic_ai.agent.AbstractAgent.run) and uses the event_stream_handler kwarg to get a stream of events from the run. Example: Arguments are the same as for [ self.run ](../agent/index.html#pydantic_ai.agent.AbstractAgent.run), except that event_stream_handler is now allowed. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py #### iter async A contextmanager which can be used to iterate over the agent graph's nodes as they are executed. This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an AgentRun object. The AgentRun can be used to async-iterate over the nodes of the graph as they are executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the stream of events coming from the execution of tools. The AgentRun also provides methods to access the full message history, new messages, and usage statistics, and the final result of the run once it has completed. For more details, see the documentation of AgentRun . Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py #### override Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions. This is particularly useful when testing. You can find an example of this [here](../../testing/index.html#overriding-model-via-pytest-fixtures). Parameters: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py ### LogfirePlugin Bases: Plugin Temporal client plugin for Logfire. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_logfire.py ### TemporalRunContext Bases: RunContext[AgentDepsT] The [ RunContext ](../tools/index.html#pydantic_ai.tools.RunContext) subclass to use to serialize and deserialize the run context for use inside a Temporal activity. By default, only the deps , retries , tool_call_id , tool_name , tool_call_approved , retry , max_retries and run_step attributes will be available. To make another attribute available, create a TemporalRunContext subclass with a custom serialize_run_context class method that returns a dictionary that includes the attribute and pass it to [ TemporalAgent ](index.html#pydantic_ai.durable_exec.temporal.TemporalAgent). Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_run_context.py #### serialize\\_run\\_context classmethod Serialize the run context to a dict[str, Any] . Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_run_context.py #### deserialize\\_run\\_context classmethod Deserialize the run context from a dict[str, Any] . Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_run_context.py ### PydanticAIPlugin Bases: Plugin , Plugin Temporal client and worker plugin for Pydantic AI. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/__init__.py ### AgentPlugin Bases: Plugin Temporal worker plugin for a specific Pydantic AI agent. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/__init__.py ### DBOSAgent Bases: WrapperAgent[AgentDepsT, OutputDataT] , DBOSConfiguredInstance Source code in pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py #### \\_\\_init\\_\\_ Wrap an agent to enable it with DBOS durable workflows, by automatically offloading model requests, tool calls, and MCP server communication to DBOS steps. After wrapping, the original agent can still be used as normal outside of the DBOS workflow. Parameters: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py #### run async Run the agent with a user prompt in async mode. This method builds an internal agent graph (using system prompts, tools and result schemas) and then runs the graph to completion. The result of the run is returned. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py #### run\\_sync Synchronously run the agent with a user prompt. This is a convenience method that wraps [ self.run ](../agent/index.html#pydantic_ai.agent.AbstractAgent.run) with loop.run_until_complete(...) . You therefore can't use this method inside async code or if there's an active event loop. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py #### run\\_stream async Run the agent with a user prompt in async mode, returning a streamed response. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py #### run\\_stream\\_events Run the agent with a user prompt in async mode and stream events from the run. This is a convenience method that wraps [ self.run ](../agent/index.html#pydantic_ai.agent.AbstractAgent.run) and uses the event_stream_handler kwarg to get a stream of events from the run. Example: Arguments are the same as for [ self.run ](../agent/index.html#pydantic_ai.agent.AbstractAgent.run), except that event_stream_handler is now allowed. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py #### iter async A contextmanager which can be used to iterate over the agent graph's nodes as they are executed. This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an AgentRun object. The AgentRun can be used to async-iterate over the nodes of the graph as they are executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the stream of events coming from the execution of tools. The AgentRun also provides methods to access the full message history, new messages, and usage statistics, and the final result of the run once it has completed. For more details, see the documentation of AgentRun . Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py #### override Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions. This is particularly useful when testing. You can find an example of this [here](../../testing/index.html#overriding-model-via-pytest-fixtures). Parameters: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py ### DBOSMCPServer Bases: WrapperToolset[AgentDepsT] , ABC A wrapper for MCPServer that integrates with DBOS, turning call\\_tool and get\\_tools to DBOS steps. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_mcp_server.py ### DBOSModel Bases: WrapperModel A wrapper for Model that integrates with DBOS, turning request and request\\_stream to DBOS steps. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_model.py ### StepConfig Bases: TypedDict Configuration for a step in the DBOS workflow. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_utils.py ### PrefectAgent Bases: WrapperAgent[AgentDepsT, OutputDataT] Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py #### \\_\\_init\\_\\_ Wrap an agent to enable it with Prefect durable flows, by automatically offloading model requests, tool calls, and MCP server communication to Prefect tasks. After wrapping, the original agent can still be used as normal outside of the Prefect flow. Parameters: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py #### run async Run the agent with a user prompt in async mode. This method builds an internal agent graph (using system prompts, tools and result schemas) and then runs the graph to completion. The result of the run is returned. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py #### run\\_sync Synchronously run the agent with a user prompt. This is a convenience method that wraps [ self.run ](../agent/index.html#pydantic_ai.agent.AbstractAgent.run) with loop.run_until_complete(...) . You therefore can't use this method inside async code or if there's an active event loop. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py #### run\\_stream async Run the agent with a user prompt in async mode, returning a streamed response. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py #### run\\_stream\\_events Run the agent with a user prompt in async mode and stream events from the run. This is a convenience method that wraps [ self.run ](../agent/index.html#pydantic_ai.agent.AbstractAgent.run) and uses the event_stream_handler kwarg to get a stream of events from the run. Example: Arguments are the same as for [ self.run ](../agent/index.html#pydantic_ai.agent.AbstractAgent.run), except that event_stream_handler is now allowed. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py #### iter async A contextmanager which can be used to iterate over the agent graph's nodes as they are executed. This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an AgentRun object. The AgentRun can be used to async-iterate over the nodes of the graph as they are executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the stream of events coming from the execution of tools. The AgentRun also provides methods to access the full message history, new messages, and usage statistics, and the final result of the run once it has completed. For more details, see the documentation of AgentRun . Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py #### override Context manager to temporarily override agent dependencies, model, toolsets, tools, or instructions. This is particularly useful when testing. You can find an example of this [here](../../testing/index.html#overriding-model-via-pytest-fixtures). Parameters: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py ### PrefectFunctionToolset Bases: PrefectWrapperToolset[AgentDepsT] A wrapper for FunctionToolset that integrates with Prefect, turning tool calls into Prefect tasks. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_function_toolset.py #### call\\_tool async Call a tool, wrapped as a Prefect task with a descriptive name. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_function_toolset.py ### PrefectMCPServer Bases: PrefectWrapperToolset[AgentDepsT] , ABC A wrapper for MCPServer that integrates with Prefect, turning call\\_tool and get\\_tools into Prefect tasks. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_mcp_server.py #### call\\_tool async Call an MCP tool, wrapped as a Prefect task with a descriptive name. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_mcp_server.py ### PrefectModel Bases: WrapperModel A wrapper for Model that integrates with Prefect, turning request and request\\_stream into Prefect tasks. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model.py #### request async Make a model request, wrapped as a Prefect task when in a flow. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model.py #### request\\_stream async Make a streaming model request. When inside a Prefect flow, the stream is consumed within a task and a non-streaming response is returned. When not in a flow, behaves normally. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model.py ### TaskConfig Bases: TypedDict Configuration for a task in Prefect. These options are passed to the @task decorator. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_types.py #### retries instance-attribute Maximum number of retries for the task. #### retry\\_delay\\_seconds instance-attribute Delay between retries in seconds. Can be a single value or a list for custom backoff. #### timeout\\_seconds instance-attribute Maximum time in seconds for the task to complete. #### cache\\_policy instance-attribute Prefect cache policy for the task. #### persist\\_result instance-attribute Whether to persist the task result. #### result\\_storage instance-attribute Prefect result storage for the task. Should be a storage block or a block slug like s3-bucket/my-storage . #### log\\_prints instance-attribute Whether to log print statements from the task.", "url": "https://ai.pydantic.dev/api/durable_exec/index.html#pydantic_aidurable_exec", "page": "api/durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "TemporalAgent", "anchor": "pydantic_ai.durable_exec.temporal.TemporalAgent", "heading_level": 3, "md_text": "Bases: WrapperAgent[AgentDepsT, OutputDataT] Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py #### \\_\\_init\\_\\_ Wrap an agent to enable it to be used inside a Temporal workflow, by automatically offloading model requests, tool calls, and MCP server communication to Temporal activities. After wrapping, the original agent can still be used as normal outside of the Temporal workflow, but any changes to its model or toolsets after wrapping will not be reflected in the durable agent. Parameters: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py #### run async Run the agent with a user prompt in async mode. This method builds an internal agent graph (using system prompts, tools and result schemas) and then runs the graph to completion. The result of the run is returned. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py #### run\\_sync Synchronously run the agent with a user prompt. This is a convenience method that wraps [ self.run ](../agent/index.html#pydantic_ai.agent.AbstractAgent.run) with loop.run_until_complete(...) . You therefore can't use this method inside async code or if there's an active event loop. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py #### run\\_stream async Run the agent with a user prompt in async mode, returning a streamed response. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py #### run\\_stream\\_events Run the agent with a user prompt in async mode and stream events from the run. This is a convenience method that wraps [ self.run ](../agent/index.html#pydantic_ai.agent.AbstractAgent.run) and uses the event_stream_handler kwarg to get a stream of events from the run. Example: Arguments are the same as for [ self.run ](../agent/index.html#pydantic_ai.agent.AbstractAgent.run), except that event_stream_handler is now allowed. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py #### iter async A contextmanager which can be used to iterate over the agent graph's nodes as they are executed. This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an AgentRun object. The AgentRun can be used to async-iterate over the nodes of the graph as they are executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the stream of events coming from the execution of tools. The AgentRun also provides methods to access the full message history, new messages, and usage statistics, and the final result of the run once it has completed. For more details, see the documentation of AgentRun . Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py #### override Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions. This is particularly useful when testing. You can find an example of this [here](../../testing/index.html#overriding-model-via-pytest-fixtures). Parameters: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_agent.py", "url": "https://ai.pydantic.dev/api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.TemporalAgent", "page": "api/durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "LogfirePlugin", "anchor": "pydantic_ai.durable_exec.temporal.LogfirePlugin", "heading_level": 3, "md_text": "Bases: Plugin Temporal client plugin for Logfire. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_logfire.py", "url": "https://ai.pydantic.dev/api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.LogfirePlugin", "page": "api/durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "TemporalRunContext", "anchor": "pydantic_ai.durable_exec.temporal.TemporalRunContext", "heading_level": 3, "md_text": "Bases: RunContext[AgentDepsT] The [ RunContext ](../tools/index.html#pydantic_ai.tools.RunContext) subclass to use to serialize and deserialize the run context for use inside a Temporal activity. By default, only the deps , retries , tool_call_id , tool_name , tool_call_approved , retry , max_retries and run_step attributes will be available. To make another attribute available, create a TemporalRunContext subclass with a custom serialize_run_context class method that returns a dictionary that includes the attribute and pass it to [ TemporalAgent ](index.html#pydantic_ai.durable_exec.temporal.TemporalAgent). Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_run_context.py #### serialize\\_run\\_context classmethod Serialize the run context to a dict[str, Any] . Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_run_context.py #### deserialize\\_run\\_context classmethod Deserialize the run context from a dict[str, Any] . Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/_run_context.py", "url": "https://ai.pydantic.dev/api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.TemporalRunContext", "page": "api/durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PydanticAIPlugin", "anchor": "pydantic_ai.durable_exec.temporal.PydanticAIPlugin", "heading_level": 3, "md_text": "Bases: Plugin , Plugin Temporal client and worker plugin for Pydantic AI. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/__init__.py", "url": "https://ai.pydantic.dev/api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.PydanticAIPlugin", "page": "api/durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "AgentPlugin", "anchor": "pydantic_ai.durable_exec.temporal.AgentPlugin", "heading_level": 3, "md_text": "Bases: Plugin Temporal worker plugin for a specific Pydantic AI agent. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/temporal/__init__.py", "url": "https://ai.pydantic.dev/api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.AgentPlugin", "page": "api/durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "DBOSAgent", "anchor": "pydantic_ai.durable_exec.dbos.DBOSAgent", "heading_level": 3, "md_text": "Bases: WrapperAgent[AgentDepsT, OutputDataT] , DBOSConfiguredInstance Source code in pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py #### \\_\\_init\\_\\_ Wrap an agent to enable it with DBOS durable workflows, by automatically offloading model requests, tool calls, and MCP server communication to DBOS steps. After wrapping, the original agent can still be used as normal outside of the DBOS workflow. Parameters: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py #### run async Run the agent with a user prompt in async mode. This method builds an internal agent graph (using system prompts, tools and result schemas) and then runs the graph to completion. The result of the run is returned. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py #### run\\_sync Synchronously run the agent with a user prompt. This is a convenience method that wraps [ self.run ](../agent/index.html#pydantic_ai.agent.AbstractAgent.run) with loop.run_until_complete(...) . You therefore can't use this method inside async code or if there's an active event loop. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py #### run\\_stream async Run the agent with a user prompt in async mode, returning a streamed response. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py #### run\\_stream\\_events Run the agent with a user prompt in async mode and stream events from the run. This is a convenience method that wraps [ self.run ](../agent/index.html#pydantic_ai.agent.AbstractAgent.run) and uses the event_stream_handler kwarg to get a stream of events from the run. Example: Arguments are the same as for [ self.run ](../agent/index.html#pydantic_ai.agent.AbstractAgent.run), except that event_stream_handler is now allowed. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py #### iter async A contextmanager which can be used to iterate over the agent graph's nodes as they are executed. This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an AgentRun object. The AgentRun can be used to async-iterate over the nodes of the graph as they are executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the stream of events coming from the execution of tools. The AgentRun also provides methods to access the full message history, new messages, and usage statistics, and the final result of the run once it has completed. For more details, see the documentation of AgentRun . Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py #### override Context manager to temporarily override agent name, dependencies, model, toolsets, tools, or instructions. This is particularly useful when testing. You can find an example of this [here](../../testing/index.html#overriding-model-via-pytest-fixtures). Parameters: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_agent.py", "url": "https://ai.pydantic.dev/api/durable_exec/index.html#pydantic_ai.durable_exec.dbos.DBOSAgent", "page": "api/durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "DBOSMCPServer", "anchor": "pydantic_ai.durable_exec.dbos.DBOSMCPServer", "heading_level": 3, "md_text": "Bases: WrapperToolset[AgentDepsT] , ABC A wrapper for MCPServer that integrates with DBOS, turning call\\_tool and get\\_tools to DBOS steps. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_mcp_server.py", "url": "https://ai.pydantic.dev/api/durable_exec/index.html#pydantic_ai.durable_exec.dbos.DBOSMCPServer", "page": "api/durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "DBOSModel", "anchor": "pydantic_ai.durable_exec.dbos.DBOSModel", "heading_level": 3, "md_text": "Bases: WrapperModel A wrapper for Model that integrates with DBOS, turning request and request\\_stream to DBOS steps. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_model.py", "url": "https://ai.pydantic.dev/api/durable_exec/index.html#pydantic_ai.durable_exec.dbos.DBOSModel", "page": "api/durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "StepConfig", "anchor": "pydantic_ai.durable_exec.dbos.StepConfig", "heading_level": 3, "md_text": "Bases: TypedDict Configuration for a step in the DBOS workflow. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/dbos/_utils.py", "url": "https://ai.pydantic.dev/api/durable_exec/index.html#pydantic_ai.durable_exec.dbos.StepConfig", "page": "api/durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PrefectAgent", "anchor": "pydantic_ai.durable_exec.prefect.PrefectAgent", "heading_level": 3, "md_text": "Bases: WrapperAgent[AgentDepsT, OutputDataT] Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py #### \\_\\_init\\_\\_ Wrap an agent to enable it with Prefect durable flows, by automatically offloading model requests, tool calls, and MCP server communication to Prefect tasks. After wrapping, the original agent can still be used as normal outside of the Prefect flow. Parameters: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py #### run async Run the agent with a user prompt in async mode. This method builds an internal agent graph (using system prompts, tools and result schemas) and then runs the graph to completion. The result of the run is returned. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py #### run\\_sync Synchronously run the agent with a user prompt. This is a convenience method that wraps [ self.run ](../agent/index.html#pydantic_ai.agent.AbstractAgent.run) with loop.run_until_complete(...) . You therefore can't use this method inside async code or if there's an active event loop. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py #### run\\_stream async Run the agent with a user prompt in async mode, returning a streamed response. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py #### run\\_stream\\_events Run the agent with a user prompt in async mode and stream events from the run. This is a convenience method that wraps [ self.run ](../agent/index.html#pydantic_ai.agent.AbstractAgent.run) and uses the event_stream_handler kwarg to get a stream of events from the run. Example: Arguments are the same as for [ self.run ](../agent/index.html#pydantic_ai.agent.AbstractAgent.run), except that event_stream_handler is now allowed. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py #### iter async A contextmanager which can be used to iterate over the agent graph's nodes as they are executed. This method builds an internal agent graph (using system prompts, tools and output schemas) and then returns an AgentRun object. The AgentRun can be used to async-iterate over the nodes of the graph as they are executed. This is the API to use if you want to consume the outputs coming from each LLM model response, or the stream of events coming from the execution of tools. The AgentRun also provides methods to access the full message history, new messages, and usage statistics, and the final result of the run once it has completed. For more details, see the documentation of AgentRun . Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py #### override Context manager to temporarily override agent dependencies, model, toolsets, tools, or instructions. This is particularly useful when testing. You can find an example of this [here](../../testing/index.html#overriding-model-via-pytest-fixtures). Parameters: Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_agent.py", "url": "https://ai.pydantic.dev/api/durable_exec/index.html#pydantic_ai.durable_exec.prefect.PrefectAgent", "page": "api/durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PrefectFunctionToolset", "anchor": "pydantic_ai.durable_exec.prefect.PrefectFunctionToolset", "heading_level": 3, "md_text": "Bases: PrefectWrapperToolset[AgentDepsT] A wrapper for FunctionToolset that integrates with Prefect, turning tool calls into Prefect tasks. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_function_toolset.py #### call\\_tool async Call a tool, wrapped as a Prefect task with a descriptive name. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_function_toolset.py", "url": "https://ai.pydantic.dev/api/durable_exec/index.html#pydantic_ai.durable_exec.prefect.PrefectFunctionToolset", "page": "api/durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PrefectMCPServer", "anchor": "pydantic_ai.durable_exec.prefect.PrefectMCPServer", "heading_level": 3, "md_text": "Bases: PrefectWrapperToolset[AgentDepsT] , ABC A wrapper for MCPServer that integrates with Prefect, turning call\\_tool and get\\_tools into Prefect tasks. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_mcp_server.py #### call\\_tool async Call an MCP tool, wrapped as a Prefect task with a descriptive name. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_mcp_server.py", "url": "https://ai.pydantic.dev/api/durable_exec/index.html#pydantic_ai.durable_exec.prefect.PrefectMCPServer", "page": "api/durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "PrefectModel", "anchor": "pydantic_ai.durable_exec.prefect.PrefectModel", "heading_level": 3, "md_text": "Bases: WrapperModel A wrapper for Model that integrates with Prefect, turning request and request\\_stream into Prefect tasks. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model.py #### request async Make a model request, wrapped as a Prefect task when in a flow. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model.py #### request\\_stream async Make a streaming model request. When inside a Prefect flow, the stream is consumed within a task and a non-streaming response is returned. When not in a flow, behaves normally. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_model.py", "url": "https://ai.pydantic.dev/api/durable_exec/index.html#pydantic_ai.durable_exec.prefect.PrefectModel", "page": "api/durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "TaskConfig", "anchor": "pydantic_ai.durable_exec.prefect.TaskConfig", "heading_level": 3, "md_text": "Bases: TypedDict Configuration for a task in Prefect. These options are passed to the @task decorator. Source code in pydantic_ai_slim/pydantic_ai/durable_exec/prefect/_types.py #### retries instance-attribute Maximum number of retries for the task. #### retry\\_delay\\_seconds instance-attribute Delay between retries in seconds. Can be a single value or a list for custom backoff. #### timeout\\_seconds instance-attribute Maximum time in seconds for the task to complete. #### cache\\_policy instance-attribute Prefect cache policy for the task. #### persist\\_result instance-attribute Whether to persist the task result. #### result\\_storage instance-attribute Prefect result storage for the task. Should be a storage block or a block slug like s3-bucket/my-storage . #### log\\_prints instance-attribute Whether to log print statements from the task.", "url": "https://ai.pydantic.dev/api/durable_exec/index.html#pydantic_ai.durable_exec.prefect.TaskConfig", "page": "api/durable_exec/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_graph.beta.node", "anchor": "pydantic_graphbetanode", "heading_level": 1, "md_text": "Core node types for graph construction and execution. This module defines the fundamental node types used to build execution graphs, including start/end nodes and fork nodes for parallel execution. ### StateT module-attribute Type variable for graph state. ### OutputT module-attribute Type variable for node output data. ### InputT module-attribute Type variable for node input data. ### StartNode Bases: Generic[OutputT] Entry point node for graph execution. The StartNode represents the beginning of a graph execution flow. Source code in pydantic_graph/pydantic_graph/beta/node.py #### id class-attribute instance-attribute Fixed identifier for the start node. ### EndNode Bases: Generic[InputT] Terminal node representing the completion of graph execution. The EndNode marks the successful completion of a graph execution flow and can collect the final output data. Source code in pydantic_graph/pydantic_graph/beta/node.py #### id class-attribute instance-attribute Fixed identifier for the end node. ### Fork dataclass Bases: Generic[InputT, OutputT] Fork node that creates parallel execution branches. A Fork node splits the execution flow into multiple parallel branches, enabling concurrent execution of downstream nodes. It can either map a sequence across multiple branches or duplicate data to each branch. Source code in pydantic_graph/pydantic_graph/beta/node.py #### id instance-attribute Unique identifier for this fork node. #### is\\_map instance-attribute Determines fork behavior. If True, InputT must be Sequence[OutputT] and each element is sent to a separate branch. If False, InputT must be OutputT and the same data is sent to all branches. #### downstream\\_join\\_id instance-attribute Optional identifier of a downstream join node that should be jumped to if mapping an empty iterable.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_node/index.html#pydantic_graphbetanode", "page": "api/pydantic_graph/beta_node/index.html", "source_site": "pydantic_ai"}
{"title": "StateT module-attribute", "anchor": "pydantic_graph.beta.node.StateT", "heading_level": 3, "md_text": "Type variable for graph state.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_node/index.html#pydantic_graph.beta.node.StateT", "page": "api/pydantic_graph/beta_node/index.html", "source_site": "pydantic_ai"}
{"title": "OutputT module-attribute", "anchor": "pydantic_graph.beta.node.OutputT", "heading_level": 3, "md_text": "Type variable for node output data.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_node/index.html#pydantic_graph.beta.node.OutputT", "page": "api/pydantic_graph/beta_node/index.html", "source_site": "pydantic_ai"}
{"title": "InputT module-attribute", "anchor": "pydantic_graph.beta.node.InputT", "heading_level": 3, "md_text": "Type variable for node input data.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_node/index.html#pydantic_graph.beta.node.InputT", "page": "api/pydantic_graph/beta_node/index.html", "source_site": "pydantic_ai"}
{"title": "StartNode", "anchor": "pydantic_graph.beta.node.StartNode", "heading_level": 3, "md_text": "Bases: Generic[OutputT] Entry point node for graph execution. The StartNode represents the beginning of a graph execution flow. Source code in pydantic_graph/pydantic_graph/beta/node.py #### id class-attribute instance-attribute Fixed identifier for the start node.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_node/index.html#pydantic_graph.beta.node.StartNode", "page": "api/pydantic_graph/beta_node/index.html", "source_site": "pydantic_ai"}
{"title": "EndNode", "anchor": "pydantic_graph.beta.node.EndNode", "heading_level": 3, "md_text": "Bases: Generic[InputT] Terminal node representing the completion of graph execution. The EndNode marks the successful completion of a graph execution flow and can collect the final output data. Source code in pydantic_graph/pydantic_graph/beta/node.py #### id class-attribute instance-attribute Fixed identifier for the end node.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_node/index.html#pydantic_graph.beta.node.EndNode", "page": "api/pydantic_graph/beta_node/index.html", "source_site": "pydantic_ai"}
{"title": "Fork dataclass", "anchor": "pydantic_graph.beta.node.Fork", "heading_level": 3, "md_text": "Bases: Generic[InputT, OutputT] Fork node that creates parallel execution branches. A Fork node splits the execution flow into multiple parallel branches, enabling concurrent execution of downstream nodes. It can either map a sequence across multiple branches or duplicate data to each branch. Source code in pydantic_graph/pydantic_graph/beta/node.py #### id instance-attribute Unique identifier for this fork node. #### is\\_map instance-attribute Determines fork behavior. If True, InputT must be Sequence[OutputT] and each element is sent to a separate branch. If False, InputT must be OutputT and the same data is sent to all branches. #### downstream\\_join\\_id instance-attribute Optional identifier of a downstream join node that should be jumped to if mapping an empty iterable.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_node/index.html#pydantic_graph.beta.node.Fork", "page": "api/pydantic_graph/beta_node/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_graph.beta.join", "anchor": "pydantic_graphbetajoin", "heading_level": 1, "md_text": "Join operations and reducers for graph execution. This module provides the core components for joining parallel execution paths in a graph, including various reducer types that aggregate data from multiple sources into a single output. ### JoinState dataclass The state of a join during graph execution associated to a particular fork run. Source code in pydantic_graph/pydantic_graph/beta/join.py ### ReducerContext dataclass Bases: Generic[StateT, DepsT] Context information passed to reducer functions during graph execution. The reducer context provides access to the current graph state and dependencies. Type Parameters StateT: The type of the graph state DepsT: The type of the dependencies Source code in pydantic_graph/pydantic_graph/beta/join.py #### state property The state of the graph run. #### deps property The deps for the graph run. #### cancel\\_sibling\\_tasks Cancel all sibling tasks created from the same fork. You can call this if you want your join to have early-stopping behavior. Source code in pydantic_graph/pydantic_graph/beta/join.py ### ReducerFunction module-attribute A function used for reducing inputs to a join node. ### reduce\\_null A reducer that discards all input data and returns None. Source code in pydantic_graph/pydantic_graph/beta/join.py ### reduce\\_list\\_append A reducer that appends to a list. Source code in pydantic_graph/pydantic_graph/beta/join.py ### reduce\\_list\\_extend A reducer that extends a list. Source code in pydantic_graph/pydantic_graph/beta/join.py ### reduce\\_dict\\_update A reducer that updates a dict. Source code in pydantic_graph/pydantic_graph/beta/join.py ### SupportsSum Bases: Protocol A protocol for a type that supports adding to itself. Source code in pydantic_graph/pydantic_graph/beta/join.py ### reduce\\_sum A reducer that sums numbers. Source code in pydantic_graph/pydantic_graph/beta/join.py ### ReduceFirstValue dataclass Bases: Generic[T] A reducer that returns the first value it encounters, and cancels all other tasks. Source code in pydantic_graph/pydantic_graph/beta/join.py #### \\_\\_call\\_\\_ The reducer function. Source code in pydantic_graph/pydantic_graph/beta/join.py ### Join dataclass Bases: Generic[StateT, DepsT, InputT, OutputT] A join operation that synchronizes and aggregates parallel execution paths. A join defines how to combine outputs from multiple parallel execution paths using a [ ReducerFunction ](index.html#pydantic_graph.beta.join.ReducerFunction). It specifies which fork it joins (if any) and manages the initialization of reducers. Type Parameters StateT: The type of the graph state DepsT: The type of the dependencies InputT: The type of input data to join OutputT: The type of the final joined output Source code in pydantic_graph/pydantic_graph/beta/join.py #### as\\_node Create a step node with bound inputs. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/join.py ### JoinNode dataclass Bases: BaseNode[StateT, DepsT, Any] A base node that represents a join item with bound inputs. JoinNode bridges between the v1 and v2 graph execution systems by wrapping a [ Join ](index.html#pydantic_graph.beta.join.Join) with bound inputs in a BaseNode interface. It is not meant to be run directly but rather used to indicate transitions to v2-style steps. Source code in pydantic_graph/pydantic_graph/beta/join.py #### join instance-attribute The step to execute. #### inputs instance-attribute The inputs bound to this step. #### run async Attempt to run the join node. Parameters: Returns: Raises: Source code in pydantic_graph/pydantic_graph/beta/join.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_join/index.html#pydantic_graphbetajoin", "page": "api/pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "JoinState dataclass", "anchor": "pydantic_graph.beta.join.JoinState", "heading_level": 3, "md_text": "The state of a join during graph execution associated to a particular fork run. Source code in pydantic_graph/pydantic_graph/beta/join.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.JoinState", "page": "api/pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "ReducerContext dataclass", "anchor": "pydantic_graph.beta.join.ReducerContext", "heading_level": 3, "md_text": "Bases: Generic[StateT, DepsT] Context information passed to reducer functions during graph execution. The reducer context provides access to the current graph state and dependencies. Type Parameters StateT: The type of the graph state DepsT: The type of the dependencies Source code in pydantic_graph/pydantic_graph/beta/join.py #### state property The state of the graph run. #### deps property The deps for the graph run. #### cancel\\_sibling\\_tasks Cancel all sibling tasks created from the same fork. You can call this if you want your join to have early-stopping behavior. Source code in pydantic_graph/pydantic_graph/beta/join.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.ReducerContext", "page": "api/pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "ReducerFunction module-attribute", "anchor": "pydantic_graph.beta.join.ReducerFunction", "heading_level": 3, "md_text": "A function used for reducing inputs to a join node.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.ReducerFunction", "page": "api/pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "reduce_null", "anchor": "pydantic_graph.beta.join.reduce_null", "heading_level": 3, "md_text": "A reducer that discards all input data and returns None. Source code in pydantic_graph/pydantic_graph/beta/join.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_null", "page": "api/pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "reduce_list_append", "anchor": "pydantic_graph.beta.join.reduce_list_append", "heading_level": 3, "md_text": "A reducer that appends to a list. Source code in pydantic_graph/pydantic_graph/beta/join.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_list_append", "page": "api/pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "reduce_list_extend", "anchor": "pydantic_graph.beta.join.reduce_list_extend", "heading_level": 3, "md_text": "A reducer that extends a list. Source code in pydantic_graph/pydantic_graph/beta/join.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_list_extend", "page": "api/pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "reduce_dict_update", "anchor": "pydantic_graph.beta.join.reduce_dict_update", "heading_level": 3, "md_text": "A reducer that updates a dict. Source code in pydantic_graph/pydantic_graph/beta/join.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_dict_update", "page": "api/pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "SupportsSum", "anchor": "pydantic_graph.beta.join.SupportsSum", "heading_level": 3, "md_text": "Bases: Protocol A protocol for a type that supports adding to itself. Source code in pydantic_graph/pydantic_graph/beta/join.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.SupportsSum", "page": "api/pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "reduce_sum", "anchor": "pydantic_graph.beta.join.reduce_sum", "heading_level": 3, "md_text": "A reducer that sums numbers. Source code in pydantic_graph/pydantic_graph/beta/join.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_sum", "page": "api/pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "ReduceFirstValue dataclass", "anchor": "pydantic_graph.beta.join.ReduceFirstValue", "heading_level": 3, "md_text": "Bases: Generic[T] A reducer that returns the first value it encounters, and cancels all other tasks. Source code in pydantic_graph/pydantic_graph/beta/join.py #### \\_\\_call\\_\\_ The reducer function. Source code in pydantic_graph/pydantic_graph/beta/join.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.ReduceFirstValue", "page": "api/pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "Join dataclass", "anchor": "pydantic_graph.beta.join.Join", "heading_level": 3, "md_text": "Bases: Generic[StateT, DepsT, InputT, OutputT] A join operation that synchronizes and aggregates parallel execution paths. A join defines how to combine outputs from multiple parallel execution paths using a [ ReducerFunction ](index.html#pydantic_graph.beta.join.ReducerFunction). It specifies which fork it joins (if any) and manages the initialization of reducers. Type Parameters StateT: The type of the graph state DepsT: The type of the dependencies InputT: The type of input data to join OutputT: The type of the final joined output Source code in pydantic_graph/pydantic_graph/beta/join.py #### as\\_node Create a step node with bound inputs. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/join.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.Join", "page": "api/pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "JoinNode dataclass", "anchor": "pydantic_graph.beta.join.JoinNode", "heading_level": 3, "md_text": "Bases: BaseNode[StateT, DepsT, Any] A base node that represents a join item with bound inputs. JoinNode bridges between the v1 and v2 graph execution systems by wrapping a [ Join ](index.html#pydantic_graph.beta.join.Join) with bound inputs in a BaseNode interface. It is not meant to be run directly but rather used to indicate transitions to v2-style steps. Source code in pydantic_graph/pydantic_graph/beta/join.py #### join instance-attribute The step to execute. #### inputs instance-attribute The inputs bound to this step. #### run async Attempt to run the join node. Parameters: Returns: Raises: Source code in pydantic_graph/pydantic_graph/beta/join.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.JoinNode", "page": "api/pydantic_graph/beta_join/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_graph.exceptions", "anchor": "pydantic_graphexceptions", "heading_level": 1, "md_text": "### GraphSetupError Bases: TypeError Error caused by an incorrectly configured graph. Source code in pydantic_graph/pydantic_graph/exceptions.py #### message instance-attribute Description of the mistake. ### GraphBuildingError Bases: ValueError An error raised during graph-building. Source code in pydantic_graph/pydantic_graph/exceptions.py #### message instance-attribute The error message. ### GraphValidationError Bases: ValueError An error raised during graph validation. Source code in pydantic_graph/pydantic_graph/exceptions.py #### message instance-attribute The error message. ### GraphRuntimeError Bases: RuntimeError Error caused by an issue during graph execution. Source code in pydantic_graph/pydantic_graph/exceptions.py #### message instance-attribute The error message. ### GraphNodeStatusError Bases: GraphRuntimeError Error caused by trying to run a node that already has status 'running' , 'success' , or 'error' . Source code in pydantic_graph/pydantic_graph/exceptions.py #### check classmethod Check if the status is valid. Source code in pydantic_graph/pydantic_graph/exceptions.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/exceptions/index.html#pydantic_graphexceptions", "page": "api/pydantic_graph/exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "GraphSetupError", "anchor": "pydantic_graph.exceptions.GraphSetupError", "heading_level": 3, "md_text": "Bases: TypeError Error caused by an incorrectly configured graph. Source code in pydantic_graph/pydantic_graph/exceptions.py #### message instance-attribute Description of the mistake.", "url": "https://ai.pydantic.dev/api/pydantic_graph/exceptions/index.html#pydantic_graph.exceptions.GraphSetupError", "page": "api/pydantic_graph/exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "GraphBuildingError", "anchor": "pydantic_graph.exceptions.GraphBuildingError", "heading_level": 3, "md_text": "Bases: ValueError An error raised during graph-building. Source code in pydantic_graph/pydantic_graph/exceptions.py #### message instance-attribute The error message.", "url": "https://ai.pydantic.dev/api/pydantic_graph/exceptions/index.html#pydantic_graph.exceptions.GraphBuildingError", "page": "api/pydantic_graph/exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "GraphValidationError", "anchor": "pydantic_graph.exceptions.GraphValidationError", "heading_level": 3, "md_text": "Bases: ValueError An error raised during graph validation. Source code in pydantic_graph/pydantic_graph/exceptions.py #### message instance-attribute The error message.", "url": "https://ai.pydantic.dev/api/pydantic_graph/exceptions/index.html#pydantic_graph.exceptions.GraphValidationError", "page": "api/pydantic_graph/exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "GraphRuntimeError", "anchor": "pydantic_graph.exceptions.GraphRuntimeError", "heading_level": 3, "md_text": "Bases: RuntimeError Error caused by an issue during graph execution. Source code in pydantic_graph/pydantic_graph/exceptions.py #### message instance-attribute The error message.", "url": "https://ai.pydantic.dev/api/pydantic_graph/exceptions/index.html#pydantic_graph.exceptions.GraphRuntimeError", "page": "api/pydantic_graph/exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "GraphNodeStatusError", "anchor": "pydantic_graph.exceptions.GraphNodeStatusError", "heading_level": 3, "md_text": "Bases: GraphRuntimeError Error caused by trying to run a node that already has status 'running' , 'success' , or 'error' . Source code in pydantic_graph/pydantic_graph/exceptions.py #### check classmethod Check if the status is valid. Source code in pydantic_graph/pydantic_graph/exceptions.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/exceptions/index.html#pydantic_graph.exceptions.GraphNodeStatusError", "page": "api/pydantic_graph/exceptions/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_graph.beta.graph", "anchor": "pydantic_graphbetagraph", "heading_level": 1, "md_text": "Core graph execution engine for the next version of the pydantic-graph library. This module provides the main Graph class and GraphRun execution engine that handles the orchestration of nodes, edges, and parallel execution paths in the graph-based workflow system. ### StateT module-attribute Type variable for graph state. ### DepsT module-attribute Type variable for graph dependencies. ### InputT module-attribute Type variable for graph inputs. ### OutputT module-attribute Type variable for graph outputs. ### EndMarker dataclass Bases: Generic[OutputT] A marker indicating the end of graph execution with a final value. EndMarker is used internally to signal that the graph has completed execution and carries the final output value. Type Parameters OutputT: The type of the final output value Source code in pydantic_graph/pydantic_graph/beta/graph.py ### JoinItem dataclass An item representing data flowing into a join operation. JoinItem carries input data from a parallel execution path to a join node, along with metadata about which execution 'fork' it originated from. Source code in pydantic_graph/pydantic_graph/beta/graph.py #### join\\_id instance-attribute The ID of the join node this item is targeting. #### inputs instance-attribute The input data for the join operation. #### fork\\_stack instance-attribute The stack of ForkStackItems that led to producing this join item. ### Graph dataclass Bases: Generic[StateT, DepsT, InputT, OutputT] A complete graph definition ready for execution. The Graph class represents a complete workflow graph with typed inputs, outputs, state, and dependencies. It contains all nodes, edges, and metadata needed for execution. Type Parameters StateT: The type of the graph state DepsT: The type of the dependencies InputT: The type of the input data OutputT: The type of the output data Source code in pydantic_graph/pydantic_graph/beta/graph.py #### name instance-attribute Optional name for the graph, if not provided the name will be inferred from the calling frame on the first call to a graph method. #### state\\_type instance-attribute The type of the graph state. #### deps\\_type instance-attribute The type of the dependencies. #### input\\_type instance-attribute The type of the input data. #### output\\_type instance-attribute The type of the output data. #### auto\\_instrument instance-attribute Whether to automatically create instrumentation spans. #### nodes instance-attribute All nodes in the graph indexed by their ID. #### edges\\_by\\_source instance-attribute Outgoing paths from each source node. #### parent\\_forks instance-attribute Parent fork information for each join node. #### get\\_parent\\_fork Get the parent fork information for a join node. Parameters: Returns: Raises: Source code in pydantic_graph/pydantic_graph/beta/graph.py #### run async Execute the graph and return the final output. This is the main entry point for graph execution. It runs the graph to completion and returns the final output value. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph.py #### iter async Create an iterator for step-by-step graph execution. This method allows for more fine-grained control over graph execution, enabling inspection of intermediate states and results. Parameters: Yields: Source code in pydantic_graph/pydantic_graph/beta/graph.py #### render Render the graph as a Mermaid diagram string. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph.py #### \\_\\_str\\_\\_ Return a Mermaid diagram representation of the graph. Returns: Source code in pydantic_graph/pydantic_graph/beta/graph.py ### GraphTask dataclass A single task representing the execution of a node in the graph. GraphTask encapsulates all the information needed to execute a specific node, including its inputs and the fork context it's executing within. Source code in pydantic_graph/pydantic_graph/beta/graph.py #### node\\_id instance-attribute The ID of the node to execute. #### inputs instance-attribute The input data for the node. #### fork\\_stack class-attribute instance-attribute Stack of forks that have been entered. Used by the GraphRun to decide when to proceed through joins. #### task\\_id class-attribute instance-attribute Unique identifier for this task. ### GraphRun Bases: Generic[StateT, DepsT, OutputT] A single execution instance of a graph. GraphRun manages the execution state for a single run of a graph, including task scheduling, fork/join coordination, and result tracking. Type Parameters StateT: The type of the graph state DepsT: The type of the dependencies OutputT: The type of the output data Source code in pydantic_graph/pydantic_graph/beta/graph.py #### \\_\\_init\\_\\_ Initialize a graph run. Parameters: Source code in pydantic_graph/pydantic_graph/beta/graph.py #### graph instance-attribute The graph being executed. #### state instance-attribute The graph state instance. #### deps instance-attribute The dependencies instance. #### inputs instance-attribute The initial input data. #### \\_\\_aiter\\_\\_ Return self as an async iterator. Returns: Source code in pydantic_graph/pydantic_graph/beta/graph.py #### \\_\\_anext\\_\\_ async Get the next item in the async iteration. Returns: Source code in pydantic_graph/pydantic_graph/beta/graph.py #### next async Advance the graph execution by one step. This method allows for sending a value to the iterator, which is useful for resuming iteration or overriding intermediate results. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph.py #### next\\_task property Get the next task(s) to be executed. Returns: #### output property Get the final output if the graph has completed. Returns:", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_graph/index.html#pydantic_graphbetagraph", "page": "api/pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "StateT module-attribute", "anchor": "pydantic_graph.beta.graph.StateT", "heading_level": 3, "md_text": "Type variable for graph state.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_graph/index.html#pydantic_graph.beta.graph.StateT", "page": "api/pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "DepsT module-attribute", "anchor": "pydantic_graph.beta.graph.DepsT", "heading_level": 3, "md_text": "Type variable for graph dependencies.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_graph/index.html#pydantic_graph.beta.graph.DepsT", "page": "api/pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "InputT module-attribute", "anchor": "pydantic_graph.beta.graph.InputT", "heading_level": 3, "md_text": "Type variable for graph inputs.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_graph/index.html#pydantic_graph.beta.graph.InputT", "page": "api/pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "OutputT module-attribute", "anchor": "pydantic_graph.beta.graph.OutputT", "heading_level": 3, "md_text": "Type variable for graph outputs.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_graph/index.html#pydantic_graph.beta.graph.OutputT", "page": "api/pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "EndMarker dataclass", "anchor": "pydantic_graph.beta.graph.EndMarker", "heading_level": 3, "md_text": "Bases: Generic[OutputT] A marker indicating the end of graph execution with a final value. EndMarker is used internally to signal that the graph has completed execution and carries the final output value. Type Parameters OutputT: The type of the final output value Source code in pydantic_graph/pydantic_graph/beta/graph.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_graph/index.html#pydantic_graph.beta.graph.EndMarker", "page": "api/pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "JoinItem dataclass", "anchor": "pydantic_graph.beta.graph.JoinItem", "heading_level": 3, "md_text": "An item representing data flowing into a join operation. JoinItem carries input data from a parallel execution path to a join node, along with metadata about which execution 'fork' it originated from. Source code in pydantic_graph/pydantic_graph/beta/graph.py #### join\\_id instance-attribute The ID of the join node this item is targeting. #### inputs instance-attribute The input data for the join operation. #### fork\\_stack instance-attribute The stack of ForkStackItems that led to producing this join item.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_graph/index.html#pydantic_graph.beta.graph.JoinItem", "page": "api/pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "Graph dataclass", "anchor": "pydantic_graph.beta.graph.Graph", "heading_level": 3, "md_text": "Bases: Generic[StateT, DepsT, InputT, OutputT] A complete graph definition ready for execution. The Graph class represents a complete workflow graph with typed inputs, outputs, state, and dependencies. It contains all nodes, edges, and metadata needed for execution. Type Parameters StateT: The type of the graph state DepsT: The type of the dependencies InputT: The type of the input data OutputT: The type of the output data Source code in pydantic_graph/pydantic_graph/beta/graph.py #### name instance-attribute Optional name for the graph, if not provided the name will be inferred from the calling frame on the first call to a graph method. #### state\\_type instance-attribute The type of the graph state. #### deps\\_type instance-attribute The type of the dependencies. #### input\\_type instance-attribute The type of the input data. #### output\\_type instance-attribute The type of the output data. #### auto\\_instrument instance-attribute Whether to automatically create instrumentation spans. #### nodes instance-attribute All nodes in the graph indexed by their ID. #### edges\\_by\\_source instance-attribute Outgoing paths from each source node. #### parent\\_forks instance-attribute Parent fork information for each join node. #### get\\_parent\\_fork Get the parent fork information for a join node. Parameters: Returns: Raises: Source code in pydantic_graph/pydantic_graph/beta/graph.py #### run async Execute the graph and return the final output. This is the main entry point for graph execution. It runs the graph to completion and returns the final output value. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph.py #### iter async Create an iterator for step-by-step graph execution. This method allows for more fine-grained control over graph execution, enabling inspection of intermediate states and results. Parameters: Yields: Source code in pydantic_graph/pydantic_graph/beta/graph.py #### render Render the graph as a Mermaid diagram string. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph.py #### \\_\\_str\\_\\_ Return a Mermaid diagram representation of the graph. Returns: Source code in pydantic_graph/pydantic_graph/beta/graph.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_graph/index.html#pydantic_graph.beta.graph.Graph", "page": "api/pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "GraphTask dataclass", "anchor": "pydantic_graph.beta.graph.GraphTask", "heading_level": 3, "md_text": "A single task representing the execution of a node in the graph. GraphTask encapsulates all the information needed to execute a specific node, including its inputs and the fork context it's executing within. Source code in pydantic_graph/pydantic_graph/beta/graph.py #### node\\_id instance-attribute The ID of the node to execute. #### inputs instance-attribute The input data for the node. #### fork\\_stack class-attribute instance-attribute Stack of forks that have been entered. Used by the GraphRun to decide when to proceed through joins. #### task\\_id class-attribute instance-attribute Unique identifier for this task.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_graph/index.html#pydantic_graph.beta.graph.GraphTask", "page": "api/pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "GraphRun", "anchor": "pydantic_graph.beta.graph.GraphRun", "heading_level": 3, "md_text": "Bases: Generic[StateT, DepsT, OutputT] A single execution instance of a graph. GraphRun manages the execution state for a single run of a graph, including task scheduling, fork/join coordination, and result tracking. Type Parameters StateT: The type of the graph state DepsT: The type of the dependencies OutputT: The type of the output data Source code in pydantic_graph/pydantic_graph/beta/graph.py #### \\_\\_init\\_\\_ Initialize a graph run. Parameters: Source code in pydantic_graph/pydantic_graph/beta/graph.py #### graph instance-attribute The graph being executed. #### state instance-attribute The graph state instance. #### deps instance-attribute The dependencies instance. #### inputs instance-attribute The initial input data. #### \\_\\_aiter\\_\\_ Return self as an async iterator. Returns: Source code in pydantic_graph/pydantic_graph/beta/graph.py #### \\_\\_anext\\_\\_ async Get the next item in the async iteration. Returns: Source code in pydantic_graph/pydantic_graph/beta/graph.py #### next async Advance the graph execution by one step. This method allows for sending a value to the iterator, which is useful for resuming iteration or overriding intermediate results. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph.py #### next\\_task property Get the next task(s) to be executed. Returns: #### output property Get the final output if the graph has completed. Returns:", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_graph/index.html#pydantic_graph.beta.graph.GraphRun", "page": "api/pydantic_graph/beta_graph/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_graph.beta.step", "anchor": "pydantic_graphbetastep", "heading_level": 1, "md_text": "Step-based graph execution components. This module provides the core abstractions for step-based graph execution, including step contexts, step functions, and step nodes that bridge between the v1 and v2 graph execution systems. ### StepContext dataclass Bases: Generic[StateT, DepsT, InputT] Context information passed to step functions during graph execution. The step context provides access to the current graph state, dependencies, and input data for a step. Type Parameters StateT: The type of the graph state DepsT: The type of the dependencies InputT: The type of the input data Source code in pydantic_graph/pydantic_graph/beta/step.py #### inputs property The input data for this step. This must be a property to ensure correct variance behavior ### StepFunction Bases: Protocol[StateT, DepsT, InputT, OutputT] Protocol for step functions that can be executed in the graph. Step functions are async callables that receive a step context and return a result. Type Parameters StateT: The type of the graph state DepsT: The type of the dependencies InputT: The type of the input data OutputT: The type of the output data Source code in pydantic_graph/pydantic_graph/beta/step.py #### \\_\\_call\\_\\_ Execute the step function with the given context. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/step.py ### StreamFunction Bases: Protocol[StateT, DepsT, InputT, OutputT] Protocol for stream functions that can be executed in the graph. Stream functions are async callables that receive a step context and return an async iterator. Type Parameters StateT: The type of the graph state DepsT: The type of the dependencies InputT: The type of the input data OutputT: The type of the output data Source code in pydantic_graph/pydantic_graph/beta/step.py #### \\_\\_call\\_\\_ Execute the stream function with the given context. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/step.py ### AnyStepFunction module-attribute Type alias for a step function with any type parameters. ### Step dataclass Bases: Generic[StateT, DepsT, InputT, OutputT] A step in the graph execution that wraps a step function. Steps represent individual units of execution in the graph, encapsulating a step function along with metadata like ID and label. Type Parameters StateT: The type of the graph state DepsT: The type of the dependencies InputT: The type of the input data OutputT: The type of the output data Source code in pydantic_graph/pydantic_graph/beta/step.py #### id instance-attribute Unique identifier for this step. #### label instance-attribute Optional human-readable label for this step. #### call property The step function to execute. This needs to be a property for proper variance inference. #### as\\_node Create a step node with bound inputs. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/step.py ### StepNode dataclass Bases: BaseNode[StateT, DepsT, Any] A base node that represents a step with bound inputs. StepNode bridges between the v1 and v2 graph execution systems by wrapping a [ Step ](index.html#pydantic_graph.beta.step.Step) with bound inputs in a BaseNode interface. It is not meant to be run directly but rather used to indicate transitions to v2-style steps. Source code in pydantic_graph/pydantic_graph/beta/step.py #### step instance-attribute The step to execute. #### inputs instance-attribute The inputs bound to this step. #### run async Attempt to run the step node. Parameters: Returns: Raises: Source code in pydantic_graph/pydantic_graph/beta/step.py ### NodeStep Bases: Step[StateT, DepsT, Any, BaseNode[StateT, DepsT, Any] End[Any]] A step that wraps a BaseNode type for execution. NodeStep allows v1-style BaseNode classes to be used as steps in the v2 graph execution system. It validates that the input is of the expected node type and runs it with the appropriate graph context. Source code in pydantic_graph/pydantic_graph/beta/step.py #### \\_\\_init\\_\\_ Initialize a node step. Parameters: Source code in pydantic_graph/pydantic_graph/beta/step.py #### node\\_type instance-attribute The BaseNode type this step executes.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_step/index.html#pydantic_graphbetastep", "page": "api/pydantic_graph/beta_step/index.html", "source_site": "pydantic_ai"}
{"title": "StepContext dataclass", "anchor": "pydantic_graph.beta.step.StepContext", "heading_level": 3, "md_text": "Bases: Generic[StateT, DepsT, InputT] Context information passed to step functions during graph execution. The step context provides access to the current graph state, dependencies, and input data for a step. Type Parameters StateT: The type of the graph state DepsT: The type of the dependencies InputT: The type of the input data Source code in pydantic_graph/pydantic_graph/beta/step.py #### inputs property The input data for this step. This must be a property to ensure correct variance behavior", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_step/index.html#pydantic_graph.beta.step.StepContext", "page": "api/pydantic_graph/beta_step/index.html", "source_site": "pydantic_ai"}
{"title": "StepFunction", "anchor": "pydantic_graph.beta.step.StepFunction", "heading_level": 3, "md_text": "Bases: Protocol[StateT, DepsT, InputT, OutputT] Protocol for step functions that can be executed in the graph. Step functions are async callables that receive a step context and return a result. Type Parameters StateT: The type of the graph state DepsT: The type of the dependencies InputT: The type of the input data OutputT: The type of the output data Source code in pydantic_graph/pydantic_graph/beta/step.py #### \\_\\_call\\_\\_ Execute the step function with the given context. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/step.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_step/index.html#pydantic_graph.beta.step.StepFunction", "page": "api/pydantic_graph/beta_step/index.html", "source_site": "pydantic_ai"}
{"title": "StreamFunction", "anchor": "pydantic_graph.beta.step.StreamFunction", "heading_level": 3, "md_text": "Bases: Protocol[StateT, DepsT, InputT, OutputT] Protocol for stream functions that can be executed in the graph. Stream functions are async callables that receive a step context and return an async iterator. Type Parameters StateT: The type of the graph state DepsT: The type of the dependencies InputT: The type of the input data OutputT: The type of the output data Source code in pydantic_graph/pydantic_graph/beta/step.py #### \\_\\_call\\_\\_ Execute the stream function with the given context. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/step.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_step/index.html#pydantic_graph.beta.step.StreamFunction", "page": "api/pydantic_graph/beta_step/index.html", "source_site": "pydantic_ai"}
{"title": "AnyStepFunction module-attribute", "anchor": "pydantic_graph.beta.step.AnyStepFunction", "heading_level": 3, "md_text": "Type alias for a step function with any type parameters.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_step/index.html#pydantic_graph.beta.step.AnyStepFunction", "page": "api/pydantic_graph/beta_step/index.html", "source_site": "pydantic_ai"}
{"title": "Step dataclass", "anchor": "pydantic_graph.beta.step.Step", "heading_level": 3, "md_text": "Bases: Generic[StateT, DepsT, InputT, OutputT] A step in the graph execution that wraps a step function. Steps represent individual units of execution in the graph, encapsulating a step function along with metadata like ID and label. Type Parameters StateT: The type of the graph state DepsT: The type of the dependencies InputT: The type of the input data OutputT: The type of the output data Source code in pydantic_graph/pydantic_graph/beta/step.py #### id instance-attribute Unique identifier for this step. #### label instance-attribute Optional human-readable label for this step. #### call property The step function to execute. This needs to be a property for proper variance inference. #### as\\_node Create a step node with bound inputs. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/step.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_step/index.html#pydantic_graph.beta.step.Step", "page": "api/pydantic_graph/beta_step/index.html", "source_site": "pydantic_ai"}
{"title": "StepNode dataclass", "anchor": "pydantic_graph.beta.step.StepNode", "heading_level": 3, "md_text": "Bases: BaseNode[StateT, DepsT, Any] A base node that represents a step with bound inputs. StepNode bridges between the v1 and v2 graph execution systems by wrapping a [ Step ](index.html#pydantic_graph.beta.step.Step) with bound inputs in a BaseNode interface. It is not meant to be run directly but rather used to indicate transitions to v2-style steps. Source code in pydantic_graph/pydantic_graph/beta/step.py #### step instance-attribute The step to execute. #### inputs instance-attribute The inputs bound to this step. #### run async Attempt to run the step node. Parameters: Returns: Raises: Source code in pydantic_graph/pydantic_graph/beta/step.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_step/index.html#pydantic_graph.beta.step.StepNode", "page": "api/pydantic_graph/beta_step/index.html", "source_site": "pydantic_ai"}
{"title": "NodeStep", "anchor": "pydantic_graph.beta.step.NodeStep", "heading_level": 3, "md_text": "Bases: Step[StateT, DepsT, Any, BaseNode[StateT, DepsT, Any] End[Any]] A step that wraps a BaseNode type for execution. NodeStep allows v1-style BaseNode classes to be used as steps in the v2 graph execution system. It validates that the input is of the expected node type and runs it with the appropriate graph context. Source code in pydantic_graph/pydantic_graph/beta/step.py #### \\_\\_init\\_\\_ Initialize a node step. Parameters: Source code in pydantic_graph/pydantic_graph/beta/step.py #### node\\_type instance-attribute The BaseNode type this step executes.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_step/index.html#pydantic_graph.beta.step.NodeStep", "page": "api/pydantic_graph/beta_step/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_graph.beta", "anchor": "pydantic_graphbeta", "heading_level": 1, "md_text": "The next version of the pydantic-graph framework with enhanced graph execution capabilities. This module provides a parallel control flow graph execution framework with support for: - 'Step' nodes for task execution - 'Decision' nodes for conditional branching - 'Fork' nodes for parallel execution coordination - 'Join' nodes and 'Reducer's for re-joining parallel executions - Mermaid diagram generation for graph visualization ### Graph dataclass Bases: Generic[StateT, DepsT, InputT, OutputT] A complete graph definition ready for execution. The Graph class represents a complete workflow graph with typed inputs, outputs, state, and dependencies. It contains all nodes, edges, and metadata needed for execution. Type Parameters StateT: The type of the graph state DepsT: The type of the dependencies InputT: The type of the input data OutputT: The type of the output data Source code in pydantic_graph/pydantic_graph/beta/graph.py #### name instance-attribute Optional name for the graph, if not provided the name will be inferred from the calling frame on the first call to a graph method. #### state\\_type instance-attribute The type of the graph state. #### deps\\_type instance-attribute The type of the dependencies. #### input\\_type instance-attribute The type of the input data. #### output\\_type instance-attribute The type of the output data. #### auto\\_instrument instance-attribute Whether to automatically create instrumentation spans. #### nodes instance-attribute All nodes in the graph indexed by their ID. #### edges\\_by\\_source instance-attribute Outgoing paths from each source node. #### parent\\_forks instance-attribute Parent fork information for each join node. #### get\\_parent\\_fork Get the parent fork information for a join node. Parameters: Returns: Raises: Source code in pydantic_graph/pydantic_graph/beta/graph.py #### run async Execute the graph and return the final output. This is the main entry point for graph execution. It runs the graph to completion and returns the final output value. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph.py #### iter async Create an iterator for step-by-step graph execution. This method allows for more fine-grained control over graph execution, enabling inspection of intermediate states and results. Parameters: Yields: Source code in pydantic_graph/pydantic_graph/beta/graph.py #### render Render the graph as a Mermaid diagram string. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph.py #### \\_\\_str\\_\\_ Return a Mermaid diagram representation of the graph. Returns: Source code in pydantic_graph/pydantic_graph/beta/graph.py ### GraphBuilder dataclass Bases: Generic[StateT, DepsT, GraphInputT, GraphOutputT] A builder for constructing executable graph definitions. GraphBuilder provides a fluent interface for defining nodes, edges, and routing in a graph workflow. It supports typed state, dependencies, and input/output validation. Type Parameters StateT: The type of the graph state DepsT: The type of the dependencies GraphInputT: The type of the graph input data GraphOutputT: The type of the graph output data Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### name instance-attribute Optional name for the graph, if not provided the name will be inferred from the calling frame on the first call to a graph method. #### state\\_type instance-attribute The type of the graph state. #### deps\\_type instance-attribute The type of the dependencies. #### input\\_type instance-attribute The type of the graph input data. #### output\\_type instance-attribute The type of the graph output data. #### auto\\_instrument instance-attribute Whether to automatically create instrumentation spans. #### \\_\\_init\\_\\_ Initialize a graph builder. Parameters: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### start\\_node property Get the start node for the graph. Returns: #### end\\_node property Get the end node for the graph. Returns: #### step Create a step from a step function. This method can be used as a decorator or called directly to create a step node from an async function. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### stream Create a step from an async iterator (which functions like a \"stream\"). This method can be used as a decorator or called directly to create a step node from an async function. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### add Add one or more edge paths to the graph. This method processes edge paths and automatically creates any necessary fork nodes for broadcasts and maps. Parameters: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### add\\_edge Add a simple edge between two nodes. Parameters: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### add\\_mapping\\_edge Add an edge that maps iterable data across parallel paths. Parameters: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### edge\\_from Create an edge path builder starting from the given source nodes. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### decision Create a new decision node. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### match Create a decision branch matcher. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### match\\_node Create a decision branch for BaseNode subclasses. This is similar to match() but specifically designed for matching against BaseNode types from the v1 system. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### node Create an edge path from a BaseNode class. This method integrates v1-style BaseNode classes into the v2 graph system by analyzing their type hints and creating appropriate edges. Parameters: Returns: Raises: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### build Build the final executable graph from the accumulated nodes and edges. This method performs validation, normalization, and analysis of the graph structure to create a complete, executable graph instance. Parameters: Returns: Raises: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py ### EndNode Bases: Generic[InputT] Terminal node representing the completion of graph execution. The EndNode marks the successful completion of a graph execution flow and can collect the final output data. Source code in pydantic_graph/pydantic_graph/beta/node.py #### id class-attribute instance-attribute Fixed identifier for the end node. ### StartNode Bases: Generic[OutputT] Entry point node for graph execution. The StartNode represents the beginning of a graph execution flow. Source code in pydantic_graph/pydantic_graph/beta/node.py #### id class-attribute instance-attribute Fixed identifier for the start node. ### StepContext dataclass Bases: Generic[StateT, DepsT, InputT] Context information passed to step functions during graph execution. The step context provides access to the current graph state, dependencies, and input data for a step. Type Parameters StateT: The type of the graph state DepsT: The type of the dependencies InputT: The type of the input data Source code in pydantic_graph/pydantic_graph/beta/step.py #### inputs property The input data for this step. This must be a property to ensure correct variance behavior ### StepNode dataclass Bases: BaseNode[StateT, DepsT, Any] A base node that represents a step with bound inputs. StepNode bridges between the v1 and v2 graph execution systems by wrapping a [ Step ](../beta_step/index.html#pydantic_graph.beta.step.Step) with bound inputs in a BaseNode interface. It is not meant to be run directly but rather used to indicate transitions to v2-style steps. Source code in pydantic_graph/pydantic_graph/beta/step.py #### step instance-attribute The step to execute. #### inputs instance-attribute The inputs bound to this step. #### run async Attempt to run the step node. Parameters: Returns: Raises: Source code in pydantic_graph/pydantic_graph/beta/step.py ### TypeExpression Bases: Generic[T] A workaround for type checker limitations when using complex type expressions. requiring type[T] , such as Any , Union[...] , or Literal[...] . It provides a way to pass these complex type expressions to functions expecting concrete types. Example Instead of output_type=Union[str, int] (which may cause type errors), use output_type=TypeExpression[Union[str, int]] . Note This is a workaround for the lack of TypeForm in the Python type system. Source code in pydantic_graph/pydantic_graph/beta/util.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta/index.html#pydantic_graphbeta", "page": "api/pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "Graph dataclass", "anchor": "pydantic_graph.beta.Graph", "heading_level": 3, "md_text": "Bases: Generic[StateT, DepsT, InputT, OutputT] A complete graph definition ready for execution. The Graph class represents a complete workflow graph with typed inputs, outputs, state, and dependencies. It contains all nodes, edges, and metadata needed for execution. Type Parameters StateT: The type of the graph state DepsT: The type of the dependencies InputT: The type of the input data OutputT: The type of the output data Source code in pydantic_graph/pydantic_graph/beta/graph.py #### name instance-attribute Optional name for the graph, if not provided the name will be inferred from the calling frame on the first call to a graph method. #### state\\_type instance-attribute The type of the graph state. #### deps\\_type instance-attribute The type of the dependencies. #### input\\_type instance-attribute The type of the input data. #### output\\_type instance-attribute The type of the output data. #### auto\\_instrument instance-attribute Whether to automatically create instrumentation spans. #### nodes instance-attribute All nodes in the graph indexed by their ID. #### edges\\_by\\_source instance-attribute Outgoing paths from each source node. #### parent\\_forks instance-attribute Parent fork information for each join node. #### get\\_parent\\_fork Get the parent fork information for a join node. Parameters: Returns: Raises: Source code in pydantic_graph/pydantic_graph/beta/graph.py #### run async Execute the graph and return the final output. This is the main entry point for graph execution. It runs the graph to completion and returns the final output value. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph.py #### iter async Create an iterator for step-by-step graph execution. This method allows for more fine-grained control over graph execution, enabling inspection of intermediate states and results. Parameters: Yields: Source code in pydantic_graph/pydantic_graph/beta/graph.py #### render Render the graph as a Mermaid diagram string. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph.py #### \\_\\_str\\_\\_ Return a Mermaid diagram representation of the graph. Returns: Source code in pydantic_graph/pydantic_graph/beta/graph.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta/index.html#pydantic_graph.beta.Graph", "page": "api/pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "GraphBuilder dataclass", "anchor": "pydantic_graph.beta.GraphBuilder", "heading_level": 3, "md_text": "Bases: Generic[StateT, DepsT, GraphInputT, GraphOutputT] A builder for constructing executable graph definitions. GraphBuilder provides a fluent interface for defining nodes, edges, and routing in a graph workflow. It supports typed state, dependencies, and input/output validation. Type Parameters StateT: The type of the graph state DepsT: The type of the dependencies GraphInputT: The type of the graph input data GraphOutputT: The type of the graph output data Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### name instance-attribute Optional name for the graph, if not provided the name will be inferred from the calling frame on the first call to a graph method. #### state\\_type instance-attribute The type of the graph state. #### deps\\_type instance-attribute The type of the dependencies. #### input\\_type instance-attribute The type of the graph input data. #### output\\_type instance-attribute The type of the graph output data. #### auto\\_instrument instance-attribute Whether to automatically create instrumentation spans. #### \\_\\_init\\_\\_ Initialize a graph builder. Parameters: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### start\\_node property Get the start node for the graph. Returns: #### end\\_node property Get the end node for the graph. Returns: #### step Create a step from a step function. This method can be used as a decorator or called directly to create a step node from an async function. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### stream Create a step from an async iterator (which functions like a \"stream\"). This method can be used as a decorator or called directly to create a step node from an async function. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### add Add one or more edge paths to the graph. This method processes edge paths and automatically creates any necessary fork nodes for broadcasts and maps. Parameters: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### add\\_edge Add a simple edge between two nodes. Parameters: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### add\\_mapping\\_edge Add an edge that maps iterable data across parallel paths. Parameters: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### edge\\_from Create an edge path builder starting from the given source nodes. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### decision Create a new decision node. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### match Create a decision branch matcher. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### match\\_node Create a decision branch for BaseNode subclasses. This is similar to match() but specifically designed for matching against BaseNode types from the v1 system. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### node Create an edge path from a BaseNode class. This method integrates v1-style BaseNode classes into the v2 graph system by analyzing their type hints and creating appropriate edges. Parameters: Returns: Raises: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### build Build the final executable graph from the accumulated nodes and edges. This method performs validation, normalization, and analysis of the graph structure to create a complete, executable graph instance. Parameters: Returns: Raises: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta/index.html#pydantic_graph.beta.GraphBuilder", "page": "api/pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "EndNode", "anchor": "pydantic_graph.beta.EndNode", "heading_level": 3, "md_text": "Bases: Generic[InputT] Terminal node representing the completion of graph execution. The EndNode marks the successful completion of a graph execution flow and can collect the final output data. Source code in pydantic_graph/pydantic_graph/beta/node.py #### id class-attribute instance-attribute Fixed identifier for the end node.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta/index.html#pydantic_graph.beta.EndNode", "page": "api/pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "StartNode", "anchor": "pydantic_graph.beta.StartNode", "heading_level": 3, "md_text": "Bases: Generic[OutputT] Entry point node for graph execution. The StartNode represents the beginning of a graph execution flow. Source code in pydantic_graph/pydantic_graph/beta/node.py #### id class-attribute instance-attribute Fixed identifier for the start node.", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta/index.html#pydantic_graph.beta.StartNode", "page": "api/pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "StepContext dataclass", "anchor": "pydantic_graph.beta.StepContext", "heading_level": 3, "md_text": "Bases: Generic[StateT, DepsT, InputT] Context information passed to step functions during graph execution. The step context provides access to the current graph state, dependencies, and input data for a step. Type Parameters StateT: The type of the graph state DepsT: The type of the dependencies InputT: The type of the input data Source code in pydantic_graph/pydantic_graph/beta/step.py #### inputs property The input data for this step. This must be a property to ensure correct variance behavior", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta/index.html#pydantic_graph.beta.StepContext", "page": "api/pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "StepNode dataclass", "anchor": "pydantic_graph.beta.StepNode", "heading_level": 3, "md_text": "Bases: BaseNode[StateT, DepsT, Any] A base node that represents a step with bound inputs. StepNode bridges between the v1 and v2 graph execution systems by wrapping a [ Step ](../beta_step/index.html#pydantic_graph.beta.step.Step) with bound inputs in a BaseNode interface. It is not meant to be run directly but rather used to indicate transitions to v2-style steps. Source code in pydantic_graph/pydantic_graph/beta/step.py #### step instance-attribute The step to execute. #### inputs instance-attribute The inputs bound to this step. #### run async Attempt to run the step node. Parameters: Returns: Raises: Source code in pydantic_graph/pydantic_graph/beta/step.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta/index.html#pydantic_graph.beta.StepNode", "page": "api/pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "TypeExpression", "anchor": "pydantic_graph.beta.TypeExpression", "heading_level": 3, "md_text": "Bases: Generic[T] A workaround for type checker limitations when using complex type expressions. requiring type[T] , such as Any , Union[...] , or Literal[...] . It provides a way to pass these complex type expressions to functions expecting concrete types. Example Instead of output_type=Union[str, int] (which may cause type errors), use output_type=TypeExpression[Union[str, int]] . Note This is a workaround for the lack of TypeForm in the Python type system. Source code in pydantic_graph/pydantic_graph/beta/util.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta/index.html#pydantic_graph.beta.TypeExpression", "page": "api/pydantic_graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_graph.mermaid", "anchor": "pydantic_graphmermaid", "heading_level": 1, "md_text": "### DEFAULT\\_HIGHLIGHT\\_CSS module-attribute The default CSS to use for highlighting nodes. ### StateDiagramDirection module-attribute Used to specify the direction of the state diagram generated by mermaid. * 'TB' : Top to bottom, this is the default for mermaid charts. * 'LR' : Left to right * 'RL' : Right to left * 'BT' : Bottom to top ### generate\\_code Generate [Mermaid state diagram](https://mermaid.js.org/syntax/stateDiagram.html) code for a graph. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/mermaid.py ### request\\_image Generate an image of a Mermaid diagram using [mermaid.ink](https://mermaid.ink). Parameters: Returns: Source code in pydantic_graph/pydantic_graph/mermaid.py ### save\\_image Generate an image of a Mermaid diagram using [mermaid.ink](https://mermaid.ink) and save it to a local file. Parameters: Source code in pydantic_graph/pydantic_graph/mermaid.py ### MermaidConfig Bases: TypedDict Parameters to configure mermaid chart generation. Source code in pydantic_graph/pydantic_graph/mermaid.py #### start\\_node instance-attribute Identifiers of nodes that start the graph. #### highlighted\\_nodes instance-attribute Identifiers of nodes to highlight. #### highlight\\_css instance-attribute CSS to use for highlighting nodes. #### title instance-attribute The title of the diagram. #### edge\\_labels instance-attribute Whether to include edge labels in the diagram. #### notes instance-attribute Whether to include notes on nodes in the diagram, defaults to true. #### image\\_type instance-attribute The image type to generate. If unspecified, the default behavior is 'jpeg' . #### pdf\\_fit instance-attribute When using image\\_type='pdf', whether to fit the diagram to the PDF page. #### pdf\\_landscape instance-attribute When using image\\_type='pdf', whether to use landscape orientation for the PDF. This has no effect if using pdf_fit . #### pdf\\_paper instance-attribute When using image\\_type='pdf', the paper size of the PDF. #### background\\_color instance-attribute The background color of the diagram. If None, the default transparent background is used. The color value is interpreted as a hexadecimal color code by default (and should not have a leading '#'), but you can also use named colors by prefixing the value with '!' . For example, valid choices include background_color='!white' or background_color='FF0000' . #### theme instance-attribute The theme of the diagram. Defaults to 'default'. #### width instance-attribute The width of the diagram. #### height instance-attribute The height of the diagram. #### scale instance-attribute The scale of the diagram. The scale must be a number between 1 and 3, and you can only set a scale if one or both of width and height are set. #### httpx\\_client instance-attribute An HTTPX client to use for requests, mostly for testing purposes. #### direction instance-attribute The direction of the state diagram. ### NodeIdent module-attribute A type alias for a node identifier. This can be: * A node instance (instance of a subclass of [ BaseNode ](../nodes/index.html#pydantic_graph.nodes.BaseNode)). * A node class (subclass of [ BaseNode ](../nodes/index.html#pydantic_graph.nodes.BaseNode)). * A string representing the node ID.", "url": "https://ai.pydantic.dev/api/pydantic_graph/mermaid/index.html#pydantic_graphmermaid", "page": "api/pydantic_graph/mermaid/index.html", "source_site": "pydantic_ai"}
{"title": "DEFAULT_HIGHLIGHT_CSS module-attribute", "anchor": "pydantic_graph.mermaid.DEFAULT_HIGHLIGHT_CSS", "heading_level": 3, "md_text": "The default CSS to use for highlighting nodes.", "url": "https://ai.pydantic.dev/api/pydantic_graph/mermaid/index.html#pydantic_graph.mermaid.DEFAULT_HIGHLIGHT_CSS", "page": "api/pydantic_graph/mermaid/index.html", "source_site": "pydantic_ai"}
{"title": "StateDiagramDirection module-attribute", "anchor": "pydantic_graph.mermaid.StateDiagramDirection", "heading_level": 3, "md_text": "Used to specify the direction of the state diagram generated by mermaid. * 'TB' : Top to bottom, this is the default for mermaid charts. * 'LR' : Left to right * 'RL' : Right to left * 'BT' : Bottom to top", "url": "https://ai.pydantic.dev/api/pydantic_graph/mermaid/index.html#pydantic_graph.mermaid.StateDiagramDirection", "page": "api/pydantic_graph/mermaid/index.html", "source_site": "pydantic_ai"}
{"title": "generate_code", "anchor": "pydantic_graph.mermaid.generate_code", "heading_level": 3, "md_text": "Generate [Mermaid state diagram](https://mermaid.js.org/syntax/stateDiagram.html) code for a graph. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/mermaid.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/mermaid/index.html#pydantic_graph.mermaid.generate_code", "page": "api/pydantic_graph/mermaid/index.html", "source_site": "pydantic_ai"}
{"title": "request_image", "anchor": "pydantic_graph.mermaid.request_image", "heading_level": 3, "md_text": "Generate an image of a Mermaid diagram using [mermaid.ink](https://mermaid.ink). Parameters: Returns: Source code in pydantic_graph/pydantic_graph/mermaid.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/mermaid/index.html#pydantic_graph.mermaid.request_image", "page": "api/pydantic_graph/mermaid/index.html", "source_site": "pydantic_ai"}
{"title": "save_image", "anchor": "pydantic_graph.mermaid.save_image", "heading_level": 3, "md_text": "Generate an image of a Mermaid diagram using [mermaid.ink](https://mermaid.ink) and save it to a local file. Parameters: Source code in pydantic_graph/pydantic_graph/mermaid.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/mermaid/index.html#pydantic_graph.mermaid.save_image", "page": "api/pydantic_graph/mermaid/index.html", "source_site": "pydantic_ai"}
{"title": "MermaidConfig", "anchor": "pydantic_graph.mermaid.MermaidConfig", "heading_level": 3, "md_text": "Bases: TypedDict Parameters to configure mermaid chart generation. Source code in pydantic_graph/pydantic_graph/mermaid.py #### start\\_node instance-attribute Identifiers of nodes that start the graph. #### highlighted\\_nodes instance-attribute Identifiers of nodes to highlight. #### highlight\\_css instance-attribute CSS to use for highlighting nodes. #### title instance-attribute The title of the diagram. #### edge\\_labels instance-attribute Whether to include edge labels in the diagram. #### notes instance-attribute Whether to include notes on nodes in the diagram, defaults to true. #### image\\_type instance-attribute The image type to generate. If unspecified, the default behavior is 'jpeg' . #### pdf\\_fit instance-attribute When using image\\_type='pdf', whether to fit the diagram to the PDF page. #### pdf\\_landscape instance-attribute When using image\\_type='pdf', whether to use landscape orientation for the PDF. This has no effect if using pdf_fit . #### pdf\\_paper instance-attribute When using image\\_type='pdf', the paper size of the PDF. #### background\\_color instance-attribute The background color of the diagram. If None, the default transparent background is used. The color value is interpreted as a hexadecimal color code by default (and should not have a leading '#'), but you can also use named colors by prefixing the value with '!' . For example, valid choices include background_color='!white' or background_color='FF0000' . #### theme instance-attribute The theme of the diagram. Defaults to 'default'. #### width instance-attribute The width of the diagram. #### height instance-attribute The height of the diagram. #### scale instance-attribute The scale of the diagram. The scale must be a number between 1 and 3, and you can only set a scale if one or both of width and height are set. #### httpx\\_client instance-attribute An HTTPX client to use for requests, mostly for testing purposes. #### direction instance-attribute The direction of the state diagram.", "url": "https://ai.pydantic.dev/api/pydantic_graph/mermaid/index.html#pydantic_graph.mermaid.MermaidConfig", "page": "api/pydantic_graph/mermaid/index.html", "source_site": "pydantic_ai"}
{"title": "NodeIdent module-attribute", "anchor": "pydantic_graph.mermaid.NodeIdent", "heading_level": 3, "md_text": "A type alias for a node identifier. This can be: * A node instance (instance of a subclass of [ BaseNode ](../nodes/index.html#pydantic_graph.nodes.BaseNode)). * A node class (subclass of [ BaseNode ](../nodes/index.html#pydantic_graph.nodes.BaseNode)). * A string representing the node ID.", "url": "https://ai.pydantic.dev/api/pydantic_graph/mermaid/index.html#pydantic_graph.mermaid.NodeIdent", "page": "api/pydantic_graph/mermaid/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_graph.nodes", "anchor": "pydantic_graphnodes", "heading_level": 1, "md_text": "### StateT module-attribute Type variable for the state in a graph. ### GraphRunContext dataclass Bases: Generic[StateT, DepsT] Context for a graph. Source code in pydantic_graph/pydantic_graph/nodes.py #### state instance-attribute The state of the graph. #### deps instance-attribute Dependencies for the graph. ### BaseNode Bases: ABC , Generic[StateT, DepsT, NodeRunEndT] Base class for a node. Source code in pydantic_graph/pydantic_graph/nodes.py #### docstring\\_notes class-attribute Set to True to generate mermaid diagram notes from the class's docstring. While this can add valuable information to the diagram, it can make diagrams harder to view, hence it is disabled by default. You can also customise notes overriding the [ get_note ](index.html#pydantic_graph.nodes.BaseNode.get_note) method. #### run abstractmethod async Run the node. This is an abstract method that must be implemented by subclasses. Return types used at runtime The return type of this method are read by pydantic_graph at runtime and used to define which nodes can be called next in the graph. This is displayed in [mermaid diagrams](../mermaid/index.html) and enforced when running the graph. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/nodes.py #### get\\_node\\_id cached classmethod Get the ID of the node. Source code in pydantic_graph/pydantic_graph/nodes.py #### get\\_note classmethod Get a note about the node to render on mermaid charts. By default, this returns a note only if [ docstring_notes ](index.html#pydantic_graph.nodes.BaseNode.docstring_notes) is True . You can override this method to customise the node notes. Source code in pydantic_graph/pydantic_graph/nodes.py #### get\\_node\\_def classmethod Get the node definition. Source code in pydantic_graph/pydantic_graph/nodes.py #### deep\\_copy Returns a deep copy of the node. Source code in pydantic_graph/pydantic_graph/nodes.py ### End dataclass Bases: Generic[RunEndT] Type to return from a node to signal the end of the graph. Source code in pydantic_graph/pydantic_graph/nodes.py #### data instance-attribute Data to return from the graph. #### deep\\_copy\\_data Returns a deep copy of the end of the run. Source code in pydantic_graph/pydantic_graph/nodes.py ### Edge dataclass Annotation to apply a label to an edge in a graph. Source code in pydantic_graph/pydantic_graph/nodes.py #### label instance-attribute Label for the edge. ### DepsT module-attribute Type variable for the dependencies of a graph and node. ### RunEndT module-attribute Covariant type variable for the return type of a graph [ run ](../graph/index.html#pydantic_graph.graph.Graph.run). ### NodeRunEndT module-attribute Covariant type variable for the return type of a node [ run ](index.html#pydantic_graph.nodes.BaseNode.run).", "url": "https://ai.pydantic.dev/api/pydantic_graph/nodes/index.html#pydantic_graphnodes", "page": "api/pydantic_graph/nodes/index.html", "source_site": "pydantic_ai"}
{"title": "StateT module-attribute", "anchor": "pydantic_graph.nodes.StateT", "heading_level": 3, "md_text": "Type variable for the state in a graph.", "url": "https://ai.pydantic.dev/api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.StateT", "page": "api/pydantic_graph/nodes/index.html", "source_site": "pydantic_ai"}
{"title": "GraphRunContext dataclass", "anchor": "pydantic_graph.nodes.GraphRunContext", "heading_level": 3, "md_text": "Bases: Generic[StateT, DepsT] Context for a graph. Source code in pydantic_graph/pydantic_graph/nodes.py #### state instance-attribute The state of the graph. #### deps instance-attribute Dependencies for the graph.", "url": "https://ai.pydantic.dev/api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.GraphRunContext", "page": "api/pydantic_graph/nodes/index.html", "source_site": "pydantic_ai"}
{"title": "BaseNode", "anchor": "pydantic_graph.nodes.BaseNode", "heading_level": 3, "md_text": "Bases: ABC , Generic[StateT, DepsT, NodeRunEndT] Base class for a node. Source code in pydantic_graph/pydantic_graph/nodes.py #### docstring\\_notes class-attribute Set to True to generate mermaid diagram notes from the class's docstring. While this can add valuable information to the diagram, it can make diagrams harder to view, hence it is disabled by default. You can also customise notes overriding the [ get_note ](index.html#pydantic_graph.nodes.BaseNode.get_note) method. #### run abstractmethod async Run the node. This is an abstract method that must be implemented by subclasses. Return types used at runtime The return type of this method are read by pydantic_graph at runtime and used to define which nodes can be called next in the graph. This is displayed in [mermaid diagrams](../mermaid/index.html) and enforced when running the graph. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/nodes.py #### get\\_node\\_id cached classmethod Get the ID of the node. Source code in pydantic_graph/pydantic_graph/nodes.py #### get\\_note classmethod Get a note about the node to render on mermaid charts. By default, this returns a note only if [ docstring_notes ](index.html#pydantic_graph.nodes.BaseNode.docstring_notes) is True . You can override this method to customise the node notes. Source code in pydantic_graph/pydantic_graph/nodes.py #### get\\_node\\_def classmethod Get the node definition. Source code in pydantic_graph/pydantic_graph/nodes.py #### deep\\_copy Returns a deep copy of the node. Source code in pydantic_graph/pydantic_graph/nodes.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode", "page": "api/pydantic_graph/nodes/index.html", "source_site": "pydantic_ai"}
{"title": "End dataclass", "anchor": "pydantic_graph.nodes.End", "heading_level": 3, "md_text": "Bases: Generic[RunEndT] Type to return from a node to signal the end of the graph. Source code in pydantic_graph/pydantic_graph/nodes.py #### data instance-attribute Data to return from the graph. #### deep\\_copy\\_data Returns a deep copy of the end of the run. Source code in pydantic_graph/pydantic_graph/nodes.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.End", "page": "api/pydantic_graph/nodes/index.html", "source_site": "pydantic_ai"}
{"title": "Edge dataclass", "anchor": "pydantic_graph.nodes.Edge", "heading_level": 3, "md_text": "Annotation to apply a label to an edge in a graph. Source code in pydantic_graph/pydantic_graph/nodes.py #### label instance-attribute Label for the edge.", "url": "https://ai.pydantic.dev/api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.Edge", "page": "api/pydantic_graph/nodes/index.html", "source_site": "pydantic_ai"}
{"title": "DepsT module-attribute", "anchor": "pydantic_graph.nodes.DepsT", "heading_level": 3, "md_text": "Type variable for the dependencies of a graph and node.", "url": "https://ai.pydantic.dev/api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.DepsT", "page": "api/pydantic_graph/nodes/index.html", "source_site": "pydantic_ai"}
{"title": "RunEndT module-attribute", "anchor": "pydantic_graph.nodes.RunEndT", "heading_level": 3, "md_text": "Covariant type variable for the return type of a graph [ run ](../graph/index.html#pydantic_graph.graph.Graph.run).", "url": "https://ai.pydantic.dev/api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.RunEndT", "page": "api/pydantic_graph/nodes/index.html", "source_site": "pydantic_ai"}
{"title": "NodeRunEndT module-attribute", "anchor": "pydantic_graph.nodes.NodeRunEndT", "heading_level": 3, "md_text": "Covariant type variable for the return type of a node [ run ](index.html#pydantic_graph.nodes.BaseNode.run).", "url": "https://ai.pydantic.dev/api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.NodeRunEndT", "page": "api/pydantic_graph/nodes/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.retries", "anchor": "pydantic_airetries", "heading_level": 1, "md_text": "Retries utilities based on tenacity, especially for HTTP requests. This module provides HTTP transport wrappers and wait strategies that integrate with the tenacity library to add retry capabilities to HTTP requests. The transports can be used with HTTP clients that support custom transports (such as httpx), while the wait strategies can be used with any tenacity retry decorator. The module includes: - TenacityTransport: Synchronous HTTP transport with retry capabilities - AsyncTenacityTransport: Asynchronous HTTP transport with retry capabilities - wait\\_retry\\_after: Wait strategy that respects HTTP Retry-After headers ### RetryConfig Bases: TypedDict The configuration for tenacity-based retrying. These are precisely the arguments to the tenacity retry decorator, and they are generally used internally by passing them to that decorator via @retry(**config) or similar. All fields are optional, and if not provided, the default values from the tenacity.retry decorator will be used. Source code in pydantic_ai_slim/pydantic_ai/retries.py #### sleep instance-attribute A sleep strategy to use for sleeping between retries. Tenacity's default for this argument is tenacity.nap.sleep . #### stop instance-attribute A stop strategy to determine when to stop retrying. Tenacity's default for this argument is tenacity.stop.stop_never . #### wait instance-attribute A wait strategy to determine how long to wait between retries. Tenacity's default for this argument is tenacity.wait.wait_none . #### retry instance-attribute A retry strategy to determine which exceptions should trigger a retry. Tenacity's default for this argument is tenacity.retry.retry_if_exception_type() . #### before instance-attribute A callable that is called before each retry attempt. Tenacity's default for this argument is tenacity.before.before_nothing . #### after instance-attribute A callable that is called after each retry attempt. Tenacity's default for this argument is tenacity.after.after_nothing . #### before\\_sleep instance-attribute An optional callable that is called before sleeping between retries. Tenacity's default for this argument is None . #### reraise instance-attribute Whether to reraise the last exception if the retry attempts are exhausted, or raise a RetryError instead. Tenacity's default for this argument is False . #### retry\\_error\\_cls instance-attribute The exception class to raise when the retry attempts are exhausted and reraise is False. Tenacity's default for this argument is tenacity.RetryError . #### retry\\_error\\_callback instance-attribute An optional callable that is called when the retry attempts are exhausted and reraise is False. Tenacity's default for this argument is None . ### TenacityTransport Bases: BaseTransport Synchronous HTTP transport with tenacity-based retry functionality. This transport wraps another BaseTransport and adds retry capabilities using the tenacity library. It can be configured to retry requests based on various conditions such as specific exception types, response status codes, or custom validation logic. The transport works by intercepting HTTP requests and responses, allowing the tenacity controller to determine when and how to retry failed requests. The validate\\_response function can be used to convert HTTP responses into exceptions that trigger retries. Parameters: Example Source code in pydantic_ai_slim/pydantic_ai/retries.py #### handle\\_request Handle an HTTP request with retry logic. Parameters: Returns: Raises: Source code in pydantic_ai_slim/pydantic_ai/retries.py ### AsyncTenacityTransport Bases: AsyncBaseTransport Asynchronous HTTP transport with tenacity-based retry functionality. This transport wraps another AsyncBaseTransport and adds retry capabilities using the tenacity library. It can be configured to retry requests based on various conditions such as specific exception types, response status codes, or custom validation logic. The transport works by intercepting HTTP requests and responses, allowing the tenacity controller to determine when and how to retry failed requests. The validate\\_response function can be used to convert HTTP responses into exceptions that trigger retries. Parameters: Example Source code in pydantic_ai_slim/pydantic_ai/retries.py #### handle\\_async\\_request async Handle an async HTTP request with retry logic. Parameters: Returns: Raises: Source code in pydantic_ai_slim/pydantic_ai/retries.py ### wait\\_retry\\_after Create a tenacity-compatible wait strategy that respects HTTP Retry-After headers. This wait strategy checks if the exception contains an HTTPStatusError with a Retry-After header, and if so, waits for the time specified in the header. If no header is present or parsing fails, it falls back to the provided strategy. The Retry-After header can be in two formats: - An integer representing seconds to wait - An HTTP date string representing when to retry Parameters: Returns: Example Source code in pydantic_ai_slim/pydantic_ai/retries.py", "url": "https://ai.pydantic.dev/api/retries/index.html#pydantic_airetries", "page": "api/retries/index.html", "source_site": "pydantic_ai"}
{"title": "RetryConfig", "anchor": "pydantic_ai.retries.RetryConfig", "heading_level": 3, "md_text": "Bases: TypedDict The configuration for tenacity-based retrying. These are precisely the arguments to the tenacity retry decorator, and they are generally used internally by passing them to that decorator via @retry(**config) or similar. All fields are optional, and if not provided, the default values from the tenacity.retry decorator will be used. Source code in pydantic_ai_slim/pydantic_ai/retries.py #### sleep instance-attribute A sleep strategy to use for sleeping between retries. Tenacity's default for this argument is tenacity.nap.sleep . #### stop instance-attribute A stop strategy to determine when to stop retrying. Tenacity's default for this argument is tenacity.stop.stop_never . #### wait instance-attribute A wait strategy to determine how long to wait between retries. Tenacity's default for this argument is tenacity.wait.wait_none . #### retry instance-attribute A retry strategy to determine which exceptions should trigger a retry. Tenacity's default for this argument is tenacity.retry.retry_if_exception_type() . #### before instance-attribute A callable that is called before each retry attempt. Tenacity's default for this argument is tenacity.before.before_nothing . #### after instance-attribute A callable that is called after each retry attempt. Tenacity's default for this argument is tenacity.after.after_nothing . #### before\\_sleep instance-attribute An optional callable that is called before sleeping between retries. Tenacity's default for this argument is None . #### reraise instance-attribute Whether to reraise the last exception if the retry attempts are exhausted, or raise a RetryError instead. Tenacity's default for this argument is False . #### retry\\_error\\_cls instance-attribute The exception class to raise when the retry attempts are exhausted and reraise is False. Tenacity's default for this argument is tenacity.RetryError . #### retry\\_error\\_callback instance-attribute An optional callable that is called when the retry attempts are exhausted and reraise is False. Tenacity's default for this argument is None .", "url": "https://ai.pydantic.dev/api/retries/index.html#pydantic_ai.retries.RetryConfig", "page": "api/retries/index.html", "source_site": "pydantic_ai"}
{"title": "TenacityTransport", "anchor": "pydantic_ai.retries.TenacityTransport", "heading_level": 3, "md_text": "Bases: BaseTransport Synchronous HTTP transport with tenacity-based retry functionality. This transport wraps another BaseTransport and adds retry capabilities using the tenacity library. It can be configured to retry requests based on various conditions such as specific exception types, response status codes, or custom validation logic. The transport works by intercepting HTTP requests and responses, allowing the tenacity controller to determine when and how to retry failed requests. The validate\\_response function can be used to convert HTTP responses into exceptions that trigger retries. Parameters: Example Source code in pydantic_ai_slim/pydantic_ai/retries.py #### handle\\_request Handle an HTTP request with retry logic. Parameters: Returns: Raises: Source code in pydantic_ai_slim/pydantic_ai/retries.py", "url": "https://ai.pydantic.dev/api/retries/index.html#pydantic_ai.retries.TenacityTransport", "page": "api/retries/index.html", "source_site": "pydantic_ai"}
{"title": "AsyncTenacityTransport", "anchor": "pydantic_ai.retries.AsyncTenacityTransport", "heading_level": 3, "md_text": "Bases: AsyncBaseTransport Asynchronous HTTP transport with tenacity-based retry functionality. This transport wraps another AsyncBaseTransport and adds retry capabilities using the tenacity library. It can be configured to retry requests based on various conditions such as specific exception types, response status codes, or custom validation logic. The transport works by intercepting HTTP requests and responses, allowing the tenacity controller to determine when and how to retry failed requests. The validate\\_response function can be used to convert HTTP responses into exceptions that trigger retries. Parameters: Example Source code in pydantic_ai_slim/pydantic_ai/retries.py #### handle\\_async\\_request async Handle an async HTTP request with retry logic. Parameters: Returns: Raises: Source code in pydantic_ai_slim/pydantic_ai/retries.py", "url": "https://ai.pydantic.dev/api/retries/index.html#pydantic_ai.retries.AsyncTenacityTransport", "page": "api/retries/index.html", "source_site": "pydantic_ai"}
{"title": "wait_retry_after", "anchor": "pydantic_ai.retries.wait_retry_after", "heading_level": 3, "md_text": "Create a tenacity-compatible wait strategy that respects HTTP Retry-After headers. This wait strategy checks if the exception contains an HTTPStatusError with a Retry-After header, and if so, waits for the time specified in the header. If no header is present or parsing fails, it falls back to the provided strategy. The Retry-After header can be in two formats: - An integer representing seconds to wait - An HTTP date string representing when to retry Parameters: Returns: Example Source code in pydantic_ai_slim/pydantic_ai/retries.py", "url": "https://ai.pydantic.dev/api/retries/index.html#pydantic_ai.retries.wait_retry_after", "page": "api/retries/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.result", "anchor": "pydantic_airesult", "heading_level": 1, "md_text": "### StreamedRunResult dataclass Bases: Generic[AgentDepsT, OutputDataT] Result of a streamed run that returns structured data via a tool call. Source code in pydantic_ai_slim/pydantic_ai/result.py #### is\\_complete class-attribute instance-attribute Whether the stream has all been received. This is set to True when one of [ stream_output ](index.html#pydantic_ai.result.StreamedRunResult.stream_output), [ stream_text ](index.html#pydantic_ai.result.StreamedRunResult.stream_text), [ stream_responses ](index.html#pydantic_ai.result.StreamedRunResult.stream_responses) or [ get_output ](index.html#pydantic_ai.result.StreamedRunResult.get_output) completes. #### all\\_messages Return the history of \\_messages. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/result.py #### all\\_messages\\_json Return all messages from [ all_messages ](index.html#pydantic_ai.result.StreamedRunResult.all_messages) as JSON bytes. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/result.py #### new\\_messages Return new messages associated with this run. Messages from older runs are excluded. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/result.py #### new\\_messages\\_json Return new messages from [ new_messages ](index.html#pydantic_ai.result.StreamedRunResult.new_messages) as JSON bytes. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/result.py #### stream async deprecated Deprecated StreamedRunResult.stream is deprecated, use stream_output instead. Source code in pydantic_ai_slim/pydantic_ai/result.py #### stream\\_output async Stream the output as an async iterable. The pydantic validator for structured data will be called in [partial mode](https://docs.pydantic.dev/dev/concepts/experimental/#partial-validation) on each iteration. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/result.py #### stream\\_text async Stream the text result as an async iterable. Note Result validators will NOT be called on the text result if delta=True . Parameters: Source code in pydantic_ai_slim/pydantic_ai/result.py #### stream\\_structured async deprecated Deprecated StreamedRunResult.stream_structured is deprecated, use stream_responses instead. Source code in pydantic_ai_slim/pydantic_ai/result.py #### stream\\_responses async Stream the response as an async iterable of Structured LLM Messages. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/result.py #### get\\_output async Stream the whole response, validate and return it. Source code in pydantic_ai_slim/pydantic_ai/result.py #### response property Return the current state of the response. #### usage Return the usage of the whole run. Note This won't return the full usage until the stream is finished. Source code in pydantic_ai_slim/pydantic_ai/result.py #### timestamp Get the timestamp of the response. Source code in pydantic_ai_slim/pydantic_ai/result.py #### validate\\_structured\\_output async deprecated Deprecated validate_structured_output is deprecated, use validate_response_output instead. Source code in pydantic_ai_slim/pydantic_ai/result.py #### validate\\_response\\_output async Validate a structured result message. Source code in pydantic_ai_slim/pydantic_ai/result.py", "url": "https://ai.pydantic.dev/api/result/index.html#pydantic_airesult", "page": "api/result/index.html", "source_site": "pydantic_ai"}
{"title": "StreamedRunResult dataclass", "anchor": "pydantic_ai.result.StreamedRunResult", "heading_level": 3, "md_text": "Bases: Generic[AgentDepsT, OutputDataT] Result of a streamed run that returns structured data via a tool call. Source code in pydantic_ai_slim/pydantic_ai/result.py #### is\\_complete class-attribute instance-attribute Whether the stream has all been received. This is set to True when one of [ stream_output ](index.html#pydantic_ai.result.StreamedRunResult.stream_output), [ stream_text ](index.html#pydantic_ai.result.StreamedRunResult.stream_text), [ stream_responses ](index.html#pydantic_ai.result.StreamedRunResult.stream_responses) or [ get_output ](index.html#pydantic_ai.result.StreamedRunResult.get_output) completes. #### all\\_messages Return the history of \\_messages. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/result.py #### all\\_messages\\_json Return all messages from [ all_messages ](index.html#pydantic_ai.result.StreamedRunResult.all_messages) as JSON bytes. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/result.py #### new\\_messages Return new messages associated with this run. Messages from older runs are excluded. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/result.py #### new\\_messages\\_json Return new messages from [ new_messages ](index.html#pydantic_ai.result.StreamedRunResult.new_messages) as JSON bytes. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/result.py #### stream async deprecated Deprecated StreamedRunResult.stream is deprecated, use stream_output instead. Source code in pydantic_ai_slim/pydantic_ai/result.py #### stream\\_output async Stream the output as an async iterable. The pydantic validator for structured data will be called in [partial mode](https://docs.pydantic.dev/dev/concepts/experimental/#partial-validation) on each iteration. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/result.py #### stream\\_text async Stream the text result as an async iterable. Note Result validators will NOT be called on the text result if delta=True . Parameters: Source code in pydantic_ai_slim/pydantic_ai/result.py #### stream\\_structured async deprecated Deprecated StreamedRunResult.stream_structured is deprecated, use stream_responses instead. Source code in pydantic_ai_slim/pydantic_ai/result.py #### stream\\_responses async Stream the response as an async iterable of Structured LLM Messages. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/result.py #### get\\_output async Stream the whole response, validate and return it. Source code in pydantic_ai_slim/pydantic_ai/result.py #### response property Return the current state of the response. #### usage Return the usage of the whole run. Note This won't return the full usage until the stream is finished. Source code in pydantic_ai_slim/pydantic_ai/result.py #### timestamp Get the timestamp of the response. Source code in pydantic_ai_slim/pydantic_ai/result.py #### validate\\_structured\\_output async deprecated Deprecated validate_structured_output is deprecated, use validate_response_output instead. Source code in pydantic_ai_slim/pydantic_ai/result.py #### validate\\_response\\_output async Validate a structured result message. Source code in pydantic_ai_slim/pydantic_ai/result.py", "url": "https://ai.pydantic.dev/api/result/index.html#pydantic_ai.result.StreamedRunResult", "page": "api/result/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_graph.beta.graph_builder", "anchor": "pydantic_graphbetagraph_builder", "heading_level": 1, "md_text": "Graph builder for constructing executable graph definitions. This module provides the GraphBuilder class and related utilities for constructing typed, executable graph definitions with steps, joins, decisions, and edge routing. ### GraphBuilder dataclass Bases: Generic[StateT, DepsT, GraphInputT, GraphOutputT] A builder for constructing executable graph definitions. GraphBuilder provides a fluent interface for defining nodes, edges, and routing in a graph workflow. It supports typed state, dependencies, and input/output validation. Type Parameters StateT: The type of the graph state DepsT: The type of the dependencies GraphInputT: The type of the graph input data GraphOutputT: The type of the graph output data Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### \\_\\_init\\_\\_ Initialize a graph builder. Parameters: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### name instance-attribute Optional name for the graph, if not provided the name will be inferred from the calling frame on the first call to a graph method. #### state\\_type instance-attribute The type of the graph state. #### deps\\_type instance-attribute The type of the dependencies. #### input\\_type instance-attribute The type of the graph input data. #### output\\_type instance-attribute The type of the graph output data. #### auto\\_instrument instance-attribute Whether to automatically create instrumentation spans. #### start\\_node property Get the start node for the graph. Returns: #### end\\_node property Get the end node for the graph. Returns: #### step Create a step from a step function. This method can be used as a decorator or called directly to create a step node from an async function. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### stream Create a step from an async iterator (which functions like a \"stream\"). This method can be used as a decorator or called directly to create a step node from an async function. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### add Add one or more edge paths to the graph. This method processes edge paths and automatically creates any necessary fork nodes for broadcasts and maps. Parameters: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### add\\_edge Add a simple edge between two nodes. Parameters: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### add\\_mapping\\_edge Add an edge that maps iterable data across parallel paths. Parameters: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### edge\\_from Create an edge path builder starting from the given source nodes. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### decision Create a new decision node. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### match Create a decision branch matcher. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### match\\_node Create a decision branch for BaseNode subclasses. This is similar to match() but specifically designed for matching against BaseNode types from the v1 system. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### node Create an edge path from a BaseNode class. This method integrates v1-style BaseNode classes into the v2 graph system by analyzing their type hints and creating appropriate edges. Parameters: Returns: Raises: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### build Build the final executable graph from the accumulated nodes and edges. This method performs validation, normalization, and analysis of the graph structure to create a complete, executable graph instance. Parameters: Returns: Raises: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_graph_builder/index.html#pydantic_graphbetagraph_builder", "page": "api/pydantic_graph/beta_graph_builder/index.html", "source_site": "pydantic_ai"}
{"title": "GraphBuilder dataclass", "anchor": "pydantic_graph.beta.graph_builder.GraphBuilder", "heading_level": 3, "md_text": "Bases: Generic[StateT, DepsT, GraphInputT, GraphOutputT] A builder for constructing executable graph definitions. GraphBuilder provides a fluent interface for defining nodes, edges, and routing in a graph workflow. It supports typed state, dependencies, and input/output validation. Type Parameters StateT: The type of the graph state DepsT: The type of the dependencies GraphInputT: The type of the graph input data GraphOutputT: The type of the graph output data Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### \\_\\_init\\_\\_ Initialize a graph builder. Parameters: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### name instance-attribute Optional name for the graph, if not provided the name will be inferred from the calling frame on the first call to a graph method. #### state\\_type instance-attribute The type of the graph state. #### deps\\_type instance-attribute The type of the dependencies. #### input\\_type instance-attribute The type of the graph input data. #### output\\_type instance-attribute The type of the graph output data. #### auto\\_instrument instance-attribute Whether to automatically create instrumentation spans. #### start\\_node property Get the start node for the graph. Returns: #### end\\_node property Get the end node for the graph. Returns: #### step Create a step from a step function. This method can be used as a decorator or called directly to create a step node from an async function. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### stream Create a step from an async iterator (which functions like a \"stream\"). This method can be used as a decorator or called directly to create a step node from an async function. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### add Add one or more edge paths to the graph. This method processes edge paths and automatically creates any necessary fork nodes for broadcasts and maps. Parameters: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### add\\_edge Add a simple edge between two nodes. Parameters: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### add\\_mapping\\_edge Add an edge that maps iterable data across parallel paths. Parameters: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### edge\\_from Create an edge path builder starting from the given source nodes. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### decision Create a new decision node. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### match Create a decision branch matcher. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### match\\_node Create a decision branch for BaseNode subclasses. This is similar to match() but specifically designed for matching against BaseNode types from the v1 system. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### node Create an edge path from a BaseNode class. This method integrates v1-style BaseNode classes into the v2 graph system by analyzing their type hints and creating appropriate edges. Parameters: Returns: Raises: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py #### build Build the final executable graph from the accumulated nodes and edges. This method performs validation, normalization, and analysis of the graph structure to create a complete, executable graph instance. Parameters: Returns: Raises: Source code in pydantic_graph/pydantic_graph/beta/graph_builder.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder", "page": "api/pydantic_graph/beta_graph_builder/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.settings", "anchor": "pydantic_aisettings", "heading_level": 1, "md_text": "### ModelSettings Bases: TypedDict Settings to configure an LLM. Here we include only settings which apply to multiple models / model providers, though not all of these settings are supported by all models. Source code in pydantic_ai_slim/pydantic_ai/settings.py #### max\\_tokens instance-attribute The maximum number of tokens to generate before stopping. Supported by: * Gemini * Anthropic * OpenAI * Groq * Cohere * Mistral * Bedrock * MCP Sampling #### temperature instance-attribute Amount of randomness injected into the response. Use temperature closer to 0.0 for analytical / multiple choice, and closer to a model's maximum temperature for creative and generative tasks. Note that even with temperature of 0.0 , the results will not be fully deterministic. Supported by: * Gemini * Anthropic * OpenAI * Groq * Cohere * Mistral * Bedrock #### top\\_p instance-attribute An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top\\_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. You should either alter temperature or top_p , but not both. Supported by: * Gemini * Anthropic * OpenAI * Groq * Cohere * Mistral * Bedrock #### timeout instance-attribute Override the client-level default timeout for a request, in seconds. Supported by: * Gemini * Anthropic * OpenAI * Groq * Mistral #### parallel\\_tool\\_calls instance-attribute Whether to allow parallel tool calls. Supported by: * OpenAI (some models, not o1) * Groq * Anthropic #### seed instance-attribute The random seed to use for the model, theoretically allowing for deterministic results. Supported by: * OpenAI * Groq * Cohere * Mistral * Gemini #### presence\\_penalty instance-attribute Penalize new tokens based on whether they have appeared in the text so far. Supported by: * OpenAI * Groq * Cohere * Gemini * Mistral #### frequency\\_penalty instance-attribute Penalize new tokens based on their existing frequency in the text so far. Supported by: * OpenAI * Groq * Cohere * Gemini * Mistral #### logit\\_bias instance-attribute Modify the likelihood of specified tokens appearing in the completion. Supported by: * OpenAI * Groq #### stop\\_sequences instance-attribute Sequences that will cause the model to stop generating. Supported by: * OpenAI * Anthropic * Bedrock * Mistral * Groq * Cohere * Google #### extra\\_headers instance-attribute Extra headers to send to the model. Supported by: * OpenAI * Anthropic * Groq #### extra\\_body instance-attribute Extra body to send to the model. Supported by: * OpenAI * Anthropic * Groq", "url": "https://ai.pydantic.dev/api/settings/index.html#pydantic_aisettings", "page": "api/settings/index.html", "source_site": "pydantic_ai"}
{"title": "ModelSettings", "anchor": "pydantic_ai.settings.ModelSettings", "heading_level": 3, "md_text": "Bases: TypedDict Settings to configure an LLM. Here we include only settings which apply to multiple models / model providers, though not all of these settings are supported by all models. Source code in pydantic_ai_slim/pydantic_ai/settings.py #### max\\_tokens instance-attribute The maximum number of tokens to generate before stopping. Supported by: * Gemini * Anthropic * OpenAI * Groq * Cohere * Mistral * Bedrock * MCP Sampling #### temperature instance-attribute Amount of randomness injected into the response. Use temperature closer to 0.0 for analytical / multiple choice, and closer to a model's maximum temperature for creative and generative tasks. Note that even with temperature of 0.0 , the results will not be fully deterministic. Supported by: * Gemini * Anthropic * OpenAI * Groq * Cohere * Mistral * Bedrock #### top\\_p instance-attribute An alternative to sampling with temperature, called nucleus sampling, where the model considers the results of the tokens with top\\_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. You should either alter temperature or top_p , but not both. Supported by: * Gemini * Anthropic * OpenAI * Groq * Cohere * Mistral * Bedrock #### timeout instance-attribute Override the client-level default timeout for a request, in seconds. Supported by: * Gemini * Anthropic * OpenAI * Groq * Mistral #### parallel\\_tool\\_calls instance-attribute Whether to allow parallel tool calls. Supported by: * OpenAI (some models, not o1) * Groq * Anthropic #### seed instance-attribute The random seed to use for the model, theoretically allowing for deterministic results. Supported by: * OpenAI * Groq * Cohere * Mistral * Gemini #### presence\\_penalty instance-attribute Penalize new tokens based on whether they have appeared in the text so far. Supported by: * OpenAI * Groq * Cohere * Gemini * Mistral #### frequency\\_penalty instance-attribute Penalize new tokens based on their existing frequency in the text so far. Supported by: * OpenAI * Groq * Cohere * Gemini * Mistral #### logit\\_bias instance-attribute Modify the likelihood of specified tokens appearing in the completion. Supported by: * OpenAI * Groq #### stop\\_sequences instance-attribute Sequences that will cause the model to stop generating. Supported by: * OpenAI * Anthropic * Bedrock * Mistral * Groq * Cohere * Google #### extra\\_headers instance-attribute Extra headers to send to the model. Supported by: * OpenAI * Anthropic * Groq #### extra\\_body instance-attribute Extra body to send to the model. Supported by: * OpenAI * Anthropic * Groq", "url": "https://ai.pydantic.dev/api/settings/index.html#pydantic_ai.settings.ModelSettings", "page": "api/settings/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_graph.persistence", "anchor": "pydantic_graphpersistence", "heading_level": 1, "md_text": "### SnapshotStatus module-attribute The status of a snapshot. * 'created' : The snapshot has been created but not yet run. * 'pending' : The snapshot has been retrieved with [ load_next ](index.html#pydantic_graph.persistence.BaseStatePersistence.load_next) but not yet run. * 'running' : The snapshot is currently running. * 'success' : The snapshot has been run successfully. * 'error' : The snapshot has been run but an error occurred. ### NodeSnapshot dataclass Bases: Generic[StateT, RunEndT] History step describing the execution of a node in a graph. Source code in pydantic_graph/pydantic_graph/persistence/__init__.py #### state instance-attribute The state of the graph before the node is run. #### node instance-attribute The node to run next. #### start\\_ts class-attribute instance-attribute The timestamp when the node started running, None until the run starts. #### duration class-attribute instance-attribute The duration of the node run in seconds, if the node has been run. #### status class-attribute instance-attribute The status of the snapshot. #### kind class-attribute instance-attribute The kind of history step, can be used as a discriminator when deserializing history. #### id class-attribute instance-attribute Unique ID of the snapshot. ### EndSnapshot dataclass Bases: Generic[StateT, RunEndT] History step describing the end of a graph run. Source code in pydantic_graph/pydantic_graph/persistence/__init__.py #### state instance-attribute The state of the graph at the end of the run. #### result instance-attribute The result of the graph run. #### ts class-attribute instance-attribute The timestamp when the graph run ended. #### kind class-attribute instance-attribute The kind of history step, can be used as a discriminator when deserializing history. #### id class-attribute instance-attribute Unique ID of the snapshot. #### node property Shim to get the [ result ](index.html#pydantic_graph.persistence.EndSnapshot.result). Useful to allow [snapshot.node for snapshot in persistence.history] . ### Snapshot module-attribute A step in the history of a graph run. [ Graph.run ](../graph/index.html#pydantic_graph.graph.Graph.run) returns a list of these steps describing the execution of the graph, together with the run return value. ### BaseStatePersistence Bases: ABC , Generic[StateT, RunEndT] Abstract base class for storing the state of a graph run. Each instance of a BaseStatePersistence subclass should be used for a single graph run. Source code in pydantic_graph/pydantic_graph/persistence/__init__.py #### snapshot\\_node abstractmethod async Snapshot the state of a graph, when the next step is to run a node. This method should add a [ NodeSnapshot ](index.html#pydantic_graph.persistence.NodeSnapshot) to persistence. Parameters: Source code in pydantic_graph/pydantic_graph/persistence/__init__.py #### snapshot\\_node\\_if\\_new abstractmethod async Snapshot the state of a graph if the snapshot ID doesn't already exist in persistence. This method will generally call [ snapshot_node ](index.html#pydantic_graph.persistence.BaseStatePersistence.snapshot_node) but should do so in an atomic way. Parameters: Source code in pydantic_graph/pydantic_graph/persistence/__init__.py #### snapshot\\_end abstractmethod async Snapshot the state of a graph when the graph has ended. This method should add an [ EndSnapshot ](index.html#pydantic_graph.persistence.EndSnapshot) to persistence. Parameters: Source code in pydantic_graph/pydantic_graph/persistence/__init__.py #### record\\_run abstractmethod Record the run of the node, or error if the node is already running. Parameters: Raises: Returns: In particular this should set: * [ NodeSnapshot.status ](index.html#pydantic_graph.persistence.NodeSnapshot.status) to 'running' and [ NodeSnapshot.start_ts ](index.html#pydantic_graph.persistence.NodeSnapshot.start_ts) when the run starts. * [ NodeSnapshot.status ](index.html#pydantic_graph.persistence.NodeSnapshot.status) to 'success' or 'error' and [ NodeSnapshot.duration ](index.html#pydantic_graph.persistence.NodeSnapshot.duration) when the run finishes. Source code in pydantic_graph/pydantic_graph/persistence/__init__.py #### load\\_next abstractmethod async Retrieve a node snapshot with status 'created ' and set its status to 'pending' . This is used by [ Graph.iter_from_persistence ](../graph/index.html#pydantic_graph.graph.Graph.iter_from_persistence) to get the next node to run. Returns: The snapshot, or None if no snapshot with status 'created ' exists. Source code in pydantic_graph/pydantic_graph/persistence/__init__.py #### load\\_all abstractmethod async Load the entire history of snapshots. load_all is not used by pydantic-graph itself, instead it's provided to make it convenient to get all [snapshots](index.html#pydantic_graph.persistence.Snapshot) from persistence. Returns: The list of snapshots. Source code in pydantic_graph/pydantic_graph/persistence/__init__.py #### set\\_graph\\_types Set the types of the state and run end from a graph. You generally won't need to customise this method, instead implement [ set_types ](index.html#pydantic_graph.persistence.BaseStatePersistence.set_types) and [ should_set_types ](index.html#pydantic_graph.persistence.BaseStatePersistence.should_set_types). Source code in pydantic_graph/pydantic_graph/persistence/__init__.py #### should\\_set\\_types Whether types need to be set. Implementations should override this method to return True when types have not been set if they are needed. Source code in pydantic_graph/pydantic_graph/persistence/__init__.py #### set\\_types Set the types of the state and run end. This can be used to create [type adapters](https://docs.pydantic.dev/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter) for serializing and deserializing snapshots, e.g. with [ build_snapshot_list_type_adapter ](index.html#pydantic_graph.persistence.build_snapshot_list_type_adapter). Parameters: Source code in pydantic_graph/pydantic_graph/persistence/__init__.py ### build\\_snapshot\\_list\\_type\\_adapter Build a type adapter for a list of snapshots. This method should be called from within [ set_types ](index.html#pydantic_graph.persistence.BaseStatePersistence.set_types) where context variables will be set such that Pydantic can create a schema for [ NodeSnapshot.node ](index.html#pydantic_graph.persistence.NodeSnapshot.node). Source code in pydantic_graph/pydantic_graph/persistence/__init__.py In memory state persistence. This module provides simple in memory state persistence for graphs. ### SimpleStatePersistence dataclass Bases: BaseStatePersistence[StateT, RunEndT] Simple in memory state persistence that just hold the latest snapshot. If no state persistence implementation is provided when running a graph, this is used by default. Source code in pydantic_graph/pydantic_graph/persistence/in_mem.py #### last\\_snapshot class-attribute instance-attribute The last snapshot. ### FullStatePersistence dataclass Bases: BaseStatePersistence[StateT, RunEndT] In memory state persistence that hold a list of snapshots. Source code in pydantic_graph/pydantic_graph/persistence/in_mem.py #### deep\\_copy class-attribute instance-attribute Whether to deep copy the state and nodes when storing them. Defaults to True so even if nodes or state are modified after the snapshot is taken, the persistence history will record the value at the time of the snapshot. #### history class-attribute instance-attribute List of snapshots taken during the graph run. #### dump\\_json Dump the history to JSON bytes. Source code in pydantic_graph/pydantic_graph/persistence/in_mem.py #### load\\_json Load the history from JSON. Source code in pydantic_graph/pydantic_graph/persistence/in_mem.py ### FileStatePersistence dataclass Bases: BaseStatePersistence[StateT, RunEndT] File based state persistence that hold graph run state in a JSON file. Source code in pydantic_graph/pydantic_graph/persistence/file.py #### json\\_file instance-attribute Path to the JSON file where the snapshots are stored. You should use a different file for each graph run, but a single file should be reused for multiple steps of the same run. For example if you have a run ID of the form run_123abc , you might create a FileStatePersistence thus: #### should\\_set\\_types Whether types need to be set. Source code in pydantic_graph/pydantic_graph/persistence/file.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/persistence/index.html#pydantic_graphpersistence", "page": "api/pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "SnapshotStatus module-attribute", "anchor": "pydantic_graph.persistence.SnapshotStatus", "heading_level": 3, "md_text": "The status of a snapshot. * 'created' : The snapshot has been created but not yet run. * 'pending' : The snapshot has been retrieved with [ load_next ](index.html#pydantic_graph.persistence.BaseStatePersistence.load_next) but not yet run. * 'running' : The snapshot is currently running. * 'success' : The snapshot has been run successfully. * 'error' : The snapshot has been run but an error occurred.", "url": "https://ai.pydantic.dev/api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.SnapshotStatus", "page": "api/pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "NodeSnapshot dataclass", "anchor": "pydantic_graph.persistence.NodeSnapshot", "heading_level": 3, "md_text": "Bases: Generic[StateT, RunEndT] History step describing the execution of a node in a graph. Source code in pydantic_graph/pydantic_graph/persistence/__init__.py #### state instance-attribute The state of the graph before the node is run. #### node instance-attribute The node to run next. #### start\\_ts class-attribute instance-attribute The timestamp when the node started running, None until the run starts. #### duration class-attribute instance-attribute The duration of the node run in seconds, if the node has been run. #### status class-attribute instance-attribute The status of the snapshot. #### kind class-attribute instance-attribute The kind of history step, can be used as a discriminator when deserializing history. #### id class-attribute instance-attribute Unique ID of the snapshot.", "url": "https://ai.pydantic.dev/api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.NodeSnapshot", "page": "api/pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "EndSnapshot dataclass", "anchor": "pydantic_graph.persistence.EndSnapshot", "heading_level": 3, "md_text": "Bases: Generic[StateT, RunEndT] History step describing the end of a graph run. Source code in pydantic_graph/pydantic_graph/persistence/__init__.py #### state instance-attribute The state of the graph at the end of the run. #### result instance-attribute The result of the graph run. #### ts class-attribute instance-attribute The timestamp when the graph run ended. #### kind class-attribute instance-attribute The kind of history step, can be used as a discriminator when deserializing history. #### id class-attribute instance-attribute Unique ID of the snapshot. #### node property Shim to get the [ result ](index.html#pydantic_graph.persistence.EndSnapshot.result). Useful to allow [snapshot.node for snapshot in persistence.history] .", "url": "https://ai.pydantic.dev/api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.EndSnapshot", "page": "api/pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "Snapshot module-attribute", "anchor": "pydantic_graph.persistence.Snapshot", "heading_level": 3, "md_text": "A step in the history of a graph run. [ Graph.run ](../graph/index.html#pydantic_graph.graph.Graph.run) returns a list of these steps describing the execution of the graph, together with the run return value.", "url": "https://ai.pydantic.dev/api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.Snapshot", "page": "api/pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "BaseStatePersistence", "anchor": "pydantic_graph.persistence.BaseStatePersistence", "heading_level": 3, "md_text": "Bases: ABC , Generic[StateT, RunEndT] Abstract base class for storing the state of a graph run. Each instance of a BaseStatePersistence subclass should be used for a single graph run. Source code in pydantic_graph/pydantic_graph/persistence/__init__.py #### snapshot\\_node abstractmethod async Snapshot the state of a graph, when the next step is to run a node. This method should add a [ NodeSnapshot ](index.html#pydantic_graph.persistence.NodeSnapshot) to persistence. Parameters: Source code in pydantic_graph/pydantic_graph/persistence/__init__.py #### snapshot\\_node\\_if\\_new abstractmethod async Snapshot the state of a graph if the snapshot ID doesn't already exist in persistence. This method will generally call [ snapshot_node ](index.html#pydantic_graph.persistence.BaseStatePersistence.snapshot_node) but should do so in an atomic way. Parameters: Source code in pydantic_graph/pydantic_graph/persistence/__init__.py #### snapshot\\_end abstractmethod async Snapshot the state of a graph when the graph has ended. This method should add an [ EndSnapshot ](index.html#pydantic_graph.persistence.EndSnapshot) to persistence. Parameters: Source code in pydantic_graph/pydantic_graph/persistence/__init__.py #### record\\_run abstractmethod Record the run of the node, or error if the node is already running. Parameters: Raises: Returns: In particular this should set: * [ NodeSnapshot.status ](index.html#pydantic_graph.persistence.NodeSnapshot.status) to 'running' and [ NodeSnapshot.start_ts ](index.html#pydantic_graph.persistence.NodeSnapshot.start_ts) when the run starts. * [ NodeSnapshot.status ](index.html#pydantic_graph.persistence.NodeSnapshot.status) to 'success' or 'error' and [ NodeSnapshot.duration ](index.html#pydantic_graph.persistence.NodeSnapshot.duration) when the run finishes. Source code in pydantic_graph/pydantic_graph/persistence/__init__.py #### load\\_next abstractmethod async Retrieve a node snapshot with status 'created ' and set its status to 'pending' . This is used by [ Graph.iter_from_persistence ](../graph/index.html#pydantic_graph.graph.Graph.iter_from_persistence) to get the next node to run. Returns: The snapshot, or None if no snapshot with status 'created ' exists. Source code in pydantic_graph/pydantic_graph/persistence/__init__.py #### load\\_all abstractmethod async Load the entire history of snapshots. load_all is not used by pydantic-graph itself, instead it's provided to make it convenient to get all [snapshots](index.html#pydantic_graph.persistence.Snapshot) from persistence. Returns: The list of snapshots. Source code in pydantic_graph/pydantic_graph/persistence/__init__.py #### set\\_graph\\_types Set the types of the state and run end from a graph. You generally won't need to customise this method, instead implement [ set_types ](index.html#pydantic_graph.persistence.BaseStatePersistence.set_types) and [ should_set_types ](index.html#pydantic_graph.persistence.BaseStatePersistence.should_set_types). Source code in pydantic_graph/pydantic_graph/persistence/__init__.py #### should\\_set\\_types Whether types need to be set. Implementations should override this method to return True when types have not been set if they are needed. Source code in pydantic_graph/pydantic_graph/persistence/__init__.py #### set\\_types Set the types of the state and run end. This can be used to create [type adapters](https://docs.pydantic.dev/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter) for serializing and deserializing snapshots, e.g. with [ build_snapshot_list_type_adapter ](index.html#pydantic_graph.persistence.build_snapshot_list_type_adapter). Parameters: Source code in pydantic_graph/pydantic_graph/persistence/__init__.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.BaseStatePersistence", "page": "api/pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "build_snapshot_list_type_adapter", "anchor": "pydantic_graph.persistence.build_snapshot_list_type_adapter", "heading_level": 3, "md_text": "Build a type adapter for a list of snapshots. This method should be called from within [ set_types ](index.html#pydantic_graph.persistence.BaseStatePersistence.set_types) where context variables will be set such that Pydantic can create a schema for [ NodeSnapshot.node ](index.html#pydantic_graph.persistence.NodeSnapshot.node). Source code in pydantic_graph/pydantic_graph/persistence/__init__.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.build_snapshot_list_type_adapter", "page": "api/pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "SimpleStatePersistence dataclass", "anchor": "pydantic_graph.persistence.in_mem.SimpleStatePersistence", "heading_level": 3, "md_text": "Bases: BaseStatePersistence[StateT, RunEndT] Simple in memory state persistence that just hold the latest snapshot. If no state persistence implementation is provided when running a graph, this is used by default. Source code in pydantic_graph/pydantic_graph/persistence/in_mem.py #### last\\_snapshot class-attribute instance-attribute The last snapshot.", "url": "https://ai.pydantic.dev/api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.in_mem.SimpleStatePersistence", "page": "api/pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "FullStatePersistence dataclass", "anchor": "pydantic_graph.persistence.in_mem.FullStatePersistence", "heading_level": 3, "md_text": "Bases: BaseStatePersistence[StateT, RunEndT] In memory state persistence that hold a list of snapshots. Source code in pydantic_graph/pydantic_graph/persistence/in_mem.py #### deep\\_copy class-attribute instance-attribute Whether to deep copy the state and nodes when storing them. Defaults to True so even if nodes or state are modified after the snapshot is taken, the persistence history will record the value at the time of the snapshot. #### history class-attribute instance-attribute List of snapshots taken during the graph run. #### dump\\_json Dump the history to JSON bytes. Source code in pydantic_graph/pydantic_graph/persistence/in_mem.py #### load\\_json Load the history from JSON. Source code in pydantic_graph/pydantic_graph/persistence/in_mem.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.in_mem.FullStatePersistence", "page": "api/pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "FileStatePersistence dataclass", "anchor": "pydantic_graph.persistence.file.FileStatePersistence", "heading_level": 3, "md_text": "Bases: BaseStatePersistence[StateT, RunEndT] File based state persistence that hold graph run state in a JSON file. Source code in pydantic_graph/pydantic_graph/persistence/file.py #### json\\_file instance-attribute Path to the JSON file where the snapshots are stored. You should use a different file for each graph run, but a single file should be reused for multiple steps of the same run. For example if you have a run ID of the form run_123abc , you might create a FileStatePersistence thus: #### should\\_set\\_types Whether types need to be set. Source code in pydantic_graph/pydantic_graph/persistence/file.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.file.FileStatePersistence", "page": "api/pydantic_graph/persistence/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.run", "anchor": "pydantic_airun", "heading_level": 1, "md_text": "### AgentRun dataclass Bases: Generic[AgentDepsT, OutputDataT] A stateful, async-iterable run of an [ Agent ](../agent/index.html#pydantic_ai.agent.Agent). You generally obtain an AgentRun instance by calling async with my_agent.iter(...) as agent_run: . Once you have an instance, you can use it to iterate through the run's nodes as they execute. When an [ End ](../pydantic_graph/nodes/index.html#pydantic_graph.nodes.End) is reached, the run finishes and [ result ](../agent/index.html#pydantic_ai.agent.AgentRun.result) becomes available. Example: You can also manually drive the iteration using the [ next ](../agent/index.html#pydantic_ai.agent.AgentRun.next) method for more granular control. Source code in pydantic_ai_slim/pydantic_ai/run.py #### ctx property The current context of the agent run. #### next\\_node property The next node that will be run in the agent graph. This is the next node that will be used during async iteration, or if a node is not passed to self.next(...) . #### result property The final result of the run if it has ended, otherwise None . Once the run returns an [ End ](../pydantic_graph/nodes/index.html#pydantic_graph.nodes.End) node, result is populated with an [ AgentRunResult ](../agent/index.html#pydantic_ai.agent.AgentRunResult). #### \\_\\_aiter\\_\\_ Provide async-iteration over the nodes in the agent run. Source code in pydantic_ai_slim/pydantic_ai/run.py #### \\_\\_anext\\_\\_ async Advance to the next node automatically based on the last returned node. Source code in pydantic_ai_slim/pydantic_ai/run.py #### next async Manually drive the agent run by passing in the node you want to run next. This lets you inspect or mutate the node before continuing execution, or skip certain nodes under dynamic conditions. The agent run should be stopped when you return an [ End ](../pydantic_graph/nodes/index.html#pydantic_graph.nodes.End) node. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/run.py #### usage Get usage statistics for the run so far, including token usage, model requests, and so on. Source code in pydantic_ai_slim/pydantic_ai/run.py ### AgentRunResult dataclass Bases: Generic[OutputDataT] The final result of an agent run. Source code in pydantic_ai_slim/pydantic_ai/run.py #### output instance-attribute The output data from the agent run. #### all\\_messages Return the history of \\_messages. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/run.py #### all\\_messages\\_json Return all messages from [ all_messages ](../agent/index.html#pydantic_ai.agent.AgentRunResult.all_messages) as JSON bytes. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/run.py #### new\\_messages Return new messages associated with this run. Messages from older runs are excluded. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/run.py #### new\\_messages\\_json Return new messages from [ new_messages ](../agent/index.html#pydantic_ai.agent.AgentRunResult.new_messages) as JSON bytes. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/run.py #### response property Return the last response from the message history. #### usage Return the usage of the whole run. Source code in pydantic_ai_slim/pydantic_ai/run.py #### timestamp Return the timestamp of last response. Source code in pydantic_ai_slim/pydantic_ai/run.py ### AgentRunResultEvent dataclass Bases: Generic[OutputDataT] An event indicating the agent run ended and containing the final result of the agent run. Source code in pydantic_ai_slim/pydantic_ai/run.py #### result instance-attribute The result of the run. #### event\\_kind class-attribute instance-attribute Event type identifier, used as a discriminator.", "url": "https://ai.pydantic.dev/api/run/index.html#pydantic_airun", "page": "api/run/index.html", "source_site": "pydantic_ai"}
{"title": "AgentRun dataclass", "anchor": "pydantic_ai.run.AgentRun", "heading_level": 3, "md_text": "Bases: Generic[AgentDepsT, OutputDataT] A stateful, async-iterable run of an [ Agent ](../agent/index.html#pydantic_ai.agent.Agent). You generally obtain an AgentRun instance by calling async with my_agent.iter(...) as agent_run: . Once you have an instance, you can use it to iterate through the run's nodes as they execute. When an [ End ](../pydantic_graph/nodes/index.html#pydantic_graph.nodes.End) is reached, the run finishes and [ result ](../agent/index.html#pydantic_ai.agent.AgentRun.result) becomes available. Example: You can also manually drive the iteration using the [ next ](../agent/index.html#pydantic_ai.agent.AgentRun.next) method for more granular control. Source code in pydantic_ai_slim/pydantic_ai/run.py #### ctx property The current context of the agent run. #### next\\_node property The next node that will be run in the agent graph. This is the next node that will be used during async iteration, or if a node is not passed to self.next(...) . #### result property The final result of the run if it has ended, otherwise None . Once the run returns an [ End ](../pydantic_graph/nodes/index.html#pydantic_graph.nodes.End) node, result is populated with an [ AgentRunResult ](../agent/index.html#pydantic_ai.agent.AgentRunResult). #### \\_\\_aiter\\_\\_ Provide async-iteration over the nodes in the agent run. Source code in pydantic_ai_slim/pydantic_ai/run.py #### \\_\\_anext\\_\\_ async Advance to the next node automatically based on the last returned node. Source code in pydantic_ai_slim/pydantic_ai/run.py #### next async Manually drive the agent run by passing in the node you want to run next. This lets you inspect or mutate the node before continuing execution, or skip certain nodes under dynamic conditions. The agent run should be stopped when you return an [ End ](../pydantic_graph/nodes/index.html#pydantic_graph.nodes.End) node. Example: Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/run.py #### usage Get usage statistics for the run so far, including token usage, model requests, and so on. Source code in pydantic_ai_slim/pydantic_ai/run.py", "url": "https://ai.pydantic.dev/api/run/index.html#pydantic_ai.run.AgentRun", "page": "api/run/index.html", "source_site": "pydantic_ai"}
{"title": "AgentRunResult dataclass", "anchor": "pydantic_ai.run.AgentRunResult", "heading_level": 3, "md_text": "Bases: Generic[OutputDataT] The final result of an agent run. Source code in pydantic_ai_slim/pydantic_ai/run.py #### output instance-attribute The output data from the agent run. #### all\\_messages Return the history of \\_messages. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/run.py #### all\\_messages\\_json Return all messages from [ all_messages ](../agent/index.html#pydantic_ai.agent.AgentRunResult.all_messages) as JSON bytes. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/run.py #### new\\_messages Return new messages associated with this run. Messages from older runs are excluded. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/run.py #### new\\_messages\\_json Return new messages from [ new_messages ](../agent/index.html#pydantic_ai.agent.AgentRunResult.new_messages) as JSON bytes. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/run.py #### response property Return the last response from the message history. #### usage Return the usage of the whole run. Source code in pydantic_ai_slim/pydantic_ai/run.py #### timestamp Return the timestamp of last response. Source code in pydantic_ai_slim/pydantic_ai/run.py", "url": "https://ai.pydantic.dev/api/run/index.html#pydantic_ai.run.AgentRunResult", "page": "api/run/index.html", "source_site": "pydantic_ai"}
{"title": "AgentRunResultEvent dataclass", "anchor": "pydantic_ai.run.AgentRunResultEvent", "heading_level": 3, "md_text": "Bases: Generic[OutputDataT] An event indicating the agent run ended and containing the final result of the agent run. Source code in pydantic_ai_slim/pydantic_ai/run.py #### result instance-attribute The result of the run. #### event\\_kind class-attribute instance-attribute Event type identifier, used as a discriminator.", "url": "https://ai.pydantic.dev/api/run/index.html#pydantic_ai.run.AgentRunResultEvent", "page": "api/run/index.html", "source_site": "pydantic_ai"}
{"title": "Upgrade Guide", "anchor": "upgrade-guide", "heading_level": 1, "md_text": "In September 2025, Pydantic AI reached V1, which means we're committed to API stability: we will not introduce changes that break your code until V2 (if we do, you can shout at us as it's definitely a mistake). Once we release V2, in April 2026 at the earliest, we'll continue to provide security fixes for V1 for another 6 months minimum, so you have time to upgrade your applications. ## Breaking Changes Here's a filtered list of the breaking changes for each version to help you upgrade Pydantic AI. ### v1.0.1 (2025-09-05) The following breaking change was accidentally left out of v1.0.0: * See [#2808](https://github.com/pydantic/pydantic-ai/pull/2808) - Remove Python evaluator from pydantic_evals for security reasons ### v1.0.0 (2025-09-04) * See [#2725](https://github.com/pydantic/pydantic-ai/pull/2725) - Drop support for Python 3.9 * See [#2738](https://github.com/pydantic/pydantic-ai/pull/2738) - Make many dataclasses require keyword arguments * See [#2715](https://github.com/pydantic/pydantic-ai/pull/2715) - Remove cases and averages attributes from pydantic_evals spans * See [#2798](https://github.com/pydantic/pydantic-ai/pull/2798) - Change ModelRequest.parts and ModelResponse.parts types from list to Sequence * See [#2726](https://github.com/pydantic/pydantic-ai/pull/2726) - Default InstrumentationSettings version to 2 * See [#2717](https://github.com/pydantic/pydantic-ai/pull/2717) - Remove errors when passing AsyncRetrying or Retrying object to AsyncTenacityTransport or TenacityTransport instead of RetryConfig ### v0.x.x Before V1, minor versions were used to introduce breaking changes: **v0.8.0 (2025-08-26)** See [#2689](https://github.com/pydantic/pydantic-ai/pull/2689) - AgentStreamEvent was expanded to be a union of ModelResponseStreamEvent and HandleResponseEvent , simplifying the event_stream_handler function signature. Existing code accepting AgentStreamEvent HandleResponseEvent will continue to work. **v0.7.6 (2025-08-26)** The following breaking change was inadvertently released in a patch version rather than a minor version: See [#2670](https://github.com/pydantic/pydantic-ai/pull/2670) - TenacityTransport and AsyncTenacityTransport now require the use of pydantic_ai.retries.RetryConfig (which is just a TypedDict containing the kwargs to tenacity.retry ) instead of tenacity.Retrying or tenacity.AsyncRetrying . **v0.7.0 (2025-08-12)** See [#2458](https://github.com/pydantic/pydantic-ai/pull/2458) - pydantic_ai.models.StreamedResponse now yields a FinalResultEvent along with the existing PartStartEvent and PartDeltaEvent . If you're using pydantic_ai.direct.model_request_stream or pydantic_ai.direct.model_request_stream_sync , you may need to update your code to account for this. See [#2458](https://github.com/pydantic/pydantic-ai/pull/2458) - pydantic_ai.models.Model.request_stream now receives a run_context argument. If you've implemented a custom Model subclass, you will need to account for this. See [#2458](https://github.com/pydantic/pydantic-ai/pull/2458) - pydantic_ai.models.StreamedResponse now requires a model_request_parameters field and constructor argument. If you've implemented a custom Model subclass and implemented request_stream , you will need to account for this. **v0.6.0 (2025-08-06)** This release was meant to clean some old deprecated code, so we can get a step closer to V1. See [#2440](https://github.com/pydantic/pydantic-ai/pull/2440) - The next method was removed from the Graph class. Use async with graph.iter(...) as run: run.next() instead. See [#2441](https://github.com/pydantic/pydantic-ai/pull/2441) - The result_type , result_tool_name and result_tool_description arguments were removed from the Agent class. Use output_type instead. See [#2441](https://github.com/pydantic/pydantic-ai/pull/2441) - The result_retries argument was also removed from the Agent class. Use output_retries instead. See [#2443](https://github.com/pydantic/pydantic-ai/pull/2443) - The data property was removed from the FinalResult class. Use output instead. See [#2445](https://github.com/pydantic/pydantic-ai/pull/2445) - The get_data and validate_structured_result methods were removed from the StreamedRunResult class. Use get_output and validate_structured_output instead. See [#2446](https://github.com/pydantic/pydantic-ai/pull/2446) - The format_as_xml function was moved to the pydantic_ai.format_as_xml module. Import it via from pydantic_ai import format_as_xml instead. See [#2451](https://github.com/pydantic/pydantic-ai/pull/2451) - Removed deprecated Agent.result_validator method, Agent.last_run_messages property, AgentRunResult.data property, and result_tool_return_content parameters from result classes. **v0.5.0 (2025-08-04)** See [#2388](https://github.com/pydantic/pydantic-ai/pull/2388) - The source field of an EvaluationResult is now of type EvaluatorSpec rather than the actual source Evaluator instance, to help with serialization/deserialization. See [#2163](https://github.com/pydantic/pydantic-ai/pull/2163) - The EvaluationReport.print and EvaluationReport.console_table methods now require most arguments be passed by keyword. **v0.4.0 (2025-07-08)** See [#1799](https://github.com/pydantic/pydantic-ai/pull/1799) - Pydantic Evals EvaluationReport and ReportCase are now generic dataclasses instead of Pydantic models. If you were serializing them using model_dump() , you will now need to use the EvaluationReportAdapter and ReportCaseAdapter type adapters instead. See [#1507](https://github.com/pydantic/pydantic-ai/pull/1507) - The ToolDefinition description argument is now optional and the order of positional arguments has changed from name, description, parameters_json_schema, ... to name, parameters_json_schema, description, ... to account for this. **v0.3.0 (2025-06-18)** See [#1142](https://github.com/pydantic/pydantic-ai/pull/1142)  Adds support for thinking parts. We now convert the thinking blocks ( \"<think>...\"</think>\" ) in provider specific text parts to Pydantic AI ThinkingPart s. Also, as part of this release, we made the choice to not send back the ThinkingPart s to the provider - the idea is to save costs on behalf of the user. In the future, we intend to add a setting to customize this behavior. **v0.2.0 (2025-05-12)** See [#1647](https://github.com/pydantic/pydantic-ai/pull/1647)  usage makes sense as part of ModelResponse , and could be really useful in \"messages\" (really a sequence of requests and response). In this PR: * Adds usage to ModelResponse (field has a default factory of Usage() so it'll work to load data that doesn't have usage) * changes the return type of Model.request to just ModelResponse instead of tuple[ModelResponse, Usage] **v0.1.0 (2025-04-15)** See [#1248](https://github.com/pydantic/pydantic-ai/pull/1248)  the attribute/parameter name result was renamed to output in many places. Hopefully all changes keep a deprecated attribute or parameter with the old name, so you should get many deprecation warnings. See [#1484](https://github.com/pydantic/pydantic-ai/pull/1484)  format_as_xml was moved and made available to import from the package root, e.g. from pydantic_ai import format_as_xml . ## Full Changelog For the full changelog, see [GitHub Releases](https://github.com/pydantic/pydantic-ai/releases).", "url": "https://ai.pydantic.dev/changelog/index.html#upgrade-guide", "page": "changelog/index.html", "source_site": "pydantic_ai"}
{"title": "Breaking Changes", "anchor": "breaking-changes", "heading_level": 2, "md_text": "Here's a filtered list of the breaking changes for each version to help you upgrade Pydantic AI. ### v1.0.1 (2025-09-05) The following breaking change was accidentally left out of v1.0.0: * See [#2808](https://github.com/pydantic/pydantic-ai/pull/2808) - Remove Python evaluator from pydantic_evals for security reasons ### v1.0.0 (2025-09-04) * See [#2725](https://github.com/pydantic/pydantic-ai/pull/2725) - Drop support for Python 3.9 * See [#2738](https://github.com/pydantic/pydantic-ai/pull/2738) - Make many dataclasses require keyword arguments * See [#2715](https://github.com/pydantic/pydantic-ai/pull/2715) - Remove cases and averages attributes from pydantic_evals spans * See [#2798](https://github.com/pydantic/pydantic-ai/pull/2798) - Change ModelRequest.parts and ModelResponse.parts types from list to Sequence * See [#2726](https://github.com/pydantic/pydantic-ai/pull/2726) - Default InstrumentationSettings version to 2 * See [#2717](https://github.com/pydantic/pydantic-ai/pull/2717) - Remove errors when passing AsyncRetrying or Retrying object to AsyncTenacityTransport or TenacityTransport instead of RetryConfig ### v0.x.x Before V1, minor versions were used to introduce breaking changes: **v0.8.0 (2025-08-26)** See [#2689](https://github.com/pydantic/pydantic-ai/pull/2689) - AgentStreamEvent was expanded to be a union of ModelResponseStreamEvent and HandleResponseEvent , simplifying the event_stream_handler function signature. Existing code accepting AgentStreamEvent HandleResponseEvent will continue to work. **v0.7.6 (2025-08-26)** The following breaking change was inadvertently released in a patch version rather than a minor version: See [#2670](https://github.com/pydantic/pydantic-ai/pull/2670) - TenacityTransport and AsyncTenacityTransport now require the use of pydantic_ai.retries.RetryConfig (which is just a TypedDict containing the kwargs to tenacity.retry ) instead of tenacity.Retrying or tenacity.AsyncRetrying . **v0.7.0 (2025-08-12)** See [#2458](https://github.com/pydantic/pydantic-ai/pull/2458) - pydantic_ai.models.StreamedResponse now yields a FinalResultEvent along with the existing PartStartEvent and PartDeltaEvent . If you're using pydantic_ai.direct.model_request_stream or pydantic_ai.direct.model_request_stream_sync , you may need to update your code to account for this. See [#2458](https://github.com/pydantic/pydantic-ai/pull/2458) - pydantic_ai.models.Model.request_stream now receives a run_context argument. If you've implemented a custom Model subclass, you will need to account for this. See [#2458](https://github.com/pydantic/pydantic-ai/pull/2458) - pydantic_ai.models.StreamedResponse now requires a model_request_parameters field and constructor argument. If you've implemented a custom Model subclass and implemented request_stream , you will need to account for this. **v0.6.0 (2025-08-06)** This release was meant to clean some old deprecated code, so we can get a step closer to V1. See [#2440](https://github.com/pydantic/pydantic-ai/pull/2440) - The next method was removed from the Graph class. Use async with graph.iter(...) as run: run.next() instead. See [#2441](https://github.com/pydantic/pydantic-ai/pull/2441) - The result_type , result_tool_name and result_tool_description arguments were removed from the Agent class. Use output_type instead. See [#2441](https://github.com/pydantic/pydantic-ai/pull/2441) - The result_retries argument was also removed from the Agent class. Use output_retries instead. See [#2443](https://github.com/pydantic/pydantic-ai/pull/2443) - The data property was removed from the FinalResult class. Use output instead. See [#2445](https://github.com/pydantic/pydantic-ai/pull/2445) - The get_data and validate_structured_result methods were removed from the StreamedRunResult class. Use get_output and validate_structured_output instead. See [#2446](https://github.com/pydantic/pydantic-ai/pull/2446) - The format_as_xml function was moved to the pydantic_ai.format_as_xml module. Import it via from pydantic_ai import format_as_xml instead. See [#2451](https://github.com/pydantic/pydantic-ai/pull/2451) - Removed deprecated Agent.result_validator method, Agent.last_run_messages property, AgentRunResult.data property, and result_tool_return_content parameters from result classes. **v0.5.0 (2025-08-04)** See [#2388](https://github.com/pydantic/pydantic-ai/pull/2388) - The source field of an EvaluationResult is now of type EvaluatorSpec rather than the actual source Evaluator instance, to help with serialization/deserialization. See [#2163](https://github.com/pydantic/pydantic-ai/pull/2163) - The EvaluationReport.print and EvaluationReport.console_table methods now require most arguments be passed by keyword. **v0.4.0 (2025-07-08)** See [#1799](https://github.com/pydantic/pydantic-ai/pull/1799) - Pydantic Evals EvaluationReport and ReportCase are now generic dataclasses instead of Pydantic models. If you were serializing them using model_dump() , you will now need to use the EvaluationReportAdapter and ReportCaseAdapter type adapters instead. See [#1507](https://github.com/pydantic/pydantic-ai/pull/1507) - The ToolDefinition description argument is now optional and the order of positional arguments has changed from name, description, parameters_json_schema, ... to name, parameters_json_schema, description, ... to account for this. **v0.3.0 (2025-06-18)** See [#1142](https://github.com/pydantic/pydantic-ai/pull/1142)  Adds support for thinking parts. We now convert the thinking blocks ( \"<think>...\"</think>\" ) in provider specific text parts to Pydantic AI ThinkingPart s. Also, as part of this release, we made the choice to not send back the ThinkingPart s to the provider - the idea is to save costs on behalf of the user. In the future, we intend to add a setting to customize this behavior. **v0.2.0 (2025-05-12)** See [#1647](https://github.com/pydantic/pydantic-ai/pull/1647)  usage makes sense as part of ModelResponse , and could be really useful in \"messages\" (really a sequence of requests and response). In this PR: * Adds usage to ModelResponse (field has a default factory of Usage() so it'll work to load data that doesn't have usage) * changes the return type of Model.request to just ModelResponse instead of tuple[ModelResponse, Usage] **v0.1.0 (2025-04-15)** See [#1248](https://github.com/pydantic/pydantic-ai/pull/1248)  the attribute/parameter name result was renamed to output in many places. Hopefully all changes keep a deprecated attribute or parameter with the old name, so you should get many deprecation warnings. See [#1484](https://github.com/pydantic/pydantic-ai/pull/1484)  format_as_xml was moved and made available to import from the package root, e.g. from pydantic_ai import format_as_xml .", "url": "https://ai.pydantic.dev/changelog/index.html#breaking-changes", "page": "changelog/index.html", "source_site": "pydantic_ai"}
{"title": "v1.0.1 (2025-09-05)", "anchor": "v101-2025-09-05", "heading_level": 3, "md_text": "The following breaking change was accidentally left out of v1.0.0: * See [#2808](https://github.com/pydantic/pydantic-ai/pull/2808) - Remove Python evaluator from pydantic_evals for security reasons", "url": "https://ai.pydantic.dev/changelog/index.html#v101-2025-09-05", "page": "changelog/index.html", "source_site": "pydantic_ai"}
{"title": "v1.0.0 (2025-09-04)", "anchor": "v100-2025-09-04", "heading_level": 3, "md_text": "* See [#2725](https://github.com/pydantic/pydantic-ai/pull/2725) - Drop support for Python 3.9 * See [#2738](https://github.com/pydantic/pydantic-ai/pull/2738) - Make many dataclasses require keyword arguments * See [#2715](https://github.com/pydantic/pydantic-ai/pull/2715) - Remove cases and averages attributes from pydantic_evals spans * See [#2798](https://github.com/pydantic/pydantic-ai/pull/2798) - Change ModelRequest.parts and ModelResponse.parts types from list to Sequence * See [#2726](https://github.com/pydantic/pydantic-ai/pull/2726) - Default InstrumentationSettings version to 2 * See [#2717](https://github.com/pydantic/pydantic-ai/pull/2717) - Remove errors when passing AsyncRetrying or Retrying object to AsyncTenacityTransport or TenacityTransport instead of RetryConfig", "url": "https://ai.pydantic.dev/changelog/index.html#v100-2025-09-04", "page": "changelog/index.html", "source_site": "pydantic_ai"}
{"title": "v0.x.x", "anchor": "v0xx", "heading_level": 3, "md_text": "Before V1, minor versions were used to introduce breaking changes: **v0.8.0 (2025-08-26)** See [#2689](https://github.com/pydantic/pydantic-ai/pull/2689) - AgentStreamEvent was expanded to be a union of ModelResponseStreamEvent and HandleResponseEvent , simplifying the event_stream_handler function signature. Existing code accepting AgentStreamEvent HandleResponseEvent will continue to work. **v0.7.6 (2025-08-26)** The following breaking change was inadvertently released in a patch version rather than a minor version: See [#2670](https://github.com/pydantic/pydantic-ai/pull/2670) - TenacityTransport and AsyncTenacityTransport now require the use of pydantic_ai.retries.RetryConfig (which is just a TypedDict containing the kwargs to tenacity.retry ) instead of tenacity.Retrying or tenacity.AsyncRetrying . **v0.7.0 (2025-08-12)** See [#2458](https://github.com/pydantic/pydantic-ai/pull/2458) - pydantic_ai.models.StreamedResponse now yields a FinalResultEvent along with the existing PartStartEvent and PartDeltaEvent . If you're using pydantic_ai.direct.model_request_stream or pydantic_ai.direct.model_request_stream_sync , you may need to update your code to account for this. See [#2458](https://github.com/pydantic/pydantic-ai/pull/2458) - pydantic_ai.models.Model.request_stream now receives a run_context argument. If you've implemented a custom Model subclass, you will need to account for this. See [#2458](https://github.com/pydantic/pydantic-ai/pull/2458) - pydantic_ai.models.StreamedResponse now requires a model_request_parameters field and constructor argument. If you've implemented a custom Model subclass and implemented request_stream , you will need to account for this. **v0.6.0 (2025-08-06)** This release was meant to clean some old deprecated code, so we can get a step closer to V1. See [#2440](https://github.com/pydantic/pydantic-ai/pull/2440) - The next method was removed from the Graph class. Use async with graph.iter(...) as run: run.next() instead. See [#2441](https://github.com/pydantic/pydantic-ai/pull/2441) - The result_type , result_tool_name and result_tool_description arguments were removed from the Agent class. Use output_type instead. See [#2441](https://github.com/pydantic/pydantic-ai/pull/2441) - The result_retries argument was also removed from the Agent class. Use output_retries instead. See [#2443](https://github.com/pydantic/pydantic-ai/pull/2443) - The data property was removed from the FinalResult class. Use output instead. See [#2445](https://github.com/pydantic/pydantic-ai/pull/2445) - The get_data and validate_structured_result methods were removed from the StreamedRunResult class. Use get_output and validate_structured_output instead. See [#2446](https://github.com/pydantic/pydantic-ai/pull/2446) - The format_as_xml function was moved to the pydantic_ai.format_as_xml module. Import it via from pydantic_ai import format_as_xml instead. See [#2451](https://github.com/pydantic/pydantic-ai/pull/2451) - Removed deprecated Agent.result_validator method, Agent.last_run_messages property, AgentRunResult.data property, and result_tool_return_content parameters from result classes. **v0.5.0 (2025-08-04)** See [#2388](https://github.com/pydantic/pydantic-ai/pull/2388) - The source field of an EvaluationResult is now of type EvaluatorSpec rather than the actual source Evaluator instance, to help with serialization/deserialization. See [#2163](https://github.com/pydantic/pydantic-ai/pull/2163) - The EvaluationReport.print and EvaluationReport.console_table methods now require most arguments be passed by keyword. **v0.4.0 (2025-07-08)** See [#1799](https://github.com/pydantic/pydantic-ai/pull/1799) - Pydantic Evals EvaluationReport and ReportCase are now generic dataclasses instead of Pydantic models. If you were serializing them using model_dump() , you will now need to use the EvaluationReportAdapter and ReportCaseAdapter type adapters instead. See [#1507](https://github.com/pydantic/pydantic-ai/pull/1507) - The ToolDefinition description argument is now optional and the order of positional arguments has changed from name, description, parameters_json_schema, ... to name, parameters_json_schema, description, ... to account for this. **v0.3.0 (2025-06-18)** See [#1142](https://github.com/pydantic/pydantic-ai/pull/1142)  Adds support for thinking parts. We now convert the thinking blocks ( \"<think>...\"</think>\" ) in provider specific text parts to Pydantic AI ThinkingPart s. Also, as part of this release, we made the choice to not send back the ThinkingPart s to the provider - the idea is to save costs on behalf of the user. In the future, we intend to add a setting to customize this behavior. **v0.2.0 (2025-05-12)** See [#1647](https://github.com/pydantic/pydantic-ai/pull/1647)  usage makes sense as part of ModelResponse , and could be really useful in \"messages\" (really a sequence of requests and response). In this PR: * Adds usage to ModelResponse (field has a default factory of Usage() so it'll work to load data that doesn't have usage) * changes the return type of Model.request to just ModelResponse instead of tuple[ModelResponse, Usage] **v0.1.0 (2025-04-15)** See [#1248](https://github.com/pydantic/pydantic-ai/pull/1248)  the attribute/parameter name result was renamed to output in many places. Hopefully all changes keep a deprecated attribute or parameter with the old name, so you should get many deprecation warnings. See [#1484](https://github.com/pydantic/pydantic-ai/pull/1484)  format_as_xml was moved and made available to import from the package root, e.g. from pydantic_ai import format_as_xml .", "url": "https://ai.pydantic.dev/changelog/index.html#v0xx", "page": "changelog/index.html", "source_site": "pydantic_ai"}
{"title": "Full Changelog", "anchor": "full-changelog", "heading_level": 2, "md_text": "For the full changelog, see [GitHub Releases](https://github.com/pydantic/pydantic-ai/releases).", "url": "https://ai.pydantic.dev/changelog/index.html#full-changelog", "page": "changelog/index.html", "source_site": "pydantic_ai"}
{"title": "Command Line Interface (CLI)", "anchor": "command-line-interface-cli", "heading_level": 1, "md_text": "**Pydantic AI** comes with a CLI, clai (pronounced \"clay\") which you can use to interact with various LLMs from the command line. It provides a convenient way to chat with language models and quickly get answers right in the terminal. We originally developed this CLI for our own use, but found ourselves using it so frequently that we decided to share it as part of the Pydantic AI package. We plan to continue adding new features, such as interaction with MCP servers, access to tools, and more. ## Usage You'll need to set an environment variable depending on the provider you intend to use. E.g. if you're using OpenAI, set the OPENAI_API_KEY environment variable: Then with [ uvx ](https://docs.astral.sh/uv/guides/tools/), run: Or to install clai globally [with uv ](https://docs.astral.sh/uv/guides/tools/#installing-tools), run: Or with pip , run: Either way, running clai will start an interactive session where you can chat with the AI model. Special commands available in interactive mode: * /exit : Exit the session * /markdown : Show the last response in markdown format * /multiline : Toggle multiline input mode (use Ctrl+D to submit) * /cp : Copy the last response to clipboard ### Help To get help on the CLI, use the --help flag: ### Choose a model You can specify which model to use with the --model flag: (a full list of models available can be printed with uvx clai --list-models ) ### Custom Agents You can specify a custom agent using the --agent flag with a module path and variable name: custom\\_agent.py Then run: The format must be module:variable where: * module is the importable Python module path * variable is the name of the Agent instance in that module Additionally, you can directly launch CLI mode from an Agent instance using Agent.to_cli_sync() : agent\\_to\\_cli\\_sync.py You can also use the async interface with Agent.to_cli() : agent\\_to\\_cli.py *(You'll need to add asyncio.run(main()) to run main )* ### Message History Both Agent.to_cli() and Agent.to_cli_sync() support a message_history parameter, allowing you to continue an existing conversation or provide conversation context: agent\\_with\\_history.py The CLI will start with the provided conversation history, allowing the agent to refer back to previous exchanges and maintain context throughout the session.", "url": "https://ai.pydantic.dev/cli/index.html#command-line-interface-cli", "page": "cli/index.html", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 2, "md_text": "You'll need to set an environment variable depending on the provider you intend to use. E.g. if you're using OpenAI, set the OPENAI_API_KEY environment variable: Then with [ uvx ](https://docs.astral.sh/uv/guides/tools/), run: Or to install clai globally [with uv ](https://docs.astral.sh/uv/guides/tools/#installing-tools), run: Or with pip , run: Either way, running clai will start an interactive session where you can chat with the AI model. Special commands available in interactive mode: * /exit : Exit the session * /markdown : Show the last response in markdown format * /multiline : Toggle multiline input mode (use Ctrl+D to submit) * /cp : Copy the last response to clipboard ### Help To get help on the CLI, use the --help flag: ### Choose a model You can specify which model to use with the --model flag: (a full list of models available can be printed with uvx clai --list-models ) ### Custom Agents You can specify a custom agent using the --agent flag with a module path and variable name: custom\\_agent.py Then run: The format must be module:variable where: * module is the importable Python module path * variable is the name of the Agent instance in that module Additionally, you can directly launch CLI mode from an Agent instance using Agent.to_cli_sync() : agent\\_to\\_cli\\_sync.py You can also use the async interface with Agent.to_cli() : agent\\_to\\_cli.py *(You'll need to add asyncio.run(main()) to run main )* ### Message History Both Agent.to_cli() and Agent.to_cli_sync() support a message_history parameter, allowing you to continue an existing conversation or provide conversation context: agent\\_with\\_history.py The CLI will start with the provided conversation history, allowing the agent to refer back to previous exchanges and maintain context throughout the session.", "url": "https://ai.pydantic.dev/cli/index.html#usage", "page": "cli/index.html", "source_site": "pydantic_ai"}
{"title": "Help", "anchor": "help", "heading_level": 3, "md_text": "To get help on the CLI, use the --help flag:", "url": "https://ai.pydantic.dev/cli/index.html#help", "page": "cli/index.html", "source_site": "pydantic_ai"}
{"title": "Choose a model", "anchor": "choose-a-model", "heading_level": 3, "md_text": "You can specify which model to use with the --model flag: (a full list of models available can be printed with uvx clai --list-models )", "url": "https://ai.pydantic.dev/cli/index.html#choose-a-model", "page": "cli/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Agents", "anchor": "custom-agents", "heading_level": 3, "md_text": "You can specify a custom agent using the --agent flag with a module path and variable name: custom\\_agent.py Then run: The format must be module:variable where: * module is the importable Python module path * variable is the name of the Agent instance in that module Additionally, you can directly launch CLI mode from an Agent instance using Agent.to_cli_sync() : agent\\_to\\_cli\\_sync.py You can also use the async interface with Agent.to_cli() : agent\\_to\\_cli.py *(You'll need to add asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/cli/index.html#custom-agents", "page": "cli/index.html", "source_site": "pydantic_ai"}
{"title": "Message History", "anchor": "message-history", "heading_level": 3, "md_text": "Both Agent.to_cli() and Agent.to_cli_sync() support a message_history parameter, allowing you to continue an existing conversation or provide conversation context: agent\\_with\\_history.py The CLI will start with the provided conversation history, allowing the agent to refer back to previous exchanges and maintain context throughout the session.", "url": "https://ai.pydantic.dev/cli/index.html#message-history", "page": "cli/index.html", "source_site": "pydantic_ai"}
{"title": "Common Tools", "anchor": "common-tools", "heading_level": 1, "md_text": "Pydantic AI ships with native tools that can be used to enhance your agent's capabilities. ## DuckDuckGo Search Tool The DuckDuckGo search tool allows you to search the web for information. It is built on top of the [DuckDuckGo API](https://github.com/deedy5/ddgs). ### Installation To use [ duckduckgo_search_tool ](../api/common_tools/index.html#pydantic_ai.common_tools.duckduckgo.duckduckgo_search_tool), you need to install [ pydantic-ai-slim ](../install/index.html#slim-install) with the duckduckgo optional group: pipuv ### Usage Here's an example of how you can use the DuckDuckGo search tool with an agent: duckduckgo\\_search.py ## Tavily Search Tool Info Tavily is a paid service, but they have free credits to explore their product. You need to [sign up for an account](https://app.tavily.com/home) and get an API key to use the Tavily search tool. The Tavily search tool allows you to search the web for information. It is built on top of the [Tavily API](https://tavily.com/). ### Installation To use [ tavily_search_tool ](../api/common_tools/index.html#pydantic_ai.common_tools.tavily.tavily_search_tool), you need to install [ pydantic-ai-slim ](../install/index.html#slim-install) with the tavily optional group: pipuv ### Usage Here's an example of how you can use the Tavily search tool with an agent: tavily\\_search.py", "url": "https://ai.pydantic.dev/common-tools/index.html#common-tools", "page": "common-tools/index.html", "source_site": "pydantic_ai"}
{"title": "DuckDuckGo Search Tool", "anchor": "duckduckgo-search-tool", "heading_level": 2, "md_text": "The DuckDuckGo search tool allows you to search the web for information. It is built on top of the [DuckDuckGo API](https://github.com/deedy5/ddgs). ### Installation To use [ duckduckgo_search_tool ](../api/common_tools/index.html#pydantic_ai.common_tools.duckduckgo.duckduckgo_search_tool), you need to install [ pydantic-ai-slim ](../install/index.html#slim-install) with the duckduckgo optional group: pipuv ### Usage Here's an example of how you can use the DuckDuckGo search tool with an agent: duckduckgo\\_search.py", "url": "https://ai.pydantic.dev/common-tools/index.html#duckduckgo-search-tool", "page": "common-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Installation", "anchor": "installation", "heading_level": 3, "md_text": "To use [ duckduckgo_search_tool ](../api/common_tools/index.html#pydantic_ai.common_tools.duckduckgo.duckduckgo_search_tool), you need to install [ pydantic-ai-slim ](../install/index.html#slim-install) with the duckduckgo optional group: pipuv", "url": "https://ai.pydantic.dev/common-tools/index.html#installation", "page": "common-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 3, "md_text": "Here's an example of how you can use the DuckDuckGo search tool with an agent: duckduckgo\\_search.py", "url": "https://ai.pydantic.dev/common-tools/index.html#usage", "page": "common-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Tavily Search Tool", "anchor": "tavily-search-tool", "heading_level": 2, "md_text": "Info Tavily is a paid service, but they have free credits to explore their product. You need to [sign up for an account](https://app.tavily.com/home) and get an API key to use the Tavily search tool. The Tavily search tool allows you to search the web for information. It is built on top of the [Tavily API](https://tavily.com/). ### Installation To use [ tavily_search_tool ](../api/common_tools/index.html#pydantic_ai.common_tools.tavily.tavily_search_tool), you need to install [ pydantic-ai-slim ](../install/index.html#slim-install) with the tavily optional group: pipuv ### Usage Here's an example of how you can use the Tavily search tool with an agent: tavily\\_search.py", "url": "https://ai.pydantic.dev/common-tools/index.html#tavily-search-tool", "page": "common-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Installation", "anchor": "installation_1", "heading_level": 3, "md_text": "To use [ tavily_search_tool ](../api/common_tools/index.html#pydantic_ai.common_tools.tavily.tavily_search_tool), you need to install [ pydantic-ai-slim ](../install/index.html#slim-install) with the tavily optional group: pipuv", "url": "https://ai.pydantic.dev/common-tools/index.html#installation_1", "page": "common-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage_1", "heading_level": 3, "md_text": "Here's an example of how you can use the Tavily search tool with an agent: tavily\\_search.py", "url": "https://ai.pydantic.dev/common-tools/index.html#usage_1", "page": "common-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Installation and Setup", "anchor": "installation-and-setup", "heading_level": 2, "md_text": "Clone your fork and cd into the repo directory Install uv (version 0.4.30 or later), pre-commit and deno : * [ uv install docs](https://docs.astral.sh/uv/getting-started/installation/) * [ pre-commit install docs](https://pre-commit.com/#install) * [ deno install docs](https://docs.deno.com/runtime/getting_started/installation/) To install pre-commit you can run the following command: For deno , you can run the following, or check [their documentation](https://docs.deno.com/runtime/getting_started/installation/) for alternative installation methods: Install pydantic-ai , all dependencies and pre-commit hooks", "url": "https://ai.pydantic.dev/contributing/index.html#installation-and-setup", "page": "contributing/index.html", "source_site": "pydantic_ai"}
{"title": "Running Tests etc.", "anchor": "running-tests-etc", "heading_level": 2, "md_text": "We use make to manage most commands you'll need to run. For details on available commands, run: To run code formatting, linting, static type checks, and tests with coverage report generation, run:", "url": "https://ai.pydantic.dev/contributing/index.html#running-tests-etc", "page": "contributing/index.html", "source_site": "pydantic_ai"}
{"title": "Documentation Changes", "anchor": "documentation-changes", "heading_level": 2, "md_text": "To run the documentation page locally, run:", "url": "https://ai.pydantic.dev/contributing/index.html#documentation-changes", "page": "contributing/index.html", "source_site": "pydantic_ai"}
{"title": "Rules for adding new models to Pydantic AI", "anchor": "new-model-rules", "heading_level": 2, "md_text": "To avoid an excessive workload for the maintainers of Pydantic AI, we can't accept all model contributions, so we're setting the following rules for when we'll accept new models and when we won't. This should hopefully reduce the chances of disappointment and wasted work. * To add a new model with an extra dependency, that dependency needs > 500k monthly downloads from PyPI consistently over 3 months or more * To add a new model which uses another models logic internally and has no extra dependencies, that model's GitHub org needs > 20k stars in total * For any other model that's just a custom URL and API key, we're happy to add a one-paragraph description with a link and instructions on the URL to use * For any other model that requires more logic, we recommend you release your own Python package pydantic-ai-xxx , which depends on [ pydantic-ai-slim ](../install/index.html#slim-install) and implements a model that inherits from our [ Model ](../api/models/base/index.html#pydantic_ai.models.Model) ABC If you're unsure about adding a model, please [create an issue](https://github.com/pydantic/pydantic-ai/issues).", "url": "https://ai.pydantic.dev/contributing/index.html#new-model-rules", "page": "contributing/index.html", "source_site": "pydantic_ai"}
{"title": "Built-in Tools", "anchor": "built-in-tools", "heading_level": 1, "md_text": "Built-in tools are native tools provided by LLM providers that can be used to enhance your agent's capabilities. Unlike [common tools](../common-tools/index.html), which are custom implementations that Pydantic AI executes, built-in tools are executed directly by the model provider. ## Overview Pydantic AI supports the following built-in tools: * **[ WebSearchTool ](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.WebSearchTool)**: Allows agents to search the web * **[ CodeExecutionTool ](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.CodeExecutionTool)**: Enables agents to execute code in a secure environment * **[ ImageGenerationTool ](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.ImageGenerationTool)**: Enables agents to generate images * **[ UrlContextTool ](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.UrlContextTool)**: Enables agents to pull URL contents into their context * **[ MemoryTool ](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.MemoryTool)**: Enables agents to use memory * **[ MCPServerTool ](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.MCPServerTool)**: Enables agents to use remote MCP servers with communication handled by the model provider These tools are passed to the agent via the builtin_tools parameter and are executed by the model provider's infrastructure. Provider Support Not all model providers support built-in tools. If you use a built-in tool with an unsupported provider, Pydantic AI will raise a [ UserError ](../api/exceptions/index.html#pydantic_ai.exceptions.UserError) when you try to run the agent. If a provider supports a built-in tool that is not currently supported by Pydantic AI, please file an issue. ## Web Search Tool The [ WebSearchTool ](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.WebSearchTool) allows your agent to search the web, making it ideal for queries that require up-to-date data. ### Provider Support ### Usage web\\_search\\_anthropic.py *(This example is complete, it can be run \"as is\")* With OpenAI, you must use their Responses API to access the web search tool. web\\_search\\_openai.py *(This example is complete, it can be run \"as is\")* ### Configuration Options The WebSearchTool supports several configuration parameters: web\\_search\\_configured.py *(This example is complete, it can be run \"as is\")* #### Provider Support Anthropic Domain Filtering With Anthropic, you can only use either blocked_domains or allowed_domains , not both. ## Code Execution Tool The [ CodeExecutionTool ](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.CodeExecutionTool) enables your agent to execute code in a secure environment, making it perfect for computational tasks, data analysis, and mathematical operations. ### Provider Support ### Usage code\\_execution\\_basic.py *(This example is complete, it can be run \"as is\")* In addition to text output, code execution with OpenAI can generate images as part of their response. Accessing this image via [ ModelResponse.images ](../api/messages/index.html#pydantic_ai.messages.ModelResponse.images) or [image output](../output/index.html#image-output) requires the [ OpenAIResponsesModelSettings.openai_include_code_execution_outputs ](../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIResponsesModelSettings.openai_include_code_execution_outputs) [model setting](../agents/index.html#model-run-settings) to be enabled. code\\_execution\\_openai.py *(This example is complete, it can be run \"as is\")* ## Image Generation Tool The [ ImageGenerationTool ](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.ImageGenerationTool) enables your agent to generate images. ### Provider Support ### Usage Generated images are available on [ ModelResponse.images ](../api/messages/index.html#pydantic_ai.messages.ModelResponse.images) as [ BinaryImage ](../api/messages/index.html#pydantic_ai.messages.BinaryImage) objects: image\\_generation\\_openai.py *(This example is complete, it can be run \"as is\")* Image generation with Google [image generation models](https://ai.google.dev/gemini-api/docs/image-generation) does not require the ImageGenerationTool built-in tool to be explicitly specified: image\\_generation\\_google.py *(This example is complete, it can be run \"as is\")* The ImageGenerationTool can be used together with output_type=BinaryImage to get [image output](../output/index.html#image-output). If the ImageGenerationTool built-in tool is not explicitly specified, it will be enabled automatically: image\\_generation\\_output.py *(This example is complete, it can be run \"as is\")* ### Configuration Options The ImageGenerationTool supports several configuration parameters: image\\_generation\\_configured.py *(This example is complete, it can be run \"as is\")* For more details, check the [API documentation](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.ImageGenerationTool). #### Provider Support ## URL Context Tool The [ UrlContextTool ](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.UrlContextTool) enables your agent to pull URL contents into its context, allowing it to pull up-to-date information from the web. ### Provider Support ### Usage url\\_context\\_basic.py *(This example is complete, it can be run \"as is\")* ## Memory Tool The [ MemoryTool ](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.MemoryTool) enables your agent to use memory. ### Provider Support ### Usage The Anthropic SDK provides an abstract [ BetaAbstractMemoryTool ](https://github.com/anthropics/anthropic-sdk-python/blob/main/src/anthropic/lib/tools/_beta_builtin_memory_tool.py) class that you can subclass to create your own memory storage solution (e.g., database, cloud storage, encrypted files, etc.). Their [ LocalFilesystemMemoryTool ](https://github.com/anthropics/anthropic-sdk-python/blob/main/examples/memory/basic.py) example can serve as a starting point. The following example uses a subclass that hard-codes a specific memory. The bits specific to Pydantic AI are the MemoryTool built-in tool and the memory tool definition that forwards commands to the call method of the BetaAbstractMemoryTool subclass. anthropic\\_memory.py *(This example is complete, it can be run \"as is\")* ## MCP Server Tool The [ MCPServerTool ](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.MCPServerTool) allows your agent to use remote MCP servers with communication handled by the model provider. This requires the MCP server to live at a public URL the provider can reach and does not support many of the advanced features of Pydantic AI's agent-side [MCP support](../mcp/client/index.html), but can result in optimized context use and caching, and faster performance due to the lack of a round-trip back to Pydantic AI. ### Provider Support ### Usage mcp\\_server\\_anthropic.py 1. The [DeepWiki MCP server](https://docs.devin.ai/work-with-devin/deepwiki-mcp) does not require authorization. *(This example is complete, it can be run \"as is\")* With OpenAI, you must use their Responses API to access the MCP server tool: mcp\\_server\\_openai.py 1. The [DeepWiki MCP server](https://docs.devin.ai/work-with-devin/deepwiki-mcp) does not require authorization. *(This example is complete, it can be run \"as is\")* ### Configuration Options The MCPServerTool supports several configuration parameters for custom MCP servers: mcp\\_server\\_configured\\_url.py 1. The [GitHub MCP server](https://github.com/github/github-mcp-server) requires an authorization token. For OpenAI Responses, you can use a [connector](https://platform.openai.com/docs/guides/tools-connectors-mcp#connectors) by specifying a special x-openai-connector: URL: *(This example is complete, it can be run \"as is\")* mcp\\_server\\_configured\\_connector\\_id.py 1. OpenAI's Google Calendar connector requires an [authorization token](https://platform.openai.com/docs/guides/tools-connectors-mcp#authorizing-a-connector). *(This example is complete, it can be run \"as is\")* #### Provider Support ## API Reference For complete API documentation, see the [API Reference](../api/builtin_tools/index.html).", "url": "https://ai.pydantic.dev/builtin-tools/index.html#built-in-tools", "page": "builtin-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "Pydantic AI supports the following built-in tools: * **[ WebSearchTool ](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.WebSearchTool)**: Allows agents to search the web * **[ CodeExecutionTool ](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.CodeExecutionTool)**: Enables agents to execute code in a secure environment * **[ ImageGenerationTool ](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.ImageGenerationTool)**: Enables agents to generate images * **[ UrlContextTool ](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.UrlContextTool)**: Enables agents to pull URL contents into their context * **[ MemoryTool ](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.MemoryTool)**: Enables agents to use memory * **[ MCPServerTool ](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.MCPServerTool)**: Enables agents to use remote MCP servers with communication handled by the model provider These tools are passed to the agent via the builtin_tools parameter and are executed by the model provider's infrastructure. Provider Support Not all model providers support built-in tools. If you use a built-in tool with an unsupported provider, Pydantic AI will raise a [ UserError ](../api/exceptions/index.html#pydantic_ai.exceptions.UserError) when you try to run the agent. If a provider supports a built-in tool that is not currently supported by Pydantic AI, please file an issue.", "url": "https://ai.pydantic.dev/builtin-tools/index.html#overview", "page": "builtin-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Web Search Tool", "anchor": "web-search-tool", "heading_level": 2, "md_text": "The [ WebSearchTool ](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.WebSearchTool) allows your agent to search the web, making it ideal for queries that require up-to-date data. ### Provider Support ### Usage web\\_search\\_anthropic.py *(This example is complete, it can be run \"as is\")* With OpenAI, you must use their Responses API to access the web search tool. web\\_search\\_openai.py *(This example is complete, it can be run \"as is\")* ### Configuration Options The WebSearchTool supports several configuration parameters: web\\_search\\_configured.py *(This example is complete, it can be run \"as is\")* #### Provider Support Anthropic Domain Filtering With Anthropic, you can only use either blocked_domains or allowed_domains , not both.", "url": "https://ai.pydantic.dev/builtin-tools/index.html#web-search-tool", "page": "builtin-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Provider Support", "anchor": "provider-support", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/builtin-tools/index.html#provider-support", "page": "builtin-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 3, "md_text": "web\\_search\\_anthropic.py *(This example is complete, it can be run \"as is\")* With OpenAI, you must use their Responses API to access the web search tool. web\\_search\\_openai.py *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/builtin-tools/index.html#usage", "page": "builtin-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Configuration Options", "anchor": "configuration-options", "heading_level": 3, "md_text": "The WebSearchTool supports several configuration parameters: web\\_search\\_configured.py *(This example is complete, it can be run \"as is\")* #### Provider Support Anthropic Domain Filtering With Anthropic, you can only use either blocked_domains or allowed_domains , not both.", "url": "https://ai.pydantic.dev/builtin-tools/index.html#configuration-options", "page": "builtin-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Code Execution Tool", "anchor": "code-execution-tool", "heading_level": 2, "md_text": "The [ CodeExecutionTool ](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.CodeExecutionTool) enables your agent to execute code in a secure environment, making it perfect for computational tasks, data analysis, and mathematical operations. ### Provider Support ### Usage code\\_execution\\_basic.py *(This example is complete, it can be run \"as is\")* In addition to text output, code execution with OpenAI can generate images as part of their response. Accessing this image via [ ModelResponse.images ](../api/messages/index.html#pydantic_ai.messages.ModelResponse.images) or [image output](../output/index.html#image-output) requires the [ OpenAIResponsesModelSettings.openai_include_code_execution_outputs ](../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIResponsesModelSettings.openai_include_code_execution_outputs) [model setting](../agents/index.html#model-run-settings) to be enabled. code\\_execution\\_openai.py *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/builtin-tools/index.html#code-execution-tool", "page": "builtin-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Provider Support", "anchor": "provider-support_2", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/builtin-tools/index.html#provider-support_2", "page": "builtin-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage_1", "heading_level": 3, "md_text": "code\\_execution\\_basic.py *(This example is complete, it can be run \"as is\")* In addition to text output, code execution with OpenAI can generate images as part of their response. Accessing this image via [ ModelResponse.images ](../api/messages/index.html#pydantic_ai.messages.ModelResponse.images) or [image output](../output/index.html#image-output) requires the [ OpenAIResponsesModelSettings.openai_include_code_execution_outputs ](../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIResponsesModelSettings.openai_include_code_execution_outputs) [model setting](../agents/index.html#model-run-settings) to be enabled. code\\_execution\\_openai.py *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/builtin-tools/index.html#usage_1", "page": "builtin-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Image Generation Tool", "anchor": "image-generation-tool", "heading_level": 2, "md_text": "The [ ImageGenerationTool ](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.ImageGenerationTool) enables your agent to generate images. ### Provider Support ### Usage Generated images are available on [ ModelResponse.images ](../api/messages/index.html#pydantic_ai.messages.ModelResponse.images) as [ BinaryImage ](../api/messages/index.html#pydantic_ai.messages.BinaryImage) objects: image\\_generation\\_openai.py *(This example is complete, it can be run \"as is\")* Image generation with Google [image generation models](https://ai.google.dev/gemini-api/docs/image-generation) does not require the ImageGenerationTool built-in tool to be explicitly specified: image\\_generation\\_google.py *(This example is complete, it can be run \"as is\")* The ImageGenerationTool can be used together with output_type=BinaryImage to get [image output](../output/index.html#image-output). If the ImageGenerationTool built-in tool is not explicitly specified, it will be enabled automatically: image\\_generation\\_output.py *(This example is complete, it can be run \"as is\")* ### Configuration Options The ImageGenerationTool supports several configuration parameters: image\\_generation\\_configured.py *(This example is complete, it can be run \"as is\")* For more details, check the [API documentation](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.ImageGenerationTool). #### Provider Support", "url": "https://ai.pydantic.dev/builtin-tools/index.html#image-generation-tool", "page": "builtin-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Provider Support", "anchor": "provider-support_3", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/builtin-tools/index.html#provider-support_3", "page": "builtin-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage_2", "heading_level": 3, "md_text": "Generated images are available on [ ModelResponse.images ](../api/messages/index.html#pydantic_ai.messages.ModelResponse.images) as [ BinaryImage ](../api/messages/index.html#pydantic_ai.messages.BinaryImage) objects: image\\_generation\\_openai.py *(This example is complete, it can be run \"as is\")* Image generation with Google [image generation models](https://ai.google.dev/gemini-api/docs/image-generation) does not require the ImageGenerationTool built-in tool to be explicitly specified: image\\_generation\\_google.py *(This example is complete, it can be run \"as is\")* The ImageGenerationTool can be used together with output_type=BinaryImage to get [image output](../output/index.html#image-output). If the ImageGenerationTool built-in tool is not explicitly specified, it will be enabled automatically: image\\_generation\\_output.py *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/builtin-tools/index.html#usage_2", "page": "builtin-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Configuration Options", "anchor": "configuration-options_1", "heading_level": 3, "md_text": "The ImageGenerationTool supports several configuration parameters: image\\_generation\\_configured.py *(This example is complete, it can be run \"as is\")* For more details, check the [API documentation](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.ImageGenerationTool). #### Provider Support", "url": "https://ai.pydantic.dev/builtin-tools/index.html#configuration-options_1", "page": "builtin-tools/index.html", "source_site": "pydantic_ai"}
{"title": "URL Context Tool", "anchor": "url-context-tool", "heading_level": 2, "md_text": "The [ UrlContextTool ](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.UrlContextTool) enables your agent to pull URL contents into its context, allowing it to pull up-to-date information from the web. ### Provider Support ### Usage url\\_context\\_basic.py *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/builtin-tools/index.html#url-context-tool", "page": "builtin-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Provider Support", "anchor": "provider-support_5", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/builtin-tools/index.html#provider-support_5", "page": "builtin-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage_3", "heading_level": 3, "md_text": "url\\_context\\_basic.py *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/builtin-tools/index.html#usage_3", "page": "builtin-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Memory Tool", "anchor": "memory-tool", "heading_level": 2, "md_text": "The [ MemoryTool ](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.MemoryTool) enables your agent to use memory. ### Provider Support ### Usage The Anthropic SDK provides an abstract [ BetaAbstractMemoryTool ](https://github.com/anthropics/anthropic-sdk-python/blob/main/src/anthropic/lib/tools/_beta_builtin_memory_tool.py) class that you can subclass to create your own memory storage solution (e.g., database, cloud storage, encrypted files, etc.). Their [ LocalFilesystemMemoryTool ](https://github.com/anthropics/anthropic-sdk-python/blob/main/examples/memory/basic.py) example can serve as a starting point. The following example uses a subclass that hard-codes a specific memory. The bits specific to Pydantic AI are the MemoryTool built-in tool and the memory tool definition that forwards commands to the call method of the BetaAbstractMemoryTool subclass. anthropic\\_memory.py *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/builtin-tools/index.html#memory-tool", "page": "builtin-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Provider Support", "anchor": "provider-support_6", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/builtin-tools/index.html#provider-support_6", "page": "builtin-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage_4", "heading_level": 3, "md_text": "The Anthropic SDK provides an abstract [ BetaAbstractMemoryTool ](https://github.com/anthropics/anthropic-sdk-python/blob/main/src/anthropic/lib/tools/_beta_builtin_memory_tool.py) class that you can subclass to create your own memory storage solution (e.g., database, cloud storage, encrypted files, etc.). Their [ LocalFilesystemMemoryTool ](https://github.com/anthropics/anthropic-sdk-python/blob/main/examples/memory/basic.py) example can serve as a starting point. The following example uses a subclass that hard-codes a specific memory. The bits specific to Pydantic AI are the MemoryTool built-in tool and the memory tool definition that forwards commands to the call method of the BetaAbstractMemoryTool subclass. anthropic\\_memory.py *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/builtin-tools/index.html#usage_4", "page": "builtin-tools/index.html", "source_site": "pydantic_ai"}
{"title": "MCP Server Tool", "anchor": "mcp-server-tool", "heading_level": 2, "md_text": "The [ MCPServerTool ](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.MCPServerTool) allows your agent to use remote MCP servers with communication handled by the model provider. This requires the MCP server to live at a public URL the provider can reach and does not support many of the advanced features of Pydantic AI's agent-side [MCP support](../mcp/client/index.html), but can result in optimized context use and caching, and faster performance due to the lack of a round-trip back to Pydantic AI. ### Provider Support ### Usage mcp\\_server\\_anthropic.py 1. The [DeepWiki MCP server](https://docs.devin.ai/work-with-devin/deepwiki-mcp) does not require authorization. *(This example is complete, it can be run \"as is\")* With OpenAI, you must use their Responses API to access the MCP server tool: mcp\\_server\\_openai.py 1. The [DeepWiki MCP server](https://docs.devin.ai/work-with-devin/deepwiki-mcp) does not require authorization. *(This example is complete, it can be run \"as is\")* ### Configuration Options The MCPServerTool supports several configuration parameters for custom MCP servers: mcp\\_server\\_configured\\_url.py 1. The [GitHub MCP server](https://github.com/github/github-mcp-server) requires an authorization token. For OpenAI Responses, you can use a [connector](https://platform.openai.com/docs/guides/tools-connectors-mcp#connectors) by specifying a special x-openai-connector: URL: *(This example is complete, it can be run \"as is\")* mcp\\_server\\_configured\\_connector\\_id.py 1. OpenAI's Google Calendar connector requires an [authorization token](https://platform.openai.com/docs/guides/tools-connectors-mcp#authorizing-a-connector). *(This example is complete, it can be run \"as is\")* #### Provider Support", "url": "https://ai.pydantic.dev/builtin-tools/index.html#mcp-server-tool", "page": "builtin-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Provider Support", "anchor": "provider-support_7", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/builtin-tools/index.html#provider-support_7", "page": "builtin-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage_5", "heading_level": 3, "md_text": "mcp\\_server\\_anthropic.py 1. The [DeepWiki MCP server](https://docs.devin.ai/work-with-devin/deepwiki-mcp) does not require authorization. *(This example is complete, it can be run \"as is\")* With OpenAI, you must use their Responses API to access the MCP server tool: mcp\\_server\\_openai.py 1. The [DeepWiki MCP server](https://docs.devin.ai/work-with-devin/deepwiki-mcp) does not require authorization. *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/builtin-tools/index.html#usage_5", "page": "builtin-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Configuration Options", "anchor": "configuration-options_2", "heading_level": 3, "md_text": "The MCPServerTool supports several configuration parameters for custom MCP servers: mcp\\_server\\_configured\\_url.py 1. The [GitHub MCP server](https://github.com/github/github-mcp-server) requires an authorization token. For OpenAI Responses, you can use a [connector](https://platform.openai.com/docs/guides/tools-connectors-mcp#connectors) by specifying a special x-openai-connector: URL: *(This example is complete, it can be run \"as is\")* mcp\\_server\\_configured\\_connector\\_id.py 1. OpenAI's Google Calendar connector requires an [authorization token](https://platform.openai.com/docs/guides/tools-connectors-mcp#authorizing-a-connector). *(This example is complete, it can be run \"as is\")* #### Provider Support", "url": "https://ai.pydantic.dev/builtin-tools/index.html#configuration-options_2", "page": "builtin-tools/index.html", "source_site": "pydantic_ai"}
{"title": "API Reference", "anchor": "api-reference", "heading_level": 2, "md_text": "For complete API documentation, see the [API Reference](../api/builtin_tools/index.html).", "url": "https://ai.pydantic.dev/builtin-tools/index.html#api-reference", "page": "builtin-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Deferred Tools", "anchor": "deferred-tools", "heading_level": 1, "md_text": "There are a few scenarios where the model should be able to call a tool that should not or cannot be executed during the same agent run inside the same Python process: * it may need to be approved by the user first * it may depend on an upstream service, frontend, or user to provide the result * the result could take longer to generate than it's reasonable to keep the agent process running To support these use cases, Pydantic AI provides the concept of deferred tools, which come in two flavors documented below: * tools that [require approval](index.html#human-in-the-loop-tool-approval) * tools that are [executed externally](index.html#external-tool-execution) When the model calls a deferred tool, the agent run will end with a [ DeferredToolRequests ](../api/output/index.html#pydantic_ai.output.DeferredToolRequests) output object containing information about the deferred tool calls. Once the approvals and/or results are ready, a new agent run can then be started with the original run's [message history](../message-history/index.html) plus a [ DeferredToolResults ](../api/tools/index.html#pydantic_ai.tools.DeferredToolResults) object holding results for each tool call in DeferredToolRequests , which will continue the original run where it left off. Note that handling deferred tool calls requires DeferredToolRequests to be in the Agent 's [ output_type ](../output/index.html#structured-output) so that the possible types of the agent run output are correctly inferred. If your agent can also be used in a context where no deferred tools are available and you don't want to deal with that type everywhere you use the agent, you can instead pass the output_type argument when you run the agent using [ agent.run() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run), [ agent.run_sync() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_sync), [ agent.run_stream() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream), or [ agent.iter() ](../api/agent/index.html#pydantic_ai.agent.Agent.iter). Note that the run-time output_type overrides the one specified at construction time (for type inference reasons), so you'll need to include the original output type explicitly. ## Human-in-the-Loop Tool Approval If a tool function always requires approval, you can pass the requires_approval=True argument to the [ @agent.tool ](../api/agent/index.html#pydantic_ai.agent.Agent.tool) decorator, [ @agent.tool_plain ](../api/agent/index.html#pydantic_ai.agent.Agent.tool_plain) decorator, [ Tool ](../api/tools/index.html#pydantic_ai.tools.Tool) class, [ FunctionToolset.tool ](../api/toolsets/index.html#pydantic_ai.toolsets.FunctionToolset.tool) decorator, or [ FunctionToolset.add_function() ](../api/toolsets/index.html#pydantic_ai.toolsets.FunctionToolset.add_function) method. Inside the function, you can then assume that the tool call has been approved. If whether a tool function requires approval depends on the tool call arguments or the agent [run context](../api/tools/index.html#pydantic_ai.tools.RunContext) (e.g. [dependencies](../dependencies/index.html) or message history), you can raise the [ ApprovalRequired ](../api/exceptions/index.html#pydantic_ai.exceptions.ApprovalRequired) exception from the tool function. The [ RunContext.tool_call_approved ](../api/tools/index.html#pydantic_ai.tools.RunContext.tool_call_approved) property will be True if the tool call has already been approved. To require approval for calls to tools provided by a [toolset](../toolsets/index.html) (like an [MCP server](../mcp/client/index.html)), see the [ ApprovalRequiredToolset documentation](../toolsets/index.html#requiring-tool-approval). When the model calls a tool that requires approval, the agent run will end with a [ DeferredToolRequests ](../api/output/index.html#pydantic_ai.output.DeferredToolRequests) output object with an approvals list holding [ ToolCallPart s](../api/messages/index.html#pydantic_ai.messages.ToolCallPart) containing the tool name, validated arguments, and a unique tool call ID. Once you've gathered the user's approvals or denials, you can build a [ DeferredToolResults ](../api/tools/index.html#pydantic_ai.tools.DeferredToolResults) object with an approvals dictionary that maps each tool call ID to a boolean, a [ ToolApproved ](../api/tools/index.html#pydantic_ai.tools.ToolApproved) object (with optional override_args ), or a [ ToolDenied ](../api/tools/index.html#pydantic_ai.tools.ToolDenied) object (with an optional custom message to provide to the model). This DeferredToolResults object can then be provided to one of the agent run methods as deferred_tool_results , alongside the original run's [message history](../message-history/index.html). Here's an example that shows how to require approval for all file deletions, and for updates of specific protected files: tool\\_requires\\_approval.py *(This example is complete, it can be run \"as is\")* ## External Tool Execution When the result of a tool call cannot be generated inside the same agent run in which it was called, the tool is considered to be external. Examples of external tools are client-side tools implemented by a web or app frontend, and slow tasks that are passed off to a background worker or external service instead of keeping the agent process running. If whether a tool call should be executed externally depends on the tool call arguments, the agent [run context](../api/tools/index.html#pydantic_ai.tools.RunContext) (e.g. [dependencies](../dependencies/index.html) or message history), or how long the task is expected to take, you can define a tool function and conditionally raise the [ CallDeferred ](../api/exceptions/index.html#pydantic_ai.exceptions.CallDeferred) exception. Before raising the exception, the tool function would typically schedule some background task and pass along the [ RunContext.tool_call_id ](../api/tools/index.html#pydantic_ai.tools.RunContext.tool_call_id) so that the result can be matched to the deferred tool call later. If a tool is always executed externally and its definition is provided to your code along with a JSON schema for its arguments, you can use an [ ExternalToolset ](../toolsets/index.html#external-toolset). If the external tools are known up front and you don't have the arguments JSON schema handy, you can also define a tool function with the appropriate signature that does nothing but raise the [ CallDeferred ](../api/exceptions/index.html#pydantic_ai.exceptions.CallDeferred) exception. When the model calls an external tool, the agent run will end with a [ DeferredToolRequests ](../api/output/index.html#pydantic_ai.output.DeferredToolRequests) output object with a calls list holding [ ToolCallPart s](../api/messages/index.html#pydantic_ai.messages.ToolCallPart) containing the tool name, validated arguments, and a unique tool call ID. Once the tool call results are ready, you can build a [ DeferredToolResults ](../api/tools/index.html#pydantic_ai.tools.DeferredToolResults) object with a calls dictionary that maps each tool call ID to an arbitrary value to be returned to the model, a [ ToolReturn ](../tools-advanced/index.html#advanced-tool-returns) object, or a [ ModelRetry ](../api/exceptions/index.html#pydantic_ai.exceptions.ModelRetry) exception in case the tool call failed and the model should [try again](../tools-advanced/index.html#tool-retries). This DeferredToolResults object can then be provided to one of the agent run methods as deferred_tool_results , alongside the original run's [message history](../message-history/index.html). Here's an example that shows how to move a task that takes a while to complete to the background and return the result to the model once the task is complete: external\\_tool.py 1. In reality, you'd likely use Celery or a similar task queue to run the task in the background. 2. In reality, this would typically happen in a separate process that polls for the task status or is notified when all pending tasks are complete. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* ## See Also * [Function Tools](../tools/index.html) - Basic tool concepts and registration * [Advanced Tool Features](../tools-advanced/index.html) - Custom schemas, dynamic tools, and execution details * [Toolsets](../toolsets/index.html) - Managing collections of tools, including ExternalToolset for external tools * [Message History](../message-history/index.html) - Understanding how to work with message history for deferred tools", "url": "https://ai.pydantic.dev/deferred-tools/index.html#deferred-tools", "page": "deferred-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Human-in-the-Loop Tool Approval", "anchor": "human-in-the-loop-tool-approval", "heading_level": 2, "md_text": "If a tool function always requires approval, you can pass the requires_approval=True argument to the [ @agent.tool ](../api/agent/index.html#pydantic_ai.agent.Agent.tool) decorator, [ @agent.tool_plain ](../api/agent/index.html#pydantic_ai.agent.Agent.tool_plain) decorator, [ Tool ](../api/tools/index.html#pydantic_ai.tools.Tool) class, [ FunctionToolset.tool ](../api/toolsets/index.html#pydantic_ai.toolsets.FunctionToolset.tool) decorator, or [ FunctionToolset.add_function() ](../api/toolsets/index.html#pydantic_ai.toolsets.FunctionToolset.add_function) method. Inside the function, you can then assume that the tool call has been approved. If whether a tool function requires approval depends on the tool call arguments or the agent [run context](../api/tools/index.html#pydantic_ai.tools.RunContext) (e.g. [dependencies](../dependencies/index.html) or message history), you can raise the [ ApprovalRequired ](../api/exceptions/index.html#pydantic_ai.exceptions.ApprovalRequired) exception from the tool function. The [ RunContext.tool_call_approved ](../api/tools/index.html#pydantic_ai.tools.RunContext.tool_call_approved) property will be True if the tool call has already been approved. To require approval for calls to tools provided by a [toolset](../toolsets/index.html) (like an [MCP server](../mcp/client/index.html)), see the [ ApprovalRequiredToolset documentation](../toolsets/index.html#requiring-tool-approval). When the model calls a tool that requires approval, the agent run will end with a [ DeferredToolRequests ](../api/output/index.html#pydantic_ai.output.DeferredToolRequests) output object with an approvals list holding [ ToolCallPart s](../api/messages/index.html#pydantic_ai.messages.ToolCallPart) containing the tool name, validated arguments, and a unique tool call ID. Once you've gathered the user's approvals or denials, you can build a [ DeferredToolResults ](../api/tools/index.html#pydantic_ai.tools.DeferredToolResults) object with an approvals dictionary that maps each tool call ID to a boolean, a [ ToolApproved ](../api/tools/index.html#pydantic_ai.tools.ToolApproved) object (with optional override_args ), or a [ ToolDenied ](../api/tools/index.html#pydantic_ai.tools.ToolDenied) object (with an optional custom message to provide to the model). This DeferredToolResults object can then be provided to one of the agent run methods as deferred_tool_results , alongside the original run's [message history](../message-history/index.html). Here's an example that shows how to require approval for all file deletions, and for updates of specific protected files: tool\\_requires\\_approval.py *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/deferred-tools/index.html#human-in-the-loop-tool-approval", "page": "deferred-tools/index.html", "source_site": "pydantic_ai"}
{"title": "External Tool Execution", "anchor": "external-tool-execution", "heading_level": 2, "md_text": "When the result of a tool call cannot be generated inside the same agent run in which it was called, the tool is considered to be external. Examples of external tools are client-side tools implemented by a web or app frontend, and slow tasks that are passed off to a background worker or external service instead of keeping the agent process running. If whether a tool call should be executed externally depends on the tool call arguments, the agent [run context](../api/tools/index.html#pydantic_ai.tools.RunContext) (e.g. [dependencies](../dependencies/index.html) or message history), or how long the task is expected to take, you can define a tool function and conditionally raise the [ CallDeferred ](../api/exceptions/index.html#pydantic_ai.exceptions.CallDeferred) exception. Before raising the exception, the tool function would typically schedule some background task and pass along the [ RunContext.tool_call_id ](../api/tools/index.html#pydantic_ai.tools.RunContext.tool_call_id) so that the result can be matched to the deferred tool call later. If a tool is always executed externally and its definition is provided to your code along with a JSON schema for its arguments, you can use an [ ExternalToolset ](../toolsets/index.html#external-toolset). If the external tools are known up front and you don't have the arguments JSON schema handy, you can also define a tool function with the appropriate signature that does nothing but raise the [ CallDeferred ](../api/exceptions/index.html#pydantic_ai.exceptions.CallDeferred) exception. When the model calls an external tool, the agent run will end with a [ DeferredToolRequests ](../api/output/index.html#pydantic_ai.output.DeferredToolRequests) output object with a calls list holding [ ToolCallPart s](../api/messages/index.html#pydantic_ai.messages.ToolCallPart) containing the tool name, validated arguments, and a unique tool call ID. Once the tool call results are ready, you can build a [ DeferredToolResults ](../api/tools/index.html#pydantic_ai.tools.DeferredToolResults) object with a calls dictionary that maps each tool call ID to an arbitrary value to be returned to the model, a [ ToolReturn ](../tools-advanced/index.html#advanced-tool-returns) object, or a [ ModelRetry ](../api/exceptions/index.html#pydantic_ai.exceptions.ModelRetry) exception in case the tool call failed and the model should [try again](../tools-advanced/index.html#tool-retries). This DeferredToolResults object can then be provided to one of the agent run methods as deferred_tool_results , alongside the original run's [message history](../message-history/index.html). Here's an example that shows how to move a task that takes a while to complete to the background and return the result to the model once the task is complete: external\\_tool.py 1. In reality, you'd likely use Celery or a similar task queue to run the task in the background. 2. In reality, this would typically happen in a separate process that polls for the task status or is notified when all pending tasks are complete. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/deferred-tools/index.html#external-tool-execution", "page": "deferred-tools/index.html", "source_site": "pydantic_ai"}
{"title": "See Also", "anchor": "see-also", "heading_level": 2, "md_text": "* [Function Tools](../tools/index.html) - Basic tool concepts and registration * [Advanced Tool Features](../tools-advanced/index.html) - Custom schemas, dynamic tools, and execution details * [Toolsets](../toolsets/index.html) - Managing collections of tools, including ExternalToolset for external tools * [Message History](../message-history/index.html) - Understanding how to work with message history for deferred tools", "url": "https://ai.pydantic.dev/deferred-tools/index.html#see-also", "page": "deferred-tools/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.usage", "anchor": "pydantic_aiusage", "heading_level": 1, "md_text": "### RequestUsage dataclass Bases: UsageBase LLM usage associated with a single request. This is an implementation of genai_prices.types.AbstractUsage so it can be used to calculate the price of the request using [genai-prices](https://github.com/pydantic/genai-prices). Source code in pydantic_ai_slim/pydantic_ai/usage.py #### incr Increment the usage in place. Parameters: Source code in pydantic_ai_slim/pydantic_ai/usage.py #### \\_\\_add\\_\\_ Add two RequestUsages together. This is provided so it's trivial to sum usage information from multiple parts of a response. **WARNING:** this CANNOT be used to sum multiple requests without breaking some pricing calculations. Source code in pydantic_ai_slim/pydantic_ai/usage.py #### extract classmethod Extract usage information from the response data using genai-prices. Parameters: Source code in pydantic_ai_slim/pydantic_ai/usage.py ### RunUsage dataclass Bases: UsageBase LLM usage associated with an agent run. Responsibility for calculating request usage is on the model; Pydantic AI simply sums the usage information across requests. Source code in pydantic_ai_slim/pydantic_ai/usage.py #### requests class-attribute instance-attribute Number of requests made to the LLM API. #### tool\\_calls class-attribute instance-attribute Number of successful tool calls executed during the run. #### input\\_tokens class-attribute instance-attribute Total number of input/prompt tokens. #### cache\\_write\\_tokens class-attribute instance-attribute Total number of tokens written to the cache. #### cache\\_read\\_tokens class-attribute instance-attribute Total number of tokens read from the cache. #### input\\_audio\\_tokens class-attribute instance-attribute Total number of audio input tokens. #### cache\\_audio\\_read\\_tokens class-attribute instance-attribute Total number of audio tokens read from the cache. #### output\\_tokens class-attribute instance-attribute Total number of output/completion tokens. #### details class-attribute instance-attribute Any extra details returned by the model. #### incr Increment the usage in place. Parameters: Source code in pydantic_ai_slim/pydantic_ai/usage.py #### \\_\\_add\\_\\_ Add two RunUsages together. This is provided so it's trivial to sum usage information from multiple runs. Source code in pydantic_ai_slim/pydantic_ai/usage.py ### Usage dataclass deprecated Bases: RunUsage Deprecated Usage is deprecated, use RunUsage instead Deprecated alias for RunUsage . Source code in pydantic_ai_slim/pydantic_ai/usage.py ### UsageLimits dataclass Limits on model usage. The request count is tracked by pydantic\\_ai, and the request limit is checked before each request to the model. Token counts are provided in responses from the model, and the token limits are checked after each response. Each of the limits can be set to None to disable that limit. Source code in pydantic_ai_slim/pydantic_ai/usage.py #### request\\_limit class-attribute instance-attribute The maximum number of requests allowed to the model. #### tool\\_calls\\_limit class-attribute instance-attribute The maximum number of successful tool calls allowed to be executed. #### input\\_tokens\\_limit class-attribute instance-attribute The maximum number of input/prompt tokens allowed. #### output\\_tokens\\_limit class-attribute instance-attribute The maximum number of output/response tokens allowed. #### total\\_tokens\\_limit class-attribute instance-attribute The maximum number of tokens allowed in requests and responses combined. #### count\\_tokens\\_before\\_request class-attribute instance-attribute If True, perform a token counting pass before sending the request to the model, to enforce request_tokens_limit ahead of time. This may incur additional overhead (from calling the model's count_tokens API before making the actual request) and is disabled by default. #### has\\_token\\_limits Returns True if this instance places any limits on token counts. If this returns False , the check_tokens method will never raise an error. This is useful because if we have token limits, we need to check them after receiving each streamed message. If there are no limits, we can skip that processing in the streaming response iterator. Source code in pydantic_ai_slim/pydantic_ai/usage.py #### check\\_before\\_request Raises a UsageLimitExceeded exception if the next request would exceed any of the limits. Source code in pydantic_ai_slim/pydantic_ai/usage.py #### check\\_tokens Raises a UsageLimitExceeded exception if the usage exceeds any of the token limits. Source code in pydantic_ai_slim/pydantic_ai/usage.py #### check\\_before\\_tool\\_call Raises a UsageLimitExceeded exception if the next tool call(s) would exceed the tool call limit. Source code in pydantic_ai_slim/pydantic_ai/usage.py", "url": "https://ai.pydantic.dev/api/usage/index.html#pydantic_aiusage", "page": "api/usage/index.html", "source_site": "pydantic_ai"}
{"title": "RequestUsage dataclass", "anchor": "pydantic_ai.usage.RequestUsage", "heading_level": 3, "md_text": "Bases: UsageBase LLM usage associated with a single request. This is an implementation of genai_prices.types.AbstractUsage so it can be used to calculate the price of the request using [genai-prices](https://github.com/pydantic/genai-prices). Source code in pydantic_ai_slim/pydantic_ai/usage.py #### incr Increment the usage in place. Parameters: Source code in pydantic_ai_slim/pydantic_ai/usage.py #### \\_\\_add\\_\\_ Add two RequestUsages together. This is provided so it's trivial to sum usage information from multiple parts of a response. **WARNING:** this CANNOT be used to sum multiple requests without breaking some pricing calculations. Source code in pydantic_ai_slim/pydantic_ai/usage.py #### extract classmethod Extract usage information from the response data using genai-prices. Parameters: Source code in pydantic_ai_slim/pydantic_ai/usage.py", "url": "https://ai.pydantic.dev/api/usage/index.html#pydantic_ai.usage.RequestUsage", "page": "api/usage/index.html", "source_site": "pydantic_ai"}
{"title": "RunUsage dataclass", "anchor": "pydantic_ai.usage.RunUsage", "heading_level": 3, "md_text": "Bases: UsageBase LLM usage associated with an agent run. Responsibility for calculating request usage is on the model; Pydantic AI simply sums the usage information across requests. Source code in pydantic_ai_slim/pydantic_ai/usage.py #### requests class-attribute instance-attribute Number of requests made to the LLM API. #### tool\\_calls class-attribute instance-attribute Number of successful tool calls executed during the run. #### input\\_tokens class-attribute instance-attribute Total number of input/prompt tokens. #### cache\\_write\\_tokens class-attribute instance-attribute Total number of tokens written to the cache. #### cache\\_read\\_tokens class-attribute instance-attribute Total number of tokens read from the cache. #### input\\_audio\\_tokens class-attribute instance-attribute Total number of audio input tokens. #### cache\\_audio\\_read\\_tokens class-attribute instance-attribute Total number of audio tokens read from the cache. #### output\\_tokens class-attribute instance-attribute Total number of output/completion tokens. #### details class-attribute instance-attribute Any extra details returned by the model. #### incr Increment the usage in place. Parameters: Source code in pydantic_ai_slim/pydantic_ai/usage.py #### \\_\\_add\\_\\_ Add two RunUsages together. This is provided so it's trivial to sum usage information from multiple runs. Source code in pydantic_ai_slim/pydantic_ai/usage.py", "url": "https://ai.pydantic.dev/api/usage/index.html#pydantic_ai.usage.RunUsage", "page": "api/usage/index.html", "source_site": "pydantic_ai"}
{"title": "Usage dataclass deprecated", "anchor": "pydantic_ai.usage.Usage", "heading_level": 3, "md_text": "Bases: RunUsage Deprecated Usage is deprecated, use RunUsage instead Deprecated alias for RunUsage . Source code in pydantic_ai_slim/pydantic_ai/usage.py", "url": "https://ai.pydantic.dev/api/usage/index.html#pydantic_ai.usage.Usage", "page": "api/usage/index.html", "source_site": "pydantic_ai"}
{"title": "UsageLimits dataclass", "anchor": "pydantic_ai.usage.UsageLimits", "heading_level": 3, "md_text": "Limits on model usage. The request count is tracked by pydantic\\_ai, and the request limit is checked before each request to the model. Token counts are provided in responses from the model, and the token limits are checked after each response. Each of the limits can be set to None to disable that limit. Source code in pydantic_ai_slim/pydantic_ai/usage.py #### request\\_limit class-attribute instance-attribute The maximum number of requests allowed to the model. #### tool\\_calls\\_limit class-attribute instance-attribute The maximum number of successful tool calls allowed to be executed. #### input\\_tokens\\_limit class-attribute instance-attribute The maximum number of input/prompt tokens allowed. #### output\\_tokens\\_limit class-attribute instance-attribute The maximum number of output/response tokens allowed. #### total\\_tokens\\_limit class-attribute instance-attribute The maximum number of tokens allowed in requests and responses combined. #### count\\_tokens\\_before\\_request class-attribute instance-attribute If True, perform a token counting pass before sending the request to the model, to enforce request_tokens_limit ahead of time. This may incur additional overhead (from calling the model's count_tokens API before making the actual request) and is disabled by default. #### has\\_token\\_limits Returns True if this instance places any limits on token counts. If this returns False , the check_tokens method will never raise an error. This is useful because if we have token limits, we need to check them after receiving each streamed message. If there are no limits, we can skip that processing in the streaming response iterator. Source code in pydantic_ai_slim/pydantic_ai/usage.py #### check\\_before\\_request Raises a UsageLimitExceeded exception if the next request would exceed any of the limits. Source code in pydantic_ai_slim/pydantic_ai/usage.py #### check\\_tokens Raises a UsageLimitExceeded exception if the usage exceeds any of the token limits. Source code in pydantic_ai_slim/pydantic_ai/usage.py #### check\\_before\\_tool\\_call Raises a UsageLimitExceeded exception if the next tool call(s) would exceed the tool call limit. Source code in pydantic_ai_slim/pydantic_ai/usage.py", "url": "https://ai.pydantic.dev/api/usage/index.html#pydantic_ai.usage.UsageLimits", "page": "api/usage/index.html", "source_site": "pydantic_ai"}
{"title": "Direct Model Requests", "anchor": "direct-model-requests", "heading_level": 1, "md_text": "The direct module provides low-level methods for making imperative requests to LLMs where the only abstraction is input and output schema translation, enabling you to use all models with the same API. These methods are thin wrappers around the [ Model ](../api/models/base/index.html#pydantic_ai.models.Model) implementations, offering a simpler interface when you don't need the full functionality of an [ Agent ](../api/agent/index.html#pydantic_ai.agent.Agent). The following functions are available: * [ model_request ](../api/direct/index.html#pydantic_ai.direct.model_request): Make a non-streamed async request to a model * [ model_request_sync ](../api/direct/index.html#pydantic_ai.direct.model_request_sync): Make a non-streamed synchronous request to a model * [ model_request_stream ](../api/direct/index.html#pydantic_ai.direct.model_request_stream): Make a streamed async request to a model * [ model_request_stream_sync ](../api/direct/index.html#pydantic_ai.direct.model_request_stream_sync): Make a streamed sync request to a model ## Basic Example Here's a simple example demonstrating how to use the direct API to make a basic request: direct\\_basic.py *(This example is complete, it can be run \"as is\")* ## Advanced Example with Tool Calling You can also use the direct API to work with function/tool calling. Even here we can use Pydantic to generate the JSON schema for the tool: *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* ## When to Use the direct API vs Agent The direct API is ideal when: 1. You need more direct control over model interactions 2. You want to implement custom behavior around model requests 3. You're building your own abstractions on top of model interactions For most application use cases, the higher-level [ Agent ](../api/agent/index.html#pydantic_ai.agent.Agent) API provides a more convenient interface with additional features such as built-in tool execution, retrying, structured output parsing, and more. ## OpenTelemetry or Logfire Instrumentation As with [agents](../api/agent/index.html#pydantic_ai.agent.Agent), you can enable OpenTelemetry/Logfire instrumentation with just a few extra lines direct\\_instrumented.py *(This example is complete, it can be run \"as is\")* You can also enable OpenTelemetry on a per call basis: direct\\_instrumented.py See [Debugging and Monitoring](../logfire/index.html) for more details, including how to instrument with plain OpenTelemetry without Logfire.", "url": "https://ai.pydantic.dev/direct/index.html#direct-model-requests", "page": "direct/index.html", "source_site": "pydantic_ai"}
{"title": "Basic Example", "anchor": "basic-example", "heading_level": 2, "md_text": "Here's a simple example demonstrating how to use the direct API to make a basic request: direct\\_basic.py *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/direct/index.html#basic-example", "page": "direct/index.html", "source_site": "pydantic_ai"}
{"title": "Advanced Example with Tool Calling", "anchor": "advanced-example-with-tool-calling", "heading_level": 2, "md_text": "You can also use the direct API to work with function/tool calling. Even here we can use Pydantic to generate the JSON schema for the tool: *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/direct/index.html#advanced-example-with-tool-calling", "page": "direct/index.html", "source_site": "pydantic_ai"}
{"title": "When to Use the direct API vs Agent", "anchor": "when-to-use-the-direct-api-vs-agent", "heading_level": 2, "md_text": "The direct API is ideal when: 1. You need more direct control over model interactions 2. You want to implement custom behavior around model requests 3. You're building your own abstractions on top of model interactions For most application use cases, the higher-level [ Agent ](../api/agent/index.html#pydantic_ai.agent.Agent) API provides a more convenient interface with additional features such as built-in tool execution, retrying, structured output parsing, and more.", "url": "https://ai.pydantic.dev/direct/index.html#when-to-use-the-direct-api-vs-agent", "page": "direct/index.html", "source_site": "pydantic_ai"}
{"title": "OpenTelemetry or Logfire Instrumentation", "anchor": "opentelemetry-or-logfire-instrumentation", "heading_level": 2, "md_text": "As with [agents](../api/agent/index.html#pydantic_ai.agent.Agent), you can enable OpenTelemetry/Logfire instrumentation with just a few extra lines direct\\_instrumented.py *(This example is complete, it can be run \"as is\")* You can also enable OpenTelemetry on a per call basis: direct\\_instrumented.py See [Debugging and Monitoring](../logfire/index.html) for more details, including how to instrument with plain OpenTelemetry without Logfire.", "url": "https://ai.pydantic.dev/direct/index.html#opentelemetry-or-logfire-instrumentation", "page": "direct/index.html", "source_site": "pydantic_ai"}
{"title": "Durable Execution", "anchor": "durable-execution", "heading_level": 1, "md_text": "Pydantic AI allows you to build durable agents that can preserve their progress across transient API failures and application errors or restarts, and handle long-running, asynchronous, and human-in-the-loop workflows with production-grade reliability. Durable agents have full support for [streaming](../../agents/index.html#streaming-all-events) and [MCP](../../mcp/client/index.html), with the added benefit of fault tolerance. Pydantic AI natively supports three durable execution solutions: * [Temporal](../temporal/index.html) * [DBOS](../dbos/index.html) * [Prefect](../prefect/index.html) These integrations only use Pydantic AI's public interface, so they also serve as a reference for integrating with other durable systems.", "url": "https://ai.pydantic.dev/durable_execution/overview/index.html#durable-execution", "page": "durable_execution/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Durable Execution with DBOS", "anchor": "durable-execution-with-dbos", "heading_level": 1, "md_text": "[DBOS](https://www.dbos.dev/) is a lightweight [durable execution](https://docs.dbos.dev/architecture) library natively integrated with Pydantic AI. ## Durable Execution DBOS workflows make your program **durable** by checkpointing its state in a database. If your program ever fails, when it restarts all your workflows will automatically resume from the last completed step. * **Workflows** must be deterministic and generally cannot include I/O. * **Steps** may perform I/O (network, disk, API calls). If a step fails, it restarts from the beginning. Every workflow input and step output is durably stored in the system database. When workflow execution fails, whether from crashes, network issues, or server restarts, DBOS leverages these checkpoints to recover workflows from their last completed step. DBOS **queues** provide durable, database-backed alternatives to systems like Celery or BullMQ, supporting features such as concurrency limits, rate limits, timeouts, and prioritization. See the [DBOS docs](https://docs.dbos.dev/architecture) for details. The diagram below shows the overall architecture of an agentic application in DBOS. DBOS runs fully in-process as a library. Functions remain normal Python functions but are checkpointed into a database (Postgres or SQLite). See the [DBOS documentation](https://docs.dbos.dev/architecture) for more information. ## Durable Agent Any agent can be wrapped in a [ DBOSAgent ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.dbos.DBOSAgent) to get durable execution. DBOSAgent automatically:, * Wraps Agent.run and Agent.run_sync as DBOS workflows. * Wraps [model requests](../../models/overview/index.html) and [MCP communication](../../mcp/client/index.html) as DBOS steps. Custom tool functions and event stream handlers are **not automatically wrapped** by DBOS. If they involve non-deterministic behavior or perform I/O, you should explicitly decorate them with @DBOS.step . The original agent, model, and MCP server can still be used as normal outside the DBOS workflow. Here is a simple but complete example of wrapping an agent for durable execution. All it requires is to install Pydantic AI with the DBOS [open-source library](https://github.com/dbos-inc/dbos-transact-py): pipuv Or if you're using the slim package, you can install it with the dbos optional group: pipuv dbos\\_agent.py 1. Workflows and DBOSAgent must be defined before DBOS.launch() so that recovery can correctly find all workflows. 2. [ DBOSAgent.run() ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.dbos.DBOSAgent.run) works like [ Agent.run() ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run), but runs as a DBOS workflow and executes model requests, decorated tool calls, and MCP communication as DBOS steps. 3. This example uses SQLite. Postgres is recommended for production. 4. The agent's name is used to uniquely identify its workflows. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* Because DBOS workflows need to be defined before calling DBOS.launch() and the DBOSAgent instance automatically registers run and run_sync as workflows, it needs to be defined before calling DBOS.launch() as well. For more information on how to use DBOS in Python applications, see their [Python SDK guide](https://docs.dbos.dev/python/programming-guide). ## DBOS Integration Considerations When using DBOS with Pydantic AI agents, there are a few important considerations to ensure workflows and toolsets behave correctly. ### Agent and Toolset Requirements Each agent instance must have a unique name so DBOS can correctly resume workflows after a failure or restart. Tools and event stream handlers are not automatically wrapped by DBOS. You can decide how to integrate them: * Decorate with @DBOS.step if the function involves non-determinism or I/O. * Skip the decorator if durability isn't needed, so you avoid the extra DB checkpoint write. * If the function needs to enqueue tasks or invoke other DBOS workflows, run it inside the agent's main workflow (not as a step). Other than that, any agent and toolset will just work! ### Agent Run Context and Dependencies DBOS checkpoints workflow inputs/outputs and step outputs into a database using [ pickle ](https://docs.python.org/3/library/pickle.html). This means you need to make sure [dependencies](../../dependencies/index.html) object provided to [ DBOSAgent.run() ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.dbos.DBOSAgent.run) or [ DBOSAgent.run_sync() ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.dbos.DBOSAgent.run_sync), and tool outputs can be serialized using pickle. You may also want to keep the inputs and outputs small (under ~2 MB). PostgreSQL and SQLite support up to 1 GB per field, but large objects may impact performance. ### Streaming Because DBOS cannot stream output directly to the workflow or step call site, [ Agent.run_stream() ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream) and [ Agent.run_stream_events() ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream_events) are not supported when running inside of a DBOS workflow. Instead, you can implement streaming by setting an [ event_stream_handler ](../../api/agent/index.html#pydantic_ai.agent.EventStreamHandler) on the Agent or DBOSAgent instance and using [ DBOSAgent.run() ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.dbos.DBOSAgent.run). The event stream handler function will receive the agent [run context](../../api/tools/index.html#pydantic_ai.tools.RunContext) and an async iterable of events from the model's streaming response and the agent's execution of tools. For examples, see the [streaming docs](../../agents/index.html#streaming-all-events). ## Step Configuration You can customize DBOS step behavior, such as retries, by passing [ StepConfig ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.dbos.StepConfig) objects to the DBOSAgent constructor: * mcp_step_config : The DBOS step config to use for MCP server communication. No retries if omitted. * model_step_config : The DBOS step config to use for model request steps. No retries if omitted. For custom tools, you can annotate them directly with [ @DBOS.step ](https://docs.dbos.dev/python/reference/decorators#step) or [ @DBOS.workflow ](https://docs.dbos.dev/python/reference/decorators#workflow) decorators as needed. These decorators have no effect outside DBOS workflows, so tools remain usable in non-DBOS agents. ## Step Retries On top of the automatic retries for request failures that DBOS will perform, Pydantic AI and various provider API clients also have their own request retry logic. Enabling these at the same time may cause the request to be retried more often than expected, with improper Retry-After handling. When using DBOS, it's recommended to not use [HTTP Request Retries](../../retries/index.html) and to turn off your provider API client's own retry logic, for example by setting max_retries=0 on a [custom OpenAIProvider API client](../../models/openai/index.html#custom-openai-client). You can customize DBOS's retry policy using [step configuration](index.html#step-configuration). ## Observability with Logfire DBOS can be configured to generate OpenTelemetry spans for each workflow and step execution, and Pydantic AI emits spans for each agent run, model request, and tool invocation. You can send these spans to [Pydantic Logfire](../../logfire/index.html) to get a full, end-to-end view of what's happening in your application. For more information about DBOS logging and tracing, please see the [DBOS docs](https://docs.dbos.dev/python/tutorials/logging-and-tracing) for details.", "url": "https://ai.pydantic.dev/durable_execution/dbos/index.html#durable-execution-with-dbos", "page": "durable_execution/dbos/index.html", "source_site": "pydantic_ai"}
{"title": "Durable Execution", "anchor": "durable-execution", "heading_level": 2, "md_text": "DBOS workflows make your program **durable** by checkpointing its state in a database. If your program ever fails, when it restarts all your workflows will automatically resume from the last completed step. * **Workflows** must be deterministic and generally cannot include I/O. * **Steps** may perform I/O (network, disk, API calls). If a step fails, it restarts from the beginning. Every workflow input and step output is durably stored in the system database. When workflow execution fails, whether from crashes, network issues, or server restarts, DBOS leverages these checkpoints to recover workflows from their last completed step. DBOS **queues** provide durable, database-backed alternatives to systems like Celery or BullMQ, supporting features such as concurrency limits, rate limits, timeouts, and prioritization. See the [DBOS docs](https://docs.dbos.dev/architecture) for details. The diagram below shows the overall architecture of an agentic application in DBOS. DBOS runs fully in-process as a library. Functions remain normal Python functions but are checkpointed into a database (Postgres or SQLite). See the [DBOS documentation](https://docs.dbos.dev/architecture) for more information.", "url": "https://ai.pydantic.dev/durable_execution/dbos/index.html#durable-execution", "page": "durable_execution/dbos/index.html", "source_site": "pydantic_ai"}
{"title": "Durable Agent", "anchor": "durable-agent", "heading_level": 2, "md_text": "Any agent can be wrapped in a [ DBOSAgent ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.dbos.DBOSAgent) to get durable execution. DBOSAgent automatically:, * Wraps Agent.run and Agent.run_sync as DBOS workflows. * Wraps [model requests](../../models/overview/index.html) and [MCP communication](../../mcp/client/index.html) as DBOS steps. Custom tool functions and event stream handlers are **not automatically wrapped** by DBOS. If they involve non-deterministic behavior or perform I/O, you should explicitly decorate them with @DBOS.step . The original agent, model, and MCP server can still be used as normal outside the DBOS workflow. Here is a simple but complete example of wrapping an agent for durable execution. All it requires is to install Pydantic AI with the DBOS [open-source library](https://github.com/dbos-inc/dbos-transact-py): pipuv Or if you're using the slim package, you can install it with the dbos optional group: pipuv dbos\\_agent.py 1. Workflows and DBOSAgent must be defined before DBOS.launch() so that recovery can correctly find all workflows. 2. [ DBOSAgent.run() ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.dbos.DBOSAgent.run) works like [ Agent.run() ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run), but runs as a DBOS workflow and executes model requests, decorated tool calls, and MCP communication as DBOS steps. 3. This example uses SQLite. Postgres is recommended for production. 4. The agent's name is used to uniquely identify its workflows. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* Because DBOS workflows need to be defined before calling DBOS.launch() and the DBOSAgent instance automatically registers run and run_sync as workflows, it needs to be defined before calling DBOS.launch() as well. For more information on how to use DBOS in Python applications, see their [Python SDK guide](https://docs.dbos.dev/python/programming-guide).", "url": "https://ai.pydantic.dev/durable_execution/dbos/index.html#durable-agent", "page": "durable_execution/dbos/index.html", "source_site": "pydantic_ai"}
{"title": "DBOS Integration Considerations", "anchor": "dbos-integration-considerations", "heading_level": 2, "md_text": "When using DBOS with Pydantic AI agents, there are a few important considerations to ensure workflows and toolsets behave correctly. ### Agent and Toolset Requirements Each agent instance must have a unique name so DBOS can correctly resume workflows after a failure or restart. Tools and event stream handlers are not automatically wrapped by DBOS. You can decide how to integrate them: * Decorate with @DBOS.step if the function involves non-determinism or I/O. * Skip the decorator if durability isn't needed, so you avoid the extra DB checkpoint write. * If the function needs to enqueue tasks or invoke other DBOS workflows, run it inside the agent's main workflow (not as a step). Other than that, any agent and toolset will just work! ### Agent Run Context and Dependencies DBOS checkpoints workflow inputs/outputs and step outputs into a database using [ pickle ](https://docs.python.org/3/library/pickle.html). This means you need to make sure [dependencies](../../dependencies/index.html) object provided to [ DBOSAgent.run() ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.dbos.DBOSAgent.run) or [ DBOSAgent.run_sync() ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.dbos.DBOSAgent.run_sync), and tool outputs can be serialized using pickle. You may also want to keep the inputs and outputs small (under ~2 MB). PostgreSQL and SQLite support up to 1 GB per field, but large objects may impact performance. ### Streaming Because DBOS cannot stream output directly to the workflow or step call site, [ Agent.run_stream() ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream) and [ Agent.run_stream_events() ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream_events) are not supported when running inside of a DBOS workflow. Instead, you can implement streaming by setting an [ event_stream_handler ](../../api/agent/index.html#pydantic_ai.agent.EventStreamHandler) on the Agent or DBOSAgent instance and using [ DBOSAgent.run() ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.dbos.DBOSAgent.run). The event stream handler function will receive the agent [run context](../../api/tools/index.html#pydantic_ai.tools.RunContext) and an async iterable of events from the model's streaming response and the agent's execution of tools. For examples, see the [streaming docs](../../agents/index.html#streaming-all-events).", "url": "https://ai.pydantic.dev/durable_execution/dbos/index.html#dbos-integration-considerations", "page": "durable_execution/dbos/index.html", "source_site": "pydantic_ai"}
{"title": "Agent and Toolset Requirements", "anchor": "agent-and-toolset-requirements", "heading_level": 3, "md_text": "Each agent instance must have a unique name so DBOS can correctly resume workflows after a failure or restart. Tools and event stream handlers are not automatically wrapped by DBOS. You can decide how to integrate them: * Decorate with @DBOS.step if the function involves non-determinism or I/O. * Skip the decorator if durability isn't needed, so you avoid the extra DB checkpoint write. * If the function needs to enqueue tasks or invoke other DBOS workflows, run it inside the agent's main workflow (not as a step). Other than that, any agent and toolset will just work!", "url": "https://ai.pydantic.dev/durable_execution/dbos/index.html#agent-and-toolset-requirements", "page": "durable_execution/dbos/index.html", "source_site": "pydantic_ai"}
{"title": "Agent Run Context and Dependencies", "anchor": "agent-run-context-and-dependencies", "heading_level": 3, "md_text": "DBOS checkpoints workflow inputs/outputs and step outputs into a database using [ pickle ](https://docs.python.org/3/library/pickle.html). This means you need to make sure [dependencies](../../dependencies/index.html) object provided to [ DBOSAgent.run() ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.dbos.DBOSAgent.run) or [ DBOSAgent.run_sync() ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.dbos.DBOSAgent.run_sync), and tool outputs can be serialized using pickle. You may also want to keep the inputs and outputs small (under ~2 MB). PostgreSQL and SQLite support up to 1 GB per field, but large objects may impact performance.", "url": "https://ai.pydantic.dev/durable_execution/dbos/index.html#agent-run-context-and-dependencies", "page": "durable_execution/dbos/index.html", "source_site": "pydantic_ai"}
{"title": "Streaming", "anchor": "streaming", "heading_level": 3, "md_text": "Because DBOS cannot stream output directly to the workflow or step call site, [ Agent.run_stream() ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream) and [ Agent.run_stream_events() ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream_events) are not supported when running inside of a DBOS workflow. Instead, you can implement streaming by setting an [ event_stream_handler ](../../api/agent/index.html#pydantic_ai.agent.EventStreamHandler) on the Agent or DBOSAgent instance and using [ DBOSAgent.run() ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.dbos.DBOSAgent.run). The event stream handler function will receive the agent [run context](../../api/tools/index.html#pydantic_ai.tools.RunContext) and an async iterable of events from the model's streaming response and the agent's execution of tools. For examples, see the [streaming docs](../../agents/index.html#streaming-all-events).", "url": "https://ai.pydantic.dev/durable_execution/dbos/index.html#streaming", "page": "durable_execution/dbos/index.html", "source_site": "pydantic_ai"}
{"title": "Step Configuration", "anchor": "step-configuration", "heading_level": 2, "md_text": "You can customize DBOS step behavior, such as retries, by passing [ StepConfig ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.dbos.StepConfig) objects to the DBOSAgent constructor: * mcp_step_config : The DBOS step config to use for MCP server communication. No retries if omitted. * model_step_config : The DBOS step config to use for model request steps. No retries if omitted. For custom tools, you can annotate them directly with [ @DBOS.step ](https://docs.dbos.dev/python/reference/decorators#step) or [ @DBOS.workflow ](https://docs.dbos.dev/python/reference/decorators#workflow) decorators as needed. These decorators have no effect outside DBOS workflows, so tools remain usable in non-DBOS agents.", "url": "https://ai.pydantic.dev/durable_execution/dbos/index.html#step-configuration", "page": "durable_execution/dbos/index.html", "source_site": "pydantic_ai"}
{"title": "Step Retries", "anchor": "step-retries", "heading_level": 2, "md_text": "On top of the automatic retries for request failures that DBOS will perform, Pydantic AI and various provider API clients also have their own request retry logic. Enabling these at the same time may cause the request to be retried more often than expected, with improper Retry-After handling. When using DBOS, it's recommended to not use [HTTP Request Retries](../../retries/index.html) and to turn off your provider API client's own retry logic, for example by setting max_retries=0 on a [custom OpenAIProvider API client](../../models/openai/index.html#custom-openai-client). You can customize DBOS's retry policy using [step configuration](index.html#step-configuration).", "url": "https://ai.pydantic.dev/durable_execution/dbos/index.html#step-retries", "page": "durable_execution/dbos/index.html", "source_site": "pydantic_ai"}
{"title": "Observability with Logfire", "anchor": "observability-with-logfire", "heading_level": 2, "md_text": "DBOS can be configured to generate OpenTelemetry spans for each workflow and step execution, and Pydantic AI emits spans for each agent run, model request, and tool invocation. You can send these spans to [Pydantic Logfire](../../logfire/index.html) to get a full, end-to-end view of what's happening in your application. For more information about DBOS logging and tracing, please see the [DBOS docs](https://docs.dbos.dev/python/tutorials/logging-and-tracing) for details.", "url": "https://ai.pydantic.dev/durable_execution/dbos/index.html#observability-with-logfire", "page": "durable_execution/dbos/index.html", "source_site": "pydantic_ai"}
{"title": "Dependencies", "anchor": "dependencies", "heading_level": 1, "md_text": "Pydantic AI uses a dependency injection system to provide data and services to your agent's [system prompts](../agents/index.html#system-prompts), [tools](../tools/index.html) and [output validators](../output/index.html#output-validator-functions). Matching Pydantic AI's design philosophy, our dependency system tries to use existing best practice in Python development rather than inventing esoteric \"magic\", this should make dependencies type-safe, understandable easier to test and ultimately easier to deploy in production. ## Defining Dependencies Dependencies can be any python type. While in simple cases you might be able to pass a single object as a dependency (e.g. an HTTP connection), [dataclasses](https://docs.python.org/3/library/dataclasses.html#module-dataclasses) are generally a convenient container when your dependencies included multiple objects. Here's an example of defining an agent that requires dependencies. (**Note:** dependencies aren't actually used in this example, see [Accessing Dependencies](index.html#accessing-dependencies) below) unused\\_dependencies.py 1. Define a dataclass to hold dependencies. 2. Pass the dataclass type to the deps_type argument of the [ Agent constructor](../api/agent/index.html#pydantic_ai.agent.Agent.__init__). **Note**: we're passing the type here, NOT an instance, this parameter is not actually used at runtime, it's here so we can get full type checking of the agent. 3. When running the agent, pass an instance of the dataclass to the deps parameter. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* ## Accessing Dependencies Dependencies are accessed through the [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext) type, this should be the first parameter of system prompt functions etc. system\\_prompt\\_dependencies.py 1. [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext) may optionally be passed to a [ system_prompt ](../api/agent/index.html#pydantic_ai.agent.Agent.system_prompt) function as the only argument. 2. [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext) is parameterized with the type of the dependencies, if this type is incorrect, static type checkers will raise an error. 3. Access dependencies through the [ .deps ](../api/tools/index.html#pydantic_ai.tools.RunContext.deps) attribute. 4. Access dependencies through the [ .deps ](../api/tools/index.html#pydantic_ai.tools.RunContext.deps) attribute. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* ### Asynchronous vs. Synchronous dependencies [System prompt functions](../agents/index.html#system-prompts), [function tools](../tools/index.html) and [output validators](../output/index.html#output-validator-functions) are all run in the async context of an agent run. If these functions are not coroutines (e.g. async def ) they are called with [ run_in_executor ](https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.run_in_executor) in a thread pool, it's therefore marginally preferable to use async methods where dependencies perform IO, although synchronous dependencies should work fine too. run vs. run_sync and Asynchronous vs. Synchronous dependencies Whether you use synchronous or asynchronous dependencies, is completely independent of whether you use run or run_sync  run_sync is just a wrapper around run and agents are always run in an async context. Here's the same example as above, but with a synchronous dependency: sync\\_dependencies.py 1. Here we use a synchronous httpx.Client instead of an asynchronous httpx.AsyncClient . 2. To match the synchronous dependency, the system prompt function is now a plain function, not a coroutine. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* ## Full Example As well as system prompts, dependencies can be used in [tools](../tools/index.html) and [output validators](../output/index.html#output-validator-functions). full\\_example.py 1. To pass RunContext to a tool, use the [ tool ](../api/agent/index.html#pydantic_ai.agent.Agent.tool) decorator. 2. RunContext may optionally be passed to a [ output_validator ](../api/agent/index.html#pydantic_ai.agent.Agent.output_validator) function as the first argument. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* ## Overriding Dependencies When testing agents, it's useful to be able to customise dependencies. While this can sometimes be done by calling the agent directly within unit tests, we can also override dependencies while calling application code which in turn calls the agent. This is done via the [ override ](../api/agent/index.html#pydantic_ai.agent.Agent.override) method on the agent. joke\\_app.py 1. Define a method on the dependency to make the system prompt easier to customise. 2. Call the system prompt factory from within the system prompt function. 3. Application code that calls the agent, in a real application this might be an API endpoint. 4. Call the agent from within the application code, in a real application this call might be deep within a call stack. Note app_deps here will NOT be used when deps are overridden. *(This example is complete, it can be run \"as is\")* test\\_joke\\_app.py 1. Define a subclass of MyDeps in tests to customise the system prompt factory. 2. Create an instance of the test dependency, we don't need to pass an http_client here as it's not used. 3. Override the dependencies of the agent for the duration of the with block, test_deps will be used when the agent is run. 4. Now we can safely call our application code, the agent will use the overridden dependencies. ## Examples The following examples demonstrate how to use dependencies in Pydantic AI: * [Weather Agent](../examples/weather-agent/index.html) * [SQL Generation](../examples/sql-gen/index.html) * [RAG](../examples/rag/index.html)", "url": "https://ai.pydantic.dev/dependencies/index.html#dependencies", "page": "dependencies/index.html", "source_site": "pydantic_ai"}
{"title": "Defining Dependencies", "anchor": "defining-dependencies", "heading_level": 2, "md_text": "Dependencies can be any python type. While in simple cases you might be able to pass a single object as a dependency (e.g. an HTTP connection), [dataclasses](https://docs.python.org/3/library/dataclasses.html#module-dataclasses) are generally a convenient container when your dependencies included multiple objects. Here's an example of defining an agent that requires dependencies. (**Note:** dependencies aren't actually used in this example, see [Accessing Dependencies](index.html#accessing-dependencies) below) unused\\_dependencies.py 1. Define a dataclass to hold dependencies. 2. Pass the dataclass type to the deps_type argument of the [ Agent constructor](../api/agent/index.html#pydantic_ai.agent.Agent.__init__). **Note**: we're passing the type here, NOT an instance, this parameter is not actually used at runtime, it's here so we can get full type checking of the agent. 3. When running the agent, pass an instance of the dataclass to the deps parameter. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/dependencies/index.html#defining-dependencies", "page": "dependencies/index.html", "source_site": "pydantic_ai"}
{"title": "Accessing Dependencies", "anchor": "accessing-dependencies", "heading_level": 2, "md_text": "Dependencies are accessed through the [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext) type, this should be the first parameter of system prompt functions etc. system\\_prompt\\_dependencies.py 1. [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext) may optionally be passed to a [ system_prompt ](../api/agent/index.html#pydantic_ai.agent.Agent.system_prompt) function as the only argument. 2. [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext) is parameterized with the type of the dependencies, if this type is incorrect, static type checkers will raise an error. 3. Access dependencies through the [ .deps ](../api/tools/index.html#pydantic_ai.tools.RunContext.deps) attribute. 4. Access dependencies through the [ .deps ](../api/tools/index.html#pydantic_ai.tools.RunContext.deps) attribute. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* ### Asynchronous vs. Synchronous dependencies [System prompt functions](../agents/index.html#system-prompts), [function tools](../tools/index.html) and [output validators](../output/index.html#output-validator-functions) are all run in the async context of an agent run. If these functions are not coroutines (e.g. async def ) they are called with [ run_in_executor ](https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.run_in_executor) in a thread pool, it's therefore marginally preferable to use async methods where dependencies perform IO, although synchronous dependencies should work fine too. run vs. run_sync and Asynchronous vs. Synchronous dependencies Whether you use synchronous or asynchronous dependencies, is completely independent of whether you use run or run_sync  run_sync is just a wrapper around run and agents are always run in an async context. Here's the same example as above, but with a synchronous dependency: sync\\_dependencies.py 1. Here we use a synchronous httpx.Client instead of an asynchronous httpx.AsyncClient . 2. To match the synchronous dependency, the system prompt function is now a plain function, not a coroutine. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/dependencies/index.html#accessing-dependencies", "page": "dependencies/index.html", "source_site": "pydantic_ai"}
{"title": "Asynchronous vs. Synchronous dependencies", "anchor": "asynchronous-vs-synchronous-dependencies", "heading_level": 3, "md_text": "[System prompt functions](../agents/index.html#system-prompts), [function tools](../tools/index.html) and [output validators](../output/index.html#output-validator-functions) are all run in the async context of an agent run. If these functions are not coroutines (e.g. async def ) they are called with [ run_in_executor ](https://docs.python.org/3/library/asyncio-eventloop.html#asyncio.loop.run_in_executor) in a thread pool, it's therefore marginally preferable to use async methods where dependencies perform IO, although synchronous dependencies should work fine too. run vs. run_sync and Asynchronous vs. Synchronous dependencies Whether you use synchronous or asynchronous dependencies, is completely independent of whether you use run or run_sync  run_sync is just a wrapper around run and agents are always run in an async context. Here's the same example as above, but with a synchronous dependency: sync\\_dependencies.py 1. Here we use a synchronous httpx.Client instead of an asynchronous httpx.AsyncClient . 2. To match the synchronous dependency, the system prompt function is now a plain function, not a coroutine. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/dependencies/index.html#asynchronous-vs-synchronous-dependencies", "page": "dependencies/index.html", "source_site": "pydantic_ai"}
{"title": "Full Example", "anchor": "full-example", "heading_level": 2, "md_text": "As well as system prompts, dependencies can be used in [tools](../tools/index.html) and [output validators](../output/index.html#output-validator-functions). full\\_example.py 1. To pass RunContext to a tool, use the [ tool ](../api/agent/index.html#pydantic_ai.agent.Agent.tool) decorator. 2. RunContext may optionally be passed to a [ output_validator ](../api/agent/index.html#pydantic_ai.agent.Agent.output_validator) function as the first argument. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/dependencies/index.html#full-example", "page": "dependencies/index.html", "source_site": "pydantic_ai"}
{"title": "Overriding Dependencies", "anchor": "overriding-dependencies", "heading_level": 2, "md_text": "When testing agents, it's useful to be able to customise dependencies. While this can sometimes be done by calling the agent directly within unit tests, we can also override dependencies while calling application code which in turn calls the agent. This is done via the [ override ](../api/agent/index.html#pydantic_ai.agent.Agent.override) method on the agent. joke\\_app.py 1. Define a method on the dependency to make the system prompt easier to customise. 2. Call the system prompt factory from within the system prompt function. 3. Application code that calls the agent, in a real application this might be an API endpoint. 4. Call the agent from within the application code, in a real application this call might be deep within a call stack. Note app_deps here will NOT be used when deps are overridden. *(This example is complete, it can be run \"as is\")* test\\_joke\\_app.py 1. Define a subclass of MyDeps in tests to customise the system prompt factory. 2. Create an instance of the test dependency, we don't need to pass an http_client here as it's not used. 3. Override the dependencies of the agent for the duration of the with block, test_deps will be used when the agent is run. 4. Now we can safely call our application code, the agent will use the overridden dependencies.", "url": "https://ai.pydantic.dev/dependencies/index.html#overriding-dependencies", "page": "dependencies/index.html", "source_site": "pydantic_ai"}
{"title": "Examples", "anchor": "examples", "heading_level": 2, "md_text": "The following examples demonstrate how to use dependencies in Pydantic AI: * [Weather Agent](../examples/weather-agent/index.html) * [SQL Generation](../examples/sql-gen/index.html) * [RAG](../examples/rag/index.html)", "url": "https://ai.pydantic.dev/dependencies/index.html#examples", "page": "dependencies/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_graph", "anchor": "pydantic_graph", "heading_level": 1, "md_text": "### Graph dataclass Bases: Generic[StateT, DepsT, RunEndT] Definition of a graph. In pydantic-graph , a graph is a collection of nodes that can be run in sequence. The nodes define their outgoing edges  e.g. which nodes may be run next, and thereby the structure of the graph. Here's a very simple example of a graph which increments a number by 1, but makes sure the number is never 42 at the end. never\\_42.py *(This example is complete, it can be run \"as is\")* See [ run ](index.html#pydantic_graph.graph.Graph.run) For an example of running graph, and [ mermaid_code ](index.html#pydantic_graph.graph.Graph.mermaid_code) for an example of generating a mermaid diagram from the graph. Source code in pydantic_graph/pydantic_graph/graph.py #### \\_\\_init\\_\\_ Create a graph from a sequence of nodes. Parameters: Source code in pydantic_graph/pydantic_graph/graph.py #### run async Run the graph from a starting node until it ends. Parameters: Returns: Here's an example of running the graph from [above](index.html#pydantic_graph.graph.Graph): run\\_never\\_42.py Source code in pydantic_graph/pydantic_graph/graph.py #### run\\_sync Synchronously run the graph. This is a convenience method that wraps [ self.run ](index.html#pydantic_graph.graph.Graph.run) with loop.run_until_complete(...) . You therefore can't use this method inside async code or if there's an active event loop. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/graph.py #### iter async A contextmanager which can be used to iterate over the graph's nodes as they are executed. This method returns a GraphRun object which can be used to async-iterate over the nodes of this Graph as they are executed. This is the API to use if you want to record or interact with the nodes as the graph execution unfolds. The GraphRun can also be used to manually drive the graph execution by calling [ GraphRun.next ](index.html#pydantic_graph.graph.GraphRun.next). The GraphRun provides access to the full run history, state, deps, and the final result of the run once it has completed. For more details, see the API documentation of [ GraphRun ](index.html#pydantic_graph.graph.GraphRun). Parameters: Returns: A GraphRun that can be async iterated over to drive the graph to completion. Source code in pydantic_graph/pydantic_graph/graph.py #### iter\\_from\\_persistence async A contextmanager to iterate over the graph's nodes as they are executed, created from a persistence object. This method has similar functionality to [ iter ](index.html#pydantic_graph.graph.Graph.iter), but instead of passing the node to run, it will restore the node and state from state persistence. Parameters: Returns: A GraphRun that can be async iterated over to drive the graph to completion. Source code in pydantic_graph/pydantic_graph/graph.py #### initialize async Initialize a new graph run in persistence without running it. This is useful if you want to set up a graph run to be run later, e.g. via [ iter_from_persistence ](index.html#pydantic_graph.graph.Graph.iter_from_persistence). Parameters: Source code in pydantic_graph/pydantic_graph/graph.py #### mermaid\\_code Generate a diagram representing the graph as [mermaid](https://mermaid.js.org/) diagram. This method calls [ pydantic_graph.mermaid.generate_code ](../mermaid/index.html#pydantic_graph.mermaid.generate_code). Parameters: Returns: Here's an example of generating a diagram for the graph from [above](index.html#pydantic_graph.graph.Graph): mermaid\\_never\\_42.py The rendered diagram will look like this: Source code in pydantic_graph/pydantic_graph/graph.py #### mermaid\\_image Generate a diagram representing the graph as an image. The format and diagram can be customized using kwargs , see [ pydantic_graph.mermaid.MermaidConfig ](../mermaid/index.html#pydantic_graph.mermaid.MermaidConfig). Uses external service This method makes a request to [mermaid.ink](https://mermaid.ink) to render the image, mermaid.ink is a free service not affiliated with Pydantic. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/graph.py #### mermaid\\_save Generate a diagram representing the graph and save it as an image. The format and diagram can be customized using kwargs , see [ pydantic_graph.mermaid.MermaidConfig ](../mermaid/index.html#pydantic_graph.mermaid.MermaidConfig). Uses external service This method makes a request to [mermaid.ink](https://mermaid.ink) to render the image, mermaid.ink is a free service not affiliated with Pydantic. Parameters: Source code in pydantic_graph/pydantic_graph/graph.py #### get\\_nodes Get the nodes in the graph. Source code in pydantic_graph/pydantic_graph/graph.py ### GraphRun Bases: Generic[StateT, DepsT, RunEndT] A stateful, async-iterable run of a [ Graph ](index.html#pydantic_graph.graph.Graph). You typically get a GraphRun instance from calling async with [my_graph.iter(...)][pydantic_graph.graph.Graph.iter] as graph_run: . That gives you the ability to iterate through nodes as they run, either by async for iteration or by repeatedly calling .next(...) . Here's an example of iterating over the graph from [above](index.html#pydantic_graph.graph.Graph): iter\\_never\\_42.py See the [ GraphRun.next documentation](index.html#pydantic_graph.graph.GraphRun.next) for an example of how to manually drive the graph run. Source code in pydantic_graph/pydantic_graph/graph.py #### \\_\\_init\\_\\_ Create a new run for a given graph, starting at the specified node. Typically, you'll use [ Graph.iter ](index.html#pydantic_graph.graph.Graph.iter) rather than calling this directly. Parameters: Source code in pydantic_graph/pydantic_graph/graph.py #### next\\_node property The next node that will be run in the graph. This is the next node that will be used during async iteration, or if a node is not passed to self.next(...) . #### result property The final result of the graph run if the run is completed, otherwise None . #### next async Manually drive the graph run by passing in the node you want to run next. This lets you inspect or mutate the node before continuing execution, or skip certain nodes under dynamic conditions. The graph run should stop when you return an [ End ](../nodes/index.html#pydantic_graph.nodes.End) node. Here's an example of using next to drive the graph from [above](index.html#pydantic_graph.graph.Graph): next\\_never\\_42.py Parameters: Returns: Source code in pydantic_graph/pydantic_graph/graph.py #### \\_\\_anext\\_\\_ async Use the last returned node as the input to Graph.next . Source code in pydantic_graph/pydantic_graph/graph.py ### GraphRunResult dataclass Bases: Generic[StateT, RunEndT] The final result of running a graph. Source code in pydantic_graph/pydantic_graph/graph.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/graph/index.html#pydantic_graph", "page": "api/pydantic_graph/graph/index.html", "source_site": "pydantic_ai"}
{"title": "Graph dataclass", "anchor": "pydantic_graph.graph.Graph", "heading_level": 3, "md_text": "Bases: Generic[StateT, DepsT, RunEndT] Definition of a graph. In pydantic-graph , a graph is a collection of nodes that can be run in sequence. The nodes define their outgoing edges  e.g. which nodes may be run next, and thereby the structure of the graph. Here's a very simple example of a graph which increments a number by 1, but makes sure the number is never 42 at the end. never\\_42.py *(This example is complete, it can be run \"as is\")* See [ run ](index.html#pydantic_graph.graph.Graph.run) For an example of running graph, and [ mermaid_code ](index.html#pydantic_graph.graph.Graph.mermaid_code) for an example of generating a mermaid diagram from the graph. Source code in pydantic_graph/pydantic_graph/graph.py #### \\_\\_init\\_\\_ Create a graph from a sequence of nodes. Parameters: Source code in pydantic_graph/pydantic_graph/graph.py #### run async Run the graph from a starting node until it ends. Parameters: Returns: Here's an example of running the graph from [above](index.html#pydantic_graph.graph.Graph): run\\_never\\_42.py Source code in pydantic_graph/pydantic_graph/graph.py #### run\\_sync Synchronously run the graph. This is a convenience method that wraps [ self.run ](index.html#pydantic_graph.graph.Graph.run) with loop.run_until_complete(...) . You therefore can't use this method inside async code or if there's an active event loop. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/graph.py #### iter async A contextmanager which can be used to iterate over the graph's nodes as they are executed. This method returns a GraphRun object which can be used to async-iterate over the nodes of this Graph as they are executed. This is the API to use if you want to record or interact with the nodes as the graph execution unfolds. The GraphRun can also be used to manually drive the graph execution by calling [ GraphRun.next ](index.html#pydantic_graph.graph.GraphRun.next). The GraphRun provides access to the full run history, state, deps, and the final result of the run once it has completed. For more details, see the API documentation of [ GraphRun ](index.html#pydantic_graph.graph.GraphRun). Parameters: Returns: A GraphRun that can be async iterated over to drive the graph to completion. Source code in pydantic_graph/pydantic_graph/graph.py #### iter\\_from\\_persistence async A contextmanager to iterate over the graph's nodes as they are executed, created from a persistence object. This method has similar functionality to [ iter ](index.html#pydantic_graph.graph.Graph.iter), but instead of passing the node to run, it will restore the node and state from state persistence. Parameters: Returns: A GraphRun that can be async iterated over to drive the graph to completion. Source code in pydantic_graph/pydantic_graph/graph.py #### initialize async Initialize a new graph run in persistence without running it. This is useful if you want to set up a graph run to be run later, e.g. via [ iter_from_persistence ](index.html#pydantic_graph.graph.Graph.iter_from_persistence). Parameters: Source code in pydantic_graph/pydantic_graph/graph.py #### mermaid\\_code Generate a diagram representing the graph as [mermaid](https://mermaid.js.org/) diagram. This method calls [ pydantic_graph.mermaid.generate_code ](../mermaid/index.html#pydantic_graph.mermaid.generate_code). Parameters: Returns: Here's an example of generating a diagram for the graph from [above](index.html#pydantic_graph.graph.Graph): mermaid\\_never\\_42.py The rendered diagram will look like this: Source code in pydantic_graph/pydantic_graph/graph.py #### mermaid\\_image Generate a diagram representing the graph as an image. The format and diagram can be customized using kwargs , see [ pydantic_graph.mermaid.MermaidConfig ](../mermaid/index.html#pydantic_graph.mermaid.MermaidConfig). Uses external service This method makes a request to [mermaid.ink](https://mermaid.ink) to render the image, mermaid.ink is a free service not affiliated with Pydantic. Parameters: Returns: Source code in pydantic_graph/pydantic_graph/graph.py #### mermaid\\_save Generate a diagram representing the graph and save it as an image. The format and diagram can be customized using kwargs , see [ pydantic_graph.mermaid.MermaidConfig ](../mermaid/index.html#pydantic_graph.mermaid.MermaidConfig). Uses external service This method makes a request to [mermaid.ink](https://mermaid.ink) to render the image, mermaid.ink is a free service not affiliated with Pydantic. Parameters: Source code in pydantic_graph/pydantic_graph/graph.py #### get\\_nodes Get the nodes in the graph. Source code in pydantic_graph/pydantic_graph/graph.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph", "page": "api/pydantic_graph/graph/index.html", "source_site": "pydantic_ai"}
{"title": "GraphRun", "anchor": "pydantic_graph.graph.GraphRun", "heading_level": 3, "md_text": "Bases: Generic[StateT, DepsT, RunEndT] A stateful, async-iterable run of a [ Graph ](index.html#pydantic_graph.graph.Graph). You typically get a GraphRun instance from calling async with [my_graph.iter(...)][pydantic_graph.graph.Graph.iter] as graph_run: . That gives you the ability to iterate through nodes as they run, either by async for iteration or by repeatedly calling .next(...) . Here's an example of iterating over the graph from [above](index.html#pydantic_graph.graph.Graph): iter\\_never\\_42.py See the [ GraphRun.next documentation](index.html#pydantic_graph.graph.GraphRun.next) for an example of how to manually drive the graph run. Source code in pydantic_graph/pydantic_graph/graph.py #### \\_\\_init\\_\\_ Create a new run for a given graph, starting at the specified node. Typically, you'll use [ Graph.iter ](index.html#pydantic_graph.graph.Graph.iter) rather than calling this directly. Parameters: Source code in pydantic_graph/pydantic_graph/graph.py #### next\\_node property The next node that will be run in the graph. This is the next node that will be used during async iteration, or if a node is not passed to self.next(...) . #### result property The final result of the graph run if the run is completed, otherwise None . #### next async Manually drive the graph run by passing in the node you want to run next. This lets you inspect or mutate the node before continuing execution, or skip certain nodes under dynamic conditions. The graph run should stop when you return an [ End ](../nodes/index.html#pydantic_graph.nodes.End) node. Here's an example of using next to drive the graph from [above](index.html#pydantic_graph.graph.Graph): next\\_never\\_42.py Parameters: Returns: Source code in pydantic_graph/pydantic_graph/graph.py #### \\_\\_anext\\_\\_ async Use the last returned node as the input to Graph.next . Source code in pydantic_graph/pydantic_graph/graph.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/graph/index.html#pydantic_graph.graph.GraphRun", "page": "api/pydantic_graph/graph/index.html", "source_site": "pydantic_ai"}
{"title": "GraphRunResult dataclass", "anchor": "pydantic_graph.graph.GraphRunResult", "heading_level": 3, "md_text": "Bases: Generic[StateT, RunEndT] The final result of running a graph. Source code in pydantic_graph/pydantic_graph/graph.py", "url": "https://ai.pydantic.dev/api/pydantic_graph/graph/index.html#pydantic_graph.graph.GraphRunResult", "page": "api/pydantic_graph/graph/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.tools", "anchor": "pydantic_aitools", "heading_level": 1, "md_text": "### AgentDepsT module-attribute Type variable for agent dependencies. ### RunContext dataclass Bases: Generic[AgentDepsT] Information about the current call. Source code in pydantic_ai_slim/pydantic_ai/_run_context.py #### deps instance-attribute Dependencies for the agent. #### model instance-attribute The model used in this run. #### usage instance-attribute LLM usage associated with the run. #### prompt class-attribute instance-attribute The original user prompt passed to the run. #### messages class-attribute instance-attribute Messages exchanged in the conversation so far. #### tracer class-attribute instance-attribute The tracer to use for tracing the run. #### trace\\_include\\_content class-attribute instance-attribute Whether to include the content of the messages in the trace. #### instrumentation\\_version class-attribute instance-attribute Instrumentation settings version, if instrumentation is enabled. #### retries class-attribute instance-attribute Number of retries for each tool so far. #### tool\\_call\\_id class-attribute instance-attribute The ID of the tool call. #### tool\\_name class-attribute instance-attribute Name of the tool being called. #### retry class-attribute instance-attribute Number of retries of this tool so far. #### max\\_retries class-attribute instance-attribute The maximum number of retries of this tool. #### run\\_step class-attribute instance-attribute The current step in the run. #### tool\\_call\\_approved class-attribute instance-attribute Whether a tool call that required approval has now been approved. #### last\\_attempt property Whether this is the last attempt at running this tool before an error is raised. ### ToolParams module-attribute Retrieval function param spec. ### SystemPromptFunc module-attribute A function that may or maybe not take RunContext as an argument, and may or may not be async. Usage SystemPromptFunc[AgentDepsT] . ### ToolFuncContext module-attribute A tool function that takes RunContext as the first argument. Usage ToolContextFunc[AgentDepsT, ToolParams] . ### ToolFuncPlain module-attribute A tool function that does not take RunContext as the first argument. Usage ToolPlainFunc[ToolParams] . ### ToolFuncEither module-attribute Either kind of tool function. This is just a union of [ ToolFuncContext ](index.html#pydantic_ai.tools.ToolFuncContext) and [ ToolFuncPlain ](index.html#pydantic_ai.tools.ToolFuncPlain). Usage ToolFuncEither[AgentDepsT, ToolParams] . ### ToolPrepareFunc module-attribute Definition of a function that can prepare a tool definition at call time. See [tool docs](../../tools-advanced/index.html#tool-prepare) for more information. Example  here only_if_42 is valid as a ToolPrepareFunc : Usage ToolPrepareFunc[AgentDepsT] . ### ToolsPrepareFunc module-attribute Definition of a function that can prepare the tool definition of all tools for each step. This is useful if you want to customize the definition of multiple tools or you want to register a subset of tools for a given step. Example  here turn_on_strict_if_openai is valid as a ToolsPrepareFunc : Usage ToolsPrepareFunc[AgentDepsT] . ### DocstringFormat module-attribute Supported docstring formats. * 'google'  [Google-style](https://google.github.io/styleguide/pyguide.html#381-docstrings) docstrings. * 'numpy'  [Numpy-style](https://numpydoc.readthedocs.io/en/latest/format.html) docstrings. * 'sphinx'  [Sphinx-style](https://sphinx-rtd-tutorial.readthedocs.io/en/latest/docstrings.html#the-sphinx-docstring-format) docstrings. * 'auto'  Automatically infer the format based on the structure of the docstring. ### DeferredToolRequests dataclass Tool calls that require approval or external execution. This can be used as an agent's output_type and will be used as the output of the agent run if the model called any deferred tools. Results can be passed to the next agent run using a [ DeferredToolResults ](index.html#pydantic_ai.tools.DeferredToolResults) object with the same tool call IDs. See [deferred tools docs](../../deferred-tools/index.html#deferred-tools) for more information. Source code in pydantic_ai_slim/pydantic_ai/tools.py #### calls class-attribute instance-attribute Tool calls that require external execution. #### approvals class-attribute instance-attribute Tool calls that require human-in-the-loop approval. ### ToolApproved dataclass Indicates that a tool call has been approved and that the tool function should be executed. Source code in pydantic_ai_slim/pydantic_ai/tools.py #### override\\_args class-attribute instance-attribute Optional tool call arguments to use instead of the original arguments. ### ToolDenied dataclass Indicates that a tool call has been denied and that a denial message should be returned to the model. Source code in pydantic_ai_slim/pydantic_ai/tools.py #### message class-attribute instance-attribute The message to return to the model. ### DeferredToolResults dataclass Results for deferred tool calls from a previous run that required approval or external execution. The tool call IDs need to match those from the [ DeferredToolRequests ](../output/index.html#pydantic_ai.output.DeferredToolRequests) output object from the previous run. See [deferred tools docs](../../deferred-tools/index.html#deferred-tools) for more information. Source code in pydantic_ai_slim/pydantic_ai/tools.py #### calls class-attribute instance-attribute Map of tool call IDs to results for tool calls that required external execution. #### approvals class-attribute instance-attribute Map of tool call IDs to results for tool calls that required human-in-the-loop approval. ### Tool dataclass Bases: Generic[AgentDepsT] A tool function for an agent. Source code in pydantic_ai_slim/pydantic_ai/tools.py #### \\_\\_init\\_\\_ Create a new tool instance. Example usage: or with a custom prepare method: Parameters: Source code in pydantic_ai_slim/pydantic_ai/tools.py #### function\\_schema instance-attribute The base JSON schema for the tool's parameters. This schema may be modified by the prepare function or by the Model class prior to including it in an API request. #### from\\_schema classmethod Creates a Pydantic tool from a function and a JSON schema. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/tools.py #### prepare\\_tool\\_def async Get the tool definition. By default, this method creates a tool definition, then either returns it, or calls self.prepare if it's set. Returns: Source code in pydantic_ai_slim/pydantic_ai/tools.py ### ObjectJsonSchema module-attribute Type representing JSON schema of an object, e.g. where \"type\": \"object\" . This type is used to define tools parameters (aka arguments) in [ToolDefinition](index.html#pydantic_ai.tools.ToolDefinition). With PEP-728 this should be a TypedDict with type: Literal['object'] , and extra_parts=Any ### ToolDefinition dataclass Definition of a tool passed to a model. This is used for both function tools and output tools. Source code in pydantic_ai_slim/pydantic_ai/tools.py #### name instance-attribute The name of the tool. #### parameters\\_json\\_schema class-attribute instance-attribute The JSON schema for the tool's parameters. #### description class-attribute instance-attribute The description of the tool. #### outer\\_typed\\_dict\\_key class-attribute instance-attribute The key in the outer [TypedDict] that wraps an output tool. This will only be set for output tools which don't have an object JSON schema. #### strict class-attribute instance-attribute Whether to enforce (vendor-specific) strict JSON schema validation for tool calls. Setting this to True while using a supported model generally imposes some restrictions on the tool's JSON schema in exchange for guaranteeing the API responses strictly match that schema. When False , the model may be free to generate other properties or types (depending on the vendor). When None (the default), the value will be inferred based on the compatibility of the parameters\\_json\\_schema. Note: this is currently only supported by OpenAI models. #### sequential class-attribute instance-attribute Whether this tool requires a sequential/serial execution environment. #### kind class-attribute instance-attribute The kind of tool: * 'function' : a tool that will be executed by Pydantic AI during an agent run and has its result returned to the model * 'output' : a tool that passes through an output value that ends the run * 'external' : a tool whose result will be produced outside of the Pydantic AI agent run in which it was called, because it depends on an upstream service (or user) or could take longer to generate than it's reasonable to keep the agent process running. See the [tools documentation](../../deferred-tools/index.html#deferred-tools) for more info. * 'unapproved' : a tool that requires human-in-the-loop approval. See the [tools documentation](../../deferred-tools/index.html#human-in-the-loop-tool-approval) for more info. #### metadata class-attribute instance-attribute Tool metadata that can be set by the toolset this tool came from. It is not sent to the model, but can be used for filtering and tool behavior customization. For MCP tools, this contains the meta , annotations , and output_schema fields from the tool definition. #### defer property Whether calls to this tool will be deferred. See the [tools documentation](../../deferred-tools/index.html#deferred-tools) for more info.", "url": "https://ai.pydantic.dev/api/tools/index.html#pydantic_aitools", "page": "api/tools/index.html", "source_site": "pydantic_ai"}
{"title": "AgentDepsT module-attribute", "anchor": "pydantic_ai.tools.AgentDepsT", "heading_level": 3, "md_text": "Type variable for agent dependencies.", "url": "https://ai.pydantic.dev/api/tools/index.html#pydantic_ai.tools.AgentDepsT", "page": "api/tools/index.html", "source_site": "pydantic_ai"}
{"title": "RunContext dataclass", "anchor": "pydantic_ai.tools.RunContext", "heading_level": 3, "md_text": "Bases: Generic[AgentDepsT] Information about the current call. Source code in pydantic_ai_slim/pydantic_ai/_run_context.py #### deps instance-attribute Dependencies for the agent. #### model instance-attribute The model used in this run. #### usage instance-attribute LLM usage associated with the run. #### prompt class-attribute instance-attribute The original user prompt passed to the run. #### messages class-attribute instance-attribute Messages exchanged in the conversation so far. #### tracer class-attribute instance-attribute The tracer to use for tracing the run. #### trace\\_include\\_content class-attribute instance-attribute Whether to include the content of the messages in the trace. #### instrumentation\\_version class-attribute instance-attribute Instrumentation settings version, if instrumentation is enabled. #### retries class-attribute instance-attribute Number of retries for each tool so far. #### tool\\_call\\_id class-attribute instance-attribute The ID of the tool call. #### tool\\_name class-attribute instance-attribute Name of the tool being called. #### retry class-attribute instance-attribute Number of retries of this tool so far. #### max\\_retries class-attribute instance-attribute The maximum number of retries of this tool. #### run\\_step class-attribute instance-attribute The current step in the run. #### tool\\_call\\_approved class-attribute instance-attribute Whether a tool call that required approval has now been approved. #### last\\_attempt property Whether this is the last attempt at running this tool before an error is raised.", "url": "https://ai.pydantic.dev/api/tools/index.html#pydantic_ai.tools.RunContext", "page": "api/tools/index.html", "source_site": "pydantic_ai"}
{"title": "ToolParams module-attribute", "anchor": "pydantic_ai.tools.ToolParams", "heading_level": 3, "md_text": "Retrieval function param spec.", "url": "https://ai.pydantic.dev/api/tools/index.html#pydantic_ai.tools.ToolParams", "page": "api/tools/index.html", "source_site": "pydantic_ai"}
{"title": "SystemPromptFunc module-attribute", "anchor": "pydantic_ai.tools.SystemPromptFunc", "heading_level": 3, "md_text": "A function that may or maybe not take RunContext as an argument, and may or may not be async. Usage SystemPromptFunc[AgentDepsT] .", "url": "https://ai.pydantic.dev/api/tools/index.html#pydantic_ai.tools.SystemPromptFunc", "page": "api/tools/index.html", "source_site": "pydantic_ai"}
{"title": "ToolFuncContext module-attribute", "anchor": "pydantic_ai.tools.ToolFuncContext", "heading_level": 3, "md_text": "A tool function that takes RunContext as the first argument. Usage ToolContextFunc[AgentDepsT, ToolParams] .", "url": "https://ai.pydantic.dev/api/tools/index.html#pydantic_ai.tools.ToolFuncContext", "page": "api/tools/index.html", "source_site": "pydantic_ai"}
{"title": "ToolFuncPlain module-attribute", "anchor": "pydantic_ai.tools.ToolFuncPlain", "heading_level": 3, "md_text": "A tool function that does not take RunContext as the first argument. Usage ToolPlainFunc[ToolParams] .", "url": "https://ai.pydantic.dev/api/tools/index.html#pydantic_ai.tools.ToolFuncPlain", "page": "api/tools/index.html", "source_site": "pydantic_ai"}
{"title": "ToolFuncEither module-attribute", "anchor": "pydantic_ai.tools.ToolFuncEither", "heading_level": 3, "md_text": "Either kind of tool function. This is just a union of [ ToolFuncContext ](index.html#pydantic_ai.tools.ToolFuncContext) and [ ToolFuncPlain ](index.html#pydantic_ai.tools.ToolFuncPlain). Usage ToolFuncEither[AgentDepsT, ToolParams] .", "url": "https://ai.pydantic.dev/api/tools/index.html#pydantic_ai.tools.ToolFuncEither", "page": "api/tools/index.html", "source_site": "pydantic_ai"}
{"title": "ToolPrepareFunc module-attribute", "anchor": "pydantic_ai.tools.ToolPrepareFunc", "heading_level": 3, "md_text": "Definition of a function that can prepare a tool definition at call time. See [tool docs](../../tools-advanced/index.html#tool-prepare) for more information. Example  here only_if_42 is valid as a ToolPrepareFunc : Usage ToolPrepareFunc[AgentDepsT] .", "url": "https://ai.pydantic.dev/api/tools/index.html#pydantic_ai.tools.ToolPrepareFunc", "page": "api/tools/index.html", "source_site": "pydantic_ai"}
{"title": "ToolsPrepareFunc module-attribute", "anchor": "pydantic_ai.tools.ToolsPrepareFunc", "heading_level": 3, "md_text": "Definition of a function that can prepare the tool definition of all tools for each step. This is useful if you want to customize the definition of multiple tools or you want to register a subset of tools for a given step. Example  here turn_on_strict_if_openai is valid as a ToolsPrepareFunc : Usage ToolsPrepareFunc[AgentDepsT] .", "url": "https://ai.pydantic.dev/api/tools/index.html#pydantic_ai.tools.ToolsPrepareFunc", "page": "api/tools/index.html", "source_site": "pydantic_ai"}
{"title": "DocstringFormat module-attribute", "anchor": "pydantic_ai.tools.DocstringFormat", "heading_level": 3, "md_text": "Supported docstring formats. * 'google'  [Google-style](https://google.github.io/styleguide/pyguide.html#381-docstrings) docstrings. * 'numpy'  [Numpy-style](https://numpydoc.readthedocs.io/en/latest/format.html) docstrings. * 'sphinx'  [Sphinx-style](https://sphinx-rtd-tutorial.readthedocs.io/en/latest/docstrings.html#the-sphinx-docstring-format) docstrings. * 'auto'  Automatically infer the format based on the structure of the docstring.", "url": "https://ai.pydantic.dev/api/tools/index.html#pydantic_ai.tools.DocstringFormat", "page": "api/tools/index.html", "source_site": "pydantic_ai"}
{"title": "DeferredToolRequests dataclass", "anchor": "pydantic_ai.tools.DeferredToolRequests", "heading_level": 3, "md_text": "Tool calls that require approval or external execution. This can be used as an agent's output_type and will be used as the output of the agent run if the model called any deferred tools. Results can be passed to the next agent run using a [ DeferredToolResults ](index.html#pydantic_ai.tools.DeferredToolResults) object with the same tool call IDs. See [deferred tools docs](../../deferred-tools/index.html#deferred-tools) for more information. Source code in pydantic_ai_slim/pydantic_ai/tools.py #### calls class-attribute instance-attribute Tool calls that require external execution. #### approvals class-attribute instance-attribute Tool calls that require human-in-the-loop approval.", "url": "https://ai.pydantic.dev/api/tools/index.html#pydantic_ai.tools.DeferredToolRequests", "page": "api/tools/index.html", "source_site": "pydantic_ai"}
{"title": "ToolApproved dataclass", "anchor": "pydantic_ai.tools.ToolApproved", "heading_level": 3, "md_text": "Indicates that a tool call has been approved and that the tool function should be executed. Source code in pydantic_ai_slim/pydantic_ai/tools.py #### override\\_args class-attribute instance-attribute Optional tool call arguments to use instead of the original arguments.", "url": "https://ai.pydantic.dev/api/tools/index.html#pydantic_ai.tools.ToolApproved", "page": "api/tools/index.html", "source_site": "pydantic_ai"}
{"title": "ToolDenied dataclass", "anchor": "pydantic_ai.tools.ToolDenied", "heading_level": 3, "md_text": "Indicates that a tool call has been denied and that a denial message should be returned to the model. Source code in pydantic_ai_slim/pydantic_ai/tools.py #### message class-attribute instance-attribute The message to return to the model.", "url": "https://ai.pydantic.dev/api/tools/index.html#pydantic_ai.tools.ToolDenied", "page": "api/tools/index.html", "source_site": "pydantic_ai"}
{"title": "DeferredToolResults dataclass", "anchor": "pydantic_ai.tools.DeferredToolResults", "heading_level": 3, "md_text": "Results for deferred tool calls from a previous run that required approval or external execution. The tool call IDs need to match those from the [ DeferredToolRequests ](../output/index.html#pydantic_ai.output.DeferredToolRequests) output object from the previous run. See [deferred tools docs](../../deferred-tools/index.html#deferred-tools) for more information. Source code in pydantic_ai_slim/pydantic_ai/tools.py #### calls class-attribute instance-attribute Map of tool call IDs to results for tool calls that required external execution. #### approvals class-attribute instance-attribute Map of tool call IDs to results for tool calls that required human-in-the-loop approval.", "url": "https://ai.pydantic.dev/api/tools/index.html#pydantic_ai.tools.DeferredToolResults", "page": "api/tools/index.html", "source_site": "pydantic_ai"}
{"title": "Tool dataclass", "anchor": "pydantic_ai.tools.Tool", "heading_level": 3, "md_text": "Bases: Generic[AgentDepsT] A tool function for an agent. Source code in pydantic_ai_slim/pydantic_ai/tools.py #### \\_\\_init\\_\\_ Create a new tool instance. Example usage: or with a custom prepare method: Parameters: Source code in pydantic_ai_slim/pydantic_ai/tools.py #### function\\_schema instance-attribute The base JSON schema for the tool's parameters. This schema may be modified by the prepare function or by the Model class prior to including it in an API request. #### from\\_schema classmethod Creates a Pydantic tool from a function and a JSON schema. Parameters: Returns: Source code in pydantic_ai_slim/pydantic_ai/tools.py #### prepare\\_tool\\_def async Get the tool definition. By default, this method creates a tool definition, then either returns it, or calls self.prepare if it's set. Returns: Source code in pydantic_ai_slim/pydantic_ai/tools.py", "url": "https://ai.pydantic.dev/api/tools/index.html#pydantic_ai.tools.Tool", "page": "api/tools/index.html", "source_site": "pydantic_ai"}
{"title": "ObjectJsonSchema module-attribute", "anchor": "pydantic_ai.tools.ObjectJsonSchema", "heading_level": 3, "md_text": "Type representing JSON schema of an object, e.g. where \"type\": \"object\" . This type is used to define tools parameters (aka arguments) in [ToolDefinition](index.html#pydantic_ai.tools.ToolDefinition). With PEP-728 this should be a TypedDict with type: Literal['object'] , and extra_parts=Any", "url": "https://ai.pydantic.dev/api/tools/index.html#pydantic_ai.tools.ObjectJsonSchema", "page": "api/tools/index.html", "source_site": "pydantic_ai"}
{"title": "ToolDefinition dataclass", "anchor": "pydantic_ai.tools.ToolDefinition", "heading_level": 3, "md_text": "Definition of a tool passed to a model. This is used for both function tools and output tools. Source code in pydantic_ai_slim/pydantic_ai/tools.py #### name instance-attribute The name of the tool. #### parameters\\_json\\_schema class-attribute instance-attribute The JSON schema for the tool's parameters. #### description class-attribute instance-attribute The description of the tool. #### outer\\_typed\\_dict\\_key class-attribute instance-attribute The key in the outer [TypedDict] that wraps an output tool. This will only be set for output tools which don't have an object JSON schema. #### strict class-attribute instance-attribute Whether to enforce (vendor-specific) strict JSON schema validation for tool calls. Setting this to True while using a supported model generally imposes some restrictions on the tool's JSON schema in exchange for guaranteeing the API responses strictly match that schema. When False , the model may be free to generate other properties or types (depending on the vendor). When None (the default), the value will be inferred based on the compatibility of the parameters\\_json\\_schema. Note: this is currently only supported by OpenAI models. #### sequential class-attribute instance-attribute Whether this tool requires a sequential/serial execution environment. #### kind class-attribute instance-attribute The kind of tool: * 'function' : a tool that will be executed by Pydantic AI during an agent run and has its result returned to the model * 'output' : a tool that passes through an output value that ends the run * 'external' : a tool whose result will be produced outside of the Pydantic AI agent run in which it was called, because it depends on an upstream service (or user) or could take longer to generate than it's reasonable to keep the agent process running. See the [tools documentation](../../deferred-tools/index.html#deferred-tools) for more info. * 'unapproved' : a tool that requires human-in-the-loop approval. See the [tools documentation](../../deferred-tools/index.html#human-in-the-loop-tool-approval) for more info. #### metadata class-attribute instance-attribute Tool metadata that can be set by the toolset this tool came from. It is not sent to the model, but can be used for filtering and tool behavior customization. For MCP tools, this contains the meta , annotations , and output_schema fields from the tool definition. #### defer property Whether calls to this tool will be deferred. See the [tools documentation](../../deferred-tools/index.html#deferred-tools) for more info.", "url": "https://ai.pydantic.dev/api/tools/index.html#pydantic_ai.tools.ToolDefinition", "page": "api/tools/index.html", "source_site": "pydantic_ai"}
{"title": "Durable Execution with Temporal", "anchor": "durable-execution-with-temporal", "heading_level": 1, "md_text": "[Temporal](https://temporal.io) is a popular [durable execution](https://docs.temporal.io/evaluate/understanding-temporal#durable-execution) platform that's natively supported by Pydantic AI. ## Durable Execution In Temporal's durable execution implementation, a program that crashes or encounters an exception while interacting with a model or API will retry until it can successfully complete. Temporal relies primarily on a replay mechanism to recover from failures. As the program makes progress, Temporal saves key inputs and decisions, allowing a re-started program to pick up right where it left off. The key to making this work is to separate the application's repeatable (deterministic) and non-repeatable (non-deterministic) parts: 1. Deterministic pieces, termed [**workflows**](https://docs.temporal.io/workflow-definition), execute the same way when re-run with the same inputs. 2. Non-deterministic pieces, termed [**activities**](https://docs.temporal.io/activities), can run arbitrary code, performing I/O and any other operations. Workflow code can run for extended periods and, if interrupted, resume exactly where it left off. Critically, workflow code generally *cannot* include any kind of I/O, over the network, disk, etc. Activity code faces no restrictions on I/O or external interactions, but if an activity fails part-way through it is restarted from the beginning. Note If you are familiar with celery, it may be helpful to think of Temporal activities as similar to celery tasks, but where you wait for the task to complete and obtain its result before proceeding to the next step in the workflow. However, Temporal workflows and activities offer a great deal more flexibility and functionality than celery tasks. See the [Temporal documentation](https://docs.temporal.io/evaluate/understanding-temporal#temporal-application-the-building-blocks) for more information In the case of Pydantic AI agents, integration with Temporal means that [model requests](../../models/overview/index.html), [tool calls](../../tools/index.html) that may require I/O, and [MCP server communication](../../mcp/client/index.html) all need to be offloaded to Temporal activities due to their I/O requirements, while the logic that coordinates them (i.e. the agent run) lives in the workflow. Code that handles a scheduled job or web request can then execute the workflow, which will in turn execute the activities as needed. The diagram below shows the overall architecture of an agentic application in Temporal. The Temporal Server is responsible for tracking program execution and making sure the associated state is preserved reliably (i.e., stored to an internal database, and possibly replicated across cloud regions). Temporal Server manages data in encrypted form, so all data processing occurs on the Worker, which runs the workflow and activities. See the [Temporal documentation](https://docs.temporal.io/evaluate/understanding-temporal#temporal-application-the-building-blocks) for more information. ## Durable Agent Any agent can be wrapped in a [ TemporalAgent ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.TemporalAgent) to get a durable agent that can be used inside a deterministic Temporal workflow, by automatically offloading all work that requires I/O (namely model requests, tool calls, and MCP server communication) to non-deterministic activities. At the time of wrapping, the agent's [model](../../models/overview/index.html) and [toolsets](../../toolsets/index.html) (including function tools registered on the agent and MCP servers) are frozen, activities are dynamically created for each, and the original model and toolsets are wrapped to call on the worker to execute the corresponding activities instead of directly performing the actions inside the workflow. The original agent can still be used as normal outside the Temporal workflow, but any changes to its model or toolsets after wrapping will not be reflected in the durable agent. Here is a simple but complete example of wrapping an agent for durable execution, creating a Temporal workflow with durable execution logic, connecting to a Temporal server, and running the workflow from non-durable code. All it requires is a Temporal server to be [running locally](https://github.com/temporalio/temporal#download-and-start-temporal-server-locally): temporal\\_agent.py 1. The original Agent cannot be used inside a deterministic Temporal workflow, but the TemporalAgent can. 2. As explained above, the workflow represents a deterministic piece of code that can use non-deterministic activities for operations that require I/O. 3. [ TemporalAgent.run() ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.TemporalAgent.run) works just like [ Agent.run() ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run), but it will automatically offload model requests, tool calls, and MCP server communication to Temporal activities. 4. We connect to the Temporal server which keeps track of workflow and activity execution. 5. This assumes the Temporal server is [running locally](https://github.com/temporalio/temporal#download-and-start-temporal-server-locally). 6. The [ PydanticAIPlugin ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.PydanticAIPlugin) tells Temporal to use Pydantic for serialization and deserialization, and to treat [ UserError ](../../api/exceptions/index.html#pydantic_ai.exceptions.UserError) exceptions as non-retryable. 7. We start the worker that will listen on the specified task queue and run workflows and activities. In a real world application, this might be run in a separate service. 8. The [ AgentPlugin ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.AgentPlugin) registers the TemporalAgent 's activities with the worker. 9. We call on the server to execute the workflow on a worker that's listening on the specified task queue. 10. The agent's name is used to uniquely identify its activities. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* In a real world application, the agent, workflow, and worker are typically defined separately from the code that calls for a workflow to be executed. Because Temporal workflows need to be defined at the top level of the file and the TemporalAgent instance is needed inside the workflow and when starting the worker (to register the activities), it needs to be defined at the top level of the file as well. For more information on how to use Temporal in Python applications, see their [Python SDK guide](https://docs.temporal.io/develop/python). ## Temporal Integration Considerations There are a few considerations specific to agents and toolsets when using Temporal for durable execution. These are important to understand to ensure that your agents and toolsets work correctly with Temporal's workflow and activity model. ### Agent Names and Toolset IDs To ensure that Temporal knows what code to run when an activity fails or is interrupted and then restarted, even if your code is changed in between, each activity needs to have a name that's stable and unique. When TemporalAgent dynamically creates activities for the wrapped agent's model requests and toolsets (specifically those that implement their own tool listing and calling, i.e. [ FunctionToolset ](../../api/toolsets/index.html#pydantic_ai.toolsets.FunctionToolset) and [ MCPServer ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServer)), their names are derived from the agent's [ name ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.name) and the toolsets' [ id s](../../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.id). These fields are normally optional, but are required to be set when using Temporal. They should not be changed once the durable agent has been deployed to production as this would break active workflows. Other than that, any agent and toolset will just work! ### Instructions Functions, Output Functions, and History Processors Pydantic AI runs non-async [instructions](../../agents/index.html#instructions) and [system prompt](../../agents/index.html#system-prompts) functions, [history processors](../../message-history/index.html#processing-message-history), [output functions](../../output/index.html#output-functions), and [output validators](../../output/index.html#output-validator-functions) in threads, which are not supported inside Temporal workflows and require an activity. Ensure that these functions are async instead. Synchronous tool functions are supported, as tools are automatically run in activities unless this is [explicitly disabled](index.html#activity-configuration). Still, it's recommended to make tool functions async as well to improve performance. ### Agent Run Context and Dependencies As workflows and activities run in separate processes, any values passed between them need to be serializable. As these payloads are stored in the workflow execution event history, Temporal limits their size to 2MB. To account for these limitations, tool functions and the [event stream handler](index.html#streaming) running inside activities receive a limited version of the agent's [ RunContext ](../../api/tools/index.html#pydantic_ai.tools.RunContext), and it's your responsibility to make sure that the [dependencies](../../dependencies/index.html) object provided to [ TemporalAgent.run() ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.TemporalAgent.run) can be serialized using Pydantic. Specifically, only the deps , retries , tool_call_id , tool_name , tool_call_approved , retry , max_retries and run_step fields are available by default, and trying to access model , usage , prompt , messages , or tracer will raise an error. If you need one or more of these attributes to be available inside activities, you can create a [ TemporalRunContext ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.TemporalRunContext) subclass with custom serialize_run_context and deserialize_run_context class methods and pass it to [ TemporalAgent ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.TemporalAgent) as run_context_type . ### Streaming Because Temporal activities cannot stream output directly to the activity call site, [ Agent.run_stream() ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream), [ Agent.run_stream_events() ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream_events), and [ Agent.iter() ](../../api/agent/index.html#pydantic_ai.agent.Agent.iter) are not supported. Instead, you can implement streaming by setting an [ event_stream_handler ](../../api/agent/index.html#pydantic_ai.agent.EventStreamHandler) on the Agent or TemporalAgent instance and using [ TemporalAgent.run() ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.TemporalAgent.run) inside the workflow. The event stream handler function will receive the agent [run context](../../api/tools/index.html#pydantic_ai.tools.RunContext) and an async iterable of events from the model's streaming response and the agent's execution of tools. For examples, see the [streaming docs](../../agents/index.html#streaming-all-events). As the streaming model request activity, workflow, and workflow execution call all take place in separate processes, passing data between them requires some care: * To get data from the workflow call site or workflow to the event stream handler, you can use a [dependencies object](index.html#agent-run-context-and-dependencies). * To get data from the event stream handler to the workflow, workflow call site, or a frontend, you need to use an external system that the event stream handler can write to and the event consumer can read from, like a message queue. You can use the dependency object to make sure the same connection string or other unique ID is available in all the places that need it. ## Activity Configuration Temporal activity configuration, like timeouts and retry policies, can be customized by passing [ temporalio.workflow.ActivityConfig ](https://python.temporal.io/temporalio.workflow.ActivityConfig.html) objects to the TemporalAgent constructor: * activity_config : The base Temporal activity config to use for all activities. If no config is provided, a start_to_close_timeout of 60 seconds is used. * model_activity_config : The Temporal activity config to use for model request activities. This is merged with the base activity config. * toolset_activity_config : The Temporal activity config to use for get-tools and call-tool activities for specific toolsets identified by ID. This is merged with the base activity config. * tool_activity_config : The Temporal activity config to use for specific tool call activities identified by toolset ID and tool name. This is merged with the base and toolset-specific activity configs. If a tool does not use I/O, you can specify False to disable using an activity. Note that the tool is required to be defined as an async function as non-async tools are run in threads which are non-deterministic and thus not supported outside of activities. ## Activity Retries On top of the automatic retries for request failures that Temporal will perform, Pydantic AI and various provider API clients also have their own request retry logic. Enabling these at the same time may cause the request to be retried more often than expected, with improper Retry-After handling. When using Temporal, it's recommended to not use [HTTP Request Retries](../../retries/index.html) and to turn off your provider API client's own retry logic, for example by setting max_retries=0 on a [custom OpenAIProvider API client](../../models/openai/index.html#custom-openai-client). You can customize Temporal's retry policy using [activity configuration](index.html#activity-configuration). ## Observability with Logfire Temporal generates telemetry events and metrics for each workflow and activity execution, and Pydantic AI generates events for each agent run, model request and tool call. These can be sent to [Pydantic Logfire](../../logfire/index.html) to get a complete picture of what's happening in your application. To use Logfire with Temporal, you need to pass a [ LogfirePlugin ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.LogfirePlugin) object to Temporal's Client.connect() : logfire\\_plugin.py By default, the LogfirePlugin will instrument Temporal (including metrics) and Pydantic AI and send all data to Logfire. To customize Logfire configuration and instrumentation, you can pass a logfire_setup function to the LogfirePlugin constructor and return a custom Logfire instance (i.e. the result of logfire.configure() ). To disable sending Temporal metrics to Logfire, you can pass metrics=False to the LogfirePlugin constructor. ## Known Issues ### Pandas When logfire.info is used inside an activity and the pandas package is among your project's dependencies, you may encounter the following error which seems to be the result of an import race condition: To fix this, you can use the [ temporalio.workflow.unsafe.imports_passed_through() ](https://python.temporal.io/temporalio.workflow.unsafe.html#imports_passed_through) context manager to proactively import the package and not have it be reloaded in the workflow sandbox: temporal\\_activity.py", "url": "https://ai.pydantic.dev/durable_execution/temporal/index.html#durable-execution-with-temporal", "page": "durable_execution/temporal/index.html", "source_site": "pydantic_ai"}
{"title": "Durable Execution", "anchor": "durable-execution", "heading_level": 2, "md_text": "In Temporal's durable execution implementation, a program that crashes or encounters an exception while interacting with a model or API will retry until it can successfully complete. Temporal relies primarily on a replay mechanism to recover from failures. As the program makes progress, Temporal saves key inputs and decisions, allowing a re-started program to pick up right where it left off. The key to making this work is to separate the application's repeatable (deterministic) and non-repeatable (non-deterministic) parts: 1. Deterministic pieces, termed [**workflows**](https://docs.temporal.io/workflow-definition), execute the same way when re-run with the same inputs. 2. Non-deterministic pieces, termed [**activities**](https://docs.temporal.io/activities), can run arbitrary code, performing I/O and any other operations. Workflow code can run for extended periods and, if interrupted, resume exactly where it left off. Critically, workflow code generally *cannot* include any kind of I/O, over the network, disk, etc. Activity code faces no restrictions on I/O or external interactions, but if an activity fails part-way through it is restarted from the beginning. Note If you are familiar with celery, it may be helpful to think of Temporal activities as similar to celery tasks, but where you wait for the task to complete and obtain its result before proceeding to the next step in the workflow. However, Temporal workflows and activities offer a great deal more flexibility and functionality than celery tasks. See the [Temporal documentation](https://docs.temporal.io/evaluate/understanding-temporal#temporal-application-the-building-blocks) for more information In the case of Pydantic AI agents, integration with Temporal means that [model requests](../../models/overview/index.html), [tool calls](../../tools/index.html) that may require I/O, and [MCP server communication](../../mcp/client/index.html) all need to be offloaded to Temporal activities due to their I/O requirements, while the logic that coordinates them (i.e. the agent run) lives in the workflow. Code that handles a scheduled job or web request can then execute the workflow, which will in turn execute the activities as needed. The diagram below shows the overall architecture of an agentic application in Temporal. The Temporal Server is responsible for tracking program execution and making sure the associated state is preserved reliably (i.e., stored to an internal database, and possibly replicated across cloud regions). Temporal Server manages data in encrypted form, so all data processing occurs on the Worker, which runs the workflow and activities. See the [Temporal documentation](https://docs.temporal.io/evaluate/understanding-temporal#temporal-application-the-building-blocks) for more information.", "url": "https://ai.pydantic.dev/durable_execution/temporal/index.html#durable-execution", "page": "durable_execution/temporal/index.html", "source_site": "pydantic_ai"}
{"title": "Durable Agent", "anchor": "durable-agent", "heading_level": 2, "md_text": "Any agent can be wrapped in a [ TemporalAgent ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.TemporalAgent) to get a durable agent that can be used inside a deterministic Temporal workflow, by automatically offloading all work that requires I/O (namely model requests, tool calls, and MCP server communication) to non-deterministic activities. At the time of wrapping, the agent's [model](../../models/overview/index.html) and [toolsets](../../toolsets/index.html) (including function tools registered on the agent and MCP servers) are frozen, activities are dynamically created for each, and the original model and toolsets are wrapped to call on the worker to execute the corresponding activities instead of directly performing the actions inside the workflow. The original agent can still be used as normal outside the Temporal workflow, but any changes to its model or toolsets after wrapping will not be reflected in the durable agent. Here is a simple but complete example of wrapping an agent for durable execution, creating a Temporal workflow with durable execution logic, connecting to a Temporal server, and running the workflow from non-durable code. All it requires is a Temporal server to be [running locally](https://github.com/temporalio/temporal#download-and-start-temporal-server-locally): temporal\\_agent.py 1. The original Agent cannot be used inside a deterministic Temporal workflow, but the TemporalAgent can. 2. As explained above, the workflow represents a deterministic piece of code that can use non-deterministic activities for operations that require I/O. 3. [ TemporalAgent.run() ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.TemporalAgent.run) works just like [ Agent.run() ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run), but it will automatically offload model requests, tool calls, and MCP server communication to Temporal activities. 4. We connect to the Temporal server which keeps track of workflow and activity execution. 5. This assumes the Temporal server is [running locally](https://github.com/temporalio/temporal#download-and-start-temporal-server-locally). 6. The [ PydanticAIPlugin ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.PydanticAIPlugin) tells Temporal to use Pydantic for serialization and deserialization, and to treat [ UserError ](../../api/exceptions/index.html#pydantic_ai.exceptions.UserError) exceptions as non-retryable. 7. We start the worker that will listen on the specified task queue and run workflows and activities. In a real world application, this might be run in a separate service. 8. The [ AgentPlugin ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.AgentPlugin) registers the TemporalAgent 's activities with the worker. 9. We call on the server to execute the workflow on a worker that's listening on the specified task queue. 10. The agent's name is used to uniquely identify its activities. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* In a real world application, the agent, workflow, and worker are typically defined separately from the code that calls for a workflow to be executed. Because Temporal workflows need to be defined at the top level of the file and the TemporalAgent instance is needed inside the workflow and when starting the worker (to register the activities), it needs to be defined at the top level of the file as well. For more information on how to use Temporal in Python applications, see their [Python SDK guide](https://docs.temporal.io/develop/python).", "url": "https://ai.pydantic.dev/durable_execution/temporal/index.html#durable-agent", "page": "durable_execution/temporal/index.html", "source_site": "pydantic_ai"}
{"title": "Temporal Integration Considerations", "anchor": "temporal-integration-considerations", "heading_level": 2, "md_text": "There are a few considerations specific to agents and toolsets when using Temporal for durable execution. These are important to understand to ensure that your agents and toolsets work correctly with Temporal's workflow and activity model. ### Agent Names and Toolset IDs To ensure that Temporal knows what code to run when an activity fails or is interrupted and then restarted, even if your code is changed in between, each activity needs to have a name that's stable and unique. When TemporalAgent dynamically creates activities for the wrapped agent's model requests and toolsets (specifically those that implement their own tool listing and calling, i.e. [ FunctionToolset ](../../api/toolsets/index.html#pydantic_ai.toolsets.FunctionToolset) and [ MCPServer ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServer)), their names are derived from the agent's [ name ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.name) and the toolsets' [ id s](../../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.id). These fields are normally optional, but are required to be set when using Temporal. They should not be changed once the durable agent has been deployed to production as this would break active workflows. Other than that, any agent and toolset will just work! ### Instructions Functions, Output Functions, and History Processors Pydantic AI runs non-async [instructions](../../agents/index.html#instructions) and [system prompt](../../agents/index.html#system-prompts) functions, [history processors](../../message-history/index.html#processing-message-history), [output functions](../../output/index.html#output-functions), and [output validators](../../output/index.html#output-validator-functions) in threads, which are not supported inside Temporal workflows and require an activity. Ensure that these functions are async instead. Synchronous tool functions are supported, as tools are automatically run in activities unless this is [explicitly disabled](index.html#activity-configuration). Still, it's recommended to make tool functions async as well to improve performance. ### Agent Run Context and Dependencies As workflows and activities run in separate processes, any values passed between them need to be serializable. As these payloads are stored in the workflow execution event history, Temporal limits their size to 2MB. To account for these limitations, tool functions and the [event stream handler](index.html#streaming) running inside activities receive a limited version of the agent's [ RunContext ](../../api/tools/index.html#pydantic_ai.tools.RunContext), and it's your responsibility to make sure that the [dependencies](../../dependencies/index.html) object provided to [ TemporalAgent.run() ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.TemporalAgent.run) can be serialized using Pydantic. Specifically, only the deps , retries , tool_call_id , tool_name , tool_call_approved , retry , max_retries and run_step fields are available by default, and trying to access model , usage , prompt , messages , or tracer will raise an error. If you need one or more of these attributes to be available inside activities, you can create a [ TemporalRunContext ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.TemporalRunContext) subclass with custom serialize_run_context and deserialize_run_context class methods and pass it to [ TemporalAgent ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.TemporalAgent) as run_context_type . ### Streaming Because Temporal activities cannot stream output directly to the activity call site, [ Agent.run_stream() ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream), [ Agent.run_stream_events() ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream_events), and [ Agent.iter() ](../../api/agent/index.html#pydantic_ai.agent.Agent.iter) are not supported. Instead, you can implement streaming by setting an [ event_stream_handler ](../../api/agent/index.html#pydantic_ai.agent.EventStreamHandler) on the Agent or TemporalAgent instance and using [ TemporalAgent.run() ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.TemporalAgent.run) inside the workflow. The event stream handler function will receive the agent [run context](../../api/tools/index.html#pydantic_ai.tools.RunContext) and an async iterable of events from the model's streaming response and the agent's execution of tools. For examples, see the [streaming docs](../../agents/index.html#streaming-all-events). As the streaming model request activity, workflow, and workflow execution call all take place in separate processes, passing data between them requires some care: * To get data from the workflow call site or workflow to the event stream handler, you can use a [dependencies object](index.html#agent-run-context-and-dependencies). * To get data from the event stream handler to the workflow, workflow call site, or a frontend, you need to use an external system that the event stream handler can write to and the event consumer can read from, like a message queue. You can use the dependency object to make sure the same connection string or other unique ID is available in all the places that need it.", "url": "https://ai.pydantic.dev/durable_execution/temporal/index.html#temporal-integration-considerations", "page": "durable_execution/temporal/index.html", "source_site": "pydantic_ai"}
{"title": "Agent Names and Toolset IDs", "anchor": "agent-names-and-toolset-ids", "heading_level": 3, "md_text": "To ensure that Temporal knows what code to run when an activity fails or is interrupted and then restarted, even if your code is changed in between, each activity needs to have a name that's stable and unique. When TemporalAgent dynamically creates activities for the wrapped agent's model requests and toolsets (specifically those that implement their own tool listing and calling, i.e. [ FunctionToolset ](../../api/toolsets/index.html#pydantic_ai.toolsets.FunctionToolset) and [ MCPServer ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServer)), their names are derived from the agent's [ name ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.name) and the toolsets' [ id s](../../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.id). These fields are normally optional, but are required to be set when using Temporal. They should not be changed once the durable agent has been deployed to production as this would break active workflows. Other than that, any agent and toolset will just work!", "url": "https://ai.pydantic.dev/durable_execution/temporal/index.html#agent-names-and-toolset-ids", "page": "durable_execution/temporal/index.html", "source_site": "pydantic_ai"}
{"title": "Instructions Functions, Output Functions, and History Processors", "anchor": "instructions-functions-output-functions-and-history-processors", "heading_level": 3, "md_text": "Pydantic AI runs non-async [instructions](../../agents/index.html#instructions) and [system prompt](../../agents/index.html#system-prompts) functions, [history processors](../../message-history/index.html#processing-message-history), [output functions](../../output/index.html#output-functions), and [output validators](../../output/index.html#output-validator-functions) in threads, which are not supported inside Temporal workflows and require an activity. Ensure that these functions are async instead. Synchronous tool functions are supported, as tools are automatically run in activities unless this is [explicitly disabled](index.html#activity-configuration). Still, it's recommended to make tool functions async as well to improve performance.", "url": "https://ai.pydantic.dev/durable_execution/temporal/index.html#instructions-functions-output-functions-and-history-processors", "page": "durable_execution/temporal/index.html", "source_site": "pydantic_ai"}
{"title": "Agent Run Context and Dependencies", "anchor": "agent-run-context-and-dependencies", "heading_level": 3, "md_text": "As workflows and activities run in separate processes, any values passed between them need to be serializable. As these payloads are stored in the workflow execution event history, Temporal limits their size to 2MB. To account for these limitations, tool functions and the [event stream handler](index.html#streaming) running inside activities receive a limited version of the agent's [ RunContext ](../../api/tools/index.html#pydantic_ai.tools.RunContext), and it's your responsibility to make sure that the [dependencies](../../dependencies/index.html) object provided to [ TemporalAgent.run() ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.TemporalAgent.run) can be serialized using Pydantic. Specifically, only the deps , retries , tool_call_id , tool_name , tool_call_approved , retry , max_retries and run_step fields are available by default, and trying to access model , usage , prompt , messages , or tracer will raise an error. If you need one or more of these attributes to be available inside activities, you can create a [ TemporalRunContext ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.TemporalRunContext) subclass with custom serialize_run_context and deserialize_run_context class methods and pass it to [ TemporalAgent ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.TemporalAgent) as run_context_type .", "url": "https://ai.pydantic.dev/durable_execution/temporal/index.html#agent-run-context-and-dependencies", "page": "durable_execution/temporal/index.html", "source_site": "pydantic_ai"}
{"title": "Streaming", "anchor": "streaming", "heading_level": 3, "md_text": "Because Temporal activities cannot stream output directly to the activity call site, [ Agent.run_stream() ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream), [ Agent.run_stream_events() ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream_events), and [ Agent.iter() ](../../api/agent/index.html#pydantic_ai.agent.Agent.iter) are not supported. Instead, you can implement streaming by setting an [ event_stream_handler ](../../api/agent/index.html#pydantic_ai.agent.EventStreamHandler) on the Agent or TemporalAgent instance and using [ TemporalAgent.run() ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.TemporalAgent.run) inside the workflow. The event stream handler function will receive the agent [run context](../../api/tools/index.html#pydantic_ai.tools.RunContext) and an async iterable of events from the model's streaming response and the agent's execution of tools. For examples, see the [streaming docs](../../agents/index.html#streaming-all-events). As the streaming model request activity, workflow, and workflow execution call all take place in separate processes, passing data between them requires some care: * To get data from the workflow call site or workflow to the event stream handler, you can use a [dependencies object](index.html#agent-run-context-and-dependencies). * To get data from the event stream handler to the workflow, workflow call site, or a frontend, you need to use an external system that the event stream handler can write to and the event consumer can read from, like a message queue. You can use the dependency object to make sure the same connection string or other unique ID is available in all the places that need it.", "url": "https://ai.pydantic.dev/durable_execution/temporal/index.html#streaming", "page": "durable_execution/temporal/index.html", "source_site": "pydantic_ai"}
{"title": "Activity Configuration", "anchor": "activity-configuration", "heading_level": 2, "md_text": "Temporal activity configuration, like timeouts and retry policies, can be customized by passing [ temporalio.workflow.ActivityConfig ](https://python.temporal.io/temporalio.workflow.ActivityConfig.html) objects to the TemporalAgent constructor: * activity_config : The base Temporal activity config to use for all activities. If no config is provided, a start_to_close_timeout of 60 seconds is used. * model_activity_config : The Temporal activity config to use for model request activities. This is merged with the base activity config. * toolset_activity_config : The Temporal activity config to use for get-tools and call-tool activities for specific toolsets identified by ID. This is merged with the base activity config. * tool_activity_config : The Temporal activity config to use for specific tool call activities identified by toolset ID and tool name. This is merged with the base and toolset-specific activity configs. If a tool does not use I/O, you can specify False to disable using an activity. Note that the tool is required to be defined as an async function as non-async tools are run in threads which are non-deterministic and thus not supported outside of activities.", "url": "https://ai.pydantic.dev/durable_execution/temporal/index.html#activity-configuration", "page": "durable_execution/temporal/index.html", "source_site": "pydantic_ai"}
{"title": "Activity Retries", "anchor": "activity-retries", "heading_level": 2, "md_text": "On top of the automatic retries for request failures that Temporal will perform, Pydantic AI and various provider API clients also have their own request retry logic. Enabling these at the same time may cause the request to be retried more often than expected, with improper Retry-After handling. When using Temporal, it's recommended to not use [HTTP Request Retries](../../retries/index.html) and to turn off your provider API client's own retry logic, for example by setting max_retries=0 on a [custom OpenAIProvider API client](../../models/openai/index.html#custom-openai-client). You can customize Temporal's retry policy using [activity configuration](index.html#activity-configuration).", "url": "https://ai.pydantic.dev/durable_execution/temporal/index.html#activity-retries", "page": "durable_execution/temporal/index.html", "source_site": "pydantic_ai"}
{"title": "Observability with Logfire", "anchor": "observability-with-logfire", "heading_level": 2, "md_text": "Temporal generates telemetry events and metrics for each workflow and activity execution, and Pydantic AI generates events for each agent run, model request and tool call. These can be sent to [Pydantic Logfire](../../logfire/index.html) to get a complete picture of what's happening in your application. To use Logfire with Temporal, you need to pass a [ LogfirePlugin ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.temporal.LogfirePlugin) object to Temporal's Client.connect() : logfire\\_plugin.py By default, the LogfirePlugin will instrument Temporal (including metrics) and Pydantic AI and send all data to Logfire. To customize Logfire configuration and instrumentation, you can pass a logfire_setup function to the LogfirePlugin constructor and return a custom Logfire instance (i.e. the result of logfire.configure() ). To disable sending Temporal metrics to Logfire, you can pass metrics=False to the LogfirePlugin constructor.", "url": "https://ai.pydantic.dev/durable_execution/temporal/index.html#observability-with-logfire", "page": "durable_execution/temporal/index.html", "source_site": "pydantic_ai"}
{"title": "Known Issues", "anchor": "known-issues", "heading_level": 2, "md_text": "### Pandas When logfire.info is used inside an activity and the pandas package is among your project's dependencies, you may encounter the following error which seems to be the result of an import race condition: To fix this, you can use the [ temporalio.workflow.unsafe.imports_passed_through() ](https://python.temporal.io/temporalio.workflow.unsafe.html#imports_passed_through) context manager to proactively import the package and not have it be reloaded in the workflow sandbox: temporal\\_activity.py", "url": "https://ai.pydantic.dev/durable_execution/temporal/index.html#known-issues", "page": "durable_execution/temporal/index.html", "source_site": "pydantic_ai"}
{"title": "Pandas", "anchor": "pandas", "heading_level": 3, "md_text": "When logfire.info is used inside an activity and the pandas package is among your project's dependencies, you may encounter the following error which seems to be the result of an import race condition: To fix this, you can use the [ temporalio.workflow.unsafe.imports_passed_through() ](https://python.temporal.io/temporalio.workflow.unsafe.html#imports_passed_through) context manager to proactively import the package and not have it be reloaded in the workflow sandbox: temporal\\_activity.py", "url": "https://ai.pydantic.dev/durable_execution/temporal/index.html#pandas", "page": "durable_execution/temporal/index.html", "source_site": "pydantic_ai"}
{"title": "Durable Execution with Prefect", "anchor": "durable-execution-with-prefect", "heading_level": 1, "md_text": "[Prefect](https://www.prefect.io/) is a workflow orchestration framework for building resilient data pipelines in Python, natively integrated with Pydantic AI. ## Durable Execution Prefect 3.0 brings [transactional semantics](https://www.prefect.io/blog/transactional-ml-pipelines-with-prefect-3-0) to your Python workflows, allowing you to group tasks into atomic units and define failure modes. If any part of a transaction fails, the entire transaction can be rolled back to a clean state. * **Flows** are the top-level entry points for your workflow. They can contain tasks and other flows. * **Tasks** are individual units of work that can be retried, cached, and monitored independently. Prefect 3.0's approach to transactional orchestration makes your workflows automatically **idempotent**: rerunnable without duplication or inconsistency across any environment. Every task is executed within a transaction that governs when and where the task's result record is persisted. If the task runs again under an identical context, it will not re-execute but instead load its previous result. The diagram below shows the overall architecture of an agentic application with Prefect. Prefect uses client-side task orchestration by default, with optional server connectivity for advanced features like scheduling and monitoring. See the [Prefect documentation](https://docs.prefect.io/) for more information. ## Durable Agent Any agent can be wrapped in a [ PrefectAgent ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.prefect.PrefectAgent) to get durable execution. PrefectAgent automatically: * Wraps [ Agent.run ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run) and [ Agent.run_sync ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_sync) as Prefect flows. * Wraps [model requests](../../models/overview/index.html) as Prefect tasks. * Wraps [tool calls](../../tools/index.html) as Prefect tasks (configurable per-tool). * Wraps [MCP communication](../../mcp/client/index.html) as Prefect tasks. Event stream handlers are **automatically wrapped** by Prefect when running inside a Prefect flow. Each event from the stream is processed in a separate Prefect task for durability. You can customize the task behavior using the event_stream_handler_task_config parameter when creating the PrefectAgent . Do **not** manually decorate event stream handlers with @task . For examples, see the [streaming docs](../../agents/index.html#streaming-all-events) The original agent, model, and MCP server can still be used as normal outside the Prefect flow. Here is a simple but complete example of wrapping an agent for durable execution. All it requires is to install Pydantic AI with Prefect: pipuv Or if you're using the slim package, you can install it with the prefect optional group: pipuv prefect\\_agent.py 1. The agent's name is used to uniquely identify its flows and tasks. 2. Wrapping the agent with PrefectAgent enables durable execution for all agent runs. 3. [ PrefectAgent.run() ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.prefect.PrefectAgent.run) works like [ Agent.run() ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run), but runs as a Prefect flow and executes model requests, decorated tool calls, and MCP communication as Prefect tasks. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* For more information on how to use Prefect in Python applications, see their [Python documentation](https://docs.prefect.io/v3/how-to-guides/workflows/write-and-run). ## Prefect Integration Considerations When using Prefect with Pydantic AI agents, there are a few important considerations to ensure workflows behave correctly. ### Agent Requirements Each agent instance must have a unique name so Prefect can correctly identify and track its flows and tasks. ### Tool Wrapping Agent tools are automatically wrapped as Prefect tasks, which means they benefit from: * **Retry logic**: Failed tool calls can be retried automatically * **Caching**: Tool results are cached based on their inputs * **Observability**: Tool execution is tracked in the Prefect UI You can customize tool task behavior using tool_task_config (applies to all tools) or tool_task_config_by_name (per-tool configuration): prefect\\_agent\\_config.py Set a tool's config to None in tool_task_config_by_name to disable task wrapping for that specific tool. ### Streaming When running inside a Prefect flow, [ Agent.run_stream() ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream) works but doesn't provide real-time streaming because Prefect tasks consume their entire execution before returning results. The method will execute fully and return the complete result at once. For real-time streaming behavior inside Prefect flows, you can set an [ event_stream_handler ](../../api/agent/index.html#pydantic_ai.agent.EventStreamHandler) on the Agent or PrefectAgent instance and use [ PrefectAgent.run() ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.prefect.PrefectAgent.run). **Note**: Event stream handlers behave differently when running inside a Prefect flow versus outside: - **Outside a flow**: The handler receives events as they stream from the model - **Inside a flow**: Each event is wrapped as a Prefect task for durability, which may affect timing but ensures reliability The event stream handler function will receive the agent [run context](../../api/tools/index.html#pydantic_ai.tools.RunContext) and an async iterable of events from the model's streaming response and the agent's execution of tools. For examples, see the [streaming docs](../../agents/index.html#streaming-all-events). ## Task Configuration You can customize Prefect task behavior, such as retries and timeouts, by passing [ TaskConfig ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.prefect.TaskConfig) objects to the PrefectAgent constructor: * mcp_task_config : Configuration for MCP server communication tasks * model_task_config : Configuration for model request tasks * tool_task_config : Default configuration for all tool calls * tool_task_config_by_name : Per-tool task configuration (overrides tool_task_config ) * event_stream_handler_task_config : Configuration for event stream handler tasks (applies when running inside a Prefect flow) Available TaskConfig options: * retries : Maximum number of retries for the task (default: 0 ) * retry_delay_seconds : Delay between retries in seconds (can be a single value or list for exponential backoff, default: 1.0 ) * timeout_seconds : Maximum time in seconds for the task to complete * cache_policy : Custom Prefect cache policy for the task * persist_result : Whether to persist the task result * result_storage : Prefect result storage for the task (e.g., 's3-bucket/my-storage' or a WritableFileSystem block) * log_prints : Whether to log print statements from the task (default: False ) Example: prefect\\_agent\\_config.py *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* ### Retry Considerations Pydantic AI and provider API clients have their own retry logic. When using Prefect, you may want to: * Disable [HTTP Request Retries](../../retries/index.html) in Pydantic AI * Turn off your provider API client's retry logic (e.g., max_retries=0 on a [custom OpenAI client](../../models/openai/index.html#custom-openai-client)) * Rely on Prefect's task-level retry configuration for consistency This prevents requests from being retried multiple times at different layers. ## Caching and Idempotency Prefect 3.0 provides built-in caching and transactional semantics. Tasks with identical inputs will not re-execute if their results are already cached, making workflows naturally idempotent and resilient to failures. * **Task inputs**: Messages, settings, parameters, tool arguments, and serializable dependencies **Note**: For user dependencies to be included in cache keys, they must be serializable (e.g., Pydantic models or basic Python types). Non-serializable dependencies are automatically excluded from cache computation. ## Observability with Prefect and Logfire Prefect provides a built-in UI for monitoring flow runs, task executions, and failures. You can: * View real-time flow run status * Debug failures with full stack traces * Set up alerts and notifications To access the Prefect UI, you can either: 1. Use [Prefect Cloud](https://www.prefect.io/cloud) (managed service) 2. Run a local [Prefect server](https://docs.prefect.io/v3/how-to-guides/self-hosted/server-cli) with prefect server start You can also use [Pydantic Logfire](../../logfire/index.html) for detailed observability. When using both Prefect and Logfire, you'll get complementary views: * **Prefect**: Workflow-level orchestration, task status, and retry history * **Logfire**: Fine-grained tracing of agent runs, model requests, and tool invocations When using Logfire with Prefect, you can enable distributed tracing to see spans for your Prefect runs included with your agent runs, model requests, and tool invocations. For more information about Prefect monitoring, see the [Prefect documentation](https://docs.prefect.io/). ## Deployments and Scheduling To deploy and schedule a PrefectAgent , wrap it in a Prefect flow and use the flow's [ serve() ](https://docs.prefect.io/v3/how-to-guides/deployments/create-deployments#create-a-deployment-with-serve) or [ deploy() ](https://docs.prefect.io/v3/how-to-guides/deployments/deploy-via-python) methods: serve\\_agent.py 1. Each flow run executes in an isolated process, and all inputs and dependencies must be serializable. Because Agent instances cannot be serialized, instantiate the agent inside the flow rather than at the module level. The serve() method accepts scheduling options: * ** cron **: Cron schedule string (e.g., '0 9 * * *' for daily at 9am) * ** interval **: Schedule interval in seconds or as a timedelta * ** rrule **: iCalendar RRule schedule string For production deployments with Docker, Kubernetes, or other infrastructure, use the flow's [ deploy() ](https://docs.prefect.io/v3/how-to-guides/deployments/deploy-via-python) method. See the [Prefect deployment documentation](https://docs.prefect.io/v3/how-to-guides/deployments/create-deploymentsy) for more information.", "url": "https://ai.pydantic.dev/durable_execution/prefect/index.html#durable-execution-with-prefect", "page": "durable_execution/prefect/index.html", "source_site": "pydantic_ai"}
{"title": "Durable Execution", "anchor": "durable-execution", "heading_level": 2, "md_text": "Prefect 3.0 brings [transactional semantics](https://www.prefect.io/blog/transactional-ml-pipelines-with-prefect-3-0) to your Python workflows, allowing you to group tasks into atomic units and define failure modes. If any part of a transaction fails, the entire transaction can be rolled back to a clean state. * **Flows** are the top-level entry points for your workflow. They can contain tasks and other flows. * **Tasks** are individual units of work that can be retried, cached, and monitored independently. Prefect 3.0's approach to transactional orchestration makes your workflows automatically **idempotent**: rerunnable without duplication or inconsistency across any environment. Every task is executed within a transaction that governs when and where the task's result record is persisted. If the task runs again under an identical context, it will not re-execute but instead load its previous result. The diagram below shows the overall architecture of an agentic application with Prefect. Prefect uses client-side task orchestration by default, with optional server connectivity for advanced features like scheduling and monitoring. See the [Prefect documentation](https://docs.prefect.io/) for more information.", "url": "https://ai.pydantic.dev/durable_execution/prefect/index.html#durable-execution", "page": "durable_execution/prefect/index.html", "source_site": "pydantic_ai"}
{"title": "Durable Agent", "anchor": "durable-agent", "heading_level": 2, "md_text": "Any agent can be wrapped in a [ PrefectAgent ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.prefect.PrefectAgent) to get durable execution. PrefectAgent automatically: * Wraps [ Agent.run ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run) and [ Agent.run_sync ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_sync) as Prefect flows. * Wraps [model requests](../../models/overview/index.html) as Prefect tasks. * Wraps [tool calls](../../tools/index.html) as Prefect tasks (configurable per-tool). * Wraps [MCP communication](../../mcp/client/index.html) as Prefect tasks. Event stream handlers are **automatically wrapped** by Prefect when running inside a Prefect flow. Each event from the stream is processed in a separate Prefect task for durability. You can customize the task behavior using the event_stream_handler_task_config parameter when creating the PrefectAgent . Do **not** manually decorate event stream handlers with @task . For examples, see the [streaming docs](../../agents/index.html#streaming-all-events) The original agent, model, and MCP server can still be used as normal outside the Prefect flow. Here is a simple but complete example of wrapping an agent for durable execution. All it requires is to install Pydantic AI with Prefect: pipuv Or if you're using the slim package, you can install it with the prefect optional group: pipuv prefect\\_agent.py 1. The agent's name is used to uniquely identify its flows and tasks. 2. Wrapping the agent with PrefectAgent enables durable execution for all agent runs. 3. [ PrefectAgent.run() ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.prefect.PrefectAgent.run) works like [ Agent.run() ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run), but runs as a Prefect flow and executes model requests, decorated tool calls, and MCP communication as Prefect tasks. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* For more information on how to use Prefect in Python applications, see their [Python documentation](https://docs.prefect.io/v3/how-to-guides/workflows/write-and-run).", "url": "https://ai.pydantic.dev/durable_execution/prefect/index.html#durable-agent", "page": "durable_execution/prefect/index.html", "source_site": "pydantic_ai"}
{"title": "Prefect Integration Considerations", "anchor": "prefect-integration-considerations", "heading_level": 2, "md_text": "When using Prefect with Pydantic AI agents, there are a few important considerations to ensure workflows behave correctly. ### Agent Requirements Each agent instance must have a unique name so Prefect can correctly identify and track its flows and tasks. ### Tool Wrapping Agent tools are automatically wrapped as Prefect tasks, which means they benefit from: * **Retry logic**: Failed tool calls can be retried automatically * **Caching**: Tool results are cached based on their inputs * **Observability**: Tool execution is tracked in the Prefect UI You can customize tool task behavior using tool_task_config (applies to all tools) or tool_task_config_by_name (per-tool configuration): prefect\\_agent\\_config.py Set a tool's config to None in tool_task_config_by_name to disable task wrapping for that specific tool. ### Streaming When running inside a Prefect flow, [ Agent.run_stream() ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream) works but doesn't provide real-time streaming because Prefect tasks consume their entire execution before returning results. The method will execute fully and return the complete result at once. For real-time streaming behavior inside Prefect flows, you can set an [ event_stream_handler ](../../api/agent/index.html#pydantic_ai.agent.EventStreamHandler) on the Agent or PrefectAgent instance and use [ PrefectAgent.run() ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.prefect.PrefectAgent.run). **Note**: Event stream handlers behave differently when running inside a Prefect flow versus outside: - **Outside a flow**: The handler receives events as they stream from the model - **Inside a flow**: Each event is wrapped as a Prefect task for durability, which may affect timing but ensures reliability The event stream handler function will receive the agent [run context](../../api/tools/index.html#pydantic_ai.tools.RunContext) and an async iterable of events from the model's streaming response and the agent's execution of tools. For examples, see the [streaming docs](../../agents/index.html#streaming-all-events).", "url": "https://ai.pydantic.dev/durable_execution/prefect/index.html#prefect-integration-considerations", "page": "durable_execution/prefect/index.html", "source_site": "pydantic_ai"}
{"title": "Agent Requirements", "anchor": "agent-requirements", "heading_level": 3, "md_text": "Each agent instance must have a unique name so Prefect can correctly identify and track its flows and tasks.", "url": "https://ai.pydantic.dev/durable_execution/prefect/index.html#agent-requirements", "page": "durable_execution/prefect/index.html", "source_site": "pydantic_ai"}
{"title": "Tool Wrapping", "anchor": "tool-wrapping", "heading_level": 3, "md_text": "Agent tools are automatically wrapped as Prefect tasks, which means they benefit from: * **Retry logic**: Failed tool calls can be retried automatically * **Caching**: Tool results are cached based on their inputs * **Observability**: Tool execution is tracked in the Prefect UI You can customize tool task behavior using tool_task_config (applies to all tools) or tool_task_config_by_name (per-tool configuration): prefect\\_agent\\_config.py Set a tool's config to None in tool_task_config_by_name to disable task wrapping for that specific tool.", "url": "https://ai.pydantic.dev/durable_execution/prefect/index.html#tool-wrapping", "page": "durable_execution/prefect/index.html", "source_site": "pydantic_ai"}
{"title": "Streaming", "anchor": "streaming", "heading_level": 3, "md_text": "When running inside a Prefect flow, [ Agent.run_stream() ](../../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream) works but doesn't provide real-time streaming because Prefect tasks consume their entire execution before returning results. The method will execute fully and return the complete result at once. For real-time streaming behavior inside Prefect flows, you can set an [ event_stream_handler ](../../api/agent/index.html#pydantic_ai.agent.EventStreamHandler) on the Agent or PrefectAgent instance and use [ PrefectAgent.run() ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.prefect.PrefectAgent.run). **Note**: Event stream handlers behave differently when running inside a Prefect flow versus outside: - **Outside a flow**: The handler receives events as they stream from the model - **Inside a flow**: Each event is wrapped as a Prefect task for durability, which may affect timing but ensures reliability The event stream handler function will receive the agent [run context](../../api/tools/index.html#pydantic_ai.tools.RunContext) and an async iterable of events from the model's streaming response and the agent's execution of tools. For examples, see the [streaming docs](../../agents/index.html#streaming-all-events).", "url": "https://ai.pydantic.dev/durable_execution/prefect/index.html#streaming", "page": "durable_execution/prefect/index.html", "source_site": "pydantic_ai"}
{"title": "Task Configuration", "anchor": "task-configuration", "heading_level": 2, "md_text": "You can customize Prefect task behavior, such as retries and timeouts, by passing [ TaskConfig ](../../api/durable_exec/index.html#pydantic_ai.durable_exec.prefect.TaskConfig) objects to the PrefectAgent constructor: * mcp_task_config : Configuration for MCP server communication tasks * model_task_config : Configuration for model request tasks * tool_task_config : Default configuration for all tool calls * tool_task_config_by_name : Per-tool task configuration (overrides tool_task_config ) * event_stream_handler_task_config : Configuration for event stream handler tasks (applies when running inside a Prefect flow) Available TaskConfig options: * retries : Maximum number of retries for the task (default: 0 ) * retry_delay_seconds : Delay between retries in seconds (can be a single value or list for exponential backoff, default: 1.0 ) * timeout_seconds : Maximum time in seconds for the task to complete * cache_policy : Custom Prefect cache policy for the task * persist_result : Whether to persist the task result * result_storage : Prefect result storage for the task (e.g., 's3-bucket/my-storage' or a WritableFileSystem block) * log_prints : Whether to log print statements from the task (default: False ) Example: prefect\\_agent\\_config.py *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* ### Retry Considerations Pydantic AI and provider API clients have their own retry logic. When using Prefect, you may want to: * Disable [HTTP Request Retries](../../retries/index.html) in Pydantic AI * Turn off your provider API client's retry logic (e.g., max_retries=0 on a [custom OpenAI client](../../models/openai/index.html#custom-openai-client)) * Rely on Prefect's task-level retry configuration for consistency This prevents requests from being retried multiple times at different layers.", "url": "https://ai.pydantic.dev/durable_execution/prefect/index.html#task-configuration", "page": "durable_execution/prefect/index.html", "source_site": "pydantic_ai"}
{"title": "Retry Considerations", "anchor": "retry-considerations", "heading_level": 3, "md_text": "Pydantic AI and provider API clients have their own retry logic. When using Prefect, you may want to: * Disable [HTTP Request Retries](../../retries/index.html) in Pydantic AI * Turn off your provider API client's retry logic (e.g., max_retries=0 on a [custom OpenAI client](../../models/openai/index.html#custom-openai-client)) * Rely on Prefect's task-level retry configuration for consistency This prevents requests from being retried multiple times at different layers.", "url": "https://ai.pydantic.dev/durable_execution/prefect/index.html#retry-considerations", "page": "durable_execution/prefect/index.html", "source_site": "pydantic_ai"}
{"title": "Caching and Idempotency", "anchor": "caching-and-idempotency", "heading_level": 2, "md_text": "Prefect 3.0 provides built-in caching and transactional semantics. Tasks with identical inputs will not re-execute if their results are already cached, making workflows naturally idempotent and resilient to failures. * **Task inputs**: Messages, settings, parameters, tool arguments, and serializable dependencies **Note**: For user dependencies to be included in cache keys, they must be serializable (e.g., Pydantic models or basic Python types). Non-serializable dependencies are automatically excluded from cache computation.", "url": "https://ai.pydantic.dev/durable_execution/prefect/index.html#caching-and-idempotency", "page": "durable_execution/prefect/index.html", "source_site": "pydantic_ai"}
{"title": "Observability with Prefect and Logfire", "anchor": "observability-with-prefect-and-logfire", "heading_level": 2, "md_text": "Prefect provides a built-in UI for monitoring flow runs, task executions, and failures. You can: * View real-time flow run status * Debug failures with full stack traces * Set up alerts and notifications To access the Prefect UI, you can either: 1. Use [Prefect Cloud](https://www.prefect.io/cloud) (managed service) 2. Run a local [Prefect server](https://docs.prefect.io/v3/how-to-guides/self-hosted/server-cli) with prefect server start You can also use [Pydantic Logfire](../../logfire/index.html) for detailed observability. When using both Prefect and Logfire, you'll get complementary views: * **Prefect**: Workflow-level orchestration, task status, and retry history * **Logfire**: Fine-grained tracing of agent runs, model requests, and tool invocations When using Logfire with Prefect, you can enable distributed tracing to see spans for your Prefect runs included with your agent runs, model requests, and tool invocations. For more information about Prefect monitoring, see the [Prefect documentation](https://docs.prefect.io/).", "url": "https://ai.pydantic.dev/durable_execution/prefect/index.html#observability-with-prefect-and-logfire", "page": "durable_execution/prefect/index.html", "source_site": "pydantic_ai"}
{"title": "Deployments and Scheduling", "anchor": "deployments-and-scheduling", "heading_level": 2, "md_text": "To deploy and schedule a PrefectAgent , wrap it in a Prefect flow and use the flow's [ serve() ](https://docs.prefect.io/v3/how-to-guides/deployments/create-deployments#create-a-deployment-with-serve) or [ deploy() ](https://docs.prefect.io/v3/how-to-guides/deployments/deploy-via-python) methods: serve\\_agent.py 1. Each flow run executes in an isolated process, and all inputs and dependencies must be serializable. Because Agent instances cannot be serialized, instantiate the agent inside the flow rather than at the module level. The serve() method accepts scheduling options: * ** cron **: Cron schedule string (e.g., '0 9 * * *' for daily at 9am) * ** interval **: Schedule interval in seconds or as a timedelta * ** rrule **: iCalendar RRule schedule string For production deployments with Docker, Kubernetes, or other infrastructure, use the flow's [ deploy() ](https://docs.prefect.io/v3/how-to-guides/deployments/deploy-via-python) method. See the [Prefect deployment documentation](https://docs.prefect.io/v3/how-to-guides/deployments/create-deploymentsy) for more information.", "url": "https://ai.pydantic.dev/durable_execution/prefect/index.html#deployments-and-scheduling", "page": "durable_execution/prefect/index.html", "source_site": "pydantic_ai"}
{"title": "Core Concepts", "anchor": "core-concepts", "heading_level": 1, "md_text": "This page explains the key concepts in Pydantic Evals and how they work together. ## Overview Pydantic Evals is built around these core concepts: * **[ Dataset ](../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset)** - A static definition containing test cases and evaluators * **[ Case ](../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Case)** - A single test scenario with inputs and optional expected outputs * **[ Evaluator ](../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.Evaluator)** - Logic for scoring or validating outputs * **Experiment** - The act of running a task function against all cases in a dataset. (This corresponds to a call to Dataset.evaluate .) * **[ EvaluationReport ](../../api/pydantic_evals/reporting/index.html#pydantic_evals.reporting.EvaluationReport)** - The results from running an experiment The key distinction is between: * **Definition** ( Dataset with Case s and Evaluator s) - what you want to test * **Execution** (Experiment) - running your task against those tests * **Results** ( EvaluationReport ) - what happened during the experiment ## Unit Testing Analogy A helpful way to think about Pydantic Evals: **Key Difference**: AI systems are probabilistic, so instead of simple pass/fail, evaluations can have: * Quantitative scores (0.0 to 1.0) * Qualitative labels (\"good\", \"acceptable\", \"poor\") * Pass/fail assertions with explanatory reasons Just like you can run pytest multiple times on the same test suite, you can run multiple experiments on the same dataset to compare different implementations or track changes over time. ## Dataset A [ Dataset ](../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset) is a collection of test cases and evaluators that define an evaluation suite. ### Key Features * **Type-safe**: Generic over InputsT , OutputT , and MetadataT types * **Serializable**: Can be saved to/loaded from YAML or JSON files * **Evaluable**: Run against any function with matching input/output types ### Dataset -Level vs Case -Level Evaluators Evaluators can be defined at two levels: * ** Dataset -level**: Apply to all cases in the dataset * ** Case -level**: Apply only to specific cases ## Experiments An **Experiment** is what happens when you execute a task function against all cases in a dataset. This is the bridge between your static test definition (the Dataset) and your results (the EvaluationReport). ### Running an Experiment You run an experiment by calling [ evaluate() ](../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset.evaluate) or [ evaluate_sync() ](../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset.evaluate_sync) on a dataset: ### What Happens During an Experiment When you run an experiment: 1. **Setup**: The dataset loads all cases and evaluators 2. **Execution**: For each case: 1. The task function is called with case.inputs 2. Execution time is measured and OpenTelemetry spans are captured (if logfire is configured) 3. The outputs of the task function for each case are recorded 3. **Evaluation**: For each case output: 1. All dataset-level evaluators are run 2. Case-specific evaluators are run (if any) 3. Results are collected (scores, assertions, labels) 4. **Reporting**: All results are aggregated into an [ EvaluationReport ](../../api/pydantic_evals/reporting/index.html#pydantic_evals.reporting.EvaluationReport) ### Multiple Experiments from One Dataset A key feature of Pydantic Evals is that you can run the same dataset against different task implementations: This allows you to: * **Compare implementations** across versions * **Track performance** over time * **A/B test** different approaches * **Validate changes** before deployment ## Case A [ Case ](../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Case) represents a single test scenario with specific inputs and optional expected outputs. ### Case Components #### Inputs The inputs to pass to the task being evaluated. Can be any type: #### Expected Output The expected result, used by evaluators like [ EqualsExpected ](../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EqualsExpected): If no expected_output is provided, evaluators that require it (like EqualsExpected ) will skip that case. #### Metadata Arbitrary data that evaluators can access via [ EvaluatorContext ](../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorContext): Metadata is useful for: * Filtering cases during analysis * Providing context to evaluators * Organizing test suites #### Evaluators Cases can have their own evaluators that only run for that specific case. This is particularly powerful for building comprehensive evaluation suites where different cases have different requirements - if you could write one evaluator rubric that worked perfectly for all cases, you'd just incorporate it into your agent instructions. Case-specific [ LLMJudge ](../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.LLMJudge) evaluators are especially useful for quickly building maintainable golden datasets by describing what \"good\" looks like for each scenario. See [Case-specific evaluators](../evaluators/overview/index.html#case-specific-evaluators) for a more detailed explanation and examples. ## Evaluator An [ Evaluator ](../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.Evaluator) assesses the output of your task and returns one or more scores, labels, or assertions. Each score, label or assertion can also have an optional string-value reason associated. ### Evaluator Types Evaluators return different types of results: Evaluators can also return instances of [ EvaluationReason ](../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluationReason), and dictionaries mapping labels to output values. See the [custom evaluator return types](../evaluators/custom/index.html#return-types) docs for more detail. ### EvaluatorContext All evaluators receive an [ EvaluatorContext ](../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorContext) containing: * name : Case name (optional) * inputs : Task inputs * metadata : Case metadata (optional) * expected_output : Expected output (optional) * output : Actual output from task * duration : Task execution time in seconds * span_tree : OpenTelemetry spans (if logfire is configured) * attributes : Custom attributes dict * metrics : Custom metrics dict ### Multiple Evaluations Evaluators can return multiple results by returning a dictionary: ### Evaluation Reasons Add explanations to your evaluations using [ EvaluationReason ](../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluationReason): Reasons appear in reports when using include_reasons=True . ## Evaluation Report An [ EvaluationReport ](../../api/pydantic_evals/reporting/index.html#pydantic_evals.reporting.EvaluationReport) is the result of running an experiment. It contains all the data from executing your task against the dataset's cases and running all evaluators. ### Report Structure The [ EvaluationReport ](../../api/pydantic_evals/reporting/index.html#pydantic_evals.reporting.EvaluationReport) contains: * name : Experiment name * cases : List of successful case evaluations * failures : List of failed executions * trace_id : OpenTelemetry trace ID (optional) * span_id : OpenTelemetry span ID (optional) ### ReportCase Each successfulcase result contains: **Case data:** * name : Case name * inputs : Task inputs * metadata : Case metadata (optional) * expected_output : Expected output (optional) * output : Actual output from task **Evaluation results:** * scores : Dictionary of numeric scores from evaluators * labels : Dictionary of categorical labels from evaluators * assertions : Dictionary of pass/fail assertions from evaluators **Performance data:** * task_duration : Task execution time * total_duration : Total time including evaluators **Additional data:** * metrics : Custom metrics dict * attributes : Custom attributes dict **Tracing:** * trace_id : OpenTelemetry trace ID (optional) * span_id : OpenTelemetry span ID (optional) **Errors:** * evaluator_failures : List of evaluator errors ## Data Model Relationships Here's how the core concepts relate to each other: ### Static Definition * A **Dataset** contains: * Many **Cases** (test scenarios with inputs and expected outputs) * Many **Evaluators** (logic for scoring outputs) ### Execution (Experiment) When you call dataset.evaluate(task) , an **Experiment** runs: * The **Task** function is executed against all **Cases** in the **Dataset** * All **Evaluators** are run (both dataset-level and case-specific) against each output as appropriate * One **EvaluationReport** is produced as the final output ### Results * An **EvaluationReport** contains: * Results for each **Case** (inputs, outputs, scores, assertions, labels) * Summary statistics (averages, pass rates) * Performance data (durations) * Tracing information (OpenTelemetry spans) ### Key Relationships * **One Dataset  Many Experiments**: You can run the same dataset against different task implementations or multiple times to track changes * **One Experiment  One Report**: Each time you call dataset.evaluate(...) , you get one report * **One Experiment  Many Case Results**: The report contains results for every case in the dataset ## Next Steps * **[Evaluators Overview](../evaluators/overview/index.html)** - When to use different evaluator types * **[Built-in Evaluators](../evaluators/built-in/index.html)** - Complete reference of provided evaluators * **[Custom Evaluators](../evaluators/custom/index.html)** - Write your own evaluation logic * **[Dataset Management](../how-to/dataset-management/index.html)** - Save, load, and generate datasets", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#core-concepts", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "Pydantic Evals is built around these core concepts: * **[ Dataset ](../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset)** - A static definition containing test cases and evaluators * **[ Case ](../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Case)** - A single test scenario with inputs and optional expected outputs * **[ Evaluator ](../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.Evaluator)** - Logic for scoring or validating outputs * **Experiment** - The act of running a task function against all cases in a dataset. (This corresponds to a call to Dataset.evaluate .) * **[ EvaluationReport ](../../api/pydantic_evals/reporting/index.html#pydantic_evals.reporting.EvaluationReport)** - The results from running an experiment The key distinction is between: * **Definition** ( Dataset with Case s and Evaluator s) - what you want to test * **Execution** (Experiment) - running your task against those tests * **Results** ( EvaluationReport ) - what happened during the experiment", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#overview", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Unit Testing Analogy", "anchor": "unit-testing-analogy", "heading_level": 2, "md_text": "A helpful way to think about Pydantic Evals: **Key Difference**: AI systems are probabilistic, so instead of simple pass/fail, evaluations can have: * Quantitative scores (0.0 to 1.0) * Qualitative labels (\"good\", \"acceptable\", \"poor\") * Pass/fail assertions with explanatory reasons Just like you can run pytest multiple times on the same test suite, you can run multiple experiments on the same dataset to compare different implementations or track changes over time.", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#unit-testing-analogy", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Dataset", "anchor": "dataset", "heading_level": 2, "md_text": "A [ Dataset ](../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset) is a collection of test cases and evaluators that define an evaluation suite. ### Key Features * **Type-safe**: Generic over InputsT , OutputT , and MetadataT types * **Serializable**: Can be saved to/loaded from YAML or JSON files * **Evaluable**: Run against any function with matching input/output types ### Dataset -Level vs Case -Level Evaluators Evaluators can be defined at two levels: * ** Dataset -level**: Apply to all cases in the dataset * ** Case -level**: Apply only to specific cases", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#dataset", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Key Features", "anchor": "key-features", "heading_level": 3, "md_text": "* **Type-safe**: Generic over InputsT , OutputT , and MetadataT types * **Serializable**: Can be saved to/loaded from YAML or JSON files * **Evaluable**: Run against any function with matching input/output types", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#key-features", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Dataset -Level vs Case -Level Evaluators", "anchor": "dataset-level-vs-case-level-evaluators", "heading_level": 3, "md_text": "Evaluators can be defined at two levels: * ** Dataset -level**: Apply to all cases in the dataset * ** Case -level**: Apply only to specific cases", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#dataset-level-vs-case-level-evaluators", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Experiments", "anchor": "experiments", "heading_level": 2, "md_text": "An **Experiment** is what happens when you execute a task function against all cases in a dataset. This is the bridge between your static test definition (the Dataset) and your results (the EvaluationReport). ### Running an Experiment You run an experiment by calling [ evaluate() ](../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset.evaluate) or [ evaluate_sync() ](../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset.evaluate_sync) on a dataset: ### What Happens During an Experiment When you run an experiment: 1. **Setup**: The dataset loads all cases and evaluators 2. **Execution**: For each case: 1. The task function is called with case.inputs 2. Execution time is measured and OpenTelemetry spans are captured (if logfire is configured) 3. The outputs of the task function for each case are recorded 3. **Evaluation**: For each case output: 1. All dataset-level evaluators are run 2. Case-specific evaluators are run (if any) 3. Results are collected (scores, assertions, labels) 4. **Reporting**: All results are aggregated into an [ EvaluationReport ](../../api/pydantic_evals/reporting/index.html#pydantic_evals.reporting.EvaluationReport) ### Multiple Experiments from One Dataset A key feature of Pydantic Evals is that you can run the same dataset against different task implementations: This allows you to: * **Compare implementations** across versions * **Track performance** over time * **A/B test** different approaches * **Validate changes** before deployment", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#experiments", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Running an Experiment", "anchor": "running-an-experiment", "heading_level": 3, "md_text": "You run an experiment by calling [ evaluate() ](../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset.evaluate) or [ evaluate_sync() ](../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset.evaluate_sync) on a dataset:", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#running-an-experiment", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "What Happens During an Experiment", "anchor": "what-happens-during-an-experiment", "heading_level": 3, "md_text": "When you run an experiment: 1. **Setup**: The dataset loads all cases and evaluators 2. **Execution**: For each case: 1. The task function is called with case.inputs 2. Execution time is measured and OpenTelemetry spans are captured (if logfire is configured) 3. The outputs of the task function for each case are recorded 3. **Evaluation**: For each case output: 1. All dataset-level evaluators are run 2. Case-specific evaluators are run (if any) 3. Results are collected (scores, assertions, labels) 4. **Reporting**: All results are aggregated into an [ EvaluationReport ](../../api/pydantic_evals/reporting/index.html#pydantic_evals.reporting.EvaluationReport)", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#what-happens-during-an-experiment", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Multiple Experiments from One Dataset", "anchor": "multiple-experiments-from-one-dataset", "heading_level": 3, "md_text": "A key feature of Pydantic Evals is that you can run the same dataset against different task implementations: This allows you to: * **Compare implementations** across versions * **Track performance** over time * **A/B test** different approaches * **Validate changes** before deployment", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#multiple-experiments-from-one-dataset", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Case", "anchor": "case", "heading_level": 2, "md_text": "A [ Case ](../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Case) represents a single test scenario with specific inputs and optional expected outputs. ### Case Components #### Inputs The inputs to pass to the task being evaluated. Can be any type: #### Expected Output The expected result, used by evaluators like [ EqualsExpected ](../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EqualsExpected): If no expected_output is provided, evaluators that require it (like EqualsExpected ) will skip that case. #### Metadata Arbitrary data that evaluators can access via [ EvaluatorContext ](../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorContext): Metadata is useful for: * Filtering cases during analysis * Providing context to evaluators * Organizing test suites #### Evaluators Cases can have their own evaluators that only run for that specific case. This is particularly powerful for building comprehensive evaluation suites where different cases have different requirements - if you could write one evaluator rubric that worked perfectly for all cases, you'd just incorporate it into your agent instructions. Case-specific [ LLMJudge ](../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.LLMJudge) evaluators are especially useful for quickly building maintainable golden datasets by describing what \"good\" looks like for each scenario. See [Case-specific evaluators](../evaluators/overview/index.html#case-specific-evaluators) for a more detailed explanation and examples.", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#case", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Case Components", "anchor": "case-components", "heading_level": 3, "md_text": "#### Inputs The inputs to pass to the task being evaluated. Can be any type: #### Expected Output The expected result, used by evaluators like [ EqualsExpected ](../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EqualsExpected): If no expected_output is provided, evaluators that require it (like EqualsExpected ) will skip that case. #### Metadata Arbitrary data that evaluators can access via [ EvaluatorContext ](../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorContext): Metadata is useful for: * Filtering cases during analysis * Providing context to evaluators * Organizing test suites #### Evaluators Cases can have their own evaluators that only run for that specific case. This is particularly powerful for building comprehensive evaluation suites where different cases have different requirements - if you could write one evaluator rubric that worked perfectly for all cases, you'd just incorporate it into your agent instructions. Case-specific [ LLMJudge ](../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.LLMJudge) evaluators are especially useful for quickly building maintainable golden datasets by describing what \"good\" looks like for each scenario. See [Case-specific evaluators](../evaluators/overview/index.html#case-specific-evaluators) for a more detailed explanation and examples.", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#case-components", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluator", "anchor": "evaluator", "heading_level": 2, "md_text": "An [ Evaluator ](../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.Evaluator) assesses the output of your task and returns one or more scores, labels, or assertions. Each score, label or assertion can also have an optional string-value reason associated. ### Evaluator Types Evaluators return different types of results: Evaluators can also return instances of [ EvaluationReason ](../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluationReason), and dictionaries mapping labels to output values. See the [custom evaluator return types](../evaluators/custom/index.html#return-types) docs for more detail. ### EvaluatorContext All evaluators receive an [ EvaluatorContext ](../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorContext) containing: * name : Case name (optional) * inputs : Task inputs * metadata : Case metadata (optional) * expected_output : Expected output (optional) * output : Actual output from task * duration : Task execution time in seconds * span_tree : OpenTelemetry spans (if logfire is configured) * attributes : Custom attributes dict * metrics : Custom metrics dict ### Multiple Evaluations Evaluators can return multiple results by returning a dictionary: ### Evaluation Reasons Add explanations to your evaluations using [ EvaluationReason ](../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluationReason): Reasons appear in reports when using include_reasons=True .", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#evaluator", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluator Types", "anchor": "evaluator-types", "heading_level": 3, "md_text": "Evaluators return different types of results: Evaluators can also return instances of [ EvaluationReason ](../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluationReason), and dictionaries mapping labels to output values. See the [custom evaluator return types](../evaluators/custom/index.html#return-types) docs for more detail.", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#evaluator-types", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluatorContext", "anchor": "evaluatorcontext", "heading_level": 3, "md_text": "All evaluators receive an [ EvaluatorContext ](../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorContext) containing: * name : Case name (optional) * inputs : Task inputs * metadata : Case metadata (optional) * expected_output : Expected output (optional) * output : Actual output from task * duration : Task execution time in seconds * span_tree : OpenTelemetry spans (if logfire is configured) * attributes : Custom attributes dict * metrics : Custom metrics dict", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#evaluatorcontext", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Multiple Evaluations", "anchor": "multiple-evaluations", "heading_level": 3, "md_text": "Evaluators can return multiple results by returning a dictionary:", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#multiple-evaluations", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluation Reasons", "anchor": "evaluation-reasons", "heading_level": 3, "md_text": "Add explanations to your evaluations using [ EvaluationReason ](../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluationReason): Reasons appear in reports when using include_reasons=True .", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#evaluation-reasons", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluation Report", "anchor": "evaluation-report", "heading_level": 2, "md_text": "An [ EvaluationReport ](../../api/pydantic_evals/reporting/index.html#pydantic_evals.reporting.EvaluationReport) is the result of running an experiment. It contains all the data from executing your task against the dataset's cases and running all evaluators. ### Report Structure The [ EvaluationReport ](../../api/pydantic_evals/reporting/index.html#pydantic_evals.reporting.EvaluationReport) contains: * name : Experiment name * cases : List of successful case evaluations * failures : List of failed executions * trace_id : OpenTelemetry trace ID (optional) * span_id : OpenTelemetry span ID (optional) ### ReportCase Each successfulcase result contains: **Case data:** * name : Case name * inputs : Task inputs * metadata : Case metadata (optional) * expected_output : Expected output (optional) * output : Actual output from task **Evaluation results:** * scores : Dictionary of numeric scores from evaluators * labels : Dictionary of categorical labels from evaluators * assertions : Dictionary of pass/fail assertions from evaluators **Performance data:** * task_duration : Task execution time * total_duration : Total time including evaluators **Additional data:** * metrics : Custom metrics dict * attributes : Custom attributes dict **Tracing:** * trace_id : OpenTelemetry trace ID (optional) * span_id : OpenTelemetry span ID (optional) **Errors:** * evaluator_failures : List of evaluator errors", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#evaluation-report", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Report Structure", "anchor": "report-structure", "heading_level": 3, "md_text": "The [ EvaluationReport ](../../api/pydantic_evals/reporting/index.html#pydantic_evals.reporting.EvaluationReport) contains: * name : Experiment name * cases : List of successful case evaluations * failures : List of failed executions * trace_id : OpenTelemetry trace ID (optional) * span_id : OpenTelemetry span ID (optional)", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#report-structure", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "ReportCase", "anchor": "reportcase", "heading_level": 3, "md_text": "Each successfulcase result contains: **Case data:** * name : Case name * inputs : Task inputs * metadata : Case metadata (optional) * expected_output : Expected output (optional) * output : Actual output from task **Evaluation results:** * scores : Dictionary of numeric scores from evaluators * labels : Dictionary of categorical labels from evaluators * assertions : Dictionary of pass/fail assertions from evaluators **Performance data:** * task_duration : Task execution time * total_duration : Total time including evaluators **Additional data:** * metrics : Custom metrics dict * attributes : Custom attributes dict **Tracing:** * trace_id : OpenTelemetry trace ID (optional) * span_id : OpenTelemetry span ID (optional) **Errors:** * evaluator_failures : List of evaluator errors", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#reportcase", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Data Model Relationships", "anchor": "data-model-relationships", "heading_level": 2, "md_text": "Here's how the core concepts relate to each other: ### Static Definition * A **Dataset** contains: * Many **Cases** (test scenarios with inputs and expected outputs) * Many **Evaluators** (logic for scoring outputs) ### Execution (Experiment) When you call dataset.evaluate(task) , an **Experiment** runs: * The **Task** function is executed against all **Cases** in the **Dataset** * All **Evaluators** are run (both dataset-level and case-specific) against each output as appropriate * One **EvaluationReport** is produced as the final output ### Results * An **EvaluationReport** contains: * Results for each **Case** (inputs, outputs, scores, assertions, labels) * Summary statistics (averages, pass rates) * Performance data (durations) * Tracing information (OpenTelemetry spans) ### Key Relationships * **One Dataset  Many Experiments**: You can run the same dataset against different task implementations or multiple times to track changes * **One Experiment  One Report**: Each time you call dataset.evaluate(...) , you get one report * **One Experiment  Many Case Results**: The report contains results for every case in the dataset", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#data-model-relationships", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Static Definition", "anchor": "static-definition", "heading_level": 3, "md_text": "* A **Dataset** contains: * Many **Cases** (test scenarios with inputs and expected outputs) * Many **Evaluators** (logic for scoring outputs)", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#static-definition", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Execution (Experiment)", "anchor": "execution-experiment", "heading_level": 3, "md_text": "When you call dataset.evaluate(task) , an **Experiment** runs: * The **Task** function is executed against all **Cases** in the **Dataset** * All **Evaluators** are run (both dataset-level and case-specific) against each output as appropriate * One **EvaluationReport** is produced as the final output", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#execution-experiment", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Results", "anchor": "results", "heading_level": 3, "md_text": "* An **EvaluationReport** contains: * Results for each **Case** (inputs, outputs, scores, assertions, labels) * Summary statistics (averages, pass rates) * Performance data (durations) * Tracing information (OpenTelemetry spans)", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#results", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Key Relationships", "anchor": "key-relationships", "heading_level": 3, "md_text": "* **One Dataset  Many Experiments**: You can run the same dataset against different task implementations or multiple times to track changes * **One Experiment  One Report**: Each time you call dataset.evaluate(...) , you get one report * **One Experiment  Many Case Results**: The report contains results for every case in the dataset", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#key-relationships", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "* **[Evaluators Overview](../evaluators/overview/index.html)** - When to use different evaluator types * **[Built-in Evaluators](../evaluators/built-in/index.html)** - Complete reference of provided evaluators * **[Custom Evaluators](../evaluators/custom/index.html)** - Write your own evaluation logic * **[Dataset Management](../how-to/dataset-management/index.html)** - Save, load, and generate datasets", "url": "https://ai.pydantic.dev/evals/core-concepts/index.html#next-steps", "page": "evals/core-concepts/index.html", "source_site": "pydantic_ai"}
{"title": "Built-in Evaluators", "anchor": "built-in-evaluators", "heading_level": 1, "md_text": "Pydantic Evals provides several built-in evaluators for common evaluation tasks. ## Comparison Evaluators ### EqualsExpected Check if the output exactly equals the expected output from the case. **Parameters:** None **Returns:** bool - True if ctx.output == ctx.expected_output **Example:** **Notes:** * Skips evaluation if expected_output is None (returns empty dict {} ) * Uses Python's == operator, so works with any comparable types * For structured data, considers nested equality --- ### Equals Check if the output equals a specific value. **Parameters:** * value (Any): The value to compare against * evaluation_name (str None): Custom name for this evaluation in reports **Returns:** bool - True if ctx.output == value **Example:** **Use Cases:** * Checking for sentinel values * Validating consistent outputs * Testing classification into specific categories --- ### Contains Check if the output contains a specific value or substring. **Parameters:** * value (Any): The value to search for * case_sensitive (bool): Case-sensitive comparison for strings (default: True ) * as_strings (bool): Convert both values to strings before checking (default: False ) * evaluation_name (str None): Custom name for this evaluation in reports **Returns:** [ EvaluationReason ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluationReason) - Pass/fail with explanation **Behavior:** For **strings**: checks substring containment * Contains(value='hello', case_sensitive=False) * Matches: \"Hello World\", \"say hello\", \"HELLO\" * Doesn't match: \"hi there\" For **lists/tuples**: checks membership * Contains(value='apple') * Matches: ['apple', 'banana'] , ('apple',) * Doesn't match: ['apples', 'orange'] For **dicts**: checks key-value pairs * Contains(value={'name': 'Alice'}) * Matches: {'name': 'Alice', 'age': 30} * Doesn't match: {'name': 'Bob'} **Example:** **Use Cases:** * Required content verification * Keyword detection * PII/sensitive data detection * Multi-value validation --- ## Type Validation ### IsInstance Check if the output is an instance of a type with the given name. **Parameters:** * type_name (str): The type name to check (uses __name__ or __qualname__ ) * evaluation_name (str None): Custom name for this evaluation in reports **Returns:** [ EvaluationReason ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluationReason) - Pass/fail with type information **Example:** **Notes:** * Matches against both __name__ and __qualname__ of the type * Works with built-in types ( str , int , dict , list , etc.) * Works with custom classes and Pydantic models * Checks the entire MRO (Method Resolution Order) for inheritance **Use Cases:** * Format validation * Structured output verification * Type consistency checks --- ## Performance Evaluation ### MaxDuration Check if task execution time is under a maximum threshold. **Parameters:** * seconds (float timedelta): Maximum allowed duration **Returns:** bool - True if ctx.duration <= seconds **Example:** **Use Cases:** * SLA compliance * Performance regression testing * Latency requirements * Timeout validation **See Also:** [Concurrency & Performance](../../how-to/concurrency/index.html) --- ## LLM-as-a-Judge ### LLMJudge Use an LLM to evaluate subjective qualities based on a rubric. **Parameters:** * rubric (str): The evaluation criteria (required) * model (Model KnownModelName None): Model to use (default: 'openai:gpt-4o' ) * include_input (bool): Include task inputs in the prompt (default: False ) * include_expected_output (bool): Include expected output in the prompt (default: False ) * model_settings (ModelSettings None): Custom model settings * score (OutputConfig False): Configure score output (default: False ) * assertion (OutputConfig False): Configure assertion output (default: includes reason) **Returns:** Depends on score and assertion parameters (see below) **Output Modes:** By default, returns a **boolean assertion** with reason: * LLMJudge(rubric='Response is polite') * Returns: {'LLMJudge_pass': EvaluationReason(value=True, reason='...')} Return a **score** (0.0 to 1.0) instead: * LLMJudge(rubric='Response quality', score={'include_reason': True}, assertion=False) * Returns: {'LLMJudge_score': EvaluationReason(value=0.85, reason='...')} Return **both** score and assertion: * LLMJudge(rubric='Response quality', score={'include_reason': True}, assertion={'include_reason': True}) * Returns: {'LLMJudge_score': EvaluationReason(value=0.85, reason='...'), 'LLMJudge_pass': EvaluationReason(value=True, reason='...')} **Customize evaluation names:** * LLMJudge(rubric='Response is factually accurate', assertion={'evaluation_name': 'accuracy', 'include_reason': True}) * Returns: {'accuracy': EvaluationReason(value=True, reason='...')} **Example:** **See Also:** [LLM Judge Deep Dive](../llm-judge/index.html) --- ## Span-Based Evaluation ### HasMatchingSpan Check if OpenTelemetry spans match a query (requires Logfire configuration). **Parameters:** * query ([ SpanQuery ](../../../api/pydantic_evals/otel/index.html#pydantic_evals.otel.SpanQuery)): Query to match against spans * evaluation_name (str None): Custom name for this evaluation in reports **Returns:** bool - True if any span matches the query **Example:** **See Also:** [Span-Based Evaluation](../span-based/index.html) --- ## Quick Reference Table ## Combining Evaluators Best practice is to combine fast deterministic checks with slower LLM evaluations: This approach: 1. Catches format/structure issues immediately 2. Validates required content quickly 3. Only runs expensive LLM evaluation if basic checks pass 4. Provides comprehensive quality assessment ## Next Steps * **[LLM Judge](../llm-judge/index.html)** - Deep dive on LLM-as-a-Judge evaluation * **[Custom Evaluators](../custom/index.html)** - Write your own evaluation logic * **[Span-Based Evaluation](../span-based/index.html)** - Using OpenTelemetry spans for behavioral checks", "url": "https://ai.pydantic.dev/evals/evaluators/built-in/index.html#built-in-evaluators", "page": "evals/evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "Comparison Evaluators", "anchor": "comparison-evaluators", "heading_level": 2, "md_text": "### EqualsExpected Check if the output exactly equals the expected output from the case. **Parameters:** None **Returns:** bool - True if ctx.output == ctx.expected_output **Example:** **Notes:** * Skips evaluation if expected_output is None (returns empty dict {} ) * Uses Python's == operator, so works with any comparable types * For structured data, considers nested equality --- ### Equals Check if the output equals a specific value. **Parameters:** * value (Any): The value to compare against * evaluation_name (str None): Custom name for this evaluation in reports **Returns:** bool - True if ctx.output == value **Example:** **Use Cases:** * Checking for sentinel values * Validating consistent outputs * Testing classification into specific categories --- ### Contains Check if the output contains a specific value or substring. **Parameters:** * value (Any): The value to search for * case_sensitive (bool): Case-sensitive comparison for strings (default: True ) * as_strings (bool): Convert both values to strings before checking (default: False ) * evaluation_name (str None): Custom name for this evaluation in reports **Returns:** [ EvaluationReason ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluationReason) - Pass/fail with explanation **Behavior:** For **strings**: checks substring containment * Contains(value='hello', case_sensitive=False) * Matches: \"Hello World\", \"say hello\", \"HELLO\" * Doesn't match: \"hi there\" For **lists/tuples**: checks membership * Contains(value='apple') * Matches: ['apple', 'banana'] , ('apple',) * Doesn't match: ['apples', 'orange'] For **dicts**: checks key-value pairs * Contains(value={'name': 'Alice'}) * Matches: {'name': 'Alice', 'age': 30} * Doesn't match: {'name': 'Bob'} **Example:** **Use Cases:** * Required content verification * Keyword detection * PII/sensitive data detection * Multi-value validation ---", "url": "https://ai.pydantic.dev/evals/evaluators/built-in/index.html#comparison-evaluators", "page": "evals/evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "EqualsExpected", "anchor": "equalsexpected", "heading_level": 3, "md_text": "Check if the output exactly equals the expected output from the case. **Parameters:** None **Returns:** bool - True if ctx.output == ctx.expected_output **Example:** **Notes:** * Skips evaluation if expected_output is None (returns empty dict {} ) * Uses Python's == operator, so works with any comparable types * For structured data, considers nested equality ---", "url": "https://ai.pydantic.dev/evals/evaluators/built-in/index.html#equalsexpected", "page": "evals/evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "Equals", "anchor": "equals", "heading_level": 3, "md_text": "Check if the output equals a specific value. **Parameters:** * value (Any): The value to compare against * evaluation_name (str None): Custom name for this evaluation in reports **Returns:** bool - True if ctx.output == value **Example:** **Use Cases:** * Checking for sentinel values * Validating consistent outputs * Testing classification into specific categories ---", "url": "https://ai.pydantic.dev/evals/evaluators/built-in/index.html#equals", "page": "evals/evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "Contains", "anchor": "contains", "heading_level": 3, "md_text": "Check if the output contains a specific value or substring. **Parameters:** * value (Any): The value to search for * case_sensitive (bool): Case-sensitive comparison for strings (default: True ) * as_strings (bool): Convert both values to strings before checking (default: False ) * evaluation_name (str None): Custom name for this evaluation in reports **Returns:** [ EvaluationReason ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluationReason) - Pass/fail with explanation **Behavior:** For **strings**: checks substring containment * Contains(value='hello', case_sensitive=False) * Matches: \"Hello World\", \"say hello\", \"HELLO\" * Doesn't match: \"hi there\" For **lists/tuples**: checks membership * Contains(value='apple') * Matches: ['apple', 'banana'] , ('apple',) * Doesn't match: ['apples', 'orange'] For **dicts**: checks key-value pairs * Contains(value={'name': 'Alice'}) * Matches: {'name': 'Alice', 'age': 30} * Doesn't match: {'name': 'Bob'} **Example:** **Use Cases:** * Required content verification * Keyword detection * PII/sensitive data detection * Multi-value validation ---", "url": "https://ai.pydantic.dev/evals/evaluators/built-in/index.html#contains", "page": "evals/evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "Type Validation", "anchor": "type-validation", "heading_level": 2, "md_text": "### IsInstance Check if the output is an instance of a type with the given name. **Parameters:** * type_name (str): The type name to check (uses __name__ or __qualname__ ) * evaluation_name (str None): Custom name for this evaluation in reports **Returns:** [ EvaluationReason ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluationReason) - Pass/fail with type information **Example:** **Notes:** * Matches against both __name__ and __qualname__ of the type * Works with built-in types ( str , int , dict , list , etc.) * Works with custom classes and Pydantic models * Checks the entire MRO (Method Resolution Order) for inheritance **Use Cases:** * Format validation * Structured output verification * Type consistency checks ---", "url": "https://ai.pydantic.dev/evals/evaluators/built-in/index.html#type-validation", "page": "evals/evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "IsInstance", "anchor": "isinstance", "heading_level": 3, "md_text": "Check if the output is an instance of a type with the given name. **Parameters:** * type_name (str): The type name to check (uses __name__ or __qualname__ ) * evaluation_name (str None): Custom name for this evaluation in reports **Returns:** [ EvaluationReason ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluationReason) - Pass/fail with type information **Example:** **Notes:** * Matches against both __name__ and __qualname__ of the type * Works with built-in types ( str , int , dict , list , etc.) * Works with custom classes and Pydantic models * Checks the entire MRO (Method Resolution Order) for inheritance **Use Cases:** * Format validation * Structured output verification * Type consistency checks ---", "url": "https://ai.pydantic.dev/evals/evaluators/built-in/index.html#isinstance", "page": "evals/evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "Performance Evaluation", "anchor": "performance-evaluation", "heading_level": 2, "md_text": "### MaxDuration Check if task execution time is under a maximum threshold. **Parameters:** * seconds (float timedelta): Maximum allowed duration **Returns:** bool - True if ctx.duration <= seconds **Example:** **Use Cases:** * SLA compliance * Performance regression testing * Latency requirements * Timeout validation **See Also:** [Concurrency & Performance](../../how-to/concurrency/index.html) ---", "url": "https://ai.pydantic.dev/evals/evaluators/built-in/index.html#performance-evaluation", "page": "evals/evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "MaxDuration", "anchor": "maxduration", "heading_level": 3, "md_text": "Check if task execution time is under a maximum threshold. **Parameters:** * seconds (float timedelta): Maximum allowed duration **Returns:** bool - True if ctx.duration <= seconds **Example:** **Use Cases:** * SLA compliance * Performance regression testing * Latency requirements * Timeout validation **See Also:** [Concurrency & Performance](../../how-to/concurrency/index.html) ---", "url": "https://ai.pydantic.dev/evals/evaluators/built-in/index.html#maxduration", "page": "evals/evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "LLM-as-a-Judge", "anchor": "llm-as-a-judge", "heading_level": 2, "md_text": "### LLMJudge Use an LLM to evaluate subjective qualities based on a rubric. **Parameters:** * rubric (str): The evaluation criteria (required) * model (Model KnownModelName None): Model to use (default: 'openai:gpt-4o' ) * include_input (bool): Include task inputs in the prompt (default: False ) * include_expected_output (bool): Include expected output in the prompt (default: False ) * model_settings (ModelSettings None): Custom model settings * score (OutputConfig False): Configure score output (default: False ) * assertion (OutputConfig False): Configure assertion output (default: includes reason) **Returns:** Depends on score and assertion parameters (see below) **Output Modes:** By default, returns a **boolean assertion** with reason: * LLMJudge(rubric='Response is polite') * Returns: {'LLMJudge_pass': EvaluationReason(value=True, reason='...')} Return a **score** (0.0 to 1.0) instead: * LLMJudge(rubric='Response quality', score={'include_reason': True}, assertion=False) * Returns: {'LLMJudge_score': EvaluationReason(value=0.85, reason='...')} Return **both** score and assertion: * LLMJudge(rubric='Response quality', score={'include_reason': True}, assertion={'include_reason': True}) * Returns: {'LLMJudge_score': EvaluationReason(value=0.85, reason='...'), 'LLMJudge_pass': EvaluationReason(value=True, reason='...')} **Customize evaluation names:** * LLMJudge(rubric='Response is factually accurate', assertion={'evaluation_name': 'accuracy', 'include_reason': True}) * Returns: {'accuracy': EvaluationReason(value=True, reason='...')} **Example:** **See Also:** [LLM Judge Deep Dive](../llm-judge/index.html) ---", "url": "https://ai.pydantic.dev/evals/evaluators/built-in/index.html#llm-as-a-judge", "page": "evals/evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "LLMJudge", "anchor": "llmjudge", "heading_level": 3, "md_text": "Use an LLM to evaluate subjective qualities based on a rubric. **Parameters:** * rubric (str): The evaluation criteria (required) * model (Model KnownModelName None): Model to use (default: 'openai:gpt-4o' ) * include_input (bool): Include task inputs in the prompt (default: False ) * include_expected_output (bool): Include expected output in the prompt (default: False ) * model_settings (ModelSettings None): Custom model settings * score (OutputConfig False): Configure score output (default: False ) * assertion (OutputConfig False): Configure assertion output (default: includes reason) **Returns:** Depends on score and assertion parameters (see below) **Output Modes:** By default, returns a **boolean assertion** with reason: * LLMJudge(rubric='Response is polite') * Returns: {'LLMJudge_pass': EvaluationReason(value=True, reason='...')} Return a **score** (0.0 to 1.0) instead: * LLMJudge(rubric='Response quality', score={'include_reason': True}, assertion=False) * Returns: {'LLMJudge_score': EvaluationReason(value=0.85, reason='...')} Return **both** score and assertion: * LLMJudge(rubric='Response quality', score={'include_reason': True}, assertion={'include_reason': True}) * Returns: {'LLMJudge_score': EvaluationReason(value=0.85, reason='...'), 'LLMJudge_pass': EvaluationReason(value=True, reason='...')} **Customize evaluation names:** * LLMJudge(rubric='Response is factually accurate', assertion={'evaluation_name': 'accuracy', 'include_reason': True}) * Returns: {'accuracy': EvaluationReason(value=True, reason='...')} **Example:** **See Also:** [LLM Judge Deep Dive](../llm-judge/index.html) ---", "url": "https://ai.pydantic.dev/evals/evaluators/built-in/index.html#llmjudge", "page": "evals/evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "Span-Based Evaluation", "anchor": "span-based-evaluation", "heading_level": 2, "md_text": "### HasMatchingSpan Check if OpenTelemetry spans match a query (requires Logfire configuration). **Parameters:** * query ([ SpanQuery ](../../../api/pydantic_evals/otel/index.html#pydantic_evals.otel.SpanQuery)): Query to match against spans * evaluation_name (str None): Custom name for this evaluation in reports **Returns:** bool - True if any span matches the query **Example:** **See Also:** [Span-Based Evaluation](../span-based/index.html) ---", "url": "https://ai.pydantic.dev/evals/evaluators/built-in/index.html#span-based-evaluation", "page": "evals/evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "HasMatchingSpan", "anchor": "hasmatchingspan", "heading_level": 3, "md_text": "Check if OpenTelemetry spans match a query (requires Logfire configuration). **Parameters:** * query ([ SpanQuery ](../../../api/pydantic_evals/otel/index.html#pydantic_evals.otel.SpanQuery)): Query to match against spans * evaluation_name (str None): Custom name for this evaluation in reports **Returns:** bool - True if any span matches the query **Example:** **See Also:** [Span-Based Evaluation](../span-based/index.html) ---", "url": "https://ai.pydantic.dev/evals/evaluators/built-in/index.html#hasmatchingspan", "page": "evals/evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "Quick Reference Table", "anchor": "quick-reference-table", "heading_level": 2, "md_text": "", "url": "https://ai.pydantic.dev/evals/evaluators/built-in/index.html#quick-reference-table", "page": "evals/evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "Combining Evaluators", "anchor": "combining-evaluators", "heading_level": 2, "md_text": "Best practice is to combine fast deterministic checks with slower LLM evaluations: This approach: 1. Catches format/structure issues immediately 2. Validates required content quickly 3. Only runs expensive LLM evaluation if basic checks pass 4. Provides comprehensive quality assessment", "url": "https://ai.pydantic.dev/evals/evaluators/built-in/index.html#combining-evaluators", "page": "evals/evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "* **[LLM Judge](../llm-judge/index.html)** - Deep dive on LLM-as-a-Judge evaluation * **[Custom Evaluators](../custom/index.html)** - Write your own evaluation logic * **[Span-Based Evaluation](../span-based/index.html)** - Using OpenTelemetry spans for behavioral checks", "url": "https://ai.pydantic.dev/evals/evaluators/built-in/index.html#next-steps", "page": "evals/evaluators/built-in/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluators Overview", "anchor": "evaluators-overview", "heading_level": 1, "md_text": "Evaluators are the core of Pydantic Evals. They analyze task outputs and provide scores, labels, or pass/fail assertions. ## When to Use Different Evaluators ### Deterministic Checks (Fast & Reliable) Use deterministic evaluators when you can define exact rules: **Advantages:** * Fast execution (microseconds to milliseconds) * Deterministic results * No cost * Easy to debug **When to use:** * Format validation (JSON structure, type checking) * Required content checks (must contain X, must not contain Y) * Performance requirements (latency, token counts) * Behavioral checks (which tools were called, which code paths executed) ### LLM-as-a-Judge (Flexible & Nuanced) Use [ LLMJudge ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.LLMJudge) when evaluation requires understanding or judgment: **Advantages:** * Can evaluate subjective qualities (helpfulness, tone, creativity) * Understands natural language * Can follow complex rubrics * Flexible across domains **Disadvantages:** * Slower (seconds per evaluation) * Costs money * Non-deterministic * Can have biases **When to use:** * Factual accuracy * Relevance and helpfulness * Tone and style * Completeness * Following instructions * RAG quality (groundedness, citation accuracy) ### Custom Evaluators Custom evaluators can be useful if you want to make use of any evaluation logic we don't provide with the framework. They are frequently useful for domain-specific logic: **When to use:** * Domain-specific validation (SQL syntax, regex patterns, business rules) * External API calls (running generated code, checking databases) * Complex calculations (precision/recall, BLEU scores) * Integration checks (does API call succeed?) ## Evaluation Types Detailed Return Types Guide For full detail about precisely what custom Evaluators may return, see [Custom Evaluator Return Types](../custom/index.html#return-types). Evaluators essentially return three types of results: ### 1. Assertions (bool) Pass/fail checks that appear as  or  in reports: **Use for:** Binary checks, quality gates, compliance requirements ### 2. Scores (int or float) Numeric metrics: **Use for:** Quality metrics, ranking, A/B testing, regression tracking ### 3. Labels (str) Categorical classifications: **Use for:** Classification, error categorization, quality buckets ### Multiple Results You can return multiple evaluations from a single evaluator: ## Combining Evaluators Mix and match evaluators to create comprehensive evaluation suites: ## Case-specific evaluators Case-specific evaluators are one of the most powerful features for building comprehensive evaluation suites. You can attach evaluators to individual [ Case ](../../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Case) objects that only run for those specific cases: ### Why Case-Specific Evaluators Matter Case-specific evaluators solve a fundamental problem with one-size-fits-all evaluation: **if you could write a single evaluator rubric that perfectly captured your requirements across all cases, you'd just incorporate that rubric into your agent's instructions**. (Note: this is less relevant in cases where you want to use a cheaper model in production and assess it using a more expensive model, but in many cases it makes sense to use the best model you can in production.) The power of case-specific evaluation comes from the nuance: * **Different cases have different requirements**: A customer support response needs empathy; a technical API response needs precision * **Avoid \"inmates running the asylum\"**: If your LLMJudge rubric is generic enough to work everywhere, your agent should already be following it * **Capture nuanced golden behavior**: Each case can specify exactly what \"good\" looks like for that scenario ### Building Golden Datasets with Case-Specific LLMJudge A particularly powerful pattern is using case-specific [ LLMJudge ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.LLMJudge) evaluators to quickly build comprehensive, maintainable evaluation suites. Instead of needing exact expected_output values, you can describe what you care about: This approach lets you: * **Build comprehensive test suites quickly**: Just describe what you want per case * **Maintain easily**: Update rubrics as requirements change, without regenerating outputs * **Cover edge cases naturally**: Add new cases with specific requirements as you discover them * **Capture domain knowledge**: Each rubric documents what \"good\" means for that scenario The LLM evaluator excels at understanding nuanced requirements and assessing compliance, making this a practical way to create thorough evaluation coverage without brittleness. ## Async vs Sync Evaluators can be sync or async: Pydantic Evals handles both automatically. Use async when: - Making API calls - Running database queries - Performing I/O operations - Calling LLMs (like [ LLMJudge ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.LLMJudge)) ## Evaluation Context All evaluators receive an [ EvaluatorContext ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorContext): * ctx.inputs - Task inputs * ctx.output - Task output (to evaluate) * ctx.expected_output - Expected output (if provided) * ctx.metadata - Case metadata (if provided) * ctx.duration - Task execution time (seconds) * ctx.span_tree - OpenTelemetry spans (if logfire configured) * ctx.metrics - Custom metrics dict * ctx.attributes - Custom attributes dict This gives evaluators full context to make informed assessments. ## Error Handling If an evaluator raises an exception, it's captured as an [ EvaluatorFailure ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorFailure): Failures appear in report.cases[i].evaluator_failures with: * Evaluator name * Error message * Full stacktrace Use retry configuration to handle transient failures (see [Retry Strategies](../../how-to/retry-strategies/index.html)). ## Next Steps * **[Built-in Evaluators](../built-in/index.html)** - Complete reference of all provided evaluators * **[LLM Judge](../llm-judge/index.html)** - Deep dive on LLM-as-a-Judge evaluation * **[Custom Evaluators](../custom/index.html)** - Write your own evaluation logic * **[Span-Based Evaluation](../span-based/index.html)** - Evaluate using OpenTelemetry spans", "url": "https://ai.pydantic.dev/evals/evaluators/overview/index.html#evaluators-overview", "page": "evals/evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "When to Use Different Evaluators", "anchor": "when-to-use-different-evaluators", "heading_level": 2, "md_text": "### Deterministic Checks (Fast & Reliable) Use deterministic evaluators when you can define exact rules: **Advantages:** * Fast execution (microseconds to milliseconds) * Deterministic results * No cost * Easy to debug **When to use:** * Format validation (JSON structure, type checking) * Required content checks (must contain X, must not contain Y) * Performance requirements (latency, token counts) * Behavioral checks (which tools were called, which code paths executed) ### LLM-as-a-Judge (Flexible & Nuanced) Use [ LLMJudge ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.LLMJudge) when evaluation requires understanding or judgment: **Advantages:** * Can evaluate subjective qualities (helpfulness, tone, creativity) * Understands natural language * Can follow complex rubrics * Flexible across domains **Disadvantages:** * Slower (seconds per evaluation) * Costs money * Non-deterministic * Can have biases **When to use:** * Factual accuracy * Relevance and helpfulness * Tone and style * Completeness * Following instructions * RAG quality (groundedness, citation accuracy) ### Custom Evaluators Custom evaluators can be useful if you want to make use of any evaluation logic we don't provide with the framework. They are frequently useful for domain-specific logic: **When to use:** * Domain-specific validation (SQL syntax, regex patterns, business rules) * External API calls (running generated code, checking databases) * Complex calculations (precision/recall, BLEU scores) * Integration checks (does API call succeed?)", "url": "https://ai.pydantic.dev/evals/evaluators/overview/index.html#when-to-use-different-evaluators", "page": "evals/evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Deterministic Checks (Fast & Reliable)", "anchor": "deterministic-checks-fast-reliable", "heading_level": 3, "md_text": "Use deterministic evaluators when you can define exact rules: **Advantages:** * Fast execution (microseconds to milliseconds) * Deterministic results * No cost * Easy to debug **When to use:** * Format validation (JSON structure, type checking) * Required content checks (must contain X, must not contain Y) * Performance requirements (latency, token counts) * Behavioral checks (which tools were called, which code paths executed)", "url": "https://ai.pydantic.dev/evals/evaluators/overview/index.html#deterministic-checks-fast-reliable", "page": "evals/evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "LLM-as-a-Judge (Flexible & Nuanced)", "anchor": "llm-as-a-judge-flexible-nuanced", "heading_level": 3, "md_text": "Use [ LLMJudge ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.LLMJudge) when evaluation requires understanding or judgment: **Advantages:** * Can evaluate subjective qualities (helpfulness, tone, creativity) * Understands natural language * Can follow complex rubrics * Flexible across domains **Disadvantages:** * Slower (seconds per evaluation) * Costs money * Non-deterministic * Can have biases **When to use:** * Factual accuracy * Relevance and helpfulness * Tone and style * Completeness * Following instructions * RAG quality (groundedness, citation accuracy)", "url": "https://ai.pydantic.dev/evals/evaluators/overview/index.html#llm-as-a-judge-flexible-nuanced", "page": "evals/evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Evaluators", "anchor": "custom-evaluators", "heading_level": 3, "md_text": "Custom evaluators can be useful if you want to make use of any evaluation logic we don't provide with the framework. They are frequently useful for domain-specific logic: **When to use:** * Domain-specific validation (SQL syntax, regex patterns, business rules) * External API calls (running generated code, checking databases) * Complex calculations (precision/recall, BLEU scores) * Integration checks (does API call succeed?)", "url": "https://ai.pydantic.dev/evals/evaluators/overview/index.html#custom-evaluators", "page": "evals/evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluation Types", "anchor": "evaluation-types", "heading_level": 2, "md_text": "Detailed Return Types Guide For full detail about precisely what custom Evaluators may return, see [Custom Evaluator Return Types](../custom/index.html#return-types). Evaluators essentially return three types of results: ### 1. Assertions (bool) Pass/fail checks that appear as  or  in reports: **Use for:** Binary checks, quality gates, compliance requirements ### 2. Scores (int or float) Numeric metrics: **Use for:** Quality metrics, ranking, A/B testing, regression tracking ### 3. Labels (str) Categorical classifications: **Use for:** Classification, error categorization, quality buckets ### Multiple Results You can return multiple evaluations from a single evaluator:", "url": "https://ai.pydantic.dev/evals/evaluators/overview/index.html#evaluation-types", "page": "evals/evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "1. Assertions (bool)", "anchor": "1-assertions-bool", "heading_level": 3, "md_text": "Pass/fail checks that appear as  or  in reports: **Use for:** Binary checks, quality gates, compliance requirements", "url": "https://ai.pydantic.dev/evals/evaluators/overview/index.html#1-assertions-bool", "page": "evals/evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "2. Scores (int or float)", "anchor": "2-scores-int-or-float", "heading_level": 3, "md_text": "Numeric metrics: **Use for:** Quality metrics, ranking, A/B testing, regression tracking", "url": "https://ai.pydantic.dev/evals/evaluators/overview/index.html#2-scores-int-or-float", "page": "evals/evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "3. Labels (str)", "anchor": "3-labels-str", "heading_level": 3, "md_text": "Categorical classifications: **Use for:** Classification, error categorization, quality buckets", "url": "https://ai.pydantic.dev/evals/evaluators/overview/index.html#3-labels-str", "page": "evals/evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Multiple Results", "anchor": "multiple-results", "heading_level": 3, "md_text": "You can return multiple evaluations from a single evaluator:", "url": "https://ai.pydantic.dev/evals/evaluators/overview/index.html#multiple-results", "page": "evals/evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Combining Evaluators", "anchor": "combining-evaluators", "heading_level": 2, "md_text": "Mix and match evaluators to create comprehensive evaluation suites:", "url": "https://ai.pydantic.dev/evals/evaluators/overview/index.html#combining-evaluators", "page": "evals/evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Case-specific evaluators", "anchor": "case-specific-evaluators", "heading_level": 2, "md_text": "Case-specific evaluators are one of the most powerful features for building comprehensive evaluation suites. You can attach evaluators to individual [ Case ](../../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Case) objects that only run for those specific cases: ### Why Case-Specific Evaluators Matter Case-specific evaluators solve a fundamental problem with one-size-fits-all evaluation: **if you could write a single evaluator rubric that perfectly captured your requirements across all cases, you'd just incorporate that rubric into your agent's instructions**. (Note: this is less relevant in cases where you want to use a cheaper model in production and assess it using a more expensive model, but in many cases it makes sense to use the best model you can in production.) The power of case-specific evaluation comes from the nuance: * **Different cases have different requirements**: A customer support response needs empathy; a technical API response needs precision * **Avoid \"inmates running the asylum\"**: If your LLMJudge rubric is generic enough to work everywhere, your agent should already be following it * **Capture nuanced golden behavior**: Each case can specify exactly what \"good\" looks like for that scenario ### Building Golden Datasets with Case-Specific LLMJudge A particularly powerful pattern is using case-specific [ LLMJudge ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.LLMJudge) evaluators to quickly build comprehensive, maintainable evaluation suites. Instead of needing exact expected_output values, you can describe what you care about: This approach lets you: * **Build comprehensive test suites quickly**: Just describe what you want per case * **Maintain easily**: Update rubrics as requirements change, without regenerating outputs * **Cover edge cases naturally**: Add new cases with specific requirements as you discover them * **Capture domain knowledge**: Each rubric documents what \"good\" means for that scenario The LLM evaluator excels at understanding nuanced requirements and assessing compliance, making this a practical way to create thorough evaluation coverage without brittleness.", "url": "https://ai.pydantic.dev/evals/evaluators/overview/index.html#case-specific-evaluators", "page": "evals/evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Why Case-Specific Evaluators Matter", "anchor": "why-case-specific-evaluators-matter", "heading_level": 3, "md_text": "Case-specific evaluators solve a fundamental problem with one-size-fits-all evaluation: **if you could write a single evaluator rubric that perfectly captured your requirements across all cases, you'd just incorporate that rubric into your agent's instructions**. (Note: this is less relevant in cases where you want to use a cheaper model in production and assess it using a more expensive model, but in many cases it makes sense to use the best model you can in production.) The power of case-specific evaluation comes from the nuance: * **Different cases have different requirements**: A customer support response needs empathy; a technical API response needs precision * **Avoid \"inmates running the asylum\"**: If your LLMJudge rubric is generic enough to work everywhere, your agent should already be following it * **Capture nuanced golden behavior**: Each case can specify exactly what \"good\" looks like for that scenario", "url": "https://ai.pydantic.dev/evals/evaluators/overview/index.html#why-case-specific-evaluators-matter", "page": "evals/evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Building Golden Datasets with Case-Specific LLMJudge", "anchor": "building-golden-datasets-with-case-specific-llmjudge", "heading_level": 3, "md_text": "A particularly powerful pattern is using case-specific [ LLMJudge ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.LLMJudge) evaluators to quickly build comprehensive, maintainable evaluation suites. Instead of needing exact expected_output values, you can describe what you care about: This approach lets you: * **Build comprehensive test suites quickly**: Just describe what you want per case * **Maintain easily**: Update rubrics as requirements change, without regenerating outputs * **Cover edge cases naturally**: Add new cases with specific requirements as you discover them * **Capture domain knowledge**: Each rubric documents what \"good\" means for that scenario The LLM evaluator excels at understanding nuanced requirements and assessing compliance, making this a practical way to create thorough evaluation coverage without brittleness.", "url": "https://ai.pydantic.dev/evals/evaluators/overview/index.html#building-golden-datasets-with-case-specific-llmjudge", "page": "evals/evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Async vs Sync", "anchor": "async-vs-sync", "heading_level": 2, "md_text": "Evaluators can be sync or async: Pydantic Evals handles both automatically. Use async when: - Making API calls - Running database queries - Performing I/O operations - Calling LLMs (like [ LLMJudge ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.LLMJudge))", "url": "https://ai.pydantic.dev/evals/evaluators/overview/index.html#async-vs-sync", "page": "evals/evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluation Context", "anchor": "evaluation-context", "heading_level": 2, "md_text": "All evaluators receive an [ EvaluatorContext ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorContext): * ctx.inputs - Task inputs * ctx.output - Task output (to evaluate) * ctx.expected_output - Expected output (if provided) * ctx.metadata - Case metadata (if provided) * ctx.duration - Task execution time (seconds) * ctx.span_tree - OpenTelemetry spans (if logfire configured) * ctx.metrics - Custom metrics dict * ctx.attributes - Custom attributes dict This gives evaluators full context to make informed assessments.", "url": "https://ai.pydantic.dev/evals/evaluators/overview/index.html#evaluation-context", "page": "evals/evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Error Handling", "anchor": "error-handling", "heading_level": 2, "md_text": "If an evaluator raises an exception, it's captured as an [ EvaluatorFailure ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorFailure): Failures appear in report.cases[i].evaluator_failures with: * Evaluator name * Error message * Full stacktrace Use retry configuration to handle transient failures (see [Retry Strategies](../../how-to/retry-strategies/index.html)).", "url": "https://ai.pydantic.dev/evals/evaluators/overview/index.html#error-handling", "page": "evals/evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "* **[Built-in Evaluators](../built-in/index.html)** - Complete reference of all provided evaluators * **[LLM Judge](../llm-judge/index.html)** - Deep dive on LLM-as-a-Judge evaluation * **[Custom Evaluators](../custom/index.html)** - Write your own evaluation logic * **[Span-Based Evaluation](../span-based/index.html)** - Evaluate using OpenTelemetry spans", "url": "https://ai.pydantic.dev/evals/evaluators/overview/index.html#next-steps", "page": "evals/evaluators/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Example: Simple Validation", "anchor": "example-simple-validation", "heading_level": 1, "md_text": "A proof of concept example of evaluating a simple text transformation function with deterministic checks. ## Scenario We're testing a function that converts text to title case. We want to verify: * Output is always a string * Output matches expected format * Function handles edge cases correctly * Performance meets requirements ## Complete Example ## Expected Output Note: The empty_string case has one failed assertion ( has_capitals ) because an empty string contains no capital letters. ## Saving and Loading Save the dataset for future use: ## Adding More Cases As you find bugs or edge cases, add them to the dataset: ## Using with pytest Integrate with pytest for CI/CD: ## Next Steps * **[Built-in Evaluators](../../evaluators/built-in/index.html)** - Explore all available evaluators * **[Custom Evaluators](../../evaluators/custom/index.html)** - Write your own evaluation logic * **[Dataset Management](../../how-to/dataset-management/index.html)** - Save, load, and manage datasets * **[Concurrency & Performance](../../how-to/concurrency/index.html)** - Optimize evaluation performance", "url": "https://ai.pydantic.dev/evals/examples/simple-validation/index.html#example-simple-validation", "page": "evals/examples/simple-validation/index.html", "source_site": "pydantic_ai"}
{"title": "Scenario", "anchor": "scenario", "heading_level": 2, "md_text": "We're testing a function that converts text to title case. We want to verify: * Output is always a string * Output matches expected format * Function handles edge cases correctly * Performance meets requirements", "url": "https://ai.pydantic.dev/evals/examples/simple-validation/index.html#scenario", "page": "evals/examples/simple-validation/index.html", "source_site": "pydantic_ai"}
{"title": "Complete Example", "anchor": "complete-example", "heading_level": 2, "md_text": "", "url": "https://ai.pydantic.dev/evals/examples/simple-validation/index.html#complete-example", "page": "evals/examples/simple-validation/index.html", "source_site": "pydantic_ai"}
{"title": "Expected Output", "anchor": "expected-output", "heading_level": 2, "md_text": "Note: The empty_string case has one failed assertion ( has_capitals ) because an empty string contains no capital letters.", "url": "https://ai.pydantic.dev/evals/examples/simple-validation/index.html#expected-output", "page": "evals/examples/simple-validation/index.html", "source_site": "pydantic_ai"}
{"title": "Saving and Loading", "anchor": "saving-and-loading", "heading_level": 2, "md_text": "Save the dataset for future use:", "url": "https://ai.pydantic.dev/evals/examples/simple-validation/index.html#saving-and-loading", "page": "evals/examples/simple-validation/index.html", "source_site": "pydantic_ai"}
{"title": "Adding More Cases", "anchor": "adding-more-cases", "heading_level": 2, "md_text": "As you find bugs or edge cases, add them to the dataset:", "url": "https://ai.pydantic.dev/evals/examples/simple-validation/index.html#adding-more-cases", "page": "evals/examples/simple-validation/index.html", "source_site": "pydantic_ai"}
{"title": "Using with pytest", "anchor": "using-with-pytest", "heading_level": 2, "md_text": "Integrate with pytest for CI/CD:", "url": "https://ai.pydantic.dev/evals/examples/simple-validation/index.html#using-with-pytest", "page": "evals/examples/simple-validation/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "* **[Built-in Evaluators](../../evaluators/built-in/index.html)** - Explore all available evaluators * **[Custom Evaluators](../../evaluators/custom/index.html)** - Write your own evaluation logic * **[Dataset Management](../../how-to/dataset-management/index.html)** - Save, load, and manage datasets * **[Concurrency & Performance](../../how-to/concurrency/index.html)** - Optimize evaluation performance", "url": "https://ai.pydantic.dev/evals/examples/simple-validation/index.html#next-steps", "page": "evals/examples/simple-validation/index.html", "source_site": "pydantic_ai"}
{"title": "LLM Judge Deep Dive", "anchor": "llm-judge-deep-dive", "heading_level": 1, "md_text": "The [ LLMJudge ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.LLMJudge) evaluator uses an LLM to assess subjective qualities of outputs based on a rubric. ## When to Use LLM-as-a-Judge LLM judges are ideal for evaluating qualities that require understanding and judgment: **Good Use Cases:** * Factual accuracy * Helpfulness and relevance * Tone and style compliance * Completeness of responses * Following complex instructions * RAG groundedness (does the answer use provided context?) * Citation accuracy **Poor Use Cases:** * Format validation (use [ IsInstance ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.IsInstance) instead) * Exact matching (use [ EqualsExpected ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EqualsExpected)) * Performance checks (use [ MaxDuration ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.MaxDuration)) * Deterministic logic (write a custom evaluator) ## Basic Usage ## Configuration Options ### Rubric The rubric is your evaluation criteria. Be specific and clear: **Bad rubrics (vague):** **Good rubrics (specific):** ### Including Context Control what information the judge sees: **Example:** ### Model Selection Choose the judge model based on cost/quality tradeoffs: ### Model Settings Customize model behavior: ## Output Modes ### Assertion Only (Default) Returns pass/fail with reason: In reports: ### Score Only Returns a numeric score (0.0 to 1.0): In reports: ### Both Score and Assertion ### Custom Names In reports: ## Practical Examples ### RAG Evaluation Evaluate whether a RAG system uses provided context: ### Recipe Generation with Case-Specific Rubrics This example shows how to use both dataset-level and case-specific evaluators: recipe\\_evaluation.py 1. Case-specific evaluator - only runs for the vegetarian recipe case 2. Case-specific evaluator - only runs for the gluten-free recipe case 3. Dataset-level evaluators - run for all cases ### Multi-Aspect Evaluation Use multiple judges for different quality dimensions: ### Comparative Evaluation Compare output against expected output: ## Best Practices ### 1. Be Specific in Rubrics **Bad:** **Better:** **Best:** ### 2. Use Multiple Judges Don't always try to evaluate everything with one rubric: ### 3. Combine with Deterministic Checks Don't use LLM evaluation for checks that can be done deterministically: ### 4. Use Temperature 0 for Consistency ## Limitations ### Non-Determinism LLM judges are not deterministic. The same output may receive different scores across runs. **Mitigation:** * Use temperature=0.0 for more consistency * Run multiple evaluations and average * Use retry strategies for flaky evaluations ### Cost LLM judges make API calls, which cost money and time. **Mitigation:** * Use cheaper models for simple checks ( gpt-4o-mini ) * Run deterministic checks first to fail fast * Cache results when possible * Limit evaluation to changed cases ### Model Biases LLM judges inherit biases from their training data. **Mitigation:** * Use multiple judge models and compare * Review evaluation reasons, not just scores * Validate judges against human-labeled test sets * Be aware of known biases (length bias, style preferences) ### Context Limits Judges have token limits for inputs. **Mitigation:** * Truncate long inputs/outputs intelligently * Use focused rubrics that don't require full context * Consider chunked evaluation for very long content ## Debugging LLM Judges ### View Reasons Output: ### Access Programmatically ### Compare Judges Test the same cases with different judge models: ## Advanced: Custom Judge Models Set a default judge model for all LLMJudge evaluators: ## Next Steps * **[Custom Evaluators](../custom/index.html)** - Write custom evaluation logic * **[Built-in Evaluators](../built-in/index.html)** - Complete evaluator reference", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#llm-judge-deep-dive", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "When to Use LLM-as-a-Judge", "anchor": "when-to-use-llm-as-a-judge", "heading_level": 2, "md_text": "LLM judges are ideal for evaluating qualities that require understanding and judgment: **Good Use Cases:** * Factual accuracy * Helpfulness and relevance * Tone and style compliance * Completeness of responses * Following complex instructions * RAG groundedness (does the answer use provided context?) * Citation accuracy **Poor Use Cases:** * Format validation (use [ IsInstance ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.IsInstance) instead) * Exact matching (use [ EqualsExpected ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EqualsExpected)) * Performance checks (use [ MaxDuration ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.MaxDuration)) * Deterministic logic (write a custom evaluator)", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#when-to-use-llm-as-a-judge", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Basic Usage", "anchor": "basic-usage", "heading_level": 2, "md_text": "", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#basic-usage", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Configuration Options", "anchor": "configuration-options", "heading_level": 2, "md_text": "### Rubric The rubric is your evaluation criteria. Be specific and clear: **Bad rubrics (vague):** **Good rubrics (specific):** ### Including Context Control what information the judge sees: **Example:** ### Model Selection Choose the judge model based on cost/quality tradeoffs: ### Model Settings Customize model behavior:", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#configuration-options", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Rubric", "anchor": "rubric", "heading_level": 3, "md_text": "The rubric is your evaluation criteria. Be specific and clear: **Bad rubrics (vague):** **Good rubrics (specific):**", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#rubric", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Including Context", "anchor": "including-context", "heading_level": 3, "md_text": "Control what information the judge sees: **Example:**", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#including-context", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Model Selection", "anchor": "model-selection", "heading_level": 3, "md_text": "Choose the judge model based on cost/quality tradeoffs:", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#model-selection", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Model Settings", "anchor": "model-settings", "heading_level": 3, "md_text": "Customize model behavior:", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#model-settings", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Output Modes", "anchor": "output-modes", "heading_level": 2, "md_text": "### Assertion Only (Default) Returns pass/fail with reason: In reports: ### Score Only Returns a numeric score (0.0 to 1.0): In reports: ### Both Score and Assertion ### Custom Names In reports:", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#output-modes", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Assertion Only (Default)", "anchor": "assertion-only-default", "heading_level": 3, "md_text": "Returns pass/fail with reason: In reports:", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#assertion-only-default", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Score Only", "anchor": "score-only", "heading_level": 3, "md_text": "Returns a numeric score (0.0 to 1.0): In reports:", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#score-only", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Both Score and Assertion", "anchor": "both-score-and-assertion", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#both-score-and-assertion", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Names", "anchor": "custom-names", "heading_level": 3, "md_text": "In reports:", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#custom-names", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Practical Examples", "anchor": "practical-examples", "heading_level": 2, "md_text": "### RAG Evaluation Evaluate whether a RAG system uses provided context: ### Recipe Generation with Case-Specific Rubrics This example shows how to use both dataset-level and case-specific evaluators: recipe\\_evaluation.py 1. Case-specific evaluator - only runs for the vegetarian recipe case 2. Case-specific evaluator - only runs for the gluten-free recipe case 3. Dataset-level evaluators - run for all cases ### Multi-Aspect Evaluation Use multiple judges for different quality dimensions: ### Comparative Evaluation Compare output against expected output:", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#practical-examples", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "RAG Evaluation", "anchor": "rag-evaluation", "heading_level": 3, "md_text": "Evaluate whether a RAG system uses provided context:", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#rag-evaluation", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Recipe Generation with Case-Specific Rubrics", "anchor": "recipe-generation-with-case-specific-rubrics", "heading_level": 3, "md_text": "This example shows how to use both dataset-level and case-specific evaluators: recipe\\_evaluation.py 1. Case-specific evaluator - only runs for the vegetarian recipe case 2. Case-specific evaluator - only runs for the gluten-free recipe case 3. Dataset-level evaluators - run for all cases", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#recipe-generation-with-case-specific-rubrics", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Multi-Aspect Evaluation", "anchor": "multi-aspect-evaluation", "heading_level": 3, "md_text": "Use multiple judges for different quality dimensions:", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#multi-aspect-evaluation", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Comparative Evaluation", "anchor": "comparative-evaluation", "heading_level": 3, "md_text": "Compare output against expected output:", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#comparative-evaluation", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Best Practices", "anchor": "best-practices", "heading_level": 2, "md_text": "### 1. Be Specific in Rubrics **Bad:** **Better:** **Best:** ### 2. Use Multiple Judges Don't always try to evaluate everything with one rubric: ### 3. Combine with Deterministic Checks Don't use LLM evaluation for checks that can be done deterministically: ### 4. Use Temperature 0 for Consistency", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#best-practices", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "1. Be Specific in Rubrics", "anchor": "1-be-specific-in-rubrics", "heading_level": 3, "md_text": "**Bad:** **Better:** **Best:**", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#1-be-specific-in-rubrics", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "2. Use Multiple Judges", "anchor": "2-use-multiple-judges", "heading_level": 3, "md_text": "Don't always try to evaluate everything with one rubric:", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#2-use-multiple-judges", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "3. Combine with Deterministic Checks", "anchor": "3-combine-with-deterministic-checks", "heading_level": 3, "md_text": "Don't use LLM evaluation for checks that can be done deterministically:", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#3-combine-with-deterministic-checks", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "4. Use Temperature 0 for Consistency", "anchor": "4-use-temperature-0-for-consistency", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#4-use-temperature-0-for-consistency", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Limitations", "anchor": "limitations", "heading_level": 2, "md_text": "### Non-Determinism LLM judges are not deterministic. The same output may receive different scores across runs. **Mitigation:** * Use temperature=0.0 for more consistency * Run multiple evaluations and average * Use retry strategies for flaky evaluations ### Cost LLM judges make API calls, which cost money and time. **Mitigation:** * Use cheaper models for simple checks ( gpt-4o-mini ) * Run deterministic checks first to fail fast * Cache results when possible * Limit evaluation to changed cases ### Model Biases LLM judges inherit biases from their training data. **Mitigation:** * Use multiple judge models and compare * Review evaluation reasons, not just scores * Validate judges against human-labeled test sets * Be aware of known biases (length bias, style preferences) ### Context Limits Judges have token limits for inputs. **Mitigation:** * Truncate long inputs/outputs intelligently * Use focused rubrics that don't require full context * Consider chunked evaluation for very long content", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#limitations", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Non-Determinism", "anchor": "non-determinism", "heading_level": 3, "md_text": "LLM judges are not deterministic. The same output may receive different scores across runs. **Mitigation:** * Use temperature=0.0 for more consistency * Run multiple evaluations and average * Use retry strategies for flaky evaluations", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#non-determinism", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Cost", "anchor": "cost", "heading_level": 3, "md_text": "LLM judges make API calls, which cost money and time. **Mitigation:** * Use cheaper models for simple checks ( gpt-4o-mini ) * Run deterministic checks first to fail fast * Cache results when possible * Limit evaluation to changed cases", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#cost", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Model Biases", "anchor": "model-biases", "heading_level": 3, "md_text": "LLM judges inherit biases from their training data. **Mitigation:** * Use multiple judge models and compare * Review evaluation reasons, not just scores * Validate judges against human-labeled test sets * Be aware of known biases (length bias, style preferences)", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#model-biases", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Context Limits", "anchor": "context-limits", "heading_level": 3, "md_text": "Judges have token limits for inputs. **Mitigation:** * Truncate long inputs/outputs intelligently * Use focused rubrics that don't require full context * Consider chunked evaluation for very long content", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#context-limits", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Debugging LLM Judges", "anchor": "debugging-llm-judges", "heading_level": 2, "md_text": "### View Reasons Output: ### Access Programmatically ### Compare Judges Test the same cases with different judge models:", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#debugging-llm-judges", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "View Reasons", "anchor": "view-reasons", "heading_level": 3, "md_text": "Output:", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#view-reasons", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Access Programmatically", "anchor": "access-programmatically", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#access-programmatically", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Compare Judges", "anchor": "compare-judges", "heading_level": 3, "md_text": "Test the same cases with different judge models:", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#compare-judges", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Advanced: Custom Judge Models", "anchor": "advanced-custom-judge-models", "heading_level": 2, "md_text": "Set a default judge model for all LLMJudge evaluators:", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#advanced-custom-judge-models", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "* **[Custom Evaluators](../custom/index.html)** - Write custom evaluation logic * **[Built-in Evaluators](../built-in/index.html)** - Complete evaluator reference", "url": "https://ai.pydantic.dev/evals/evaluators/llm-judge/index.html#next-steps", "page": "evals/evaluators/llm-judge/index.html", "source_site": "pydantic_ai"}
{"title": "Concurrency & Performance", "anchor": "concurrency-performance", "heading_level": 1, "md_text": "Control how evaluation cases are executed in parallel. ## Overview By default, Pydantic Evals runs all cases concurrently to maximize throughput. You can control this behavior using the max_concurrency parameter. ## Basic Usage ## When to Limit Concurrency ### Rate Limiting Many APIs have rate limits that restrict concurrent requests: ### Resource Constraints Limit concurrency to avoid overwhelming system resources: ### Debugging Run sequentially to see clear error traces: ## Performance Comparison Here's an example showing the performance difference: concurrency\\_example.py ## Concurrency with Evaluators Both task execution and evaluator execution happen concurrently by default: If your evaluators are expensive (e.g., [ LLMJudge ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.LLMJudge)), limiting concurrency helps manage: - API rate limits - Cost (fewer concurrent API calls) - Memory usage ## Async vs Sync Both sync and async evaluation support concurrency control: ### Sync API ### Async API ## Monitoring Concurrency Track execution to optimize settings: ## Handling Rate Limits If you hit rate limits, the evaluation will fail. Use retry strategies: See [Retry Strategies](../retry-strategies/index.html) for handling transient failures. ## Next Steps * **[Retry Strategies](../retry-strategies/index.html)** - Handle transient failures * **[Dataset Management](../dataset-management/index.html)** - Work with large datasets * **[Logfire Integration](../logfire-integration/index.html)** - Monitor performance", "url": "https://ai.pydantic.dev/evals/how-to/concurrency/index.html#concurrency-performance", "page": "evals/how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "By default, Pydantic Evals runs all cases concurrently to maximize throughput. You can control this behavior using the max_concurrency parameter.", "url": "https://ai.pydantic.dev/evals/how-to/concurrency/index.html#overview", "page": "evals/how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Basic Usage", "anchor": "basic-usage", "heading_level": 2, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/concurrency/index.html#basic-usage", "page": "evals/how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "When to Limit Concurrency", "anchor": "when-to-limit-concurrency", "heading_level": 2, "md_text": "### Rate Limiting Many APIs have rate limits that restrict concurrent requests: ### Resource Constraints Limit concurrency to avoid overwhelming system resources: ### Debugging Run sequentially to see clear error traces:", "url": "https://ai.pydantic.dev/evals/how-to/concurrency/index.html#when-to-limit-concurrency", "page": "evals/how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Rate Limiting", "anchor": "rate-limiting", "heading_level": 3, "md_text": "Many APIs have rate limits that restrict concurrent requests:", "url": "https://ai.pydantic.dev/evals/how-to/concurrency/index.html#rate-limiting", "page": "evals/how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Resource Constraints", "anchor": "resource-constraints", "heading_level": 3, "md_text": "Limit concurrency to avoid overwhelming system resources:", "url": "https://ai.pydantic.dev/evals/how-to/concurrency/index.html#resource-constraints", "page": "evals/how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Debugging", "anchor": "debugging", "heading_level": 3, "md_text": "Run sequentially to see clear error traces:", "url": "https://ai.pydantic.dev/evals/how-to/concurrency/index.html#debugging", "page": "evals/how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Performance Comparison", "anchor": "performance-comparison", "heading_level": 2, "md_text": "Here's an example showing the performance difference: concurrency\\_example.py", "url": "https://ai.pydantic.dev/evals/how-to/concurrency/index.html#performance-comparison", "page": "evals/how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Concurrency with Evaluators", "anchor": "concurrency-with-evaluators", "heading_level": 2, "md_text": "Both task execution and evaluator execution happen concurrently by default: If your evaluators are expensive (e.g., [ LLMJudge ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.LLMJudge)), limiting concurrency helps manage: - API rate limits - Cost (fewer concurrent API calls) - Memory usage", "url": "https://ai.pydantic.dev/evals/how-to/concurrency/index.html#concurrency-with-evaluators", "page": "evals/how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Async vs Sync", "anchor": "async-vs-sync", "heading_level": 2, "md_text": "Both sync and async evaluation support concurrency control: ### Sync API ### Async API", "url": "https://ai.pydantic.dev/evals/how-to/concurrency/index.html#async-vs-sync", "page": "evals/how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Sync API", "anchor": "sync-api", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/concurrency/index.html#sync-api", "page": "evals/how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Async API", "anchor": "async-api", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/concurrency/index.html#async-api", "page": "evals/how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Monitoring Concurrency", "anchor": "monitoring-concurrency", "heading_level": 2, "md_text": "Track execution to optimize settings:", "url": "https://ai.pydantic.dev/evals/how-to/concurrency/index.html#monitoring-concurrency", "page": "evals/how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Handling Rate Limits", "anchor": "handling-rate-limits", "heading_level": 2, "md_text": "If you hit rate limits, the evaluation will fail. Use retry strategies: See [Retry Strategies](../retry-strategies/index.html) for handling transient failures.", "url": "https://ai.pydantic.dev/evals/how-to/concurrency/index.html#handling-rate-limits", "page": "evals/how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "* **[Retry Strategies](../retry-strategies/index.html)** - Handle transient failures * **[Dataset Management](../dataset-management/index.html)** - Work with large datasets * **[Logfire Integration](../logfire-integration/index.html)** - Monitor performance", "url": "https://ai.pydantic.dev/evals/how-to/concurrency/index.html#next-steps", "page": "evals/how-to/concurrency/index.html", "source_site": "pydantic_ai"}
{"title": "Span-Based Evaluation", "anchor": "span-based-evaluation", "heading_level": 1, "md_text": "Evaluate AI system behavior by analyzing OpenTelemetry spans captured during execution. Requires Logfire Span-based evaluation requires logfire to be installed and configured: ## Overview Span-based evaluation enables you to evaluate **how** your AI system executes, not just **what** it produces. This is essential for complex agents where ensuring the desired behavior depends on the execution path taken, not just the final output. ### Why Span-Based Evaluation? Traditional evaluators assess task inputs and outputs. For simple tasks, this may be sufficientif the output is correct, the task succeeded. But for complex multi-step agents, the *process* matters as much as the result: * **A correct answer reached incorrectly** - An agent might produce the right output by accident (e.g., guessing, using cached data when it should have searched, calling the wrong tools but getting lucky) * **Verification of required behaviors** - You need to ensure specific tools were called, certain code paths executed, or particular patterns followed * **Performance and efficiency** - The agent should reach the answer efficiently, without unnecessary tool calls, infinite loops, or excessive retries * **Safety and compliance** - Critical to verify that dangerous operations weren't attempted, sensitive data wasn't accessed inappropriately, or guardrails weren't bypassed ### Real-World Scenarios Span-based evaluation is particularly valuable for: * **RAG systems** - Verify documents were retrieved and reranked before generation, not just that the answer included citations * **Multi-agent coordination** - Ensure the orchestrator delegated to the right specialist agents in the correct order * **Tool-calling agents** - Confirm specific tools were used (or avoided), and in the expected sequence * **Debugging and regression testing** - Catch behavioral regressions where outputs remain correct but the internal logic deteriorates * **Production alignment** - Ensure your evaluation assertions operate on the same telemetry data captured in production, so eval insights directly translate to production monitoring ### How It Works When you configure logfire ( logfire.configure() ), Pydantic Evals captures all OpenTelemetry spans generated during task execution. You can then write evaluators that assert conditions on: * **Which tools were called** - HasMatchingSpan(query={'name_contains': 'search_tool'}) * **Code paths executed** - Verify specific functions ran or particular branches taken * **Timing characteristics** - Check that operations complete within SLA bounds * **Error conditions** - Detect retries, fallbacks, or specific failure modes * **Execution structure** - Verify parent-child relationships, delegation patterns, or execution order This creates a fundamentally different evaluation paradigm: you're testing behavioral contracts, not just input-output relationships. ## Basic Usage ## HasMatchingSpan Evaluator The [ HasMatchingSpan ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.HasMatchingSpan) evaluator checks if any span matches a query: **Returns:** bool - True if any span matches the query ## SpanQuery Reference A [ SpanQuery ](../../../api/pydantic_evals/otel/index.html#pydantic_evals.otel.SpanQuery) is a dictionary with query conditions: ### Name Conditions Match spans by name: ### Attribute Conditions Match spans with specific attributes: ### Duration Conditions Match based on execution time: ### Logical Operators Combine conditions: ### Child/Descendant Conditions Query relationships between spans: ### Ancestor/Depth Conditions Query span hierarchy: ### Stop Recursing Control recursive queries: ## Practical Examples ### Verify Tool Usage Check that specific tools were called: ### Check Multiple Tools Verify a sequence of operations: ### Performance Assertions Ensure operations meet latency requirements: ### Error Detection Check for error conditions: ### Complex Behavioral Checks Verify sophisticated behavior patterns: ## Custom Evaluators with SpanTree For more complex span analysis, write custom evaluators: ### SpanTree API The [ SpanTree ](../../../api/pydantic_evals/otel/index.html#pydantic_evals.otel.SpanTree) provides methods for span analysis: ### SpanNode Properties Each [ SpanNode ](../../../api/pydantic_evals/otel/index.html#pydantic_evals.otel.SpanNode) has: ## Debugging Span Queries ### View Spans in Logfire If you're sending data to Logfire, you can view all spans in the web UI to understand the trace structure. ### Print Span Tree ### Query Testing Test queries incrementally: ## Use Cases ### RAG System Verification Verify retrieval-augmented generation workflow: ### Multi-Agent Systems Verify agent coordination: ### Tool Usage Patterns Verify intelligent tool selection: ## Best Practices 1. **Start Simple**: Begin with basic name queries, add complexity as needed 2. **Use Descriptive Names**: Name your spans well in your application code 3. **Test Queries**: Verify queries work before running full evaluations 4. **Combine with Other Evaluators**: Use span checks alongside output validation 5. **Document Expectations**: Comment why specific spans should/shouldn't exist ## Next Steps * **[Logfire Integration](../../how-to/logfire-integration/index.html)** - Set up Logfire for span capture * **[Custom Evaluators](../custom/index.html)** - Write advanced span analysis * **[Built-in Evaluators](../built-in/index.html)** - Other evaluator types", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#span-based-evaluation", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "Span-based evaluation enables you to evaluate **how** your AI system executes, not just **what** it produces. This is essential for complex agents where ensuring the desired behavior depends on the execution path taken, not just the final output. ### Why Span-Based Evaluation? Traditional evaluators assess task inputs and outputs. For simple tasks, this may be sufficientif the output is correct, the task succeeded. But for complex multi-step agents, the *process* matters as much as the result: * **A correct answer reached incorrectly** - An agent might produce the right output by accident (e.g., guessing, using cached data when it should have searched, calling the wrong tools but getting lucky) * **Verification of required behaviors** - You need to ensure specific tools were called, certain code paths executed, or particular patterns followed * **Performance and efficiency** - The agent should reach the answer efficiently, without unnecessary tool calls, infinite loops, or excessive retries * **Safety and compliance** - Critical to verify that dangerous operations weren't attempted, sensitive data wasn't accessed inappropriately, or guardrails weren't bypassed ### Real-World Scenarios Span-based evaluation is particularly valuable for: * **RAG systems** - Verify documents were retrieved and reranked before generation, not just that the answer included citations * **Multi-agent coordination** - Ensure the orchestrator delegated to the right specialist agents in the correct order * **Tool-calling agents** - Confirm specific tools were used (or avoided), and in the expected sequence * **Debugging and regression testing** - Catch behavioral regressions where outputs remain correct but the internal logic deteriorates * **Production alignment** - Ensure your evaluation assertions operate on the same telemetry data captured in production, so eval insights directly translate to production monitoring ### How It Works When you configure logfire ( logfire.configure() ), Pydantic Evals captures all OpenTelemetry spans generated during task execution. You can then write evaluators that assert conditions on: * **Which tools were called** - HasMatchingSpan(query={'name_contains': 'search_tool'}) * **Code paths executed** - Verify specific functions ran or particular branches taken * **Timing characteristics** - Check that operations complete within SLA bounds * **Error conditions** - Detect retries, fallbacks, or specific failure modes * **Execution structure** - Verify parent-child relationships, delegation patterns, or execution order This creates a fundamentally different evaluation paradigm: you're testing behavioral contracts, not just input-output relationships.", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#overview", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Why Span-Based Evaluation?", "anchor": "why-span-based-evaluation", "heading_level": 3, "md_text": "Traditional evaluators assess task inputs and outputs. For simple tasks, this may be sufficientif the output is correct, the task succeeded. But for complex multi-step agents, the *process* matters as much as the result: * **A correct answer reached incorrectly** - An agent might produce the right output by accident (e.g., guessing, using cached data when it should have searched, calling the wrong tools but getting lucky) * **Verification of required behaviors** - You need to ensure specific tools were called, certain code paths executed, or particular patterns followed * **Performance and efficiency** - The agent should reach the answer efficiently, without unnecessary tool calls, infinite loops, or excessive retries * **Safety and compliance** - Critical to verify that dangerous operations weren't attempted, sensitive data wasn't accessed inappropriately, or guardrails weren't bypassed", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#why-span-based-evaluation", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Real-World Scenarios", "anchor": "real-world-scenarios", "heading_level": 3, "md_text": "Span-based evaluation is particularly valuable for: * **RAG systems** - Verify documents were retrieved and reranked before generation, not just that the answer included citations * **Multi-agent coordination** - Ensure the orchestrator delegated to the right specialist agents in the correct order * **Tool-calling agents** - Confirm specific tools were used (or avoided), and in the expected sequence * **Debugging and regression testing** - Catch behavioral regressions where outputs remain correct but the internal logic deteriorates * **Production alignment** - Ensure your evaluation assertions operate on the same telemetry data captured in production, so eval insights directly translate to production monitoring", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#real-world-scenarios", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "How It Works", "anchor": "how-it-works", "heading_level": 3, "md_text": "When you configure logfire ( logfire.configure() ), Pydantic Evals captures all OpenTelemetry spans generated during task execution. You can then write evaluators that assert conditions on: * **Which tools were called** - HasMatchingSpan(query={'name_contains': 'search_tool'}) * **Code paths executed** - Verify specific functions ran or particular branches taken * **Timing characteristics** - Check that operations complete within SLA bounds * **Error conditions** - Detect retries, fallbacks, or specific failure modes * **Execution structure** - Verify parent-child relationships, delegation patterns, or execution order This creates a fundamentally different evaluation paradigm: you're testing behavioral contracts, not just input-output relationships.", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#how-it-works", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Basic Usage", "anchor": "basic-usage", "heading_level": 2, "md_text": "", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#basic-usage", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "HasMatchingSpan Evaluator", "anchor": "hasmatchingspan-evaluator", "heading_level": 2, "md_text": "The [ HasMatchingSpan ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.HasMatchingSpan) evaluator checks if any span matches a query: **Returns:** bool - True if any span matches the query", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#hasmatchingspan-evaluator", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "SpanQuery Reference", "anchor": "spanquery-reference", "heading_level": 2, "md_text": "A [ SpanQuery ](../../../api/pydantic_evals/otel/index.html#pydantic_evals.otel.SpanQuery) is a dictionary with query conditions: ### Name Conditions Match spans by name: ### Attribute Conditions Match spans with specific attributes: ### Duration Conditions Match based on execution time: ### Logical Operators Combine conditions: ### Child/Descendant Conditions Query relationships between spans: ### Ancestor/Depth Conditions Query span hierarchy: ### Stop Recursing Control recursive queries:", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#spanquery-reference", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Name Conditions", "anchor": "name-conditions", "heading_level": 3, "md_text": "Match spans by name:", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#name-conditions", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Attribute Conditions", "anchor": "attribute-conditions", "heading_level": 3, "md_text": "Match spans with specific attributes:", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#attribute-conditions", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Duration Conditions", "anchor": "duration-conditions", "heading_level": 3, "md_text": "Match based on execution time:", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#duration-conditions", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Logical Operators", "anchor": "logical-operators", "heading_level": 3, "md_text": "Combine conditions:", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#logical-operators", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Child/Descendant Conditions", "anchor": "childdescendant-conditions", "heading_level": 3, "md_text": "Query relationships between spans:", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#childdescendant-conditions", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Ancestor/Depth Conditions", "anchor": "ancestordepth-conditions", "heading_level": 3, "md_text": "Query span hierarchy:", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#ancestordepth-conditions", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Stop Recursing", "anchor": "stop-recursing", "heading_level": 3, "md_text": "Control recursive queries:", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#stop-recursing", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Practical Examples", "anchor": "practical-examples", "heading_level": 2, "md_text": "### Verify Tool Usage Check that specific tools were called: ### Check Multiple Tools Verify a sequence of operations: ### Performance Assertions Ensure operations meet latency requirements: ### Error Detection Check for error conditions: ### Complex Behavioral Checks Verify sophisticated behavior patterns:", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#practical-examples", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Verify Tool Usage", "anchor": "verify-tool-usage", "heading_level": 3, "md_text": "Check that specific tools were called:", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#verify-tool-usage", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Check Multiple Tools", "anchor": "check-multiple-tools", "heading_level": 3, "md_text": "Verify a sequence of operations:", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#check-multiple-tools", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Performance Assertions", "anchor": "performance-assertions", "heading_level": 3, "md_text": "Ensure operations meet latency requirements:", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#performance-assertions", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Error Detection", "anchor": "error-detection", "heading_level": 3, "md_text": "Check for error conditions:", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#error-detection", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Complex Behavioral Checks", "anchor": "complex-behavioral-checks", "heading_level": 3, "md_text": "Verify sophisticated behavior patterns:", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#complex-behavioral-checks", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Evaluators with SpanTree", "anchor": "custom-evaluators-with-spantree", "heading_level": 2, "md_text": "For more complex span analysis, write custom evaluators: ### SpanTree API The [ SpanTree ](../../../api/pydantic_evals/otel/index.html#pydantic_evals.otel.SpanTree) provides methods for span analysis: ### SpanNode Properties Each [ SpanNode ](../../../api/pydantic_evals/otel/index.html#pydantic_evals.otel.SpanNode) has:", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#custom-evaluators-with-spantree", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "SpanTree API", "anchor": "spantree-api", "heading_level": 3, "md_text": "The [ SpanTree ](../../../api/pydantic_evals/otel/index.html#pydantic_evals.otel.SpanTree) provides methods for span analysis:", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#spantree-api", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "SpanNode Properties", "anchor": "spannode-properties", "heading_level": 3, "md_text": "Each [ SpanNode ](../../../api/pydantic_evals/otel/index.html#pydantic_evals.otel.SpanNode) has:", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#spannode-properties", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Debugging Span Queries", "anchor": "debugging-span-queries", "heading_level": 2, "md_text": "### View Spans in Logfire If you're sending data to Logfire, you can view all spans in the web UI to understand the trace structure. ### Print Span Tree ### Query Testing Test queries incrementally:", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#debugging-span-queries", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "View Spans in Logfire", "anchor": "view-spans-in-logfire", "heading_level": 3, "md_text": "If you're sending data to Logfire, you can view all spans in the web UI to understand the trace structure.", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#view-spans-in-logfire", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Print Span Tree", "anchor": "print-span-tree", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#print-span-tree", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Query Testing", "anchor": "query-testing", "heading_level": 3, "md_text": "Test queries incrementally:", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#query-testing", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Use Cases", "anchor": "use-cases", "heading_level": 2, "md_text": "### RAG System Verification Verify retrieval-augmented generation workflow: ### Multi-Agent Systems Verify agent coordination: ### Tool Usage Patterns Verify intelligent tool selection:", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#use-cases", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "RAG System Verification", "anchor": "rag-system-verification", "heading_level": 3, "md_text": "Verify retrieval-augmented generation workflow:", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#rag-system-verification", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Multi-Agent Systems", "anchor": "multi-agent-systems", "heading_level": 3, "md_text": "Verify agent coordination:", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#multi-agent-systems", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Tool Usage Patterns", "anchor": "tool-usage-patterns", "heading_level": 3, "md_text": "Verify intelligent tool selection:", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#tool-usage-patterns", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Best Practices", "anchor": "best-practices", "heading_level": 2, "md_text": "1. **Start Simple**: Begin with basic name queries, add complexity as needed 2. **Use Descriptive Names**: Name your spans well in your application code 3. **Test Queries**: Verify queries work before running full evaluations 4. **Combine with Other Evaluators**: Use span checks alongside output validation 5. **Document Expectations**: Comment why specific spans should/shouldn't exist", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#best-practices", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "* **[Logfire Integration](../../how-to/logfire-integration/index.html)** - Set up Logfire for span capture * **[Custom Evaluators](../custom/index.html)** - Write advanced span analysis * **[Built-in Evaluators](../built-in/index.html)** - Other evaluator types", "url": "https://ai.pydantic.dev/evals/evaluators/span-based/index.html#next-steps", "page": "evals/evaluators/span-based/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Evaluators", "anchor": "custom-evaluators", "heading_level": 1, "md_text": "Write custom evaluators for domain-specific logic, external integrations, or specialized metrics. ## Basic Custom Evaluator All evaluators inherit from [ Evaluator ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.Evaluator) and must implement evaluate : **Key Points:** * Use @dataclass decorator (required) * Inherit from Evaluator * Implement evaluate(self, ctx: EvaluatorContext) -> EvaluatorOutput * Return bool , int , float , str , [ EvaluationReason ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluationReason), or dict of these ## EvaluatorContext The context provides all information about the case execution: ## Evaluator Parameters Add configurable parameters as dataclass fields: ## Return Types ### Boolean Assertions Simple pass/fail checks: ### Numeric Scores Quality metrics: ### String Labels Categorical classifications: ### With Reasons Add explanations to any result: ### Multiple Results You can return multiple evaluations from one evaluator by returning a dictionary of key-value pairs. Each key in the returned dictionary becomes a separate result in the report. Values can be: * Primitives ( bool , int , float , str ) * [ EvaluationReason ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluationReason) (value with explanation) * Nested dicts of these types The [ EvaluatorOutput ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorOutput) type represents all legal values that can be returned by an evaluator, and can be used as the return type annotation for your custom evaluate method. ### Conditional Results Evaluators can dynamically choose whether to produce results for a given case by returning an empty dict when not applicable: This pattern is useful when: * An evaluator only applies to certain types of outputs (e.g., code validation only for code outputs) * Validation depends on metadata tags (e.g., only evaluate cases marked with language='python' ) * You want to run expensive checks conditionally based on other evaluator results **Key Points:** * Returning {} means \"this evaluator doesn't apply here\" - the case won't show results from this evaluator * Returning {'key': value} means \"this evaluator applies and here are the results\" * This is more practical than using case-level evaluators when it applies to a large fraction of cases, or when the condition is based on the output itself * The evaluator still runs for every case, but can short-circuit when not relevant ## Async Evaluators Use async def for I/O-bound operations: Pydantic Evals handles both sync and async evaluators automatically. ## Using Metadata Access case metadata for context-aware evaluation: ## Using Metrics Access custom metrics set during task execution: See [Metrics & Attributes Guide](../../how-to/metrics-attributes/index.html) for more. ## Generic Type Parameters Make evaluators type-safe with generics: ## Custom Evaluation Names Control how evaluations appear in reports: Or use the evaluation_name field (if using the built-in pattern): ## Real-World Examples ### SQL Validation ### Code Execution ### External API Validation ## Testing Evaluators Test evaluators like any other Python code: ## Best Practices ### 1. Keep Evaluators Focused Each evaluator should check one thing: Some exceptions to this: * When there is a significant amount of shared computation or network request latency, it may be better to have a single evaluator calculate all dependent outputs together. * If multiple checks are tightly coupled or very closely related to each other, it may make sense to include all their logic in one evaluator. ### 2. Handle Missing Data Gracefully ### 3. Provide Helpful Reasons ### 4. Use Timeouts for External Calls ## Next Steps * **[Span-Based Evaluation](../span-based/index.html)** - Using OpenTelemetry spans * **[Examples](../../examples/simple-validation/index.html)** - Practical examples", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#custom-evaluators", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Basic Custom Evaluator", "anchor": "basic-custom-evaluator", "heading_level": 2, "md_text": "All evaluators inherit from [ Evaluator ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.Evaluator) and must implement evaluate : **Key Points:** * Use @dataclass decorator (required) * Inherit from Evaluator * Implement evaluate(self, ctx: EvaluatorContext) -> EvaluatorOutput * Return bool , int , float , str , [ EvaluationReason ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluationReason), or dict of these", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#basic-custom-evaluator", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "EvaluatorContext", "anchor": "evaluatorcontext", "heading_level": 2, "md_text": "The context provides all information about the case execution:", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#evaluatorcontext", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluator Parameters", "anchor": "evaluator-parameters", "heading_level": 2, "md_text": "Add configurable parameters as dataclass fields:", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#evaluator-parameters", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Return Types", "anchor": "return-types", "heading_level": 2, "md_text": "### Boolean Assertions Simple pass/fail checks: ### Numeric Scores Quality metrics: ### String Labels Categorical classifications: ### With Reasons Add explanations to any result: ### Multiple Results You can return multiple evaluations from one evaluator by returning a dictionary of key-value pairs. Each key in the returned dictionary becomes a separate result in the report. Values can be: * Primitives ( bool , int , float , str ) * [ EvaluationReason ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluationReason) (value with explanation) * Nested dicts of these types The [ EvaluatorOutput ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorOutput) type represents all legal values that can be returned by an evaluator, and can be used as the return type annotation for your custom evaluate method. ### Conditional Results Evaluators can dynamically choose whether to produce results for a given case by returning an empty dict when not applicable: This pattern is useful when: * An evaluator only applies to certain types of outputs (e.g., code validation only for code outputs) * Validation depends on metadata tags (e.g., only evaluate cases marked with language='python' ) * You want to run expensive checks conditionally based on other evaluator results **Key Points:** * Returning {} means \"this evaluator doesn't apply here\" - the case won't show results from this evaluator * Returning {'key': value} means \"this evaluator applies and here are the results\" * This is more practical than using case-level evaluators when it applies to a large fraction of cases, or when the condition is based on the output itself * The evaluator still runs for every case, but can short-circuit when not relevant", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#return-types", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Boolean Assertions", "anchor": "boolean-assertions", "heading_level": 3, "md_text": "Simple pass/fail checks:", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#boolean-assertions", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Numeric Scores", "anchor": "numeric-scores", "heading_level": 3, "md_text": "Quality metrics:", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#numeric-scores", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "String Labels", "anchor": "string-labels", "heading_level": 3, "md_text": "Categorical classifications:", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#string-labels", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "With Reasons", "anchor": "with-reasons", "heading_level": 3, "md_text": "Add explanations to any result:", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#with-reasons", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Multiple Results", "anchor": "multiple-results", "heading_level": 3, "md_text": "You can return multiple evaluations from one evaluator by returning a dictionary of key-value pairs. Each key in the returned dictionary becomes a separate result in the report. Values can be: * Primitives ( bool , int , float , str ) * [ EvaluationReason ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluationReason) (value with explanation) * Nested dicts of these types The [ EvaluatorOutput ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorOutput) type represents all legal values that can be returned by an evaluator, and can be used as the return type annotation for your custom evaluate method.", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#multiple-results", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Conditional Results", "anchor": "conditional-results", "heading_level": 3, "md_text": "Evaluators can dynamically choose whether to produce results for a given case by returning an empty dict when not applicable: This pattern is useful when: * An evaluator only applies to certain types of outputs (e.g., code validation only for code outputs) * Validation depends on metadata tags (e.g., only evaluate cases marked with language='python' ) * You want to run expensive checks conditionally based on other evaluator results **Key Points:** * Returning {} means \"this evaluator doesn't apply here\" - the case won't show results from this evaluator * Returning {'key': value} means \"this evaluator applies and here are the results\" * This is more practical than using case-level evaluators when it applies to a large fraction of cases, or when the condition is based on the output itself * The evaluator still runs for every case, but can short-circuit when not relevant", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#conditional-results", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Async Evaluators", "anchor": "async-evaluators", "heading_level": 2, "md_text": "Use async def for I/O-bound operations: Pydantic Evals handles both sync and async evaluators automatically.", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#async-evaluators", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Using Metadata", "anchor": "using-metadata", "heading_level": 2, "md_text": "Access case metadata for context-aware evaluation:", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#using-metadata", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Using Metrics", "anchor": "using-metrics", "heading_level": 2, "md_text": "Access custom metrics set during task execution: See [Metrics & Attributes Guide](../../how-to/metrics-attributes/index.html) for more.", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#using-metrics", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Generic Type Parameters", "anchor": "generic-type-parameters", "heading_level": 2, "md_text": "Make evaluators type-safe with generics:", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#generic-type-parameters", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Evaluation Names", "anchor": "custom-evaluation-names", "heading_level": 2, "md_text": "Control how evaluations appear in reports: Or use the evaluation_name field (if using the built-in pattern):", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#custom-evaluation-names", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Real-World Examples", "anchor": "real-world-examples", "heading_level": 2, "md_text": "### SQL Validation ### Code Execution ### External API Validation", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#real-world-examples", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "SQL Validation", "anchor": "sql-validation", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#sql-validation", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Code Execution", "anchor": "code-execution", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#code-execution", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "External API Validation", "anchor": "external-api-validation", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#external-api-validation", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Testing Evaluators", "anchor": "testing-evaluators", "heading_level": 2, "md_text": "Test evaluators like any other Python code:", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#testing-evaluators", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Best Practices", "anchor": "best-practices", "heading_level": 2, "md_text": "### 1. Keep Evaluators Focused Each evaluator should check one thing: Some exceptions to this: * When there is a significant amount of shared computation or network request latency, it may be better to have a single evaluator calculate all dependent outputs together. * If multiple checks are tightly coupled or very closely related to each other, it may make sense to include all their logic in one evaluator. ### 2. Handle Missing Data Gracefully ### 3. Provide Helpful Reasons ### 4. Use Timeouts for External Calls", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#best-practices", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "1. Keep Evaluators Focused", "anchor": "1-keep-evaluators-focused", "heading_level": 3, "md_text": "Each evaluator should check one thing: Some exceptions to this: * When there is a significant amount of shared computation or network request latency, it may be better to have a single evaluator calculate all dependent outputs together. * If multiple checks are tightly coupled or very closely related to each other, it may make sense to include all their logic in one evaluator.", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#1-keep-evaluators-focused", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "2. Handle Missing Data Gracefully", "anchor": "2-handle-missing-data-gracefully", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#2-handle-missing-data-gracefully", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "3. Provide Helpful Reasons", "anchor": "3-provide-helpful-reasons", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#3-provide-helpful-reasons", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "4. Use Timeouts for External Calls", "anchor": "4-use-timeouts-for-external-calls", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#4-use-timeouts-for-external-calls", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "* **[Span-Based Evaluation](../span-based/index.html)** - Using OpenTelemetry spans * **[Examples](../../examples/simple-validation/index.html)** - Practical examples", "url": "https://ai.pydantic.dev/evals/evaluators/custom/index.html#next-steps", "page": "evals/evaluators/custom/index.html", "source_site": "pydantic_ai"}
{"title": "Logfire Integration", "anchor": "logfire-integration", "heading_level": 1, "md_text": "Visualize and analyze evaluation results using Pydantic Logfire. ## Overview Pydantic Evals uses OpenTelemetry to record traces of the evaluation process. These traces contain all the information from your evaluation reports, plus full tracing from the execution of your task function. You can send these traces to any OpenTelemetry-compatible backend, including [Pydantic Logfire](https://logfire.pydantic.dev/docs/guides/web-ui/evals/). ## Installation Install the optional logfire dependency: ## Basic Setup Configure Logfire before running evaluations: basic\\_logfire\\_setup.py 1. Sends data to Logfire only if the LOGFIRE_TOKEN environment variable is set That's it! Your evaluation traces will now appear in the Logfire web UI as long as you have the LOGFIRE_TOKEN environment variable set. ## What Gets Sent to Logfire When you run an evaluation, Logfire receives: 1. **Evaluation metadata** 1. Dataset name 2. Number of cases 3. Evaluator names 2. **Per-case data** 1. Inputs and outputs 2. Expected outputs 3. Metadata 4. Execution duration 3. **Evaluation results** 1. Scores, assertions, and labels 2. Reasons (if included) 3. Evaluator failures 4. **Task execution traces** 1. All OpenTelemetry spans from your task function 2. Tool calls (for Pydantic AI agents) 3. API calls, database queries, etc. ## Viewing Results in Logfire ### Evaluation Overview Logfire provides a special table view for evaluation results on the root evaluation span: [![Logfire Evals Overview](../../../img/logfire-evals-overview.png)](../../../img/logfire-evals-overview.png) This view shows: * Case names * Pass/fail status * Scores and assertions * Execution duration * Quick filtering and sorting ### Individual Case Details Click any case to see detailed inputs and outputs: [![Logfire Evals Case](../../../img/logfire-evals-case.png)](../../../img/logfire-evals-case.png) ### Full Trace View View the complete execution trace including all spans generated during evaluation: [![Logfire Evals Case Trace](../../../img/logfire-evals-case-trace.png)](../../../img/logfire-evals-case-trace.png) This is especially useful for: * Debugging failed cases * Understanding performance bottlenecks * Analyzing tool usage patterns * Writing span-based evaluators ## Analyzing Traces ### Comparing Runs Run the same evaluation multiple times and compare in Logfire: ### Debugging Failed Cases Find failed cases quickly: 1. Search for service_name = 'my_service_evals' AND is_exception (replace with the actual service name you are using) 2. View the full span tree to see where the failure occurred 3. Inspect attributes and logs for error messages ## Span-Based Evaluation Logfire integration enables powerful span-based evaluators. See [Span-Based Evaluation](../../evaluators/span-based/index.html) for details. Example: Verify specific tools were called: The span tree is available in both: * Your evaluator code (via ctx.span_tree ) * Logfire UI (visual trace view) ## Troubleshooting ### No Data Appearing in Logfire Check: 1. **Token is set**: echo $LOGFIRE_TOKEN 2. **Configuration is correct**: 3. **Network connectivity**: Check firewall settings 4. **Project exists**: Verify project name in Logfire UI ### Traces Missing Spans If some spans are missing: 1. **Ensure logfire is configured before imports**: 2. **Check instrumentation**: Ensure your code has enabled all instrumentations you want: ## Best Practices ### 1. Configure Early Always configure Logfire before running evaluations: ### 2. Use Descriptive Service Names And Environments ### 3. Review Periodically * Check Logfire regularly to identify patterns * Look for consistently failing cases * Analyze performance trends * Adjust evaluators based on insights ## Next Steps * **[Span-Based Evaluation](../../evaluators/span-based/index.html)** - Use OpenTelemetry spans in evaluators * **[Logfire Documentation](https://logfire.pydantic.dev/docs/guides/web-ui/evals/)** - Complete Logfire guide * **[Metrics & Attributes](../metrics-attributes/index.html)** - Add custom data to traces", "url": "https://ai.pydantic.dev/evals/how-to/logfire-integration/index.html#logfire-integration", "page": "evals/how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "Pydantic Evals uses OpenTelemetry to record traces of the evaluation process. These traces contain all the information from your evaluation reports, plus full tracing from the execution of your task function. You can send these traces to any OpenTelemetry-compatible backend, including [Pydantic Logfire](https://logfire.pydantic.dev/docs/guides/web-ui/evals/).", "url": "https://ai.pydantic.dev/evals/how-to/logfire-integration/index.html#overview", "page": "evals/how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Installation", "anchor": "installation", "heading_level": 2, "md_text": "Install the optional logfire dependency:", "url": "https://ai.pydantic.dev/evals/how-to/logfire-integration/index.html#installation", "page": "evals/how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Basic Setup", "anchor": "basic-setup", "heading_level": 2, "md_text": "Configure Logfire before running evaluations: basic\\_logfire\\_setup.py 1. Sends data to Logfire only if the LOGFIRE_TOKEN environment variable is set That's it! Your evaluation traces will now appear in the Logfire web UI as long as you have the LOGFIRE_TOKEN environment variable set.", "url": "https://ai.pydantic.dev/evals/how-to/logfire-integration/index.html#basic-setup", "page": "evals/how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "What Gets Sent to Logfire", "anchor": "what-gets-sent-to-logfire", "heading_level": 2, "md_text": "When you run an evaluation, Logfire receives: 1. **Evaluation metadata** 1. Dataset name 2. Number of cases 3. Evaluator names 2. **Per-case data** 1. Inputs and outputs 2. Expected outputs 3. Metadata 4. Execution duration 3. **Evaluation results** 1. Scores, assertions, and labels 2. Reasons (if included) 3. Evaluator failures 4. **Task execution traces** 1. All OpenTelemetry spans from your task function 2. Tool calls (for Pydantic AI agents) 3. API calls, database queries, etc.", "url": "https://ai.pydantic.dev/evals/how-to/logfire-integration/index.html#what-gets-sent-to-logfire", "page": "evals/how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Viewing Results in Logfire", "anchor": "viewing-results-in-logfire", "heading_level": 2, "md_text": "### Evaluation Overview Logfire provides a special table view for evaluation results on the root evaluation span: [![Logfire Evals Overview](../../../img/logfire-evals-overview.png)](../../../img/logfire-evals-overview.png) This view shows: * Case names * Pass/fail status * Scores and assertions * Execution duration * Quick filtering and sorting ### Individual Case Details Click any case to see detailed inputs and outputs: [![Logfire Evals Case](../../../img/logfire-evals-case.png)](../../../img/logfire-evals-case.png) ### Full Trace View View the complete execution trace including all spans generated during evaluation: [![Logfire Evals Case Trace](../../../img/logfire-evals-case-trace.png)](../../../img/logfire-evals-case-trace.png) This is especially useful for: * Debugging failed cases * Understanding performance bottlenecks * Analyzing tool usage patterns * Writing span-based evaluators", "url": "https://ai.pydantic.dev/evals/how-to/logfire-integration/index.html#viewing-results-in-logfire", "page": "evals/how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluation Overview", "anchor": "evaluation-overview", "heading_level": 3, "md_text": "Logfire provides a special table view for evaluation results on the root evaluation span: [![Logfire Evals Overview](../../../img/logfire-evals-overview.png)](../../../img/logfire-evals-overview.png) This view shows: * Case names * Pass/fail status * Scores and assertions * Execution duration * Quick filtering and sorting", "url": "https://ai.pydantic.dev/evals/how-to/logfire-integration/index.html#evaluation-overview", "page": "evals/how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Individual Case Details", "anchor": "individual-case-details", "heading_level": 3, "md_text": "Click any case to see detailed inputs and outputs: [![Logfire Evals Case](../../../img/logfire-evals-case.png)](../../../img/logfire-evals-case.png)", "url": "https://ai.pydantic.dev/evals/how-to/logfire-integration/index.html#individual-case-details", "page": "evals/how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Full Trace View", "anchor": "full-trace-view", "heading_level": 3, "md_text": "View the complete execution trace including all spans generated during evaluation: [![Logfire Evals Case Trace](../../../img/logfire-evals-case-trace.png)](../../../img/logfire-evals-case-trace.png) This is especially useful for: * Debugging failed cases * Understanding performance bottlenecks * Analyzing tool usage patterns * Writing span-based evaluators", "url": "https://ai.pydantic.dev/evals/how-to/logfire-integration/index.html#full-trace-view", "page": "evals/how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Analyzing Traces", "anchor": "analyzing-traces", "heading_level": 2, "md_text": "### Comparing Runs Run the same evaluation multiple times and compare in Logfire: ### Debugging Failed Cases Find failed cases quickly: 1. Search for service_name = 'my_service_evals' AND is_exception (replace with the actual service name you are using) 2. View the full span tree to see where the failure occurred 3. Inspect attributes and logs for error messages", "url": "https://ai.pydantic.dev/evals/how-to/logfire-integration/index.html#analyzing-traces", "page": "evals/how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Comparing Runs", "anchor": "comparing-runs", "heading_level": 3, "md_text": "Run the same evaluation multiple times and compare in Logfire:", "url": "https://ai.pydantic.dev/evals/how-to/logfire-integration/index.html#comparing-runs", "page": "evals/how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Debugging Failed Cases", "anchor": "debugging-failed-cases", "heading_level": 3, "md_text": "Find failed cases quickly: 1. Search for service_name = 'my_service_evals' AND is_exception (replace with the actual service name you are using) 2. View the full span tree to see where the failure occurred 3. Inspect attributes and logs for error messages", "url": "https://ai.pydantic.dev/evals/how-to/logfire-integration/index.html#debugging-failed-cases", "page": "evals/how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Span-Based Evaluation", "anchor": "span-based-evaluation", "heading_level": 2, "md_text": "Logfire integration enables powerful span-based evaluators. See [Span-Based Evaluation](../../evaluators/span-based/index.html) for details. Example: Verify specific tools were called: The span tree is available in both: * Your evaluator code (via ctx.span_tree ) * Logfire UI (visual trace view)", "url": "https://ai.pydantic.dev/evals/how-to/logfire-integration/index.html#span-based-evaluation", "page": "evals/how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Troubleshooting", "anchor": "troubleshooting", "heading_level": 2, "md_text": "### No Data Appearing in Logfire Check: 1. **Token is set**: echo $LOGFIRE_TOKEN 2. **Configuration is correct**: 3. **Network connectivity**: Check firewall settings 4. **Project exists**: Verify project name in Logfire UI ### Traces Missing Spans If some spans are missing: 1. **Ensure logfire is configured before imports**: 2. **Check instrumentation**: Ensure your code has enabled all instrumentations you want:", "url": "https://ai.pydantic.dev/evals/how-to/logfire-integration/index.html#troubleshooting", "page": "evals/how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "No Data Appearing in Logfire", "anchor": "no-data-appearing-in-logfire", "heading_level": 3, "md_text": "Check: 1. **Token is set**: echo $LOGFIRE_TOKEN 2. **Configuration is correct**: 3. **Network connectivity**: Check firewall settings 4. **Project exists**: Verify project name in Logfire UI", "url": "https://ai.pydantic.dev/evals/how-to/logfire-integration/index.html#no-data-appearing-in-logfire", "page": "evals/how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Traces Missing Spans", "anchor": "traces-missing-spans", "heading_level": 3, "md_text": "If some spans are missing: 1. **Ensure logfire is configured before imports**: 2. **Check instrumentation**: Ensure your code has enabled all instrumentations you want:", "url": "https://ai.pydantic.dev/evals/how-to/logfire-integration/index.html#traces-missing-spans", "page": "evals/how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Best Practices", "anchor": "best-practices", "heading_level": 2, "md_text": "### 1. Configure Early Always configure Logfire before running evaluations: ### 2. Use Descriptive Service Names And Environments ### 3. Review Periodically * Check Logfire regularly to identify patterns * Look for consistently failing cases * Analyze performance trends * Adjust evaluators based on insights", "url": "https://ai.pydantic.dev/evals/how-to/logfire-integration/index.html#best-practices", "page": "evals/how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "1. Configure Early", "anchor": "1-configure-early", "heading_level": 3, "md_text": "Always configure Logfire before running evaluations:", "url": "https://ai.pydantic.dev/evals/how-to/logfire-integration/index.html#1-configure-early", "page": "evals/how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "2. Use Descriptive Service Names And Environments", "anchor": "2-use-descriptive-service-names-and-environments", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/logfire-integration/index.html#2-use-descriptive-service-names-and-environments", "page": "evals/how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "3. Review Periodically", "anchor": "3-review-periodically", "heading_level": 3, "md_text": "* Check Logfire regularly to identify patterns * Look for consistently failing cases * Analyze performance trends * Adjust evaluators based on insights", "url": "https://ai.pydantic.dev/evals/how-to/logfire-integration/index.html#3-review-periodically", "page": "evals/how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "* **[Span-Based Evaluation](../../evaluators/span-based/index.html)** - Use OpenTelemetry spans in evaluators * **[Logfire Documentation](https://logfire.pydantic.dev/docs/guides/web-ui/evals/)** - Complete Logfire guide * **[Metrics & Attributes](../metrics-attributes/index.html)** - Add custom data to traces", "url": "https://ai.pydantic.dev/evals/how-to/logfire-integration/index.html#next-steps", "page": "evals/how-to/logfire-integration/index.html", "source_site": "pydantic_ai"}
{"title": "Pydantic Evals", "anchor": "pydantic-evals", "heading_level": 1, "md_text": "**Pydantic Evals** is a powerful evaluation framework for systematically testing and evaluating AI systems, from simple LLM calls to complex multi-agent applications. ## Design Philosophy Code-First Approach Pydantic Evals follows a code-first philosophy where all evaluation components are defined in Python. This differs from platforms with web-based configuration. You write and run evals in code, and can write the results to disk or view them in your terminal or in [Pydantic Logfire](https://logfire.pydantic.dev/docs/guides/web-ui/evals/). Evals are an Emerging Practice Unlike unit tests, evals are an emerging art/science. Anyone who claims to know exactly how your evals should be defined can safely be ignored. We've designed Pydantic Evals to be flexible and useful without being too opinionated. ## Quick Navigation **Getting Started:** * [Installation](index.html#installation) * [Quick Start](quick-start/index.html) * [Core Concepts](core-concepts/index.html) **Evaluators:** * [Evaluators Overview](evaluators/overview/index.html) - Compare evaluator types and learn when to use each approach * [Built-in Evaluators](evaluators/built-in/index.html) - Complete reference for exact match, instance checks, and other ready-to-use evaluators * [LLM as a Judge](evaluators/llm-judge/index.html) - Use LLMs to evaluate subjective qualities, complex criteria, and natural language outputs * [Custom Evaluators](evaluators/custom/index.html) - Implement domain-specific scoring logic and custom evaluation metrics * [Span-Based Evaluation](evaluators/span-based/index.html) - Evaluate internal agent behavior (tool calls, execution flow) using OpenTelemetry traces. Essential for complex agents where correctness depends on *how* the answer was reached, not just the final output. Also ensures eval assertions align with production telemetry. **How-To Guides:** * [Logfire Integration](how-to/logfire-integration/index.html) - Visualize results * [Dataset Management](how-to/dataset-management/index.html) - Save, load, generate * [Concurrency & Performance](how-to/concurrency/index.html) - Control parallel execution * [Retry Strategies](how-to/retry-strategies/index.html) - Handle transient failures * [Metrics & Attributes](how-to/metrics-attributes/index.html) - Track custom data **Examples:** * [Simple Validation](examples/simple-validation/index.html) - Basic example **Reference:** * [API Documentation](../api/pydantic_evals/dataset/index.html) ## Code-First Evaluation Pydantic Evals follows a **code-first approach** where you define all evaluation components (datasets, experiments, tasks, cases and evaluators) in Python code, or as serialized data loaded by Python code. This differs from platforms with fully web-based configuration. When you run an *Experiment* you'll see a progress indicator and can print the results wherever you run your python code (IDE, terminal, etc). You also get a report object back that you can serialize and store or send to a notebook or other application for further visualization and analysis. If you are using [Pydantic Logfire](https://logfire.pydantic.dev/docs/guides/web-ui/evals/), your experiment results automatically appear in the Logfire web interface for visualization, comparison, and collaborative analysis. Logfire serves as a observability layer - you write and run evals in code, then view and analyze results in the web UI. ## Installation To install the Pydantic Evals package, run: pipuv pydantic-evals does not depend on pydantic-ai , but has an optional dependency on logfire if you'd like to use OpenTelemetry traces in your evals, or send evaluation results to [logfire](https://pydantic.dev/logfire). pipuv ## Pydantic Evals Data Model Pydantic Evals is built around a simple data model: ### Data Model Diagram ### Key Relationships 1. **Dataset  Cases**: One Dataset contains many Cases 2. **Dataset  Experiments**: One Dataset can be used across many Experiments over time 3. **Experiment  Case results**: One Experiment generates results by executing each Case 4. **Experiment  Task**: One Experiment evaluates one defined Task 5. **Experiment  Evaluators**: One Experiment uses multiple Evaluators. Dataset-wide Evaluators are run against all Cases, and Case-specific Evaluators against their respective Cases ### Data Flow 1. **Dataset creation**: Define cases and evaluators in YAML/JSON, or directly in Python 2. **Experiment execution**: Run dataset.evaluate_sync(task_function) 3. **Cases run**: Each Case is executed against the Task 4. **Evaluation**: Evaluators score the Task outputs for each Case 5. **Results**: All Case results are collected into a summary report A metaphor A useful metaphor (although not perfect) is to think of evals like a **Unit Testing** framework: * **Cases + Evaluators** are your individual unit tests - each one defines a specific scenario you want to test, complete with inputs and expected outcomes. Just like a unit test, a case asks: *\"Given this input, does my system produce the right output?\"* * **Datasets** are like test suites - they are the scaffolding that holds your unit tests together. They group related cases and define shared evaluation criteria that should apply across all tests in the suite. * **Experiments** are like running your entire test suite and getting a report. When you execute dataset.evaluate_sync(my_ai_function) , you're running all your cases against your AI system and collecting the results - just like running pytest and getting a summary of passes, failures, and performance metrics. The key difference from traditional unit testing is that AI systems are probabilistic. If you're type checking you'll still get a simple pass/fail, but scores for text outputs are likely qualitative and/or categorical, and more open to interpretation. For a deeper understanding, see [Core Concepts](core-concepts/index.html). ## Datasets and Cases In Pydantic Evals, everything begins with [ Dataset ](../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset)s and [ Case ](../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Case)s: * **[ Dataset ](../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset)**: A collection of test Cases designed for the evaluation of a specific task or function * **[ Case ](../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Case)**: A single test scenario corresponding to Task inputs, with optional expected outputs, metadata, and case-specific evaluators simple\\_eval\\_dataset.py *(This example is complete, it can be run \"as is\")* See [Dataset Management](how-to/dataset-management/index.html) to learn about saving, loading, and generating datasets. ## Evaluators [ Evaluator ](../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.Evaluator)s analyze and score the results of your Task when tested against a Case. These can be deterministic, code-based checks (such as testing model output format with a regex, or checking for the appearance of PII or sensitive data), or they can assess non-deterministic model outputs for qualities like accuracy, precision/recall, hallucinations, or instruction-following. While both kinds of testing are useful in LLM systems, classical code-based tests are cheaper and easier than tests which require either human or machine review of model outputs. Pydantic Evals includes several [built-in evaluators](evaluators/built-in/index.html) and allows you to define [custom evaluators](evaluators/custom/index.html): simple\\_eval\\_evaluator.py 1. You can add built-in evaluators to a dataset using the [ add_evaluator ](../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset.add_evaluator) method. 2. This custom evaluator returns a simple score based on whether the output matches the expected output. *(This example is complete, it can be run \"as is\")* Learn more: * [Evaluators Overview](evaluators/overview/index.html) - When to use different types * [Built-in Evaluators](evaluators/built-in/index.html) - Complete reference * [LLM Judge](evaluators/llm-judge/index.html) - Using LLMs as evaluators * [Custom Evaluators](evaluators/custom/index.html) - Write your own logic * [Span-Based Evaluation](evaluators/span-based/index.html) - Analyze execution traces ## Running Experiments Performing evaluations involves running a task against all cases in a dataset, also known as running an \"experiment\". Putting the above two examples together and using the more declarative evaluators kwarg to [ Dataset ](../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset): simple\\_eval\\_complete.py 1. Create a [test case](../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Case) as above 2. Create a [ Dataset ](../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset) with test cases and [ evaluators ](../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset.evaluators) 3. Our function to evaluate. 4. Run the evaluation with [ evaluate_sync ](../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset.evaluate_sync), which runs the function against all test cases in the dataset, and returns an [ EvaluationReport ](../api/pydantic_evals/reporting/index.html#pydantic_evals.reporting.EvaluationReport) object. 5. Print the report with [ print ](../api/pydantic_evals/reporting/index.html#pydantic_evals.reporting.EvaluationReport.print), which shows the results of the evaluation. We have omitted duration here just to keep the printed output from changing from run to run. *(This example is complete, it can be run \"as is\")* See [Quick Start](quick-start/index.html) for more examples and [Concurrency & Performance](how-to/concurrency/index.html) to learn about controlling parallel execution. ## API Reference For comprehensive coverage of all classes, methods, and configuration options, see the detailed [API Reference documentation](../api/pydantic_evals/dataset/index.html). ## Next Steps 1. **Start with simple evaluations** using [Quick Start](quick-start/index.html) 2. **Understand the data model** with [Core Concepts](core-concepts/index.html) 3. **Explore built-in evaluators** in [Built-in Evaluators](evaluators/built-in/index.html) 4. **Integrate with Logfire** for visualization: [Logfire Integration](how-to/logfire-integration/index.html) 5. **Build comprehensive test suites** with [Dataset Management](how-to/dataset-management/index.html) 6. **Implement custom evaluators** for domain-specific metrics: [Custom Evaluators](evaluators/custom/index.html)", "url": "https://ai.pydantic.dev/evals/index.html#pydantic-evals", "page": "evals/index.html", "source_site": "pydantic_ai"}
{"title": "Design Philosophy", "anchor": "design-philosophy", "heading_level": 2, "md_text": "Code-First Approach Pydantic Evals follows a code-first philosophy where all evaluation components are defined in Python. This differs from platforms with web-based configuration. You write and run evals in code, and can write the results to disk or view them in your terminal or in [Pydantic Logfire](https://logfire.pydantic.dev/docs/guides/web-ui/evals/). Evals are an Emerging Practice Unlike unit tests, evals are an emerging art/science. Anyone who claims to know exactly how your evals should be defined can safely be ignored. We've designed Pydantic Evals to be flexible and useful without being too opinionated.", "url": "https://ai.pydantic.dev/evals/index.html#design-philosophy", "page": "evals/index.html", "source_site": "pydantic_ai"}
{"title": "Quick Navigation", "anchor": "quick-navigation", "heading_level": 2, "md_text": "**Getting Started:** * [Installation](index.html#installation) * [Quick Start](quick-start/index.html) * [Core Concepts](core-concepts/index.html) **Evaluators:** * [Evaluators Overview](evaluators/overview/index.html) - Compare evaluator types and learn when to use each approach * [Built-in Evaluators](evaluators/built-in/index.html) - Complete reference for exact match, instance checks, and other ready-to-use evaluators * [LLM as a Judge](evaluators/llm-judge/index.html) - Use LLMs to evaluate subjective qualities, complex criteria, and natural language outputs * [Custom Evaluators](evaluators/custom/index.html) - Implement domain-specific scoring logic and custom evaluation metrics * [Span-Based Evaluation](evaluators/span-based/index.html) - Evaluate internal agent behavior (tool calls, execution flow) using OpenTelemetry traces. Essential for complex agents where correctness depends on *how* the answer was reached, not just the final output. Also ensures eval assertions align with production telemetry. **How-To Guides:** * [Logfire Integration](how-to/logfire-integration/index.html) - Visualize results * [Dataset Management](how-to/dataset-management/index.html) - Save, load, generate * [Concurrency & Performance](how-to/concurrency/index.html) - Control parallel execution * [Retry Strategies](how-to/retry-strategies/index.html) - Handle transient failures * [Metrics & Attributes](how-to/metrics-attributes/index.html) - Track custom data **Examples:** * [Simple Validation](examples/simple-validation/index.html) - Basic example **Reference:** * [API Documentation](../api/pydantic_evals/dataset/index.html)", "url": "https://ai.pydantic.dev/evals/index.html#quick-navigation", "page": "evals/index.html", "source_site": "pydantic_ai"}
{"title": "Code-First Evaluation", "anchor": "code-first-evaluation", "heading_level": 2, "md_text": "Pydantic Evals follows a **code-first approach** where you define all evaluation components (datasets, experiments, tasks, cases and evaluators) in Python code, or as serialized data loaded by Python code. This differs from platforms with fully web-based configuration. When you run an *Experiment* you'll see a progress indicator and can print the results wherever you run your python code (IDE, terminal, etc). You also get a report object back that you can serialize and store or send to a notebook or other application for further visualization and analysis. If you are using [Pydantic Logfire](https://logfire.pydantic.dev/docs/guides/web-ui/evals/), your experiment results automatically appear in the Logfire web interface for visualization, comparison, and collaborative analysis. Logfire serves as a observability layer - you write and run evals in code, then view and analyze results in the web UI.", "url": "https://ai.pydantic.dev/evals/index.html#code-first-evaluation", "page": "evals/index.html", "source_site": "pydantic_ai"}
{"title": "Installation", "anchor": "installation", "heading_level": 2, "md_text": "To install the Pydantic Evals package, run: pipuv pydantic-evals does not depend on pydantic-ai , but has an optional dependency on logfire if you'd like to use OpenTelemetry traces in your evals, or send evaluation results to [logfire](https://pydantic.dev/logfire). pipuv", "url": "https://ai.pydantic.dev/evals/index.html#installation", "page": "evals/index.html", "source_site": "pydantic_ai"}
{"title": "Pydantic Evals Data Model", "anchor": "pydantic-evals-data-model", "heading_level": 2, "md_text": "Pydantic Evals is built around a simple data model: ### Data Model Diagram ### Key Relationships 1. **Dataset  Cases**: One Dataset contains many Cases 2. **Dataset  Experiments**: One Dataset can be used across many Experiments over time 3. **Experiment  Case results**: One Experiment generates results by executing each Case 4. **Experiment  Task**: One Experiment evaluates one defined Task 5. **Experiment  Evaluators**: One Experiment uses multiple Evaluators. Dataset-wide Evaluators are run against all Cases, and Case-specific Evaluators against their respective Cases ### Data Flow 1. **Dataset creation**: Define cases and evaluators in YAML/JSON, or directly in Python 2. **Experiment execution**: Run dataset.evaluate_sync(task_function) 3. **Cases run**: Each Case is executed against the Task 4. **Evaluation**: Evaluators score the Task outputs for each Case 5. **Results**: All Case results are collected into a summary report A metaphor A useful metaphor (although not perfect) is to think of evals like a **Unit Testing** framework: * **Cases + Evaluators** are your individual unit tests - each one defines a specific scenario you want to test, complete with inputs and expected outcomes. Just like a unit test, a case asks: *\"Given this input, does my system produce the right output?\"* * **Datasets** are like test suites - they are the scaffolding that holds your unit tests together. They group related cases and define shared evaluation criteria that should apply across all tests in the suite. * **Experiments** are like running your entire test suite and getting a report. When you execute dataset.evaluate_sync(my_ai_function) , you're running all your cases against your AI system and collecting the results - just like running pytest and getting a summary of passes, failures, and performance metrics. The key difference from traditional unit testing is that AI systems are probabilistic. If you're type checking you'll still get a simple pass/fail, but scores for text outputs are likely qualitative and/or categorical, and more open to interpretation. For a deeper understanding, see [Core Concepts](core-concepts/index.html).", "url": "https://ai.pydantic.dev/evals/index.html#pydantic-evals-data-model", "page": "evals/index.html", "source_site": "pydantic_ai"}
{"title": "Data Model Diagram", "anchor": "data-model-diagram", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/index.html#data-model-diagram", "page": "evals/index.html", "source_site": "pydantic_ai"}
{"title": "Key Relationships", "anchor": "key-relationships", "heading_level": 3, "md_text": "1. **Dataset  Cases**: One Dataset contains many Cases 2. **Dataset  Experiments**: One Dataset can be used across many Experiments over time 3. **Experiment  Case results**: One Experiment generates results by executing each Case 4. **Experiment  Task**: One Experiment evaluates one defined Task 5. **Experiment  Evaluators**: One Experiment uses multiple Evaluators. Dataset-wide Evaluators are run against all Cases, and Case-specific Evaluators against their respective Cases", "url": "https://ai.pydantic.dev/evals/index.html#key-relationships", "page": "evals/index.html", "source_site": "pydantic_ai"}
{"title": "Data Flow", "anchor": "data-flow", "heading_level": 3, "md_text": "1. **Dataset creation**: Define cases and evaluators in YAML/JSON, or directly in Python 2. **Experiment execution**: Run dataset.evaluate_sync(task_function) 3. **Cases run**: Each Case is executed against the Task 4. **Evaluation**: Evaluators score the Task outputs for each Case 5. **Results**: All Case results are collected into a summary report A metaphor A useful metaphor (although not perfect) is to think of evals like a **Unit Testing** framework: * **Cases + Evaluators** are your individual unit tests - each one defines a specific scenario you want to test, complete with inputs and expected outcomes. Just like a unit test, a case asks: *\"Given this input, does my system produce the right output?\"* * **Datasets** are like test suites - they are the scaffolding that holds your unit tests together. They group related cases and define shared evaluation criteria that should apply across all tests in the suite. * **Experiments** are like running your entire test suite and getting a report. When you execute dataset.evaluate_sync(my_ai_function) , you're running all your cases against your AI system and collecting the results - just like running pytest and getting a summary of passes, failures, and performance metrics. The key difference from traditional unit testing is that AI systems are probabilistic. If you're type checking you'll still get a simple pass/fail, but scores for text outputs are likely qualitative and/or categorical, and more open to interpretation. For a deeper understanding, see [Core Concepts](core-concepts/index.html).", "url": "https://ai.pydantic.dev/evals/index.html#data-flow", "page": "evals/index.html", "source_site": "pydantic_ai"}
{"title": "Datasets and Cases", "anchor": "datasets-and-cases", "heading_level": 2, "md_text": "In Pydantic Evals, everything begins with [ Dataset ](../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset)s and [ Case ](../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Case)s: * **[ Dataset ](../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset)**: A collection of test Cases designed for the evaluation of a specific task or function * **[ Case ](../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Case)**: A single test scenario corresponding to Task inputs, with optional expected outputs, metadata, and case-specific evaluators simple\\_eval\\_dataset.py *(This example is complete, it can be run \"as is\")* See [Dataset Management](how-to/dataset-management/index.html) to learn about saving, loading, and generating datasets.", "url": "https://ai.pydantic.dev/evals/index.html#datasets-and-cases", "page": "evals/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluators", "anchor": "evaluators", "heading_level": 2, "md_text": "[ Evaluator ](../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.Evaluator)s analyze and score the results of your Task when tested against a Case. These can be deterministic, code-based checks (such as testing model output format with a regex, or checking for the appearance of PII or sensitive data), or they can assess non-deterministic model outputs for qualities like accuracy, precision/recall, hallucinations, or instruction-following. While both kinds of testing are useful in LLM systems, classical code-based tests are cheaper and easier than tests which require either human or machine review of model outputs. Pydantic Evals includes several [built-in evaluators](evaluators/built-in/index.html) and allows you to define [custom evaluators](evaluators/custom/index.html): simple\\_eval\\_evaluator.py 1. You can add built-in evaluators to a dataset using the [ add_evaluator ](../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset.add_evaluator) method. 2. This custom evaluator returns a simple score based on whether the output matches the expected output. *(This example is complete, it can be run \"as is\")* Learn more: * [Evaluators Overview](evaluators/overview/index.html) - When to use different types * [Built-in Evaluators](evaluators/built-in/index.html) - Complete reference * [LLM Judge](evaluators/llm-judge/index.html) - Using LLMs as evaluators * [Custom Evaluators](evaluators/custom/index.html) - Write your own logic * [Span-Based Evaluation](evaluators/span-based/index.html) - Analyze execution traces", "url": "https://ai.pydantic.dev/evals/index.html#evaluators", "page": "evals/index.html", "source_site": "pydantic_ai"}
{"title": "Running Experiments", "anchor": "running-experiments", "heading_level": 2, "md_text": "Performing evaluations involves running a task against all cases in a dataset, also known as running an \"experiment\". Putting the above two examples together and using the more declarative evaluators kwarg to [ Dataset ](../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset): simple\\_eval\\_complete.py 1. Create a [test case](../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Case) as above 2. Create a [ Dataset ](../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset) with test cases and [ evaluators ](../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset.evaluators) 3. Our function to evaluate. 4. Run the evaluation with [ evaluate_sync ](../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset.evaluate_sync), which runs the function against all test cases in the dataset, and returns an [ EvaluationReport ](../api/pydantic_evals/reporting/index.html#pydantic_evals.reporting.EvaluationReport) object. 5. Print the report with [ print ](../api/pydantic_evals/reporting/index.html#pydantic_evals.reporting.EvaluationReport.print), which shows the results of the evaluation. We have omitted duration here just to keep the printed output from changing from run to run. *(This example is complete, it can be run \"as is\")* See [Quick Start](quick-start/index.html) for more examples and [Concurrency & Performance](how-to/concurrency/index.html) to learn about controlling parallel execution.", "url": "https://ai.pydantic.dev/evals/index.html#running-experiments", "page": "evals/index.html", "source_site": "pydantic_ai"}
{"title": "API Reference", "anchor": "api-reference", "heading_level": 2, "md_text": "For comprehensive coverage of all classes, methods, and configuration options, see the detailed [API Reference documentation](../api/pydantic_evals/dataset/index.html).", "url": "https://ai.pydantic.dev/evals/index.html#api-reference", "page": "evals/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "1. **Start with simple evaluations** using [Quick Start](quick-start/index.html) 2. **Understand the data model** with [Core Concepts](core-concepts/index.html) 3. **Explore built-in evaluators** in [Built-in Evaluators](evaluators/built-in/index.html) 4. **Integrate with Logfire** for visualization: [Logfire Integration](how-to/logfire-integration/index.html) 5. **Build comprehensive test suites** with [Dataset Management](how-to/dataset-management/index.html) 6. **Implement custom evaluators** for domain-specific metrics: [Custom Evaluators](evaluators/custom/index.html)", "url": "https://ai.pydantic.dev/evals/index.html#next-steps", "page": "evals/index.html", "source_site": "pydantic_ai"}
{"title": "Dataset Management", "anchor": "dataset-management", "heading_level": 1, "md_text": "Create, save, load, and generate evaluation datasets. ## Creating Datasets ### From Code Define datasets directly in Python: ### Adding Cases Dynamically ## Saving Datasets Detailed Serialization Guide For complete details on serialization formats, JSON schema generation, and custom evaluators, see [Dataset Serialization](../dataset-serialization/index.html). ### Save to YAML Output ( my_dataset.yaml ): ### Save to JSON ### Custom Schema Path ## Loading Datasets ### From YAML/JSON ### From String ### From Dict ### With Custom Evaluators When loading datasets that use custom evaluators, you must pass them to from_file() : For complete details on serialization with custom evaluators, see [Dataset Serialization](../dataset-serialization/index.html). ## Generating Datasets Pydantic Evals allows you to generate test datasets using LLMs with [ generate_dataset ](../../../api/pydantic_evals/generation/index.html#pydantic_evals.generation.generate_dataset). Datasets can be generated in either JSON or YAML format, in both cases a JSON schema file is generated alongside the dataset and referenced in the dataset, so you should get type checking and auto-completion in your editor. generate\\_dataset\\_example.py 1. Define the schema for the inputs to the task. 2. Define the schema for the expected outputs of the task. 3. Define the schema for the metadata of the test cases. 4. Call [ generate_dataset ](../../../api/pydantic_evals/generation/index.html#pydantic_evals.generation.generate_dataset) to create a [ Dataset ](../../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset) with 2 cases confirming to the schema. 5. Save the dataset to a YAML file, this will also write questions_cases_schema.json with the schema JSON schema for questions_cases.yaml to make editing easier. The magic yaml-language-server comment is supported by at least vscode, jetbrains/pycharm (more details [here](https://github.com/redhat-developer/yaml-language-server#using-inlined-schema)). *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main(answer)) to run main )* You can also write datasets as JSON files: generate\\_dataset\\_example\\_json.py 1. Generate the [ Dataset ](../../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset) exactly as above. 2. Save the dataset to a JSON file, this will also write questions_cases_schema.json with th JSON schema for questions_cases.json . This time the $schema key is included in the JSON file to define the schema for IDEs to use while you edit the file, there's no formal spec for this, but it works in vscode and pycharm and is discussed at length in [json-schema-org/json-schema-spec#828](https://github.com/json-schema-org/json-schema-spec/issues/828). *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main(answer)) to run main )* ## Type-Safe Datasets Use generic type parameters for type safety: ## Schema Generation Generate JSON Schema for IDE support: Manual schema generation: ## Best Practices ### 1. Use Clear Names ### 2. Organize by Difficulty ### 3. Start Small, Grow Gradually ### 4. Use Case-specific Evaluators Where Appropriate Case-specific evaluators let different cases have different evaluation criteria, which is essential for comprehensive \"test coverage\". Rather than trying to write one-size-fits-all evaluators, you can specify exactly what \"good\" looks like for each scenario. This is particularly powerful with [ LLMJudge ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.LLMJudge) evaluators where you can describe nuanced requirements per case, making it easy to build and maintain golden datasets. See [Case-specific evaluators](../../evaluators/overview/index.html#case-specific-evaluators) for detailed guidance. ### 5. Separate Datasets by Purpose ## Next Steps * **[Dataset Serialization](../dataset-serialization/index.html)** - In-depth guide to saving and loading datasets * **[Generating Datasets](index.html#generating-datasets)** - Use LLMs to generate test cases * **[Examples: Simple Validation](../../examples/simple-validation/index.html)** - Practical examples", "url": "https://ai.pydantic.dev/evals/how-to/dataset-management/index.html#dataset-management", "page": "evals/how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Creating Datasets", "anchor": "creating-datasets", "heading_level": 2, "md_text": "### From Code Define datasets directly in Python: ### Adding Cases Dynamically", "url": "https://ai.pydantic.dev/evals/how-to/dataset-management/index.html#creating-datasets", "page": "evals/how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "From Code", "anchor": "from-code", "heading_level": 3, "md_text": "Define datasets directly in Python:", "url": "https://ai.pydantic.dev/evals/how-to/dataset-management/index.html#from-code", "page": "evals/how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Adding Cases Dynamically", "anchor": "adding-cases-dynamically", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/dataset-management/index.html#adding-cases-dynamically", "page": "evals/how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Saving Datasets", "anchor": "saving-datasets", "heading_level": 2, "md_text": "Detailed Serialization Guide For complete details on serialization formats, JSON schema generation, and custom evaluators, see [Dataset Serialization](../dataset-serialization/index.html). ### Save to YAML Output ( my_dataset.yaml ): ### Save to JSON ### Custom Schema Path", "url": "https://ai.pydantic.dev/evals/how-to/dataset-management/index.html#saving-datasets", "page": "evals/how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Save to YAML", "anchor": "save-to-yaml", "heading_level": 3, "md_text": "Output ( my_dataset.yaml ):", "url": "https://ai.pydantic.dev/evals/how-to/dataset-management/index.html#save-to-yaml", "page": "evals/how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Save to JSON", "anchor": "save-to-json", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/dataset-management/index.html#save-to-json", "page": "evals/how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Schema Path", "anchor": "custom-schema-path", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/dataset-management/index.html#custom-schema-path", "page": "evals/how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Loading Datasets", "anchor": "loading-datasets", "heading_level": 2, "md_text": "### From YAML/JSON ### From String ### From Dict ### With Custom Evaluators When loading datasets that use custom evaluators, you must pass them to from_file() : For complete details on serialization with custom evaluators, see [Dataset Serialization](../dataset-serialization/index.html).", "url": "https://ai.pydantic.dev/evals/how-to/dataset-management/index.html#loading-datasets", "page": "evals/how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "From YAML/JSON", "anchor": "from-yamljson", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/dataset-management/index.html#from-yamljson", "page": "evals/how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "From String", "anchor": "from-string", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/dataset-management/index.html#from-string", "page": "evals/how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "From Dict", "anchor": "from-dict", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/dataset-management/index.html#from-dict", "page": "evals/how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "With Custom Evaluators", "anchor": "with-custom-evaluators", "heading_level": 3, "md_text": "When loading datasets that use custom evaluators, you must pass them to from_file() : For complete details on serialization with custom evaluators, see [Dataset Serialization](../dataset-serialization/index.html).", "url": "https://ai.pydantic.dev/evals/how-to/dataset-management/index.html#with-custom-evaluators", "page": "evals/how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Generating Datasets", "anchor": "generating-datasets", "heading_level": 2, "md_text": "Pydantic Evals allows you to generate test datasets using LLMs with [ generate_dataset ](../../../api/pydantic_evals/generation/index.html#pydantic_evals.generation.generate_dataset). Datasets can be generated in either JSON or YAML format, in both cases a JSON schema file is generated alongside the dataset and referenced in the dataset, so you should get type checking and auto-completion in your editor. generate\\_dataset\\_example.py 1. Define the schema for the inputs to the task. 2. Define the schema for the expected outputs of the task. 3. Define the schema for the metadata of the test cases. 4. Call [ generate_dataset ](../../../api/pydantic_evals/generation/index.html#pydantic_evals.generation.generate_dataset) to create a [ Dataset ](../../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset) with 2 cases confirming to the schema. 5. Save the dataset to a YAML file, this will also write questions_cases_schema.json with the schema JSON schema for questions_cases.yaml to make editing easier. The magic yaml-language-server comment is supported by at least vscode, jetbrains/pycharm (more details [here](https://github.com/redhat-developer/yaml-language-server#using-inlined-schema)). *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main(answer)) to run main )* You can also write datasets as JSON files: generate\\_dataset\\_example\\_json.py 1. Generate the [ Dataset ](../../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset) exactly as above. 2. Save the dataset to a JSON file, this will also write questions_cases_schema.json with th JSON schema for questions_cases.json . This time the $schema key is included in the JSON file to define the schema for IDEs to use while you edit the file, there's no formal spec for this, but it works in vscode and pycharm and is discussed at length in [json-schema-org/json-schema-spec#828](https://github.com/json-schema-org/json-schema-spec/issues/828). *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main(answer)) to run main )*", "url": "https://ai.pydantic.dev/evals/how-to/dataset-management/index.html#generating-datasets", "page": "evals/how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Type-Safe Datasets", "anchor": "type-safe-datasets", "heading_level": 2, "md_text": "Use generic type parameters for type safety:", "url": "https://ai.pydantic.dev/evals/how-to/dataset-management/index.html#type-safe-datasets", "page": "evals/how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Schema Generation", "anchor": "schema-generation", "heading_level": 2, "md_text": "Generate JSON Schema for IDE support: Manual schema generation:", "url": "https://ai.pydantic.dev/evals/how-to/dataset-management/index.html#schema-generation", "page": "evals/how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Best Practices", "anchor": "best-practices", "heading_level": 2, "md_text": "### 1. Use Clear Names ### 2. Organize by Difficulty ### 3. Start Small, Grow Gradually ### 4. Use Case-specific Evaluators Where Appropriate Case-specific evaluators let different cases have different evaluation criteria, which is essential for comprehensive \"test coverage\". Rather than trying to write one-size-fits-all evaluators, you can specify exactly what \"good\" looks like for each scenario. This is particularly powerful with [ LLMJudge ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.LLMJudge) evaluators where you can describe nuanced requirements per case, making it easy to build and maintain golden datasets. See [Case-specific evaluators](../../evaluators/overview/index.html#case-specific-evaluators) for detailed guidance. ### 5. Separate Datasets by Purpose", "url": "https://ai.pydantic.dev/evals/how-to/dataset-management/index.html#best-practices", "page": "evals/how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "1. Use Clear Names", "anchor": "1-use-clear-names", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/dataset-management/index.html#1-use-clear-names", "page": "evals/how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "2. Organize by Difficulty", "anchor": "2-organize-by-difficulty", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/dataset-management/index.html#2-organize-by-difficulty", "page": "evals/how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "3. Start Small, Grow Gradually", "anchor": "3-start-small-grow-gradually", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/dataset-management/index.html#3-start-small-grow-gradually", "page": "evals/how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "4. Use Case-specific Evaluators Where Appropriate", "anchor": "4-use-case-specific-evaluators-where-appropriate", "heading_level": 3, "md_text": "Case-specific evaluators let different cases have different evaluation criteria, which is essential for comprehensive \"test coverage\". Rather than trying to write one-size-fits-all evaluators, you can specify exactly what \"good\" looks like for each scenario. This is particularly powerful with [ LLMJudge ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.LLMJudge) evaluators where you can describe nuanced requirements per case, making it easy to build and maintain golden datasets. See [Case-specific evaluators](../../evaluators/overview/index.html#case-specific-evaluators) for detailed guidance.", "url": "https://ai.pydantic.dev/evals/how-to/dataset-management/index.html#4-use-case-specific-evaluators-where-appropriate", "page": "evals/how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "5. Separate Datasets by Purpose", "anchor": "5-separate-datasets-by-purpose", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/dataset-management/index.html#5-separate-datasets-by-purpose", "page": "evals/how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "* **[Dataset Serialization](../dataset-serialization/index.html)** - In-depth guide to saving and loading datasets * **[Generating Datasets](index.html#generating-datasets)** - Use LLMs to generate test cases * **[Examples: Simple Validation](../../examples/simple-validation/index.html)** - Practical examples", "url": "https://ai.pydantic.dev/evals/how-to/dataset-management/index.html#next-steps", "page": "evals/how-to/dataset-management/index.html", "source_site": "pydantic_ai"}
{"title": "Dataset Serialization", "anchor": "dataset-serialization", "heading_level": 1, "md_text": "Learn how to save and load datasets in different formats, with support for custom evaluators and IDE integration. ## Overview Pydantic Evals supports serializing datasets to files in two formats: * **YAML** ( .yaml , .yml ) - Human-readable, great for version control * **JSON** ( .json ) - Structured, machine-readable Both formats support: - Automatic JSON schema generation for IDE autocomplete and validation - Custom evaluator serialization/deserialization - Type-safe loading with generic parameters ## YAML Format YAML is the recommended format for most use cases due to its readability and compact syntax. ### Basic Example This creates two files: 1. ** my_tests.yaml ** - The dataset 2. ** my_tests_schema.json ** - JSON schema for IDE support ### YAML Output ### JSON Schema for IDEs The first line references the schema file: This enables: -  **Autocomplete** in VS Code, PyCharm, and other editors -  **Inline validation** while editing -  **Documentation tooltips** for fields -  **Error highlighting** for invalid data Editor Support The yaml-language-server comment is supported by: * VS Code (with YAML extension) * JetBrains IDEs (PyCharm, IntelliJ, etc.) * Most editors with YAML language server support See the [YAML Language Server docs](https://github.com/redhat-developer/yaml-language-server#using-inlined-schema) for more details. ### Loading from YAML ## JSON Format JSON format is useful for programmatic generation or when strict structure is required. ### Basic Example ### JSON Output The $schema key at the top enables IDE support similar to YAML. ### Loading from JSON ## Schema Generation ### Automatic Schema Creation By default, to_file() creates a JSON schema file alongside your dataset: ### Custom Schema Location ### Schema Path Templates Use {stem} to reference the dataset filename: ### Manual Schema Generation Generate a schema without saving the dataset: ## Custom Evaluators Custom evaluators require special handling during serialization and deserialization. ### Requirements Custom evaluators must: 1. Be decorated with @dataclass 2. Inherit from Evaluator 3. Be passed to both to_file() and from_file() ### Complete Example ### Saved YAML ### Loading with Custom Evaluators Important You must pass custom_evaluator_types to **both** to_file() and from_file() . * to_file() : Includes the evaluator in the JSON schema * from_file() : Registers the evaluator for deserialization ## Evaluator Serialization Formats Evaluators can be serialized in three forms: ### 1. Name Only (No Parameters) ### 2. Single Parameter (Short Form) ### 3. Multiple Parameters (Dict Form) ## Format Comparison **Recommendation**: Use YAML for most cases, JSON for programmatic generation. ## Advanced: Evaluator Serialization Name Customize how your evaluator appears in serialized files: In YAML: ## Troubleshooting ### Schema Not Found in IDE **Problem**: YAML file doesn't show autocomplete **Solutions**: 1. **Check the schema path** in the first line of YAML: 2. **Verify schema file exists** in the same directory 3. **Restart the language server** in your IDE 4. **Install YAML extension** (VS Code: \"YAML\" by Red Hat) ### Custom Evaluator Not Found **Problem**: ValueError: Unknown evaluator name: 'CustomEvaluator' **Solution**: Pass custom_evaluator_types when loading: ### Format Inference Failed **Problem**: ValueError: Cannot infer format from extension **Solution**: Specify format explicitly: ### Schema Generation Error **Problem**: Custom evaluator causes schema generation to fail **Solution**: Ensure evaluator is a proper dataclass: ## Next Steps * **[Dataset Management](../dataset-management/index.html)** - Creating and organizing datasets * **[Custom Evaluators](../../evaluators/custom/index.html)** - Write custom evaluation logic * **[Core Concepts](../../core-concepts/index.html)** - Understand the data model", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#dataset-serialization", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "Pydantic Evals supports serializing datasets to files in two formats: * **YAML** ( .yaml , .yml ) - Human-readable, great for version control * **JSON** ( .json ) - Structured, machine-readable Both formats support: - Automatic JSON schema generation for IDE autocomplete and validation - Custom evaluator serialization/deserialization - Type-safe loading with generic parameters", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#overview", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "YAML Format", "anchor": "yaml-format", "heading_level": 2, "md_text": "YAML is the recommended format for most use cases due to its readability and compact syntax. ### Basic Example This creates two files: 1. ** my_tests.yaml ** - The dataset 2. ** my_tests_schema.json ** - JSON schema for IDE support ### YAML Output ### JSON Schema for IDEs The first line references the schema file: This enables: -  **Autocomplete** in VS Code, PyCharm, and other editors -  **Inline validation** while editing -  **Documentation tooltips** for fields -  **Error highlighting** for invalid data Editor Support The yaml-language-server comment is supported by: * VS Code (with YAML extension) * JetBrains IDEs (PyCharm, IntelliJ, etc.) * Most editors with YAML language server support See the [YAML Language Server docs](https://github.com/redhat-developer/yaml-language-server#using-inlined-schema) for more details. ### Loading from YAML", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#yaml-format", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Basic Example", "anchor": "basic-example", "heading_level": 3, "md_text": "This creates two files: 1. ** my_tests.yaml ** - The dataset 2. ** my_tests_schema.json ** - JSON schema for IDE support", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#basic-example", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "YAML Output", "anchor": "yaml-output", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#yaml-output", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "JSON Schema for IDEs", "anchor": "json-schema-for-ides", "heading_level": 3, "md_text": "The first line references the schema file: This enables: -  **Autocomplete** in VS Code, PyCharm, and other editors -  **Inline validation** while editing -  **Documentation tooltips** for fields -  **Error highlighting** for invalid data Editor Support The yaml-language-server comment is supported by: * VS Code (with YAML extension) * JetBrains IDEs (PyCharm, IntelliJ, etc.) * Most editors with YAML language server support See the [YAML Language Server docs](https://github.com/redhat-developer/yaml-language-server#using-inlined-schema) for more details.", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#json-schema-for-ides", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Loading from YAML", "anchor": "loading-from-yaml", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#loading-from-yaml", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "JSON Format", "anchor": "json-format", "heading_level": 2, "md_text": "JSON format is useful for programmatic generation or when strict structure is required. ### Basic Example ### JSON Output The $schema key at the top enables IDE support similar to YAML. ### Loading from JSON", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#json-format", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Basic Example", "anchor": "basic-example_1", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#basic-example_1", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "JSON Output", "anchor": "json-output", "heading_level": 3, "md_text": "The $schema key at the top enables IDE support similar to YAML.", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#json-output", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Loading from JSON", "anchor": "loading-from-json", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#loading-from-json", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Schema Generation", "anchor": "schema-generation", "heading_level": 2, "md_text": "### Automatic Schema Creation By default, to_file() creates a JSON schema file alongside your dataset: ### Custom Schema Location ### Schema Path Templates Use {stem} to reference the dataset filename: ### Manual Schema Generation Generate a schema without saving the dataset:", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#schema-generation", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Automatic Schema Creation", "anchor": "automatic-schema-creation", "heading_level": 3, "md_text": "By default, to_file() creates a JSON schema file alongside your dataset:", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#automatic-schema-creation", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Schema Location", "anchor": "custom-schema-location", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#custom-schema-location", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Schema Path Templates", "anchor": "schema-path-templates", "heading_level": 3, "md_text": "Use {stem} to reference the dataset filename:", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#schema-path-templates", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Manual Schema Generation", "anchor": "manual-schema-generation", "heading_level": 3, "md_text": "Generate a schema without saving the dataset:", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#manual-schema-generation", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Evaluators", "anchor": "custom-evaluators", "heading_level": 2, "md_text": "Custom evaluators require special handling during serialization and deserialization. ### Requirements Custom evaluators must: 1. Be decorated with @dataclass 2. Inherit from Evaluator 3. Be passed to both to_file() and from_file() ### Complete Example ### Saved YAML ### Loading with Custom Evaluators Important You must pass custom_evaluator_types to **both** to_file() and from_file() . * to_file() : Includes the evaluator in the JSON schema * from_file() : Registers the evaluator for deserialization", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#custom-evaluators", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Requirements", "anchor": "requirements", "heading_level": 3, "md_text": "Custom evaluators must: 1. Be decorated with @dataclass 2. Inherit from Evaluator 3. Be passed to both to_file() and from_file()", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#requirements", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Complete Example", "anchor": "complete-example", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#complete-example", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Saved YAML", "anchor": "saved-yaml", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#saved-yaml", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Loading with Custom Evaluators", "anchor": "loading-with-custom-evaluators", "heading_level": 3, "md_text": "Important You must pass custom_evaluator_types to **both** to_file() and from_file() . * to_file() : Includes the evaluator in the JSON schema * from_file() : Registers the evaluator for deserialization", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#loading-with-custom-evaluators", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluator Serialization Formats", "anchor": "evaluator-serialization-formats", "heading_level": 2, "md_text": "Evaluators can be serialized in three forms: ### 1. Name Only (No Parameters) ### 2. Single Parameter (Short Form) ### 3. Multiple Parameters (Dict Form)", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#evaluator-serialization-formats", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "1. Name Only (No Parameters)", "anchor": "1-name-only-no-parameters", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#1-name-only-no-parameters", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "2. Single Parameter (Short Form)", "anchor": "2-single-parameter-short-form", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#2-single-parameter-short-form", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "3. Multiple Parameters (Dict Form)", "anchor": "3-multiple-parameters-dict-form", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#3-multiple-parameters-dict-form", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Format Comparison", "anchor": "format-comparison", "heading_level": 2, "md_text": "**Recommendation**: Use YAML for most cases, JSON for programmatic generation.", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#format-comparison", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Advanced: Evaluator Serialization Name", "anchor": "advanced-evaluator-serialization-name", "heading_level": 2, "md_text": "Customize how your evaluator appears in serialized files: In YAML:", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#advanced-evaluator-serialization-name", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Troubleshooting", "anchor": "troubleshooting", "heading_level": 2, "md_text": "### Schema Not Found in IDE **Problem**: YAML file doesn't show autocomplete **Solutions**: 1. **Check the schema path** in the first line of YAML: 2. **Verify schema file exists** in the same directory 3. **Restart the language server** in your IDE 4. **Install YAML extension** (VS Code: \"YAML\" by Red Hat) ### Custom Evaluator Not Found **Problem**: ValueError: Unknown evaluator name: 'CustomEvaluator' **Solution**: Pass custom_evaluator_types when loading: ### Format Inference Failed **Problem**: ValueError: Cannot infer format from extension **Solution**: Specify format explicitly: ### Schema Generation Error **Problem**: Custom evaluator causes schema generation to fail **Solution**: Ensure evaluator is a proper dataclass:", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#troubleshooting", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Schema Not Found in IDE", "anchor": "schema-not-found-in-ide", "heading_level": 3, "md_text": "**Problem**: YAML file doesn't show autocomplete **Solutions**: 1. **Check the schema path** in the first line of YAML: 2. **Verify schema file exists** in the same directory 3. **Restart the language server** in your IDE 4. **Install YAML extension** (VS Code: \"YAML\" by Red Hat)", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#schema-not-found-in-ide", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Evaluator Not Found", "anchor": "custom-evaluator-not-found", "heading_level": 3, "md_text": "**Problem**: ValueError: Unknown evaluator name: 'CustomEvaluator' **Solution**: Pass custom_evaluator_types when loading:", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#custom-evaluator-not-found", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Format Inference Failed", "anchor": "format-inference-failed", "heading_level": 3, "md_text": "**Problem**: ValueError: Cannot infer format from extension **Solution**: Specify format explicitly:", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#format-inference-failed", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Schema Generation Error", "anchor": "schema-generation-error", "heading_level": 3, "md_text": "**Problem**: Custom evaluator causes schema generation to fail **Solution**: Ensure evaluator is a proper dataclass:", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#schema-generation-error", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "* **[Dataset Management](../dataset-management/index.html)** - Creating and organizing datasets * **[Custom Evaluators](../../evaluators/custom/index.html)** - Write custom evaluation logic * **[Core Concepts](../../core-concepts/index.html)** - Understand the data model", "url": "https://ai.pydantic.dev/evals/how-to/dataset-serialization/index.html#next-steps", "page": "evals/how-to/dataset-serialization/index.html", "source_site": "pydantic_ai"}
{"title": "Pydantic Evals", "anchor": "pydantic-evals", "heading_level": 1, "md_text": "**Pydantic Evals** is a powerful evaluation framework for systematically testing and evaluating AI systems, from simple LLM calls to complex multi-agent applications. ## What is Pydantic Evals? Pydantic Evals helps you: * **Create test datasets** with type-safe structured inputs and expected outputs * **Run evaluations** against your AI systems with automatic concurrency * **Score results** using deterministic checks, LLM judges, or custom evaluators * **Generate reports** with detailed metrics, assertions, and performance data * **Track changes** by comparing evaluation runs over time * **Integrate with Logfire** for visualization and collaborative analysis ## Installation For OpenTelemetry tracing and Logfire integration: ## Quick Start While evaluations are typically used to test AI systems, the Pydantic Evals framework works with any function call. To demonstrate the core functionality, we'll start with a simple, deterministic example. Here's a complete example of evaluating a simple text transformation function: Output: ## Key Concepts Understanding a few core concepts will help you get the most out of Pydantic Evals: * **[ Dataset ](../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset)** - A collection of test cases and (optional) evaluators * **[ Case ](../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Case)** - A single test scenario with inputs and optional expected outputs and case-specific evaluators * **[ Evaluator ](../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.Evaluator)** - A function that scores or validates task outputs * **[ EvaluationReport ](../../api/pydantic_evals/reporting/index.html#pydantic_evals.reporting.EvaluationReport)** - Results from running an evaluation For a deeper dive, see [Core Concepts](../core-concepts/index.html). ## Common Use Cases ### Deterministic Validation Test that your AI system produces correctly-structured outputs: ### LLM-as-a-Judge Evaluation Use an LLM to evaluate subjective qualities like accuracy or helpfulness: ### Performance Testing Ensure your system meets performance requirements: ## Next Steps Explore the documentation to learn more: * **[Core Concepts](../core-concepts/index.html)** - Understand the data model and evaluation flow * **[Built-in Evaluators](../evaluators/built-in/index.html)** - Learn about all available evaluators * **[Custom Evaluators](../evaluators/custom/index.html)** - Write your own evaluation logic * **[Dataset Management](../how-to/dataset-management/index.html)** - Save, load, and generate datasets * **[Examples](../examples/simple-validation/index.html)** - Practical examples for common scenarios", "url": "https://ai.pydantic.dev/evals/quick-start/index.html#pydantic-evals", "page": "evals/quick-start/index.html", "source_site": "pydantic_ai"}
{"title": "What is Pydantic Evals?", "anchor": "what-is-pydantic-evals", "heading_level": 2, "md_text": "Pydantic Evals helps you: * **Create test datasets** with type-safe structured inputs and expected outputs * **Run evaluations** against your AI systems with automatic concurrency * **Score results** using deterministic checks, LLM judges, or custom evaluators * **Generate reports** with detailed metrics, assertions, and performance data * **Track changes** by comparing evaluation runs over time * **Integrate with Logfire** for visualization and collaborative analysis", "url": "https://ai.pydantic.dev/evals/quick-start/index.html#what-is-pydantic-evals", "page": "evals/quick-start/index.html", "source_site": "pydantic_ai"}
{"title": "Installation", "anchor": "installation", "heading_level": 2, "md_text": "For OpenTelemetry tracing and Logfire integration:", "url": "https://ai.pydantic.dev/evals/quick-start/index.html#installation", "page": "evals/quick-start/index.html", "source_site": "pydantic_ai"}
{"title": "Quick Start", "anchor": "quick-start", "heading_level": 2, "md_text": "While evaluations are typically used to test AI systems, the Pydantic Evals framework works with any function call. To demonstrate the core functionality, we'll start with a simple, deterministic example. Here's a complete example of evaluating a simple text transformation function: Output:", "url": "https://ai.pydantic.dev/evals/quick-start/index.html#quick-start", "page": "evals/quick-start/index.html", "source_site": "pydantic_ai"}
{"title": "Key Concepts", "anchor": "key-concepts", "heading_level": 2, "md_text": "Understanding a few core concepts will help you get the most out of Pydantic Evals: * **[ Dataset ](../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Dataset)** - A collection of test cases and (optional) evaluators * **[ Case ](../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.Case)** - A single test scenario with inputs and optional expected outputs and case-specific evaluators * **[ Evaluator ](../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.Evaluator)** - A function that scores or validates task outputs * **[ EvaluationReport ](../../api/pydantic_evals/reporting/index.html#pydantic_evals.reporting.EvaluationReport)** - Results from running an evaluation For a deeper dive, see [Core Concepts](../core-concepts/index.html).", "url": "https://ai.pydantic.dev/evals/quick-start/index.html#key-concepts", "page": "evals/quick-start/index.html", "source_site": "pydantic_ai"}
{"title": "Common Use Cases", "anchor": "common-use-cases", "heading_level": 2, "md_text": "### Deterministic Validation Test that your AI system produces correctly-structured outputs: ### LLM-as-a-Judge Evaluation Use an LLM to evaluate subjective qualities like accuracy or helpfulness: ### Performance Testing Ensure your system meets performance requirements:", "url": "https://ai.pydantic.dev/evals/quick-start/index.html#common-use-cases", "page": "evals/quick-start/index.html", "source_site": "pydantic_ai"}
{"title": "Deterministic Validation", "anchor": "deterministic-validation", "heading_level": 3, "md_text": "Test that your AI system produces correctly-structured outputs:", "url": "https://ai.pydantic.dev/evals/quick-start/index.html#deterministic-validation", "page": "evals/quick-start/index.html", "source_site": "pydantic_ai"}
{"title": "LLM-as-a-Judge Evaluation", "anchor": "llm-as-a-judge-evaluation", "heading_level": 3, "md_text": "Use an LLM to evaluate subjective qualities like accuracy or helpfulness:", "url": "https://ai.pydantic.dev/evals/quick-start/index.html#llm-as-a-judge-evaluation", "page": "evals/quick-start/index.html", "source_site": "pydantic_ai"}
{"title": "Performance Testing", "anchor": "performance-testing", "heading_level": 3, "md_text": "Ensure your system meets performance requirements:", "url": "https://ai.pydantic.dev/evals/quick-start/index.html#performance-testing", "page": "evals/quick-start/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "Explore the documentation to learn more: * **[Core Concepts](../core-concepts/index.html)** - Understand the data model and evaluation flow * **[Built-in Evaluators](../evaluators/built-in/index.html)** - Learn about all available evaluators * **[Custom Evaluators](../evaluators/custom/index.html)** - Write your own evaluation logic * **[Dataset Management](../how-to/dataset-management/index.html)** - Save, load, and generate datasets * **[Examples](../examples/simple-validation/index.html)** - Practical examples for common scenarios", "url": "https://ai.pydantic.dev/evals/quick-start/index.html#next-steps", "page": "evals/quick-start/index.html", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "heading_level": 2, "md_text": "With [dependencies installed and environment variables set](../setup/index.html#usage), run: pipuv (or PYDANTIC_AI_MODEL=gemini-1.5-flash ... )", "url": "https://ai.pydantic.dev/examples/bank-support/index.html#running-the-example", "page": "examples/bank-support/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "heading_level": 2, "md_text": "[bank\\_support.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/bank_support.py)", "url": "https://ai.pydantic.dev/examples/bank-support/index.html#example-code", "page": "examples/bank-support/index.html", "source_site": "pydantic_ai"}
{"title": "pydantic_ai.toolsets", "anchor": "pydantic_aitoolsets", "heading_level": 1, "md_text": "### AbstractToolset Bases: ABC , Generic[AgentDepsT] A toolset is a collection of tools that can be used by an agent. It is responsible for: * Listing the tools it contains * Validating the arguments of the tools * Calling the tools See [toolset docs](../../toolsets/index.html) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/abstract.py #### id abstractmethod property An ID for the toolset that is unique among all toolsets registered with the same agent. If you're implementing a concrete implementation that users can instantiate more than once, you should let them optionally pass a custom ID to the constructor and return that here. A toolset needs to have an ID in order to be used in a durable execution environment like Temporal, in which case the ID will be used to identify the toolset's activities within the workflow. #### label property The name of the toolset for use in error messages. #### tool\\_name\\_conflict\\_hint property A hint for how to avoid name conflicts with other toolsets for use in error messages. #### \\_\\_aenter\\_\\_ async Enter the toolset context. This is where you can set up network connections in a concrete implementation. Source code in pydantic_ai_slim/pydantic_ai/toolsets/abstract.py #### \\_\\_aexit\\_\\_ async Exit the toolset context. This is where you can tear down network connections in a concrete implementation. Source code in pydantic_ai_slim/pydantic_ai/toolsets/abstract.py #### get\\_tools abstractmethod async The tools that are available in this toolset. Source code in pydantic_ai_slim/pydantic_ai/toolsets/abstract.py #### call\\_tool abstractmethod async Call a tool with the given arguments. Parameters: Source code in pydantic_ai_slim/pydantic_ai/toolsets/abstract.py #### apply Run a visitor function on all \"leaf\" toolsets (i.e. those that implement their own tool listing and calling). Source code in pydantic_ai_slim/pydantic_ai/toolsets/abstract.py #### visit\\_and\\_replace Run a visitor function on all \"leaf\" toolsets (i.e. those that implement their own tool listing and calling) and replace them in the hierarchy with the result of the function. Source code in pydantic_ai_slim/pydantic_ai/toolsets/abstract.py #### filtered Returns a new toolset that filters this toolset's tools using a filter function that takes the agent context and the tool definition. See [toolset docs](../../toolsets/index.html#filtering-tools) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/abstract.py #### prefixed Returns a new toolset that prefixes the names of this toolset's tools. See [toolset docs](../../toolsets/index.html#prefixing-tool-names) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/abstract.py #### prepared Returns a new toolset that prepares this toolset's tools using a prepare function that takes the agent context and the original tool definitions. See [toolset docs](../../toolsets/index.html#preparing-tool-definitions) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/abstract.py #### renamed Returns a new toolset that renames this toolset's tools using a dictionary mapping new names to original names. See [toolset docs](../../toolsets/index.html#renaming-tools) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/abstract.py #### approval\\_required Returns a new toolset that requires (some) calls to tools it contains to be approved. See [toolset docs](../../toolsets/index.html#requiring-tool-approval) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/abstract.py ### CombinedToolset dataclass Bases: AbstractToolset[AgentDepsT] A toolset that combines multiple toolsets. See [toolset docs](../../toolsets/index.html#combining-toolsets) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/combined.py ### ExternalToolset Bases: AbstractToolset[AgentDepsT] A toolset that holds tools whose results will be produced outside of the Pydantic AI agent run in which they were called. See [toolset docs](../../toolsets/index.html#external-toolset) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/external.py ### ApprovalRequiredToolset dataclass Bases: WrapperToolset[AgentDepsT] A toolset that requires (some) calls to tools it contains to be approved. See [toolset docs](../../toolsets/index.html#requiring-tool-approval) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/approval_required.py ### FilteredToolset dataclass Bases: WrapperToolset[AgentDepsT] A toolset that filters the tools it contains using a filter function that takes the agent context and the tool definition. See [toolset docs](../../toolsets/index.html#filtering-tools) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/filtered.py ### FunctionToolset Bases: AbstractToolset[AgentDepsT] A toolset that lets Python functions be used as tools. See [toolset docs](../../toolsets/index.html#function-toolset) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/function.py #### \\_\\_init\\_\\_ Build a new function toolset. Parameters: Source code in pydantic_ai_slim/pydantic_ai/toolsets/function.py #### tool Decorator to register a tool function which takes [ RunContext ](../tools/index.html#pydantic_ai.tools.RunContext) as its first argument. Can decorate a sync or async functions. The docstring is inspected to extract both the tool description and description of each parameter, [learn more](../../tools/index.html#function-tools-and-schema). We can't add overloads for every possible signature of tool, since the return type is a recursive union so the signature of functions decorated with @toolset.tool is obscured. Example: Parameters: Source code in pydantic_ai_slim/pydantic_ai/toolsets/function.py #### add\\_function Add a function as a tool to the toolset. Can take a sync or async function. The docstring is inspected to extract both the tool description and description of each parameter, [learn more](../../tools/index.html#function-tools-and-schema). Parameters: Source code in pydantic_ai_slim/pydantic_ai/toolsets/function.py #### add\\_tool Add a tool to the toolset. Parameters: Source code in pydantic_ai_slim/pydantic_ai/toolsets/function.py ### PrefixedToolset dataclass Bases: WrapperToolset[AgentDepsT] A toolset that prefixes the names of the tools it contains. See [toolset docs](../../toolsets/index.html#prefixing-tool-names) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/prefixed.py ### RenamedToolset dataclass Bases: WrapperToolset[AgentDepsT] A toolset that renames the tools it contains using a dictionary mapping new names to original names. See [toolset docs](../../toolsets/index.html#renaming-tools) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/renamed.py ### PreparedToolset dataclass Bases: WrapperToolset[AgentDepsT] A toolset that prepares the tools it contains using a prepare function that takes the agent context and the original tool definitions. See [toolset docs](../../toolsets/index.html#preparing-tool-definitions) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/prepared.py ### WrapperToolset dataclass Bases: AbstractToolset[AgentDepsT] A toolset that wraps another toolset and delegates to it. See [toolset docs](../../toolsets/index.html#wrapping-a-toolset) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/wrapper.py ### ToolsetFunc module-attribute A sync/async function which takes a run context and returns a toolset. ### FastMCPToolset dataclass Bases: AbstractToolset[AgentDepsT] A FastMCP Toolset that uses the FastMCP Client to call tools from a local or remote MCP Server. The Toolset can accept a FastMCP Client, a FastMCP Transport, or any other object which a FastMCP Transport can be created from. See https://gofastmcp.com/clients/transports for a full list of transports available. Source code in pydantic_ai_slim/pydantic_ai/toolsets/fastmcp.py #### client instance-attribute The FastMCP client to use. #### max\\_retries instance-attribute The maximum number of retries to attempt if a tool call fails. #### tool\\_error\\_behavior instance-attribute The behavior to take when a tool error occurs.", "url": "https://ai.pydantic.dev/api/toolsets/index.html#pydantic_aitoolsets", "page": "api/toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "AbstractToolset", "anchor": "pydantic_ai.toolsets.AbstractToolset", "heading_level": 3, "md_text": "Bases: ABC , Generic[AgentDepsT] A toolset is a collection of tools that can be used by an agent. It is responsible for: * Listing the tools it contains * Validating the arguments of the tools * Calling the tools See [toolset docs](../../toolsets/index.html) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/abstract.py #### id abstractmethod property An ID for the toolset that is unique among all toolsets registered with the same agent. If you're implementing a concrete implementation that users can instantiate more than once, you should let them optionally pass a custom ID to the constructor and return that here. A toolset needs to have an ID in order to be used in a durable execution environment like Temporal, in which case the ID will be used to identify the toolset's activities within the workflow. #### label property The name of the toolset for use in error messages. #### tool\\_name\\_conflict\\_hint property A hint for how to avoid name conflicts with other toolsets for use in error messages. #### \\_\\_aenter\\_\\_ async Enter the toolset context. This is where you can set up network connections in a concrete implementation. Source code in pydantic_ai_slim/pydantic_ai/toolsets/abstract.py #### \\_\\_aexit\\_\\_ async Exit the toolset context. This is where you can tear down network connections in a concrete implementation. Source code in pydantic_ai_slim/pydantic_ai/toolsets/abstract.py #### get\\_tools abstractmethod async The tools that are available in this toolset. Source code in pydantic_ai_slim/pydantic_ai/toolsets/abstract.py #### call\\_tool abstractmethod async Call a tool with the given arguments. Parameters: Source code in pydantic_ai_slim/pydantic_ai/toolsets/abstract.py #### apply Run a visitor function on all \"leaf\" toolsets (i.e. those that implement their own tool listing and calling). Source code in pydantic_ai_slim/pydantic_ai/toolsets/abstract.py #### visit\\_and\\_replace Run a visitor function on all \"leaf\" toolsets (i.e. those that implement their own tool listing and calling) and replace them in the hierarchy with the result of the function. Source code in pydantic_ai_slim/pydantic_ai/toolsets/abstract.py #### filtered Returns a new toolset that filters this toolset's tools using a filter function that takes the agent context and the tool definition. See [toolset docs](../../toolsets/index.html#filtering-tools) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/abstract.py #### prefixed Returns a new toolset that prefixes the names of this toolset's tools. See [toolset docs](../../toolsets/index.html#prefixing-tool-names) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/abstract.py #### prepared Returns a new toolset that prepares this toolset's tools using a prepare function that takes the agent context and the original tool definitions. See [toolset docs](../../toolsets/index.html#preparing-tool-definitions) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/abstract.py #### renamed Returns a new toolset that renames this toolset's tools using a dictionary mapping new names to original names. See [toolset docs](../../toolsets/index.html#renaming-tools) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/abstract.py #### approval\\_required Returns a new toolset that requires (some) calls to tools it contains to be approved. See [toolset docs](../../toolsets/index.html#requiring-tool-approval) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/abstract.py", "url": "https://ai.pydantic.dev/api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset", "page": "api/toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "CombinedToolset dataclass", "anchor": "pydantic_ai.toolsets.CombinedToolset", "heading_level": 3, "md_text": "Bases: AbstractToolset[AgentDepsT] A toolset that combines multiple toolsets. See [toolset docs](../../toolsets/index.html#combining-toolsets) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/combined.py", "url": "https://ai.pydantic.dev/api/toolsets/index.html#pydantic_ai.toolsets.CombinedToolset", "page": "api/toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "ExternalToolset", "anchor": "pydantic_ai.toolsets.ExternalToolset", "heading_level": 3, "md_text": "Bases: AbstractToolset[AgentDepsT] A toolset that holds tools whose results will be produced outside of the Pydantic AI agent run in which they were called. See [toolset docs](../../toolsets/index.html#external-toolset) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/external.py", "url": "https://ai.pydantic.dev/api/toolsets/index.html#pydantic_ai.toolsets.ExternalToolset", "page": "api/toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "ApprovalRequiredToolset dataclass", "anchor": "pydantic_ai.toolsets.ApprovalRequiredToolset", "heading_level": 3, "md_text": "Bases: WrapperToolset[AgentDepsT] A toolset that requires (some) calls to tools it contains to be approved. See [toolset docs](../../toolsets/index.html#requiring-tool-approval) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/approval_required.py", "url": "https://ai.pydantic.dev/api/toolsets/index.html#pydantic_ai.toolsets.ApprovalRequiredToolset", "page": "api/toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "FilteredToolset dataclass", "anchor": "pydantic_ai.toolsets.FilteredToolset", "heading_level": 3, "md_text": "Bases: WrapperToolset[AgentDepsT] A toolset that filters the tools it contains using a filter function that takes the agent context and the tool definition. See [toolset docs](../../toolsets/index.html#filtering-tools) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/filtered.py", "url": "https://ai.pydantic.dev/api/toolsets/index.html#pydantic_ai.toolsets.FilteredToolset", "page": "api/toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "FunctionToolset", "anchor": "pydantic_ai.toolsets.FunctionToolset", "heading_level": 3, "md_text": "Bases: AbstractToolset[AgentDepsT] A toolset that lets Python functions be used as tools. See [toolset docs](../../toolsets/index.html#function-toolset) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/function.py #### \\_\\_init\\_\\_ Build a new function toolset. Parameters: Source code in pydantic_ai_slim/pydantic_ai/toolsets/function.py #### tool Decorator to register a tool function which takes [ RunContext ](../tools/index.html#pydantic_ai.tools.RunContext) as its first argument. Can decorate a sync or async functions. The docstring is inspected to extract both the tool description and description of each parameter, [learn more](../../tools/index.html#function-tools-and-schema). We can't add overloads for every possible signature of tool, since the return type is a recursive union so the signature of functions decorated with @toolset.tool is obscured. Example: Parameters: Source code in pydantic_ai_slim/pydantic_ai/toolsets/function.py #### add\\_function Add a function as a tool to the toolset. Can take a sync or async function. The docstring is inspected to extract both the tool description and description of each parameter, [learn more](../../tools/index.html#function-tools-and-schema). Parameters: Source code in pydantic_ai_slim/pydantic_ai/toolsets/function.py #### add\\_tool Add a tool to the toolset. Parameters: Source code in pydantic_ai_slim/pydantic_ai/toolsets/function.py", "url": "https://ai.pydantic.dev/api/toolsets/index.html#pydantic_ai.toolsets.FunctionToolset", "page": "api/toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "PrefixedToolset dataclass", "anchor": "pydantic_ai.toolsets.PrefixedToolset", "heading_level": 3, "md_text": "Bases: WrapperToolset[AgentDepsT] A toolset that prefixes the names of the tools it contains. See [toolset docs](../../toolsets/index.html#prefixing-tool-names) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/prefixed.py", "url": "https://ai.pydantic.dev/api/toolsets/index.html#pydantic_ai.toolsets.PrefixedToolset", "page": "api/toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "RenamedToolset dataclass", "anchor": "pydantic_ai.toolsets.RenamedToolset", "heading_level": 3, "md_text": "Bases: WrapperToolset[AgentDepsT] A toolset that renames the tools it contains using a dictionary mapping new names to original names. See [toolset docs](../../toolsets/index.html#renaming-tools) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/renamed.py", "url": "https://ai.pydantic.dev/api/toolsets/index.html#pydantic_ai.toolsets.RenamedToolset", "page": "api/toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "PreparedToolset dataclass", "anchor": "pydantic_ai.toolsets.PreparedToolset", "heading_level": 3, "md_text": "Bases: WrapperToolset[AgentDepsT] A toolset that prepares the tools it contains using a prepare function that takes the agent context and the original tool definitions. See [toolset docs](../../toolsets/index.html#preparing-tool-definitions) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/prepared.py", "url": "https://ai.pydantic.dev/api/toolsets/index.html#pydantic_ai.toolsets.PreparedToolset", "page": "api/toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "WrapperToolset dataclass", "anchor": "pydantic_ai.toolsets.WrapperToolset", "heading_level": 3, "md_text": "Bases: AbstractToolset[AgentDepsT] A toolset that wraps another toolset and delegates to it. See [toolset docs](../../toolsets/index.html#wrapping-a-toolset) for more information. Source code in pydantic_ai_slim/pydantic_ai/toolsets/wrapper.py", "url": "https://ai.pydantic.dev/api/toolsets/index.html#pydantic_ai.toolsets.WrapperToolset", "page": "api/toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "ToolsetFunc module-attribute", "anchor": "pydantic_ai.toolsets.ToolsetFunc", "heading_level": 3, "md_text": "A sync/async function which takes a run context and returns a toolset.", "url": "https://ai.pydantic.dev/api/toolsets/index.html#pydantic_ai.toolsets.ToolsetFunc", "page": "api/toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "FastMCPToolset dataclass", "anchor": "pydantic_ai.toolsets.fastmcp.FastMCPToolset", "heading_level": 3, "md_text": "Bases: AbstractToolset[AgentDepsT] A FastMCP Toolset that uses the FastMCP Client to call tools from a local or remote MCP Server. The Toolset can accept a FastMCP Client, a FastMCP Transport, or any other object which a FastMCP Transport can be created from. See https://gofastmcp.com/clients/transports for a full list of transports available. Source code in pydantic_ai_slim/pydantic_ai/toolsets/fastmcp.py #### client instance-attribute The FastMCP client to use. #### max\\_retries instance-attribute The maximum number of retries to attempt if a tool call fails. #### tool\\_error\\_behavior instance-attribute The behavior to take when a tool error occurs.", "url": "https://ai.pydantic.dev/api/toolsets/index.html#pydantic_ai.toolsets.fastmcp.FastMCPToolset", "page": "api/toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "Metrics & Attributes", "anchor": "metrics-attributes", "heading_level": 1, "md_text": "Track custom metrics and attributes during task execution for richer evaluation insights. ## Overview While executing evaluation tasks, you can record: * **Metrics** - Numeric values (int/float) for quantitative measurements * **Attributes** - Any data for qualitative information These appear in evaluation reports and can be used by evaluators for assessment. ## Recording Metrics Use [ increment_eval_metric ](../../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.increment_eval_metric) to track numeric values: ## Recording Attributes Use [ set_eval_attribute ](../../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.set_eval_attribute) to store any data: ## Accessing in Evaluators Metrics and attributes are available in the [ EvaluatorContext ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorContext): ## Viewing in Reports Metrics and attributes appear in report data: You can also display them in printed reports: ## Automatic Metrics When using Pydantic AI and Logfire, some metrics are automatically tracked: Access these in evaluators: ## Practical Examples ### API Usage Tracking ### Tool Usage Tracking ### Performance Tracking ### Quality Tracking ## Metrics vs Attributes vs Metadata Understanding the differences: ## Troubleshooting ### \"Metrics/attributes not appearing\" Ensure you're calling the functions inside the task: ### \"Metrics not incrementing\" Check you're using increment_eval_metric , not set_eval_attribute : ### \"Too much data in attributes\" Store summaries, not raw data: ## Next Steps * **[Custom Evaluators](../../evaluators/custom/index.html)** - Use metrics/attributes in evaluators * **[Logfire Integration](../logfire-integration/index.html)** - View metrics in Logfire * **[Concurrency & Performance](../concurrency/index.html)** - Optimize evaluation performance", "url": "https://ai.pydantic.dev/evals/how-to/metrics-attributes/index.html#metrics-attributes", "page": "evals/how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "While executing evaluation tasks, you can record: * **Metrics** - Numeric values (int/float) for quantitative measurements * **Attributes** - Any data for qualitative information These appear in evaluation reports and can be used by evaluators for assessment.", "url": "https://ai.pydantic.dev/evals/how-to/metrics-attributes/index.html#overview", "page": "evals/how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Recording Metrics", "anchor": "recording-metrics", "heading_level": 2, "md_text": "Use [ increment_eval_metric ](../../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.increment_eval_metric) to track numeric values:", "url": "https://ai.pydantic.dev/evals/how-to/metrics-attributes/index.html#recording-metrics", "page": "evals/how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Recording Attributes", "anchor": "recording-attributes", "heading_level": 2, "md_text": "Use [ set_eval_attribute ](../../../api/pydantic_evals/dataset/index.html#pydantic_evals.dataset.set_eval_attribute) to store any data:", "url": "https://ai.pydantic.dev/evals/how-to/metrics-attributes/index.html#recording-attributes", "page": "evals/how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Accessing in Evaluators", "anchor": "accessing-in-evaluators", "heading_level": 2, "md_text": "Metrics and attributes are available in the [ EvaluatorContext ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorContext):", "url": "https://ai.pydantic.dev/evals/how-to/metrics-attributes/index.html#accessing-in-evaluators", "page": "evals/how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Viewing in Reports", "anchor": "viewing-in-reports", "heading_level": 2, "md_text": "Metrics and attributes appear in report data: You can also display them in printed reports:", "url": "https://ai.pydantic.dev/evals/how-to/metrics-attributes/index.html#viewing-in-reports", "page": "evals/how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Automatic Metrics", "anchor": "automatic-metrics", "heading_level": 2, "md_text": "When using Pydantic AI and Logfire, some metrics are automatically tracked: Access these in evaluators:", "url": "https://ai.pydantic.dev/evals/how-to/metrics-attributes/index.html#automatic-metrics", "page": "evals/how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Practical Examples", "anchor": "practical-examples", "heading_level": 2, "md_text": "### API Usage Tracking ### Tool Usage Tracking ### Performance Tracking ### Quality Tracking", "url": "https://ai.pydantic.dev/evals/how-to/metrics-attributes/index.html#practical-examples", "page": "evals/how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "API Usage Tracking", "anchor": "api-usage-tracking", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/metrics-attributes/index.html#api-usage-tracking", "page": "evals/how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Tool Usage Tracking", "anchor": "tool-usage-tracking", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/metrics-attributes/index.html#tool-usage-tracking", "page": "evals/how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Performance Tracking", "anchor": "performance-tracking", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/metrics-attributes/index.html#performance-tracking", "page": "evals/how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Quality Tracking", "anchor": "quality-tracking", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/metrics-attributes/index.html#quality-tracking", "page": "evals/how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Metrics vs Attributes vs Metadata", "anchor": "metrics-vs-attributes-vs-metadata", "heading_level": 2, "md_text": "Understanding the differences:", "url": "https://ai.pydantic.dev/evals/how-to/metrics-attributes/index.html#metrics-vs-attributes-vs-metadata", "page": "evals/how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Troubleshooting", "anchor": "troubleshooting", "heading_level": 2, "md_text": "### \"Metrics/attributes not appearing\" Ensure you're calling the functions inside the task: ### \"Metrics not incrementing\" Check you're using increment_eval_metric , not set_eval_attribute : ### \"Too much data in attributes\" Store summaries, not raw data:", "url": "https://ai.pydantic.dev/evals/how-to/metrics-attributes/index.html#troubleshooting", "page": "evals/how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "\"Metrics/attributes not appearing\"", "anchor": "metricsattributes-not-appearing", "heading_level": 3, "md_text": "Ensure you're calling the functions inside the task:", "url": "https://ai.pydantic.dev/evals/how-to/metrics-attributes/index.html#metricsattributes-not-appearing", "page": "evals/how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "\"Metrics not incrementing\"", "anchor": "metrics-not-incrementing", "heading_level": 3, "md_text": "Check you're using increment_eval_metric , not set_eval_attribute :", "url": "https://ai.pydantic.dev/evals/how-to/metrics-attributes/index.html#metrics-not-incrementing", "page": "evals/how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "\"Too much data in attributes\"", "anchor": "too-much-data-in-attributes", "heading_level": 3, "md_text": "Store summaries, not raw data:", "url": "https://ai.pydantic.dev/evals/how-to/metrics-attributes/index.html#too-much-data-in-attributes", "page": "evals/how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "* **[Custom Evaluators](../../evaluators/custom/index.html)** - Use metrics/attributes in evaluators * **[Logfire Integration](../logfire-integration/index.html)** - View metrics in Logfire * **[Concurrency & Performance](../concurrency/index.html)** - Optimize evaluation performance", "url": "https://ai.pydantic.dev/evals/how-to/metrics-attributes/index.html#next-steps", "page": "evals/how-to/metrics-attributes/index.html", "source_site": "pydantic_ai"}
{"title": "Retry Strategies", "anchor": "retry-strategies", "heading_level": 1, "md_text": "Handle transient failures in tasks and evaluators with automatic retry logic. ## Overview LLM-based systems can experience transient failures: * Rate limits * Network timeouts * Temporary API outages * Context length errors Pydantic Evals supports retry configuration for both: * **Task execution** - The function being evaluated * **Evaluator execution** - The evaluators themselves ## Basic Retry Configuration Pass a retry configuration to evaluate() or evaluate_sync() using [Tenacity](https://tenacity.readthedocs.io/) parameters: ## Retry Configuration Options Retry configurations use [Tenacity](https://tenacity.readthedocs.io/) and support the same options as Pydantic AI's [ RetryConfig ](../../../api/retries/index.html#pydantic_ai.retries.RetryConfig): ### Common Parameters The retry configuration accepts any parameters from the tenacity retry decorator. Common ones include: See the [Tenacity documentation](https://tenacity.readthedocs.io/) for all available options. ## Task Retries Retry the task function when it fails: ### When Task Retries Trigger Retries trigger when the task raises an exception: ### Exponential Backoff When using wait_exponential() , delays increase exponentially: The actual delay depends on the multiplier , min , and max parameters passed to wait_exponential() . ## Evaluator Retries Retry evaluators when they fail: ### When Evaluator Retries Trigger Retries trigger when an evaluator raises an exception: ### Evaluator Failures If an evaluator fails after all retries, it's recorded as an [ EvaluatorFailure ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorFailure): View evaluator failures in reports: ## Combining Task and Evaluator Retries You can configure both independently: ## Practical Examples ### Rate Limit Handling ### Network Timeout Handling ### Context Length Handling ## Retry vs Error Handling **Use retries for:** - Transient failures (rate limits, timeouts) - Network issues - Temporary service outages - Recoverable errors **Use error handling for:** - Validation errors - Logic errors - Permanent failures - Expected error conditions ## Troubleshooting ### \"Still failing after retries\" Increase retry attempts or check if error is retriable: ### \"Evaluations taking too long\" Reduce retry attempts or wait times: ### \"Hitting rate limits despite retries\" Increase delays or use max_concurrency : ## Next Steps * **[Concurrency & Performance](../concurrency/index.html)** - Optimize evaluation performance * **[Logfire Integration](../logfire-integration/index.html)** - View retries in Logfire", "url": "https://ai.pydantic.dev/evals/how-to/retry-strategies/index.html#retry-strategies", "page": "evals/how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "LLM-based systems can experience transient failures: * Rate limits * Network timeouts * Temporary API outages * Context length errors Pydantic Evals supports retry configuration for both: * **Task execution** - The function being evaluated * **Evaluator execution** - The evaluators themselves", "url": "https://ai.pydantic.dev/evals/how-to/retry-strategies/index.html#overview", "page": "evals/how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Basic Retry Configuration", "anchor": "basic-retry-configuration", "heading_level": 2, "md_text": "Pass a retry configuration to evaluate() or evaluate_sync() using [Tenacity](https://tenacity.readthedocs.io/) parameters:", "url": "https://ai.pydantic.dev/evals/how-to/retry-strategies/index.html#basic-retry-configuration", "page": "evals/how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Retry Configuration Options", "anchor": "retry-configuration-options", "heading_level": 2, "md_text": "Retry configurations use [Tenacity](https://tenacity.readthedocs.io/) and support the same options as Pydantic AI's [ RetryConfig ](../../../api/retries/index.html#pydantic_ai.retries.RetryConfig): ### Common Parameters The retry configuration accepts any parameters from the tenacity retry decorator. Common ones include: See the [Tenacity documentation](https://tenacity.readthedocs.io/) for all available options.", "url": "https://ai.pydantic.dev/evals/how-to/retry-strategies/index.html#retry-configuration-options", "page": "evals/how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Common Parameters", "anchor": "common-parameters", "heading_level": 3, "md_text": "The retry configuration accepts any parameters from the tenacity retry decorator. Common ones include: See the [Tenacity documentation](https://tenacity.readthedocs.io/) for all available options.", "url": "https://ai.pydantic.dev/evals/how-to/retry-strategies/index.html#common-parameters", "page": "evals/how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Task Retries", "anchor": "task-retries", "heading_level": 2, "md_text": "Retry the task function when it fails: ### When Task Retries Trigger Retries trigger when the task raises an exception: ### Exponential Backoff When using wait_exponential() , delays increase exponentially: The actual delay depends on the multiplier , min , and max parameters passed to wait_exponential() .", "url": "https://ai.pydantic.dev/evals/how-to/retry-strategies/index.html#task-retries", "page": "evals/how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "When Task Retries Trigger", "anchor": "when-task-retries-trigger", "heading_level": 3, "md_text": "Retries trigger when the task raises an exception:", "url": "https://ai.pydantic.dev/evals/how-to/retry-strategies/index.html#when-task-retries-trigger", "page": "evals/how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Exponential Backoff", "anchor": "exponential-backoff", "heading_level": 3, "md_text": "When using wait_exponential() , delays increase exponentially: The actual delay depends on the multiplier , min , and max parameters passed to wait_exponential() .", "url": "https://ai.pydantic.dev/evals/how-to/retry-strategies/index.html#exponential-backoff", "page": "evals/how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluator Retries", "anchor": "evaluator-retries", "heading_level": 2, "md_text": "Retry evaluators when they fail: ### When Evaluator Retries Trigger Retries trigger when an evaluator raises an exception: ### Evaluator Failures If an evaluator fails after all retries, it's recorded as an [ EvaluatorFailure ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorFailure): View evaluator failures in reports:", "url": "https://ai.pydantic.dev/evals/how-to/retry-strategies/index.html#evaluator-retries", "page": "evals/how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "When Evaluator Retries Trigger", "anchor": "when-evaluator-retries-trigger", "heading_level": 3, "md_text": "Retries trigger when an evaluator raises an exception:", "url": "https://ai.pydantic.dev/evals/how-to/retry-strategies/index.html#when-evaluator-retries-trigger", "page": "evals/how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Evaluator Failures", "anchor": "evaluator-failures", "heading_level": 3, "md_text": "If an evaluator fails after all retries, it's recorded as an [ EvaluatorFailure ](../../../api/pydantic_evals/evaluators/index.html#pydantic_evals.evaluators.EvaluatorFailure): View evaluator failures in reports:", "url": "https://ai.pydantic.dev/evals/how-to/retry-strategies/index.html#evaluator-failures", "page": "evals/how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Combining Task and Evaluator Retries", "anchor": "combining-task-and-evaluator-retries", "heading_level": 2, "md_text": "You can configure both independently:", "url": "https://ai.pydantic.dev/evals/how-to/retry-strategies/index.html#combining-task-and-evaluator-retries", "page": "evals/how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Practical Examples", "anchor": "practical-examples", "heading_level": 2, "md_text": "### Rate Limit Handling ### Network Timeout Handling ### Context Length Handling", "url": "https://ai.pydantic.dev/evals/how-to/retry-strategies/index.html#practical-examples", "page": "evals/how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Rate Limit Handling", "anchor": "rate-limit-handling", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/retry-strategies/index.html#rate-limit-handling", "page": "evals/how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Network Timeout Handling", "anchor": "network-timeout-handling", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/retry-strategies/index.html#network-timeout-handling", "page": "evals/how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Context Length Handling", "anchor": "context-length-handling", "heading_level": 3, "md_text": "", "url": "https://ai.pydantic.dev/evals/how-to/retry-strategies/index.html#context-length-handling", "page": "evals/how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Retry vs Error Handling", "anchor": "retry-vs-error-handling", "heading_level": 2, "md_text": "**Use retries for:** - Transient failures (rate limits, timeouts) - Network issues - Temporary service outages - Recoverable errors **Use error handling for:** - Validation errors - Logic errors - Permanent failures - Expected error conditions", "url": "https://ai.pydantic.dev/evals/how-to/retry-strategies/index.html#retry-vs-error-handling", "page": "evals/how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Troubleshooting", "anchor": "troubleshooting", "heading_level": 2, "md_text": "### \"Still failing after retries\" Increase retry attempts or check if error is retriable: ### \"Evaluations taking too long\" Reduce retry attempts or wait times: ### \"Hitting rate limits despite retries\" Increase delays or use max_concurrency :", "url": "https://ai.pydantic.dev/evals/how-to/retry-strategies/index.html#troubleshooting", "page": "evals/how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "\"Still failing after retries\"", "anchor": "still-failing-after-retries", "heading_level": 3, "md_text": "Increase retry attempts or check if error is retriable:", "url": "https://ai.pydantic.dev/evals/how-to/retry-strategies/index.html#still-failing-after-retries", "page": "evals/how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "\"Evaluations taking too long\"", "anchor": "evaluations-taking-too-long", "heading_level": 3, "md_text": "Reduce retry attempts or wait times:", "url": "https://ai.pydantic.dev/evals/how-to/retry-strategies/index.html#evaluations-taking-too-long", "page": "evals/how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "\"Hitting rate limits despite retries\"", "anchor": "hitting-rate-limits-despite-retries", "heading_level": 3, "md_text": "Increase delays or use max_concurrency :", "url": "https://ai.pydantic.dev/evals/how-to/retry-strategies/index.html#hitting-rate-limits-despite-retries", "page": "evals/how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "* **[Concurrency & Performance](../concurrency/index.html)** - Optimize evaluation performance * **[Logfire Integration](../logfire-integration/index.html)** - View retries in Logfire", "url": "https://ai.pydantic.dev/evals/how-to/retry-strategies/index.html#next-steps", "page": "evals/how-to/retry-strategies/index.html", "source_site": "pydantic_ai"}
{"title": "Data Analyst", "anchor": "data-analyst", "heading_level": 1, "md_text": "Sometimes in an agent workflow, the agent does not need to know the exact tool output, but still needs to process the tool output in some ways. This is especially common in data analytics: the agent needs to know that the result of a query tool is a DataFrame with certain named columns, but not necessarily the content of every single row. With Pydantic AI, you can use a [dependencies object](../../dependencies/index.html) to store the result from one tool and use it in another tool. In this example, we'll build an agent that analyzes the [Rotten Tomatoes movie review dataset from Cornell](https://huggingface.co/datasets/cornell-movie-review-data/rotten_tomatoes). Demonstrates: * [agent dependencies](../../dependencies/index.html) ## Running the Example With [dependencies installed and environment variables set](../setup/index.html#usage), run: pipuv Output (debug): > Based on my analysis of the Cornell Movie Review dataset (rotten\\_tomatoes), there are **4,265 negative comments** in the training split. These are the reviews labeled as 'neg' (represented by 0 in the dataset). ## Example Code [data\\_analyst.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/data_analyst.py) ## Appendix ### Choosing a Model This example requires using a model that understands DuckDB SQL. You can check with clai :", "url": "https://ai.pydantic.dev/examples/data-analyst/index.html#data-analyst", "page": "examples/data-analyst/index.html", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "heading_level": 2, "md_text": "With [dependencies installed and environment variables set](../setup/index.html#usage), run: pipuv Output (debug): > Based on my analysis of the Cornell Movie Review dataset (rotten\\_tomatoes), there are **4,265 negative comments** in the training split. These are the reviews labeled as 'neg' (represented by 0 in the dataset).", "url": "https://ai.pydantic.dev/examples/data-analyst/index.html#running-the-example", "page": "examples/data-analyst/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "heading_level": 2, "md_text": "[data\\_analyst.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/data_analyst.py)", "url": "https://ai.pydantic.dev/examples/data-analyst/index.html#example-code", "page": "examples/data-analyst/index.html", "source_site": "pydantic_ai"}
{"title": "Appendix", "anchor": "appendix", "heading_level": 2, "md_text": "### Choosing a Model This example requires using a model that understands DuckDB SQL. You can check with clai :", "url": "https://ai.pydantic.dev/examples/data-analyst/index.html#appendix", "page": "examples/data-analyst/index.html", "source_site": "pydantic_ai"}
{"title": "Choosing a Model", "anchor": "choosing-a-model", "heading_level": 3, "md_text": "This example requires using a model that understands DuckDB SQL. You can check with clai :", "url": "https://ai.pydantic.dev/examples/data-analyst/index.html#choosing-a-model", "page": "examples/data-analyst/index.html", "source_site": "pydantic_ai"}
{"title": "Pydantic Model", "anchor": "pydantic-model", "heading_level": 1, "md_text": "Simple example of using Pydantic AI to construct a Pydantic model from a text input. Demonstrates: * [structured output_type ](../../output/index.html#structured-output) ## Running the Example With [dependencies installed and environment variables set](../setup/index.html#usage), run: pipuv This examples uses openai:gpt-4o by default, but it works well with other models, e.g. you can run it with Gemini using: pipuv (or PYDANTIC_AI_MODEL=gemini-1.5-flash ... ) ## Example Code [pydantic\\_model.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/pydantic_model.py)", "url": "https://ai.pydantic.dev/examples/pydantic-model/index.html#pydantic-model", "page": "examples/pydantic-model/index.html", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "heading_level": 2, "md_text": "With [dependencies installed and environment variables set](../setup/index.html#usage), run: pipuv This examples uses openai:gpt-4o by default, but it works well with other models, e.g. you can run it with Gemini using: pipuv (or PYDANTIC_AI_MODEL=gemini-1.5-flash ... )", "url": "https://ai.pydantic.dev/examples/pydantic-model/index.html#running-the-example", "page": "examples/pydantic-model/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "heading_level": 2, "md_text": "[pydantic\\_model.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/pydantic_model.py)", "url": "https://ai.pydantic.dev/examples/pydantic-model/index.html#example-code", "page": "examples/pydantic-model/index.html", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "heading_level": 2, "md_text": "With [dependencies installed and environment variables set](../setup/index.html#usage), run: pipuv", "url": "https://ai.pydantic.dev/examples/flight-booking/index.html#running-the-example", "page": "examples/flight-booking/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "heading_level": 2, "md_text": "[flight\\_booking.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/flight_booking.py)", "url": "https://ai.pydantic.dev/examples/flight-booking/index.html#example-code", "page": "examples/flight-booking/index.html", "source_site": "pydantic_ai"}
{"title": "Examples", "anchor": "examples", "heading_level": 1, "md_text": "Here we include some examples of how to use Pydantic AI and what it can do. ## Usage These examples are distributed with pydantic-ai so you can run them either by cloning the [pydantic-ai repo](https://github.com/pydantic/pydantic-ai) or by simply installing pydantic-ai from PyPI with pip or uv . ### Installing required dependencies Either way you'll need to install extra dependencies to run some examples, you just need to install the examples optional dependency group. If you've installed pydantic-ai via pip/uv, you can install the extra dependencies with: pipuv If you clone the repo, you should instead use uv sync --extra examples to install extra dependencies. ### Setting model environment variables These examples will need you to set up authentication with one or more of the LLMs, see the [model configuration](../../models/overview/index.html) docs for details on how to do this. TL;DR: in most cases you'll need to set one of the following environment variables: OpenAIGoogle Gemini ### Running Examples To run the examples (this will work whether you installed pydantic_ai , or cloned the repo), run: pipuv For examples, to run the very simple [ pydantic_model ](../pydantic-model/index.html) example: pipuv If you like one-liners and you're using uv, you can run a pydantic-ai example with zero setup: --- You'll probably want to edit examples in addition to just running them. You can copy the examples to a new directory with: pipuv", "url": "https://ai.pydantic.dev/examples/setup/index.html#examples", "page": "examples/setup/index.html", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 2, "md_text": "These examples are distributed with pydantic-ai so you can run them either by cloning the [pydantic-ai repo](https://github.com/pydantic/pydantic-ai) or by simply installing pydantic-ai from PyPI with pip or uv . ### Installing required dependencies Either way you'll need to install extra dependencies to run some examples, you just need to install the examples optional dependency group. If you've installed pydantic-ai via pip/uv, you can install the extra dependencies with: pipuv If you clone the repo, you should instead use uv sync --extra examples to install extra dependencies. ### Setting model environment variables These examples will need you to set up authentication with one or more of the LLMs, see the [model configuration](../../models/overview/index.html) docs for details on how to do this. TL;DR: in most cases you'll need to set one of the following environment variables: OpenAIGoogle Gemini ### Running Examples To run the examples (this will work whether you installed pydantic_ai , or cloned the repo), run: pipuv For examples, to run the very simple [ pydantic_model ](../pydantic-model/index.html) example: pipuv If you like one-liners and you're using uv, you can run a pydantic-ai example with zero setup: --- You'll probably want to edit examples in addition to just running them. You can copy the examples to a new directory with: pipuv", "url": "https://ai.pydantic.dev/examples/setup/index.html#usage", "page": "examples/setup/index.html", "source_site": "pydantic_ai"}
{"title": "Installing required dependencies", "anchor": "installing-required-dependencies", "heading_level": 3, "md_text": "Either way you'll need to install extra dependencies to run some examples, you just need to install the examples optional dependency group. If you've installed pydantic-ai via pip/uv, you can install the extra dependencies with: pipuv If you clone the repo, you should instead use uv sync --extra examples to install extra dependencies.", "url": "https://ai.pydantic.dev/examples/setup/index.html#installing-required-dependencies", "page": "examples/setup/index.html", "source_site": "pydantic_ai"}
{"title": "Setting model environment variables", "anchor": "setting-model-environment-variables", "heading_level": 3, "md_text": "These examples will need you to set up authentication with one or more of the LLMs, see the [model configuration](../../models/overview/index.html) docs for details on how to do this. TL;DR: in most cases you'll need to set one of the following environment variables: OpenAIGoogle Gemini", "url": "https://ai.pydantic.dev/examples/setup/index.html#setting-model-environment-variables", "page": "examples/setup/index.html", "source_site": "pydantic_ai"}
{"title": "Running Examples", "anchor": "running-examples", "heading_level": 3, "md_text": "To run the examples (this will work whether you installed pydantic_ai , or cloned the repo), run: pipuv For examples, to run the very simple [ pydantic_model ](../pydantic-model/index.html) example: pipuv If you like one-liners and you're using uv, you can run a pydantic-ai example with zero setup: --- You'll probably want to edit examples in addition to just running them. You can copy the examples to a new directory with: pipuv", "url": "https://ai.pydantic.dev/examples/setup/index.html#running-examples", "page": "examples/setup/index.html", "source_site": "pydantic_ai"}
{"title": "Agent User Interaction (AG-UI)", "anchor": "agent-user-interaction-ag-ui", "heading_level": 1, "md_text": "Example of using Pydantic AI agents with the [AG-UI Dojo](https://github.com/ag-ui-protocol/ag-ui/tree/main/typescript-sdk/apps/dojo) example app. See the [AG-UI docs](../../ag-ui/index.html) for more information about the AG-UI integration. Demonstrates: * [AG-UI](../../ag-ui/index.html) * [Tools](../../tools/index.html) ## Prerequisites * An [OpenAI API key](https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key) ## Running the Example With [dependencies installed and environment variables set](../setup/index.html#usage) you will need two command line windows. ### Pydantic AI AG-UI backend Setup your OpenAI API Key Start the Pydantic AI AG-UI example backend. pipuv ### AG-UI Dojo example frontend Next run the AG-UI Dojo example frontend. 1. Clone the [AG-UI repository](https://github.com/ag-ui-protocol/ag-ui) 2. Change into to the ag-ui/typescript-sdk directory 3. Run the Dojo app following the [official instructions](https://github.com/ag-ui-protocol/ag-ui/tree/main/typescript-sdk/apps/dojo#development-setup) 4. Visit <http://localhost:3000/pydantic-ai> 5. Select View Pydantic AI from the sidebar ## Feature Examples ### Agentic Chat This demonstrates a basic agent interaction including Pydantic AI server side tools and AG-UI client side tools. If you've [run the example](index.html#running-the-example), you can view it at <http://localhost:3000/pydantic-ai/feature/agentic_chat>. #### Agent Tools * time - Pydantic AI tool to check the current time for a time zone * background - AG-UI tool to set the background color of the client window #### Agent Prompts A complex example which mixes both AG-UI and Pydantic AI tools: #### Agentic Chat - Code [ag\\_ui/api/agentic\\_chat.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/agentic_chat.py) ### Agentic Generative UI Demonstrates a long running task where the agent sends updates to the frontend to let the user know what's happening. If you've [run the example](index.html#running-the-example), you can view it at <http://localhost:3000/pydantic-ai/feature/agentic_generative_ui>. #### Plan Prompts #### Agentic Generative UI - Code [ag\\_ui/api/agentic\\_generative\\_ui.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py) ### Human in the Loop Demonstrates simple human in the loop workflow where the agent comes up with a plan and the user can approve it using checkboxes. #### Task Planning Tools * generate_task_steps - AG-UI tool to generate and confirm steps #### Task Planning Prompt #### Human in the Loop - Code [ag\\_ui/api/human\\_in\\_the\\_loop.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/human_in_the_loop.py) ### Predictive State Updates Demonstrates how to use the predictive state updates feature to update the state of the UI based on agent responses, including user interaction via user confirmation. If you've [run the example](index.html#running-the-example), you can view it at <http://localhost:3000/pydantic-ai/feature/predictive_state_updates>. #### Story Tools * write_document - AG-UI tool to write the document to a window * document_predict_state - Pydantic AI tool that enables document state prediction for the write_document tool This also shows how to use custom instructions based on shared state information. #### Story Example Starting document text Agent prompt #### Predictive State Updates - Code [ag\\_ui/api/predictive\\_state\\_updates.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/predictive_state_updates.py) ### Shared State Demonstrates how to use the shared state between the UI and the agent. State sent to the agent is detected by a function based instruction. This then validates the data using a custom pydantic model before using to create the instructions for the agent to follow and send to the client using a AG-UI tool. If you've [run the example](index.html#running-the-example), you can view it at <http://localhost:3000/pydantic-ai/feature/shared_state>. #### Recipe Tools * display_recipe - AG-UI tool to display the recipe in a graphical format #### Recipe Example 1. Customise the basic settings of your recipe 2. Click Improve with AI #### Shared State - Code [ag\\_ui/api/shared\\_state.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/shared_state.py) ### Tool Based Generative UI Demonstrates customised rendering for tool output with used confirmation. If you've [run the example](index.html#running-the-example), you can view it at <http://localhost:3000/pydantic-ai/feature/tool_based_generative_ui>. #### Haiku Tools * generate_haiku - AG-UI tool to display a haiku in English and Japanese #### Haiku Prompt #### Tool Based Generative UI - Code [ag\\_ui/api/tool\\_based\\_generative\\_ui.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/tool_based_generative_ui.py)", "url": "https://ai.pydantic.dev/examples/ag-ui/index.html#agent-user-interaction-ag-ui", "page": "examples/ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Prerequisites", "anchor": "prerequisites", "heading_level": 2, "md_text": "* An [OpenAI API key](https://help.openai.com/en/articles/4936850-where-do-i-find-my-openai-api-key)", "url": "https://ai.pydantic.dev/examples/ag-ui/index.html#prerequisites", "page": "examples/ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "heading_level": 2, "md_text": "With [dependencies installed and environment variables set](../setup/index.html#usage) you will need two command line windows. ### Pydantic AI AG-UI backend Setup your OpenAI API Key Start the Pydantic AI AG-UI example backend. pipuv ### AG-UI Dojo example frontend Next run the AG-UI Dojo example frontend. 1. Clone the [AG-UI repository](https://github.com/ag-ui-protocol/ag-ui) 2. Change into to the ag-ui/typescript-sdk directory 3. Run the Dojo app following the [official instructions](https://github.com/ag-ui-protocol/ag-ui/tree/main/typescript-sdk/apps/dojo#development-setup) 4. Visit <http://localhost:3000/pydantic-ai> 5. Select View Pydantic AI from the sidebar", "url": "https://ai.pydantic.dev/examples/ag-ui/index.html#running-the-example", "page": "examples/ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Pydantic AI AG-UI backend", "anchor": "pydantic-ai-ag-ui-backend", "heading_level": 3, "md_text": "Setup your OpenAI API Key Start the Pydantic AI AG-UI example backend. pipuv", "url": "https://ai.pydantic.dev/examples/ag-ui/index.html#pydantic-ai-ag-ui-backend", "page": "examples/ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "AG-UI Dojo example frontend", "anchor": "ag-ui-dojo-example-frontend", "heading_level": 3, "md_text": "Next run the AG-UI Dojo example frontend. 1. Clone the [AG-UI repository](https://github.com/ag-ui-protocol/ag-ui) 2. Change into to the ag-ui/typescript-sdk directory 3. Run the Dojo app following the [official instructions](https://github.com/ag-ui-protocol/ag-ui/tree/main/typescript-sdk/apps/dojo#development-setup) 4. Visit <http://localhost:3000/pydantic-ai> 5. Select View Pydantic AI from the sidebar", "url": "https://ai.pydantic.dev/examples/ag-ui/index.html#ag-ui-dojo-example-frontend", "page": "examples/ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Feature Examples", "anchor": "feature-examples", "heading_level": 2, "md_text": "### Agentic Chat This demonstrates a basic agent interaction including Pydantic AI server side tools and AG-UI client side tools. If you've [run the example](index.html#running-the-example), you can view it at <http://localhost:3000/pydantic-ai/feature/agentic_chat>. #### Agent Tools * time - Pydantic AI tool to check the current time for a time zone * background - AG-UI tool to set the background color of the client window #### Agent Prompts A complex example which mixes both AG-UI and Pydantic AI tools: #### Agentic Chat - Code [ag\\_ui/api/agentic\\_chat.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/agentic_chat.py) ### Agentic Generative UI Demonstrates a long running task where the agent sends updates to the frontend to let the user know what's happening. If you've [run the example](index.html#running-the-example), you can view it at <http://localhost:3000/pydantic-ai/feature/agentic_generative_ui>. #### Plan Prompts #### Agentic Generative UI - Code [ag\\_ui/api/agentic\\_generative\\_ui.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py) ### Human in the Loop Demonstrates simple human in the loop workflow where the agent comes up with a plan and the user can approve it using checkboxes. #### Task Planning Tools * generate_task_steps - AG-UI tool to generate and confirm steps #### Task Planning Prompt #### Human in the Loop - Code [ag\\_ui/api/human\\_in\\_the\\_loop.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/human_in_the_loop.py) ### Predictive State Updates Demonstrates how to use the predictive state updates feature to update the state of the UI based on agent responses, including user interaction via user confirmation. If you've [run the example](index.html#running-the-example), you can view it at <http://localhost:3000/pydantic-ai/feature/predictive_state_updates>. #### Story Tools * write_document - AG-UI tool to write the document to a window * document_predict_state - Pydantic AI tool that enables document state prediction for the write_document tool This also shows how to use custom instructions based on shared state information. #### Story Example Starting document text Agent prompt #### Predictive State Updates - Code [ag\\_ui/api/predictive\\_state\\_updates.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/predictive_state_updates.py) ### Shared State Demonstrates how to use the shared state between the UI and the agent. State sent to the agent is detected by a function based instruction. This then validates the data using a custom pydantic model before using to create the instructions for the agent to follow and send to the client using a AG-UI tool. If you've [run the example](index.html#running-the-example), you can view it at <http://localhost:3000/pydantic-ai/feature/shared_state>. #### Recipe Tools * display_recipe - AG-UI tool to display the recipe in a graphical format #### Recipe Example 1. Customise the basic settings of your recipe 2. Click Improve with AI #### Shared State - Code [ag\\_ui/api/shared\\_state.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/shared_state.py) ### Tool Based Generative UI Demonstrates customised rendering for tool output with used confirmation. If you've [run the example](index.html#running-the-example), you can view it at <http://localhost:3000/pydantic-ai/feature/tool_based_generative_ui>. #### Haiku Tools * generate_haiku - AG-UI tool to display a haiku in English and Japanese #### Haiku Prompt #### Tool Based Generative UI - Code [ag\\_ui/api/tool\\_based\\_generative\\_ui.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/tool_based_generative_ui.py)", "url": "https://ai.pydantic.dev/examples/ag-ui/index.html#feature-examples", "page": "examples/ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Agentic Chat", "anchor": "agentic-chat", "heading_level": 3, "md_text": "This demonstrates a basic agent interaction including Pydantic AI server side tools and AG-UI client side tools. If you've [run the example](index.html#running-the-example), you can view it at <http://localhost:3000/pydantic-ai/feature/agentic_chat>. #### Agent Tools * time - Pydantic AI tool to check the current time for a time zone * background - AG-UI tool to set the background color of the client window #### Agent Prompts A complex example which mixes both AG-UI and Pydantic AI tools: #### Agentic Chat - Code [ag\\_ui/api/agentic\\_chat.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/agentic_chat.py)", "url": "https://ai.pydantic.dev/examples/ag-ui/index.html#agentic-chat", "page": "examples/ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Agentic Generative UI", "anchor": "agentic-generative-ui", "heading_level": 3, "md_text": "Demonstrates a long running task where the agent sends updates to the frontend to let the user know what's happening. If you've [run the example](index.html#running-the-example), you can view it at <http://localhost:3000/pydantic-ai/feature/agentic_generative_ui>. #### Plan Prompts #### Agentic Generative UI - Code [ag\\_ui/api/agentic\\_generative\\_ui.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/agentic_generative_ui.py)", "url": "https://ai.pydantic.dev/examples/ag-ui/index.html#agentic-generative-ui", "page": "examples/ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Human in the Loop", "anchor": "human-in-the-loop", "heading_level": 3, "md_text": "Demonstrates simple human in the loop workflow where the agent comes up with a plan and the user can approve it using checkboxes. #### Task Planning Tools * generate_task_steps - AG-UI tool to generate and confirm steps #### Task Planning Prompt #### Human in the Loop - Code [ag\\_ui/api/human\\_in\\_the\\_loop.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/human_in_the_loop.py)", "url": "https://ai.pydantic.dev/examples/ag-ui/index.html#human-in-the-loop", "page": "examples/ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Predictive State Updates", "anchor": "predictive-state-updates", "heading_level": 3, "md_text": "Demonstrates how to use the predictive state updates feature to update the state of the UI based on agent responses, including user interaction via user confirmation. If you've [run the example](index.html#running-the-example), you can view it at <http://localhost:3000/pydantic-ai/feature/predictive_state_updates>. #### Story Tools * write_document - AG-UI tool to write the document to a window * document_predict_state - Pydantic AI tool that enables document state prediction for the write_document tool This also shows how to use custom instructions based on shared state information. #### Story Example Starting document text Agent prompt #### Predictive State Updates - Code [ag\\_ui/api/predictive\\_state\\_updates.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/predictive_state_updates.py)", "url": "https://ai.pydantic.dev/examples/ag-ui/index.html#predictive-state-updates", "page": "examples/ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Shared State", "anchor": "shared-state", "heading_level": 3, "md_text": "Demonstrates how to use the shared state between the UI and the agent. State sent to the agent is detected by a function based instruction. This then validates the data using a custom pydantic model before using to create the instructions for the agent to follow and send to the client using a AG-UI tool. If you've [run the example](index.html#running-the-example), you can view it at <http://localhost:3000/pydantic-ai/feature/shared_state>. #### Recipe Tools * display_recipe - AG-UI tool to display the recipe in a graphical format #### Recipe Example 1. Customise the basic settings of your recipe 2. Click Improve with AI #### Shared State - Code [ag\\_ui/api/shared\\_state.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/shared_state.py)", "url": "https://ai.pydantic.dev/examples/ag-ui/index.html#shared-state", "page": "examples/ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Tool Based Generative UI", "anchor": "tool-based-generative-ui", "heading_level": 3, "md_text": "Demonstrates customised rendering for tool output with used confirmation. If you've [run the example](index.html#running-the-example), you can view it at <http://localhost:3000/pydantic-ai/feature/tool_based_generative_ui>. #### Haiku Tools * generate_haiku - AG-UI tool to display a haiku in English and Japanese #### Haiku Prompt #### Tool Based Generative UI - Code [ag\\_ui/api/tool\\_based\\_generative\\_ui.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/ag_ui/api/tool_based_generative_ui.py)", "url": "https://ai.pydantic.dev/examples/ag-ui/index.html#tool-based-generative-ui", "page": "examples/ag-ui/index.html", "source_site": "pydantic_ai"}
{"title": "Question Graph", "anchor": "question-graph", "heading_level": 1, "md_text": "Example of a graph for asking and evaluating questions. Demonstrates: * [ pydantic_graph ](../../graph/index.html) ## Running the Example With [dependencies installed and environment variables set](../setup/index.html#usage), run: pipuv ## Example Code [question\\_graph.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/question_graph.py) The mermaid diagram generated in this example looks like this:", "url": "https://ai.pydantic.dev/examples/question-graph/index.html#question-graph", "page": "examples/question-graph/index.html", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "heading_level": 2, "md_text": "With [dependencies installed and environment variables set](../setup/index.html#usage), run: pipuv", "url": "https://ai.pydantic.dev/examples/question-graph/index.html#running-the-example", "page": "examples/question-graph/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "heading_level": 2, "md_text": "[question\\_graph.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/question_graph.py) The mermaid diagram generated in this example looks like this:", "url": "https://ai.pydantic.dev/examples/question-graph/index.html#example-code", "page": "examples/question-graph/index.html", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "heading_level": 2, "md_text": "With [dependencies installed and environment variables set](../setup/index.html#usage), run: pipuv", "url": "https://ai.pydantic.dev/examples/stream-markdown/index.html#running-the-example", "page": "examples/stream-markdown/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "heading_level": 2, "md_text": "[stream\\_markdown.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/stream_markdown.py)", "url": "https://ai.pydantic.dev/examples/stream-markdown/index.html#example-code", "page": "examples/stream-markdown/index.html", "source_site": "pydantic_ai"}
{"title": "SQL Generation", "anchor": "sql-generation", "heading_level": 1, "md_text": "Example demonstrating how to use Pydantic AI to generate SQL queries based on user input. Demonstrates: * [dynamic system prompt](../../agents/index.html#system-prompts) * [structured output_type ](../../output/index.html#structured-output) * [output validation](../../output/index.html#output-validator-functions) * [agent dependencies](../../dependencies/index.html) ## Running the Example The resulting SQL is validated by running it as an EXPLAIN query on PostgreSQL. To run the example, you first need to run PostgreSQL, e.g. via Docker: *(we run postgres on port 54320 to avoid conflicts with any other postgres instances you may have running)* With [dependencies installed and environment variables set](../setup/index.html#usage), run: pipuv or to use a custom prompt: pipuv This model uses gemini-1.5-flash by default since Gemini is good at single shot queries of this kind. ## Example Code [sql\\_gen.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/sql_gen.py)", "url": "https://ai.pydantic.dev/examples/sql-gen/index.html#sql-generation", "page": "examples/sql-gen/index.html", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "heading_level": 2, "md_text": "The resulting SQL is validated by running it as an EXPLAIN query on PostgreSQL. To run the example, you first need to run PostgreSQL, e.g. via Docker: *(we run postgres on port 54320 to avoid conflicts with any other postgres instances you may have running)* With [dependencies installed and environment variables set](../setup/index.html#usage), run: pipuv or to use a custom prompt: pipuv This model uses gemini-1.5-flash by default since Gemini is good at single shot queries of this kind.", "url": "https://ai.pydantic.dev/examples/sql-gen/index.html#running-the-example", "page": "examples/sql-gen/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "heading_level": 2, "md_text": "[sql\\_gen.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/sql_gen.py)", "url": "https://ai.pydantic.dev/examples/sql-gen/index.html#example-code", "page": "examples/sql-gen/index.html", "source_site": "pydantic_ai"}
{"title": "RAG", "anchor": "rag", "heading_level": 1, "md_text": "RAG search example. This demo allows you to ask question of the [logfire](https://pydantic.dev/logfire) documentation. Demonstrates: * [tools](../../tools/index.html) * [agent dependencies](../../dependencies/index.html) * RAG search This is done by creating a database containing each section of the markdown documentation, then registering the search tool with the Pydantic AI agent. Logic for extracting sections from markdown files and a JSON file with that data is available in [this gist](https://gist.github.com/samuelcolvin/4b5bb9bb163b1122ff17e29e48c10992). [PostgreSQL with pgvector](https://github.com/pgvector/pgvector) is used as the search database, the easiest way to download and run pgvector is using Docker: As with the [SQL gen](../sql-gen/index.html) example, we run postgres on port 54320 to avoid conflicts with any other postgres instances you may have running. We also mount the PostgreSQL data directory locally to persist the data if you need to stop and restart the container. With that running and [dependencies installed and environment variables set](../setup/index.html#usage), we can build the search database with (**WARNING**: this requires the OPENAI_API_KEY env variable and will calling the OpenAI embedding API around 300 times to generate embeddings for each section of the documentation): pipuv (Note building the database doesn't use Pydantic AI right now, instead it uses the OpenAI SDK directly.) You can then ask the agent a question with: pipuv ## Example Code [rag.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/rag.py)", "url": "https://ai.pydantic.dev/examples/rag/index.html#rag", "page": "examples/rag/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "heading_level": 2, "md_text": "[rag.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/rag.py)", "url": "https://ai.pydantic.dev/examples/rag/index.html#example-code", "page": "examples/rag/index.html", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "heading_level": 2, "md_text": "With [dependencies installed and environment variables set](../setup/index.html#usage), run: pipuv Should give an output like this:", "url": "https://ai.pydantic.dev/examples/stream-whales/index.html#running-the-example", "page": "examples/stream-whales/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "heading_level": 2, "md_text": "[stream\\_whales.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/stream_whales.py)", "url": "https://ai.pydantic.dev/examples/stream-whales/index.html#example-code", "page": "examples/stream-whales/index.html", "source_site": "pydantic_ai"}
{"title": "Chat App with FastAPI", "anchor": "chat-app-with-fastapi", "heading_level": 1, "md_text": "Simple chat app example build with FastAPI. Demonstrates: * [reusing chat history](../../message-history/index.html) * [serializing messages](../../message-history/index.html#accessing-messages-from-results) * [streaming responses](../../output/index.html#streamed-results) This demonstrates storing chat history between requests and using it to give the model context for new responses. Most of the complex logic here is between chat_app.py which streams the response to the browser, and chat_app.ts which renders messages in the browser. ## Running the Example With [dependencies installed and environment variables set](../setup/index.html#usage), run: pipuv Then open the app at [localhost:8000](http://localhost:8000). [![Example conversation](../../img/chat-app-example.png)](../../img/chat-app-example.png) ## Example Code Python code that runs the chat app: [chat\\_app.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/chat_app.py) Simple HTML page to render the app: [chat\\_app.html](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/chat_app.html) TypeScript to handle rendering the messages, to keep this simple (and at the risk of offending frontend developers) the typescript code is passed to the browser as plain text and transpiled in the browser. [chat\\_app.ts](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/chat_app.ts)", "url": "https://ai.pydantic.dev/examples/chat-app/index.html#chat-app-with-fastapi", "page": "examples/chat-app/index.html", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "heading_level": 2, "md_text": "With [dependencies installed and environment variables set](../setup/index.html#usage), run: pipuv Then open the app at [localhost:8000](http://localhost:8000). [![Example conversation](../../img/chat-app-example.png)](../../img/chat-app-example.png)", "url": "https://ai.pydantic.dev/examples/chat-app/index.html#running-the-example", "page": "examples/chat-app/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "heading_level": 2, "md_text": "Python code that runs the chat app: [chat\\_app.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/chat_app.py) Simple HTML page to render the app: [chat\\_app.html](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/chat_app.html) TypeScript to handle rendering the messages, to keep this simple (and at the risk of offending frontend developers) the typescript code is passed to the browser as plain text and transpiled in the browser. [chat\\_app.ts](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/chat_app.ts)", "url": "https://ai.pydantic.dev/examples/chat-app/index.html#example-code", "page": "examples/chat-app/index.html", "source_site": "pydantic_ai"}
{"title": "Running the Example", "anchor": "running-the-example", "heading_level": 2, "md_text": "To run this example properly, you might want to add two extra API keys **(Note if either key is missing, the code will fall back to dummy data, so they're not required)**: * A weather API key from [tomorrow.io](https://www.tomorrow.io/weather-api/) set via WEATHER_API_KEY * A geocoding API key from [geocode.maps.co](https://geocode.maps.co/) set via GEO_API_KEY With [dependencies installed and environment variables set](../setup/index.html#usage), run: pipuv", "url": "https://ai.pydantic.dev/examples/weather-agent/index.html#running-the-example", "page": "examples/weather-agent/index.html", "source_site": "pydantic_ai"}
{"title": "Example Code", "anchor": "example-code", "heading_level": 2, "md_text": "[weather\\_agent.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/weather_agent.py)", "url": "https://ai.pydantic.dev/examples/weather-agent/index.html#example-code", "page": "examples/weather-agent/index.html", "source_site": "pydantic_ai"}
{"title": "Running the UI", "anchor": "running-the-ui", "heading_level": 2, "md_text": "You can build multi-turn chat applications for your agent with [Gradio](https://www.gradio.app/), a framework for building AI web applications entirely in python. Gradio comes with built-in chat components and agent support so the entire UI will be implemented in a single python file! Here's what the UI looks like for the weather agent:", "url": "https://ai.pydantic.dev/examples/weather-agent/index.html#running-the-ui", "page": "examples/weather-agent/index.html", "source_site": "pydantic_ai"}
{"title": "UI Code", "anchor": "ui-code", "heading_level": 2, "md_text": "[weather\\_agent\\_gradio.py](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/weather_agent_gradio.py)", "url": "https://ai.pydantic.dev/examples/weather-agent/index.html#ui-code", "page": "examples/weather-agent/index.html", "source_site": "pydantic_ai"}
{"title": "Slack Lead Qualifier with Modal", "anchor": "slack-lead-qualifier-with-modal", "heading_level": 1, "md_text": "In this example, we're going to build an agentic app that: * automatically researches each new member that joins a company's public Slack community to see how good of a fit they are for the company's commercial product, * sends this analysis into a (private) Slack channel, and * sends a daily summary of the top 5 leads from the previous 24 hours into a (different) Slack channel. We'll be deploying the app on [Modal](https://modal.com), as it lets you use Python to define an app with web endpoints, scheduled functions, and background functions, and deploy them with a CLI, without needing to set up or manage any infrastructure. It's a great way to lower the barrier for people in your organization to start building and deploying AI agents to make their jobs easier. We also add [Pydantic Logfire](https://pydantic.dev/logfire) to get observability into the app and agent as they're running in response to webhooks and the schedule ## Screenshots This is what the analysis sent into Slack will look like: [![Slack message](../../img/slack-lead-qualifier-slack.png)](../../img/slack-lead-qualifier-slack.png) This is what the corresponding trace in [Logfire](https://pydantic.dev/logfire) will look like: [![Logfire trace](../../img/slack-lead-qualifier-logfire.png)](../../img/slack-lead-qualifier-logfire.png) All of these entries can be clicked on to get more details about what happened at that step, including the full conversation with the LLM and HTTP requests and responses. ## Prerequisites If you just want to see the code without actually going through the effort of setting up the bits necessary to run it, feel free to [jump ahead](index.html#the-code). ### Slack app You need to have a Slack workspace and the necessary permissions to create apps. 2. Create a new Slack app using the instructions at <https://docs.slack.dev/quickstart>. 1. In step 2, \"Requesting scopes\", request the following scopes: * [ users.read ](https://docs.slack.dev/reference/scopes/users.read) * [ users.read.email ](https://docs.slack.dev/reference/scopes/users.read.email) * [ users.profile.read ](https://docs.slack.dev/reference/scopes/users.profile.read) 2. In step 3, \"Installing and authorizing the app\", note down the Access Token as we're going to need to store it as a Secret in Modal. 3. You can skip steps 4 and 5. We're going to need to subscribe to the team_join event, but at this point you don't have a webhook URL yet. 3. Create the channels the app will post into, and add the Slack app to them: * #new-slack-leads * #daily-slack-leads-summary These names are hard-coded in the example. If you want to use different channels, you can clone the repo and change them in examples/pydantic_examples/slack_lead_qualifier/functions.py . ### Logfire Write Token 1. If you don't have a Logfire account yet, create one on <https://logfire-us.pydantic.dev/>. 2. Create a new project named, for example, slack-lead-qualifier . 3. Generate a new Write Token and note it down, as we're going to need to store it as a Secret in Modal. ### OpenAI API Key 1. If you don't have an OpenAI account yet, create one on <https://platform.openai.com/>. 2. Create a new API Key in Settings and note it down, as we're going to need to store it as a Secret in Modal. ### Modal account 1. If you don't have a Modal account yet, create one on <https://modal.com/signup>. 2. Create 3 Secrets of type \"Custom\" on <https://modal.com/secrets>: * Name: slack , key: SLACK_API_KEY , value: the Slack Access Token you generated earlier * Name: logfire , key: LOGFIRE_TOKEN , value: the Logfire Write Token you generated earlier * Name: openai , key: OPENAI_API_KEY , value: the OpenAI API Key you generated earlier ## Usage 1. Make sure you have the [dependencies installed](../setup/index.html#usage). 2. Authenticate with Modal: 3. Run the example as an [ephemeral Modal app](https://modal.com/docs/guide/apps#ephemeral-apps), meaning it will only run until you quit it using Ctrl+C: 4. Note down the URL after Created web function web_app => , this is your webhook endpoint URL. 5. Go back to <https://docs.slack.dev/quickstart> and follow step 4, \"Configuring the app for event listening\", to subscribe to the team_join event with the webhook endpoint URL you noted down as the Request URL. Now when someone new (possibly you with a throwaway email) joins the Slack workspace, you'll see the webhook event being processed in the terminal where you ran modal serve and in the Logfire Live view, and after waiting a few seconds you should see the result appear in the #new-slack-leads Slack channel! Faking a Slack signup You can also fake a Slack signup event and try out the agent like this, with any name or email you please: Deploying to production If you'd like to deploy this app into your Modal workspace in a persistent fashion, you can use this command: You'll likely want to [download the code](https://github.com/pydantic/pydantic-ai/tree/main/examples/pydantic_ai_examples/slack_lead_qualifier) first, put it in a new repo, and then do [continuous deployment](https://modal.com/docs/guide/continuous-deployment#github-actions) using GitHub Actions. Don't forget to update the Slack event request URL to the new persistent URL! You'll also want to modify the [instructions for the agent](index.html#agent) to your own situation. ## The code We're going to start with the basics, and then gradually build up into the full app. ### Models #### Profile First, we define a [Pydantic](https://docs.pydantic.dev) model that represents a Slack user profile. These are the fields we get from the [ team_join ](https://docs.slack.dev/reference/events/team_join) event that's sent to the webhook endpoint that we'll define in a bit. [slack\\_lead\\_qualifier/models.py (L11-L15)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/models.py#L11-L15) We also define a Profile.as_prompt() helper method that uses [ format_as_xml ](../../api/format_prompt/index.html#pydantic_ai.format_prompt.format_as_xml) to turn the profile into a string that can be sent to the model. [slack\\_lead\\_qualifier/models.py (L7-L19)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/models.py#L7-L19) #### Analysis The second model we'll need represents the result of the analysis that the agent will perform. We include docstrings to provide additional context to the model on what these fields should contain. [slack\\_lead\\_qualifier/models.py (L23-L31)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/models.py#L23-L31) We also define a Analysis.as_slack_blocks() helper method that turns the analysis into some [Slack blocks](https://api.slack.com/reference/block-kit/blocks) that can be sent to the Slack API to post a new message. [slack\\_lead\\_qualifier/models.py (L23-L46)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/models.py#L23-L46) ### Agent Now it's time to get into Pydantic AI and define the agent that will do the actual analysis! We specify the model we'll use ( openai:gpt-4o ), provide [instructions](../../agents/index.html#instructions), give the agent access to the [DuckDuckGo search tool](../../common-tools/index.html#duckduckgo-search-tool), and tell it to output either an Analysis or None using the [Native Output](../../output/index.html#native-output) structured output mode. The real meat of the app is in the instructions that tell the agent how to evaluate each new Slack member. If you plan to use this app yourself, you'll of course want to modify them to your own situation. [slack\\_lead\\_qualifier/agent.py (L7-L40)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/agent.py#L7-L40) #### analyze_profile We also define a analyze_profile helper function that takes a Profile , runs the agent, and returns an Analysis (or None ), and instrument it using [Logfire](../../logfire/index.html). [slack\\_lead\\_qualifier/agent.py (L44-L47)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/agent.py#L44-L47) ### Analysis store The next building block we'll need is a place to store all the analyses that have been done so that we can look them up when we send the daily summary. Fortunately, Modal provides us with a convenient way to store some data that can be read back in a subsequent Modal run (webhook or scheduled): [ modal.Dict ](https://modal.com/docs/reference/modal.Dict). We define some convenience methods to easily add, list, and clear analyses. [slack\\_lead\\_qualifier/store.py (L4-L31)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/store.py#L4-L31) Note Note that # type: ignore on the last line -- unfortunately modal does not fully define its types, so we need this to stop our static type checker pyright , which we run over all Pydantic AI code including examples, from complaining. ### Send Slack message Next, we'll need a way to actually send a Slack message, so we define a simple function that uses Slack's [ chat.postMessage ](https://api.slack.com/methods/chat.postMessage) API. [slack\\_lead\\_qualifier/slack.py (L8-L30)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/slack.py#L8-L30) ### Features Now we can start putting these building blocks together to implement the actual features we want! #### process_slack_member This function takes a [ Profile ](index.html#profile), [analyzes](index.html#analyze_profile) it using the agent, adds it to the [ AnalysisStore ](index.html#analysis-store), and [sends](index.html#send-slack-message) the analysis into the #new-slack-leads channel. [slack\\_lead\\_qualifier/functions.py (L4-L45)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/functions.py#L4-L45) #### send_daily_summary This function list all of the analyses in the [ AnalysisStore ](index.html#analysis-store), takes the top 5 by relevance, [sends](index.html#send-slack-message) them into the #daily-slack-leads-summary channel, and clears the AnalysisStore so that the next daily run won't process these analyses again. [slack\\_lead\\_qualifier/functions.py (L8-L85)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/functions.py#L8-L85) ### Web app As it stands, neither of these functions are actually being called from anywhere. Let's implement a [FastAPI](https://fastapi.tiangolo.com/) endpoint to handle the team_join Slack webhook (also known as the [Slack Events API](https://docs.slack.dev/apis/events-api)) and call the [ process_slack_member ](index.html#process_slack_member) function we just defined. We also instrument FastAPI using Logfire for good measure. [slack\\_lead\\_qualifier/app.py (L20-L36)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/app.py#L20-L36) #### process_slack_member with Modal I was a little sneaky there -- we're not actually calling the [ process_slack_member ](index.html#process_slack_member) function we defined in functions.py directly, as Slack requires webhooks to respond within 3 seconds, and we need a bit more time than that to talk to the LLM, do some web searches, and send the Slack message. Instead, we're calling the following function defined alongside the app, which uses Modal's [ modal.Function.spawn ](https://modal.com/docs/reference/modal.Function#spawn) feature to run a function in the background. (If you're curious what the Modal side of this function looks like, you can [jump ahead](index.html#backgrounded-process_slack_member).) Because modal.py (which we'll see in the next section) imports app.py , we import from modal.py inside the function definition because doing so at the top level would have resulted in a circular import error. We also pass along the current Logfire context to get [Distributed Tracing](https://logfire.pydantic.dev/docs/how-to-guides/distributed-tracing/), meaning that the background function execution will show up nested under the webhook request trace, so that we have everything related to that request in one place. [slack\\_lead\\_qualifier/app.py (L11-L16)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/app.py#L11-L16) ### Modal app Now let's see how easy Modal makes it to deploy all of this. #### Set up Modal The first thing we do is define the Modal app, by specifying the base image to use (Debian with Python 3.13), all the Python packages it needs, and all of the secrets defined in the Modal interface that need to be made available during runtime. [slack\\_lead\\_qualifier/modal.py (L4-L21)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/modal.py#L4-L21) #### Set up Logfire Next, we define a function to set up Logfire instrumentation for Pydantic AI and HTTPX. We cannot do this at the top level of the file, as the requested packages (like logfire ) will only be available within functions running on Modal (like the ones we'll define next). This file, modal.py , runs on your local machine and only has access to the modal package. [slack\\_lead\\_qualifier/modal.py (L25-L30)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/modal.py#L25-L30) #### Web app To deploy a [web endpoint](https://modal.com/docs/guide/webhooks) on Modal, we simply define a function that returns an ASGI app (like FastAPI) and decorate it with @app.function() and @modal.asgi_app() . This web_app function will be run on Modal, so inside the function we can call the setup_logfire function that requires the logfire package, and import app.py which uses the other requested packages. By default, Modal spins up a container to handle a function call (like a web request) on-demand, meaning there's a little bit of startup time to each request. However, Slack requires webhooks to respond within 3 seconds, so we specify min_containers=1 to keep the web endpoint running and ready to answer requests at all times. This is a bit annoying and wasteful, but fortunately [Modal's pricing](https://modal.com/pricing) is pretty reasonable, you get $30 free monthly compute, and they offer up to $50k in free credits for startup and academic researchers. [slack\\_lead\\_qualifier/modal.py (L34-L41)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/modal.py#L34-L41) Note Note that # type: ignore on the @modal.asgi_app() line -- unfortunately modal does not fully define its types, so we need this to stop our static type checker pyright , which we run over all Pydantic AI code including examples, from complaining. #### Scheduled send_daily_summary To define a [scheduled function](https://modal.com/docs/guide/cron), we can use the @app.function() decorator with a schedule argument. This Modal function will call our imported [ send_daily_summary ](index.html#send_daily_summary) function every day at 8 am UTC. [slack\\_lead\\_qualifier/modal.py (L60-L66)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/modal.py#L60-L66) #### Backgrounded process_slack_member Finally, we define a Modal function that wraps our [ process_slack_member ](index.html#process_slack_member) function, so that it can run in the background. As you'll remember from when we [spawned this function from the web app](index.html#process_slack_member-with-modal), we passed along the Logfire context to get [Distributed Tracing](https://logfire.pydantic.dev/docs/how-to-guides/distributed-tracing/), so we need to attach it here. [slack\\_lead\\_qualifier/modal.py (L45-L56)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/modal.py#L45-L56) ## Conclusion And that's it! Now, assuming you've met the [prerequisites](index.html#prerequisites), you can run or deploy the app using the commands under [usage](index.html#usage).", "url": "https://ai.pydantic.dev/examples/slack-lead-qualifier/index.html#slack-lead-qualifier-with-modal", "page": "examples/slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Screenshots", "anchor": "screenshots", "heading_level": 2, "md_text": "This is what the analysis sent into Slack will look like: [![Slack message](../../img/slack-lead-qualifier-slack.png)](../../img/slack-lead-qualifier-slack.png) This is what the corresponding trace in [Logfire](https://pydantic.dev/logfire) will look like: [![Logfire trace](../../img/slack-lead-qualifier-logfire.png)](../../img/slack-lead-qualifier-logfire.png) All of these entries can be clicked on to get more details about what happened at that step, including the full conversation with the LLM and HTTP requests and responses.", "url": "https://ai.pydantic.dev/examples/slack-lead-qualifier/index.html#screenshots", "page": "examples/slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Prerequisites", "anchor": "prerequisites", "heading_level": 2, "md_text": "If you just want to see the code without actually going through the effort of setting up the bits necessary to run it, feel free to [jump ahead](index.html#the-code). ### Slack app You need to have a Slack workspace and the necessary permissions to create apps. 2. Create a new Slack app using the instructions at <https://docs.slack.dev/quickstart>. 1. In step 2, \"Requesting scopes\", request the following scopes: * [ users.read ](https://docs.slack.dev/reference/scopes/users.read) * [ users.read.email ](https://docs.slack.dev/reference/scopes/users.read.email) * [ users.profile.read ](https://docs.slack.dev/reference/scopes/users.profile.read) 2. In step 3, \"Installing and authorizing the app\", note down the Access Token as we're going to need to store it as a Secret in Modal. 3. You can skip steps 4 and 5. We're going to need to subscribe to the team_join event, but at this point you don't have a webhook URL yet. 3. Create the channels the app will post into, and add the Slack app to them: * #new-slack-leads * #daily-slack-leads-summary These names are hard-coded in the example. If you want to use different channels, you can clone the repo and change them in examples/pydantic_examples/slack_lead_qualifier/functions.py . ### Logfire Write Token 1. If you don't have a Logfire account yet, create one on <https://logfire-us.pydantic.dev/>. 2. Create a new project named, for example, slack-lead-qualifier . 3. Generate a new Write Token and note it down, as we're going to need to store it as a Secret in Modal. ### OpenAI API Key 1. If you don't have an OpenAI account yet, create one on <https://platform.openai.com/>. 2. Create a new API Key in Settings and note it down, as we're going to need to store it as a Secret in Modal. ### Modal account 1. If you don't have a Modal account yet, create one on <https://modal.com/signup>. 2. Create 3 Secrets of type \"Custom\" on <https://modal.com/secrets>: * Name: slack , key: SLACK_API_KEY , value: the Slack Access Token you generated earlier * Name: logfire , key: LOGFIRE_TOKEN , value: the Logfire Write Token you generated earlier * Name: openai , key: OPENAI_API_KEY , value: the OpenAI API Key you generated earlier", "url": "https://ai.pydantic.dev/examples/slack-lead-qualifier/index.html#prerequisites", "page": "examples/slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Slack app", "anchor": "slack-app", "heading_level": 3, "md_text": "You need to have a Slack workspace and the necessary permissions to create apps. 2. Create a new Slack app using the instructions at <https://docs.slack.dev/quickstart>. 1. In step 2, \"Requesting scopes\", request the following scopes: * [ users.read ](https://docs.slack.dev/reference/scopes/users.read) * [ users.read.email ](https://docs.slack.dev/reference/scopes/users.read.email) * [ users.profile.read ](https://docs.slack.dev/reference/scopes/users.profile.read) 2. In step 3, \"Installing and authorizing the app\", note down the Access Token as we're going to need to store it as a Secret in Modal. 3. You can skip steps 4 and 5. We're going to need to subscribe to the team_join event, but at this point you don't have a webhook URL yet. 3. Create the channels the app will post into, and add the Slack app to them: * #new-slack-leads * #daily-slack-leads-summary These names are hard-coded in the example. If you want to use different channels, you can clone the repo and change them in examples/pydantic_examples/slack_lead_qualifier/functions.py .", "url": "https://ai.pydantic.dev/examples/slack-lead-qualifier/index.html#slack-app", "page": "examples/slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Logfire Write Token", "anchor": "logfire-write-token", "heading_level": 3, "md_text": "1. If you don't have a Logfire account yet, create one on <https://logfire-us.pydantic.dev/>. 2. Create a new project named, for example, slack-lead-qualifier . 3. Generate a new Write Token and note it down, as we're going to need to store it as a Secret in Modal.", "url": "https://ai.pydantic.dev/examples/slack-lead-qualifier/index.html#logfire-write-token", "page": "examples/slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAI API Key", "anchor": "openai-api-key", "heading_level": 3, "md_text": "1. If you don't have an OpenAI account yet, create one on <https://platform.openai.com/>. 2. Create a new API Key in Settings and note it down, as we're going to need to store it as a Secret in Modal.", "url": "https://ai.pydantic.dev/examples/slack-lead-qualifier/index.html#openai-api-key", "page": "examples/slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Modal account", "anchor": "modal-account", "heading_level": 3, "md_text": "1. If you don't have a Modal account yet, create one on <https://modal.com/signup>. 2. Create 3 Secrets of type \"Custom\" on <https://modal.com/secrets>: * Name: slack , key: SLACK_API_KEY , value: the Slack Access Token you generated earlier * Name: logfire , key: LOGFIRE_TOKEN , value: the Logfire Write Token you generated earlier * Name: openai , key: OPENAI_API_KEY , value: the OpenAI API Key you generated earlier", "url": "https://ai.pydantic.dev/examples/slack-lead-qualifier/index.html#modal-account", "page": "examples/slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 2, "md_text": "1. Make sure you have the [dependencies installed](../setup/index.html#usage). 2. Authenticate with Modal: 3. Run the example as an [ephemeral Modal app](https://modal.com/docs/guide/apps#ephemeral-apps), meaning it will only run until you quit it using Ctrl+C: 4. Note down the URL after Created web function web_app => , this is your webhook endpoint URL. 5. Go back to <https://docs.slack.dev/quickstart> and follow step 4, \"Configuring the app for event listening\", to subscribe to the team_join event with the webhook endpoint URL you noted down as the Request URL. Now when someone new (possibly you with a throwaway email) joins the Slack workspace, you'll see the webhook event being processed in the terminal where you ran modal serve and in the Logfire Live view, and after waiting a few seconds you should see the result appear in the #new-slack-leads Slack channel! Faking a Slack signup You can also fake a Slack signup event and try out the agent like this, with any name or email you please: Deploying to production If you'd like to deploy this app into your Modal workspace in a persistent fashion, you can use this command: You'll likely want to [download the code](https://github.com/pydantic/pydantic-ai/tree/main/examples/pydantic_ai_examples/slack_lead_qualifier) first, put it in a new repo, and then do [continuous deployment](https://modal.com/docs/guide/continuous-deployment#github-actions) using GitHub Actions. Don't forget to update the Slack event request URL to the new persistent URL! You'll also want to modify the [instructions for the agent](index.html#agent) to your own situation.", "url": "https://ai.pydantic.dev/examples/slack-lead-qualifier/index.html#usage", "page": "examples/slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "The code", "anchor": "the-code", "heading_level": 2, "md_text": "We're going to start with the basics, and then gradually build up into the full app. ### Models #### Profile First, we define a [Pydantic](https://docs.pydantic.dev) model that represents a Slack user profile. These are the fields we get from the [ team_join ](https://docs.slack.dev/reference/events/team_join) event that's sent to the webhook endpoint that we'll define in a bit. [slack\\_lead\\_qualifier/models.py (L11-L15)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/models.py#L11-L15) We also define a Profile.as_prompt() helper method that uses [ format_as_xml ](../../api/format_prompt/index.html#pydantic_ai.format_prompt.format_as_xml) to turn the profile into a string that can be sent to the model. [slack\\_lead\\_qualifier/models.py (L7-L19)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/models.py#L7-L19) #### Analysis The second model we'll need represents the result of the analysis that the agent will perform. We include docstrings to provide additional context to the model on what these fields should contain. [slack\\_lead\\_qualifier/models.py (L23-L31)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/models.py#L23-L31) We also define a Analysis.as_slack_blocks() helper method that turns the analysis into some [Slack blocks](https://api.slack.com/reference/block-kit/blocks) that can be sent to the Slack API to post a new message. [slack\\_lead\\_qualifier/models.py (L23-L46)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/models.py#L23-L46) ### Agent Now it's time to get into Pydantic AI and define the agent that will do the actual analysis! We specify the model we'll use ( openai:gpt-4o ), provide [instructions](../../agents/index.html#instructions), give the agent access to the [DuckDuckGo search tool](../../common-tools/index.html#duckduckgo-search-tool), and tell it to output either an Analysis or None using the [Native Output](../../output/index.html#native-output) structured output mode. The real meat of the app is in the instructions that tell the agent how to evaluate each new Slack member. If you plan to use this app yourself, you'll of course want to modify them to your own situation. [slack\\_lead\\_qualifier/agent.py (L7-L40)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/agent.py#L7-L40) #### analyze_profile We also define a analyze_profile helper function that takes a Profile , runs the agent, and returns an Analysis (or None ), and instrument it using [Logfire](../../logfire/index.html). [slack\\_lead\\_qualifier/agent.py (L44-L47)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/agent.py#L44-L47) ### Analysis store The next building block we'll need is a place to store all the analyses that have been done so that we can look them up when we send the daily summary. Fortunately, Modal provides us with a convenient way to store some data that can be read back in a subsequent Modal run (webhook or scheduled): [ modal.Dict ](https://modal.com/docs/reference/modal.Dict). We define some convenience methods to easily add, list, and clear analyses. [slack\\_lead\\_qualifier/store.py (L4-L31)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/store.py#L4-L31) Note Note that # type: ignore on the last line -- unfortunately modal does not fully define its types, so we need this to stop our static type checker pyright , which we run over all Pydantic AI code including examples, from complaining. ### Send Slack message Next, we'll need a way to actually send a Slack message, so we define a simple function that uses Slack's [ chat.postMessage ](https://api.slack.com/methods/chat.postMessage) API. [slack\\_lead\\_qualifier/slack.py (L8-L30)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/slack.py#L8-L30) ### Features Now we can start putting these building blocks together to implement the actual features we want! #### process_slack_member This function takes a [ Profile ](index.html#profile), [analyzes](index.html#analyze_profile) it using the agent, adds it to the [ AnalysisStore ](index.html#analysis-store), and [sends](index.html#send-slack-message) the analysis into the #new-slack-leads channel. [slack\\_lead\\_qualifier/functions.py (L4-L45)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/functions.py#L4-L45) #### send_daily_summary This function list all of the analyses in the [ AnalysisStore ](index.html#analysis-store), takes the top 5 by relevance, [sends](index.html#send-slack-message) them into the #daily-slack-leads-summary channel, and clears the AnalysisStore so that the next daily run won't process these analyses again. [slack\\_lead\\_qualifier/functions.py (L8-L85)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/functions.py#L8-L85) ### Web app As it stands, neither of these functions are actually being called from anywhere. Let's implement a [FastAPI](https://fastapi.tiangolo.com/) endpoint to handle the team_join Slack webhook (also known as the [Slack Events API](https://docs.slack.dev/apis/events-api)) and call the [ process_slack_member ](index.html#process_slack_member) function we just defined. We also instrument FastAPI using Logfire for good measure. [slack\\_lead\\_qualifier/app.py (L20-L36)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/app.py#L20-L36) #### process_slack_member with Modal I was a little sneaky there -- we're not actually calling the [ process_slack_member ](index.html#process_slack_member) function we defined in functions.py directly, as Slack requires webhooks to respond within 3 seconds, and we need a bit more time than that to talk to the LLM, do some web searches, and send the Slack message. Instead, we're calling the following function defined alongside the app, which uses Modal's [ modal.Function.spawn ](https://modal.com/docs/reference/modal.Function#spawn) feature to run a function in the background. (If you're curious what the Modal side of this function looks like, you can [jump ahead](index.html#backgrounded-process_slack_member).) Because modal.py (which we'll see in the next section) imports app.py , we import from modal.py inside the function definition because doing so at the top level would have resulted in a circular import error. We also pass along the current Logfire context to get [Distributed Tracing](https://logfire.pydantic.dev/docs/how-to-guides/distributed-tracing/), meaning that the background function execution will show up nested under the webhook request trace, so that we have everything related to that request in one place. [slack\\_lead\\_qualifier/app.py (L11-L16)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/app.py#L11-L16) ### Modal app Now let's see how easy Modal makes it to deploy all of this. #### Set up Modal The first thing we do is define the Modal app, by specifying the base image to use (Debian with Python 3.13), all the Python packages it needs, and all of the secrets defined in the Modal interface that need to be made available during runtime. [slack\\_lead\\_qualifier/modal.py (L4-L21)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/modal.py#L4-L21) #### Set up Logfire Next, we define a function to set up Logfire instrumentation for Pydantic AI and HTTPX. We cannot do this at the top level of the file, as the requested packages (like logfire ) will only be available within functions running on Modal (like the ones we'll define next). This file, modal.py , runs on your local machine and only has access to the modal package. [slack\\_lead\\_qualifier/modal.py (L25-L30)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/modal.py#L25-L30) #### Web app To deploy a [web endpoint](https://modal.com/docs/guide/webhooks) on Modal, we simply define a function that returns an ASGI app (like FastAPI) and decorate it with @app.function() and @modal.asgi_app() . This web_app function will be run on Modal, so inside the function we can call the setup_logfire function that requires the logfire package, and import app.py which uses the other requested packages. By default, Modal spins up a container to handle a function call (like a web request) on-demand, meaning there's a little bit of startup time to each request. However, Slack requires webhooks to respond within 3 seconds, so we specify min_containers=1 to keep the web endpoint running and ready to answer requests at all times. This is a bit annoying and wasteful, but fortunately [Modal's pricing](https://modal.com/pricing) is pretty reasonable, you get $30 free monthly compute, and they offer up to $50k in free credits for startup and academic researchers. [slack\\_lead\\_qualifier/modal.py (L34-L41)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/modal.py#L34-L41) Note Note that # type: ignore on the @modal.asgi_app() line -- unfortunately modal does not fully define its types, so we need this to stop our static type checker pyright , which we run over all Pydantic AI code including examples, from complaining. #### Scheduled send_daily_summary To define a [scheduled function](https://modal.com/docs/guide/cron), we can use the @app.function() decorator with a schedule argument. This Modal function will call our imported [ send_daily_summary ](index.html#send_daily_summary) function every day at 8 am UTC. [slack\\_lead\\_qualifier/modal.py (L60-L66)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/modal.py#L60-L66) #### Backgrounded process_slack_member Finally, we define a Modal function that wraps our [ process_slack_member ](index.html#process_slack_member) function, so that it can run in the background. As you'll remember from when we [spawned this function from the web app](index.html#process_slack_member-with-modal), we passed along the Logfire context to get [Distributed Tracing](https://logfire.pydantic.dev/docs/how-to-guides/distributed-tracing/), so we need to attach it here. [slack\\_lead\\_qualifier/modal.py (L45-L56)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/modal.py#L45-L56)", "url": "https://ai.pydantic.dev/examples/slack-lead-qualifier/index.html#the-code", "page": "examples/slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Models", "anchor": "models", "heading_level": 3, "md_text": "#### Profile First, we define a [Pydantic](https://docs.pydantic.dev) model that represents a Slack user profile. These are the fields we get from the [ team_join ](https://docs.slack.dev/reference/events/team_join) event that's sent to the webhook endpoint that we'll define in a bit. [slack\\_lead\\_qualifier/models.py (L11-L15)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/models.py#L11-L15) We also define a Profile.as_prompt() helper method that uses [ format_as_xml ](../../api/format_prompt/index.html#pydantic_ai.format_prompt.format_as_xml) to turn the profile into a string that can be sent to the model. [slack\\_lead\\_qualifier/models.py (L7-L19)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/models.py#L7-L19) #### Analysis The second model we'll need represents the result of the analysis that the agent will perform. We include docstrings to provide additional context to the model on what these fields should contain. [slack\\_lead\\_qualifier/models.py (L23-L31)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/models.py#L23-L31) We also define a Analysis.as_slack_blocks() helper method that turns the analysis into some [Slack blocks](https://api.slack.com/reference/block-kit/blocks) that can be sent to the Slack API to post a new message. [slack\\_lead\\_qualifier/models.py (L23-L46)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/models.py#L23-L46)", "url": "https://ai.pydantic.dev/examples/slack-lead-qualifier/index.html#models", "page": "examples/slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Agent", "anchor": "agent", "heading_level": 3, "md_text": "Now it's time to get into Pydantic AI and define the agent that will do the actual analysis! We specify the model we'll use ( openai:gpt-4o ), provide [instructions](../../agents/index.html#instructions), give the agent access to the [DuckDuckGo search tool](../../common-tools/index.html#duckduckgo-search-tool), and tell it to output either an Analysis or None using the [Native Output](../../output/index.html#native-output) structured output mode. The real meat of the app is in the instructions that tell the agent how to evaluate each new Slack member. If you plan to use this app yourself, you'll of course want to modify them to your own situation. [slack\\_lead\\_qualifier/agent.py (L7-L40)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/agent.py#L7-L40) #### analyze_profile We also define a analyze_profile helper function that takes a Profile , runs the agent, and returns an Analysis (or None ), and instrument it using [Logfire](../../logfire/index.html). [slack\\_lead\\_qualifier/agent.py (L44-L47)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/agent.py#L44-L47)", "url": "https://ai.pydantic.dev/examples/slack-lead-qualifier/index.html#agent", "page": "examples/slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Analysis store", "anchor": "analysis-store", "heading_level": 3, "md_text": "The next building block we'll need is a place to store all the analyses that have been done so that we can look them up when we send the daily summary. Fortunately, Modal provides us with a convenient way to store some data that can be read back in a subsequent Modal run (webhook or scheduled): [ modal.Dict ](https://modal.com/docs/reference/modal.Dict). We define some convenience methods to easily add, list, and clear analyses. [slack\\_lead\\_qualifier/store.py (L4-L31)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/store.py#L4-L31) Note Note that # type: ignore on the last line -- unfortunately modal does not fully define its types, so we need this to stop our static type checker pyright , which we run over all Pydantic AI code including examples, from complaining.", "url": "https://ai.pydantic.dev/examples/slack-lead-qualifier/index.html#analysis-store", "page": "examples/slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Send Slack message", "anchor": "send-slack-message", "heading_level": 3, "md_text": "Next, we'll need a way to actually send a Slack message, so we define a simple function that uses Slack's [ chat.postMessage ](https://api.slack.com/methods/chat.postMessage) API. [slack\\_lead\\_qualifier/slack.py (L8-L30)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/slack.py#L8-L30)", "url": "https://ai.pydantic.dev/examples/slack-lead-qualifier/index.html#send-slack-message", "page": "examples/slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Features", "anchor": "features", "heading_level": 3, "md_text": "Now we can start putting these building blocks together to implement the actual features we want! #### process_slack_member This function takes a [ Profile ](index.html#profile), [analyzes](index.html#analyze_profile) it using the agent, adds it to the [ AnalysisStore ](index.html#analysis-store), and [sends](index.html#send-slack-message) the analysis into the #new-slack-leads channel. [slack\\_lead\\_qualifier/functions.py (L4-L45)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/functions.py#L4-L45) #### send_daily_summary This function list all of the analyses in the [ AnalysisStore ](index.html#analysis-store), takes the top 5 by relevance, [sends](index.html#send-slack-message) them into the #daily-slack-leads-summary channel, and clears the AnalysisStore so that the next daily run won't process these analyses again. [slack\\_lead\\_qualifier/functions.py (L8-L85)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/functions.py#L8-L85)", "url": "https://ai.pydantic.dev/examples/slack-lead-qualifier/index.html#features", "page": "examples/slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Web app", "anchor": "web-app", "heading_level": 3, "md_text": "As it stands, neither of these functions are actually being called from anywhere. Let's implement a [FastAPI](https://fastapi.tiangolo.com/) endpoint to handle the team_join Slack webhook (also known as the [Slack Events API](https://docs.slack.dev/apis/events-api)) and call the [ process_slack_member ](index.html#process_slack_member) function we just defined. We also instrument FastAPI using Logfire for good measure. [slack\\_lead\\_qualifier/app.py (L20-L36)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/app.py#L20-L36) #### process_slack_member with Modal I was a little sneaky there -- we're not actually calling the [ process_slack_member ](index.html#process_slack_member) function we defined in functions.py directly, as Slack requires webhooks to respond within 3 seconds, and we need a bit more time than that to talk to the LLM, do some web searches, and send the Slack message. Instead, we're calling the following function defined alongside the app, which uses Modal's [ modal.Function.spawn ](https://modal.com/docs/reference/modal.Function#spawn) feature to run a function in the background. (If you're curious what the Modal side of this function looks like, you can [jump ahead](index.html#backgrounded-process_slack_member).) Because modal.py (which we'll see in the next section) imports app.py , we import from modal.py inside the function definition because doing so at the top level would have resulted in a circular import error. We also pass along the current Logfire context to get [Distributed Tracing](https://logfire.pydantic.dev/docs/how-to-guides/distributed-tracing/), meaning that the background function execution will show up nested under the webhook request trace, so that we have everything related to that request in one place. [slack\\_lead\\_qualifier/app.py (L11-L16)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/app.py#L11-L16)", "url": "https://ai.pydantic.dev/examples/slack-lead-qualifier/index.html#web-app", "page": "examples/slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Modal app", "anchor": "modal-app", "heading_level": 3, "md_text": "Now let's see how easy Modal makes it to deploy all of this. #### Set up Modal The first thing we do is define the Modal app, by specifying the base image to use (Debian with Python 3.13), all the Python packages it needs, and all of the secrets defined in the Modal interface that need to be made available during runtime. [slack\\_lead\\_qualifier/modal.py (L4-L21)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/modal.py#L4-L21) #### Set up Logfire Next, we define a function to set up Logfire instrumentation for Pydantic AI and HTTPX. We cannot do this at the top level of the file, as the requested packages (like logfire ) will only be available within functions running on Modal (like the ones we'll define next). This file, modal.py , runs on your local machine and only has access to the modal package. [slack\\_lead\\_qualifier/modal.py (L25-L30)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/modal.py#L25-L30) #### Web app To deploy a [web endpoint](https://modal.com/docs/guide/webhooks) on Modal, we simply define a function that returns an ASGI app (like FastAPI) and decorate it with @app.function() and @modal.asgi_app() . This web_app function will be run on Modal, so inside the function we can call the setup_logfire function that requires the logfire package, and import app.py which uses the other requested packages. By default, Modal spins up a container to handle a function call (like a web request) on-demand, meaning there's a little bit of startup time to each request. However, Slack requires webhooks to respond within 3 seconds, so we specify min_containers=1 to keep the web endpoint running and ready to answer requests at all times. This is a bit annoying and wasteful, but fortunately [Modal's pricing](https://modal.com/pricing) is pretty reasonable, you get $30 free monthly compute, and they offer up to $50k in free credits for startup and academic researchers. [slack\\_lead\\_qualifier/modal.py (L34-L41)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/modal.py#L34-L41) Note Note that # type: ignore on the @modal.asgi_app() line -- unfortunately modal does not fully define its types, so we need this to stop our static type checker pyright , which we run over all Pydantic AI code including examples, from complaining. #### Scheduled send_daily_summary To define a [scheduled function](https://modal.com/docs/guide/cron), we can use the @app.function() decorator with a schedule argument. This Modal function will call our imported [ send_daily_summary ](index.html#send_daily_summary) function every day at 8 am UTC. [slack\\_lead\\_qualifier/modal.py (L60-L66)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/modal.py#L60-L66) #### Backgrounded process_slack_member Finally, we define a Modal function that wraps our [ process_slack_member ](index.html#process_slack_member) function, so that it can run in the background. As you'll remember from when we [spawned this function from the web app](index.html#process_slack_member-with-modal), we passed along the Logfire context to get [Distributed Tracing](https://logfire.pydantic.dev/docs/how-to-guides/distributed-tracing/), so we need to attach it here. [slack\\_lead\\_qualifier/modal.py (L45-L56)](https://github.com/pydantic/pydantic-ai/blob/main/examples/pydantic_ai_examples/slack_lead_qualifier/modal.py#L45-L56)", "url": "https://ai.pydantic.dev/examples/slack-lead-qualifier/index.html#modal-app", "page": "examples/slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Conclusion", "anchor": "conclusion", "heading_level": 2, "md_text": "And that's it! Now, assuming you've met the [prerequisites](index.html#prerequisites), you can run or deploy the app using the commands under [usage](index.html#usage).", "url": "https://ai.pydantic.dev/examples/slack-lead-qualifier/index.html#conclusion", "page": "examples/slack-lead-qualifier/index.html", "source_site": "pydantic_ai"}
{"title": "Beta Graph API", "anchor": "beta-graph-api", "heading_level": 1, "md_text": "Beta API This is the new beta graph API. It provides enhanced capabilities for parallel execution, conditional branching, and complex workflows. The original graph API is still available (and compatible of interop with the new beta API) and is documented in the [main graph documentation](../index.html). ## Overview The beta graph API in pydantic-graph provides a powerful builder pattern for constructing parallel execution graphs with: * **Step nodes** for executing async functions * **Decision nodes** for conditional branching * **Spread operations** for parallel processing of iterables * **Broadcast operations** for sending the same data to multiple parallel paths * **Join nodes and Reducers** for aggregating results from parallel execution This API is designed for advanced workflows where you want declarative control over parallelism, routing, and data aggregation. ## Installation The beta graph API is included with pydantic-graph : Or as part of pydantic-ai : ## Quick Start Here's a simple example to get you started: simple\\_counter.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ## Key Concepts ### GraphBuilder The [ GraphBuilder ](../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder) is the main entry point for constructing graphs. It's generic over: * StateT - The type of mutable state shared across all nodes * DepsT - The type of dependencies injected into nodes * InputT - The type of initial input to the graph * OutputT - The type of final output from the graph ### Steps Steps are async functions decorated with [ @g.step ](../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.step) that define the actual work to be done in each node. They receive a [ StepContext ](../../api/pydantic_graph/beta_step/index.html#pydantic_graph.beta.step.StepContext) with access to: * ctx.state - The mutable graph state * ctx.deps - Injected dependencies * ctx.inputs - Input data for this step ### Edges Edges define the connections between nodes. The builder provides multiple ways to create edges: * [ g.add() ](../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.add) - Add one or more edge paths * [ g.add_edge() ](../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.add_edge) - Add a simple edge between two nodes * [ g.edge_from() ](../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.edge_from) - Start building a complex edge path ### Start and End Nodes Every graph has: * [ g.start_node ](../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.start_node) - The entry point receiving initial inputs * [ g.end_node ](../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.end_node) - The exit point producing final outputs ## A More Complex Example Here's an example showcasing parallel execution with a map operation: parallel\\_processing.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* In this example: 1. The start node receives a list of integers 2. The .map() operation fans out each item to a separate parallel execution of the square step 3. All results are collected back together using [ reduce_list_append ](../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_list_append) 4. The joined results flow to the end node ## Next Steps Explore the detailed documentation for each feature: * [**Steps**](steps/index.html) - Learn about step nodes and execution contexts * [**Joins**](joins/index.html) - Understand join nodes and reducer patterns * [**Decisions**](decisions/index.html) - Implement conditional branching * [**Parallel Execution**](parallel/index.html) - Master broadcasting and mapping ## Advanced Execution Control Beyond the basic [ graph.run() ](../../api/pydantic_graph/beta_graph/index.html#pydantic_graph.beta.graph.Graph.run) method, the beta API provides fine-grained control over graph execution. ### Step-by-Step Execution Use [ graph.iter() ](../../api/pydantic_graph/beta_graph/index.html#pydantic_graph.beta.graph.Graph.iter) to execute the graph one step at a time: step\\_by\\_step.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* The [ GraphRun ](../../api/pydantic_graph/beta_graph/index.html#pydantic_graph.beta.graph.GraphRun) object provides: * **Async iteration**: Iterate through execution events * ** next_task property**: Inspect upcoming tasks * ** output property**: Check if the graph has completed and get the final output * ** next() method**: Manually advance execution with optional value injection ### Visualizing Graphs Generate Mermaid diagrams of your graph structure using [ graph.render() ](../../api/pydantic_graph/beta_graph/index.html#pydantic_graph.beta.graph.Graph.render): visualize\\_graph.py The rendered diagram can be displayed in documentation, notebooks, or any tool that supports Mermaid syntax. ## Comparison with Original API The original graph API (documented in the [main graph page](../index.html)) uses a class-based approach with [ BaseNode ](../../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode) subclasses. The beta API uses a builder pattern with decorated functions, which provides: **Advantages:** - More concise syntax for simple workflows - Explicit control over parallelism with map/broadcast - Built-in reducers for common aggregation patterns - Easier to visualize complex data flows **Trade-offs:** - Requires understanding of builder patterns - Less object-oriented, more functional style Both APIs are fully supported and can even be integrated together when needed.", "url": "https://ai.pydantic.dev/graph/beta/index.html#beta-graph-api", "page": "graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "The beta graph API in pydantic-graph provides a powerful builder pattern for constructing parallel execution graphs with: * **Step nodes** for executing async functions * **Decision nodes** for conditional branching * **Spread operations** for parallel processing of iterables * **Broadcast operations** for sending the same data to multiple parallel paths * **Join nodes and Reducers** for aggregating results from parallel execution This API is designed for advanced workflows where you want declarative control over parallelism, routing, and data aggregation.", "url": "https://ai.pydantic.dev/graph/beta/index.html#overview", "page": "graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "Installation", "anchor": "installation", "heading_level": 2, "md_text": "The beta graph API is included with pydantic-graph : Or as part of pydantic-ai :", "url": "https://ai.pydantic.dev/graph/beta/index.html#installation", "page": "graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "Quick Start", "anchor": "quick-start", "heading_level": 2, "md_text": "Here's a simple example to get you started: simple\\_counter.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/index.html#quick-start", "page": "graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "Key Concepts", "anchor": "key-concepts", "heading_level": 2, "md_text": "### GraphBuilder The [ GraphBuilder ](../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder) is the main entry point for constructing graphs. It's generic over: * StateT - The type of mutable state shared across all nodes * DepsT - The type of dependencies injected into nodes * InputT - The type of initial input to the graph * OutputT - The type of final output from the graph ### Steps Steps are async functions decorated with [ @g.step ](../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.step) that define the actual work to be done in each node. They receive a [ StepContext ](../../api/pydantic_graph/beta_step/index.html#pydantic_graph.beta.step.StepContext) with access to: * ctx.state - The mutable graph state * ctx.deps - Injected dependencies * ctx.inputs - Input data for this step ### Edges Edges define the connections between nodes. The builder provides multiple ways to create edges: * [ g.add() ](../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.add) - Add one or more edge paths * [ g.add_edge() ](../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.add_edge) - Add a simple edge between two nodes * [ g.edge_from() ](../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.edge_from) - Start building a complex edge path ### Start and End Nodes Every graph has: * [ g.start_node ](../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.start_node) - The entry point receiving initial inputs * [ g.end_node ](../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.end_node) - The exit point producing final outputs", "url": "https://ai.pydantic.dev/graph/beta/index.html#key-concepts", "page": "graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "GraphBuilder", "anchor": "graphbuilder", "heading_level": 3, "md_text": "The [ GraphBuilder ](../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder) is the main entry point for constructing graphs. It's generic over: * StateT - The type of mutable state shared across all nodes * DepsT - The type of dependencies injected into nodes * InputT - The type of initial input to the graph * OutputT - The type of final output from the graph", "url": "https://ai.pydantic.dev/graph/beta/index.html#graphbuilder", "page": "graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "Steps", "anchor": "steps", "heading_level": 3, "md_text": "Steps are async functions decorated with [ @g.step ](../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.step) that define the actual work to be done in each node. They receive a [ StepContext ](../../api/pydantic_graph/beta_step/index.html#pydantic_graph.beta.step.StepContext) with access to: * ctx.state - The mutable graph state * ctx.deps - Injected dependencies * ctx.inputs - Input data for this step", "url": "https://ai.pydantic.dev/graph/beta/index.html#steps", "page": "graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "Edges", "anchor": "edges", "heading_level": 3, "md_text": "Edges define the connections between nodes. The builder provides multiple ways to create edges: * [ g.add() ](../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.add) - Add one or more edge paths * [ g.add_edge() ](../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.add_edge) - Add a simple edge between two nodes * [ g.edge_from() ](../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.edge_from) - Start building a complex edge path", "url": "https://ai.pydantic.dev/graph/beta/index.html#edges", "page": "graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "Start and End Nodes", "anchor": "start-and-end-nodes", "heading_level": 3, "md_text": "Every graph has: * [ g.start_node ](../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.start_node) - The entry point receiving initial inputs * [ g.end_node ](../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.end_node) - The exit point producing final outputs", "url": "https://ai.pydantic.dev/graph/beta/index.html#start-and-end-nodes", "page": "graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "A More Complex Example", "anchor": "a-more-complex-example", "heading_level": 2, "md_text": "Here's an example showcasing parallel execution with a map operation: parallel\\_processing.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* In this example: 1. The start node receives a list of integers 2. The .map() operation fans out each item to a separate parallel execution of the square step 3. All results are collected back together using [ reduce_list_append ](../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_list_append) 4. The joined results flow to the end node", "url": "https://ai.pydantic.dev/graph/beta/index.html#a-more-complex-example", "page": "graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "Explore the detailed documentation for each feature: * [**Steps**](steps/index.html) - Learn about step nodes and execution contexts * [**Joins**](joins/index.html) - Understand join nodes and reducer patterns * [**Decisions**](decisions/index.html) - Implement conditional branching * [**Parallel Execution**](parallel/index.html) - Master broadcasting and mapping", "url": "https://ai.pydantic.dev/graph/beta/index.html#next-steps", "page": "graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "Advanced Execution Control", "anchor": "advanced-execution-control", "heading_level": 2, "md_text": "Beyond the basic [ graph.run() ](../../api/pydantic_graph/beta_graph/index.html#pydantic_graph.beta.graph.Graph.run) method, the beta API provides fine-grained control over graph execution. ### Step-by-Step Execution Use [ graph.iter() ](../../api/pydantic_graph/beta_graph/index.html#pydantic_graph.beta.graph.Graph.iter) to execute the graph one step at a time: step\\_by\\_step.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* The [ GraphRun ](../../api/pydantic_graph/beta_graph/index.html#pydantic_graph.beta.graph.GraphRun) object provides: * **Async iteration**: Iterate through execution events * ** next_task property**: Inspect upcoming tasks * ** output property**: Check if the graph has completed and get the final output * ** next() method**: Manually advance execution with optional value injection ### Visualizing Graphs Generate Mermaid diagrams of your graph structure using [ graph.render() ](../../api/pydantic_graph/beta_graph/index.html#pydantic_graph.beta.graph.Graph.render): visualize\\_graph.py The rendered diagram can be displayed in documentation, notebooks, or any tool that supports Mermaid syntax.", "url": "https://ai.pydantic.dev/graph/beta/index.html#advanced-execution-control", "page": "graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "Step-by-Step Execution", "anchor": "step-by-step-execution", "heading_level": 3, "md_text": "Use [ graph.iter() ](../../api/pydantic_graph/beta_graph/index.html#pydantic_graph.beta.graph.Graph.iter) to execute the graph one step at a time: step\\_by\\_step.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* The [ GraphRun ](../../api/pydantic_graph/beta_graph/index.html#pydantic_graph.beta.graph.GraphRun) object provides: * **Async iteration**: Iterate through execution events * ** next_task property**: Inspect upcoming tasks * ** output property**: Check if the graph has completed and get the final output * ** next() method**: Manually advance execution with optional value injection", "url": "https://ai.pydantic.dev/graph/beta/index.html#step-by-step-execution", "page": "graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "Visualizing Graphs", "anchor": "visualizing-graphs", "heading_level": 3, "md_text": "Generate Mermaid diagrams of your graph structure using [ graph.render() ](../../api/pydantic_graph/beta_graph/index.html#pydantic_graph.beta.graph.Graph.render): visualize\\_graph.py The rendered diagram can be displayed in documentation, notebooks, or any tool that supports Mermaid syntax.", "url": "https://ai.pydantic.dev/graph/beta/index.html#visualizing-graphs", "page": "graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "Comparison with Original API", "anchor": "comparison-with-original-api", "heading_level": 2, "md_text": "The original graph API (documented in the [main graph page](../index.html)) uses a class-based approach with [ BaseNode ](../../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode) subclasses. The beta API uses a builder pattern with decorated functions, which provides: **Advantages:** - More concise syntax for simple workflows - Explicit control over parallelism with map/broadcast - Built-in reducers for common aggregation patterns - Easier to visualize complex data flows **Trade-offs:** - Requires understanding of builder patterns - Less object-oriented, more functional style Both APIs are fully supported and can even be integrated together when needed.", "url": "https://ai.pydantic.dev/graph/beta/index.html#comparison-with-original-api", "page": "graph/beta/index.html", "source_site": "pydantic_ai"}
{"title": "Getting Help", "anchor": "getting-help", "heading_level": 1, "md_text": "If you need help getting started with Pydantic AI or with advanced usage, the following sources may be useful. ## Slack Join the #pydantic-ai channel in the [Pydantic Slack](https://logfire.pydantic.dev/docs/join-slack/) to ask questions, get help, and chat about Pydantic AI. There's also channels for Pydantic, Logfire, and FastUI. If you're on a [Logfire](https://pydantic.dev/logfire) Pro plan, you can also get a dedicated private slack collab channel with us. ## GitHub Issues The [Pydantic AI GitHub Issues](https://github.com/pydantic/pydantic-ai/issues) are a great place to ask questions and give us feedback.", "url": "https://ai.pydantic.dev/help/index.html#getting-help", "page": "help/index.html", "source_site": "pydantic_ai"}
{"title": "Slack", "anchor": "slack", "heading_level": 2, "md_text": "Join the #pydantic-ai channel in the [Pydantic Slack](https://logfire.pydantic.dev/docs/join-slack/) to ask questions, get help, and chat about Pydantic AI. There's also channels for Pydantic, Logfire, and FastUI. If you're on a [Logfire](https://pydantic.dev/logfire) Pro plan, you can also get a dedicated private slack collab channel with us.", "url": "https://ai.pydantic.dev/help/index.html#slack", "page": "help/index.html", "source_site": "pydantic_ai"}
{"title": "GitHub Issues", "anchor": "github-issues", "heading_level": 2, "md_text": "The [Pydantic AI GitHub Issues](https://github.com/pydantic/pydantic-ai/issues) are a great place to ask questions and give us feedback.", "url": "https://ai.pydantic.dev/help/index.html#github-issues", "page": "help/index.html", "source_site": "pydantic_ai"}
{"title": "Pydantic AI", "anchor": "pydantic-ai", "heading_level": 1, "md_text": "![Pydantic AI](img/pydantic-ai-dark.svg#only-dark) ![Pydantic AI](img/pydantic-ai-light.svg#only-light) *GenAI Agent Framework, the Pydantic way* [![CI](https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml/badge.svg?event=push)](https://github.com/pydantic/pydantic-ai/actions/workflows/ci.yml?query=branch%3Amain) [![Coverage](https://coverage-badge.samuelcolvin.workers.dev/pydantic/pydantic-ai.svg)](https://coverage-badge.samuelcolvin.workers.dev/redirect/pydantic/pydantic-ai) [![PyPI](https://img.shields.io/pypi/v/pydantic-ai.svg)](https://pypi.python.org/pypi/pydantic-ai) [![versions](https://img.shields.io/pypi/pyversions/pydantic-ai.svg)](https://github.com/pydantic/pydantic-ai) [![license](https://img.shields.io/github/license/pydantic/pydantic-ai.svg)](https://github.com/pydantic/pydantic-ai/blob/main/LICENSE) [![Join Slack](https://img.shields.io/badge/Slack-Join%20Slack-4A154B?logo=slack)](https://logfire.pydantic.dev/docs/join-slack/) Pydantic AI is a Python agent framework designed to help you quickly, confidently, and painlessly build production grade applications and workflows with Generative AI. FastAPI revolutionized web development by offering an innovative and ergonomic design, built on the foundation of [Pydantic Validation](https://docs.pydantic.dev) and modern Python features like type hints. Yet despite virtually every Python agent framework and LLM library using Pydantic Validation, when we began to use LLMs in [Pydantic Logfire](https://pydantic.dev/logfire), we couldn't find anything that gave us the same feeling. We built Pydantic AI with one simple aim: to bring that FastAPI feeling to GenAI app and agent development. ## Why use Pydantic AI 1. **Built by the Pydantic Team**: [Pydantic Validation](https://docs.pydantic.dev/latest/) is the validation layer of the OpenAI SDK, the Google ADK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more. *Why use the derivative when you can go straight to the source?* ![](https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f603.svg \":smiley:\") 2. **Model-agnostic**: Supports virtually every [model](models/overview/index.html) and provider: OpenAI, Anthropic, Gemini, DeepSeek, Grok, Cohere, Mistral, and Perplexity; Azure AI Foundry, Amazon Bedrock, Google Vertex AI, Ollama, LiteLLM, Groq, OpenRouter, Together AI, Fireworks AI, Cerebras, Hugging Face, GitHub, Heroku, Vercel, Nebius, OVHcloud. If your favorite model or provider is not listed, you can easily implement a [custom model](models/overview/index.html#custom-models). 3. **Seamless Observability**: Tightly [integrates](logfire/index.html) with [Pydantic Logfire](https://pydantic.dev/logfire), our general-purpose OpenTelemetry observability platform, for real-time debugging, evals-based performance monitoring, and behavior, tracing, and cost tracking. If you already have an observability platform that supports OTel, you can [use that too](logfire/index.html#alternative-observability-backends). 4. **Fully Type-safe**: Designed to give your IDE or AI coding agent as much context as possible for auto-completion and [type checking](agents/index.html#static-type-checking), moving entire classes of errors from runtime to write-time for a bit of that Rust \"if it compiles, it works\" feel. 5. **Powerful Evals**: Enables you to systematically test and [evaluate](evals/index.html) the performance and accuracy of the agentic systems you build, and monitor the performance over time in Pydantic Logfire. 6. **MCP, A2A, and AG-UI**: Integrates the [Model Context Protocol](mcp/client/index.html), [Agent2Agent](a2a/index.html), and [AG-UI](ag-ui/index.html) standards to give your agent access to external tools and data, let it interoperate with other agents, and build interactive applications with streaming event-based communication. 7. **Human-in-the-Loop Tool Approval**: Easily lets you flag that certain tool calls [require approval](deferred-tools/index.html#human-in-the-loop-tool-approval) before they can proceed, possibly depending on tool call arguments, conversation history, or user preferences. 8. **Durable Execution**: Enables you to build [durable agents](durable_execution/overview/index.html) that can preserve their progress across transient API failures and application errors or restarts, and handle long-running, asynchronous, and human-in-the-loop workflows with production-grade reliability. 9. **Streamed Outputs**: Provides the ability to [stream](output/index.html#streamed-results) structured output continuously, with immediate validation, ensuring real time access to generated data. 10. **Graph Support**: Provides a powerful way to define [graphs](graph/index.html) using type hints, for use in complex applications where standard control flow can degrade to spaghetti code. Realistically though, no list is going to be as convincing as [giving it a try](index.html#next-steps) and seeing how it makes you feel! **Sign up for our newsletter, *The Pydantic Stack*, with updates & tutorials on Pydantic AI, Logfire, and Pydantic:** Subscribe ## Hello World Example Here's a minimal example of Pydantic AI: hello\\_world.py 1. We configure the agent to use [Anthropic's Claude Sonnet 4.0](api/models/anthropic/index.html) model, but you can also set the model when running the agent. 2. Register static [instructions](agents/index.html#instructions) using a keyword argument to the agent. 3. [Run the agent](agents/index.html#running-agents) synchronously, starting a conversation with the LLM. *(This example is complete, it can be run \"as is\", assuming you've [installed the pydantic_ai package](install/index.html))* The exchange will be very short: Pydantic AI will send the instructions and the user prompt to the LLM, and the model will return a text response. Not very interesting yet, but we can easily add [tools](tools/index.html), [dynamic instructions](agents/index.html#instructions), and [structured outputs](output/index.html) to build more powerful agents. ## Tools & Dependency Injection Example Here is a concise example using Pydantic AI to build a support agent for a bank: bank\\_support.py 1. This [agent](agents/index.html) will act as first-tier support in a bank. Agents are generic in the type of dependencies they accept and the type of output they return. In this case, the support agent has type Agent[SupportDependencies, SupportOutput] . 2. Here we configure the agent to use [OpenAI's GPT-5 model](api/models/openai/index.html), you can also set the model when running the agent. 3. The SupportDependencies dataclass is used to pass data, connections, and logic into the model that will be needed when running [instructions](agents/index.html#instructions) and [tool](tools/index.html) functions. Pydantic AI's system of dependency injection provides a [type-safe](agents/index.html#static-type-checking) way to customise the behavior of your agents, and can be especially useful when running [unit tests](testing/index.html) and evals. 4. Static [instructions](agents/index.html#instructions) can be registered with the [ instructions keyword argument](api/agent/index.html#pydantic_ai.agent.Agent.__init__) to the agent. 5. Dynamic [instructions](agents/index.html#instructions) can be registered with the [ @agent.instructions ](api/agent/index.html#pydantic_ai.agent.Agent.instructions) decorator, and can make use of dependency injection. Dependencies are carried via the [ RunContext ](api/tools/index.html#pydantic_ai.tools.RunContext) argument, which is parameterized with the deps_type from above. If the type annotation here is wrong, static type checkers will catch it. 6. The [ @agent.tool ](tools/index.html) decorator let you register functions which the LLM may call while responding to a user. Again, dependencies are carried via [ RunContext ](api/tools/index.html#pydantic_ai.tools.RunContext), any other arguments become the tool schema passed to the LLM. Pydantic is used to validate these arguments, and errors are passed back to the LLM so it can retry. 7. The docstring of a tool is also passed to the LLM as the description of the tool. Parameter descriptions are [extracted](tools/index.html#function-tools-and-schema) from the docstring and added to the parameter schema sent to the LLM. 8. [Run the agent](agents/index.html#running-agents) asynchronously, conducting a conversation with the LLM until a final response is reached. Even in this fairly simple case, the agent will exchange multiple messages with the LLM as tools are called to retrieve an output. 9. The response from the agent will be guaranteed to be a SupportOutput . If validation fails [reflection](agents/index.html#reflection-and-self-correction), the agent is prompted to try again. 10. The output will be validated with Pydantic to guarantee it is a SupportOutput , since the agent is generic, it'll also be typed as a SupportOutput to aid with static type checking. 11. In a real use case, you'd add more tools and longer instructions to the agent to extend the context it's equipped with and support it can provide. 12. This is a simple sketch of a database connection, used to keep the example short and readable. In reality, you'd be connecting to an external database (e.g. PostgreSQL) to get information about customers. 13. This [Pydantic](https://docs.pydantic.dev) model is used to constrain the structured data returned by the agent. From this simple definition, Pydantic builds the JSON Schema that tells the LLM how to return the data, and performs validation to guarantee the data is correct at the end of the run. Complete bank_support.py example The code included here is incomplete for the sake of brevity (the definition of DatabaseConn is missing); you can find the complete bank_support.py example [here](examples/bank-support/index.html). ## Instrumentation with Pydantic Logfire Even a simple agent with just a handful of tools can result in a lot of back-and-forth with the LLM, making it nearly impossible to be confident of what's going on just from reading the code. To understand the flow of the above runs, we can watch the agent in action using Pydantic Logfire. To do this, we need to [set up Logfire](logfire/index.html#using-logfire), and add the following to our code: bank\\_support\\_with\\_logfire.py 1. Configure the Logfire SDK, this will fail if project is not set up. 2. This will instrument all Pydantic AI agents used from here on out. If you want to instrument only a specific agent, you can pass the [ instrument=True keyword argument](api/agent/index.html#pydantic_ai.agent.Agent.__init__) to the agent. 3. In our demo, DatabaseConn uses [ asyncpg ](index.html) to connect to a PostgreSQL database, so [ logfire.instrument_asyncpg() ](https://magicstack.github.io/asyncpg/current/) is used to log the database queries. That's enough to get the following view of your agent in action: See [Monitoring and Performance](logfire/index.html) to learn more. ## llms.txt The Pydantic AI documentation is available in the [llms.txt](https://llmstxt.org/) format. This format is defined in Markdown and suited for LLMs and AI coding assistants and agents. Two formats are available: * [ llms.txt ](llms.txt): a file containing a brief description of the project, along with links to the different sections of the documentation. The structure of this file is described in details [here](https://llmstxt.org/#format). * [ llms-full.txt ](llms-full.txt): Similar to the llms.txt file, but every link content is included. Note that this file may be too large for some LLMs. As of today, these files are not automatically leveraged by IDEs or coding agents, but they will use it if you provide a link or the full text. ## Next Steps To try Pydantic AI for yourself, [install it](install/index.html) and follow the instructions [in the examples](examples/setup/index.html). Read the [docs](agents/index.html) to learn more about building applications with Pydantic AI. Read the [API Reference](api/agent/index.html) to understand Pydantic AI's interface. Join [Slack](https://logfire.pydantic.dev/docs/join-slack/) or file an issue on [GitHub](https://github.com/pydantic/pydantic-ai/issues) if you have any questions.", "url": "https://ai.pydantic.dev/index.html#pydantic-ai", "page": "index.html", "source_site": "pydantic_ai"}
{"title": "Why use Pydantic AI", "anchor": "why-use-pydantic-ai", "heading_level": 2, "md_text": "1. **Built by the Pydantic Team**: [Pydantic Validation](https://docs.pydantic.dev/latest/) is the validation layer of the OpenAI SDK, the Google ADK, the Anthropic SDK, LangChain, LlamaIndex, AutoGPT, Transformers, CrewAI, Instructor and many more. *Why use the derivative when you can go straight to the source?* ![](https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f603.svg \":smiley:\") 2. **Model-agnostic**: Supports virtually every [model](models/overview/index.html) and provider: OpenAI, Anthropic, Gemini, DeepSeek, Grok, Cohere, Mistral, and Perplexity; Azure AI Foundry, Amazon Bedrock, Google Vertex AI, Ollama, LiteLLM, Groq, OpenRouter, Together AI, Fireworks AI, Cerebras, Hugging Face, GitHub, Heroku, Vercel, Nebius, OVHcloud. If your favorite model or provider is not listed, you can easily implement a [custom model](models/overview/index.html#custom-models). 3. **Seamless Observability**: Tightly [integrates](logfire/index.html) with [Pydantic Logfire](https://pydantic.dev/logfire), our general-purpose OpenTelemetry observability platform, for real-time debugging, evals-based performance monitoring, and behavior, tracing, and cost tracking. If you already have an observability platform that supports OTel, you can [use that too](logfire/index.html#alternative-observability-backends). 4. **Fully Type-safe**: Designed to give your IDE or AI coding agent as much context as possible for auto-completion and [type checking](agents/index.html#static-type-checking), moving entire classes of errors from runtime to write-time for a bit of that Rust \"if it compiles, it works\" feel. 5. **Powerful Evals**: Enables you to systematically test and [evaluate](evals/index.html) the performance and accuracy of the agentic systems you build, and monitor the performance over time in Pydantic Logfire. 6. **MCP, A2A, and AG-UI**: Integrates the [Model Context Protocol](mcp/client/index.html), [Agent2Agent](a2a/index.html), and [AG-UI](ag-ui/index.html) standards to give your agent access to external tools and data, let it interoperate with other agents, and build interactive applications with streaming event-based communication. 7. **Human-in-the-Loop Tool Approval**: Easily lets you flag that certain tool calls [require approval](deferred-tools/index.html#human-in-the-loop-tool-approval) before they can proceed, possibly depending on tool call arguments, conversation history, or user preferences. 8. **Durable Execution**: Enables you to build [durable agents](durable_execution/overview/index.html) that can preserve their progress across transient API failures and application errors or restarts, and handle long-running, asynchronous, and human-in-the-loop workflows with production-grade reliability. 9. **Streamed Outputs**: Provides the ability to [stream](output/index.html#streamed-results) structured output continuously, with immediate validation, ensuring real time access to generated data. 10. **Graph Support**: Provides a powerful way to define [graphs](graph/index.html) using type hints, for use in complex applications where standard control flow can degrade to spaghetti code. Realistically though, no list is going to be as convincing as [giving it a try](index.html#next-steps) and seeing how it makes you feel! **Sign up for our newsletter, *The Pydantic Stack*, with updates & tutorials on Pydantic AI, Logfire, and Pydantic:** Subscribe", "url": "https://ai.pydantic.dev/index.html#why-use-pydantic-ai", "page": "index.html", "source_site": "pydantic_ai"}
{"title": "Hello World Example", "anchor": "hello-world-example", "heading_level": 2, "md_text": "Here's a minimal example of Pydantic AI: hello\\_world.py 1. We configure the agent to use [Anthropic's Claude Sonnet 4.0](api/models/anthropic/index.html) model, but you can also set the model when running the agent. 2. Register static [instructions](agents/index.html#instructions) using a keyword argument to the agent. 3. [Run the agent](agents/index.html#running-agents) synchronously, starting a conversation with the LLM. *(This example is complete, it can be run \"as is\", assuming you've [installed the pydantic_ai package](install/index.html))* The exchange will be very short: Pydantic AI will send the instructions and the user prompt to the LLM, and the model will return a text response. Not very interesting yet, but we can easily add [tools](tools/index.html), [dynamic instructions](agents/index.html#instructions), and [structured outputs](output/index.html) to build more powerful agents.", "url": "https://ai.pydantic.dev/index.html#hello-world-example", "page": "index.html", "source_site": "pydantic_ai"}
{"title": "Tools & Dependency Injection Example", "anchor": "tools-dependency-injection-example", "heading_level": 2, "md_text": "Here is a concise example using Pydantic AI to build a support agent for a bank: bank\\_support.py 1. This [agent](agents/index.html) will act as first-tier support in a bank. Agents are generic in the type of dependencies they accept and the type of output they return. In this case, the support agent has type Agent[SupportDependencies, SupportOutput] . 2. Here we configure the agent to use [OpenAI's GPT-5 model](api/models/openai/index.html), you can also set the model when running the agent. 3. The SupportDependencies dataclass is used to pass data, connections, and logic into the model that will be needed when running [instructions](agents/index.html#instructions) and [tool](tools/index.html) functions. Pydantic AI's system of dependency injection provides a [type-safe](agents/index.html#static-type-checking) way to customise the behavior of your agents, and can be especially useful when running [unit tests](testing/index.html) and evals. 4. Static [instructions](agents/index.html#instructions) can be registered with the [ instructions keyword argument](api/agent/index.html#pydantic_ai.agent.Agent.__init__) to the agent. 5. Dynamic [instructions](agents/index.html#instructions) can be registered with the [ @agent.instructions ](api/agent/index.html#pydantic_ai.agent.Agent.instructions) decorator, and can make use of dependency injection. Dependencies are carried via the [ RunContext ](api/tools/index.html#pydantic_ai.tools.RunContext) argument, which is parameterized with the deps_type from above. If the type annotation here is wrong, static type checkers will catch it. 6. The [ @agent.tool ](tools/index.html) decorator let you register functions which the LLM may call while responding to a user. Again, dependencies are carried via [ RunContext ](api/tools/index.html#pydantic_ai.tools.RunContext), any other arguments become the tool schema passed to the LLM. Pydantic is used to validate these arguments, and errors are passed back to the LLM so it can retry. 7. The docstring of a tool is also passed to the LLM as the description of the tool. Parameter descriptions are [extracted](tools/index.html#function-tools-and-schema) from the docstring and added to the parameter schema sent to the LLM. 8. [Run the agent](agents/index.html#running-agents) asynchronously, conducting a conversation with the LLM until a final response is reached. Even in this fairly simple case, the agent will exchange multiple messages with the LLM as tools are called to retrieve an output. 9. The response from the agent will be guaranteed to be a SupportOutput . If validation fails [reflection](agents/index.html#reflection-and-self-correction), the agent is prompted to try again. 10. The output will be validated with Pydantic to guarantee it is a SupportOutput , since the agent is generic, it'll also be typed as a SupportOutput to aid with static type checking. 11. In a real use case, you'd add more tools and longer instructions to the agent to extend the context it's equipped with and support it can provide. 12. This is a simple sketch of a database connection, used to keep the example short and readable. In reality, you'd be connecting to an external database (e.g. PostgreSQL) to get information about customers. 13. This [Pydantic](https://docs.pydantic.dev) model is used to constrain the structured data returned by the agent. From this simple definition, Pydantic builds the JSON Schema that tells the LLM how to return the data, and performs validation to guarantee the data is correct at the end of the run. Complete bank_support.py example The code included here is incomplete for the sake of brevity (the definition of DatabaseConn is missing); you can find the complete bank_support.py example [here](examples/bank-support/index.html).", "url": "https://ai.pydantic.dev/index.html#tools-dependency-injection-example", "page": "index.html", "source_site": "pydantic_ai"}
{"title": "Instrumentation with Pydantic Logfire", "anchor": "instrumentation-with-pydantic-logfire", "heading_level": 2, "md_text": "Even a simple agent with just a handful of tools can result in a lot of back-and-forth with the LLM, making it nearly impossible to be confident of what's going on just from reading the code. To understand the flow of the above runs, we can watch the agent in action using Pydantic Logfire. To do this, we need to [set up Logfire](logfire/index.html#using-logfire), and add the following to our code: bank\\_support\\_with\\_logfire.py 1. Configure the Logfire SDK, this will fail if project is not set up. 2. This will instrument all Pydantic AI agents used from here on out. If you want to instrument only a specific agent, you can pass the [ instrument=True keyword argument](api/agent/index.html#pydantic_ai.agent.Agent.__init__) to the agent. 3. In our demo, DatabaseConn uses [ asyncpg ](index.html) to connect to a PostgreSQL database, so [ logfire.instrument_asyncpg() ](https://magicstack.github.io/asyncpg/current/) is used to log the database queries. That's enough to get the following view of your agent in action: See [Monitoring and Performance](logfire/index.html) to learn more.", "url": "https://ai.pydantic.dev/index.html#instrumentation-with-pydantic-logfire", "page": "index.html", "source_site": "pydantic_ai"}
{"title": "llms.txt", "anchor": "llmstxt", "heading_level": 2, "md_text": "The Pydantic AI documentation is available in the [llms.txt](https://llmstxt.org/) format. This format is defined in Markdown and suited for LLMs and AI coding assistants and agents. Two formats are available: * [ llms.txt ](llms.txt): a file containing a brief description of the project, along with links to the different sections of the documentation. The structure of this file is described in details [here](https://llmstxt.org/#format). * [ llms-full.txt ](llms-full.txt): Similar to the llms.txt file, but every link content is included. Note that this file may be too large for some LLMs. As of today, these files are not automatically leveraged by IDEs or coding agents, but they will use it if you provide a link or the full text.", "url": "https://ai.pydantic.dev/index.html#llmstxt", "page": "index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "To try Pydantic AI for yourself, [install it](install/index.html) and follow the instructions [in the examples](examples/setup/index.html). Read the [docs](agents/index.html) to learn more about building applications with Pydantic AI. Read the [API Reference](api/agent/index.html) to understand Pydantic AI's interface. Join [Slack](https://logfire.pydantic.dev/docs/join-slack/) or file an issue on [GitHub](https://github.com/pydantic/pydantic-ai/issues) if you have any questions.", "url": "https://ai.pydantic.dev/index.html#next-steps", "page": "index.html", "source_site": "pydantic_ai"}
{"title": "Decision Nodes", "anchor": "decision-nodes", "heading_level": 1, "md_text": "Decision nodes enable conditional branching in your graph based on the type or value of data flowing through it. ## Overview A decision node evaluates incoming data and routes it to different branches based on: * Type matching (using isinstance ) * Literal value matching * Custom predicate functions The first matching branch is taken, similar to pattern matching or if-elif-else chains. ## Creating Decisions Use [ g.decision() ](../../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.decision) to create a decision node, then add branches with [ g.match() ](../../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.match): simple\\_decision.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ## Type Matching Match by type using regular Python types: type\\_matching.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ### Matching Union Types For more complex type expressions like unions, you need to use [ TypeExpression ](../../../api/pydantic_graph/beta/index.html#pydantic_graph.beta.TypeExpression) because Python's type system doesn't allow union types to be used directly as runtime values: union\\_type\\_matching.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* Note [ TypeExpression ](../../../api/pydantic_graph/beta/index.html#pydantic_graph.beta.TypeExpression) is only necessary for complex type expressions like unions ( int str ), Literal , and other type forms that aren't valid as runtime type objects. For simple types like int , str , or custom classes, you can pass them directly to g.match() . The TypeForm class introduced in [PEP 747](https://peps.python.org/pep-0747/) should eventually eliminate the need for this workaround. ## Custom Matchers Provide custom matching logic with the matches parameter: custom\\_matcher.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ## Branch Priority Branches are evaluated in the order they're added. The first matching branch is taken: branch\\_priority.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* Both branches could match 10 , but Branch A is first, so it's taken. ## Catch-All Branches Use object or Any to create a catch-all branch: catch\\_all.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ## Nested Decisions Decisions can be nested for complex conditional logic: nested\\_decisions.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ## Branching with Labels Add labels to branches for documentation and diagram generation: labeled\\_branches.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ## Next Steps * Learn about [parallel execution](../parallel/index.html) with broadcasting and mapping * Understand [join nodes](../joins/index.html) for aggregating parallel results * See the [API reference](../../../api/pydantic_graph/beta_decision/index.html#pydantic_graph.beta.decision) for complete decision documentation", "url": "https://ai.pydantic.dev/graph/beta/decisions/index.html#decision-nodes", "page": "graph/beta/decisions/index.html", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "A decision node evaluates incoming data and routes it to different branches based on: * Type matching (using isinstance ) * Literal value matching * Custom predicate functions The first matching branch is taken, similar to pattern matching or if-elif-else chains.", "url": "https://ai.pydantic.dev/graph/beta/decisions/index.html#overview", "page": "graph/beta/decisions/index.html", "source_site": "pydantic_ai"}
{"title": "Creating Decisions", "anchor": "creating-decisions", "heading_level": 2, "md_text": "Use [ g.decision() ](../../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.decision) to create a decision node, then add branches with [ g.match() ](../../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.match): simple\\_decision.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/decisions/index.html#creating-decisions", "page": "graph/beta/decisions/index.html", "source_site": "pydantic_ai"}
{"title": "Type Matching", "anchor": "type-matching", "heading_level": 2, "md_text": "Match by type using regular Python types: type\\_matching.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ### Matching Union Types For more complex type expressions like unions, you need to use [ TypeExpression ](../../../api/pydantic_graph/beta/index.html#pydantic_graph.beta.TypeExpression) because Python's type system doesn't allow union types to be used directly as runtime values: union\\_type\\_matching.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* Note [ TypeExpression ](../../../api/pydantic_graph/beta/index.html#pydantic_graph.beta.TypeExpression) is only necessary for complex type expressions like unions ( int str ), Literal , and other type forms that aren't valid as runtime type objects. For simple types like int , str , or custom classes, you can pass them directly to g.match() . The TypeForm class introduced in [PEP 747](https://peps.python.org/pep-0747/) should eventually eliminate the need for this workaround.", "url": "https://ai.pydantic.dev/graph/beta/decisions/index.html#type-matching", "page": "graph/beta/decisions/index.html", "source_site": "pydantic_ai"}
{"title": "Matching Union Types", "anchor": "matching-union-types", "heading_level": 3, "md_text": "For more complex type expressions like unions, you need to use [ TypeExpression ](../../../api/pydantic_graph/beta/index.html#pydantic_graph.beta.TypeExpression) because Python's type system doesn't allow union types to be used directly as runtime values: union\\_type\\_matching.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* Note [ TypeExpression ](../../../api/pydantic_graph/beta/index.html#pydantic_graph.beta.TypeExpression) is only necessary for complex type expressions like unions ( int str ), Literal , and other type forms that aren't valid as runtime type objects. For simple types like int , str , or custom classes, you can pass them directly to g.match() . The TypeForm class introduced in [PEP 747](https://peps.python.org/pep-0747/) should eventually eliminate the need for this workaround.", "url": "https://ai.pydantic.dev/graph/beta/decisions/index.html#matching-union-types", "page": "graph/beta/decisions/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Matchers", "anchor": "custom-matchers", "heading_level": 2, "md_text": "Provide custom matching logic with the matches parameter: custom\\_matcher.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/decisions/index.html#custom-matchers", "page": "graph/beta/decisions/index.html", "source_site": "pydantic_ai"}
{"title": "Branch Priority", "anchor": "branch-priority", "heading_level": 2, "md_text": "Branches are evaluated in the order they're added. The first matching branch is taken: branch\\_priority.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* Both branches could match 10 , but Branch A is first, so it's taken.", "url": "https://ai.pydantic.dev/graph/beta/decisions/index.html#branch-priority", "page": "graph/beta/decisions/index.html", "source_site": "pydantic_ai"}
{"title": "Catch-All Branches", "anchor": "catch-all-branches", "heading_level": 2, "md_text": "Use object or Any to create a catch-all branch: catch\\_all.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/decisions/index.html#catch-all-branches", "page": "graph/beta/decisions/index.html", "source_site": "pydantic_ai"}
{"title": "Nested Decisions", "anchor": "nested-decisions", "heading_level": 2, "md_text": "Decisions can be nested for complex conditional logic: nested\\_decisions.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/decisions/index.html#nested-decisions", "page": "graph/beta/decisions/index.html", "source_site": "pydantic_ai"}
{"title": "Branching with Labels", "anchor": "branching-with-labels", "heading_level": 2, "md_text": "Add labels to branches for documentation and diagram generation: labeled\\_branches.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/decisions/index.html#branching-with-labels", "page": "graph/beta/decisions/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "* Learn about [parallel execution](../parallel/index.html) with broadcasting and mapping * Understand [join nodes](../joins/index.html) for aggregating parallel results * See the [API reference](../../../api/pydantic_graph/beta_decision/index.html#pydantic_graph.beta.decision) for complete decision documentation", "url": "https://ai.pydantic.dev/graph/beta/decisions/index.html#next-steps", "page": "graph/beta/decisions/index.html", "source_site": "pydantic_ai"}
{"title": "Image, Audio, Video & Document Input", "anchor": "image-audio-video-document-input", "heading_level": 1, "md_text": "Some LLMs are now capable of understanding audio, video, image and document content. ## Image Input Info Some models do not support image input. Please check the model's documentation to confirm whether it supports image input. If you have a direct URL for the image, you can use [ ImageUrl ](../api/messages/index.html#pydantic_ai.messages.ImageUrl): image\\_input.py If you have the image locally, you can also use [ BinaryContent ](../api/messages/index.html#pydantic_ai.messages.BinaryContent): local\\_image\\_input.py 1. To ensure the example is runnable we download this image from the web, but you can also use Path().read_bytes() to read a local file's contents. ## Audio Input Info Some models do not support audio input. Please check the model's documentation to confirm whether it supports audio input. You can provide audio input using either [ AudioUrl ](../api/messages/index.html#pydantic_ai.messages.AudioUrl) or [ BinaryContent ](../api/messages/index.html#pydantic_ai.messages.BinaryContent). The process is analogous to the examples above. ## Video Input Info Some models do not support video input. Please check the model's documentation to confirm whether it supports video input. You can provide video input using either [ VideoUrl ](../api/messages/index.html#pydantic_ai.messages.VideoUrl) or [ BinaryContent ](../api/messages/index.html#pydantic_ai.messages.BinaryContent). The process is analogous to the examples above. ## Document Input Info Some models do not support document input. Please check the model's documentation to confirm whether it supports document input. You can provide document input using either [ DocumentUrl ](../api/messages/index.html#pydantic_ai.messages.DocumentUrl) or [ BinaryContent ](../api/messages/index.html#pydantic_ai.messages.BinaryContent). The process is similar to the examples above. If you have a direct URL for the document, you can use [ DocumentUrl ](../api/messages/index.html#pydantic_ai.messages.DocumentUrl): document\\_input.py The supported document formats vary by model. You can also use [ BinaryContent ](../api/messages/index.html#pydantic_ai.messages.BinaryContent) to pass document data directly: binary\\_content\\_input.py ## User-side download vs. direct file URL As a general rule, when you provide a URL using any of ImageUrl , AudioUrl , VideoUrl or DocumentUrl , Pydantic AI downloads the file content and then sends it as part of the API request. The situation is different for certain models: * [ AnthropicModel ](../api/models/anthropic/index.html#pydantic_ai.models.anthropic.AnthropicModel): if you provide a PDF document via DocumentUrl , the URL is sent directly in the API request, so no download happens on the user side. * [ GoogleModel ](../api/models/google/index.html#pydantic_ai.models.google.GoogleModel) on Vertex AI: any URL provided using ImageUrl , AudioUrl , VideoUrl , or DocumentUrl is sent as-is in the API request and no data is downloaded beforehand. See the [Gemini API docs for Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#filedata) to learn more about supported URLs, formats and limitations: * Cloud Storage bucket URIs (with protocol gs:// ) * Public HTTP(S) URLs * Public YouTube video URL (maximum one URL per request) However, because of crawling restrictions, it may happen that Gemini can't access certain URLs. In that case, you can instruct Pydantic AI to download the file content and send that instead of the URL by setting the boolean flag force_download to True . This attribute is available on all objects that inherit from [ FileUrl ](../api/messages/index.html#pydantic_ai.messages.FileUrl). * [ GoogleModel ](../api/models/google/index.html#pydantic_ai.models.google.GoogleModel) on GLA: YouTube video URLs are sent directly in the request to the model.", "url": "https://ai.pydantic.dev/input/index.html#image-audio-video-document-input", "page": "input/index.html", "source_site": "pydantic_ai"}
{"title": "Image Input", "anchor": "image-input", "heading_level": 2, "md_text": "Info Some models do not support image input. Please check the model's documentation to confirm whether it supports image input. If you have a direct URL for the image, you can use [ ImageUrl ](../api/messages/index.html#pydantic_ai.messages.ImageUrl): image\\_input.py If you have the image locally, you can also use [ BinaryContent ](../api/messages/index.html#pydantic_ai.messages.BinaryContent): local\\_image\\_input.py 1. To ensure the example is runnable we download this image from the web, but you can also use Path().read_bytes() to read a local file's contents.", "url": "https://ai.pydantic.dev/input/index.html#image-input", "page": "input/index.html", "source_site": "pydantic_ai"}
{"title": "Audio Input", "anchor": "audio-input", "heading_level": 2, "md_text": "Info Some models do not support audio input. Please check the model's documentation to confirm whether it supports audio input. You can provide audio input using either [ AudioUrl ](../api/messages/index.html#pydantic_ai.messages.AudioUrl) or [ BinaryContent ](../api/messages/index.html#pydantic_ai.messages.BinaryContent). The process is analogous to the examples above.", "url": "https://ai.pydantic.dev/input/index.html#audio-input", "page": "input/index.html", "source_site": "pydantic_ai"}
{"title": "Video Input", "anchor": "video-input", "heading_level": 2, "md_text": "Info Some models do not support video input. Please check the model's documentation to confirm whether it supports video input. You can provide video input using either [ VideoUrl ](../api/messages/index.html#pydantic_ai.messages.VideoUrl) or [ BinaryContent ](../api/messages/index.html#pydantic_ai.messages.BinaryContent). The process is analogous to the examples above.", "url": "https://ai.pydantic.dev/input/index.html#video-input", "page": "input/index.html", "source_site": "pydantic_ai"}
{"title": "Document Input", "anchor": "document-input", "heading_level": 2, "md_text": "Info Some models do not support document input. Please check the model's documentation to confirm whether it supports document input. You can provide document input using either [ DocumentUrl ](../api/messages/index.html#pydantic_ai.messages.DocumentUrl) or [ BinaryContent ](../api/messages/index.html#pydantic_ai.messages.BinaryContent). The process is similar to the examples above. If you have a direct URL for the document, you can use [ DocumentUrl ](../api/messages/index.html#pydantic_ai.messages.DocumentUrl): document\\_input.py The supported document formats vary by model. You can also use [ BinaryContent ](../api/messages/index.html#pydantic_ai.messages.BinaryContent) to pass document data directly: binary\\_content\\_input.py", "url": "https://ai.pydantic.dev/input/index.html#document-input", "page": "input/index.html", "source_site": "pydantic_ai"}
{"title": "User-side download vs. direct file URL", "anchor": "user-side-download-vs-direct-file-url", "heading_level": 2, "md_text": "As a general rule, when you provide a URL using any of ImageUrl , AudioUrl , VideoUrl or DocumentUrl , Pydantic AI downloads the file content and then sends it as part of the API request. The situation is different for certain models: * [ AnthropicModel ](../api/models/anthropic/index.html#pydantic_ai.models.anthropic.AnthropicModel): if you provide a PDF document via DocumentUrl , the URL is sent directly in the API request, so no download happens on the user side. * [ GoogleModel ](../api/models/google/index.html#pydantic_ai.models.google.GoogleModel) on Vertex AI: any URL provided using ImageUrl , AudioUrl , VideoUrl , or DocumentUrl is sent as-is in the API request and no data is downloaded beforehand. See the [Gemini API docs for Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/inference#filedata) to learn more about supported URLs, formats and limitations: * Cloud Storage bucket URIs (with protocol gs:// ) * Public HTTP(S) URLs * Public YouTube video URL (maximum one URL per request) However, because of crawling restrictions, it may happen that Gemini can't access certain URLs. In that case, you can instruct Pydantic AI to download the file content and send that instead of the URL by setting the boolean flag force_download to True . This attribute is available on all objects that inherit from [ FileUrl ](../api/messages/index.html#pydantic_ai.messages.FileUrl). * [ GoogleModel ](../api/models/google/index.html#pydantic_ai.models.google.GoogleModel) on GLA: YouTube video URLs are sent directly in the request to the model.", "url": "https://ai.pydantic.dev/input/index.html#user-side-download-vs-direct-file-url", "page": "input/index.html", "source_site": "pydantic_ai"}
{"title": "Installation", "anchor": "installation", "heading_level": 1, "md_text": "Pydantic AI is available on PyPI as [ pydantic-ai ](https://pypi.org/project/pydantic-ai/) so installation is as simple as: pipuv (Requires Python 3.10+) This installs the pydantic_ai package, core dependencies, and libraries required to use all the models included in Pydantic AI. If you want to install only those dependencies required to use a specific model, you can install the [\"slim\"](index.html#slim-install) version of Pydantic AI. ## Use with Pydantic Logfire Pydantic AI has an excellent (but completely optional) integration with [Pydantic Logfire](https://pydantic.dev/logfire) to help you view and understand agent runs. Logfire comes included with pydantic-ai (but not the [\"slim\" version](index.html#slim-install)), so you can typically start using it immediately by following the [Logfire setup docs](../logfire/index.html#using-logfire). ## Running Examples We distribute the [ pydantic_ai_examples ](https://github.com/pydantic/pydantic-ai/tree/main/examples/pydantic_ai_examples) directory as a separate PyPI package ([ pydantic-ai-examples ](https://pypi.org/project/pydantic-ai-examples/)) to make examples extremely easy to customize and run. To install examples, use the examples optional group: pipuv To run the examples, follow instructions in the [examples docs](../examples/setup/index.html). ## Slim Install If you know which model you're going to use and want to avoid installing superfluous packages, you can use the [ pydantic-ai-slim ](https://pypi.org/project/pydantic-ai-slim/) package. For example, if you're using just [ OpenAIChatModel ](../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIChatModel), you would run: pipuv pydantic-ai-slim has the following optional groups: * logfire  installs [ logfire ](../logfire/index.html) [PyPI ](https://pypi.org/project/logfire) * evals  installs [ pydantic-evals ](../evals/index.html) [PyPI ](https://pypi.org/project/pydantic-evals) * openai  installs openai [PyPI ](https://pypi.org/project/openai) * vertexai  installs google-auth [PyPI ](https://pypi.org/project/google-auth) and requests [PyPI ](https://pypi.org/project/requests) * google  installs google-genai [PyPI ](https://pypi.org/project/google-genai) * anthropic  installs anthropic [PyPI ](https://pypi.org/project/anthropic) * groq  installs groq [PyPI ](https://pypi.org/project/groq) * mistral  installs mistralai [PyPI ](https://pypi.org/project/mistralai) * cohere - installs cohere [PyPI ](https://pypi.org/project/cohere) * bedrock - installs boto3 [PyPI ](https://pypi.org/project/boto3) * huggingface - installs huggingface-hub[inference] [PyPI ](https://pypi.org/project/huggingface-hub) * duckduckgo - installs ddgs [PyPI ](https://pypi.org/project/ddgs) * tavily - installs tavily-python [PyPI ](https://pypi.org/project/tavily-python) * cli - installs rich [PyPI ](https://pypi.org/project/rich), prompt-toolkit [PyPI ](https://pypi.org/project/prompt-toolkit), and argcomplete [PyPI ](https://pypi.org/project/argcomplete) * mcp - installs mcp [PyPI ](https://pypi.org/project/mcp) * fastmcp - installs fastmcp [PyPI ](https://pypi.org/project/fastmcp) * a2a - installs fasta2a [PyPI ](https://pypi.org/project/fasta2a) * ag-ui - installs ag-ui-protocol [PyPI ](https://pypi.org/project/ag-ui-protocol) and starlette [PyPI ](https://pypi.org/project/starlette) * dbos - installs [ dbos ](../durable_execution/dbos/index.html) [PyPI ](https://pypi.org/project/dbos) * prefect - installs [ prefect ](../durable_execution/prefect/index.html) [PyPI ](https://pypi.org/project/prefect) See the [models](../models/overview/index.html) documentation for information on which optional dependencies are required for each model. You can also install dependencies for multiple models and use cases, for example: pipuv", "url": "https://ai.pydantic.dev/install/index.html#installation", "page": "install/index.html", "source_site": "pydantic_ai"}
{"title": "Use with Pydantic Logfire", "anchor": "use-with-pydantic-logfire", "heading_level": 2, "md_text": "Pydantic AI has an excellent (but completely optional) integration with [Pydantic Logfire](https://pydantic.dev/logfire) to help you view and understand agent runs. Logfire comes included with pydantic-ai (but not the [\"slim\" version](index.html#slim-install)), so you can typically start using it immediately by following the [Logfire setup docs](../logfire/index.html#using-logfire).", "url": "https://ai.pydantic.dev/install/index.html#use-with-pydantic-logfire", "page": "install/index.html", "source_site": "pydantic_ai"}
{"title": "Running Examples", "anchor": "running-examples", "heading_level": 2, "md_text": "We distribute the [ pydantic_ai_examples ](https://github.com/pydantic/pydantic-ai/tree/main/examples/pydantic_ai_examples) directory as a separate PyPI package ([ pydantic-ai-examples ](https://pypi.org/project/pydantic-ai-examples/)) to make examples extremely easy to customize and run. To install examples, use the examples optional group: pipuv To run the examples, follow instructions in the [examples docs](../examples/setup/index.html).", "url": "https://ai.pydantic.dev/install/index.html#running-examples", "page": "install/index.html", "source_site": "pydantic_ai"}
{"title": "Slim Install", "anchor": "slim-install", "heading_level": 2, "md_text": "If you know which model you're going to use and want to avoid installing superfluous packages, you can use the [ pydantic-ai-slim ](https://pypi.org/project/pydantic-ai-slim/) package. For example, if you're using just [ OpenAIChatModel ](../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIChatModel), you would run: pipuv pydantic-ai-slim has the following optional groups: * logfire  installs [ logfire ](../logfire/index.html) [PyPI ](https://pypi.org/project/logfire) * evals  installs [ pydantic-evals ](../evals/index.html) [PyPI ](https://pypi.org/project/pydantic-evals) * openai  installs openai [PyPI ](https://pypi.org/project/openai) * vertexai  installs google-auth [PyPI ](https://pypi.org/project/google-auth) and requests [PyPI ](https://pypi.org/project/requests) * google  installs google-genai [PyPI ](https://pypi.org/project/google-genai) * anthropic  installs anthropic [PyPI ](https://pypi.org/project/anthropic) * groq  installs groq [PyPI ](https://pypi.org/project/groq) * mistral  installs mistralai [PyPI ](https://pypi.org/project/mistralai) * cohere - installs cohere [PyPI ](https://pypi.org/project/cohere) * bedrock - installs boto3 [PyPI ](https://pypi.org/project/boto3) * huggingface - installs huggingface-hub[inference] [PyPI ](https://pypi.org/project/huggingface-hub) * duckduckgo - installs ddgs [PyPI ](https://pypi.org/project/ddgs) * tavily - installs tavily-python [PyPI ](https://pypi.org/project/tavily-python) * cli - installs rich [PyPI ](https://pypi.org/project/rich), prompt-toolkit [PyPI ](https://pypi.org/project/prompt-toolkit), and argcomplete [PyPI ](https://pypi.org/project/argcomplete) * mcp - installs mcp [PyPI ](https://pypi.org/project/mcp) * fastmcp - installs fastmcp [PyPI ](https://pypi.org/project/fastmcp) * a2a - installs fasta2a [PyPI ](https://pypi.org/project/fasta2a) * ag-ui - installs ag-ui-protocol [PyPI ](https://pypi.org/project/ag-ui-protocol) and starlette [PyPI ](https://pypi.org/project/starlette) * dbos - installs [ dbos ](../durable_execution/dbos/index.html) [PyPI ](https://pypi.org/project/dbos) * prefect - installs [ prefect ](../durable_execution/prefect/index.html) [PyPI ](https://pypi.org/project/prefect) See the [models](../models/overview/index.html) documentation for information on which optional dependencies are required for each model. You can also install dependencies for multiple models and use cases, for example: pipuv", "url": "https://ai.pydantic.dev/install/index.html#slim-install", "page": "install/index.html", "source_site": "pydantic_ai"}
{"title": "Steps", "anchor": "steps", "heading_level": 1, "md_text": "Steps are the fundamental units of work in a graph. They're async functions that receive a [ StepContext ](../../../api/pydantic_graph/beta_step/index.html#pydantic_graph.beta.step.StepContext) and return a value. ## Creating Steps Steps are created using the [ @g.step ](../../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.step) decorator on the [ GraphBuilder ](../../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder): basic\\_step.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ## Step Context Every step function receives a [ StepContext ](../../../api/pydantic_graph/beta_step/index.html#pydantic_graph.beta.step.StepContext) as its first parameter. The context provides access to: * ctx.state - The mutable graph state (type: StateT ) * ctx.deps - Injected dependencies (type: DepsT ) * ctx.inputs - Input data for this step (type: InputT ) ### Accessing State State is shared across all steps in a graph and can be freely mutated: state\\_access.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ### Working with Inputs Steps can receive and transform input data: step\\_inputs.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ## Dependency Injection Steps can access injected dependencies through ctx.deps : dependencies.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ## Customizing Steps ### Custom Node IDs By default, step node IDs are inferred from the function name. You can override this: custom\\_id.py ### Human-Readable Labels Labels provide documentation for diagram generation: labels.py ## Sequential Steps Multiple steps can be chained sequentially: sequential.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* The computation is: (10 + 5) * 2 - 3 = 27 ## Streaming Steps In addition to regular steps that return a single value, you can create streaming steps that yield multiple values over time using the [ @g.stream ](../../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.stream) decorator: streaming\\_step.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ### How Streaming Steps Work Streaming steps return an AsyncIterable that yields values over time. When you use .map() on a streaming step's output, the graph processes each yielded value as it becomes available, creating parallel tasks dynamically. This is particularly useful for: * Processing data from APIs that stream responses * Handling real-time data feeds * Progressive processing of large datasets * Any scenario where you want to start processing results before all data is available Like regular steps, streaming steps can also have custom node IDs and labels: labeled\\_stream.py ## Edge Building Convenience Methods The builder provides helper methods for common edge patterns: ### Simple Edges with add_edge() add\\_edge\\_example.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ## Type Safety The beta graph API provides strong type checking through generics. Type parameters on [ StepContext ](../../../api/pydantic_graph/beta_step/index.html#pydantic_graph.beta.step.StepContext) ensure: * State access is properly typed * Dependencies are correctly typed * Input/output types match across edges ## Next Steps * Learn about [parallel execution](../parallel/index.html) with broadcasting and mapping * Understand [join nodes](../joins/index.html) for aggregating parallel results * Explore [conditional branching](../decisions/index.html) with decision nodes", "url": "https://ai.pydantic.dev/graph/beta/steps/index.html#steps", "page": "graph/beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Creating Steps", "anchor": "creating-steps", "heading_level": 2, "md_text": "Steps are created using the [ @g.step ](../../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.step) decorator on the [ GraphBuilder ](../../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder): basic\\_step.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/steps/index.html#creating-steps", "page": "graph/beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Step Context", "anchor": "step-context", "heading_level": 2, "md_text": "Every step function receives a [ StepContext ](../../../api/pydantic_graph/beta_step/index.html#pydantic_graph.beta.step.StepContext) as its first parameter. The context provides access to: * ctx.state - The mutable graph state (type: StateT ) * ctx.deps - Injected dependencies (type: DepsT ) * ctx.inputs - Input data for this step (type: InputT ) ### Accessing State State is shared across all steps in a graph and can be freely mutated: state\\_access.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ### Working with Inputs Steps can receive and transform input data: step\\_inputs.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/steps/index.html#step-context", "page": "graph/beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Accessing State", "anchor": "accessing-state", "heading_level": 3, "md_text": "State is shared across all steps in a graph and can be freely mutated: state\\_access.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/steps/index.html#accessing-state", "page": "graph/beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Working with Inputs", "anchor": "working-with-inputs", "heading_level": 3, "md_text": "Steps can receive and transform input data: step\\_inputs.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/steps/index.html#working-with-inputs", "page": "graph/beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Dependency Injection", "anchor": "dependency-injection", "heading_level": 2, "md_text": "Steps can access injected dependencies through ctx.deps : dependencies.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/steps/index.html#dependency-injection", "page": "graph/beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Customizing Steps", "anchor": "customizing-steps", "heading_level": 2, "md_text": "### Custom Node IDs By default, step node IDs are inferred from the function name. You can override this: custom\\_id.py ### Human-Readable Labels Labels provide documentation for diagram generation: labels.py", "url": "https://ai.pydantic.dev/graph/beta/steps/index.html#customizing-steps", "page": "graph/beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Node IDs", "anchor": "custom-node-ids", "heading_level": 3, "md_text": "By default, step node IDs are inferred from the function name. You can override this: custom\\_id.py", "url": "https://ai.pydantic.dev/graph/beta/steps/index.html#custom-node-ids", "page": "graph/beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Human-Readable Labels", "anchor": "human-readable-labels", "heading_level": 3, "md_text": "Labels provide documentation for diagram generation: labels.py", "url": "https://ai.pydantic.dev/graph/beta/steps/index.html#human-readable-labels", "page": "graph/beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Sequential Steps", "anchor": "sequential-steps", "heading_level": 2, "md_text": "Multiple steps can be chained sequentially: sequential.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* The computation is: (10 + 5) * 2 - 3 = 27", "url": "https://ai.pydantic.dev/graph/beta/steps/index.html#sequential-steps", "page": "graph/beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Streaming Steps", "anchor": "streaming-steps", "heading_level": 2, "md_text": "In addition to regular steps that return a single value, you can create streaming steps that yield multiple values over time using the [ @g.stream ](../../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.stream) decorator: streaming\\_step.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ### How Streaming Steps Work Streaming steps return an AsyncIterable that yields values over time. When you use .map() on a streaming step's output, the graph processes each yielded value as it becomes available, creating parallel tasks dynamically. This is particularly useful for: * Processing data from APIs that stream responses * Handling real-time data feeds * Progressive processing of large datasets * Any scenario where you want to start processing results before all data is available Like regular steps, streaming steps can also have custom node IDs and labels: labeled\\_stream.py", "url": "https://ai.pydantic.dev/graph/beta/steps/index.html#streaming-steps", "page": "graph/beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "How Streaming Steps Work", "anchor": "how-streaming-steps-work", "heading_level": 3, "md_text": "Streaming steps return an AsyncIterable that yields values over time. When you use .map() on a streaming step's output, the graph processes each yielded value as it becomes available, creating parallel tasks dynamically. This is particularly useful for: * Processing data from APIs that stream responses * Handling real-time data feeds * Progressive processing of large datasets * Any scenario where you want to start processing results before all data is available Like regular steps, streaming steps can also have custom node IDs and labels: labeled\\_stream.py", "url": "https://ai.pydantic.dev/graph/beta/steps/index.html#how-streaming-steps-work", "page": "graph/beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Edge Building Convenience Methods", "anchor": "edge-building-convenience-methods", "heading_level": 2, "md_text": "The builder provides helper methods for common edge patterns: ### Simple Edges with add_edge() add\\_edge\\_example.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/steps/index.html#edge-building-convenience-methods", "page": "graph/beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Simple Edges with add_edge()", "anchor": "simple-edges-with-add_edge", "heading_level": 3, "md_text": "add\\_edge\\_example.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/steps/index.html#simple-edges-with-add_edge", "page": "graph/beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Type Safety", "anchor": "type-safety", "heading_level": 2, "md_text": "The beta graph API provides strong type checking through generics. Type parameters on [ StepContext ](../../../api/pydantic_graph/beta_step/index.html#pydantic_graph.beta.step.StepContext) ensure: * State access is properly typed * Dependencies are correctly typed * Input/output types match across edges", "url": "https://ai.pydantic.dev/graph/beta/steps/index.html#type-safety", "page": "graph/beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "* Learn about [parallel execution](../parallel/index.html) with broadcasting and mapping * Understand [join nodes](../joins/index.html) for aggregating parallel results * Explore [conditional branching](../decisions/index.html) with decision nodes", "url": "https://ai.pydantic.dev/graph/beta/steps/index.html#next-steps", "page": "graph/beta/steps/index.html", "source_site": "pydantic_ai"}
{"title": "FastMCP Client", "anchor": "fastmcp-client", "heading_level": 1, "md_text": "[FastMCP](https://gofastmcp.com/) is a higher-level MCP framework that bills itself as \"The fast, Pythonic way to build MCP servers and clients.\" It supports additional capabilities on top of the MCP specification like [Tool Transformation](https://gofastmcp.com/patterns/tool-transformation), [OAuth](https://gofastmcp.com/clients/auth/oauth), and more. As an alternative to Pydantic AI's standard [ MCPServer MCP client](../client/index.html) built on the [MCP SDK](https://github.com/modelcontextprotocol/python-sdk), you can use the [ FastMCPToolset ](../../api/toolsets/index.html#pydantic_ai.toolsets.fastmcp.FastMCPToolset) [toolset](../../toolsets/index.html) that leverages the [FastMCP Client](https://gofastmcp.com/clients/) to connect to local and remote MCP servers, whether or not they're built using [FastMCP Server](https://gofastmcp.com/servers/). Note that it does not yet support integration elicitation or sampling, which are supported by the [standard MCPServer client](../client/index.html). ## Install To use the FastMCPToolset , you will need to install [ pydantic-ai-slim ](../../install/index.html#slim-install) with the fastmcp optional group: pipuv ## Usage A FastMCPToolset can then be created from: * A FastMCP Server: FastMCPToolset(fastmcp.FastMCP('my_server')) * A FastMCP Client: FastMCPToolset(fastmcp.Client(...)) * A FastMCP Transport: FastMCPToolset(fastmcp.StdioTransport(command='uvx', args=['mcp-run-python', 'stdio'])) * A Streamable HTTP URL: FastMCPToolset('http://localhost:8000/mcp') * An HTTP SSE URL: FastMCPToolset('http://localhost:8000/sse') * A Python Script: FastMCPToolset('my_server.py') * A Node.js Script: FastMCPToolset('my_server.js') * A JSON MCP Configuration: FastMCPToolset({'mcpServers': {'my_server': {'command': 'uvx', 'args': ['mcp-run-python', 'stdio']}}}) If you already have a [FastMCP Server](https://gofastmcp.com/servers) in the same codebase as your Pydantic AI agent, you can create a FastMCPToolset directly from it and save agent a network round trip: *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* Connecting your agent to a Streamable HTTP MCP Server is as simple as: *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* You can also create a FastMCPToolset from a JSON MCP Configuration: *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/mcp/fastmcp-client/index.html#fastmcp-client", "page": "mcp/fastmcp-client/index.html", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "heading_level": 2, "md_text": "To use the FastMCPToolset , you will need to install [ pydantic-ai-slim ](../../install/index.html#slim-install) with the fastmcp optional group: pipuv", "url": "https://ai.pydantic.dev/mcp/fastmcp-client/index.html#install", "page": "mcp/fastmcp-client/index.html", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 2, "md_text": "A FastMCPToolset can then be created from: * A FastMCP Server: FastMCPToolset(fastmcp.FastMCP('my_server')) * A FastMCP Client: FastMCPToolset(fastmcp.Client(...)) * A FastMCP Transport: FastMCPToolset(fastmcp.StdioTransport(command='uvx', args=['mcp-run-python', 'stdio'])) * A Streamable HTTP URL: FastMCPToolset('http://localhost:8000/mcp') * An HTTP SSE URL: FastMCPToolset('http://localhost:8000/sse') * A Python Script: FastMCPToolset('my_server.py') * A Node.js Script: FastMCPToolset('my_server.js') * A JSON MCP Configuration: FastMCPToolset({'mcpServers': {'my_server': {'command': 'uvx', 'args': ['mcp-run-python', 'stdio']}}}) If you already have a [FastMCP Server](https://gofastmcp.com/servers) in the same codebase as your Pydantic AI agent, you can create a FastMCPToolset directly from it and save agent a network round trip: *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* Connecting your agent to a Streamable HTTP MCP Server is as simple as: *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* You can also create a FastMCPToolset from a JSON MCP Configuration: *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/mcp/fastmcp-client/index.html#usage", "page": "mcp/fastmcp-client/index.html", "source_site": "pydantic_ai"}
{"title": "Model Context Protocol (MCP)", "anchor": "model-context-protocol-mcp", "heading_level": 1, "md_text": "Pydantic AI supports [Model Context Protocol (MCP)](https://modelcontextprotocol.io) in multiple ways: 1. [Agents](../../agents/index.html) can connect to MCP servers and use their tools using three different methods: 1. Pydantic AI can act as an MCP client and connect directly to local and remote MCP servers. [Learn more](../client/index.html) about [ MCPServer ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServer). 2. Pydantic AI can use the [FastMCP Client](https://gofastmcp.com/clients/client/) to connect to local and remote MCP servers, whether or not they're built using [FastMCP Server](https://gofastmcp.com/servers). [Learn more](../fastmcp-client/index.html) about [ FastMCPToolset ](../../api/toolsets/index.html#pydantic_ai.toolsets.fastmcp.FastMCPToolset). 3. Some model providers can themselves connect to remote MCP servers using a \"built-in tool\". [Learn more](../../builtin-tools/index.html#mcp-server-tool) about [ MCPServerTool ](../../api/builtin_tools/index.html#pydantic_ai.builtin_tools.MCPServerTool). 2. Agents can be used within MCP servers. [Learn more](../server/index.html) ## What is MCP? The Model Context Protocol is a standardized protocol that allow AI applications (including programmatic agents like Pydantic AI, coding agents like [cursor](https://www.cursor.com/), and desktop applications like [Claude Desktop](https://claude.ai/download)) to connect to external tools and services using a common interface. As with other protocols, the dream of MCP is that a wide range of applications can speak to each other without the need for specific integrations. There is a great list of MCP servers at [github.com/modelcontextprotocol/servers](https://github.com/modelcontextprotocol/servers). Some examples of what this means: * Pydantic AI could use a web search service implemented as an MCP server to implement a deep research agent * Cursor could connect to the [Pydantic Logfire](https://github.com/pydantic/logfire-mcp) MCP server to search logs, traces and metrics to gain context while fixing a bug * Pydantic AI, or any other MCP client could connect to our [Run Python](https://github.com/pydantic/mcp-run-python) MCP server to run arbitrary Python code in a sandboxed environment", "url": "https://ai.pydantic.dev/mcp/overview/index.html#model-context-protocol-mcp", "page": "mcp/overview/index.html", "source_site": "pydantic_ai"}
{"title": "What is MCP?", "anchor": "what-is-mcp", "heading_level": 2, "md_text": "The Model Context Protocol is a standardized protocol that allow AI applications (including programmatic agents like Pydantic AI, coding agents like [cursor](https://www.cursor.com/), and desktop applications like [Claude Desktop](https://claude.ai/download)) to connect to external tools and services using a common interface. As with other protocols, the dream of MCP is that a wide range of applications can speak to each other without the need for specific integrations. There is a great list of MCP servers at [github.com/modelcontextprotocol/servers](https://github.com/modelcontextprotocol/servers). Some examples of what this means: * Pydantic AI could use a web search service implemented as an MCP server to implement a deep research agent * Cursor could connect to the [Pydantic Logfire](https://github.com/pydantic/logfire-mcp) MCP server to search logs, traces and metrics to gain context while fixing a bug * Pydantic AI, or any other MCP client could connect to our [Run Python](https://github.com/pydantic/mcp-run-python) MCP server to run arbitrary Python code in a sandboxed environment", "url": "https://ai.pydantic.dev/mcp/overview/index.html#what-is-mcp", "page": "mcp/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Parallel Execution", "anchor": "parallel-execution", "heading_level": 1, "md_text": "The beta graph API provides two powerful mechanisms for parallel execution: **broadcasting** and **mapping**. ## Overview * **Broadcasting** - Send the same data to multiple parallel paths * **Spreading** - Fan out items from an iterable to parallel paths Both create \"forks\" in the execution graph that can later be synchronized with [join nodes](../joins/index.html). ## Broadcasting Broadcasting sends identical data to multiple destinations simultaneously: basic\\_broadcast.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* All three steps receive the same input value ( 10 ) and execute in parallel. ## Spreading Spreading fans out elements from an iterable, processing each element in parallel: basic\\_map.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ### Spreading AsyncIterables The .map() operation also works with AsyncIterable values. When mapping over an async iterable, the graph creates parallel tasks dynamically as values are yielded. This is particularly useful for streaming data or processing data that's being generated on-the-fly: async\\_iterable\\_map.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* This allows for progressive processing where downstream steps can start working on early results while later results are still being generated. ### Using add_mapping_edge() The convenience method [ add_mapping_edge() ](../../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.add_mapping_edge) provides a simpler syntax: mapping\\_convenience.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ## Empty Iterables When mapping an empty iterable, you can specify a downstream_join_id to ensure the join still executes: empty\\_map.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ## Nested Parallel Operations You can nest broadcasts and maps for complex parallel patterns: ### Spread then Broadcast map\\_then\\_broadcast.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* The result contains: - From 10: 10+1=11 and 10+2=12 - From 20: 20+1=21 and 20+2=22 ### Multiple Sequential Spreads sequential\\_maps.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ## Edge Labels Add labels to parallel edges for better documentation: labeled\\_parallel.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ## State Sharing in Parallel Execution All parallel tasks share the same graph state. Be careful with mutations: parallel\\_state.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ## Edge Transformations You can transform data inline as it flows along edges using the .transform() method: edge\\_transform.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* The transform function receives a [ StepContext ](../../../api/pydantic_graph/beta_step/index.html#pydantic_graph.beta.step.StepContext) with the current inputs and has access to state and dependencies. This is useful for: * Converting data types between incompatible steps * Extracting specific fields from complex objects * Applying simple computations without creating a full step * Adapting data formats during routing Transforms can be chained and combined with other edge operations like .map() and .label() : chained\\_transforms.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ## Next Steps * Learn about [join nodes](../joins/index.html) for aggregating parallel results * Explore [conditional branching](../decisions/index.html) with decision nodes * See the [steps documentation](../steps/index.html) for more on step execution", "url": "https://ai.pydantic.dev/graph/beta/parallel/index.html#parallel-execution", "page": "graph/beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "* **Broadcasting** - Send the same data to multiple parallel paths * **Spreading** - Fan out items from an iterable to parallel paths Both create \"forks\" in the execution graph that can later be synchronized with [join nodes](../joins/index.html).", "url": "https://ai.pydantic.dev/graph/beta/parallel/index.html#overview", "page": "graph/beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "Broadcasting", "anchor": "broadcasting", "heading_level": 2, "md_text": "Broadcasting sends identical data to multiple destinations simultaneously: basic\\_broadcast.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* All three steps receive the same input value ( 10 ) and execute in parallel.", "url": "https://ai.pydantic.dev/graph/beta/parallel/index.html#broadcasting", "page": "graph/beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "Spreading", "anchor": "spreading", "heading_level": 2, "md_text": "Spreading fans out elements from an iterable, processing each element in parallel: basic\\_map.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ### Spreading AsyncIterables The .map() operation also works with AsyncIterable values. When mapping over an async iterable, the graph creates parallel tasks dynamically as values are yielded. This is particularly useful for streaming data or processing data that's being generated on-the-fly: async\\_iterable\\_map.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* This allows for progressive processing where downstream steps can start working on early results while later results are still being generated. ### Using add_mapping_edge() The convenience method [ add_mapping_edge() ](../../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.add_mapping_edge) provides a simpler syntax: mapping\\_convenience.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/parallel/index.html#spreading", "page": "graph/beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "Spreading AsyncIterables", "anchor": "spreading-asynciterables", "heading_level": 3, "md_text": "The .map() operation also works with AsyncIterable values. When mapping over an async iterable, the graph creates parallel tasks dynamically as values are yielded. This is particularly useful for streaming data or processing data that's being generated on-the-fly: async\\_iterable\\_map.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* This allows for progressive processing where downstream steps can start working on early results while later results are still being generated.", "url": "https://ai.pydantic.dev/graph/beta/parallel/index.html#spreading-asynciterables", "page": "graph/beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "Using add_mapping_edge()", "anchor": "using-add_mapping_edge", "heading_level": 3, "md_text": "The convenience method [ add_mapping_edge() ](../../../api/pydantic_graph/beta_graph_builder/index.html#pydantic_graph.beta.graph_builder.GraphBuilder.add_mapping_edge) provides a simpler syntax: mapping\\_convenience.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/parallel/index.html#using-add_mapping_edge", "page": "graph/beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "Empty Iterables", "anchor": "empty-iterables", "heading_level": 2, "md_text": "When mapping an empty iterable, you can specify a downstream_join_id to ensure the join still executes: empty\\_map.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/parallel/index.html#empty-iterables", "page": "graph/beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "Nested Parallel Operations", "anchor": "nested-parallel-operations", "heading_level": 2, "md_text": "You can nest broadcasts and maps for complex parallel patterns: ### Spread then Broadcast map\\_then\\_broadcast.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* The result contains: - From 10: 10+1=11 and 10+2=12 - From 20: 20+1=21 and 20+2=22 ### Multiple Sequential Spreads sequential\\_maps.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/parallel/index.html#nested-parallel-operations", "page": "graph/beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "Spread then Broadcast", "anchor": "spread-then-broadcast", "heading_level": 3, "md_text": "map\\_then\\_broadcast.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* The result contains: - From 10: 10+1=11 and 10+2=12 - From 20: 20+1=21 and 20+2=22", "url": "https://ai.pydantic.dev/graph/beta/parallel/index.html#spread-then-broadcast", "page": "graph/beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "Multiple Sequential Spreads", "anchor": "multiple-sequential-spreads", "heading_level": 3, "md_text": "sequential\\_maps.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/parallel/index.html#multiple-sequential-spreads", "page": "graph/beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "Edge Labels", "anchor": "edge-labels", "heading_level": 2, "md_text": "Add labels to parallel edges for better documentation: labeled\\_parallel.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/parallel/index.html#edge-labels", "page": "graph/beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "State Sharing in Parallel Execution", "anchor": "state-sharing-in-parallel-execution", "heading_level": 2, "md_text": "All parallel tasks share the same graph state. Be careful with mutations: parallel\\_state.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/parallel/index.html#state-sharing-in-parallel-execution", "page": "graph/beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "Edge Transformations", "anchor": "edge-transformations", "heading_level": 2, "md_text": "You can transform data inline as it flows along edges using the .transform() method: edge\\_transform.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* The transform function receives a [ StepContext ](../../../api/pydantic_graph/beta_step/index.html#pydantic_graph.beta.step.StepContext) with the current inputs and has access to state and dependencies. This is useful for: * Converting data types between incompatible steps * Extracting specific fields from complex objects * Applying simple computations without creating a full step * Adapting data formats during routing Transforms can be chained and combined with other edge operations like .map() and .label() : chained\\_transforms.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/parallel/index.html#edge-transformations", "page": "graph/beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "* Learn about [join nodes](../joins/index.html) for aggregating parallel results * Explore [conditional branching](../decisions/index.html) with decision nodes * See the [steps documentation](../steps/index.html) for more on step execution", "url": "https://ai.pydantic.dev/graph/beta/parallel/index.html#next-steps", "page": "graph/beta/parallel/index.html", "source_site": "pydantic_ai"}
{"title": "Pydantic Logfire Debugging and Monitoring", "anchor": "pydantic-logfire-debugging-and-monitoring", "heading_level": 1, "md_text": "Applications that use LLMs have some challenges that are well known and understood: LLMs are **slow**, **unreliable** and **expensive**. These applications also have some challenges that most developers have encountered much less often: LLMs are **fickle** and **non-deterministic**. Subtle changes in a prompt can completely change a model's performance, and there's no EXPLAIN query you can run to understand why. Warning From a software engineers point of view, you can think of LLMs as the worst database you've ever heard of, but worse. If LLMs weren't so bloody useful, we'd never touch them. To build successful applications with LLMs, we need new tools to understand both model performance, and the behavior of applications that rely on them. LLM Observability tools that just let you understand how your model is performing are useless: making API calls to an LLM is easy, it's building that into an application that's hard. ## Pydantic Logfire [Pydantic Logfire](https://pydantic.dev/logfire) is an observability platform developed by the team who created and maintain Pydantic Validation and Pydantic AI. Logfire aims to let you understand your entire application: Gen AI, classic predictive AI, HTTP traffic, database queries and everything else a modern application needs, all using OpenTelemetry. Pydantic Logfire is a commercial product Logfire is a commercially supported, hosted platform with an extremely generous and perpetual [free tier](https://pydantic.dev/pricing/). You can sign up and start using Logfire in a couple of minutes. Logfire can also be self-hosted on the enterprise tier. Pydantic AI has built-in (but optional) support for Logfire. That means if the logfire package is installed and configured and agent instrumentation is enabled then detailed information about agent runs is sent to Logfire. Otherwise there's virtually no overhead and nothing is sent. Here's an example showing details of running the [Weather Agent](../examples/weather-agent/index.html) in Logfire: [![Weather Agent Logfire](../img/logfire-weather-agent.png)](../img/logfire-weather-agent.png) A trace is generated for the agent run, and spans are emitted for each model request and tool call. ## Using Logfire To use Logfire, you'll need a Logfire [account](https://logfire.pydantic.dev). The Logfire Python SDK is included with pydantic-ai : pipuv Or if you're using the slim package, you can install it with the logfire optional group: pipuv Then authenticate your local environment with Logfire: pipuv And configure a project to send data to: pipuv (Or use an existing project with logfire projects use ) This will write to a .logfire directory in the current working directory, which the Logfire SDK will use for configuration at run time. With that, you can start using Logfire to instrument Pydantic AI code: instrument\\_pydantic\\_ai.py 1. [ logfire.configure() ](https://logfire.pydantic.dev/docs/reference/api/logfire/#logfire.configure) configures the SDK, by default it will find the write token from the .logfire directory, but you can also pass a token directly. 2. [ logfire.instrument_pydantic_ai() ](https://logfire.pydantic.dev/docs/reference/api/logfire/#logfire.Logfire.instrument_pydantic_ai) enables instrumentation of Pydantic AI. 3. Since we've enabled instrumentation, a trace will be generated for each run, with spans emitted for models calls and tool function execution *(This example is complete, it can be run \"as is\")* Which will display in Logfire thus: [![Logfire Simple Agent Run](../img/logfire-simple-agent.png)](../img/logfire-simple-agent.png) The [Logfire documentation](https://logfire.pydantic.dev/docs/) has more details on how to use Logfire, including how to instrument other libraries like [HTTPX](https://logfire.pydantic.dev/docs/integrations/http-clients/httpx/) and [FastAPI](https://logfire.pydantic.dev/docs/integrations/web-frameworks/fastapi/). Since Logfire is built on [OpenTelemetry](https://opentelemetry.io/), you can use the Logfire Python SDK to send data to any OpenTelemetry collector, see [below](index.html#using-opentelemetry). ### Debugging To demonstrate how Logfire can let you visualise the flow of a Pydantic AI run, here's the view you get from Logfire while running the [chat app examples](../examples/chat-app/index.html): ### Monitoring Performance We can also query data with SQL in Logfire to monitor the performance of an application. Here's a real world example of using Logfire to monitor Pydantic AI runs inside Logfire itself: [![Logfire monitoring Pydantic AI](../img/logfire-monitoring-pydanticai.png)](../img/logfire-monitoring-pydanticai.png) ### Monitoring HTTP Requests \\\"F\\*\\*k you, show me the prompt.\\\" As per Hamel Husain's influential 2024 blog post [\"Fuck You, Show Me The Prompt.\"](https://hamel.dev/blog/posts/prompt/) (bear with the capitalization, the point is valid), it's often useful to be able to view the raw HTTP requests and responses made to model providers. To observe raw HTTP requests made to model providers, you can use Logfire's [HTTPX instrumentation](https://logfire.pydantic.dev/docs/integrations/http-clients/httpx/) since all provider SDKs use the [HTTPX](https://www.python-httpx.org/) library internally. With HTTP instrumentationWithout HTTP instrumentation with\\_logfire\\_instrument\\_httpx.py 1. See the [ logfire.instrument_httpx docs](https://logfire.pydantic.dev/docs/reference/api/logfire/#logfire.Logfire.instrument_httpx) more details, capture_all=True means both headers and body are captured for both the request and response. [![Logfire with HTTPX instrumentation](../img/logfire-with-httpx.png)](../img/logfire-with-httpx.png) without\\_logfire\\_instrument\\_httpx.py [![Logfire without HTTPX instrumentation](../img/logfire-without-httpx.png)](../img/logfire-without-httpx.png) ## Using OpenTelemetry Pydantic AI's instrumentation uses [OpenTelemetry](https://opentelemetry.io/) (OTel), which Logfire is based on. This means you can debug and monitor Pydantic AI with any OpenTelemetry backend. Pydantic AI follows the [OpenTelemetry Semantic Conventions for Generative AI systems](https://opentelemetry.io/docs/specs/semconv/gen-ai/), so while we think you'll have the best experience using the Logfire platform ![](https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f609.svg \":wink:\"), you should be able to use any OTel service with GenAI support. ### Logfire with an alternative OTel backend You can use the Logfire SDK completely freely and send the data to any OpenTelemetry backend. Here's an example of configuring the Logfire library to send data to the excellent [otel-tui](https://github.com/ymtdzzz/otel-tui)  an open source terminal based OTel backend and viewer (no association with Pydantic Validation). Run otel-tui with docker (see [the otel-tui readme](https://github.com/ymtdzzz/otel-tui) for more instructions): Terminal then run, otel\\_tui.py 1. Set the OTEL_EXPORTER_OTLP_ENDPOINT environment variable to the URL of your OpenTelemetry backend. If you're using a backend that requires authentication, you may need to set [other environment variables](https://opentelemetry.io/docs/languages/sdk-configuration/otlp-exporter/). Of course, these can also be set outside the process, e.g. with export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318 . 2. We [configure](https://logfire.pydantic.dev/docs/reference/api/logfire/#logfire.configure) Logfire to disable sending data to the Logfire OTel backend itself. If you removed send_to_logfire=False , data would be sent to both Logfire and your OpenTelemetry backend. Running the above code will send tracing data to otel-tui , which will display like this: [![otel tui simple](../img/otel-tui-simple.png)](../img/otel-tui-simple.png) Running the [weather agent](../examples/weather-agent/index.html) example connected to otel-tui shows how it can be used to visualise a more complex trace: [![otel tui weather agent](../img/otel-tui-weather.png)](../img/otel-tui-weather.png) For more information on using the Logfire SDK to send data to alternative backends, see [the Logfire documentation](https://logfire.pydantic.dev/docs/how-to-guides/alternative-backends/). ### OTel without Logfire You can also emit OpenTelemetry data from Pydantic AI without using Logfire at all. To do this, you'll need to install and configure the OpenTelemetry packages you need. To run the following examples, use Terminal raw\\_otel.py ### Alternative Observability backends Because Pydantic AI uses OpenTelemetry for observability, you can easily configure it to send data to any OpenTelemetry-compatible backend, not just our observability platform [Pydantic Logfire](index.html#pydantic-logfire). The following providers have dedicated documentation on Pydantic AI: * [Langfuse](https://langfuse.com/docs/integrations/pydantic-ai) * [W&B Weave](https://weave-docs.wandb.ai/guides/integrations/pydantic_ai/) * [Arize](https://arize.com/docs/ax/observe/tracing-integrations-auto/pydantic-ai) * [Openlayer](https://www.openlayer.com/docs/integrations/pydantic-ai) * [OpenLIT](https://docs.openlit.io/latest/integrations/pydantic) * [LangWatch](https://docs.langwatch.ai/integration/python/integrations/pydantic-ai) * [Patronus AI](https://docs.patronus.ai/docs/percival/pydantic) * [Opik](https://www.comet.com/docs/opik/tracing/integrations/pydantic-ai) * [mlflow](https://mlflow.org/docs/latest/genai/tracing/integrations/listing/pydantic_ai) * [Agenta](https://docs.agenta.ai/observability/integrations/pydanticai) * [Confident AI](https://documentation.confident-ai.com/docs/llm-tracing/integrations/pydanticai) * [LangWatch](https://docs.langwatch.ai/integration/python/integrations/pydantic-ai) ## Advanced usage ### Configuring data format Pydantic AI follows the [OpenTelemetry Semantic Conventions for Generative AI systems](https://opentelemetry.io/docs/specs/semconv/gen-ai/). Specifically, it follows version 1.37.0 of the conventions by default, with a few exceptions. Certain span and attribute names are not spec compliant by default for compatibility reasons, but can be made compliant by passing [ InstrumentationSettings(version=3) ](../api/models/instrumented/index.html#pydantic_ai.models.instrumented.InstrumentationSettings) (the default is currently version=2 ). This will change the following: * The span name agent run becomes invoke_agent {gen_ai.agent.name} (with the agent name filled in) * The span name running tool becomes execute_tool {gen_ai.tool.name} (with the tool name filled in) * The attribute name tool_arguments becomes gen_ai.tool.call.arguments * The attribute name tool_response becomes gen_ai.tool.call.result To use [OpenTelemetry semantic conventions version 1.36.0](https://github.com/open-telemetry/semantic-conventions/blob/v1.36.0/docs/gen-ai/README.md) or older, pass [ InstrumentationSettings(version=1) ](../api/models/instrumented/index.html#pydantic_ai.models.instrumented.InstrumentationSettings). Moreover, those semantic conventions specify that messages should be captured as individual events (logs) that are children of the request span, whereas by default, Pydantic AI instead collects these events into a JSON array which is set as a single large attribute called events on the request span. To change this, use event_mode='logs' : instrumentation\\_settings\\_event\\_mode.py This won't look as good in the Logfire UI, and will also be removed from Pydantic AI in a future release, but may be useful for backwards compatibility. Note that the OpenTelemetry Semantic Conventions are still experimental and are likely to change. ### Setting OpenTelemetry SDK providers By default, the global TracerProvider and EventLoggerProvider are used. These are set automatically by logfire.configure() . They can also be set by the set_tracer_provider and set_event_logger_provider functions in the OpenTelemetry Python SDK. You can set custom providers with [ InstrumentationSettings ](../api/models/instrumented/index.html#pydantic_ai.models.instrumented.InstrumentationSettings). instrumentation\\_settings\\_providers.py ### Instrumenting a specific Model instrumented\\_model\\_example.py ### Excluding binary content excluding\\_binary\\_content.py ### Excluding prompts and completions For privacy and security reasons, you may want to monitor your agent's behavior and performance without exposing sensitive user data or proprietary prompts in your observability platform. Pydantic AI allows you to exclude the actual content from instrumentation events while preserving the structural information needed for debugging and monitoring. When include_content=False is set, Pydantic AI will exclude sensitive content from OpenTelemetry events, including user prompts and model completions, tool call arguments and responses, and any other message content. excluding\\_sensitive\\_content.py This setting is particularly useful in production environments where compliance requirements or data sensitivity concerns make it necessary to limit what content is sent to your observability platform.", "url": "https://ai.pydantic.dev/logfire/index.html#pydantic-logfire-debugging-and-monitoring", "page": "logfire/index.html", "source_site": "pydantic_ai"}
{"title": "Pydantic Logfire", "anchor": "pydantic-logfire", "heading_level": 2, "md_text": "[Pydantic Logfire](https://pydantic.dev/logfire) is an observability platform developed by the team who created and maintain Pydantic Validation and Pydantic AI. Logfire aims to let you understand your entire application: Gen AI, classic predictive AI, HTTP traffic, database queries and everything else a modern application needs, all using OpenTelemetry. Pydantic Logfire is a commercial product Logfire is a commercially supported, hosted platform with an extremely generous and perpetual [free tier](https://pydantic.dev/pricing/). You can sign up and start using Logfire in a couple of minutes. Logfire can also be self-hosted on the enterprise tier. Pydantic AI has built-in (but optional) support for Logfire. That means if the logfire package is installed and configured and agent instrumentation is enabled then detailed information about agent runs is sent to Logfire. Otherwise there's virtually no overhead and nothing is sent. Here's an example showing details of running the [Weather Agent](../examples/weather-agent/index.html) in Logfire: [![Weather Agent Logfire](../img/logfire-weather-agent.png)](../img/logfire-weather-agent.png) A trace is generated for the agent run, and spans are emitted for each model request and tool call.", "url": "https://ai.pydantic.dev/logfire/index.html#pydantic-logfire", "page": "logfire/index.html", "source_site": "pydantic_ai"}
{"title": "Using Logfire", "anchor": "using-logfire", "heading_level": 2, "md_text": "To use Logfire, you'll need a Logfire [account](https://logfire.pydantic.dev). The Logfire Python SDK is included with pydantic-ai : pipuv Or if you're using the slim package, you can install it with the logfire optional group: pipuv Then authenticate your local environment with Logfire: pipuv And configure a project to send data to: pipuv (Or use an existing project with logfire projects use ) This will write to a .logfire directory in the current working directory, which the Logfire SDK will use for configuration at run time. With that, you can start using Logfire to instrument Pydantic AI code: instrument\\_pydantic\\_ai.py 1. [ logfire.configure() ](https://logfire.pydantic.dev/docs/reference/api/logfire/#logfire.configure) configures the SDK, by default it will find the write token from the .logfire directory, but you can also pass a token directly. 2. [ logfire.instrument_pydantic_ai() ](https://logfire.pydantic.dev/docs/reference/api/logfire/#logfire.Logfire.instrument_pydantic_ai) enables instrumentation of Pydantic AI. 3. Since we've enabled instrumentation, a trace will be generated for each run, with spans emitted for models calls and tool function execution *(This example is complete, it can be run \"as is\")* Which will display in Logfire thus: [![Logfire Simple Agent Run](../img/logfire-simple-agent.png)](../img/logfire-simple-agent.png) The [Logfire documentation](https://logfire.pydantic.dev/docs/) has more details on how to use Logfire, including how to instrument other libraries like [HTTPX](https://logfire.pydantic.dev/docs/integrations/http-clients/httpx/) and [FastAPI](https://logfire.pydantic.dev/docs/integrations/web-frameworks/fastapi/). Since Logfire is built on [OpenTelemetry](https://opentelemetry.io/), you can use the Logfire Python SDK to send data to any OpenTelemetry collector, see [below](index.html#using-opentelemetry). ### Debugging To demonstrate how Logfire can let you visualise the flow of a Pydantic AI run, here's the view you get from Logfire while running the [chat app examples](../examples/chat-app/index.html): ### Monitoring Performance We can also query data with SQL in Logfire to monitor the performance of an application. Here's a real world example of using Logfire to monitor Pydantic AI runs inside Logfire itself: [![Logfire monitoring Pydantic AI](../img/logfire-monitoring-pydanticai.png)](../img/logfire-monitoring-pydanticai.png) ### Monitoring HTTP Requests \\\"F\\*\\*k you, show me the prompt.\\\" As per Hamel Husain's influential 2024 blog post [\"Fuck You, Show Me The Prompt.\"](https://hamel.dev/blog/posts/prompt/) (bear with the capitalization, the point is valid), it's often useful to be able to view the raw HTTP requests and responses made to model providers. To observe raw HTTP requests made to model providers, you can use Logfire's [HTTPX instrumentation](https://logfire.pydantic.dev/docs/integrations/http-clients/httpx/) since all provider SDKs use the [HTTPX](https://www.python-httpx.org/) library internally. With HTTP instrumentationWithout HTTP instrumentation with\\_logfire\\_instrument\\_httpx.py 1. See the [ logfire.instrument_httpx docs](https://logfire.pydantic.dev/docs/reference/api/logfire/#logfire.Logfire.instrument_httpx) more details, capture_all=True means both headers and body are captured for both the request and response. [![Logfire with HTTPX instrumentation](../img/logfire-with-httpx.png)](../img/logfire-with-httpx.png) without\\_logfire\\_instrument\\_httpx.py [![Logfire without HTTPX instrumentation](../img/logfire-without-httpx.png)](../img/logfire-without-httpx.png)", "url": "https://ai.pydantic.dev/logfire/index.html#using-logfire", "page": "logfire/index.html", "source_site": "pydantic_ai"}
{"title": "Debugging", "anchor": "debugging", "heading_level": 3, "md_text": "To demonstrate how Logfire can let you visualise the flow of a Pydantic AI run, here's the view you get from Logfire while running the [chat app examples](../examples/chat-app/index.html):", "url": "https://ai.pydantic.dev/logfire/index.html#debugging", "page": "logfire/index.html", "source_site": "pydantic_ai"}
{"title": "Monitoring Performance", "anchor": "monitoring-performance", "heading_level": 3, "md_text": "We can also query data with SQL in Logfire to monitor the performance of an application. Here's a real world example of using Logfire to monitor Pydantic AI runs inside Logfire itself: [![Logfire monitoring Pydantic AI](../img/logfire-monitoring-pydanticai.png)](../img/logfire-monitoring-pydanticai.png)", "url": "https://ai.pydantic.dev/logfire/index.html#monitoring-performance", "page": "logfire/index.html", "source_site": "pydantic_ai"}
{"title": "Monitoring HTTP Requests", "anchor": "monitoring-http-requests", "heading_level": 3, "md_text": "\\\"F\\*\\*k you, show me the prompt.\\\" As per Hamel Husain's influential 2024 blog post [\"Fuck You, Show Me The Prompt.\"](https://hamel.dev/blog/posts/prompt/) (bear with the capitalization, the point is valid), it's often useful to be able to view the raw HTTP requests and responses made to model providers. To observe raw HTTP requests made to model providers, you can use Logfire's [HTTPX instrumentation](https://logfire.pydantic.dev/docs/integrations/http-clients/httpx/) since all provider SDKs use the [HTTPX](https://www.python-httpx.org/) library internally. With HTTP instrumentationWithout HTTP instrumentation with\\_logfire\\_instrument\\_httpx.py 1. See the [ logfire.instrument_httpx docs](https://logfire.pydantic.dev/docs/reference/api/logfire/#logfire.Logfire.instrument_httpx) more details, capture_all=True means both headers and body are captured for both the request and response. [![Logfire with HTTPX instrumentation](../img/logfire-with-httpx.png)](../img/logfire-with-httpx.png) without\\_logfire\\_instrument\\_httpx.py [![Logfire without HTTPX instrumentation](../img/logfire-without-httpx.png)](../img/logfire-without-httpx.png)", "url": "https://ai.pydantic.dev/logfire/index.html#monitoring-http-requests", "page": "logfire/index.html", "source_site": "pydantic_ai"}
{"title": "Using OpenTelemetry", "anchor": "using-opentelemetry", "heading_level": 2, "md_text": "Pydantic AI's instrumentation uses [OpenTelemetry](https://opentelemetry.io/) (OTel), which Logfire is based on. This means you can debug and monitor Pydantic AI with any OpenTelemetry backend. Pydantic AI follows the [OpenTelemetry Semantic Conventions for Generative AI systems](https://opentelemetry.io/docs/specs/semconv/gen-ai/), so while we think you'll have the best experience using the Logfire platform ![](https://cdn.jsdelivr.net/gh/jdecked/twemoji@15.1.0/assets/svg/1f609.svg \":wink:\"), you should be able to use any OTel service with GenAI support. ### Logfire with an alternative OTel backend You can use the Logfire SDK completely freely and send the data to any OpenTelemetry backend. Here's an example of configuring the Logfire library to send data to the excellent [otel-tui](https://github.com/ymtdzzz/otel-tui)  an open source terminal based OTel backend and viewer (no association with Pydantic Validation). Run otel-tui with docker (see [the otel-tui readme](https://github.com/ymtdzzz/otel-tui) for more instructions): Terminal then run, otel\\_tui.py 1. Set the OTEL_EXPORTER_OTLP_ENDPOINT environment variable to the URL of your OpenTelemetry backend. If you're using a backend that requires authentication, you may need to set [other environment variables](https://opentelemetry.io/docs/languages/sdk-configuration/otlp-exporter/). Of course, these can also be set outside the process, e.g. with export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318 . 2. We [configure](https://logfire.pydantic.dev/docs/reference/api/logfire/#logfire.configure) Logfire to disable sending data to the Logfire OTel backend itself. If you removed send_to_logfire=False , data would be sent to both Logfire and your OpenTelemetry backend. Running the above code will send tracing data to otel-tui , which will display like this: [![otel tui simple](../img/otel-tui-simple.png)](../img/otel-tui-simple.png) Running the [weather agent](../examples/weather-agent/index.html) example connected to otel-tui shows how it can be used to visualise a more complex trace: [![otel tui weather agent](../img/otel-tui-weather.png)](../img/otel-tui-weather.png) For more information on using the Logfire SDK to send data to alternative backends, see [the Logfire documentation](https://logfire.pydantic.dev/docs/how-to-guides/alternative-backends/). ### OTel without Logfire You can also emit OpenTelemetry data from Pydantic AI without using Logfire at all. To do this, you'll need to install and configure the OpenTelemetry packages you need. To run the following examples, use Terminal raw\\_otel.py ### Alternative Observability backends Because Pydantic AI uses OpenTelemetry for observability, you can easily configure it to send data to any OpenTelemetry-compatible backend, not just our observability platform [Pydantic Logfire](index.html#pydantic-logfire). The following providers have dedicated documentation on Pydantic AI: * [Langfuse](https://langfuse.com/docs/integrations/pydantic-ai) * [W&B Weave](https://weave-docs.wandb.ai/guides/integrations/pydantic_ai/) * [Arize](https://arize.com/docs/ax/observe/tracing-integrations-auto/pydantic-ai) * [Openlayer](https://www.openlayer.com/docs/integrations/pydantic-ai) * [OpenLIT](https://docs.openlit.io/latest/integrations/pydantic) * [LangWatch](https://docs.langwatch.ai/integration/python/integrations/pydantic-ai) * [Patronus AI](https://docs.patronus.ai/docs/percival/pydantic) * [Opik](https://www.comet.com/docs/opik/tracing/integrations/pydantic-ai) * [mlflow](https://mlflow.org/docs/latest/genai/tracing/integrations/listing/pydantic_ai) * [Agenta](https://docs.agenta.ai/observability/integrations/pydanticai) * [Confident AI](https://documentation.confident-ai.com/docs/llm-tracing/integrations/pydanticai) * [LangWatch](https://docs.langwatch.ai/integration/python/integrations/pydantic-ai)", "url": "https://ai.pydantic.dev/logfire/index.html#using-opentelemetry", "page": "logfire/index.html", "source_site": "pydantic_ai"}
{"title": "Logfire with an alternative OTel backend", "anchor": "logfire-with-an-alternative-otel-backend", "heading_level": 3, "md_text": "You can use the Logfire SDK completely freely and send the data to any OpenTelemetry backend. Here's an example of configuring the Logfire library to send data to the excellent [otel-tui](https://github.com/ymtdzzz/otel-tui)  an open source terminal based OTel backend and viewer (no association with Pydantic Validation). Run otel-tui with docker (see [the otel-tui readme](https://github.com/ymtdzzz/otel-tui) for more instructions): Terminal then run, otel\\_tui.py 1. Set the OTEL_EXPORTER_OTLP_ENDPOINT environment variable to the URL of your OpenTelemetry backend. If you're using a backend that requires authentication, you may need to set [other environment variables](https://opentelemetry.io/docs/languages/sdk-configuration/otlp-exporter/). Of course, these can also be set outside the process, e.g. with export OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318 . 2. We [configure](https://logfire.pydantic.dev/docs/reference/api/logfire/#logfire.configure) Logfire to disable sending data to the Logfire OTel backend itself. If you removed send_to_logfire=False , data would be sent to both Logfire and your OpenTelemetry backend. Running the above code will send tracing data to otel-tui , which will display like this: [![otel tui simple](../img/otel-tui-simple.png)](../img/otel-tui-simple.png) Running the [weather agent](../examples/weather-agent/index.html) example connected to otel-tui shows how it can be used to visualise a more complex trace: [![otel tui weather agent](../img/otel-tui-weather.png)](../img/otel-tui-weather.png) For more information on using the Logfire SDK to send data to alternative backends, see [the Logfire documentation](https://logfire.pydantic.dev/docs/how-to-guides/alternative-backends/).", "url": "https://ai.pydantic.dev/logfire/index.html#logfire-with-an-alternative-otel-backend", "page": "logfire/index.html", "source_site": "pydantic_ai"}
{"title": "OTel without Logfire", "anchor": "otel-without-logfire", "heading_level": 3, "md_text": "You can also emit OpenTelemetry data from Pydantic AI without using Logfire at all. To do this, you'll need to install and configure the OpenTelemetry packages you need. To run the following examples, use Terminal raw\\_otel.py", "url": "https://ai.pydantic.dev/logfire/index.html#otel-without-logfire", "page": "logfire/index.html", "source_site": "pydantic_ai"}
{"title": "Alternative Observability backends", "anchor": "alternative-observability-backends", "heading_level": 3, "md_text": "Because Pydantic AI uses OpenTelemetry for observability, you can easily configure it to send data to any OpenTelemetry-compatible backend, not just our observability platform [Pydantic Logfire](index.html#pydantic-logfire). The following providers have dedicated documentation on Pydantic AI: * [Langfuse](https://langfuse.com/docs/integrations/pydantic-ai) * [W&B Weave](https://weave-docs.wandb.ai/guides/integrations/pydantic_ai/) * [Arize](https://arize.com/docs/ax/observe/tracing-integrations-auto/pydantic-ai) * [Openlayer](https://www.openlayer.com/docs/integrations/pydantic-ai) * [OpenLIT](https://docs.openlit.io/latest/integrations/pydantic) * [LangWatch](https://docs.langwatch.ai/integration/python/integrations/pydantic-ai) * [Patronus AI](https://docs.patronus.ai/docs/percival/pydantic) * [Opik](https://www.comet.com/docs/opik/tracing/integrations/pydantic-ai) * [mlflow](https://mlflow.org/docs/latest/genai/tracing/integrations/listing/pydantic_ai) * [Agenta](https://docs.agenta.ai/observability/integrations/pydanticai) * [Confident AI](https://documentation.confident-ai.com/docs/llm-tracing/integrations/pydanticai) * [LangWatch](https://docs.langwatch.ai/integration/python/integrations/pydantic-ai)", "url": "https://ai.pydantic.dev/logfire/index.html#alternative-observability-backends", "page": "logfire/index.html", "source_site": "pydantic_ai"}
{"title": "Advanced usage", "anchor": "advanced-usage", "heading_level": 2, "md_text": "### Configuring data format Pydantic AI follows the [OpenTelemetry Semantic Conventions for Generative AI systems](https://opentelemetry.io/docs/specs/semconv/gen-ai/). Specifically, it follows version 1.37.0 of the conventions by default, with a few exceptions. Certain span and attribute names are not spec compliant by default for compatibility reasons, but can be made compliant by passing [ InstrumentationSettings(version=3) ](../api/models/instrumented/index.html#pydantic_ai.models.instrumented.InstrumentationSettings) (the default is currently version=2 ). This will change the following: * The span name agent run becomes invoke_agent {gen_ai.agent.name} (with the agent name filled in) * The span name running tool becomes execute_tool {gen_ai.tool.name} (with the tool name filled in) * The attribute name tool_arguments becomes gen_ai.tool.call.arguments * The attribute name tool_response becomes gen_ai.tool.call.result To use [OpenTelemetry semantic conventions version 1.36.0](https://github.com/open-telemetry/semantic-conventions/blob/v1.36.0/docs/gen-ai/README.md) or older, pass [ InstrumentationSettings(version=1) ](../api/models/instrumented/index.html#pydantic_ai.models.instrumented.InstrumentationSettings). Moreover, those semantic conventions specify that messages should be captured as individual events (logs) that are children of the request span, whereas by default, Pydantic AI instead collects these events into a JSON array which is set as a single large attribute called events on the request span. To change this, use event_mode='logs' : instrumentation\\_settings\\_event\\_mode.py This won't look as good in the Logfire UI, and will also be removed from Pydantic AI in a future release, but may be useful for backwards compatibility. Note that the OpenTelemetry Semantic Conventions are still experimental and are likely to change. ### Setting OpenTelemetry SDK providers By default, the global TracerProvider and EventLoggerProvider are used. These are set automatically by logfire.configure() . They can also be set by the set_tracer_provider and set_event_logger_provider functions in the OpenTelemetry Python SDK. You can set custom providers with [ InstrumentationSettings ](../api/models/instrumented/index.html#pydantic_ai.models.instrumented.InstrumentationSettings). instrumentation\\_settings\\_providers.py ### Instrumenting a specific Model instrumented\\_model\\_example.py ### Excluding binary content excluding\\_binary\\_content.py ### Excluding prompts and completions For privacy and security reasons, you may want to monitor your agent's behavior and performance without exposing sensitive user data or proprietary prompts in your observability platform. Pydantic AI allows you to exclude the actual content from instrumentation events while preserving the structural information needed for debugging and monitoring. When include_content=False is set, Pydantic AI will exclude sensitive content from OpenTelemetry events, including user prompts and model completions, tool call arguments and responses, and any other message content. excluding\\_sensitive\\_content.py This setting is particularly useful in production environments where compliance requirements or data sensitivity concerns make it necessary to limit what content is sent to your observability platform.", "url": "https://ai.pydantic.dev/logfire/index.html#advanced-usage", "page": "logfire/index.html", "source_site": "pydantic_ai"}
{"title": "Configuring data format", "anchor": "configuring-data-format", "heading_level": 3, "md_text": "Pydantic AI follows the [OpenTelemetry Semantic Conventions for Generative AI systems](https://opentelemetry.io/docs/specs/semconv/gen-ai/). Specifically, it follows version 1.37.0 of the conventions by default, with a few exceptions. Certain span and attribute names are not spec compliant by default for compatibility reasons, but can be made compliant by passing [ InstrumentationSettings(version=3) ](../api/models/instrumented/index.html#pydantic_ai.models.instrumented.InstrumentationSettings) (the default is currently version=2 ). This will change the following: * The span name agent run becomes invoke_agent {gen_ai.agent.name} (with the agent name filled in) * The span name running tool becomes execute_tool {gen_ai.tool.name} (with the tool name filled in) * The attribute name tool_arguments becomes gen_ai.tool.call.arguments * The attribute name tool_response becomes gen_ai.tool.call.result To use [OpenTelemetry semantic conventions version 1.36.0](https://github.com/open-telemetry/semantic-conventions/blob/v1.36.0/docs/gen-ai/README.md) or older, pass [ InstrumentationSettings(version=1) ](../api/models/instrumented/index.html#pydantic_ai.models.instrumented.InstrumentationSettings). Moreover, those semantic conventions specify that messages should be captured as individual events (logs) that are children of the request span, whereas by default, Pydantic AI instead collects these events into a JSON array which is set as a single large attribute called events on the request span. To change this, use event_mode='logs' : instrumentation\\_settings\\_event\\_mode.py This won't look as good in the Logfire UI, and will also be removed from Pydantic AI in a future release, but may be useful for backwards compatibility. Note that the OpenTelemetry Semantic Conventions are still experimental and are likely to change.", "url": "https://ai.pydantic.dev/logfire/index.html#configuring-data-format", "page": "logfire/index.html", "source_site": "pydantic_ai"}
{"title": "Setting OpenTelemetry SDK providers", "anchor": "setting-opentelemetry-sdk-providers", "heading_level": 3, "md_text": "By default, the global TracerProvider and EventLoggerProvider are used. These are set automatically by logfire.configure() . They can also be set by the set_tracer_provider and set_event_logger_provider functions in the OpenTelemetry Python SDK. You can set custom providers with [ InstrumentationSettings ](../api/models/instrumented/index.html#pydantic_ai.models.instrumented.InstrumentationSettings). instrumentation\\_settings\\_providers.py", "url": "https://ai.pydantic.dev/logfire/index.html#setting-opentelemetry-sdk-providers", "page": "logfire/index.html", "source_site": "pydantic_ai"}
{"title": "Instrumenting a specific Model", "anchor": "instrumenting-a-specific-model", "heading_level": 3, "md_text": "instrumented\\_model\\_example.py", "url": "https://ai.pydantic.dev/logfire/index.html#instrumenting-a-specific-model", "page": "logfire/index.html", "source_site": "pydantic_ai"}
{"title": "Excluding binary content", "anchor": "excluding-binary-content", "heading_level": 3, "md_text": "excluding\\_binary\\_content.py", "url": "https://ai.pydantic.dev/logfire/index.html#excluding-binary-content", "page": "logfire/index.html", "source_site": "pydantic_ai"}
{"title": "Excluding prompts and completions", "anchor": "excluding-prompts-and-completions", "heading_level": 3, "md_text": "For privacy and security reasons, you may want to monitor your agent's behavior and performance without exposing sensitive user data or proprietary prompts in your observability platform. Pydantic AI allows you to exclude the actual content from instrumentation events while preserving the structural information needed for debugging and monitoring. When include_content=False is set, Pydantic AI will exclude sensitive content from OpenTelemetry events, including user prompts and model completions, tool call arguments and responses, and any other message content. excluding\\_sensitive\\_content.py This setting is particularly useful in production environments where compliance requirements or data sensitivity concerns make it necessary to limit what content is sent to your observability platform.", "url": "https://ai.pydantic.dev/logfire/index.html#excluding-prompts-and-completions", "page": "logfire/index.html", "source_site": "pydantic_ai"}
{"title": "Joins and Reducers", "anchor": "joins-and-reducers", "heading_level": 1, "md_text": "Join nodes synchronize and aggregate data from parallel execution paths. They use **Reducers** to combine multiple inputs into a single output. ## Overview When you use [parallel execution](../parallel/index.html) (broadcasting or mapping), you often need to collect and combine the results. Join nodes serve this purpose by: 1. Waiting for all parallel tasks to complete 2. Aggregating their outputs using a [ ReducerFunction ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.ReducerFunction) 3. Passing the aggregated result to the next node ## Creating Joins Create a join using GraphBuilder.join with a reducer function and initial value or factory: basic\\_join.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ## Built-in Reducers Pydantic Graph provides several common reducer types out of the box: ### reduce_list_append [ reduce_list_append ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_list_append) collects all inputs into a list: list\\_reducer.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ### reduce_list_extend [ reduce_list_extend ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_list_extend) extends a list with an iterable of items: list\\_extend\\_reducer.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ### reduce_dict_update [ reduce_dict_update ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_dict_update) merges dictionaries together: dict\\_reducer.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ### reduce_null [ reduce_null ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_null) discards all inputs and returns None . Useful when you only care about side effects: null\\_reducer.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ### reduce_sum [ reduce_sum ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_sum) sums numeric values: sum\\_reducer.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ### ReduceFirstValue [ ReduceFirstValue ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.ReduceFirstValue) returns the first value it receives and cancels all other parallel tasks. This is useful for \"race\" scenarios where you want the first successful result: first\\_value\\_reducer.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ## Custom Reducers Create custom reducers by defining a [ ReducerFunction ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.ReducerFunction): custom\\_reducer.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ## Reducers with State Access Reducers can access and modify the graph state: stateful\\_reducer.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ### Canceling Sibling Tasks Reducers with access to [ ReducerContext ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.ReducerContext) can call [ ctx.cancel_sibling_tasks() ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.ReducerContext.cancel_sibling_tasks) to cancel all other parallel tasks in the same fork. This is useful for early termination when you've found what you need: cancel\\_siblings.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* Note that only 3 searches completed instead of all 5, because the reducer canceled the remaining tasks after finding a match. ## Multiple Joins A graph can have multiple independent joins: multiple\\_joins.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ## Customizing Join Nodes ### Custom Node IDs Like steps, joins can have custom IDs: join\\_custom\\_id.py ## How Joins Work Internally, the graph tracks which \"fork\" each parallel task belongs to. A join: 1. Identifies its parent fork (the fork that created the parallel paths) 2. Waits for all tasks from that fork to reach the join 3. Calls reduce() for each incoming value 4. Calls finalize() once all values are received 5. Passes the finalized result to downstream nodes This ensures proper synchronization even with nested parallel operations. ## Next Steps * Learn about [parallel execution](../parallel/index.html) with broadcasting and mapping * Explore [conditional branching](../decisions/index.html) with decision nodes * See the [API reference](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join) for complete reducer documentation", "url": "https://ai.pydantic.dev/graph/beta/joins/index.html#joins-and-reducers", "page": "graph/beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "When you use [parallel execution](../parallel/index.html) (broadcasting or mapping), you often need to collect and combine the results. Join nodes serve this purpose by: 1. Waiting for all parallel tasks to complete 2. Aggregating their outputs using a [ ReducerFunction ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.ReducerFunction) 3. Passing the aggregated result to the next node", "url": "https://ai.pydantic.dev/graph/beta/joins/index.html#overview", "page": "graph/beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "Creating Joins", "anchor": "creating-joins", "heading_level": 2, "md_text": "Create a join using GraphBuilder.join with a reducer function and initial value or factory: basic\\_join.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/joins/index.html#creating-joins", "page": "graph/beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "Built-in Reducers", "anchor": "built-in-reducers", "heading_level": 2, "md_text": "Pydantic Graph provides several common reducer types out of the box: ### reduce_list_append [ reduce_list_append ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_list_append) collects all inputs into a list: list\\_reducer.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ### reduce_list_extend [ reduce_list_extend ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_list_extend) extends a list with an iterable of items: list\\_extend\\_reducer.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ### reduce_dict_update [ reduce_dict_update ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_dict_update) merges dictionaries together: dict\\_reducer.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ### reduce_null [ reduce_null ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_null) discards all inputs and returns None . Useful when you only care about side effects: null\\_reducer.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ### reduce_sum [ reduce_sum ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_sum) sums numeric values: sum\\_reducer.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ### ReduceFirstValue [ ReduceFirstValue ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.ReduceFirstValue) returns the first value it receives and cancels all other parallel tasks. This is useful for \"race\" scenarios where you want the first successful result: first\\_value\\_reducer.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/joins/index.html#built-in-reducers", "page": "graph/beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "reduce_list_append", "anchor": "reduce_list_append", "heading_level": 3, "md_text": "[ reduce_list_append ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_list_append) collects all inputs into a list: list\\_reducer.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/joins/index.html#reduce_list_append", "page": "graph/beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "reduce_list_extend", "anchor": "reduce_list_extend", "heading_level": 3, "md_text": "[ reduce_list_extend ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_list_extend) extends a list with an iterable of items: list\\_extend\\_reducer.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/joins/index.html#reduce_list_extend", "page": "graph/beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "reduce_dict_update", "anchor": "reduce_dict_update", "heading_level": 3, "md_text": "[ reduce_dict_update ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_dict_update) merges dictionaries together: dict\\_reducer.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/joins/index.html#reduce_dict_update", "page": "graph/beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "reduce_null", "anchor": "reduce_null", "heading_level": 3, "md_text": "[ reduce_null ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_null) discards all inputs and returns None . Useful when you only care about side effects: null\\_reducer.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/joins/index.html#reduce_null", "page": "graph/beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "reduce_sum", "anchor": "reduce_sum", "heading_level": 3, "md_text": "[ reduce_sum ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.reduce_sum) sums numeric values: sum\\_reducer.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/joins/index.html#reduce_sum", "page": "graph/beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "ReduceFirstValue", "anchor": "reducefirstvalue", "heading_level": 3, "md_text": "[ ReduceFirstValue ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.ReduceFirstValue) returns the first value it receives and cancels all other parallel tasks. This is useful for \"race\" scenarios where you want the first successful result: first\\_value\\_reducer.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/joins/index.html#reducefirstvalue", "page": "graph/beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Reducers", "anchor": "custom-reducers", "heading_level": 2, "md_text": "Create custom reducers by defining a [ ReducerFunction ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.ReducerFunction): custom\\_reducer.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/joins/index.html#custom-reducers", "page": "graph/beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "Reducers with State Access", "anchor": "reducers-with-state-access", "heading_level": 2, "md_text": "Reducers can access and modify the graph state: stateful\\_reducer.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* ### Canceling Sibling Tasks Reducers with access to [ ReducerContext ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.ReducerContext) can call [ ctx.cancel_sibling_tasks() ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.ReducerContext.cancel_sibling_tasks) to cancel all other parallel tasks in the same fork. This is useful for early termination when you've found what you need: cancel\\_siblings.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* Note that only 3 searches completed instead of all 5, because the reducer canceled the remaining tasks after finding a match.", "url": "https://ai.pydantic.dev/graph/beta/joins/index.html#reducers-with-state-access", "page": "graph/beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "Canceling Sibling Tasks", "anchor": "canceling-sibling-tasks", "heading_level": 3, "md_text": "Reducers with access to [ ReducerContext ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.ReducerContext) can call [ ctx.cancel_sibling_tasks() ](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join.ReducerContext.cancel_sibling_tasks) to cancel all other parallel tasks in the same fork. This is useful for early termination when you've found what you need: cancel\\_siblings.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )* Note that only 3 searches completed instead of all 5, because the reducer canceled the remaining tasks after finding a match.", "url": "https://ai.pydantic.dev/graph/beta/joins/index.html#canceling-sibling-tasks", "page": "graph/beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "Multiple Joins", "anchor": "multiple-joins", "heading_level": 2, "md_text": "A graph can have multiple independent joins: multiple\\_joins.py *(This example is complete, it can be run \"as is\"  you'll need to add import asyncio; asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/beta/joins/index.html#multiple-joins", "page": "graph/beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "Customizing Join Nodes", "anchor": "customizing-join-nodes", "heading_level": 2, "md_text": "### Custom Node IDs Like steps, joins can have custom IDs: join\\_custom\\_id.py", "url": "https://ai.pydantic.dev/graph/beta/joins/index.html#customizing-join-nodes", "page": "graph/beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Node IDs", "anchor": "custom-node-ids", "heading_level": 3, "md_text": "Like steps, joins can have custom IDs: join\\_custom\\_id.py", "url": "https://ai.pydantic.dev/graph/beta/joins/index.html#custom-node-ids", "page": "graph/beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "How Joins Work", "anchor": "how-joins-work", "heading_level": 2, "md_text": "Internally, the graph tracks which \"fork\" each parallel task belongs to. A join: 1. Identifies its parent fork (the fork that created the parallel paths) 2. Waits for all tasks from that fork to reach the join 3. Calls reduce() for each incoming value 4. Calls finalize() once all values are received 5. Passes the finalized result to downstream nodes This ensures proper synchronization even with nested parallel operations.", "url": "https://ai.pydantic.dev/graph/beta/joins/index.html#how-joins-work", "page": "graph/beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "Next Steps", "anchor": "next-steps", "heading_level": 2, "md_text": "* Learn about [parallel execution](../parallel/index.html) with broadcasting and mapping * Explore [conditional branching](../decisions/index.html) with decision nodes * See the [API reference](../../../api/pydantic_graph/beta_join/index.html#pydantic_graph.beta.join) for complete reducer documentation", "url": "https://ai.pydantic.dev/graph/beta/joins/index.html#next-steps", "page": "graph/beta/joins/index.html", "source_site": "pydantic_ai"}
{"title": "Server", "anchor": "server", "heading_level": 1, "md_text": "Pydantic AI models can also be used within MCP Servers. ## MCP Server Here's a simple example of a [Python MCP server](https://github.com/modelcontextprotocol/python-sdk) using Pydantic AI within a tool call: mcp\\_server.py ## Simple client This server can be queried with any MCP client. Here is an example using the Python SDK directly: mcp\\_client.py ## MCP Sampling What is MCP Sampling? See the [MCP client docs](../client/index.html#mcp-sampling) for details of what MCP sampling is, and how you can support it when using Pydantic AI as an MCP client. When Pydantic AI agents are used within MCP servers, they can use sampling via [ MCPSamplingModel ](../../api/models/mcp-sampling/index.html#pydantic_ai.models.mcp_sampling.MCPSamplingModel). We can extend the above example to use sampling so instead of connecting directly to the LLM, the agent calls back through the MCP client to make LLM calls. mcp\\_server\\_sampling.py The [above](index.html#simple-client) client does not support sampling, so if you tried to use it with this server you'd get an error. The simplest way to support sampling in an MCP client is to [use](../client/index.html#mcp-sampling) a Pydantic AI agent as the client, but if you wanted to support sampling with the vanilla MCP SDK, you could do so like this: mcp\\_client\\_sampling.py *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/mcp/server/index.html#server", "page": "mcp/server/index.html", "source_site": "pydantic_ai"}
{"title": "MCP Server", "anchor": "mcp-server", "heading_level": 2, "md_text": "Here's a simple example of a [Python MCP server](https://github.com/modelcontextprotocol/python-sdk) using Pydantic AI within a tool call: mcp\\_server.py", "url": "https://ai.pydantic.dev/mcp/server/index.html#mcp-server", "page": "mcp/server/index.html", "source_site": "pydantic_ai"}
{"title": "Simple client", "anchor": "simple-client", "heading_level": 2, "md_text": "This server can be queried with any MCP client. Here is an example using the Python SDK directly: mcp\\_client.py", "url": "https://ai.pydantic.dev/mcp/server/index.html#simple-client", "page": "mcp/server/index.html", "source_site": "pydantic_ai"}
{"title": "MCP Sampling", "anchor": "mcp-sampling", "heading_level": 2, "md_text": "What is MCP Sampling? See the [MCP client docs](../client/index.html#mcp-sampling) for details of what MCP sampling is, and how you can support it when using Pydantic AI as an MCP client. When Pydantic AI agents are used within MCP servers, they can use sampling via [ MCPSamplingModel ](../../api/models/mcp-sampling/index.html#pydantic_ai.models.mcp_sampling.MCPSamplingModel). We can extend the above example to use sampling so instead of connecting directly to the LLM, the agent calls back through the MCP client to make LLM calls. mcp\\_server\\_sampling.py The [above](index.html#simple-client) client does not support sampling, so if you tried to use it with this server you'd get an error. The simplest way to support sampling in an MCP client is to [use](../client/index.html#mcp-sampling) a Pydantic AI agent as the client, but if you wanted to support sampling with the vanilla MCP SDK, you could do so like this: mcp\\_client\\_sampling.py *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/mcp/server/index.html#mcp-sampling", "page": "mcp/server/index.html", "source_site": "pydantic_ai"}
{"title": "Anthropic", "anchor": "anthropic", "heading_level": 1, "md_text": "## Install To use AnthropicModel models, you need to either install pydantic-ai , or install pydantic-ai-slim with the anthropic optional group: pipuv ## Configuration To use [Anthropic](https://anthropic.com) through their API, go to [console.anthropic.com/settings/keys](https://console.anthropic.com/settings/keys) to generate an API key. AnthropicModelName contains a list of available Anthropic models. ## Environment variable Once you have the API key, you can set it as an environment variable: You can then use AnthropicModel by name: Or initialise the model directly with just the model name: ## provider argument You can provide a custom Provider via the provider argument: ## Custom HTTP Client You can customize the AnthropicProvider with a custom httpx.AsyncClient :", "url": "https://ai.pydantic.dev/models/anthropic/index.html#anthropic", "page": "models/anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "heading_level": 2, "md_text": "To use AnthropicModel models, you need to either install pydantic-ai , or install pydantic-ai-slim with the anthropic optional group: pipuv", "url": "https://ai.pydantic.dev/models/anthropic/index.html#install", "page": "models/anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "Configuration", "anchor": "configuration", "heading_level": 2, "md_text": "To use [Anthropic](https://anthropic.com) through their API, go to [console.anthropic.com/settings/keys](https://console.anthropic.com/settings/keys) to generate an API key. AnthropicModelName contains a list of available Anthropic models.", "url": "https://ai.pydantic.dev/models/anthropic/index.html#configuration", "page": "models/anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "Environment variable", "anchor": "environment-variable", "heading_level": 2, "md_text": "Once you have the API key, you can set it as an environment variable: You can then use AnthropicModel by name: Or initialise the model directly with just the model name:", "url": "https://ai.pydantic.dev/models/anthropic/index.html#environment-variable", "page": "models/anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "provider argument", "anchor": "provider-argument", "heading_level": 2, "md_text": "You can provide a custom Provider via the provider argument:", "url": "https://ai.pydantic.dev/models/anthropic/index.html#provider-argument", "page": "models/anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "Custom HTTP Client", "anchor": "custom-http-client", "heading_level": 2, "md_text": "You can customize the AnthropicProvider with a custom httpx.AsyncClient :", "url": "https://ai.pydantic.dev/models/anthropic/index.html#custom-http-client", "page": "models/anthropic/index.html", "source_site": "pydantic_ai"}
{"title": "Cohere", "anchor": "cohere", "heading_level": 1, "md_text": "## Install To use CohereModel , you need to either install pydantic-ai , or install pydantic-ai-slim with the cohere optional group: pipuv ## Configuration To use [Cohere](https://cohere.com/) through their API, go to [dashboard.cohere.com/api-keys](https://dashboard.cohere.com/api-keys) and follow your nose until you find the place to generate an API key. CohereModelName contains a list of the most popular Cohere models. ## Environment variable Once you have the API key, you can set it as an environment variable: You can then use CohereModel by name: Or initialise the model directly with just the model name: ## provider argument You can provide a custom Provider via the provider argument: You can also customize the CohereProvider with a custom http_client :", "url": "https://ai.pydantic.dev/models/cohere/index.html#cohere", "page": "models/cohere/index.html", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "heading_level": 2, "md_text": "To use CohereModel , you need to either install pydantic-ai , or install pydantic-ai-slim with the cohere optional group: pipuv", "url": "https://ai.pydantic.dev/models/cohere/index.html#install", "page": "models/cohere/index.html", "source_site": "pydantic_ai"}
{"title": "Configuration", "anchor": "configuration", "heading_level": 2, "md_text": "To use [Cohere](https://cohere.com/) through their API, go to [dashboard.cohere.com/api-keys](https://dashboard.cohere.com/api-keys) and follow your nose until you find the place to generate an API key. CohereModelName contains a list of the most popular Cohere models.", "url": "https://ai.pydantic.dev/models/cohere/index.html#configuration", "page": "models/cohere/index.html", "source_site": "pydantic_ai"}
{"title": "Environment variable", "anchor": "environment-variable", "heading_level": 2, "md_text": "Once you have the API key, you can set it as an environment variable: You can then use CohereModel by name: Or initialise the model directly with just the model name:", "url": "https://ai.pydantic.dev/models/cohere/index.html#environment-variable", "page": "models/cohere/index.html", "source_site": "pydantic_ai"}
{"title": "provider argument", "anchor": "provider-argument", "heading_level": 2, "md_text": "You can provide a custom Provider via the provider argument: You can also customize the CohereProvider with a custom http_client :", "url": "https://ai.pydantic.dev/models/cohere/index.html#provider-argument", "page": "models/cohere/index.html", "source_site": "pydantic_ai"}
{"title": "Bedrock", "anchor": "bedrock", "heading_level": 1, "md_text": "## Install To use BedrockConverseModel , you need to either install pydantic-ai , or install pydantic-ai-slim with the bedrock optional group: pipuv ## Configuration To use [AWS Bedrock](https://aws.amazon.com/bedrock/), you'll need an AWS account with Bedrock enabled and appropriate credentials. You can use either AWS credentials directly or a pre-configured boto3 client. BedrockModelName contains a list of available Bedrock models, including models from Anthropic, Amazon, Cohere, Meta, and Mistral. ## Environment variables You can set your AWS credentials as environment variables ([among other options](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#using-environment-variables)): You can then use BedrockConverseModel by name: Or initialize the model directly with just the model name: ## Customizing Bedrock Runtime API You can customize the Bedrock Runtime API calls by adding additional parameters, such as [guardrail configurations](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html) and [performance settings](https://docs.aws.amazon.com/bedrock/latest/userguide/latency-optimized-inference.html). For a complete list of configurable parameters, refer to the documentation for [ BedrockModelSettings ](../../api/models/bedrock/index.html#pydantic_ai.models.bedrock.BedrockModelSettings). customize\\_bedrock\\_model\\_settings.py ## provider argument You can provide a custom BedrockProvider via the provider argument. This is useful when you want to specify credentials directly or use a custom boto3 client: You can also pass a pre-configured boto3 client:", "url": "https://ai.pydantic.dev/models/bedrock/index.html#bedrock", "page": "models/bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "heading_level": 2, "md_text": "To use BedrockConverseModel , you need to either install pydantic-ai , or install pydantic-ai-slim with the bedrock optional group: pipuv", "url": "https://ai.pydantic.dev/models/bedrock/index.html#install", "page": "models/bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "Configuration", "anchor": "configuration", "heading_level": 2, "md_text": "To use [AWS Bedrock](https://aws.amazon.com/bedrock/), you'll need an AWS account with Bedrock enabled and appropriate credentials. You can use either AWS credentials directly or a pre-configured boto3 client. BedrockModelName contains a list of available Bedrock models, including models from Anthropic, Amazon, Cohere, Meta, and Mistral.", "url": "https://ai.pydantic.dev/models/bedrock/index.html#configuration", "page": "models/bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "Environment variables", "anchor": "environment-variables", "heading_level": 2, "md_text": "You can set your AWS credentials as environment variables ([among other options](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/configuration.html#using-environment-variables)): You can then use BedrockConverseModel by name: Or initialize the model directly with just the model name:", "url": "https://ai.pydantic.dev/models/bedrock/index.html#environment-variables", "page": "models/bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "Customizing Bedrock Runtime API", "anchor": "customizing-bedrock-runtime-api", "heading_level": 2, "md_text": "You can customize the Bedrock Runtime API calls by adding additional parameters, such as [guardrail configurations](https://docs.aws.amazon.com/bedrock/latest/userguide/guardrails.html) and [performance settings](https://docs.aws.amazon.com/bedrock/latest/userguide/latency-optimized-inference.html). For a complete list of configurable parameters, refer to the documentation for [ BedrockModelSettings ](../../api/models/bedrock/index.html#pydantic_ai.models.bedrock.BedrockModelSettings). customize\\_bedrock\\_model\\_settings.py", "url": "https://ai.pydantic.dev/models/bedrock/index.html#customizing-bedrock-runtime-api", "page": "models/bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "provider argument", "anchor": "provider-argument", "heading_level": 2, "md_text": "You can provide a custom BedrockProvider via the provider argument. This is useful when you want to specify credentials directly or use a custom boto3 client: You can also pass a pre-configured boto3 client:", "url": "https://ai.pydantic.dev/models/bedrock/index.html#provider-argument", "page": "models/bedrock/index.html", "source_site": "pydantic_ai"}
{"title": "Groq", "anchor": "groq", "heading_level": 1, "md_text": "## Install To use GroqModel , you need to either install pydantic-ai , or install pydantic-ai-slim with the groq optional group: pipuv ## Configuration To use [Groq](https://groq.com/) through their API, go to [console.groq.com/keys](https://console.groq.com/keys) and follow your nose until you find the place to generate an API key. GroqModelName contains a list of available Groq models. ## Environment variable Once you have the API key, you can set it as an environment variable: You can then use GroqModel by name: Or initialise the model directly with just the model name: ## provider argument You can provide a custom Provider via the provider argument: You can also customize the GroqProvider with a custom httpx.AsyncHTTPClient :", "url": "https://ai.pydantic.dev/models/groq/index.html#groq", "page": "models/groq/index.html", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "heading_level": 2, "md_text": "To use GroqModel , you need to either install pydantic-ai , or install pydantic-ai-slim with the groq optional group: pipuv", "url": "https://ai.pydantic.dev/models/groq/index.html#install", "page": "models/groq/index.html", "source_site": "pydantic_ai"}
{"title": "Configuration", "anchor": "configuration", "heading_level": 2, "md_text": "To use [Groq](https://groq.com/) through their API, go to [console.groq.com/keys](https://console.groq.com/keys) and follow your nose until you find the place to generate an API key. GroqModelName contains a list of available Groq models.", "url": "https://ai.pydantic.dev/models/groq/index.html#configuration", "page": "models/groq/index.html", "source_site": "pydantic_ai"}
{"title": "Environment variable", "anchor": "environment-variable", "heading_level": 2, "md_text": "Once you have the API key, you can set it as an environment variable: You can then use GroqModel by name: Or initialise the model directly with just the model name:", "url": "https://ai.pydantic.dev/models/groq/index.html#environment-variable", "page": "models/groq/index.html", "source_site": "pydantic_ai"}
{"title": "provider argument", "anchor": "provider-argument", "heading_level": 2, "md_text": "You can provide a custom Provider via the provider argument: You can also customize the GroqProvider with a custom httpx.AsyncHTTPClient :", "url": "https://ai.pydantic.dev/models/groq/index.html#provider-argument", "page": "models/groq/index.html", "source_site": "pydantic_ai"}
{"title": "Hugging Face", "anchor": "hugging-face", "heading_level": 1, "md_text": "[Hugging Face](https://huggingface.co/) is an AI platform with all major open source models, datasets, MCPs, and demos. You can use [Inference Providers](https://huggingface.co/docs/inference-providers) to run open source models like DeepSeek R1 on scalable serverless infrastructure. ## Install To use HuggingFaceModel , you need to either install pydantic-ai , or install pydantic-ai-slim with the huggingface optional group: pipuv ## Configuration To use [Hugging Face](https://huggingface.co/) inference, you'll need to set up an account which will give you [free tier](https://huggingface.co/docs/inference-providers/pricing) allowance on [Inference Providers](https://huggingface.co/docs/inference-providers). To setup inference, follow these steps: 1. Go to [Hugging Face](https://huggingface.co/join) and sign up for an account. 2. Create a new access token in [Hugging Face](https://huggingface.co/settings/tokens). 3. Set the HF_TOKEN environment variable to the token you just created. Once you have a Hugging Face access token, you can set it as an environment variable: ## Usage You can then use [ HuggingFaceModel ](../../api/models/huggingface/index.html#pydantic_ai.models.huggingface.HuggingFaceModel) by name: Or initialise the model directly with just the model name: By default, the [ HuggingFaceModel ](../../api/models/huggingface/index.html#pydantic_ai.models.huggingface.HuggingFaceModel) uses the [ HuggingFaceProvider ](../../api/providers/index.html#pydantic_ai.providers.huggingface.HuggingFaceProvider) that will select automatically the first of the inference providers (Cerebras, Together AI, Cohere..etc) available for the model, sorted by your preferred order in https://hf.co/settings/inference-providers. ## Configure the provider If you want to pass parameters in code to the provider, you can programmatically instantiate the [ HuggingFaceProvider ](../../api/providers/index.html#pydantic_ai.providers.huggingface.HuggingFaceProvider) and pass it to the model: ## Custom Hugging Face client [ HuggingFaceProvider ](../../api/providers/index.html#pydantic_ai.providers.huggingface.HuggingFaceProvider) also accepts a custom [ AsyncInferenceClient ](https://huggingface.co/docs/huggingface_hub/v0.29.3/en/package_reference/inference_client#huggingface_hub.AsyncInferenceClient) client via the hf_client parameter, so you can customise the headers , bill_to (billing to an HF organization you're a member of), base_url etc. as defined in the [Hugging Face Hub python library docs](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client).", "url": "https://ai.pydantic.dev/models/huggingface/index.html#hugging-face", "page": "models/huggingface/index.html", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "heading_level": 2, "md_text": "To use HuggingFaceModel , you need to either install pydantic-ai , or install pydantic-ai-slim with the huggingface optional group: pipuv", "url": "https://ai.pydantic.dev/models/huggingface/index.html#install", "page": "models/huggingface/index.html", "source_site": "pydantic_ai"}
{"title": "Configuration", "anchor": "configuration", "heading_level": 2, "md_text": "To use [Hugging Face](https://huggingface.co/) inference, you'll need to set up an account which will give you [free tier](https://huggingface.co/docs/inference-providers/pricing) allowance on [Inference Providers](https://huggingface.co/docs/inference-providers). To setup inference, follow these steps: 1. Go to [Hugging Face](https://huggingface.co/join) and sign up for an account. 2. Create a new access token in [Hugging Face](https://huggingface.co/settings/tokens). 3. Set the HF_TOKEN environment variable to the token you just created. Once you have a Hugging Face access token, you can set it as an environment variable:", "url": "https://ai.pydantic.dev/models/huggingface/index.html#configuration", "page": "models/huggingface/index.html", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 2, "md_text": "You can then use [ HuggingFaceModel ](../../api/models/huggingface/index.html#pydantic_ai.models.huggingface.HuggingFaceModel) by name: Or initialise the model directly with just the model name: By default, the [ HuggingFaceModel ](../../api/models/huggingface/index.html#pydantic_ai.models.huggingface.HuggingFaceModel) uses the [ HuggingFaceProvider ](../../api/providers/index.html#pydantic_ai.providers.huggingface.HuggingFaceProvider) that will select automatically the first of the inference providers (Cerebras, Together AI, Cohere..etc) available for the model, sorted by your preferred order in https://hf.co/settings/inference-providers.", "url": "https://ai.pydantic.dev/models/huggingface/index.html#usage", "page": "models/huggingface/index.html", "source_site": "pydantic_ai"}
{"title": "Configure the provider", "anchor": "configure-the-provider", "heading_level": 2, "md_text": "If you want to pass parameters in code to the provider, you can programmatically instantiate the [ HuggingFaceProvider ](../../api/providers/index.html#pydantic_ai.providers.huggingface.HuggingFaceProvider) and pass it to the model:", "url": "https://ai.pydantic.dev/models/huggingface/index.html#configure-the-provider", "page": "models/huggingface/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Hugging Face client", "anchor": "custom-hugging-face-client", "heading_level": 2, "md_text": "[ HuggingFaceProvider ](../../api/providers/index.html#pydantic_ai.providers.huggingface.HuggingFaceProvider) also accepts a custom [ AsyncInferenceClient ](https://huggingface.co/docs/huggingface_hub/v0.29.3/en/package_reference/inference_client#huggingface_hub.AsyncInferenceClient) client via the hf_client parameter, so you can customise the headers , bill_to (billing to an HF organization you're a member of), base_url etc. as defined in the [Hugging Face Hub python library docs](https://huggingface.co/docs/huggingface_hub/package_reference/inference_client).", "url": "https://ai.pydantic.dev/models/huggingface/index.html#custom-hugging-face-client", "page": "models/huggingface/index.html", "source_site": "pydantic_ai"}
{"title": "Client", "anchor": "client", "heading_level": 1, "md_text": "Pydantic AI can act as an [MCP client](https://modelcontextprotocol.io/quickstart/client), connecting to MCP servers to use their tools. ## Install You need to either install [ pydantic-ai ](../../install/index.html), or [ pydantic-ai-slim ](../../install/index.html#slim-install) with the mcp optional group: pipuv ## Usage Pydantic AI comes with three ways to connect to MCP servers: * [ MCPServerStreamableHTTP ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServerStreamableHTTP) which connects to an MCP server using the [Streamable HTTP](https://modelcontextprotocol.io/introduction#streamable-http) transport * [ MCPServerSSE ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServerSSE) which connects to an MCP server using the [HTTP SSE](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse) transport * [ MCPServerStdio ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServerStdio) which runs the server as a subprocess and connects to it using the [stdio](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio) transport Examples of all three are shown below. Each MCP server instance is a [toolset](../../toolsets/index.html) and can be registered with an [ Agent ](../../api/agent/index.html#pydantic_ai.agent.Agent) using the toolsets argument. You can use the [ async with agent ](../../api/agent/index.html#pydantic_ai.agent.Agent.__aenter__) context manager to open and close connections to all registered servers (and in the case of stdio servers, start and stop the subprocesses) around the context where they'll be used in agent runs. You can also use [ async with server ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServer.__aenter__) to manage the connection or subprocess of a specific server, for example if you'd like to use it with multiple agents. If you don't explicitly enter one of these context managers to set up the server, this will be done automatically when it's needed (e.g. to list the available tools or call a specific tool), but it's more efficient to do so around the entire context where you expect the servers to be used. ### Streamable HTTP Client [ MCPServerStreamableHTTP ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServerStreamableHTTP) connects over HTTP using the [Streamable HTTP](https://modelcontextprotocol.io/introduction#streamable-http) transport to a server. Note [ MCPServerStreamableHTTP ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServerStreamableHTTP) requires an MCP server to be running and accepting HTTP connections before running the agent. Running the server is not managed by Pydantic AI. Before creating the Streamable HTTP client, we need to run a server that supports the Streamable HTTP transport. streamable\\_http\\_server.py Then we can create the client: mcp\\_streamable\\_http\\_client.py 1. Define the MCP server with the URL used to connect. 2. Create an agent with the MCP server attached. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* **What's happening here?** * The model receives the prompt \"What is 7 plus 5?\" * The model decides \"Oh, I've got this add tool, that will be a good way to answer this question\" * The model returns a tool call * Pydantic AI sends the tool call to the MCP server using the Streamable HTTP transport * The model is called again with the return value of running the add tool (12) * The model returns the final answer You can visualise this clearly, and even see the tool call, by adding three lines of code to instrument the example with [logfire](https://logfire.pydantic.dev/docs): mcp\\_sse\\_client\\_logfire.py ### SSE Client [ MCPServerSSE ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServerSSE) connects over HTTP using the [HTTP + Server Sent Events transport](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse) to a server. Note The SSE transport in MCP is deprecated, you should use Streamable HTTP instead. Before creating the SSE client, we need to run a server that supports the SSE transport. sse\\_server.py Then we can create the client: mcp\\_sse\\_client.py 1. Define the MCP server with the URL used to connect. 2. Create an agent with the MCP server attached. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* ### MCP \"stdio\" Server MCP also offers [stdio transport](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio) where the server is run as a subprocess and communicates with the client over stdin and stdout . In this case, you'd use the [ MCPServerStdio ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServerStdio) class. In this example [mcp-run-python](https://github.com/pydantic/mcp-run-python) is used as the MCP server. mcp\\_stdio\\_client.py 1. See [MCP Run Python](https://github.com/pydantic/mcp-run-python) for more information. ## Loading MCP Servers from Configuration Instead of creating MCP server instances individually in code, you can load multiple servers from a JSON configuration file using [ load_mcp_servers() ](../../api/mcp/index.html#pydantic_ai.mcp.load_mcp_servers). This is particularly useful when you need to manage multiple MCP servers or want to configure servers externally without modifying code. ### Configuration Format The configuration file should be a JSON file with an mcpServers object containing server definitions. Each server is identified by a unique key and contains the configuration for that server type: mcp\\_config.json Note The MCP server is only inferred to be an SSE server because of the /sse suffix. Any other server with the \"url\" field will be inferred to be a Streamable HTTP server. We made this decision given that the SSE transport is deprecated. ### Usage mcp\\_config\\_loader.py *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* ## Tool call customization The MCP servers provide the ability to set a process_tool_call which allows the customization of tool call requests and their responses. A common use case for this is to inject metadata to the requests which the server call needs: mcp\\_process\\_tool\\_call.py How to access the metadata is MCP server SDK specific. For example with the [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk), it is accessible via the [ ctx: Context ](https://github.com/modelcontextprotocol/python-sdk#context) argument that can be included on tool call handlers: mcp\\_server.py ## Using Tool Prefixes to Avoid Naming Conflicts When connecting to multiple MCP servers that might provide tools with the same name, you can use the tool_prefix parameter to avoid naming conflicts. This parameter adds a prefix to all tool names from a specific server. This allows you to use multiple servers that might have overlapping tool names without conflicts: mcp\\_tool\\_prefix\\_http\\_client.py ## Tool metadata MCP tools can include metadata that provides additional information about the tool's characteristics, which can be useful when [filtering tools](../../api/toolsets/index.html#pydantic_ai.toolsets.FilteredToolset). The meta , annotations , and output_schema fields can be found on the metadata dict on the [ ToolDefinition ](../../api/tools/index.html#pydantic_ai.tools.ToolDefinition) object that's passed to filter functions. ## Custom TLS / SSL configuration In some environments you need to tweak how HTTPS connections are established  for example to trust an internal Certificate Authority, present a client certificate for **mTLS**, or (during local development only!) disable certificate verification altogether. All HTTP-based MCP client classes ([ MCPServerStreamableHTTP ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServerStreamableHTTP) and [ MCPServerSSE ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServerSSE)) expose an http_client parameter that lets you pass your own pre-configured [ httpx.AsyncClient ](https://www.python-httpx.org/async/). mcp\\_custom\\_tls\\_client.py 1. When you supply http_client , Pydantic AI re-uses this client for every request. Anything supported by **httpx** ( verify , cert , custom proxies, timeouts, etc.) therefore applies to all MCP traffic. ## MCP Sampling What is MCP Sampling? In MCP [sampling](https://modelcontextprotocol.io/docs/concepts/sampling) is a system by which an MCP server can make LLM calls via the MCP client - effectively proxying requests to an LLM via the client over whatever transport is being used. Sampling is extremely useful when MCP servers need to use Gen AI but you don't want to provision them each with their own LLM credentials or when a public MCP server would like the connecting client to pay for LLM calls. Confusingly it has nothing to do with the concept of \"sampling\" in observability, or frankly the concept of \"sampling\" in any other domain. Sampling Diagram Here's a mermaid diagram that may or may not make the data flow clearer: Pydantic AI supports sampling as both a client and server. See the [server](../server/index.html#mcp-sampling) documentation for details on how to use sampling within a server. Sampling is automatically supported by Pydantic AI agents when they act as a client. To be able to use sampling, an MCP server instance needs to have a [ sampling_model ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServer.sampling_model) set. This can be done either directly on the server using the constructor keyword argument or the property, or by using [ agent.set_mcp_sampling_model() ](../../api/agent/index.html#pydantic_ai.agent.Agent.set_mcp_sampling_model) to set the agent's model or one specified as an argument as the sampling model on all MCP servers registered with that agent. Let's say we have an MCP server that wants to use sampling (in this case to generate an SVG as per the tool arguments). Sampling MCP Server generate\\_svg.py \\w*$(.+?) Using this server with an Agent will automatically allow sampling: sampling\\_mcp\\_client.py *(This example is complete, it can be run \"as is\")* You can disallow sampling by setting [ allow_sampling=False ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServer.allow_sampling) when creating the server reference, e.g.: sampling\\_disallowed.py ## Elicitation In MCP, [elicitation](https://modelcontextprotocol.io/docs/concepts/elicitation) allows a server to request for [structured input](https://modelcontextprotocol.io/specification/2025-06-18/client/elicitation#supported-schema-types) from the client for missing or additional context during a session. Elicitation let models essentially say \"Hold on - I need to know X before i can continue\" rather than requiring everything upfront or taking a shot in the dark. ### How Elicitation works Elicitation introduces a new protocol message type called [ ElicitRequest ](https://modelcontextprotocol.io/specification/2025-06-18/schema#elicitrequest), which is sent from the server to the client when it needs additional information. The client can then respond with an [ ElicitResult ](https://modelcontextprotocol.io/specification/2025-06-18/schema#elicitresult) or an ErrorData message. Here's a typical interaction: * User makes a request to the MCP server (e.g. \"Book a table at that Italian place\") * The server identifies that it needs more information (e.g. \"Which Italian place?\", \"What date and time?\") * The server sends an ElicitRequest to the client asking for the missing information. * The client receives the request, presents it to the user (e.g. via a terminal prompt, GUI dialog, or web interface). * User provides the requested information, decline or cancel the request. * The client sends an ElicitResult back to the server with the user's response. * With the structured data, the server can continue processing the original request. This allows for a more interactive and user-friendly experience, especially for multi-staged workflows. Instead of requiring all information upfront, the server can ask for it as needed, making the interaction feel more natural. ### Setting up Elicitation To enable elicitation, provide an [ elicitation_callback ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServer.elicitation_callback) function when creating your MCP server instance: restaurant\\_server.py This server demonstrates elicitation by requesting structured booking details from the client when the book_table tool is called. Here's how to create a client that handles these elicitation requests: client\\_example.py ### Supported Schema Types MCP elicitation supports string, number, boolean, and enum types with flat object structures only. These limitations ensure reliable cross-client compatibility. See [supported schema types](https://modelcontextprotocol.io/specification/2025-06-18/client/elicitation#supported-schema-types) for details. ### Security MCP Elicitation requires careful handling - servers must not request sensitive information, and clients must implement user approval controls with clear explanations. See [security considerations](https://modelcontextprotocol.io/specification/2025-06-18/client/elicitation#security-considerations) for details.", "url": "https://ai.pydantic.dev/mcp/client/index.html#client", "page": "mcp/client/index.html", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "heading_level": 2, "md_text": "You need to either install [ pydantic-ai ](../../install/index.html), or [ pydantic-ai-slim ](../../install/index.html#slim-install) with the mcp optional group: pipuv", "url": "https://ai.pydantic.dev/mcp/client/index.html#install", "page": "mcp/client/index.html", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 2, "md_text": "Pydantic AI comes with three ways to connect to MCP servers: * [ MCPServerStreamableHTTP ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServerStreamableHTTP) which connects to an MCP server using the [Streamable HTTP](https://modelcontextprotocol.io/introduction#streamable-http) transport * [ MCPServerSSE ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServerSSE) which connects to an MCP server using the [HTTP SSE](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse) transport * [ MCPServerStdio ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServerStdio) which runs the server as a subprocess and connects to it using the [stdio](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio) transport Examples of all three are shown below. Each MCP server instance is a [toolset](../../toolsets/index.html) and can be registered with an [ Agent ](../../api/agent/index.html#pydantic_ai.agent.Agent) using the toolsets argument. You can use the [ async with agent ](../../api/agent/index.html#pydantic_ai.agent.Agent.__aenter__) context manager to open and close connections to all registered servers (and in the case of stdio servers, start and stop the subprocesses) around the context where they'll be used in agent runs. You can also use [ async with server ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServer.__aenter__) to manage the connection or subprocess of a specific server, for example if you'd like to use it with multiple agents. If you don't explicitly enter one of these context managers to set up the server, this will be done automatically when it's needed (e.g. to list the available tools or call a specific tool), but it's more efficient to do so around the entire context where you expect the servers to be used. ### Streamable HTTP Client [ MCPServerStreamableHTTP ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServerStreamableHTTP) connects over HTTP using the [Streamable HTTP](https://modelcontextprotocol.io/introduction#streamable-http) transport to a server. Note [ MCPServerStreamableHTTP ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServerStreamableHTTP) requires an MCP server to be running and accepting HTTP connections before running the agent. Running the server is not managed by Pydantic AI. Before creating the Streamable HTTP client, we need to run a server that supports the Streamable HTTP transport. streamable\\_http\\_server.py Then we can create the client: mcp\\_streamable\\_http\\_client.py 1. Define the MCP server with the URL used to connect. 2. Create an agent with the MCP server attached. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* **What's happening here?** * The model receives the prompt \"What is 7 plus 5?\" * The model decides \"Oh, I've got this add tool, that will be a good way to answer this question\" * The model returns a tool call * Pydantic AI sends the tool call to the MCP server using the Streamable HTTP transport * The model is called again with the return value of running the add tool (12) * The model returns the final answer You can visualise this clearly, and even see the tool call, by adding three lines of code to instrument the example with [logfire](https://logfire.pydantic.dev/docs): mcp\\_sse\\_client\\_logfire.py ### SSE Client [ MCPServerSSE ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServerSSE) connects over HTTP using the [HTTP + Server Sent Events transport](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse) to a server. Note The SSE transport in MCP is deprecated, you should use Streamable HTTP instead. Before creating the SSE client, we need to run a server that supports the SSE transport. sse\\_server.py Then we can create the client: mcp\\_sse\\_client.py 1. Define the MCP server with the URL used to connect. 2. Create an agent with the MCP server attached. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* ### MCP \"stdio\" Server MCP also offers [stdio transport](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio) where the server is run as a subprocess and communicates with the client over stdin and stdout . In this case, you'd use the [ MCPServerStdio ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServerStdio) class. In this example [mcp-run-python](https://github.com/pydantic/mcp-run-python) is used as the MCP server. mcp\\_stdio\\_client.py 1. See [MCP Run Python](https://github.com/pydantic/mcp-run-python) for more information.", "url": "https://ai.pydantic.dev/mcp/client/index.html#usage", "page": "mcp/client/index.html", "source_site": "pydantic_ai"}
{"title": "Streamable HTTP Client", "anchor": "streamable-http-client", "heading_level": 3, "md_text": "[ MCPServerStreamableHTTP ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServerStreamableHTTP) connects over HTTP using the [Streamable HTTP](https://modelcontextprotocol.io/introduction#streamable-http) transport to a server. Note [ MCPServerStreamableHTTP ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServerStreamableHTTP) requires an MCP server to be running and accepting HTTP connections before running the agent. Running the server is not managed by Pydantic AI. Before creating the Streamable HTTP client, we need to run a server that supports the Streamable HTTP transport. streamable\\_http\\_server.py Then we can create the client: mcp\\_streamable\\_http\\_client.py 1. Define the MCP server with the URL used to connect. 2. Create an agent with the MCP server attached. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* **What's happening here?** * The model receives the prompt \"What is 7 plus 5?\" * The model decides \"Oh, I've got this add tool, that will be a good way to answer this question\" * The model returns a tool call * Pydantic AI sends the tool call to the MCP server using the Streamable HTTP transport * The model is called again with the return value of running the add tool (12) * The model returns the final answer You can visualise this clearly, and even see the tool call, by adding three lines of code to instrument the example with [logfire](https://logfire.pydantic.dev/docs): mcp\\_sse\\_client\\_logfire.py", "url": "https://ai.pydantic.dev/mcp/client/index.html#streamable-http-client", "page": "mcp/client/index.html", "source_site": "pydantic_ai"}
{"title": "SSE Client", "anchor": "sse-client", "heading_level": 3, "md_text": "[ MCPServerSSE ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServerSSE) connects over HTTP using the [HTTP + Server Sent Events transport](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#http-with-sse) to a server. Note The SSE transport in MCP is deprecated, you should use Streamable HTTP instead. Before creating the SSE client, we need to run a server that supports the SSE transport. sse\\_server.py Then we can create the client: mcp\\_sse\\_client.py 1. Define the MCP server with the URL used to connect. 2. Create an agent with the MCP server attached. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/mcp/client/index.html#sse-client", "page": "mcp/client/index.html", "source_site": "pydantic_ai"}
{"title": "MCP \"stdio\" Server", "anchor": "mcp-stdio-server", "heading_level": 3, "md_text": "MCP also offers [stdio transport](https://spec.modelcontextprotocol.io/specification/2024-11-05/basic/transports/#stdio) where the server is run as a subprocess and communicates with the client over stdin and stdout . In this case, you'd use the [ MCPServerStdio ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServerStdio) class. In this example [mcp-run-python](https://github.com/pydantic/mcp-run-python) is used as the MCP server. mcp\\_stdio\\_client.py 1. See [MCP Run Python](https://github.com/pydantic/mcp-run-python) for more information.", "url": "https://ai.pydantic.dev/mcp/client/index.html#mcp-stdio-server", "page": "mcp/client/index.html", "source_site": "pydantic_ai"}
{"title": "Loading MCP Servers from Configuration", "anchor": "loading-mcp-servers-from-configuration", "heading_level": 2, "md_text": "Instead of creating MCP server instances individually in code, you can load multiple servers from a JSON configuration file using [ load_mcp_servers() ](../../api/mcp/index.html#pydantic_ai.mcp.load_mcp_servers). This is particularly useful when you need to manage multiple MCP servers or want to configure servers externally without modifying code. ### Configuration Format The configuration file should be a JSON file with an mcpServers object containing server definitions. Each server is identified by a unique key and contains the configuration for that server type: mcp\\_config.json Note The MCP server is only inferred to be an SSE server because of the /sse suffix. Any other server with the \"url\" field will be inferred to be a Streamable HTTP server. We made this decision given that the SSE transport is deprecated. ### Usage mcp\\_config\\_loader.py *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/mcp/client/index.html#loading-mcp-servers-from-configuration", "page": "mcp/client/index.html", "source_site": "pydantic_ai"}
{"title": "Configuration Format", "anchor": "configuration-format", "heading_level": 3, "md_text": "The configuration file should be a JSON file with an mcpServers object containing server definitions. Each server is identified by a unique key and contains the configuration for that server type: mcp\\_config.json Note The MCP server is only inferred to be an SSE server because of the /sse suffix. Any other server with the \"url\" field will be inferred to be a Streamable HTTP server. We made this decision given that the SSE transport is deprecated.", "url": "https://ai.pydantic.dev/mcp/client/index.html#configuration-format", "page": "mcp/client/index.html", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage_1", "heading_level": 3, "md_text": "mcp\\_config\\_loader.py *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/mcp/client/index.html#usage_1", "page": "mcp/client/index.html", "source_site": "pydantic_ai"}
{"title": "Tool call customization", "anchor": "tool-call-customization", "heading_level": 2, "md_text": "The MCP servers provide the ability to set a process_tool_call which allows the customization of tool call requests and their responses. A common use case for this is to inject metadata to the requests which the server call needs: mcp\\_process\\_tool\\_call.py How to access the metadata is MCP server SDK specific. For example with the [MCP Python SDK](https://github.com/modelcontextprotocol/python-sdk), it is accessible via the [ ctx: Context ](https://github.com/modelcontextprotocol/python-sdk#context) argument that can be included on tool call handlers: mcp\\_server.py", "url": "https://ai.pydantic.dev/mcp/client/index.html#tool-call-customization", "page": "mcp/client/index.html", "source_site": "pydantic_ai"}
{"title": "Using Tool Prefixes to Avoid Naming Conflicts", "anchor": "using-tool-prefixes-to-avoid-naming-conflicts", "heading_level": 2, "md_text": "When connecting to multiple MCP servers that might provide tools with the same name, you can use the tool_prefix parameter to avoid naming conflicts. This parameter adds a prefix to all tool names from a specific server. This allows you to use multiple servers that might have overlapping tool names without conflicts: mcp\\_tool\\_prefix\\_http\\_client.py", "url": "https://ai.pydantic.dev/mcp/client/index.html#using-tool-prefixes-to-avoid-naming-conflicts", "page": "mcp/client/index.html", "source_site": "pydantic_ai"}
{"title": "Tool metadata", "anchor": "tool-metadata", "heading_level": 2, "md_text": "MCP tools can include metadata that provides additional information about the tool's characteristics, which can be useful when [filtering tools](../../api/toolsets/index.html#pydantic_ai.toolsets.FilteredToolset). The meta , annotations , and output_schema fields can be found on the metadata dict on the [ ToolDefinition ](../../api/tools/index.html#pydantic_ai.tools.ToolDefinition) object that's passed to filter functions.", "url": "https://ai.pydantic.dev/mcp/client/index.html#tool-metadata", "page": "mcp/client/index.html", "source_site": "pydantic_ai"}
{"title": "Custom TLS / SSL configuration", "anchor": "custom-tls-ssl-configuration", "heading_level": 2, "md_text": "In some environments you need to tweak how HTTPS connections are established  for example to trust an internal Certificate Authority, present a client certificate for **mTLS**, or (during local development only!) disable certificate verification altogether. All HTTP-based MCP client classes ([ MCPServerStreamableHTTP ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServerStreamableHTTP) and [ MCPServerSSE ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServerSSE)) expose an http_client parameter that lets you pass your own pre-configured [ httpx.AsyncClient ](https://www.python-httpx.org/async/). mcp\\_custom\\_tls\\_client.py 1. When you supply http_client , Pydantic AI re-uses this client for every request. Anything supported by **httpx** ( verify , cert , custom proxies, timeouts, etc.) therefore applies to all MCP traffic.", "url": "https://ai.pydantic.dev/mcp/client/index.html#custom-tls-ssl-configuration", "page": "mcp/client/index.html", "source_site": "pydantic_ai"}
{"title": "MCP Sampling", "anchor": "mcp-sampling", "heading_level": 2, "md_text": "What is MCP Sampling? In MCP [sampling](https://modelcontextprotocol.io/docs/concepts/sampling) is a system by which an MCP server can make LLM calls via the MCP client - effectively proxying requests to an LLM via the client over whatever transport is being used. Sampling is extremely useful when MCP servers need to use Gen AI but you don't want to provision them each with their own LLM credentials or when a public MCP server would like the connecting client to pay for LLM calls. Confusingly it has nothing to do with the concept of \"sampling\" in observability, or frankly the concept of \"sampling\" in any other domain. Sampling Diagram Here's a mermaid diagram that may or may not make the data flow clearer: Pydantic AI supports sampling as both a client and server. See the [server](../server/index.html#mcp-sampling) documentation for details on how to use sampling within a server. Sampling is automatically supported by Pydantic AI agents when they act as a client. To be able to use sampling, an MCP server instance needs to have a [ sampling_model ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServer.sampling_model) set. This can be done either directly on the server using the constructor keyword argument or the property, or by using [ agent.set_mcp_sampling_model() ](../../api/agent/index.html#pydantic_ai.agent.Agent.set_mcp_sampling_model) to set the agent's model or one specified as an argument as the sampling model on all MCP servers registered with that agent. Let's say we have an MCP server that wants to use sampling (in this case to generate an SVG as per the tool arguments). Sampling MCP Server generate\\_svg.py \\w*$(.+?) Using this server with an Agent will automatically allow sampling: sampling\\_mcp\\_client.py *(This example is complete, it can be run \"as is\")* You can disallow sampling by setting [ allow_sampling=False ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServer.allow_sampling) when creating the server reference, e.g.: sampling\\_disallowed.py", "url": "https://ai.pydantic.dev/mcp/client/index.html#mcp-sampling", "page": "mcp/client/index.html", "source_site": "pydantic_ai"}
{"title": "Elicitation", "anchor": "elicitation", "heading_level": 2, "md_text": "In MCP, [elicitation](https://modelcontextprotocol.io/docs/concepts/elicitation) allows a server to request for [structured input](https://modelcontextprotocol.io/specification/2025-06-18/client/elicitation#supported-schema-types) from the client for missing or additional context during a session. Elicitation let models essentially say \"Hold on - I need to know X before i can continue\" rather than requiring everything upfront or taking a shot in the dark. ### How Elicitation works Elicitation introduces a new protocol message type called [ ElicitRequest ](https://modelcontextprotocol.io/specification/2025-06-18/schema#elicitrequest), which is sent from the server to the client when it needs additional information. The client can then respond with an [ ElicitResult ](https://modelcontextprotocol.io/specification/2025-06-18/schema#elicitresult) or an ErrorData message. Here's a typical interaction: * User makes a request to the MCP server (e.g. \"Book a table at that Italian place\") * The server identifies that it needs more information (e.g. \"Which Italian place?\", \"What date and time?\") * The server sends an ElicitRequest to the client asking for the missing information. * The client receives the request, presents it to the user (e.g. via a terminal prompt, GUI dialog, or web interface). * User provides the requested information, decline or cancel the request. * The client sends an ElicitResult back to the server with the user's response. * With the structured data, the server can continue processing the original request. This allows for a more interactive and user-friendly experience, especially for multi-staged workflows. Instead of requiring all information upfront, the server can ask for it as needed, making the interaction feel more natural. ### Setting up Elicitation To enable elicitation, provide an [ elicitation_callback ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServer.elicitation_callback) function when creating your MCP server instance: restaurant\\_server.py This server demonstrates elicitation by requesting structured booking details from the client when the book_table tool is called. Here's how to create a client that handles these elicitation requests: client\\_example.py ### Supported Schema Types MCP elicitation supports string, number, boolean, and enum types with flat object structures only. These limitations ensure reliable cross-client compatibility. See [supported schema types](https://modelcontextprotocol.io/specification/2025-06-18/client/elicitation#supported-schema-types) for details. ### Security MCP Elicitation requires careful handling - servers must not request sensitive information, and clients must implement user approval controls with clear explanations. See [security considerations](https://modelcontextprotocol.io/specification/2025-06-18/client/elicitation#security-considerations) for details.", "url": "https://ai.pydantic.dev/mcp/client/index.html#elicitation", "page": "mcp/client/index.html", "source_site": "pydantic_ai"}
{"title": "How Elicitation works", "anchor": "how-elicitation-works", "heading_level": 3, "md_text": "Elicitation introduces a new protocol message type called [ ElicitRequest ](https://modelcontextprotocol.io/specification/2025-06-18/schema#elicitrequest), which is sent from the server to the client when it needs additional information. The client can then respond with an [ ElicitResult ](https://modelcontextprotocol.io/specification/2025-06-18/schema#elicitresult) or an ErrorData message. Here's a typical interaction: * User makes a request to the MCP server (e.g. \"Book a table at that Italian place\") * The server identifies that it needs more information (e.g. \"Which Italian place?\", \"What date and time?\") * The server sends an ElicitRequest to the client asking for the missing information. * The client receives the request, presents it to the user (e.g. via a terminal prompt, GUI dialog, or web interface). * User provides the requested information, decline or cancel the request. * The client sends an ElicitResult back to the server with the user's response. * With the structured data, the server can continue processing the original request. This allows for a more interactive and user-friendly experience, especially for multi-staged workflows. Instead of requiring all information upfront, the server can ask for it as needed, making the interaction feel more natural.", "url": "https://ai.pydantic.dev/mcp/client/index.html#how-elicitation-works", "page": "mcp/client/index.html", "source_site": "pydantic_ai"}
{"title": "Setting up Elicitation", "anchor": "setting-up-elicitation", "heading_level": 3, "md_text": "To enable elicitation, provide an [ elicitation_callback ](../../api/mcp/index.html#pydantic_ai.mcp.MCPServer.elicitation_callback) function when creating your MCP server instance: restaurant\\_server.py This server demonstrates elicitation by requesting structured booking details from the client when the book_table tool is called. Here's how to create a client that handles these elicitation requests: client\\_example.py", "url": "https://ai.pydantic.dev/mcp/client/index.html#setting-up-elicitation", "page": "mcp/client/index.html", "source_site": "pydantic_ai"}
{"title": "Supported Schema Types", "anchor": "supported-schema-types", "heading_level": 3, "md_text": "MCP elicitation supports string, number, boolean, and enum types with flat object structures only. These limitations ensure reliable cross-client compatibility. See [supported schema types](https://modelcontextprotocol.io/specification/2025-06-18/client/elicitation#supported-schema-types) for details.", "url": "https://ai.pydantic.dev/mcp/client/index.html#supported-schema-types", "page": "mcp/client/index.html", "source_site": "pydantic_ai"}
{"title": "Security", "anchor": "security", "heading_level": 3, "md_text": "MCP Elicitation requires careful handling - servers must not request sensitive information, and clients must implement user approval controls with clear explanations. See [security considerations](https://modelcontextprotocol.io/specification/2025-06-18/client/elicitation#security-considerations) for details.", "url": "https://ai.pydantic.dev/mcp/client/index.html#security", "page": "mcp/client/index.html", "source_site": "pydantic_ai"}
{"title": "Messages and chat history", "anchor": "messages-and-chat-history", "heading_level": 1, "md_text": "Pydantic AI provides access to messages exchanged during an agent run. These messages can be used both to continue a coherent conversation, and to understand how an agent performed. ### Accessing Messages from Results After running an agent, you can access the messages exchanged during that run from the result object. Both [ RunResult ](../api/agent/index.html#pydantic_ai.agent.AgentRunResult) (returned by [ Agent.run ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run), [ Agent.run_sync ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_sync)) and [ StreamedRunResult ](../api/result/index.html#pydantic_ai.result.StreamedRunResult) (returned by [ Agent.run_stream ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream)) have the following methods: * [ all_messages() ](../api/agent/index.html#pydantic_ai.agent.AgentRunResult.all_messages): returns all messages, including messages from prior runs. There's also a variant that returns JSON bytes, [ all_messages_json() ](../api/agent/index.html#pydantic_ai.agent.AgentRunResult.all_messages_json). * [ new_messages() ](../api/agent/index.html#pydantic_ai.agent.AgentRunResult.new_messages): returns only the messages from the current run. There's also a variant that returns JSON bytes, [ new_messages_json() ](../api/agent/index.html#pydantic_ai.agent.AgentRunResult.new_messages_json). StreamedRunResult and complete messages On [ StreamedRunResult ](../api/result/index.html#pydantic_ai.result.StreamedRunResult), the messages returned from these methods will only include the final result message once the stream has finished. E.g. you've awaited one of the following coroutines: * [ StreamedRunResult.stream_output() ](../api/result/index.html#pydantic_ai.result.StreamedRunResult.stream_output) * [ StreamedRunResult.stream_text() ](../api/result/index.html#pydantic_ai.result.StreamedRunResult.stream_text) * [ StreamedRunResult.stream_responses() ](../api/result/index.html#pydantic_ai.result.StreamedRunResult.stream_responses) * [ StreamedRunResult.get_output() ](../api/result/index.html#pydantic_ai.result.StreamedRunResult.get_output) **Note:** The final result message will NOT be added to result messages if you use [ .stream_text(delta=True) ](../api/result/index.html#pydantic_ai.result.StreamedRunResult.stream_text) since in this case the result content is never built as one string. Example of accessing methods on a [ RunResult ](../api/agent/index.html#pydantic_ai.agent.AgentRunResult) : run\\_result\\_messages.py *(This example is complete, it can be run \"as is\")* Example of accessing methods on a [ StreamedRunResult ](../api/result/index.html#pydantic_ai.result.StreamedRunResult) : streamed\\_run\\_result\\_messages.py *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* ### Using Messages as Input for Further Agent Runs The primary use of message histories in Pydantic AI is to maintain context across multiple agent runs. To use existing messages in a run, pass them to the message_history parameter of [ Agent.run ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run), [ Agent.run_sync ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_sync) or [ Agent.run_stream ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream). If message_history is set and not empty, a new system prompt is not generated  we assume the existing message history includes a system prompt. Reusing messages in a conversation *(This example is complete, it can be run \"as is\")* ## Storing and loading messages (to JSON) While maintaining conversation state in memory is enough for many applications, often times you may want to store the messages history of an agent run on disk or in a database. This might be for evals, for sharing data between Python and JavaScript/TypeScript, or any number of other use cases. The intended way to do this is using a TypeAdapter . We export [ ModelMessagesTypeAdapter ](../api/messages/index.html#pydantic_ai.messages.ModelMessagesTypeAdapter) that can be used for this, or you can create your own. Here's an example showing how: serialize messages to json 1. Alternatively, you can create a TypeAdapter from scratch: 2. Alternatively you can serialize to/from JSON directly: 3. You can now continue the conversation with history same_history_as_step_1 despite creating a new agent run. *(This example is complete, it can be run \"as is\")* ## Other ways of using messages Since messages are defined by simple dataclasses, you can manually create and manipulate, e.g. for testing. The message format is independent of the model used, so you can use messages in different agents, or the same agent with different models. In the example below, we reuse the message from the first agent run, which uses the openai:gpt-4o model, in a second agent run using the google-gla:gemini-1.5-pro model. Reusing messages with a different model ## Processing Message History Sometimes you may want to modify the message history before it's sent to the model. This could be for privacy reasons (filtering out sensitive information), to save costs on tokens, to give less context to the LLM, or custom processing logic. Pydantic AI provides a history_processors parameter on Agent that allows you to intercept and modify the message history before each model request. History processors replace the message history History processors replace the message history in the state with the processed messages, including the new user prompt part. This means that if you want to keep the original message history, you need to make a copy of it. ### Usage The history_processors is a list of callables that take a list of [ ModelMessage ](../api/messages/index.html#pydantic_ai.messages.ModelMessage) and return a modified list of the same type. Each processor is applied in sequence, and processors can be either synchronous or asynchronous. simple\\_history\\_processor.py #### Keep Only Recent Messages You can use the history_processor to only keep the recent messages: keep\\_recent\\_messages.py Be careful when slicing the message history When slicing the message history, you need to make sure that tool calls and returns are paired, otherwise the LLM may return an error. For more details, refer to [this GitHub issue](https://github.com/pydantic/pydantic-ai/issues/2050#issuecomment-3019976269). #### RunContext parameter History processors can optionally accept a [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext) parameter to access additional information about the current run, such as dependencies, model information, and usage statistics: context\\_aware\\_processor.py This allows for more sophisticated message processing based on the current state of the agent run. #### Summarize Old Messages Use an LLM to summarize older messages to preserve context while reducing tokens. summarize\\_old\\_messages.py Be careful when summarizing the message history When summarizing the message history, you need to make sure that tool calls and returns are paired, otherwise the LLM may return an error. For more details, refer to [this GitHub issue](https://github.com/pydantic/pydantic-ai/issues/2050#issuecomment-3019976269), where you can find examples of summarizing the message history. ### Testing History Processors You can test what messages are actually sent to the model provider using [ FunctionModel ](../api/models/function/index.html#pydantic_ai.models.function.FunctionModel): test\\_history\\_processor.py ### Multiple Processors You can also use multiple processors: multiple\\_history\\_processors.py In this case, the filter_responses processor will be applied first, and the summarize_old_messages processor will be applied second. ## Examples For a more complete example of using messages in conversations, see the [chat app](../examples/chat-app/index.html) example.", "url": "https://ai.pydantic.dev/message-history/index.html#messages-and-chat-history", "page": "message-history/index.html", "source_site": "pydantic_ai"}
{"title": "Accessing Messages from Results", "anchor": "accessing-messages-from-results", "heading_level": 3, "md_text": "After running an agent, you can access the messages exchanged during that run from the result object. Both [ RunResult ](../api/agent/index.html#pydantic_ai.agent.AgentRunResult) (returned by [ Agent.run ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run), [ Agent.run_sync ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_sync)) and [ StreamedRunResult ](../api/result/index.html#pydantic_ai.result.StreamedRunResult) (returned by [ Agent.run_stream ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream)) have the following methods: * [ all_messages() ](../api/agent/index.html#pydantic_ai.agent.AgentRunResult.all_messages): returns all messages, including messages from prior runs. There's also a variant that returns JSON bytes, [ all_messages_json() ](../api/agent/index.html#pydantic_ai.agent.AgentRunResult.all_messages_json). * [ new_messages() ](../api/agent/index.html#pydantic_ai.agent.AgentRunResult.new_messages): returns only the messages from the current run. There's also a variant that returns JSON bytes, [ new_messages_json() ](../api/agent/index.html#pydantic_ai.agent.AgentRunResult.new_messages_json). StreamedRunResult and complete messages On [ StreamedRunResult ](../api/result/index.html#pydantic_ai.result.StreamedRunResult), the messages returned from these methods will only include the final result message once the stream has finished. E.g. you've awaited one of the following coroutines: * [ StreamedRunResult.stream_output() ](../api/result/index.html#pydantic_ai.result.StreamedRunResult.stream_output) * [ StreamedRunResult.stream_text() ](../api/result/index.html#pydantic_ai.result.StreamedRunResult.stream_text) * [ StreamedRunResult.stream_responses() ](../api/result/index.html#pydantic_ai.result.StreamedRunResult.stream_responses) * [ StreamedRunResult.get_output() ](../api/result/index.html#pydantic_ai.result.StreamedRunResult.get_output) **Note:** The final result message will NOT be added to result messages if you use [ .stream_text(delta=True) ](../api/result/index.html#pydantic_ai.result.StreamedRunResult.stream_text) since in this case the result content is never built as one string. Example of accessing methods on a [ RunResult ](../api/agent/index.html#pydantic_ai.agent.AgentRunResult) : run\\_result\\_messages.py *(This example is complete, it can be run \"as is\")* Example of accessing methods on a [ StreamedRunResult ](../api/result/index.html#pydantic_ai.result.StreamedRunResult) : streamed\\_run\\_result\\_messages.py *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/message-history/index.html#accessing-messages-from-results", "page": "message-history/index.html", "source_site": "pydantic_ai"}
{"title": "Using Messages as Input for Further Agent Runs", "anchor": "using-messages-as-input-for-further-agent-runs", "heading_level": 3, "md_text": "The primary use of message histories in Pydantic AI is to maintain context across multiple agent runs. To use existing messages in a run, pass them to the message_history parameter of [ Agent.run ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run), [ Agent.run_sync ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_sync) or [ Agent.run_stream ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream). If message_history is set and not empty, a new system prompt is not generated  we assume the existing message history includes a system prompt. Reusing messages in a conversation *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/message-history/index.html#using-messages-as-input-for-further-agent-runs", "page": "message-history/index.html", "source_site": "pydantic_ai"}
{"title": "Storing and loading messages (to JSON)", "anchor": "storing-and-loading-messages-to-json", "heading_level": 2, "md_text": "While maintaining conversation state in memory is enough for many applications, often times you may want to store the messages history of an agent run on disk or in a database. This might be for evals, for sharing data between Python and JavaScript/TypeScript, or any number of other use cases. The intended way to do this is using a TypeAdapter . We export [ ModelMessagesTypeAdapter ](../api/messages/index.html#pydantic_ai.messages.ModelMessagesTypeAdapter) that can be used for this, or you can create your own. Here's an example showing how: serialize messages to json 1. Alternatively, you can create a TypeAdapter from scratch: 2. Alternatively you can serialize to/from JSON directly: 3. You can now continue the conversation with history same_history_as_step_1 despite creating a new agent run. *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/message-history/index.html#storing-and-loading-messages-to-json", "page": "message-history/index.html", "source_site": "pydantic_ai"}
{"title": "Other ways of using messages", "anchor": "other-ways-of-using-messages", "heading_level": 2, "md_text": "Since messages are defined by simple dataclasses, you can manually create and manipulate, e.g. for testing. The message format is independent of the model used, so you can use messages in different agents, or the same agent with different models. In the example below, we reuse the message from the first agent run, which uses the openai:gpt-4o model, in a second agent run using the google-gla:gemini-1.5-pro model. Reusing messages with a different model", "url": "https://ai.pydantic.dev/message-history/index.html#other-ways-of-using-messages", "page": "message-history/index.html", "source_site": "pydantic_ai"}
{"title": "Processing Message History", "anchor": "processing-message-history", "heading_level": 2, "md_text": "Sometimes you may want to modify the message history before it's sent to the model. This could be for privacy reasons (filtering out sensitive information), to save costs on tokens, to give less context to the LLM, or custom processing logic. Pydantic AI provides a history_processors parameter on Agent that allows you to intercept and modify the message history before each model request. History processors replace the message history History processors replace the message history in the state with the processed messages, including the new user prompt part. This means that if you want to keep the original message history, you need to make a copy of it. ### Usage The history_processors is a list of callables that take a list of [ ModelMessage ](../api/messages/index.html#pydantic_ai.messages.ModelMessage) and return a modified list of the same type. Each processor is applied in sequence, and processors can be either synchronous or asynchronous. simple\\_history\\_processor.py #### Keep Only Recent Messages You can use the history_processor to only keep the recent messages: keep\\_recent\\_messages.py Be careful when slicing the message history When slicing the message history, you need to make sure that tool calls and returns are paired, otherwise the LLM may return an error. For more details, refer to [this GitHub issue](https://github.com/pydantic/pydantic-ai/issues/2050#issuecomment-3019976269). #### RunContext parameter History processors can optionally accept a [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext) parameter to access additional information about the current run, such as dependencies, model information, and usage statistics: context\\_aware\\_processor.py This allows for more sophisticated message processing based on the current state of the agent run. #### Summarize Old Messages Use an LLM to summarize older messages to preserve context while reducing tokens. summarize\\_old\\_messages.py Be careful when summarizing the message history When summarizing the message history, you need to make sure that tool calls and returns are paired, otherwise the LLM may return an error. For more details, refer to [this GitHub issue](https://github.com/pydantic/pydantic-ai/issues/2050#issuecomment-3019976269), where you can find examples of summarizing the message history. ### Testing History Processors You can test what messages are actually sent to the model provider using [ FunctionModel ](../api/models/function/index.html#pydantic_ai.models.function.FunctionModel): test\\_history\\_processor.py ### Multiple Processors You can also use multiple processors: multiple\\_history\\_processors.py In this case, the filter_responses processor will be applied first, and the summarize_old_messages processor will be applied second.", "url": "https://ai.pydantic.dev/message-history/index.html#processing-message-history", "page": "message-history/index.html", "source_site": "pydantic_ai"}
{"title": "Usage", "anchor": "usage", "heading_level": 3, "md_text": "The history_processors is a list of callables that take a list of [ ModelMessage ](../api/messages/index.html#pydantic_ai.messages.ModelMessage) and return a modified list of the same type. Each processor is applied in sequence, and processors can be either synchronous or asynchronous. simple\\_history\\_processor.py #### Keep Only Recent Messages You can use the history_processor to only keep the recent messages: keep\\_recent\\_messages.py Be careful when slicing the message history When slicing the message history, you need to make sure that tool calls and returns are paired, otherwise the LLM may return an error. For more details, refer to [this GitHub issue](https://github.com/pydantic/pydantic-ai/issues/2050#issuecomment-3019976269). #### RunContext parameter History processors can optionally accept a [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext) parameter to access additional information about the current run, such as dependencies, model information, and usage statistics: context\\_aware\\_processor.py This allows for more sophisticated message processing based on the current state of the agent run. #### Summarize Old Messages Use an LLM to summarize older messages to preserve context while reducing tokens. summarize\\_old\\_messages.py Be careful when summarizing the message history When summarizing the message history, you need to make sure that tool calls and returns are paired, otherwise the LLM may return an error. For more details, refer to [this GitHub issue](https://github.com/pydantic/pydantic-ai/issues/2050#issuecomment-3019976269), where you can find examples of summarizing the message history.", "url": "https://ai.pydantic.dev/message-history/index.html#usage", "page": "message-history/index.html", "source_site": "pydantic_ai"}
{"title": "Testing History Processors", "anchor": "testing-history-processors", "heading_level": 3, "md_text": "You can test what messages are actually sent to the model provider using [ FunctionModel ](../api/models/function/index.html#pydantic_ai.models.function.FunctionModel): test\\_history\\_processor.py", "url": "https://ai.pydantic.dev/message-history/index.html#testing-history-processors", "page": "message-history/index.html", "source_site": "pydantic_ai"}
{"title": "Multiple Processors", "anchor": "multiple-processors", "heading_level": 3, "md_text": "You can also use multiple processors: multiple\\_history\\_processors.py In this case, the filter_responses processor will be applied first, and the summarize_old_messages processor will be applied second.", "url": "https://ai.pydantic.dev/message-history/index.html#multiple-processors", "page": "message-history/index.html", "source_site": "pydantic_ai"}
{"title": "Examples", "anchor": "examples", "heading_level": 2, "md_text": "For a more complete example of using messages in conversations, see the [chat app](../examples/chat-app/index.html) example.", "url": "https://ai.pydantic.dev/message-history/index.html#examples", "page": "message-history/index.html", "source_site": "pydantic_ai"}
{"title": "Mistral", "anchor": "mistral", "heading_level": 1, "md_text": "## Install To use MistralModel , you need to either install pydantic-ai , or install pydantic-ai-slim with the mistral optional group: pipuv ## Configuration To use [Mistral](https://mistral.ai) through their API, go to [console.mistral.ai/api-keys/](https://console.mistral.ai/api-keys/) and follow your nose until you find the place to generate an API key. LatestMistralModelNames contains a list of the most popular Mistral models. ## Environment variable Once you have the API key, you can set it as an environment variable: You can then use MistralModel by name: Or initialise the model directly with just the model name: ## provider argument You can provide a custom Provider via the provider argument: You can also customize the provider with a custom httpx.AsyncHTTPClient :", "url": "https://ai.pydantic.dev/models/mistral/index.html#mistral", "page": "models/mistral/index.html", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "heading_level": 2, "md_text": "To use MistralModel , you need to either install pydantic-ai , or install pydantic-ai-slim with the mistral optional group: pipuv", "url": "https://ai.pydantic.dev/models/mistral/index.html#install", "page": "models/mistral/index.html", "source_site": "pydantic_ai"}
{"title": "Configuration", "anchor": "configuration", "heading_level": 2, "md_text": "To use [Mistral](https://mistral.ai) through their API, go to [console.mistral.ai/api-keys/](https://console.mistral.ai/api-keys/) and follow your nose until you find the place to generate an API key. LatestMistralModelNames contains a list of the most popular Mistral models.", "url": "https://ai.pydantic.dev/models/mistral/index.html#configuration", "page": "models/mistral/index.html", "source_site": "pydantic_ai"}
{"title": "Environment variable", "anchor": "environment-variable", "heading_level": 2, "md_text": "Once you have the API key, you can set it as an environment variable: You can then use MistralModel by name: Or initialise the model directly with just the model name:", "url": "https://ai.pydantic.dev/models/mistral/index.html#environment-variable", "page": "models/mistral/index.html", "source_site": "pydantic_ai"}
{"title": "provider argument", "anchor": "provider-argument", "heading_level": 2, "md_text": "You can provide a custom Provider via the provider argument: You can also customize the provider with a custom httpx.AsyncHTTPClient :", "url": "https://ai.pydantic.dev/models/mistral/index.html#provider-argument", "page": "models/mistral/index.html", "source_site": "pydantic_ai"}
{"title": "Google", "anchor": "google", "heading_level": 1, "md_text": "The GoogleModel is a model that uses the [ google-genai ](https://pypi.org/project/google-genai/) package under the hood to access Google's Gemini models via both the Generative Language API and Vertex AI. ## Install To use GoogleModel , you need to either install pydantic-ai , or install pydantic-ai-slim with the google optional group: pipuv ## Configuration GoogleModel lets you use Google's Gemini models through their [Generative Language API](https://ai.google.dev/api/all-methods) ( generativelanguage.googleapis.com ) or [Vertex AI API](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models) ( *-aiplatform.googleapis.com ). ### API Key (Generative Language API) To use Gemini via the Generative Language API, go to [aistudio.google.com](https://aistudio.google.com/apikey) and create an API key. Once you have the API key, set it as an environment variable: You can then use GoogleModel by name (where GLA stands for Generative Language API): Or you can explicitly create the provider: ### Vertex AI (Enterprise/Cloud) If you are an enterprise user, you can also use GoogleModel to access Gemini via Vertex AI. This interface has a number of advantages over the Generative Language API: 1. The VertexAI API comes with more enterprise readiness guarantees. 2. You can [purchase provisioned throughput](https://cloud.google.com/vertex-ai/generative-ai/docs/provisioned-throughput#purchase-provisioned-throughput) with Vertex AI to guarantee capacity. 3. If you're running Pydantic AI inside GCP, you don't need to set up authentication, it should \"just work\". 4. You can decide which region to use, which might be important from a regulatory perspective, and might improve latency. You can authenticate using [application default credentials](https://cloud.google.com/docs/authentication/application-default-credentials), a service account, or an [API key](https://cloud.google.com/vertex-ai/generative-ai/docs/start/api-keys?usertype=expressmode). Whichever way you authenticate, you'll need to have Vertex AI enabled in your GCP account. #### Application Default Credentials If you have the [ gcloud CLI](https://cloud.google.com/sdk/gcloud) installed and configured, you can use GoogleProvider in Vertex AI mode by name: Or you can explicitly create the provider and model: #### Service Account To use a service account JSON file, explicitly create the provider and model: google\\_model\\_service\\_account.py #### API Key To use Vertex AI with an API key, [create a key](https://cloud.google.com/vertex-ai/generative-ai/docs/start/api-keys?usertype=expressmode) and set it as an environment variable: You can then use GoogleModel in Vertex AI mode by name: Or you can explicitly create the provider and model: #### Customizing Location or Project You can specify the location and/or project when using Vertex AI: google\\_model\\_location.py #### Model Garden You can access models from the [Model Garden](https://cloud.google.com/model-garden?hl=en) that support the generateContent API and are available under your GCP project, including but not limited to Gemini, using one of the following model_name patterns: * {model_id} for Gemini models * {publisher}/{model_id} * publishers/{publisher}/models/{model_id} * projects/{project}/locations/{location}/publishers/{publisher}/models/{model_id} ## Custom HTTP Client You can customize the GoogleProvider with a custom httpx.AsyncClient : ## Document, Image, Audio, and Video Input GoogleModel supports multi-modal input, including documents, images, audio, and video. See the [input documentation](../../input/index.html) for details and examples. ## Model settings You can customize model behavior using [ GoogleModelSettings ](../../api/models/google/index.html#pydantic_ai.models.google.GoogleModelSettings): ### Disable thinking You can disable thinking by setting the thinking_budget to 0 on the google_thinking_config : Check out the [Gemini API docs](https://ai.google.dev/gemini-api/docs/thinking) for more on thinking. ### Safety settings You can customize the safety settings by setting the google_safety_settings field. See the [Gemini API docs](https://ai.google.dev/gemini-api/docs/safety-settings) for more on safety settings.", "url": "https://ai.pydantic.dev/models/google/index.html#google", "page": "models/google/index.html", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "heading_level": 2, "md_text": "To use GoogleModel , you need to either install pydantic-ai , or install pydantic-ai-slim with the google optional group: pipuv", "url": "https://ai.pydantic.dev/models/google/index.html#install", "page": "models/google/index.html", "source_site": "pydantic_ai"}
{"title": "Configuration", "anchor": "configuration", "heading_level": 2, "md_text": "GoogleModel lets you use Google's Gemini models through their [Generative Language API](https://ai.google.dev/api/all-methods) ( generativelanguage.googleapis.com ) or [Vertex AI API](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models) ( *-aiplatform.googleapis.com ). ### API Key (Generative Language API) To use Gemini via the Generative Language API, go to [aistudio.google.com](https://aistudio.google.com/apikey) and create an API key. Once you have the API key, set it as an environment variable: You can then use GoogleModel by name (where GLA stands for Generative Language API): Or you can explicitly create the provider: ### Vertex AI (Enterprise/Cloud) If you are an enterprise user, you can also use GoogleModel to access Gemini via Vertex AI. This interface has a number of advantages over the Generative Language API: 1. The VertexAI API comes with more enterprise readiness guarantees. 2. You can [purchase provisioned throughput](https://cloud.google.com/vertex-ai/generative-ai/docs/provisioned-throughput#purchase-provisioned-throughput) with Vertex AI to guarantee capacity. 3. If you're running Pydantic AI inside GCP, you don't need to set up authentication, it should \"just work\". 4. You can decide which region to use, which might be important from a regulatory perspective, and might improve latency. You can authenticate using [application default credentials](https://cloud.google.com/docs/authentication/application-default-credentials), a service account, or an [API key](https://cloud.google.com/vertex-ai/generative-ai/docs/start/api-keys?usertype=expressmode). Whichever way you authenticate, you'll need to have Vertex AI enabled in your GCP account. #### Application Default Credentials If you have the [ gcloud CLI](https://cloud.google.com/sdk/gcloud) installed and configured, you can use GoogleProvider in Vertex AI mode by name: Or you can explicitly create the provider and model: #### Service Account To use a service account JSON file, explicitly create the provider and model: google\\_model\\_service\\_account.py #### API Key To use Vertex AI with an API key, [create a key](https://cloud.google.com/vertex-ai/generative-ai/docs/start/api-keys?usertype=expressmode) and set it as an environment variable: You can then use GoogleModel in Vertex AI mode by name: Or you can explicitly create the provider and model: #### Customizing Location or Project You can specify the location and/or project when using Vertex AI: google\\_model\\_location.py #### Model Garden You can access models from the [Model Garden](https://cloud.google.com/model-garden?hl=en) that support the generateContent API and are available under your GCP project, including but not limited to Gemini, using one of the following model_name patterns: * {model_id} for Gemini models * {publisher}/{model_id} * publishers/{publisher}/models/{model_id} * projects/{project}/locations/{location}/publishers/{publisher}/models/{model_id}", "url": "https://ai.pydantic.dev/models/google/index.html#configuration", "page": "models/google/index.html", "source_site": "pydantic_ai"}
{"title": "API Key (Generative Language API)", "anchor": "api-key-generative-language-api", "heading_level": 3, "md_text": "To use Gemini via the Generative Language API, go to [aistudio.google.com](https://aistudio.google.com/apikey) and create an API key. Once you have the API key, set it as an environment variable: You can then use GoogleModel by name (where GLA stands for Generative Language API): Or you can explicitly create the provider:", "url": "https://ai.pydantic.dev/models/google/index.html#api-key-generative-language-api", "page": "models/google/index.html", "source_site": "pydantic_ai"}
{"title": "Vertex AI (Enterprise/Cloud)", "anchor": "vertex-ai-enterprisecloud", "heading_level": 3, "md_text": "If you are an enterprise user, you can also use GoogleModel to access Gemini via Vertex AI. This interface has a number of advantages over the Generative Language API: 1. The VertexAI API comes with more enterprise readiness guarantees. 2. You can [purchase provisioned throughput](https://cloud.google.com/vertex-ai/generative-ai/docs/provisioned-throughput#purchase-provisioned-throughput) with Vertex AI to guarantee capacity. 3. If you're running Pydantic AI inside GCP, you don't need to set up authentication, it should \"just work\". 4. You can decide which region to use, which might be important from a regulatory perspective, and might improve latency. You can authenticate using [application default credentials](https://cloud.google.com/docs/authentication/application-default-credentials), a service account, or an [API key](https://cloud.google.com/vertex-ai/generative-ai/docs/start/api-keys?usertype=expressmode). Whichever way you authenticate, you'll need to have Vertex AI enabled in your GCP account. #### Application Default Credentials If you have the [ gcloud CLI](https://cloud.google.com/sdk/gcloud) installed and configured, you can use GoogleProvider in Vertex AI mode by name: Or you can explicitly create the provider and model: #### Service Account To use a service account JSON file, explicitly create the provider and model: google\\_model\\_service\\_account.py #### API Key To use Vertex AI with an API key, [create a key](https://cloud.google.com/vertex-ai/generative-ai/docs/start/api-keys?usertype=expressmode) and set it as an environment variable: You can then use GoogleModel in Vertex AI mode by name: Or you can explicitly create the provider and model: #### Customizing Location or Project You can specify the location and/or project when using Vertex AI: google\\_model\\_location.py #### Model Garden You can access models from the [Model Garden](https://cloud.google.com/model-garden?hl=en) that support the generateContent API and are available under your GCP project, including but not limited to Gemini, using one of the following model_name patterns: * {model_id} for Gemini models * {publisher}/{model_id} * publishers/{publisher}/models/{model_id} * projects/{project}/locations/{location}/publishers/{publisher}/models/{model_id}", "url": "https://ai.pydantic.dev/models/google/index.html#vertex-ai-enterprisecloud", "page": "models/google/index.html", "source_site": "pydantic_ai"}
{"title": "Custom HTTP Client", "anchor": "custom-http-client", "heading_level": 2, "md_text": "You can customize the GoogleProvider with a custom httpx.AsyncClient :", "url": "https://ai.pydantic.dev/models/google/index.html#custom-http-client", "page": "models/google/index.html", "source_site": "pydantic_ai"}
{"title": "Document, Image, Audio, and Video Input", "anchor": "document-image-audio-and-video-input", "heading_level": 2, "md_text": "GoogleModel supports multi-modal input, including documents, images, audio, and video. See the [input documentation](../../input/index.html) for details and examples.", "url": "https://ai.pydantic.dev/models/google/index.html#document-image-audio-and-video-input", "page": "models/google/index.html", "source_site": "pydantic_ai"}
{"title": "Model settings", "anchor": "model-settings", "heading_level": 2, "md_text": "You can customize model behavior using [ GoogleModelSettings ](../../api/models/google/index.html#pydantic_ai.models.google.GoogleModelSettings): ### Disable thinking You can disable thinking by setting the thinking_budget to 0 on the google_thinking_config : Check out the [Gemini API docs](https://ai.google.dev/gemini-api/docs/thinking) for more on thinking. ### Safety settings You can customize the safety settings by setting the google_safety_settings field. See the [Gemini API docs](https://ai.google.dev/gemini-api/docs/safety-settings) for more on safety settings.", "url": "https://ai.pydantic.dev/models/google/index.html#model-settings", "page": "models/google/index.html", "source_site": "pydantic_ai"}
{"title": "Disable thinking", "anchor": "disable-thinking", "heading_level": 3, "md_text": "You can disable thinking by setting the thinking_budget to 0 on the google_thinking_config : Check out the [Gemini API docs](https://ai.google.dev/gemini-api/docs/thinking) for more on thinking.", "url": "https://ai.pydantic.dev/models/google/index.html#disable-thinking", "page": "models/google/index.html", "source_site": "pydantic_ai"}
{"title": "Safety settings", "anchor": "safety-settings", "heading_level": 3, "md_text": "You can customize the safety settings by setting the google_safety_settings field. See the [Gemini API docs](https://ai.google.dev/gemini-api/docs/safety-settings) for more on safety settings.", "url": "https://ai.pydantic.dev/models/google/index.html#safety-settings", "page": "models/google/index.html", "source_site": "pydantic_ai"}
{"title": "Graphs", "anchor": "graphs", "heading_level": 1, "md_text": "Don't use a nail gun unless you need a nail gun If Pydantic AI [agents](../agents/index.html) are a hammer, and [multi-agent workflows](../multi-agent-applications/index.html) are a sledgehammer, then graphs are a nail gun: * sure, nail guns look cooler than hammers * but nail guns take a lot more setup than hammers * and nail guns don't make you a better builder, they make you a builder with a nail gun * Lastly, (and at the risk of torturing this metaphor), if you're a fan of medieval tools like mallets and untyped Python, you probably won't like nail guns or our approach to graphs. (But then again, if you're not a fan of type hints in Python, you've probably already bounced off Pydantic AI to use one of the toy agent frameworks  good luck, and feel free to borrow my sledgehammer when you realize you need it) In short, graphs are a powerful tool, but they're not the right tool for every job. Please consider other [multi-agent approaches](../multi-agent-applications/index.html) before proceeding. If you're not confident a graph-based approach is a good idea, it might be unnecessary. Graphs and finite state machines (FSMs) are a powerful abstraction to model, execute, control and visualize complex workflows. Alongside Pydantic AI, we've developed pydantic-graph  an async graph and state machine library for Python where nodes and edges are defined using type hints. While this library is developed as part of Pydantic AI; it has no dependency on pydantic-ai and can be considered as a pure graph-based state machine library. You may find it useful whether or not you're using Pydantic AI or even building with GenAI. pydantic-graph is designed for advanced users and makes heavy use of Python generics and type hints. It is not designed to be as beginner-friendly as Pydantic AI. ## Installation pydantic-graph is a required dependency of pydantic-ai , and an optional dependency of pydantic-ai-slim , see [installation instructions](../install/index.html#slim-install) for more information. You can also install it directly: pipuv ## Graph Types pydantic-graph is made up of a few key components: ### GraphRunContext [ GraphRunContext ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.GraphRunContext)  The context for the graph run, similar to Pydantic AI's [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext). This holds the state of the graph and dependencies and is passed to nodes when they're run. GraphRunContext is generic in the state type of the graph it's used in, [ StateT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.StateT). ### End [ End ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.End)  return value to indicate the graph run should end. End is generic in the graph return type of the graph it's used in, [ RunEndT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.RunEndT). ### Nodes Subclasses of [ BaseNode ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode) define nodes for execution in the graph. Nodes, which are generally [ dataclass es](https://docs.python.org/3/library/dataclasses.html#dataclasses.dataclass), generally consist of: * fields containing any parameters required/optional when calling the node * the business logic to execute the node, in the [ run ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode.run) method * return annotations of the [ run ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode.run) method, which are read by pydantic-graph to determine the outgoing edges of the node Nodes are generic in: * **state**, which must have the same type as the state of graphs they're included in, [ StateT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.StateT) has a default of None , so if you're not using state you can omit this generic parameter, see [stateful graphs](index.html#stateful-graphs) for more information * **deps**, which must have the same type as the deps of the graph they're included in, [ DepsT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.DepsT) has a default of None , so if you're not using deps you can omit this generic parameter, see [dependency injection](index.html#dependency-injection) for more information * **graph return type**  this only applies if the node returns [ End ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.End). [ RunEndT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.RunEndT) has a default of [Never](https://docs.python.org/3/library/typing.html#typing.Never) so this generic parameter can be omitted if the node doesn't return End , but must be included if it does. Here's an example of a start or intermediate node in a graph  it can't end the run as it doesn't return [ End ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.End): intermediate\\_node.py 1. State in this example is MyState (not shown), hence BaseNode is parameterized with MyState . This node can't end the run, so the RunEndT generic parameter is omitted and defaults to Never . 2. MyNode is a dataclass and has a single field foo , an int . 3. The run method takes a GraphRunContext parameter, again parameterized with state MyState . 4. The return type of the run method is AnotherNode (not shown), this is used to determine the outgoing edges of the node. We could extend MyNode to optionally end the run if foo is divisible by 5: intermediate\\_or\\_end\\_node.py 1. We parameterize the node with the return type ( int in this case) as well as state. Because generic parameters are positional-only, we have to include None as the second parameter representing deps. 2. The return type of the run method is now a union of AnotherNode and End[int] , this allows the node to end the run if foo is divisible by 5. ### Graph [ Graph ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph)  this is the execution graph itself, made up of a set of [node classes](index.html#nodes) (i.e., BaseNode subclasses). Graph is generic in: * **state** the state type of the graph, [ StateT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.StateT) * **deps** the deps type of the graph, [ DepsT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.DepsT) * **graph return type** the return type of the graph run, [ RunEndT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.RunEndT) Here's an example of a simple graph: graph\\_example.py 1. The DivisibleBy5 node is parameterized with None for the state param and None for the deps param as this graph doesn't use state or deps, and int as it can end the run. 2. The Increment node doesn't return End , so the RunEndT generic parameter is omitted, state can also be omitted as the graph doesn't use state. 3. The graph is created with a sequence of nodes. 4. The graph is run synchronously with [ run_sync ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.run_sync). The initial node is DivisibleBy5(4) . Because the graph doesn't use external state or deps, we don't pass state or deps . *(This example is complete, it can be run \"as is\")* A [mermaid diagram](index.html#mermaid-diagrams) for this graph can be generated with the following code: graph\\_example\\_diagram.py In order to visualize a graph within a jupyter-notebook , IPython.display needs to be used: jupyter\\_display\\_mermaid.py ## Stateful Graphs The \"state\" concept in pydantic-graph provides an optional way to access and mutate an object (often a dataclass or Pydantic model) as nodes run in a graph. If you think of Graphs as a production line, then your state is the engine being passed along the line and built up by each node as the graph is run. pydantic-graph provides state persistence, with the state recorded after each node is run. (See [State Persistence](index.html#state-persistence).) Here's an example of a graph which represents a vending machine where the user may insert coins and select a product to purchase. vending\\_machine.py 1. The state of the vending machine is defined as a dataclass with the user's balance and the product they've selected, if any. 2. A dictionary of products mapped to prices. 3. The InsertCoin node, [ BaseNode ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode) is parameterized with MachineState as that's the state used in this graph. 4. The InsertCoin node prompts the user to insert coins. We keep things simple by just entering a monetary amount as a float. Before you start thinking this is a toy too since it's using [rich's Prompt.ask ](https://rich.readthedocs.io/en/stable/reference/prompt.html#rich.prompt.PromptBase.ask) within nodes, see [below](index.html#example-human-in-the-loop) for how control flow can be managed when nodes require external input. 5. The CoinsInserted node; again this is a [ dataclass ](https://docs.python.org/3/library/dataclasses.html#dataclasses.dataclass) with one field amount . 6. Update the user's balance with the amount inserted. 7. If the user has already selected a product, go to Purchase , otherwise go to SelectProduct . 8. In the Purchase node, look up the price of the product if the user entered a valid product. 9. If the user did enter a valid product, set the product in the state so we don't revisit SelectProduct . 10. If the balance is enough to purchase the product, adjust the balance to reflect the purchase and return [ End ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.End) to end the graph. We're not using the run return type, so we call End with None . 11. If the balance is insufficient, go to InsertCoin to prompt the user to insert more coins. 12. If the product is invalid, go to SelectProduct to prompt the user to select a product again. 13. The graph is created by passing a list of nodes to [ Graph ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph). Order of nodes is not important, but it can affect how [diagrams](index.html#mermaid-diagrams) are displayed. 14. Initialize the state. This will be passed to the graph run and mutated as the graph runs. 15. Run the graph with the initial state. Since the graph can be run from any node, we must pass the start node  in this case, InsertCoin . [ Graph.run ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.run) returns a [ GraphRunResult ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.GraphRunResult) that provides the final data and a history of the run. 16. The return type of the node's [ run ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode.run) method is important as it is used to determine the outgoing edges of the node. This information in turn is used to render [mermaid diagrams](index.html#mermaid-diagrams) and is enforced at runtime to detect misbehavior as soon as possible. 17. The return type of CoinsInserted 's [ run ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode.run) method is a union, meaning multiple outgoing edges are possible. 18. Unlike other nodes, Purchase can end the run, so the [ RunEndT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.RunEndT) generic parameter must be set. In this case it's None since the graph run return type is None . *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* A [mermaid diagram](index.html#mermaid-diagrams) for this graph can be generated with the following code: vending\\_machine\\_diagram.py The diagram generated by the above code is: See [below](index.html#mermaid-diagrams) for more information on generating diagrams. ## GenAI Example So far we haven't shown an example of a Graph that actually uses Pydantic AI or GenAI at all. In this example, one agent generates a welcome email to a user and the other agent provides feedback on the email. This graph has a very simple structure: genai\\_email\\_feedback.py *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* ## Iterating Over a Graph ### Using Graph.iter for async for iteration Sometimes you want direct control or insight into each node as the graph executes. The easiest way to do that is with the [ Graph.iter ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.iter) method, which returns a **context manager** that yields a [ GraphRun ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.GraphRun) object. The GraphRun is an async-iterable over the nodes of your graph, allowing you to record or modify them as they execute. Here's an example: count\\_down.py 1. Graph.iter(...) returns a [ GraphRun ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.GraphRun). 2. Here, we step through each node as it is executed. 3. Once the graph returns an [ End ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.End), the loop ends, and run.result becomes a [ GraphRunResult ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.GraphRunResult) containing the final outcome ( 0 here). ### Using GraphRun.next(node) manually Alternatively, you can drive iteration manually with the [ GraphRun.next ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.GraphRun.next) method, which allows you to pass in whichever node you want to run next. You can modify or selectively skip nodes this way. Below is a contrived example that stops whenever the counter is at 2, ignoring any node runs beyond that: count\\_down\\_next.py 1. We start by grabbing the first node that will be run in the agent's graph. 2. The agent run is finished once an End node has been produced; instances of End cannot be passed to next . 3. If the user decides to stop early, we break out of the loop. The graph run won't have a real final result in that case ( run.result remains None ). 4. At each step, we call await run.next(node) to run it and get the next node (or an End ). 5. Because we did not continue the run until it finished, the result is not set. 6. The run's history is still populated with the steps we executed so far. 7. Use [ FullStatePersistence ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.in_mem.FullStatePersistence) so we can show the history of the run, see [State Persistence](index.html#state-persistence) below for more information. ## State Persistence One of the biggest benefits of finite state machine (FSM) graphs is how they simplify the handling of interrupted execution. This might happen for a variety of reasons: * the state machine logic might fundamentally need to be paused  e.g. the returns workflow for an e-commerce order needs to wait for the item to be posted to the returns center or because execution of the next node needs input from a user so needs to wait for a new http request, * the execution takes so long that the entire graph can't reliably be executed in a single continuous run  e.g. a deep research agent that might take hours to run, * you want to run multiple graph nodes in parallel in different processes / hardware instances (note: parallel node execution is not yet supported in pydantic-graph , see [#704](https://github.com/pydantic/pydantic-ai/issues/704)). Trying to make a conventional control flow (i.e., boolean logic and nested function calls) implementation compatible with these usage scenarios generally results in brittle and over-complicated spaghetti code, with the logic required to interrupt and resume execution dominating the implementation. To allow graph runs to be interrupted and resumed, pydantic-graph provides state persistence  a system for snapshotting the state of a graph run before and after each node is run, allowing a graph run to be resumed from any point in the graph. pydantic-graph includes three state persistence implementations: * [ SimpleStatePersistence ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.in_mem.SimpleStatePersistence)  Simple in memory state persistence that just hold the latest snapshot. If no state persistence implementation is provided when running a graph, this is used by default. * [ FullStatePersistence ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.in_mem.FullStatePersistence)  In memory state persistence that hold a list of snapshots. * [ FileStatePersistence ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.file.FileStatePersistence)  File-based state persistence that saves snapshots to a JSON file. In production applications, developers should implement their own state persistence by subclassing [ BaseStatePersistence ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.BaseStatePersistence) abstract base class, which might persist runs in a relational database like PostgresQL. At a high level the role of StatePersistence implementations is to store and retrieve [ NodeSnapshot ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.NodeSnapshot) and [ EndSnapshot ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.EndSnapshot) objects. [ graph.iter_from_persistence() ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.iter_from_persistence) may be used to run the graph based on the state stored in persistence. We can run the count_down_graph from [above](index.html#iterating-over-a-graph), using [ graph.iter_from_persistence() ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.iter_from_persistence) and [ FileStatePersistence ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.file.FileStatePersistence). As you can see in this code, run_node requires no external application state (apart from state persistence) to be run, meaning graphs can easily be executed by distributed execution and queueing systems. count\\_down\\_from\\_persistence.py 1. Create a [ FileStatePersistence ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.file.FileStatePersistence) to use to start the graph. 2. Call [ graph.initialize() ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.initialize) to set the initial graph state in the persistence object. 3. run_node is a pure function that doesn't need access to any other process state to run the next node of the graph, except the ID of the run. 4. Call [ graph.iter_from_persistence() ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.iter_from_persistence) create a [ GraphRun ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.GraphRun) object that will run the next node of the graph from the state stored in persistence. This will return either a node or an End object. 5. [ graph.run() ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.run) will return either a [node](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode) or an [ End ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.End) object. 6. Check if the node is an [ End ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.End) object, if it is, the graph run is complete. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* ### Example: Human in the loop. As noted above, state persistence allows graphs to be interrupted and resumed. One use case of this is to allow user input to continue. In this example, an AI asks the user a question, the user provides an answer, the AI evaluates the answer and ends if the user got it right or asks another question if they got it wrong. Instead of running the entire graph in a single process invocation, we run the graph by running the process repeatedly, optionally providing an answer to the question as a command line argument. ai_q_and_a_graph.py  question_graph definition ai\\_q\\_and\\_a\\_graph.py *(This example is complete, it can be run \"as is\")* ai\\_q\\_and\\_a\\_run.py 1. Get the user's answer from the command line, if provided. See [question graph example](../examples/question-graph/index.html) for a complete example. 2. Create a state persistence instance the 'question_graph.json' file may or may not already exist. 3. Since we're using the [persistence interface](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.BaseStatePersistence) outside a graph, we need to call [ set_graph_types ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.BaseStatePersistence.set_graph_types) to set the graph generic types StateT and RunEndT for the persistence instance. This is necessary to allow the persistence instance to know how to serialize and deserialize graph nodes. 4. If we're run the graph before, [ load_next ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.BaseStatePersistence.load_next) will return a snapshot of the next node to run, here we use state from that snapshot, and create a new Evaluate node with the answer provided on the command line. 5. If the graph hasn't been run before, we create a new QuestionState and start with the Ask node. 6. Call [ GraphRun.next() ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.GraphRun.next) to run the node. This will return either a node or an End object. 7. If the node is an End object, the graph run is complete. The data field of the End object contains the comment returned by the evaluate_agent about the correct answer. 8. To demonstrate the state persistence, we call [ load_all ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.BaseStatePersistence.load_all) to get all the snapshots from the persistence instance. This will return a list of [ Snapshot ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.Snapshot) objects. 9. If the node is an Answer object, we print the question and break out of the loop to end the process and wait for user input. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* For a complete example of this graph, see the [question graph example](../examples/question-graph/index.html). ## Dependency Injection As with Pydantic AI, pydantic-graph supports dependency injection via a generic parameter on [ Graph ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph) and [ BaseNode ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode), and the [ GraphRunContext.deps ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.GraphRunContext.deps) field. As an example of dependency injection, let's modify the DivisibleBy5 example [above](index.html#graph) to use a [ ProcessPoolExecutor ](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor) to run the compute load in a separate process (this is a contrived example, ProcessPoolExecutor wouldn't actually improve performance in this example): deps\\_example.py *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* ## Mermaid Diagrams Pydantic Graph can generate [mermaid](https://mermaid.js.org/) [ stateDiagram-v2 ](https://mermaid.js.org/syntax/stateDiagram.html) diagrams for graphs, as shown above. These diagrams can be generated with: * [ Graph.mermaid_code ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.mermaid_code) to generate the mermaid code for a graph * [ Graph.mermaid_image ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.mermaid_image) to generate an image of the graph using [mermaid.ink](https://mermaid.ink/) * [ Graph.mermaid_save ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.mermaid_save) to generate an image of the graph using [mermaid.ink](https://mermaid.ink/) and save it to a file Beyond the diagrams shown above, you can also customize mermaid diagrams with the following options: * [ Edge ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.Edge) allows you to apply a label to an edge * [ BaseNode.docstring_notes ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode.docstring_notes) and [ BaseNode.get_note ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode.get_note) allows you to add notes to nodes * The [ highlighted_nodes ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.mermaid_code) parameter allows you to highlight specific node(s) in the diagram Putting that together, we can edit the last [ ai_q_and_a_graph.py ](index.html#example-human-in-the-loop) example to: * add labels to some edges * add a note to the Ask node * highlight the Answer node * save the diagram as a PNG image to file ai\\_q\\_and\\_a\\_graph\\_extra.py *(This example is not complete and cannot be run directly)* This would generate an image that looks like this: ### Setting Direction of the State Diagram You can specify the direction of the state diagram using one of the following values: * 'TB' : Top to bottom, the diagram flows vertically from top to bottom. * 'LR' : Left to right, the diagram flows horizontally from left to right. * 'RL' : Right to left, the diagram flows horizontally from right to left. * 'BT' : Bottom to top, the diagram flows vertically from bottom to top. Here is an example of how to do this using 'Left to Right' (LR) instead of the default 'Top to Bottom' (TB): vending\\_machine\\_diagram.py", "url": "https://ai.pydantic.dev/graph/index.html#graphs", "page": "graph/index.html", "source_site": "pydantic_ai"}
{"title": "Installation", "anchor": "installation", "heading_level": 2, "md_text": "pydantic-graph is a required dependency of pydantic-ai , and an optional dependency of pydantic-ai-slim , see [installation instructions](../install/index.html#slim-install) for more information. You can also install it directly: pipuv", "url": "https://ai.pydantic.dev/graph/index.html#installation", "page": "graph/index.html", "source_site": "pydantic_ai"}
{"title": "Graph Types", "anchor": "graph-types", "heading_level": 2, "md_text": "pydantic-graph is made up of a few key components: ### GraphRunContext [ GraphRunContext ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.GraphRunContext)  The context for the graph run, similar to Pydantic AI's [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext). This holds the state of the graph and dependencies and is passed to nodes when they're run. GraphRunContext is generic in the state type of the graph it's used in, [ StateT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.StateT). ### End [ End ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.End)  return value to indicate the graph run should end. End is generic in the graph return type of the graph it's used in, [ RunEndT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.RunEndT). ### Nodes Subclasses of [ BaseNode ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode) define nodes for execution in the graph. Nodes, which are generally [ dataclass es](https://docs.python.org/3/library/dataclasses.html#dataclasses.dataclass), generally consist of: * fields containing any parameters required/optional when calling the node * the business logic to execute the node, in the [ run ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode.run) method * return annotations of the [ run ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode.run) method, which are read by pydantic-graph to determine the outgoing edges of the node Nodes are generic in: * **state**, which must have the same type as the state of graphs they're included in, [ StateT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.StateT) has a default of None , so if you're not using state you can omit this generic parameter, see [stateful graphs](index.html#stateful-graphs) for more information * **deps**, which must have the same type as the deps of the graph they're included in, [ DepsT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.DepsT) has a default of None , so if you're not using deps you can omit this generic parameter, see [dependency injection](index.html#dependency-injection) for more information * **graph return type**  this only applies if the node returns [ End ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.End). [ RunEndT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.RunEndT) has a default of [Never](https://docs.python.org/3/library/typing.html#typing.Never) so this generic parameter can be omitted if the node doesn't return End , but must be included if it does. Here's an example of a start or intermediate node in a graph  it can't end the run as it doesn't return [ End ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.End): intermediate\\_node.py 1. State in this example is MyState (not shown), hence BaseNode is parameterized with MyState . This node can't end the run, so the RunEndT generic parameter is omitted and defaults to Never . 2. MyNode is a dataclass and has a single field foo , an int . 3. The run method takes a GraphRunContext parameter, again parameterized with state MyState . 4. The return type of the run method is AnotherNode (not shown), this is used to determine the outgoing edges of the node. We could extend MyNode to optionally end the run if foo is divisible by 5: intermediate\\_or\\_end\\_node.py 1. We parameterize the node with the return type ( int in this case) as well as state. Because generic parameters are positional-only, we have to include None as the second parameter representing deps. 2. The return type of the run method is now a union of AnotherNode and End[int] , this allows the node to end the run if foo is divisible by 5. ### Graph [ Graph ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph)  this is the execution graph itself, made up of a set of [node classes](index.html#nodes) (i.e., BaseNode subclasses). Graph is generic in: * **state** the state type of the graph, [ StateT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.StateT) * **deps** the deps type of the graph, [ DepsT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.DepsT) * **graph return type** the return type of the graph run, [ RunEndT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.RunEndT) Here's an example of a simple graph: graph\\_example.py 1. The DivisibleBy5 node is parameterized with None for the state param and None for the deps param as this graph doesn't use state or deps, and int as it can end the run. 2. The Increment node doesn't return End , so the RunEndT generic parameter is omitted, state can also be omitted as the graph doesn't use state. 3. The graph is created with a sequence of nodes. 4. The graph is run synchronously with [ run_sync ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.run_sync). The initial node is DivisibleBy5(4) . Because the graph doesn't use external state or deps, we don't pass state or deps . *(This example is complete, it can be run \"as is\")* A [mermaid diagram](index.html#mermaid-diagrams) for this graph can be generated with the following code: graph\\_example\\_diagram.py In order to visualize a graph within a jupyter-notebook , IPython.display needs to be used: jupyter\\_display\\_mermaid.py", "url": "https://ai.pydantic.dev/graph/index.html#graph-types", "page": "graph/index.html", "source_site": "pydantic_ai"}
{"title": "GraphRunContext", "anchor": "graphruncontext", "heading_level": 3, "md_text": "[ GraphRunContext ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.GraphRunContext)  The context for the graph run, similar to Pydantic AI's [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext). This holds the state of the graph and dependencies and is passed to nodes when they're run. GraphRunContext is generic in the state type of the graph it's used in, [ StateT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.StateT).", "url": "https://ai.pydantic.dev/graph/index.html#graphruncontext", "page": "graph/index.html", "source_site": "pydantic_ai"}
{"title": "End", "anchor": "end", "heading_level": 3, "md_text": "[ End ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.End)  return value to indicate the graph run should end. End is generic in the graph return type of the graph it's used in, [ RunEndT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.RunEndT).", "url": "https://ai.pydantic.dev/graph/index.html#end", "page": "graph/index.html", "source_site": "pydantic_ai"}
{"title": "Nodes", "anchor": "nodes", "heading_level": 3, "md_text": "Subclasses of [ BaseNode ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode) define nodes for execution in the graph. Nodes, which are generally [ dataclass es](https://docs.python.org/3/library/dataclasses.html#dataclasses.dataclass), generally consist of: * fields containing any parameters required/optional when calling the node * the business logic to execute the node, in the [ run ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode.run) method * return annotations of the [ run ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode.run) method, which are read by pydantic-graph to determine the outgoing edges of the node Nodes are generic in: * **state**, which must have the same type as the state of graphs they're included in, [ StateT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.StateT) has a default of None , so if you're not using state you can omit this generic parameter, see [stateful graphs](index.html#stateful-graphs) for more information * **deps**, which must have the same type as the deps of the graph they're included in, [ DepsT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.DepsT) has a default of None , so if you're not using deps you can omit this generic parameter, see [dependency injection](index.html#dependency-injection) for more information * **graph return type**  this only applies if the node returns [ End ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.End). [ RunEndT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.RunEndT) has a default of [Never](https://docs.python.org/3/library/typing.html#typing.Never) so this generic parameter can be omitted if the node doesn't return End , but must be included if it does. Here's an example of a start or intermediate node in a graph  it can't end the run as it doesn't return [ End ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.End): intermediate\\_node.py 1. State in this example is MyState (not shown), hence BaseNode is parameterized with MyState . This node can't end the run, so the RunEndT generic parameter is omitted and defaults to Never . 2. MyNode is a dataclass and has a single field foo , an int . 3. The run method takes a GraphRunContext parameter, again parameterized with state MyState . 4. The return type of the run method is AnotherNode (not shown), this is used to determine the outgoing edges of the node. We could extend MyNode to optionally end the run if foo is divisible by 5: intermediate\\_or\\_end\\_node.py 1. We parameterize the node with the return type ( int in this case) as well as state. Because generic parameters are positional-only, we have to include None as the second parameter representing deps. 2. The return type of the run method is now a union of AnotherNode and End[int] , this allows the node to end the run if foo is divisible by 5.", "url": "https://ai.pydantic.dev/graph/index.html#nodes", "page": "graph/index.html", "source_site": "pydantic_ai"}
{"title": "Graph", "anchor": "graph", "heading_level": 3, "md_text": "[ Graph ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph)  this is the execution graph itself, made up of a set of [node classes](index.html#nodes) (i.e., BaseNode subclasses). Graph is generic in: * **state** the state type of the graph, [ StateT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.StateT) * **deps** the deps type of the graph, [ DepsT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.DepsT) * **graph return type** the return type of the graph run, [ RunEndT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.RunEndT) Here's an example of a simple graph: graph\\_example.py 1. The DivisibleBy5 node is parameterized with None for the state param and None for the deps param as this graph doesn't use state or deps, and int as it can end the run. 2. The Increment node doesn't return End , so the RunEndT generic parameter is omitted, state can also be omitted as the graph doesn't use state. 3. The graph is created with a sequence of nodes. 4. The graph is run synchronously with [ run_sync ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.run_sync). The initial node is DivisibleBy5(4) . Because the graph doesn't use external state or deps, we don't pass state or deps . *(This example is complete, it can be run \"as is\")* A [mermaid diagram](index.html#mermaid-diagrams) for this graph can be generated with the following code: graph\\_example\\_diagram.py In order to visualize a graph within a jupyter-notebook , IPython.display needs to be used: jupyter\\_display\\_mermaid.py", "url": "https://ai.pydantic.dev/graph/index.html#graph", "page": "graph/index.html", "source_site": "pydantic_ai"}
{"title": "Stateful Graphs", "anchor": "stateful-graphs", "heading_level": 2, "md_text": "The \"state\" concept in pydantic-graph provides an optional way to access and mutate an object (often a dataclass or Pydantic model) as nodes run in a graph. If you think of Graphs as a production line, then your state is the engine being passed along the line and built up by each node as the graph is run. pydantic-graph provides state persistence, with the state recorded after each node is run. (See [State Persistence](index.html#state-persistence).) Here's an example of a graph which represents a vending machine where the user may insert coins and select a product to purchase. vending\\_machine.py 1. The state of the vending machine is defined as a dataclass with the user's balance and the product they've selected, if any. 2. A dictionary of products mapped to prices. 3. The InsertCoin node, [ BaseNode ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode) is parameterized with MachineState as that's the state used in this graph. 4. The InsertCoin node prompts the user to insert coins. We keep things simple by just entering a monetary amount as a float. Before you start thinking this is a toy too since it's using [rich's Prompt.ask ](https://rich.readthedocs.io/en/stable/reference/prompt.html#rich.prompt.PromptBase.ask) within nodes, see [below](index.html#example-human-in-the-loop) for how control flow can be managed when nodes require external input. 5. The CoinsInserted node; again this is a [ dataclass ](https://docs.python.org/3/library/dataclasses.html#dataclasses.dataclass) with one field amount . 6. Update the user's balance with the amount inserted. 7. If the user has already selected a product, go to Purchase , otherwise go to SelectProduct . 8. In the Purchase node, look up the price of the product if the user entered a valid product. 9. If the user did enter a valid product, set the product in the state so we don't revisit SelectProduct . 10. If the balance is enough to purchase the product, adjust the balance to reflect the purchase and return [ End ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.End) to end the graph. We're not using the run return type, so we call End with None . 11. If the balance is insufficient, go to InsertCoin to prompt the user to insert more coins. 12. If the product is invalid, go to SelectProduct to prompt the user to select a product again. 13. The graph is created by passing a list of nodes to [ Graph ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph). Order of nodes is not important, but it can affect how [diagrams](index.html#mermaid-diagrams) are displayed. 14. Initialize the state. This will be passed to the graph run and mutated as the graph runs. 15. Run the graph with the initial state. Since the graph can be run from any node, we must pass the start node  in this case, InsertCoin . [ Graph.run ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.run) returns a [ GraphRunResult ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.GraphRunResult) that provides the final data and a history of the run. 16. The return type of the node's [ run ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode.run) method is important as it is used to determine the outgoing edges of the node. This information in turn is used to render [mermaid diagrams](index.html#mermaid-diagrams) and is enforced at runtime to detect misbehavior as soon as possible. 17. The return type of CoinsInserted 's [ run ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode.run) method is a union, meaning multiple outgoing edges are possible. 18. Unlike other nodes, Purchase can end the run, so the [ RunEndT ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.RunEndT) generic parameter must be set. In this case it's None since the graph run return type is None . *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* A [mermaid diagram](index.html#mermaid-diagrams) for this graph can be generated with the following code: vending\\_machine\\_diagram.py The diagram generated by the above code is: See [below](index.html#mermaid-diagrams) for more information on generating diagrams.", "url": "https://ai.pydantic.dev/graph/index.html#stateful-graphs", "page": "graph/index.html", "source_site": "pydantic_ai"}
{"title": "GenAI Example", "anchor": "genai-example", "heading_level": 2, "md_text": "So far we haven't shown an example of a Graph that actually uses Pydantic AI or GenAI at all. In this example, one agent generates a welcome email to a user and the other agent provides feedback on the email. This graph has a very simple structure: genai\\_email\\_feedback.py *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/index.html#genai-example", "page": "graph/index.html", "source_site": "pydantic_ai"}
{"title": "Iterating Over a Graph", "anchor": "iterating-over-a-graph", "heading_level": 2, "md_text": "### Using Graph.iter for async for iteration Sometimes you want direct control or insight into each node as the graph executes. The easiest way to do that is with the [ Graph.iter ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.iter) method, which returns a **context manager** that yields a [ GraphRun ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.GraphRun) object. The GraphRun is an async-iterable over the nodes of your graph, allowing you to record or modify them as they execute. Here's an example: count\\_down.py 1. Graph.iter(...) returns a [ GraphRun ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.GraphRun). 2. Here, we step through each node as it is executed. 3. Once the graph returns an [ End ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.End), the loop ends, and run.result becomes a [ GraphRunResult ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.GraphRunResult) containing the final outcome ( 0 here). ### Using GraphRun.next(node) manually Alternatively, you can drive iteration manually with the [ GraphRun.next ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.GraphRun.next) method, which allows you to pass in whichever node you want to run next. You can modify or selectively skip nodes this way. Below is a contrived example that stops whenever the counter is at 2, ignoring any node runs beyond that: count\\_down\\_next.py 1. We start by grabbing the first node that will be run in the agent's graph. 2. The agent run is finished once an End node has been produced; instances of End cannot be passed to next . 3. If the user decides to stop early, we break out of the loop. The graph run won't have a real final result in that case ( run.result remains None ). 4. At each step, we call await run.next(node) to run it and get the next node (or an End ). 5. Because we did not continue the run until it finished, the result is not set. 6. The run's history is still populated with the steps we executed so far. 7. Use [ FullStatePersistence ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.in_mem.FullStatePersistence) so we can show the history of the run, see [State Persistence](index.html#state-persistence) below for more information.", "url": "https://ai.pydantic.dev/graph/index.html#iterating-over-a-graph", "page": "graph/index.html", "source_site": "pydantic_ai"}
{"title": "Using Graph.iter for async for iteration", "anchor": "using-graphiter-for-async-for-iteration", "heading_level": 3, "md_text": "Sometimes you want direct control or insight into each node as the graph executes. The easiest way to do that is with the [ Graph.iter ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.iter) method, which returns a **context manager** that yields a [ GraphRun ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.GraphRun) object. The GraphRun is an async-iterable over the nodes of your graph, allowing you to record or modify them as they execute. Here's an example: count\\_down.py 1. Graph.iter(...) returns a [ GraphRun ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.GraphRun). 2. Here, we step through each node as it is executed. 3. Once the graph returns an [ End ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.End), the loop ends, and run.result becomes a [ GraphRunResult ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.GraphRunResult) containing the final outcome ( 0 here).", "url": "https://ai.pydantic.dev/graph/index.html#using-graphiter-for-async-for-iteration", "page": "graph/index.html", "source_site": "pydantic_ai"}
{"title": "Using GraphRun.next(node) manually", "anchor": "using-graphrunnextnode-manually", "heading_level": 3, "md_text": "Alternatively, you can drive iteration manually with the [ GraphRun.next ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.GraphRun.next) method, which allows you to pass in whichever node you want to run next. You can modify or selectively skip nodes this way. Below is a contrived example that stops whenever the counter is at 2, ignoring any node runs beyond that: count\\_down\\_next.py 1. We start by grabbing the first node that will be run in the agent's graph. 2. The agent run is finished once an End node has been produced; instances of End cannot be passed to next . 3. If the user decides to stop early, we break out of the loop. The graph run won't have a real final result in that case ( run.result remains None ). 4. At each step, we call await run.next(node) to run it and get the next node (or an End ). 5. Because we did not continue the run until it finished, the result is not set. 6. The run's history is still populated with the steps we executed so far. 7. Use [ FullStatePersistence ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.in_mem.FullStatePersistence) so we can show the history of the run, see [State Persistence](index.html#state-persistence) below for more information.", "url": "https://ai.pydantic.dev/graph/index.html#using-graphrunnextnode-manually", "page": "graph/index.html", "source_site": "pydantic_ai"}
{"title": "State Persistence", "anchor": "state-persistence", "heading_level": 2, "md_text": "One of the biggest benefits of finite state machine (FSM) graphs is how they simplify the handling of interrupted execution. This might happen for a variety of reasons: * the state machine logic might fundamentally need to be paused  e.g. the returns workflow for an e-commerce order needs to wait for the item to be posted to the returns center or because execution of the next node needs input from a user so needs to wait for a new http request, * the execution takes so long that the entire graph can't reliably be executed in a single continuous run  e.g. a deep research agent that might take hours to run, * you want to run multiple graph nodes in parallel in different processes / hardware instances (note: parallel node execution is not yet supported in pydantic-graph , see [#704](https://github.com/pydantic/pydantic-ai/issues/704)). Trying to make a conventional control flow (i.e., boolean logic and nested function calls) implementation compatible with these usage scenarios generally results in brittle and over-complicated spaghetti code, with the logic required to interrupt and resume execution dominating the implementation. To allow graph runs to be interrupted and resumed, pydantic-graph provides state persistence  a system for snapshotting the state of a graph run before and after each node is run, allowing a graph run to be resumed from any point in the graph. pydantic-graph includes three state persistence implementations: * [ SimpleStatePersistence ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.in_mem.SimpleStatePersistence)  Simple in memory state persistence that just hold the latest snapshot. If no state persistence implementation is provided when running a graph, this is used by default. * [ FullStatePersistence ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.in_mem.FullStatePersistence)  In memory state persistence that hold a list of snapshots. * [ FileStatePersistence ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.file.FileStatePersistence)  File-based state persistence that saves snapshots to a JSON file. In production applications, developers should implement their own state persistence by subclassing [ BaseStatePersistence ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.BaseStatePersistence) abstract base class, which might persist runs in a relational database like PostgresQL. At a high level the role of StatePersistence implementations is to store and retrieve [ NodeSnapshot ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.NodeSnapshot) and [ EndSnapshot ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.EndSnapshot) objects. [ graph.iter_from_persistence() ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.iter_from_persistence) may be used to run the graph based on the state stored in persistence. We can run the count_down_graph from [above](index.html#iterating-over-a-graph), using [ graph.iter_from_persistence() ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.iter_from_persistence) and [ FileStatePersistence ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.file.FileStatePersistence). As you can see in this code, run_node requires no external application state (apart from state persistence) to be run, meaning graphs can easily be executed by distributed execution and queueing systems. count\\_down\\_from\\_persistence.py 1. Create a [ FileStatePersistence ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.file.FileStatePersistence) to use to start the graph. 2. Call [ graph.initialize() ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.initialize) to set the initial graph state in the persistence object. 3. run_node is a pure function that doesn't need access to any other process state to run the next node of the graph, except the ID of the run. 4. Call [ graph.iter_from_persistence() ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.iter_from_persistence) create a [ GraphRun ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.GraphRun) object that will run the next node of the graph from the state stored in persistence. This will return either a node or an End object. 5. [ graph.run() ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.run) will return either a [node](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode) or an [ End ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.End) object. 6. Check if the node is an [ End ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.End) object, if it is, the graph run is complete. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* ### Example: Human in the loop. As noted above, state persistence allows graphs to be interrupted and resumed. One use case of this is to allow user input to continue. In this example, an AI asks the user a question, the user provides an answer, the AI evaluates the answer and ends if the user got it right or asks another question if they got it wrong. Instead of running the entire graph in a single process invocation, we run the graph by running the process repeatedly, optionally providing an answer to the question as a command line argument. ai_q_and_a_graph.py  question_graph definition ai\\_q\\_and\\_a\\_graph.py *(This example is complete, it can be run \"as is\")* ai\\_q\\_and\\_a\\_run.py 1. Get the user's answer from the command line, if provided. See [question graph example](../examples/question-graph/index.html) for a complete example. 2. Create a state persistence instance the 'question_graph.json' file may or may not already exist. 3. Since we're using the [persistence interface](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.BaseStatePersistence) outside a graph, we need to call [ set_graph_types ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.BaseStatePersistence.set_graph_types) to set the graph generic types StateT and RunEndT for the persistence instance. This is necessary to allow the persistence instance to know how to serialize and deserialize graph nodes. 4. If we're run the graph before, [ load_next ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.BaseStatePersistence.load_next) will return a snapshot of the next node to run, here we use state from that snapshot, and create a new Evaluate node with the answer provided on the command line. 5. If the graph hasn't been run before, we create a new QuestionState and start with the Ask node. 6. Call [ GraphRun.next() ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.GraphRun.next) to run the node. This will return either a node or an End object. 7. If the node is an End object, the graph run is complete. The data field of the End object contains the comment returned by the evaluate_agent about the correct answer. 8. To demonstrate the state persistence, we call [ load_all ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.BaseStatePersistence.load_all) to get all the snapshots from the persistence instance. This will return a list of [ Snapshot ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.Snapshot) objects. 9. If the node is an Answer object, we print the question and break out of the loop to end the process and wait for user input. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* For a complete example of this graph, see the [question graph example](../examples/question-graph/index.html).", "url": "https://ai.pydantic.dev/graph/index.html#state-persistence", "page": "graph/index.html", "source_site": "pydantic_ai"}
{"title": "Example: Human in the loop.", "anchor": "example-human-in-the-loop", "heading_level": 3, "md_text": "As noted above, state persistence allows graphs to be interrupted and resumed. One use case of this is to allow user input to continue. In this example, an AI asks the user a question, the user provides an answer, the AI evaluates the answer and ends if the user got it right or asks another question if they got it wrong. Instead of running the entire graph in a single process invocation, we run the graph by running the process repeatedly, optionally providing an answer to the question as a command line argument. ai_q_and_a_graph.py  question_graph definition ai\\_q\\_and\\_a\\_graph.py *(This example is complete, it can be run \"as is\")* ai\\_q\\_and\\_a\\_run.py 1. Get the user's answer from the command line, if provided. See [question graph example](../examples/question-graph/index.html) for a complete example. 2. Create a state persistence instance the 'question_graph.json' file may or may not already exist. 3. Since we're using the [persistence interface](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.BaseStatePersistence) outside a graph, we need to call [ set_graph_types ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.BaseStatePersistence.set_graph_types) to set the graph generic types StateT and RunEndT for the persistence instance. This is necessary to allow the persistence instance to know how to serialize and deserialize graph nodes. 4. If we're run the graph before, [ load_next ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.BaseStatePersistence.load_next) will return a snapshot of the next node to run, here we use state from that snapshot, and create a new Evaluate node with the answer provided on the command line. 5. If the graph hasn't been run before, we create a new QuestionState and start with the Ask node. 6. Call [ GraphRun.next() ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.GraphRun.next) to run the node. This will return either a node or an End object. 7. If the node is an End object, the graph run is complete. The data field of the End object contains the comment returned by the evaluate_agent about the correct answer. 8. To demonstrate the state persistence, we call [ load_all ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.BaseStatePersistence.load_all) to get all the snapshots from the persistence instance. This will return a list of [ Snapshot ](../api/pydantic_graph/persistence/index.html#pydantic_graph.persistence.Snapshot) objects. 9. If the node is an Answer object, we print the question and break out of the loop to end the process and wait for user input. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* For a complete example of this graph, see the [question graph example](../examples/question-graph/index.html).", "url": "https://ai.pydantic.dev/graph/index.html#example-human-in-the-loop", "page": "graph/index.html", "source_site": "pydantic_ai"}
{"title": "Dependency Injection", "anchor": "dependency-injection", "heading_level": 2, "md_text": "As with Pydantic AI, pydantic-graph supports dependency injection via a generic parameter on [ Graph ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph) and [ BaseNode ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode), and the [ GraphRunContext.deps ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.GraphRunContext.deps) field. As an example of dependency injection, let's modify the DivisibleBy5 example [above](index.html#graph) to use a [ ProcessPoolExecutor ](https://docs.python.org/3/library/concurrent.futures.html#concurrent.futures.ProcessPoolExecutor) to run the compute load in a separate process (this is a contrived example, ProcessPoolExecutor wouldn't actually improve performance in this example): deps\\_example.py *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/graph/index.html#dependency-injection", "page": "graph/index.html", "source_site": "pydantic_ai"}
{"title": "Mermaid Diagrams", "anchor": "mermaid-diagrams", "heading_level": 2, "md_text": "Pydantic Graph can generate [mermaid](https://mermaid.js.org/) [ stateDiagram-v2 ](https://mermaid.js.org/syntax/stateDiagram.html) diagrams for graphs, as shown above. These diagrams can be generated with: * [ Graph.mermaid_code ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.mermaid_code) to generate the mermaid code for a graph * [ Graph.mermaid_image ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.mermaid_image) to generate an image of the graph using [mermaid.ink](https://mermaid.ink/) * [ Graph.mermaid_save ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.mermaid_save) to generate an image of the graph using [mermaid.ink](https://mermaid.ink/) and save it to a file Beyond the diagrams shown above, you can also customize mermaid diagrams with the following options: * [ Edge ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.Edge) allows you to apply a label to an edge * [ BaseNode.docstring_notes ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode.docstring_notes) and [ BaseNode.get_note ](../api/pydantic_graph/nodes/index.html#pydantic_graph.nodes.BaseNode.get_note) allows you to add notes to nodes * The [ highlighted_nodes ](../api/pydantic_graph/graph/index.html#pydantic_graph.graph.Graph.mermaid_code) parameter allows you to highlight specific node(s) in the diagram Putting that together, we can edit the last [ ai_q_and_a_graph.py ](index.html#example-human-in-the-loop) example to: * add labels to some edges * add a note to the Ask node * highlight the Answer node * save the diagram as a PNG image to file ai\\_q\\_and\\_a\\_graph\\_extra.py *(This example is not complete and cannot be run directly)* This would generate an image that looks like this: ### Setting Direction of the State Diagram You can specify the direction of the state diagram using one of the following values: * 'TB' : Top to bottom, the diagram flows vertically from top to bottom. * 'LR' : Left to right, the diagram flows horizontally from left to right. * 'RL' : Right to left, the diagram flows horizontally from right to left. * 'BT' : Bottom to top, the diagram flows vertically from bottom to top. Here is an example of how to do this using 'Left to Right' (LR) instead of the default 'Top to Bottom' (TB): vending\\_machine\\_diagram.py", "url": "https://ai.pydantic.dev/graph/index.html#mermaid-diagrams", "page": "graph/index.html", "source_site": "pydantic_ai"}
{"title": "Setting Direction of the State Diagram", "anchor": "setting-direction-of-the-state-diagram", "heading_level": 3, "md_text": "You can specify the direction of the state diagram using one of the following values: * 'TB' : Top to bottom, the diagram flows vertically from top to bottom. * 'LR' : Left to right, the diagram flows horizontally from left to right. * 'RL' : Right to left, the diagram flows horizontally from right to left. * 'BT' : Bottom to top, the diagram flows vertically from bottom to top. Here is an example of how to do this using 'Left to Right' (LR) instead of the default 'Top to Bottom' (TB): vending\\_machine\\_diagram.py", "url": "https://ai.pydantic.dev/graph/index.html#setting-direction-of-the-state-diagram", "page": "graph/index.html", "source_site": "pydantic_ai"}
{"title": "Model Providers", "anchor": "model-providers", "heading_level": 1, "md_text": "Pydantic AI is model-agnostic and has built-in support for multiple model providers: * [OpenAI](../openai/index.html) * [Anthropic](../anthropic/index.html) * [Gemini](../google/index.html) (via two different APIs: Generative Language API and VertexAI API) * [Groq](../groq/index.html) * [Mistral](../mistral/index.html) * [Cohere](../cohere/index.html) * [Bedrock](../bedrock/index.html) * [Hugging Face](../huggingface/index.html) ## OpenAI-compatible Providers In addition, many providers are compatible with the OpenAI API, and can be used with OpenAIChatModel in Pydantic AI: * [DeepSeek](../openai/index.html#deepseek) * [Grok (xAI)](../openai/index.html#grok-xai) * [Ollama](../openai/index.html#ollama) * [OpenRouter](../openai/index.html#openrouter) * [Vercel AI Gateway](../openai/index.html#vercel-ai-gateway) * [Perplexity](../openai/index.html#perplexity) * [Fireworks AI](../openai/index.html#fireworks-ai) * [Together AI](../openai/index.html#together-ai) * [Azure AI Foundry](../openai/index.html#azure-ai-foundry) * [Heroku](../openai/index.html#heroku-ai) * [GitHub Models](../openai/index.html#github-models) * [Cerebras](../openai/index.html#cerebras) * [LiteLLM](../openai/index.html#litellm) * [Nebius AI Studio](../openai/index.html#nebius-ai-studio) * [OVHcloud AI Endpoints](../openai/index.html#ovhcloud-ai-endpoints) Pydantic AI also comes with [ TestModel ](../../api/models/test/index.html) and [ FunctionModel ](../../api/models/function/index.html) for testing and development. To use each model provider, you need to configure your local environment and make sure you have the right packages installed. If you try to use the model without having done so, you'll be told what to install. ## Models and Providers Pydantic AI uses a few key terms to describe how it interacts with different LLMs: * **Model**: This refers to the Pydantic AI class used to make requests following a specific LLM API (generally by wrapping a vendor-provided SDK, like the openai python SDK). These classes implement a vendor-SDK-agnostic API, ensuring a single Pydantic AI agent is portable to different LLM vendors without any other code changes just by swapping out the Model it uses. Model classes are named roughly in the format <VendorSdk>Model , for example, we have OpenAIChatModel , AnthropicModel , GoogleModel , etc. When using a Model class, you specify the actual LLM model name (e.g., gpt-4o , claude-3-5-sonnet-latest , gemini-1.5-flash ) as a parameter. * **Provider**: This refers to provider-specific classes which handle the authentication and connections to an LLM vendor. Passing a non-default *Provider* as a parameter to a Model is how you can ensure that your agent will make requests to a specific endpoint, or make use of a specific approach to authentication (e.g., you can use Azure auth with the OpenAIChatModel by way of the AzureProvider ). In particular, this is how you can make use of an AI gateway, or an LLM vendor that offers API compatibility with the vendor SDK used by an existing Model (such as OpenAIChatModel ). * **Profile**: This refers to a description of how requests to a specific model or family of models need to be constructed to get the best results, independent of the model and provider classes used. For example, different models have different restrictions on the JSON schemas that can be used for tools, and the same schema transformer needs to be used for Gemini models whether you're using GoogleModel with model name gemini-2.5-pro-preview , or OpenAIChatModel with OpenRouterProvider and model name google/gemini-2.5-pro-preview . When you instantiate an [ Agent ](../../api/agent/index.html#pydantic_ai.agent.Agent) with just a name formatted as <provider>:<model> , e.g. openai:gpt-4o or openrouter:google/gemini-2.5-pro-preview , Pydantic AI will automatically select the appropriate model class, provider, and profile. If you want to use a different provider or profile, you can instantiate a model class directly and pass in provider and/or profile arguments. ## Custom Models Note If a model API is compatible with the OpenAI API, you do not need a custom model class and can provide your own [custom provider](../openai/index.html#openai-compatible-models) instead. To implement support for a model API that's not already supported, you will need to subclass the [ Model ](../../api/models/base/index.html#pydantic_ai.models.Model) abstract base class. For streaming, you'll also need to implement the [ StreamedResponse ](../../api/models/base/index.html#pydantic_ai.models.StreamedResponse) abstract base class. The best place to start is to review the source code for existing implementations, e.g. [ OpenAIChatModel ](https://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_slim/pydantic_ai/models/openai.py). For details on when we'll accept contributions adding new models to Pydantic AI, see the [contributing guidelines](../../contributing/index.html#new-model-rules). ## Fallback Model You can use [ FallbackModel ](../../api/models/fallback/index.html#pydantic_ai.models.fallback.FallbackModel) to attempt multiple models in sequence until one successfully returns a result. Under the hood, Pydantic AI automatically switches from one model to the next if the current model returns a 4xx or 5xx status code. In the following example, the agent first makes a request to the OpenAI model (which fails due to an invalid API key), and then falls back to the Anthropic model. fallback\\_model.py The ModelResponse message above indicates in the model_name field that the output was returned by the Anthropic model, which is the second model specified in the FallbackModel . Note Each model's options should be configured individually. For example, base_url , api_key , and custom clients should be set on each model itself, not on the FallbackModel . ### Per-Model Settings You can configure different [ ModelSettings ](../../api/settings/index.html#pydantic_ai.settings.ModelSettings) for each model in a fallback chain by passing the settings parameter when creating each model. This is particularly useful when different providers have different optimal configurations: fallback\\_model\\_per\\_settings.py In this example, if the OpenAI model fails, the agent will automatically fall back to the Anthropic model with its own configured settings. The FallbackModel itself doesn't have settings - it uses the individual settings of whichever model successfully handles the request. In this next example, we demonstrate the exception-handling capabilities of FallbackModel . If all models fail, a [ FallbackExceptionGroup ](../../api/exceptions/index.html#pydantic_ai.exceptions.FallbackExceptionGroup) is raised, which contains all the exceptions encountered during the run execution. Python >=3.11Python <3.11 fallback\\_model\\_failure.py Since [ except* ](https://docs.python.org/3/reference/compound_stmts.html#except-star) is only supported in Python 3.11+, we use the [ exceptiongroup ](https://github.com/agronholm/exceptiongroup) backport package for earlier Python versions: fallback\\_model\\_failure.py By default, the FallbackModel only moves on to the next model if the current model raises a [ ModelHTTPError ](../../api/exceptions/index.html#pydantic_ai.exceptions.ModelHTTPError). You can customize this behavior by passing a custom fallback_on argument to the FallbackModel constructor.", "url": "https://ai.pydantic.dev/models/overview/index.html#model-providers", "page": "models/overview/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAI-compatible Providers", "anchor": "openai-compatible-providers", "heading_level": 2, "md_text": "In addition, many providers are compatible with the OpenAI API, and can be used with OpenAIChatModel in Pydantic AI: * [DeepSeek](../openai/index.html#deepseek) * [Grok (xAI)](../openai/index.html#grok-xai) * [Ollama](../openai/index.html#ollama) * [OpenRouter](../openai/index.html#openrouter) * [Vercel AI Gateway](../openai/index.html#vercel-ai-gateway) * [Perplexity](../openai/index.html#perplexity) * [Fireworks AI](../openai/index.html#fireworks-ai) * [Together AI](../openai/index.html#together-ai) * [Azure AI Foundry](../openai/index.html#azure-ai-foundry) * [Heroku](../openai/index.html#heroku-ai) * [GitHub Models](../openai/index.html#github-models) * [Cerebras](../openai/index.html#cerebras) * [LiteLLM](../openai/index.html#litellm) * [Nebius AI Studio](../openai/index.html#nebius-ai-studio) * [OVHcloud AI Endpoints](../openai/index.html#ovhcloud-ai-endpoints) Pydantic AI also comes with [ TestModel ](../../api/models/test/index.html) and [ FunctionModel ](../../api/models/function/index.html) for testing and development. To use each model provider, you need to configure your local environment and make sure you have the right packages installed. If you try to use the model without having done so, you'll be told what to install.", "url": "https://ai.pydantic.dev/models/overview/index.html#openai-compatible-providers", "page": "models/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Models and Providers", "anchor": "models-and-providers", "heading_level": 2, "md_text": "Pydantic AI uses a few key terms to describe how it interacts with different LLMs: * **Model**: This refers to the Pydantic AI class used to make requests following a specific LLM API (generally by wrapping a vendor-provided SDK, like the openai python SDK). These classes implement a vendor-SDK-agnostic API, ensuring a single Pydantic AI agent is portable to different LLM vendors without any other code changes just by swapping out the Model it uses. Model classes are named roughly in the format <VendorSdk>Model , for example, we have OpenAIChatModel , AnthropicModel , GoogleModel , etc. When using a Model class, you specify the actual LLM model name (e.g., gpt-4o , claude-3-5-sonnet-latest , gemini-1.5-flash ) as a parameter. * **Provider**: This refers to provider-specific classes which handle the authentication and connections to an LLM vendor. Passing a non-default *Provider* as a parameter to a Model is how you can ensure that your agent will make requests to a specific endpoint, or make use of a specific approach to authentication (e.g., you can use Azure auth with the OpenAIChatModel by way of the AzureProvider ). In particular, this is how you can make use of an AI gateway, or an LLM vendor that offers API compatibility with the vendor SDK used by an existing Model (such as OpenAIChatModel ). * **Profile**: This refers to a description of how requests to a specific model or family of models need to be constructed to get the best results, independent of the model and provider classes used. For example, different models have different restrictions on the JSON schemas that can be used for tools, and the same schema transformer needs to be used for Gemini models whether you're using GoogleModel with model name gemini-2.5-pro-preview , or OpenAIChatModel with OpenRouterProvider and model name google/gemini-2.5-pro-preview . When you instantiate an [ Agent ](../../api/agent/index.html#pydantic_ai.agent.Agent) with just a name formatted as <provider>:<model> , e.g. openai:gpt-4o or openrouter:google/gemini-2.5-pro-preview , Pydantic AI will automatically select the appropriate model class, provider, and profile. If you want to use a different provider or profile, you can instantiate a model class directly and pass in provider and/or profile arguments.", "url": "https://ai.pydantic.dev/models/overview/index.html#models-and-providers", "page": "models/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Models", "anchor": "custom-models", "heading_level": 2, "md_text": "Note If a model API is compatible with the OpenAI API, you do not need a custom model class and can provide your own [custom provider](../openai/index.html#openai-compatible-models) instead. To implement support for a model API that's not already supported, you will need to subclass the [ Model ](../../api/models/base/index.html#pydantic_ai.models.Model) abstract base class. For streaming, you'll also need to implement the [ StreamedResponse ](../../api/models/base/index.html#pydantic_ai.models.StreamedResponse) abstract base class. The best place to start is to review the source code for existing implementations, e.g. [ OpenAIChatModel ](https://github.com/pydantic/pydantic-ai/blob/main/pydantic_ai_slim/pydantic_ai/models/openai.py). For details on when we'll accept contributions adding new models to Pydantic AI, see the [contributing guidelines](../../contributing/index.html#new-model-rules).", "url": "https://ai.pydantic.dev/models/overview/index.html#custom-models", "page": "models/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Fallback Model", "anchor": "fallback-model", "heading_level": 2, "md_text": "You can use [ FallbackModel ](../../api/models/fallback/index.html#pydantic_ai.models.fallback.FallbackModel) to attempt multiple models in sequence until one successfully returns a result. Under the hood, Pydantic AI automatically switches from one model to the next if the current model returns a 4xx or 5xx status code. In the following example, the agent first makes a request to the OpenAI model (which fails due to an invalid API key), and then falls back to the Anthropic model. fallback\\_model.py The ModelResponse message above indicates in the model_name field that the output was returned by the Anthropic model, which is the second model specified in the FallbackModel . Note Each model's options should be configured individually. For example, base_url , api_key , and custom clients should be set on each model itself, not on the FallbackModel . ### Per-Model Settings You can configure different [ ModelSettings ](../../api/settings/index.html#pydantic_ai.settings.ModelSettings) for each model in a fallback chain by passing the settings parameter when creating each model. This is particularly useful when different providers have different optimal configurations: fallback\\_model\\_per\\_settings.py In this example, if the OpenAI model fails, the agent will automatically fall back to the Anthropic model with its own configured settings. The FallbackModel itself doesn't have settings - it uses the individual settings of whichever model successfully handles the request. In this next example, we demonstrate the exception-handling capabilities of FallbackModel . If all models fail, a [ FallbackExceptionGroup ](../../api/exceptions/index.html#pydantic_ai.exceptions.FallbackExceptionGroup) is raised, which contains all the exceptions encountered during the run execution. Python >=3.11Python <3.11 fallback\\_model\\_failure.py Since [ except* ](https://docs.python.org/3/reference/compound_stmts.html#except-star) is only supported in Python 3.11+, we use the [ exceptiongroup ](https://github.com/agronholm/exceptiongroup) backport package for earlier Python versions: fallback\\_model\\_failure.py By default, the FallbackModel only moves on to the next model if the current model raises a [ ModelHTTPError ](../../api/exceptions/index.html#pydantic_ai.exceptions.ModelHTTPError). You can customize this behavior by passing a custom fallback_on argument to the FallbackModel constructor.", "url": "https://ai.pydantic.dev/models/overview/index.html#fallback-model", "page": "models/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Per-Model Settings", "anchor": "per-model-settings", "heading_level": 3, "md_text": "You can configure different [ ModelSettings ](../../api/settings/index.html#pydantic_ai.settings.ModelSettings) for each model in a fallback chain by passing the settings parameter when creating each model. This is particularly useful when different providers have different optimal configurations: fallback\\_model\\_per\\_settings.py In this example, if the OpenAI model fails, the agent will automatically fall back to the Anthropic model with its own configured settings. The FallbackModel itself doesn't have settings - it uses the individual settings of whichever model successfully handles the request. In this next example, we demonstrate the exception-handling capabilities of FallbackModel . If all models fail, a [ FallbackExceptionGroup ](../../api/exceptions/index.html#pydantic_ai.exceptions.FallbackExceptionGroup) is raised, which contains all the exceptions encountered during the run execution. Python >=3.11Python <3.11 fallback\\_model\\_failure.py Since [ except* ](https://docs.python.org/3/reference/compound_stmts.html#except-star) is only supported in Python 3.11+, we use the [ exceptiongroup ](https://github.com/agronholm/exceptiongroup) backport package for earlier Python versions: fallback\\_model\\_failure.py By default, the FallbackModel only moves on to the next model if the current model raises a [ ModelHTTPError ](../../api/exceptions/index.html#pydantic_ai.exceptions.ModelHTTPError). You can customize this behavior by passing a custom fallback_on argument to the FallbackModel constructor.", "url": "https://ai.pydantic.dev/models/overview/index.html#per-model-settings", "page": "models/overview/index.html", "source_site": "pydantic_ai"}
{"title": "Thinking", "anchor": "thinking", "heading_level": 1, "md_text": "Thinking (or reasoning) is the process by which a model works through a problem step-by-step before providing its final answer. This capability is typically disabled by default and depends on the specific model being used. See the sections below for how to enable thinking for each provider. ## OpenAI When using the [ OpenAIChatModel ](../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIChatModel), text output inside <think> tags are converted to [ ThinkingPart ](../api/messages/index.html#pydantic_ai.messages.ThinkingPart) objects. You can customize the tags using the [ thinking_tags ](../api/profiles/index.html#pydantic_ai.profiles.ModelProfile.thinking_tags) field on the [model profile](../models/openai/index.html#model-profile). ### OpenAI Responses The [ OpenAIResponsesModel ](../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIResponsesModel) can generate native thinking parts. To enable this functionality, you need to set the [ OpenAIResponsesModelSettings.openai_reasoning_effort ](../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIChatModelSettings.openai_reasoning_effort) and [ OpenAIResponsesModelSettings.openai_reasoning_summary ](../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIResponsesModelSettings.openai_reasoning_summary) [model settings](../agents/index.html#model-run-settings). By default, the unique IDs of reasoning, text, and function call parts from the message history are sent to the model, which can result in errors like \"Item 'rs_123' of type 'reasoning' was provided without its required following item.\" if the message history you're sending does not match exactly what was received from the Responses API in a previous response, for example if you're using a [history processor](../message-history/index.html#processing-message-history). To disable this, you can disable the [ OpenAIResponsesModelSettings.openai_send_reasoning_ids ](../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIResponsesModelSettings.openai_send_reasoning_ids) [model setting](../agents/index.html#model-run-settings). openai\\_thinking\\_part.py ## Anthropic To enable thinking, use the [ AnthropicModelSettings.anthropic_thinking ](../api/models/anthropic/index.html#pydantic_ai.models.anthropic.AnthropicModelSettings.anthropic_thinking) [model setting](../agents/index.html#model-run-settings). anthropic\\_thinking\\_part.py ## Google To enable thinking, use the [ GoogleModelSettings.google_thinking_config ](../api/models/google/index.html#pydantic_ai.models.google.GoogleModelSettings.google_thinking_config) [model setting](../agents/index.html#model-run-settings). google\\_thinking\\_part.py ## Bedrock ## Groq Groq supports different formats to receive thinking parts: * \"raw\" : The thinking part is included in the text content inside <think> tags, which are automatically converted to [ ThinkingPart ](../api/messages/index.html#pydantic_ai.messages.ThinkingPart) objects. * \"hidden\" : The thinking part is not included in the text content. * \"parsed\" : The thinking part has its own structured part in the response which is converted into a [ ThinkingPart ](../api/messages/index.html#pydantic_ai.messages.ThinkingPart) object. To enable thinking, use the [ GroqModelSettings.groq_reasoning_format ](../api/models/groq/index.html#pydantic_ai.models.groq.GroqModelSettings.groq_reasoning_format) [model setting](../agents/index.html#model-run-settings): groq\\_thinking\\_part.py ## Mistral Thinking is supported by the magistral family of models. It does not need to be specifically enabled. ## Cohere Thinking is supported by the command-a-reasoning-08-2025 model. It does not need to be specifically enabled. ## Hugging Face Text output inside <think> tags is automatically converted to [ ThinkingPart ](../api/messages/index.html#pydantic_ai.messages.ThinkingPart) objects. You can customize the tags using the [ thinking_tags ](../api/profiles/index.html#pydantic_ai.profiles.ModelProfile.thinking_tags) field on the [model profile](../models/openai/index.html#model-profile).", "url": "https://ai.pydantic.dev/thinking/index.html#thinking", "page": "thinking/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAI", "anchor": "openai", "heading_level": 2, "md_text": "When using the [ OpenAIChatModel ](../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIChatModel), text output inside <think> tags are converted to [ ThinkingPart ](../api/messages/index.html#pydantic_ai.messages.ThinkingPart) objects. You can customize the tags using the [ thinking_tags ](../api/profiles/index.html#pydantic_ai.profiles.ModelProfile.thinking_tags) field on the [model profile](../models/openai/index.html#model-profile). ### OpenAI Responses The [ OpenAIResponsesModel ](../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIResponsesModel) can generate native thinking parts. To enable this functionality, you need to set the [ OpenAIResponsesModelSettings.openai_reasoning_effort ](../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIChatModelSettings.openai_reasoning_effort) and [ OpenAIResponsesModelSettings.openai_reasoning_summary ](../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIResponsesModelSettings.openai_reasoning_summary) [model settings](../agents/index.html#model-run-settings). By default, the unique IDs of reasoning, text, and function call parts from the message history are sent to the model, which can result in errors like \"Item 'rs_123' of type 'reasoning' was provided without its required following item.\" if the message history you're sending does not match exactly what was received from the Responses API in a previous response, for example if you're using a [history processor](../message-history/index.html#processing-message-history). To disable this, you can disable the [ OpenAIResponsesModelSettings.openai_send_reasoning_ids ](../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIResponsesModelSettings.openai_send_reasoning_ids) [model setting](../agents/index.html#model-run-settings). openai\\_thinking\\_part.py", "url": "https://ai.pydantic.dev/thinking/index.html#openai", "page": "thinking/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAI Responses", "anchor": "openai-responses", "heading_level": 3, "md_text": "The [ OpenAIResponsesModel ](../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIResponsesModel) can generate native thinking parts. To enable this functionality, you need to set the [ OpenAIResponsesModelSettings.openai_reasoning_effort ](../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIChatModelSettings.openai_reasoning_effort) and [ OpenAIResponsesModelSettings.openai_reasoning_summary ](../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIResponsesModelSettings.openai_reasoning_summary) [model settings](../agents/index.html#model-run-settings). By default, the unique IDs of reasoning, text, and function call parts from the message history are sent to the model, which can result in errors like \"Item 'rs_123' of type 'reasoning' was provided without its required following item.\" if the message history you're sending does not match exactly what was received from the Responses API in a previous response, for example if you're using a [history processor](../message-history/index.html#processing-message-history). To disable this, you can disable the [ OpenAIResponsesModelSettings.openai_send_reasoning_ids ](../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIResponsesModelSettings.openai_send_reasoning_ids) [model setting](../agents/index.html#model-run-settings). openai\\_thinking\\_part.py", "url": "https://ai.pydantic.dev/thinking/index.html#openai-responses", "page": "thinking/index.html", "source_site": "pydantic_ai"}
{"title": "Anthropic", "anchor": "anthropic", "heading_level": 2, "md_text": "To enable thinking, use the [ AnthropicModelSettings.anthropic_thinking ](../api/models/anthropic/index.html#pydantic_ai.models.anthropic.AnthropicModelSettings.anthropic_thinking) [model setting](../agents/index.html#model-run-settings). anthropic\\_thinking\\_part.py", "url": "https://ai.pydantic.dev/thinking/index.html#anthropic", "page": "thinking/index.html", "source_site": "pydantic_ai"}
{"title": "Google", "anchor": "google", "heading_level": 2, "md_text": "To enable thinking, use the [ GoogleModelSettings.google_thinking_config ](../api/models/google/index.html#pydantic_ai.models.google.GoogleModelSettings.google_thinking_config) [model setting](../agents/index.html#model-run-settings). google\\_thinking\\_part.py", "url": "https://ai.pydantic.dev/thinking/index.html#google", "page": "thinking/index.html", "source_site": "pydantic_ai"}
{"title": "Bedrock", "anchor": "bedrock", "heading_level": 2, "md_text": "", "url": "https://ai.pydantic.dev/thinking/index.html#bedrock", "page": "thinking/index.html", "source_site": "pydantic_ai"}
{"title": "Groq", "anchor": "groq", "heading_level": 2, "md_text": "Groq supports different formats to receive thinking parts: * \"raw\" : The thinking part is included in the text content inside <think> tags, which are automatically converted to [ ThinkingPart ](../api/messages/index.html#pydantic_ai.messages.ThinkingPart) objects. * \"hidden\" : The thinking part is not included in the text content. * \"parsed\" : The thinking part has its own structured part in the response which is converted into a [ ThinkingPart ](../api/messages/index.html#pydantic_ai.messages.ThinkingPart) object. To enable thinking, use the [ GroqModelSettings.groq_reasoning_format ](../api/models/groq/index.html#pydantic_ai.models.groq.GroqModelSettings.groq_reasoning_format) [model setting](../agents/index.html#model-run-settings): groq\\_thinking\\_part.py", "url": "https://ai.pydantic.dev/thinking/index.html#groq", "page": "thinking/index.html", "source_site": "pydantic_ai"}
{"title": "Mistral", "anchor": "mistral", "heading_level": 2, "md_text": "Thinking is supported by the magistral family of models. It does not need to be specifically enabled.", "url": "https://ai.pydantic.dev/thinking/index.html#mistral", "page": "thinking/index.html", "source_site": "pydantic_ai"}
{"title": "Cohere", "anchor": "cohere", "heading_level": 2, "md_text": "Thinking is supported by the command-a-reasoning-08-2025 model. It does not need to be specifically enabled.", "url": "https://ai.pydantic.dev/thinking/index.html#cohere", "page": "thinking/index.html", "source_site": "pydantic_ai"}
{"title": "Hugging Face", "anchor": "hugging-face", "heading_level": 2, "md_text": "Text output inside <think> tags is automatically converted to [ ThinkingPart ](../api/messages/index.html#pydantic_ai.messages.ThinkingPart) objects. You can customize the tags using the [ thinking_tags ](../api/profiles/index.html#pydantic_ai.profiles.ModelProfile.thinking_tags) field on the [model profile](../models/openai/index.html#model-profile).", "url": "https://ai.pydantic.dev/thinking/index.html#hugging-face", "page": "thinking/index.html", "source_site": "pydantic_ai"}
{"title": "Multi-agent Applications", "anchor": "multi-agent-applications", "heading_level": 1, "md_text": "There are roughly four levels of complexity when building applications with Pydantic AI: 1. Single agent workflows  what most of the pydantic_ai documentation covers 2. [Agent delegation](index.html#agent-delegation)  agents using another agent via tools 3. [Programmatic agent hand-off](index.html#programmatic-agent-hand-off)  one agent runs, then application code calls another agent 4. [Graph based control flow](../graph/index.html)  for the most complex cases, a graph-based state machine can be used to control the execution of multiple agents Of course, you can combine multiple strategies in a single application. ## Agent delegation \"Agent delegation\" refers to the scenario where an agent delegates work to another agent, then takes back control when the delegate agent (the agent called from within a tool) finishes. If you want to hand off control to another agent completely, without coming back to the first agent, you can use an [output function](../output/index.html#output-functions). Since agents are stateless and designed to be global, you do not need to include the agent itself in agent [dependencies](../dependencies/index.html). You'll generally want to pass [ ctx.usage ](../api/tools/index.html#pydantic_ai.tools.RunContext.usage) to the [ usage ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run) keyword argument of the delegate agent run so usage within that run counts towards the total usage of the parent agent run. Multiple models Agent delegation doesn't need to use the same model for each agent. If you choose to use different models within a run, calculating the monetary cost from the final [ result.usage() ](../api/agent/index.html#pydantic_ai.agent.AgentRunResult.usage) of the run will not be possible, but you can still use [ UsageLimits ](../api/usage/index.html#pydantic_ai.usage.UsageLimits)  including request_limit , total_tokens_limit , and tool_calls_limit  to avoid unexpected costs or runaway tool loops. agent\\_delegation\\_simple.py 1. The \"parent\" or controlling agent. 2. The \"delegate\" agent, which is called from within a tool of the parent agent. 3. Call the delegate agent from within a tool of the parent agent. 4. Pass the usage from the parent agent to the delegate agent so the final [ result.usage() ](../api/agent/index.html#pydantic_ai.agent.AgentRunResult.usage) includes the usage from both agents. 5. Since the function returns list[str] , and the output_type of joke_generation_agent is also list[str] , we can simply return r.output from the tool. *(This example is complete, it can be run \"as is\")* The control flow for this example is pretty simple and can be summarised as follows: ### Agent delegation and dependencies Generally the delegate agent needs to either have the same [dependencies](../dependencies/index.html) as the calling agent, or dependencies which are a subset of the calling agent's dependencies. Initializing dependencies We say \"generally\" above since there's nothing to stop you initializing dependencies within a tool call and therefore using interdependencies in a delegate agent that are not available on the parent, this should often be avoided since it can be significantly slower than reusing connections etc. from the parent agent. agent\\_delegation\\_deps.py 1. Define a dataclass to hold the client and API key dependencies. 2. Set the deps_type of the calling agent  joke_selection_agent here. 3. Pass the dependencies to the delegate agent's run method within the tool call. 4. Also set the deps_type of the delegate agent  joke_generation_agent here. 5. Define a tool on the delegate agent that uses the dependencies to make an HTTP request. 6. Usage now includes 4 requests  2 from the calling agent and 2 from the delegate agent. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* This example shows how even a fairly simple agent delegation can lead to a complex control flow: ## Programmatic agent hand-off \"Programmatic agent hand-off\" refers to the scenario where multiple agents are called in succession, with application code and/or a human in the loop responsible for deciding which agent to call next. Here agents don't need to use the same deps. Here we show two agents used in succession, the first to find a flight and the second to extract the user's seat preference. programmatic\\_handoff.py 1. Define the first agent, which finds a flight. We use an explicit type annotation until [PEP-747](https://peps.python.org/pep-0747/) lands, see [structured output](../output/index.html#structured-output). We use a union as the output type so the model can communicate if it's unable to find a satisfactory choice; internally, each member of the union will be registered as a separate tool. 2. Define a tool on the agent to find a flight. In this simple case we could dispense with the tool and just define the agent to return structured data, then search for a flight, but in more complex scenarios the tool would be necessary. 3. Define usage limits for the entire app. 4. Define a function to find a flight, which asks the user for their preferences and then calls the agent to find a flight. 5. As with flight_search_agent above, we use an explicit type annotation to define the agent. 6. Define a function to find the user's seat preference, which asks the user for their seat preference and then calls the agent to extract the seat preference. 7. Now that we've put our logic for running each agent into separate functions, our main app becomes very simple. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* The control flow for this example can be summarised as follows: ## Pydantic Graphs See the [graph](../graph/index.html) documentation on when and how to use graphs. ## Examples The following examples demonstrate how to use dependencies in Pydantic AI: * [Flight booking](../examples/flight-booking/index.html)", "url": "https://ai.pydantic.dev/multi-agent-applications/index.html#multi-agent-applications", "page": "multi-agent-applications/index.html", "source_site": "pydantic_ai"}
{"title": "Agent delegation", "anchor": "agent-delegation", "heading_level": 2, "md_text": "\"Agent delegation\" refers to the scenario where an agent delegates work to another agent, then takes back control when the delegate agent (the agent called from within a tool) finishes. If you want to hand off control to another agent completely, without coming back to the first agent, you can use an [output function](../output/index.html#output-functions). Since agents are stateless and designed to be global, you do not need to include the agent itself in agent [dependencies](../dependencies/index.html). You'll generally want to pass [ ctx.usage ](../api/tools/index.html#pydantic_ai.tools.RunContext.usage) to the [ usage ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run) keyword argument of the delegate agent run so usage within that run counts towards the total usage of the parent agent run. Multiple models Agent delegation doesn't need to use the same model for each agent. If you choose to use different models within a run, calculating the monetary cost from the final [ result.usage() ](../api/agent/index.html#pydantic_ai.agent.AgentRunResult.usage) of the run will not be possible, but you can still use [ UsageLimits ](../api/usage/index.html#pydantic_ai.usage.UsageLimits)  including request_limit , total_tokens_limit , and tool_calls_limit  to avoid unexpected costs or runaway tool loops. agent\\_delegation\\_simple.py 1. The \"parent\" or controlling agent. 2. The \"delegate\" agent, which is called from within a tool of the parent agent. 3. Call the delegate agent from within a tool of the parent agent. 4. Pass the usage from the parent agent to the delegate agent so the final [ result.usage() ](../api/agent/index.html#pydantic_ai.agent.AgentRunResult.usage) includes the usage from both agents. 5. Since the function returns list[str] , and the output_type of joke_generation_agent is also list[str] , we can simply return r.output from the tool. *(This example is complete, it can be run \"as is\")* The control flow for this example is pretty simple and can be summarised as follows: ### Agent delegation and dependencies Generally the delegate agent needs to either have the same [dependencies](../dependencies/index.html) as the calling agent, or dependencies which are a subset of the calling agent's dependencies. Initializing dependencies We say \"generally\" above since there's nothing to stop you initializing dependencies within a tool call and therefore using interdependencies in a delegate agent that are not available on the parent, this should often be avoided since it can be significantly slower than reusing connections etc. from the parent agent. agent\\_delegation\\_deps.py 1. Define a dataclass to hold the client and API key dependencies. 2. Set the deps_type of the calling agent  joke_selection_agent here. 3. Pass the dependencies to the delegate agent's run method within the tool call. 4. Also set the deps_type of the delegate agent  joke_generation_agent here. 5. Define a tool on the delegate agent that uses the dependencies to make an HTTP request. 6. Usage now includes 4 requests  2 from the calling agent and 2 from the delegate agent. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* This example shows how even a fairly simple agent delegation can lead to a complex control flow:", "url": "https://ai.pydantic.dev/multi-agent-applications/index.html#agent-delegation", "page": "multi-agent-applications/index.html", "source_site": "pydantic_ai"}
{"title": "Agent delegation and dependencies", "anchor": "agent-delegation-and-dependencies", "heading_level": 3, "md_text": "Generally the delegate agent needs to either have the same [dependencies](../dependencies/index.html) as the calling agent, or dependencies which are a subset of the calling agent's dependencies. Initializing dependencies We say \"generally\" above since there's nothing to stop you initializing dependencies within a tool call and therefore using interdependencies in a delegate agent that are not available on the parent, this should often be avoided since it can be significantly slower than reusing connections etc. from the parent agent. agent\\_delegation\\_deps.py 1. Define a dataclass to hold the client and API key dependencies. 2. Set the deps_type of the calling agent  joke_selection_agent here. 3. Pass the dependencies to the delegate agent's run method within the tool call. 4. Also set the deps_type of the delegate agent  joke_generation_agent here. 5. Define a tool on the delegate agent that uses the dependencies to make an HTTP request. 6. Usage now includes 4 requests  2 from the calling agent and 2 from the delegate agent. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* This example shows how even a fairly simple agent delegation can lead to a complex control flow:", "url": "https://ai.pydantic.dev/multi-agent-applications/index.html#agent-delegation-and-dependencies", "page": "multi-agent-applications/index.html", "source_site": "pydantic_ai"}
{"title": "Programmatic agent hand-off", "anchor": "programmatic-agent-hand-off", "heading_level": 2, "md_text": "\"Programmatic agent hand-off\" refers to the scenario where multiple agents are called in succession, with application code and/or a human in the loop responsible for deciding which agent to call next. Here agents don't need to use the same deps. Here we show two agents used in succession, the first to find a flight and the second to extract the user's seat preference. programmatic\\_handoff.py 1. Define the first agent, which finds a flight. We use an explicit type annotation until [PEP-747](https://peps.python.org/pep-0747/) lands, see [structured output](../output/index.html#structured-output). We use a union as the output type so the model can communicate if it's unable to find a satisfactory choice; internally, each member of the union will be registered as a separate tool. 2. Define a tool on the agent to find a flight. In this simple case we could dispense with the tool and just define the agent to return structured data, then search for a flight, but in more complex scenarios the tool would be necessary. 3. Define usage limits for the entire app. 4. Define a function to find a flight, which asks the user for their preferences and then calls the agent to find a flight. 5. As with flight_search_agent above, we use an explicit type annotation to define the agent. 6. Define a function to find the user's seat preference, which asks the user for their seat preference and then calls the agent to extract the seat preference. 7. Now that we've put our logic for running each agent into separate functions, our main app becomes very simple. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* The control flow for this example can be summarised as follows:", "url": "https://ai.pydantic.dev/multi-agent-applications/index.html#programmatic-agent-hand-off", "page": "multi-agent-applications/index.html", "source_site": "pydantic_ai"}
{"title": "Pydantic Graphs", "anchor": "pydantic-graphs", "heading_level": 2, "md_text": "See the [graph](../graph/index.html) documentation on when and how to use graphs.", "url": "https://ai.pydantic.dev/multi-agent-applications/index.html#pydantic-graphs", "page": "multi-agent-applications/index.html", "source_site": "pydantic_ai"}
{"title": "Examples", "anchor": "examples", "heading_level": 2, "md_text": "The following examples demonstrate how to use dependencies in Pydantic AI: * [Flight booking](../examples/flight-booking/index.html)", "url": "https://ai.pydantic.dev/multi-agent-applications/index.html#examples", "page": "multi-agent-applications/index.html", "source_site": "pydantic_ai"}
{"title": "HTTP Request Retries", "anchor": "http-request-retries", "heading_level": 1, "md_text": "Pydantic AI provides retry functionality for HTTP requests made by model providers through custom HTTP transports. This is particularly useful for handling transient failures like rate limits, network timeouts, or temporary server errors. ## Overview The retry functionality is built on top of the [tenacity](https://github.com/jd/tenacity) library and integrates seamlessly with httpx clients. You can configure retry behavior for any provider that accepts a custom HTTP client. ## Installation To use the retry transports, you need to install tenacity , which you can do via the retries dependency group: pipuv ## Usage Example Here's an example of adding retry functionality with smart retry handling: smart\\_retry\\_example.py ## Wait Strategies ### wait\\_retry\\_after The wait_retry_after function is a smart wait strategy that automatically respects HTTP Retry-After headers: wait\\_strategy\\_example.py This wait strategy: * Automatically parses Retry-After headers from HTTP 429 responses * Supports both seconds format ( \"30\" ) and HTTP date format ( \"Wed, 21 Oct 2015 07:28:00 GMT\" ) * Falls back to your chosen strategy when no header is present * Respects the max_wait limit to prevent excessive delays ## Transport Classes ### AsyncTenacityTransport For asynchronous HTTP clients (recommended for most use cases): async\\_transport\\_example.py ### TenacityTransport For synchronous HTTP clients: sync\\_transport\\_example.py ## Common Retry Patterns ### Rate Limit Handling with Retry-After Support rate\\_limit\\_handling.py The wait_retry_after function automatically detects Retry-After headers in 429 (rate limit) responses and waits for the specified time. If no header is present, it falls back to exponential backoff. ### Network Error Handling network\\_error\\_handling.py ### Custom Retry Logic custom\\_retry\\_logic.py ## Using with Different Providers The retry transports work with any provider that accepts a custom HTTP client: ### OpenAI openai\\_with\\_retries.py ### Anthropic anthropic\\_with\\_retries.py ### Any OpenAI-Compatible Provider openai\\_compatible\\_with\\_retries.py ## Best Practices 1. **Start Conservative**: Begin with a small number of retries (3-5) and reasonable wait times. 2. **Use Exponential Backoff**: This helps avoid overwhelming servers during outages. 3. **Set Maximum Wait Times**: Prevent indefinite delays with reasonable maximum wait times. 4. **Handle Rate Limits Properly**: Respect Retry-After headers when possible. 5. **Log Retry Attempts**: Add logging to monitor retry behavior in production. (This will be picked up by Logfire automatically if you instrument httpx.) 6. **Consider Circuit Breakers**: For high-traffic applications, consider implementing circuit breaker patterns. ## Error Handling The retry transports will re-raise the last exception if all retry attempts fail. Make sure to handle these appropriately in your application: error\\_handling\\_example.py ## Performance Considerations * Retries add latency to requests, especially with exponential backoff * Consider the total timeout for your application when configuring retry behavior * Monitor retry rates to detect systemic issues * Use async transports for better concurrency when handling multiple requests For more advanced retry configurations, refer to the [tenacity documentation](https://tenacity.readthedocs.io/).", "url": "https://ai.pydantic.dev/retries/index.html#http-request-retries", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "Overview", "anchor": "overview", "heading_level": 2, "md_text": "The retry functionality is built on top of the [tenacity](https://github.com/jd/tenacity) library and integrates seamlessly with httpx clients. You can configure retry behavior for any provider that accepts a custom HTTP client.", "url": "https://ai.pydantic.dev/retries/index.html#overview", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "Installation", "anchor": "installation", "heading_level": 2, "md_text": "To use the retry transports, you need to install tenacity , which you can do via the retries dependency group: pipuv", "url": "https://ai.pydantic.dev/retries/index.html#installation", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "Usage Example", "anchor": "usage-example", "heading_level": 2, "md_text": "Here's an example of adding retry functionality with smart retry handling: smart\\_retry\\_example.py", "url": "https://ai.pydantic.dev/retries/index.html#usage-example", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "Wait Strategies", "anchor": "wait-strategies", "heading_level": 2, "md_text": "### wait\\_retry\\_after The wait_retry_after function is a smart wait strategy that automatically respects HTTP Retry-After headers: wait\\_strategy\\_example.py This wait strategy: * Automatically parses Retry-After headers from HTTP 429 responses * Supports both seconds format ( \"30\" ) and HTTP date format ( \"Wed, 21 Oct 2015 07:28:00 GMT\" ) * Falls back to your chosen strategy when no header is present * Respects the max_wait limit to prevent excessive delays", "url": "https://ai.pydantic.dev/retries/index.html#wait-strategies", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "wait_retry_after", "anchor": "wait_retry_after", "heading_level": 3, "md_text": "The wait_retry_after function is a smart wait strategy that automatically respects HTTP Retry-After headers: wait\\_strategy\\_example.py This wait strategy: * Automatically parses Retry-After headers from HTTP 429 responses * Supports both seconds format ( \"30\" ) and HTTP date format ( \"Wed, 21 Oct 2015 07:28:00 GMT\" ) * Falls back to your chosen strategy when no header is present * Respects the max_wait limit to prevent excessive delays", "url": "https://ai.pydantic.dev/retries/index.html#wait_retry_after", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "Transport Classes", "anchor": "transport-classes", "heading_level": 2, "md_text": "### AsyncTenacityTransport For asynchronous HTTP clients (recommended for most use cases): async\\_transport\\_example.py ### TenacityTransport For synchronous HTTP clients: sync\\_transport\\_example.py", "url": "https://ai.pydantic.dev/retries/index.html#transport-classes", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "AsyncTenacityTransport", "anchor": "asynctenacitytransport", "heading_level": 3, "md_text": "For asynchronous HTTP clients (recommended for most use cases): async\\_transport\\_example.py", "url": "https://ai.pydantic.dev/retries/index.html#asynctenacitytransport", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "TenacityTransport", "anchor": "tenacitytransport", "heading_level": 3, "md_text": "For synchronous HTTP clients: sync\\_transport\\_example.py", "url": "https://ai.pydantic.dev/retries/index.html#tenacitytransport", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "Common Retry Patterns", "anchor": "common-retry-patterns", "heading_level": 2, "md_text": "### Rate Limit Handling with Retry-After Support rate\\_limit\\_handling.py The wait_retry_after function automatically detects Retry-After headers in 429 (rate limit) responses and waits for the specified time. If no header is present, it falls back to exponential backoff. ### Network Error Handling network\\_error\\_handling.py ### Custom Retry Logic custom\\_retry\\_logic.py", "url": "https://ai.pydantic.dev/retries/index.html#common-retry-patterns", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "Rate Limit Handling with Retry-After Support", "anchor": "rate-limit-handling-with-retry-after-support", "heading_level": 3, "md_text": "rate\\_limit\\_handling.py The wait_retry_after function automatically detects Retry-After headers in 429 (rate limit) responses and waits for the specified time. If no header is present, it falls back to exponential backoff.", "url": "https://ai.pydantic.dev/retries/index.html#rate-limit-handling-with-retry-after-support", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "Network Error Handling", "anchor": "network-error-handling", "heading_level": 3, "md_text": "network\\_error\\_handling.py", "url": "https://ai.pydantic.dev/retries/index.html#network-error-handling", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Retry Logic", "anchor": "custom-retry-logic", "heading_level": 3, "md_text": "custom\\_retry\\_logic.py", "url": "https://ai.pydantic.dev/retries/index.html#custom-retry-logic", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "Using with Different Providers", "anchor": "using-with-different-providers", "heading_level": 2, "md_text": "The retry transports work with any provider that accepts a custom HTTP client: ### OpenAI openai\\_with\\_retries.py ### Anthropic anthropic\\_with\\_retries.py ### Any OpenAI-Compatible Provider openai\\_compatible\\_with\\_retries.py", "url": "https://ai.pydantic.dev/retries/index.html#using-with-different-providers", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAI", "anchor": "openai", "heading_level": 3, "md_text": "openai\\_with\\_retries.py", "url": "https://ai.pydantic.dev/retries/index.html#openai", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "Anthropic", "anchor": "anthropic", "heading_level": 3, "md_text": "anthropic\\_with\\_retries.py", "url": "https://ai.pydantic.dev/retries/index.html#anthropic", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "Any OpenAI-Compatible Provider", "anchor": "any-openai-compatible-provider", "heading_level": 3, "md_text": "openai\\_compatible\\_with\\_retries.py", "url": "https://ai.pydantic.dev/retries/index.html#any-openai-compatible-provider", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "Best Practices", "anchor": "best-practices", "heading_level": 2, "md_text": "1. **Start Conservative**: Begin with a small number of retries (3-5) and reasonable wait times. 2. **Use Exponential Backoff**: This helps avoid overwhelming servers during outages. 3. **Set Maximum Wait Times**: Prevent indefinite delays with reasonable maximum wait times. 4. **Handle Rate Limits Properly**: Respect Retry-After headers when possible. 5. **Log Retry Attempts**: Add logging to monitor retry behavior in production. (This will be picked up by Logfire automatically if you instrument httpx.) 6. **Consider Circuit Breakers**: For high-traffic applications, consider implementing circuit breaker patterns.", "url": "https://ai.pydantic.dev/retries/index.html#best-practices", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "Error Handling", "anchor": "error-handling", "heading_level": 2, "md_text": "The retry transports will re-raise the last exception if all retry attempts fail. Make sure to handle these appropriately in your application: error\\_handling\\_example.py", "url": "https://ai.pydantic.dev/retries/index.html#error-handling", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "Performance Considerations", "anchor": "performance-considerations", "heading_level": 2, "md_text": "* Retries add latency to requests, especially with exponential backoff * Consider the total timeout for your application when configuring retry behavior * Monitor retry rates to detect systemic issues * Use async transports for better concurrency when handling multiple requests For more advanced retry configurations, refer to the [tenacity documentation](https://tenacity.readthedocs.io/).", "url": "https://ai.pydantic.dev/retries/index.html#performance-considerations", "page": "retries/index.html", "source_site": "pydantic_ai"}
{"title": "Third-Party Tools", "anchor": "third-party-tools", "heading_level": 1, "md_text": "Pydantic AI supports integration with various third-party tool libraries, allowing you to leverage existing tool ecosystems in your agents. ## MCP Tools See the [MCP Client](../mcp/client/index.html) documentation for how to use MCP servers with Pydantic AI as [toolsets](../toolsets/index.html). ## LangChain Tools If you'd like to use a tool from LangChain's [community tool library](https://python.langchain.com/docs/integrations/tools/) with Pydantic AI, you can use the [ tool_from_langchain ](../api/ext/index.html#pydantic_ai.ext.langchain.tool_from_langchain) convenience method. Note that Pydantic AI will not validate the arguments in this case -- it's up to the model to provide arguments matching the schema specified by the LangChain tool, and up to the LangChain tool to raise an error if the arguments are invalid. You will need to install the langchain-community package and any others required by the tool in question. Here is how you can use the LangChain DuckDuckGoSearchRun tool, which requires the ddgs package: 1. The release date of this game is the 30th of May 2025, which is after the knowledge cutoff for Gemini 2.0 (August 2024). If you'd like to use multiple LangChain tools or a LangChain [toolkit](https://python.langchain.com/docs/concepts/tools/#toolkits), you can use the [ LangChainToolset ](../api/ext/index.html#pydantic_ai.ext.langchain.LangChainToolset) [toolset](../toolsets/index.html) which takes a list of LangChain tools: ## ACI.dev Tools If you'd like to use a tool from the [ACI.dev tool library](https://www.aci.dev/tools) with Pydantic AI, you can use the [ tool_from_aci ](../api/ext/index.html#pydantic_ai.ext.aci.tool_from_aci) convenience method. Note that Pydantic AI will not validate the arguments in this case -- it's up to the model to provide arguments matching the schema specified by the ACI tool, and up to the ACI tool to raise an error if the arguments are invalid. You will need to install the aci-sdk package, set your ACI API key in the ACI_API_KEY environment variable, and pass your ACI \"linked account owner ID\" to the function. Here is how you can use the ACI.dev TAVILY__SEARCH tool: 1. The release date of this game is the 30th of May 2025, which is after the knowledge cutoff for Gemini 2.0 (August 2024). If you'd like to use multiple ACI.dev tools, you can use the [ ACIToolset ](../api/ext/index.html#pydantic_ai.ext.aci.ACIToolset) [toolset](../toolsets/index.html) which takes a list of ACI tool names as well as the linked_account_owner_id : ## See Also * [Function Tools](../tools/index.html) - Basic tool concepts and registration * [Toolsets](../toolsets/index.html) - Managing collections of tools * [MCP Client](../mcp/client/index.html) - Using MCP servers with Pydantic AI * [LangChain Toolsets](../toolsets/index.html#langchain-tools) - Using LangChain toolsets * [ACI.dev Toolsets](../toolsets/index.html#aci-tools) - Using ACI.dev toolsets", "url": "https://ai.pydantic.dev/third-party-tools/index.html#third-party-tools", "page": "third-party-tools/index.html", "source_site": "pydantic_ai"}
{"title": "MCP Tools", "anchor": "mcp-tools", "heading_level": 2, "md_text": "See the [MCP Client](../mcp/client/index.html) documentation for how to use MCP servers with Pydantic AI as [toolsets](../toolsets/index.html).", "url": "https://ai.pydantic.dev/third-party-tools/index.html#mcp-tools", "page": "third-party-tools/index.html", "source_site": "pydantic_ai"}
{"title": "LangChain Tools", "anchor": "langchain-tools", "heading_level": 2, "md_text": "If you'd like to use a tool from LangChain's [community tool library](https://python.langchain.com/docs/integrations/tools/) with Pydantic AI, you can use the [ tool_from_langchain ](../api/ext/index.html#pydantic_ai.ext.langchain.tool_from_langchain) convenience method. Note that Pydantic AI will not validate the arguments in this case -- it's up to the model to provide arguments matching the schema specified by the LangChain tool, and up to the LangChain tool to raise an error if the arguments are invalid. You will need to install the langchain-community package and any others required by the tool in question. Here is how you can use the LangChain DuckDuckGoSearchRun tool, which requires the ddgs package: 1. The release date of this game is the 30th of May 2025, which is after the knowledge cutoff for Gemini 2.0 (August 2024). If you'd like to use multiple LangChain tools or a LangChain [toolkit](https://python.langchain.com/docs/concepts/tools/#toolkits), you can use the [ LangChainToolset ](../api/ext/index.html#pydantic_ai.ext.langchain.LangChainToolset) [toolset](../toolsets/index.html) which takes a list of LangChain tools:", "url": "https://ai.pydantic.dev/third-party-tools/index.html#langchain-tools", "page": "third-party-tools/index.html", "source_site": "pydantic_ai"}
{"title": "ACI.dev Tools", "anchor": "aci-tools", "heading_level": 2, "md_text": "If you'd like to use a tool from the [ACI.dev tool library](https://www.aci.dev/tools) with Pydantic AI, you can use the [ tool_from_aci ](../api/ext/index.html#pydantic_ai.ext.aci.tool_from_aci) convenience method. Note that Pydantic AI will not validate the arguments in this case -- it's up to the model to provide arguments matching the schema specified by the ACI tool, and up to the ACI tool to raise an error if the arguments are invalid. You will need to install the aci-sdk package, set your ACI API key in the ACI_API_KEY environment variable, and pass your ACI \"linked account owner ID\" to the function. Here is how you can use the ACI.dev TAVILY__SEARCH tool: 1. The release date of this game is the 30th of May 2025, which is after the knowledge cutoff for Gemini 2.0 (August 2024). If you'd like to use multiple ACI.dev tools, you can use the [ ACIToolset ](../api/ext/index.html#pydantic_ai.ext.aci.ACIToolset) [toolset](../toolsets/index.html) which takes a list of ACI tool names as well as the linked_account_owner_id :", "url": "https://ai.pydantic.dev/third-party-tools/index.html#aci-tools", "page": "third-party-tools/index.html", "source_site": "pydantic_ai"}
{"title": "See Also", "anchor": "see-also", "heading_level": 2, "md_text": "* [Function Tools](../tools/index.html) - Basic tool concepts and registration * [Toolsets](../toolsets/index.html) - Managing collections of tools * [MCP Client](../mcp/client/index.html) - Using MCP servers with Pydantic AI * [LangChain Toolsets](../toolsets/index.html#langchain-tools) - Using LangChain toolsets * [ACI.dev Toolsets](../toolsets/index.html#aci-tools) - Using ACI.dev toolsets", "url": "https://ai.pydantic.dev/third-party-tools/index.html#see-also", "page": "third-party-tools/index.html", "source_site": "pydantic_ai"}
{"title": "Unit testing", "anchor": "unit-testing", "heading_level": 1, "md_text": "Writing unit tests for Pydantic AI code is just like unit tests for any other Python code. Because for the most part they're nothing new, we have pretty well established tools and patterns for writing and running these kinds of tests. Unless you're really sure you know better, you'll probably want to follow roughly this strategy: * Use [ pytest ](https://docs.pytest.org/en/stable/) as your test harness * If you find yourself typing out long assertions, use [inline-snapshot](https://15r10nk.github.io/inline-snapshot/latest/) * Similarly, [dirty-equals](https://dirty-equals.helpmanual.io/latest/) can be useful for comparing large data structures * Use [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) or [ FunctionModel ](../api/models/function/index.html#pydantic_ai.models.function.FunctionModel) in place of your actual model to avoid the usage, latency and variability of real LLM calls * Use [ Agent.override ](../api/agent/index.html#pydantic_ai.agent.Agent.override) to replace an agent's model, dependencies, or toolsets inside your application logic * Set [ ALLOW_MODEL_REQUESTS=False ](../api/models/base/index.html#pydantic_ai.models.ALLOW_MODEL_REQUESTS) globally to block any requests from being made to non-test models accidentally ### Unit testing with TestModel The simplest and fastest way to exercise most of your application code is using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel), this will (by default) call all tools in the agent, then return either plain text or a structured response depending on the return type of the agent. TestModel is not magic The \"clever\" (but not too clever) part of TestModel is that it will attempt to generate valid structured data for [function tools](../tools/index.html) and [output types](../output/index.html#structured-output) based on the schema of the registered tools. There's no ML or AI in TestModel , it's just plain old procedural Python code that tries to generate data that satisfies the JSON schema of a tool. The resulting data won't look pretty or relevant, but it should pass Pydantic's validation in most cases. If you want something more sophisticated, use [ FunctionModel ](../api/models/function/index.html#pydantic_ai.models.function.FunctionModel) and write your own data generation logic. Let's write unit tests for the following application code: weather\\_app.py 1. DatabaseConn is a class that holds a database connection 2. WeatherService has methods to get weather forecasts and historic data about the weather 3. We need to call a different endpoint depending on whether the date is in the past or the future, you'll see why this nuance is important below 4. This function is the code we want to test, together with the agent it uses Here we have a function that takes a list of (user_prompt, user_id) tuples, gets a weather forecast for each prompt, and stores the result in the database. **We want to test this code without having to mock certain objects or modify our code so we can pass test objects in.** Here's how we would write tests using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel): test\\_weather\\_app.py 1. We're using [anyio](https://anyio.readthedocs.io/en/stable/) to run async tests. 2. This is a safety measure to make sure we don't accidentally make real requests to the LLM while testing, see [ ALLOW_MODEL_REQUESTS ](../api/models/base/index.html#pydantic_ai.models.ALLOW_MODEL_REQUESTS) for more details. 3. We're using [ Agent.override ](../api/agent/index.html#pydantic_ai.agent.Agent.override) to replace the agent's model with [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel), the nice thing about override is that we can replace the model inside agent without needing access to the agent run* methods call site. 4. Now we call the function we want to test inside the override context manager. 5. But default, TestModel will return a JSON string summarising the tools calls made, and what was returned. If you wanted to customise the response to something more closely aligned with the domain, you could add [ custom_output_text='Sunny' ](../api/models/test/index.html#pydantic_ai.models.test.TestModel.custom_output_text) when defining TestModel . 6. So far we don't actually know which tools were called and with which values, we can use [ capture_run_messages ](../api/agent/index.html#pydantic_ai.agent.capture_run_messages) to inspect messages from the most recent run and assert the exchange between the agent and the model occurred as expected. 7. The [ IsNow ](https://dirty-equals.helpmanual.io/latest/types/datetime/#dirty_equals.IsNow) helper allows us to use declarative asserts even with data which will contain timestamps that change over time. 8. TestModel isn't doing anything clever to extract values from the prompt, so these values are hardcoded. ### Unit testing with FunctionModel The above tests are a great start, but careful readers will notice that the WeatherService.get_forecast is never called since TestModel calls weather_forecast with a date in the past. To fully exercise weather_forecast , we need to use [ FunctionModel ](../api/models/function/index.html#pydantic_ai.models.function.FunctionModel) to customise how the tools is called. Here's an example of using FunctionModel to test the weather_forecast tool with custom inputs test\\_weather\\_app2.py 1. We define a function call_weather_forecast that will be called by FunctionModel in place of the LLM, this function has access to the list of [ ModelMessage ](../api/messages/index.html#pydantic_ai.messages.ModelMessage)s that make up the run, and [ AgentInfo ](../api/models/function/index.html#pydantic_ai.models.function.AgentInfo) which contains information about the agent and the function tools and return tools. 2. Our function is slightly intelligent in that it tries to extract a date from the prompt, but just hard codes the location. 3. We use [ FunctionModel ](../api/models/function/index.html#pydantic_ai.models.function.FunctionModel) to replace the agent's model with our custom function. ### Overriding model via pytest fixtures If you're writing lots of tests that all require model to be overridden, you can use [pytest fixtures](https://docs.pytest.org/en/6.2.x/fixture.html) to override the model with [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) or [ FunctionModel ](../api/models/function/index.html#pydantic_ai.models.function.FunctionModel) in a reusable way. Here's an example of a fixture that overrides the model with TestModel : test\\_agent.py", "url": "https://ai.pydantic.dev/testing/index.html#unit-testing", "page": "testing/index.html", "source_site": "pydantic_ai"}
{"title": "Unit testing with TestModel", "anchor": "unit-testing-with-testmodel", "heading_level": 3, "md_text": "The simplest and fastest way to exercise most of your application code is using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel), this will (by default) call all tools in the agent, then return either plain text or a structured response depending on the return type of the agent. TestModel is not magic The \"clever\" (but not too clever) part of TestModel is that it will attempt to generate valid structured data for [function tools](../tools/index.html) and [output types](../output/index.html#structured-output) based on the schema of the registered tools. There's no ML or AI in TestModel , it's just plain old procedural Python code that tries to generate data that satisfies the JSON schema of a tool. The resulting data won't look pretty or relevant, but it should pass Pydantic's validation in most cases. If you want something more sophisticated, use [ FunctionModel ](../api/models/function/index.html#pydantic_ai.models.function.FunctionModel) and write your own data generation logic. Let's write unit tests for the following application code: weather\\_app.py 1. DatabaseConn is a class that holds a database connection 2. WeatherService has methods to get weather forecasts and historic data about the weather 3. We need to call a different endpoint depending on whether the date is in the past or the future, you'll see why this nuance is important below 4. This function is the code we want to test, together with the agent it uses Here we have a function that takes a list of (user_prompt, user_id) tuples, gets a weather forecast for each prompt, and stores the result in the database. **We want to test this code without having to mock certain objects or modify our code so we can pass test objects in.** Here's how we would write tests using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel): test\\_weather\\_app.py 1. We're using [anyio](https://anyio.readthedocs.io/en/stable/) to run async tests. 2. This is a safety measure to make sure we don't accidentally make real requests to the LLM while testing, see [ ALLOW_MODEL_REQUESTS ](../api/models/base/index.html#pydantic_ai.models.ALLOW_MODEL_REQUESTS) for more details. 3. We're using [ Agent.override ](../api/agent/index.html#pydantic_ai.agent.Agent.override) to replace the agent's model with [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel), the nice thing about override is that we can replace the model inside agent without needing access to the agent run* methods call site. 4. Now we call the function we want to test inside the override context manager. 5. But default, TestModel will return a JSON string summarising the tools calls made, and what was returned. If you wanted to customise the response to something more closely aligned with the domain, you could add [ custom_output_text='Sunny' ](../api/models/test/index.html#pydantic_ai.models.test.TestModel.custom_output_text) when defining TestModel . 6. So far we don't actually know which tools were called and with which values, we can use [ capture_run_messages ](../api/agent/index.html#pydantic_ai.agent.capture_run_messages) to inspect messages from the most recent run and assert the exchange between the agent and the model occurred as expected. 7. The [ IsNow ](https://dirty-equals.helpmanual.io/latest/types/datetime/#dirty_equals.IsNow) helper allows us to use declarative asserts even with data which will contain timestamps that change over time. 8. TestModel isn't doing anything clever to extract values from the prompt, so these values are hardcoded.", "url": "https://ai.pydantic.dev/testing/index.html#unit-testing-with-testmodel", "page": "testing/index.html", "source_site": "pydantic_ai"}
{"title": "Unit testing with FunctionModel", "anchor": "unit-testing-with-functionmodel", "heading_level": 3, "md_text": "The above tests are a great start, but careful readers will notice that the WeatherService.get_forecast is never called since TestModel calls weather_forecast with a date in the past. To fully exercise weather_forecast , we need to use [ FunctionModel ](../api/models/function/index.html#pydantic_ai.models.function.FunctionModel) to customise how the tools is called. Here's an example of using FunctionModel to test the weather_forecast tool with custom inputs test\\_weather\\_app2.py 1. We define a function call_weather_forecast that will be called by FunctionModel in place of the LLM, this function has access to the list of [ ModelMessage ](../api/messages/index.html#pydantic_ai.messages.ModelMessage)s that make up the run, and [ AgentInfo ](../api/models/function/index.html#pydantic_ai.models.function.AgentInfo) which contains information about the agent and the function tools and return tools. 2. Our function is slightly intelligent in that it tries to extract a date from the prompt, but just hard codes the location. 3. We use [ FunctionModel ](../api/models/function/index.html#pydantic_ai.models.function.FunctionModel) to replace the agent's model with our custom function.", "url": "https://ai.pydantic.dev/testing/index.html#unit-testing-with-functionmodel", "page": "testing/index.html", "source_site": "pydantic_ai"}
{"title": "Overriding model via pytest fixtures", "anchor": "overriding-model-via-pytest-fixtures", "heading_level": 3, "md_text": "If you're writing lots of tests that all require model to be overridden, you can use [pytest fixtures](https://docs.pytest.org/en/6.2.x/fixture.html) to override the model with [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) or [ FunctionModel ](../api/models/function/index.html#pydantic_ai.models.function.FunctionModel) in a reusable way. Here's an example of a fixture that overrides the model with TestModel : test\\_agent.py", "url": "https://ai.pydantic.dev/testing/index.html#overriding-model-via-pytest-fixtures", "page": "testing/index.html", "source_site": "pydantic_ai"}
{"title": "Troubleshooting", "anchor": "troubleshooting", "heading_level": 1, "md_text": "Below are suggestions on how to fix some common errors you might encounter while using Pydantic AI. If the issue you're experiencing is not listed below or addressed in the documentation, please feel free to ask in the [Pydantic Slack](../help/index.html) or create an issue on [GitHub](https://github.com/pydantic/pydantic-ai/issues). ## Jupyter Notebook Errors ### RuntimeError: This event loop is already running This error is caused by conflicts between the event loops in Jupyter notebook and Pydantic AI's. One way to manage these conflicts is by using [ nest-asyncio ](https://pypi.org/project/nest-asyncio/). Namely, before you execute any agent runs, do the following: Note: This fix also applies to Google Colab and [Marimo](https://github.com/marimo-team/marimo). ## API Key Configuration ### UserError: API key must be provided or set in the [MODEL]_API_KEY environment variable If you're running into issues with setting the API key for your model, visit the [Models](../models/overview/index.html) page to learn more about how to set an environment variable and/or pass in an api_key argument. ## Monitoring HTTPX Requests You can use custom httpx clients in your models in order to access specific requests, responses, and headers at runtime. It's particularly helpful to use logfire 's [HTTPX integration](../logfire/index.html#monitoring-http-requests) to monitor the above.", "url": "https://ai.pydantic.dev/troubleshooting/index.html#troubleshooting", "page": "troubleshooting/index.html", "source_site": "pydantic_ai"}
{"title": "Jupyter Notebook Errors", "anchor": "jupyter-notebook-errors", "heading_level": 2, "md_text": "### RuntimeError: This event loop is already running This error is caused by conflicts between the event loops in Jupyter notebook and Pydantic AI's. One way to manage these conflicts is by using [ nest-asyncio ](https://pypi.org/project/nest-asyncio/). Namely, before you execute any agent runs, do the following: Note: This fix also applies to Google Colab and [Marimo](https://github.com/marimo-team/marimo).", "url": "https://ai.pydantic.dev/troubleshooting/index.html#jupyter-notebook-errors", "page": "troubleshooting/index.html", "source_site": "pydantic_ai"}
{"title": "RuntimeError: This event loop is already running", "anchor": "runtimeerror-this-event-loop-is-already-running", "heading_level": 3, "md_text": "This error is caused by conflicts between the event loops in Jupyter notebook and Pydantic AI's. One way to manage these conflicts is by using [ nest-asyncio ](https://pypi.org/project/nest-asyncio/). Namely, before you execute any agent runs, do the following: Note: This fix also applies to Google Colab and [Marimo](https://github.com/marimo-team/marimo).", "url": "https://ai.pydantic.dev/troubleshooting/index.html#runtimeerror-this-event-loop-is-already-running", "page": "troubleshooting/index.html", "source_site": "pydantic_ai"}
{"title": "API Key Configuration", "anchor": "api-key-configuration", "heading_level": 2, "md_text": "### UserError: API key must be provided or set in the [MODEL]_API_KEY environment variable If you're running into issues with setting the API key for your model, visit the [Models](../models/overview/index.html) page to learn more about how to set an environment variable and/or pass in an api_key argument.", "url": "https://ai.pydantic.dev/troubleshooting/index.html#api-key-configuration", "page": "troubleshooting/index.html", "source_site": "pydantic_ai"}
{"title": "UserError: API key must be provided or set in the [MODEL]_API_KEY environment variable", "anchor": "usererror-api-key-must-be-provided-or-set-in-the-model_api_key-environment-variable", "heading_level": 3, "md_text": "If you're running into issues with setting the API key for your model, visit the [Models](../models/overview/index.html) page to learn more about how to set an environment variable and/or pass in an api_key argument.", "url": "https://ai.pydantic.dev/troubleshooting/index.html#usererror-api-key-must-be-provided-or-set-in-the-model_api_key-environment-variable", "page": "troubleshooting/index.html", "source_site": "pydantic_ai"}
{"title": "Monitoring HTTPX Requests", "anchor": "monitoring-httpx-requests", "heading_level": 2, "md_text": "You can use custom httpx clients in your models in order to access specific requests, responses, and headers at runtime. It's particularly helpful to use logfire 's [HTTPX integration](../logfire/index.html#monitoring-http-requests) to monitor the above.", "url": "https://ai.pydantic.dev/troubleshooting/index.html#monitoring-httpx-requests", "page": "troubleshooting/index.html", "source_site": "pydantic_ai"}
{"title": "Function Tools", "anchor": "function-tools", "heading_level": 1, "md_text": "Function tools provide a mechanism for models to perform actions and retrieve extra information to help them generate a response. They're useful when you want to enable the model to take some action and use the result, when it is impractical or impossible to put all the context an agent might need into the instructions, or when you want to make agents' behavior more deterministic or reliable by deferring some of the logic required to generate a response to another (not necessarily AI-powered) tool. If you want a model to be able to call a function as its final action, without the result being sent back to the model, you can use an [output function](../output/index.html#output-functions) instead. There are a number of ways to register tools with an agent: * via the [ @agent.tool ](../api/agent/index.html#pydantic_ai.agent.Agent.tool) decorator  for tools that need access to the agent [context](../api/tools/index.html#pydantic_ai.tools.RunContext) * via the [ @agent.tool_plain ](../api/agent/index.html#pydantic_ai.agent.Agent.tool_plain) decorator  for tools that do not need access to the agent [context](../api/tools/index.html#pydantic_ai.tools.RunContext) * via the [ tools ](../api/agent/index.html#pydantic_ai.agent.Agent.__init__) keyword argument to Agent which can take either plain functions, or instances of [ Tool ](../api/tools/index.html#pydantic_ai.tools.Tool) For more advanced use cases, the [toolsets](../toolsets/index.html) feature lets you manage collections of tools (built by you or provided by an [MCP server](../mcp/client/index.html) or other [third party](../third-party-tools/index.html#third-party-tools)) and register them with an agent in one go via the [ toolsets ](../api/agent/index.html#pydantic_ai.agent.Agent.__init__) keyword argument to Agent . Internally, all tools and toolsets are gathered into a single [combined toolset](../toolsets/index.html#combining-toolsets) that's made available to the model. Function tools vs. RAG Function tools are basically the \"R\" of RAG (Retrieval-Augmented Generation)  they augment what the model can do by letting it request extra information. The main semantic difference between Pydantic AI Tools and RAG is RAG is synonymous with vector search, while Pydantic AI tools are more general-purpose. (Note: we may add support for vector search functionality in the future, particularly an API for generating embeddings. See [#58](https://github.com/pydantic/pydantic-ai/issues/58)) Function Tools vs. Structured Outputs As the name suggests, function tools use the model's \"tools\" or \"functions\" API to let the model know what is available to call. Tools or functions are also used to define the schema(s) for [structured output](../output/index.html) when using the default [tool output mode](../output/index.html#tool-output), thus a model might have access to many tools, some of which call function tools while others end the run and produce a final output. ## Registering via Decorator @agent.tool is considered the default decorator since in the majority of cases tools will need access to the agent [context](../api/tools/index.html#pydantic_ai.tools.RunContext). Here's an example using both: dice\\_game.py 1. This is a pretty simple task, so we can use the fast and cheap Gemini flash model. 2. We pass the user's name as the dependency, to keep things simple we use just the name as a string as the dependency. 3. This tool doesn't need any context, it just returns a random number. You could probably use dynamic instructions in this case. 4. This tool needs the player's name, so it uses RunContext to access dependencies which are just the player's name in this case. 5. Run the agent, passing the player's name as the dependency. *(This example is complete, it can be run \"as is\")* Let's print the messages from that game to see what happened: dice\\_game\\_messages.py We can represent this with a diagram: ## Registering via Agent Argument As well as using the decorators, we can register tools via the tools argument to the [ Agent constructor](../api/agent/index.html#pydantic_ai.agent.Agent.__init__). This is useful when you want to reuse tools, and can also give more fine-grained control over the tools. dice\\_game\\_tool\\_kwarg.py 1. The simplest way to register tools via the Agent constructor is to pass a list of functions, the function signature is inspected to determine if the tool takes [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext). 2. agent_a and agent_b are identical  but we can use [ Tool ](../api/tools/index.html#pydantic_ai.tools.Tool) to reuse tool definitions and give more fine-grained control over how tools are defined, e.g. setting their name or description, or using a custom [ prepare ](../tools-advanced/index.html#tool-prepare) method. *(This example is complete, it can be run \"as is\")* ## Tool Output Tools can return anything that Pydantic can serialize to JSON. For advanced output options including multi-modal content and metadata, see [Advanced Tool Features](../tools-advanced/index.html#function-tool-output). ## Tool Schema Function parameters are extracted from the function signature, and all parameters except RunContext are used to build the schema for that tool call. Even better, Pydantic AI extracts the docstring from functions and (thanks to [griffe](https://mkdocstrings.github.io/griffe/)) extracts parameter descriptions from the docstring and adds them to the schema. [Griffe supports](https://mkdocstrings.github.io/griffe/reference/docstrings/#docstrings) extracting parameter descriptions from google , numpy , and sphinx style docstrings. Pydantic AI will infer the format to use based on the docstring, but you can explicitly set it using [ docstring_format ](../api/tools/index.html#pydantic_ai.tools.DocstringFormat). You can also enforce parameter requirements by setting require_parameter_descriptions=True . This will raise a [ UserError ](../api/exceptions/index.html#pydantic_ai.exceptions.UserError) if a parameter description is missing. To demonstrate a tool's schema, here we use [ FunctionModel ](../api/models/function/index.html#pydantic_ai.models.function.FunctionModel) to print the schema a model would receive: tool\\_schema.py *(This example is complete, it can be run \"as is\")* If a tool has a single parameter that can be represented as an object in JSON schema (e.g. dataclass, TypedDict, pydantic model), the schema for the tool is simplified to be just that object. Here's an example where we use [ TestModel.last_model_request_parameters ](../api/models/test/index.html#pydantic_ai.models.test.TestModel.last_model_request_parameters) to inspect the tool schema that would be passed to the model. single\\_parameter\\_tool.py *(This example is complete, it can be run \"as is\")* ## See Also For more tool features and integrations, see: * [Advanced Tool Features](../tools-advanced/index.html) - Custom schemas, dynamic tools, tool execution and retries * [Toolsets](../toolsets/index.html) - Managing collections of tools * [Builtin Tools](../builtin-tools/index.html) - Native tools provided by LLM providers * [Common Tools](../common-tools/index.html) - Ready-to-use tool implementations * [Third-Party Tools](../third-party-tools/index.html) - Integrations with MCP, LangChain, ACI.dev and other tool libraries * [Deferred Tools](../deferred-tools/index.html) - Tools requiring approval or external execution", "url": "https://ai.pydantic.dev/tools/index.html#function-tools", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "Registering via Decorator", "anchor": "registering-function-tools-via-decorator", "heading_level": 2, "md_text": "@agent.tool is considered the default decorator since in the majority of cases tools will need access to the agent [context](../api/tools/index.html#pydantic_ai.tools.RunContext). Here's an example using both: dice\\_game.py 1. This is a pretty simple task, so we can use the fast and cheap Gemini flash model. 2. We pass the user's name as the dependency, to keep things simple we use just the name as a string as the dependency. 3. This tool doesn't need any context, it just returns a random number. You could probably use dynamic instructions in this case. 4. This tool needs the player's name, so it uses RunContext to access dependencies which are just the player's name in this case. 5. Run the agent, passing the player's name as the dependency. *(This example is complete, it can be run \"as is\")* Let's print the messages from that game to see what happened: dice\\_game\\_messages.py We can represent this with a diagram:", "url": "https://ai.pydantic.dev/tools/index.html#registering-function-tools-via-decorator", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "Registering via Agent Argument", "anchor": "registering-function-tools-via-agent-argument", "heading_level": 2, "md_text": "As well as using the decorators, we can register tools via the tools argument to the [ Agent constructor](../api/agent/index.html#pydantic_ai.agent.Agent.__init__). This is useful when you want to reuse tools, and can also give more fine-grained control over the tools. dice\\_game\\_tool\\_kwarg.py 1. The simplest way to register tools via the Agent constructor is to pass a list of functions, the function signature is inspected to determine if the tool takes [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext). 2. agent_a and agent_b are identical  but we can use [ Tool ](../api/tools/index.html#pydantic_ai.tools.Tool) to reuse tool definitions and give more fine-grained control over how tools are defined, e.g. setting their name or description, or using a custom [ prepare ](../tools-advanced/index.html#tool-prepare) method. *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/tools/index.html#registering-function-tools-via-agent-argument", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "Tool Output", "anchor": "function-tool-output", "heading_level": 2, "md_text": "Tools can return anything that Pydantic can serialize to JSON. For advanced output options including multi-modal content and metadata, see [Advanced Tool Features](../tools-advanced/index.html#function-tool-output).", "url": "https://ai.pydantic.dev/tools/index.html#function-tool-output", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "Tool Schema", "anchor": "function-tools-and-schema", "heading_level": 2, "md_text": "Function parameters are extracted from the function signature, and all parameters except RunContext are used to build the schema for that tool call. Even better, Pydantic AI extracts the docstring from functions and (thanks to [griffe](https://mkdocstrings.github.io/griffe/)) extracts parameter descriptions from the docstring and adds them to the schema. [Griffe supports](https://mkdocstrings.github.io/griffe/reference/docstrings/#docstrings) extracting parameter descriptions from google , numpy , and sphinx style docstrings. Pydantic AI will infer the format to use based on the docstring, but you can explicitly set it using [ docstring_format ](../api/tools/index.html#pydantic_ai.tools.DocstringFormat). You can also enforce parameter requirements by setting require_parameter_descriptions=True . This will raise a [ UserError ](../api/exceptions/index.html#pydantic_ai.exceptions.UserError) if a parameter description is missing. To demonstrate a tool's schema, here we use [ FunctionModel ](../api/models/function/index.html#pydantic_ai.models.function.FunctionModel) to print the schema a model would receive: tool\\_schema.py *(This example is complete, it can be run \"as is\")* If a tool has a single parameter that can be represented as an object in JSON schema (e.g. dataclass, TypedDict, pydantic model), the schema for the tool is simplified to be just that object. Here's an example where we use [ TestModel.last_model_request_parameters ](../api/models/test/index.html#pydantic_ai.models.test.TestModel.last_model_request_parameters) to inspect the tool schema that would be passed to the model. single\\_parameter\\_tool.py *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/tools/index.html#function-tools-and-schema", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "See Also", "anchor": "see-also", "heading_level": 2, "md_text": "For more tool features and integrations, see: * [Advanced Tool Features](../tools-advanced/index.html) - Custom schemas, dynamic tools, tool execution and retries * [Toolsets](../toolsets/index.html) - Managing collections of tools * [Builtin Tools](../builtin-tools/index.html) - Native tools provided by LLM providers * [Common Tools](../common-tools/index.html) - Ready-to-use tool implementations * [Third-Party Tools](../third-party-tools/index.html) - Integrations with MCP, LangChain, ACI.dev and other tool libraries * [Deferred Tools](../deferred-tools/index.html) - Tools requiring approval or external execution", "url": "https://ai.pydantic.dev/tools/index.html#see-also", "page": "tools/index.html", "source_site": "pydantic_ai"}
{"title": "Structured output data", "anchor": "structured-output", "heading_level": 2, "md_text": "The [ Agent ](../api/agent/index.html#pydantic_ai.agent.Agent) class constructor takes an output_type argument that takes one or more types or [output functions](index.html#output-functions). It supports simple scalar types, list and dict types (including TypedDict s and [ StructuredDict s](index.html#structured-dict)), dataclasses and Pydantic models, as well as type unions -- generally everything supported as type hints in a Pydantic model. You can also pass a list of multiple choices. By default, Pydantic AI leverages the model's tool calling capability to make it return structured data. When multiple output types are specified (in a union or list), each member is registered with the model as a separate output tool in order to reduce the complexity of the schema and maximise the chances a model will respond correctly. This has been shown to work well across a wide range of models. If you'd like to change the names of the output tools, use a model's native structured output feature, or pass the output schema to the model in its [instructions](../agents/index.html#instructions), you can use an [output mode](index.html#output-modes) marker class. When no output type is specified, or when str is among the output types, any plain text response from the model will be used as the output data. If str is not among the output types, the model is forced to return structured data or call an output function. If the output type schema is not of type \"object\" (e.g. it's int or list[int] ), the output type is wrapped in a single element object, so the schema of all tools registered with the model are object schemas. Structured outputs (like tools) use Pydantic to build the JSON schema used for the tool, and to validate the data returned by the model. Type checking considerations The Agent class is generic in its output type, and this type is carried through to AgentRunResult.output and StreamedRunResult.output so that your IDE or static type checker can warn you when your code doesn't properly take into account all the possible values those outputs could have. Static type checkers like pyright and mypy will do their best to infer the agent's output type from the output_type you've specified, but they're not always able to do so correctly when you provide functions or multiple types in a union or list, even though Pydantic AI will behave correctly. When this happens, your type checker will complain even when you're confident you've passed a valid output_type , and you'll need to help the type checker by explicitly specifying the generic parameters on the Agent constructor. This is shown in the second example below and the output functions example further down. Specifically, there are three valid uses of output_type where you'll need to do this: 1. When using a union of types, e.g. output_type=Foo Bar . Until [PEP-747](https://peps.python.org/pep-0747/) \"Annotating Type Forms\" lands in Python 3.15, type checkers do not consider these a valid value for output_type . In addition to the generic parameters on the Agent constructor, you'll need to add # type: ignore to the line that passes the union to output_type . Alternatively, you can use a list: output_type=[Foo, Bar] . 2. With mypy: When using a list, as a functionally equivalent alternative to a union, or because you're passing in [output functions](index.html#output-functions). Pyright does handle this correctly, and we've filed [an issue](https://github.com/python/mypy/issues/19142) with mypy to try and get this fixed. 3. With mypy: when using an async output function. Pyright does handle this correctly, and we've filed [an issue](https://github.com/python/mypy/issues/19143) with mypy to try and get this fixed. Here's an example of returning either text or structured data: box\\_or\\_error.py 1. This could also have been a union: output_type=Box str . However, as explained in the \"Type checking considerations\" section above, that would've required explicitly specifying the generic parameters on the Agent constructor and adding # type: ignore to this line in order to be type checked correctly. *(This example is complete, it can be run \"as is\")* Here's an example of using a union return type, which will register multiple output tools and wrap non-object schemas in an object: colors\\_or\\_sizes.py 1. As explained in the \"Type checking considerations\" section above, using a union rather than a list requires explicitly specifying the generic parameters on the Agent constructor and adding # type: ignore to this line in order to be type checked correctly. *(This example is complete, it can be run \"as is\")* ### Output functions Instead of plain text or structured data, you may want the output of your agent run to be the result of a function called with arguments provided by the model, for example to further process or validate the data provided through the arguments (with the option to tell the model to try again), or to hand off to another agent. Output functions are similar to [function tools](../tools/index.html), but the model is forced to call one of them, the call ends the agent run, and the result is not passed back to the model. As with tool functions, output function arguments provided by the model are validated using Pydantic, they can optionally take [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext) as the first argument, and they can raise [ ModelRetry ](../api/exceptions/index.html#pydantic_ai.exceptions.ModelRetry) to ask the model to try again with modified arguments (or with a different output type). To specify output functions, you set the agent's output_type to either a single function (or bound instance method), or a list of functions. The list can also contain other output types like simple scalars or entire Pydantic models. You typically do not want to also register your output function as a tool (using the @agent.tool decorator or tools argument), as this could confuse the model about which it should be calling. Here's an example of all of these features in action: output\\_functions.py #### Text output If you provide an output function that takes a string, Pydantic AI will by default create an output tool like for any other output function. If instead you'd like the model to provide the string using plain text output, you can wrap the function in the [ TextOutput ](../api/output/index.html#pydantic_ai.output.TextOutput) marker class. If desired, this marker class can be used alongside one or more [ ToolOutput ](index.html#tool-output) marker classes (or unmarked types or functions) in a list provided to output_type . text\\_output\\_function.py *(This example is complete, it can be run \"as is\")* ### Output modes Pydantic AI implements three different methods to get a model to output structured data: 1. [Tool Output](index.html#tool-output), where tool calls are used to produce the output. 2. [Native Output](index.html#native-output), where the model is required to produce text content compliant with a provided JSON schema. 3. [Prompted Output](index.html#prompted-output), where a prompt is injected into the model instructions including the desired JSON schema, and we attempt to parse the model's plain-text response as appropriate. #### Tool Output In the default Tool Output mode, the output JSON schema of each output type (or function) is provided to the model as the parameters schema of a special output tool. This is the default as it's supported by virtually all models and has been shown to work very well. If you'd like to change the name of the output tool, pass a custom description to aid the model, or turn on or off strict mode, you can wrap the type(s) in the [ ToolOutput ](../api/output/index.html#pydantic_ai.output.ToolOutput) marker class and provide the appropriate arguments. Note that by default, the description is taken from the docstring specified on a Pydantic model or output function, so specifying it using the marker class is typically not necessary. To dynamically modify or filter the available output tools during an agent run, you can define an agent-wide prepare_output_tools function that will be called ahead of each step of a run. This function should be of type [ ToolsPrepareFunc ](../api/tools/index.html#pydantic_ai.tools.ToolsPrepareFunc), which takes the [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext) and a list of [ ToolDefinition ](../api/tools/index.html#pydantic_ai.tools.ToolDefinition), and returns a new list of tool definitions (or None to disable all tools for that step). This is analogous to the [ prepare_tools function](../tools-advanced/index.html#prepare-tools) for non-output tools. tool\\_output.py 1. If we were passing just Fruit and Vehicle without custom tool names, we could have used a union: output_type=Fruit Vehicle . However, as ToolOutput is an object rather than a type, we have to use a list. *(This example is complete, it can be run \"as is\")* #### Native Output Native Output mode uses a model's native \"Structured Outputs\" feature (aka \"JSON Schema response format\"), where the model is forced to only output text matching the provided JSON schema. Note that this is not supported by all models, and sometimes comes with restrictions. For example, Anthropic does not support this at all, and Gemini cannot use tools at the same time as structured output, and attempting to do so will result in an error. To use this mode, you can wrap the output type(s) in the [ NativeOutput ](../api/output/index.html#pydantic_ai.output.NativeOutput) marker class that also lets you specify a name and description if the name and docstring of the type or function are not sufficient. native\\_output.py 1. This could also have been a union: output_type=Fruit Vehicle . However, as explained in the \"Type checking considerations\" section above, that would've required explicitly specifying the generic parameters on the Agent constructor and adding # type: ignore to this line in order to be type checked correctly. *(This example is complete, it can be run \"as is\")* #### Prompted Output In this mode, the model is prompted to output text matching the provided JSON schema through its [instructions](../agents/index.html#instructions) and it's up to the model to interpret those instructions correctly. This is usable with all models, but is often the least reliable approach as the model is not forced to match the schema. While we would generally suggest starting with tool or native output, in some cases this mode may result in higher quality outputs, and for models without native tool calling or structured output support it is the only option for producing structured outputs. If the model API supports the \"JSON Mode\" feature (aka \"JSON Object response format\") to force the model to output valid JSON, this is enabled, but it's still up to the model to abide by the schema. Pydantic AI will validate the returned structured data and tell the model to try again if validation fails, but if the model is not intelligent enough this may not be sufficient. To use this mode, you can wrap the output type(s) in the [ PromptedOutput ](../api/output/index.html#pydantic_ai.output.PromptedOutput) marker class that also lets you specify a name and description if the name and docstring of the type or function are not sufficient. Additionally, it supports an template argument lets you specify a custom instructions template to be used instead of the [default](../api/profiles/index.html#pydantic_ai.profiles.ModelProfile.prompted_output_template). prompted\\_output.py 1. This could also have been a union: output_type=Vehicle Device . However, as explained in the \"Type checking considerations\" section above, that would've required explicitly specifying the generic parameters on the Agent constructor and adding # type: ignore to this line in order to be type checked correctly. *(This example is complete, it can be run \"as is\")* ### Custom JSON schema If it's not feasible to define your desired structured output object using a Pydantic BaseModel , dataclass, or TypedDict , for example when you get a JSON schema from an external source or generate it dynamically, you can use the [ StructuredDict() ](../api/output/index.html#pydantic_ai.output.StructuredDict) helper function to generate a dict[str, Any] subclass with a JSON schema attached that Pydantic AI will pass to the model. Note that Pydantic AI will not perform any validation of the received JSON object and it's up to the model to correctly interpret the schema and any constraints expressed in it, like required fields or integer value ranges. The output type will be a dict[str, Any] and it's up to your code to defensively read from it in case the model made a mistake. You can use an [output validator](index.html#output-validator-functions) to reflect validation errors back to the model and get it to try again. Along with the JSON schema, you can optionally pass name and description arguments to provide additional context to the model: ### Output validators Some validation is inconvenient or impossible to do in Pydantic validators, in particular when the validation requires IO and is asynchronous. Pydantic AI provides a way to add validation functions via the [ agent.output_validator ](../api/agent/index.html#pydantic_ai.agent.Agent.output_validator) decorator. If you want to implement separate validation logic for different output types, it's recommended to use [output functions](index.html#output-functions) instead, to save you from having to do isinstance checks inside the output validator. If you want the model to output plain text, do your own processing or validation, and then have the agent's final output be the result of your function, it's recommended to use an [output function](index.html#output-functions) with the [ TextOutput marker class](index.html#text-output). Here's a simplified variant of the [SQL Generation example](../examples/sql-gen/index.html): sql\\_gen.py *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/output/index.html#structured-output", "page": "output/index.html", "source_site": "pydantic_ai"}
{"title": "Output functions", "anchor": "output-functions", "heading_level": 3, "md_text": "Instead of plain text or structured data, you may want the output of your agent run to be the result of a function called with arguments provided by the model, for example to further process or validate the data provided through the arguments (with the option to tell the model to try again), or to hand off to another agent. Output functions are similar to [function tools](../tools/index.html), but the model is forced to call one of them, the call ends the agent run, and the result is not passed back to the model. As with tool functions, output function arguments provided by the model are validated using Pydantic, they can optionally take [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext) as the first argument, and they can raise [ ModelRetry ](../api/exceptions/index.html#pydantic_ai.exceptions.ModelRetry) to ask the model to try again with modified arguments (or with a different output type). To specify output functions, you set the agent's output_type to either a single function (or bound instance method), or a list of functions. The list can also contain other output types like simple scalars or entire Pydantic models. You typically do not want to also register your output function as a tool (using the @agent.tool decorator or tools argument), as this could confuse the model about which it should be calling. Here's an example of all of these features in action: output\\_functions.py #### Text output If you provide an output function that takes a string, Pydantic AI will by default create an output tool like for any other output function. If instead you'd like the model to provide the string using plain text output, you can wrap the function in the [ TextOutput ](../api/output/index.html#pydantic_ai.output.TextOutput) marker class. If desired, this marker class can be used alongside one or more [ ToolOutput ](index.html#tool-output) marker classes (or unmarked types or functions) in a list provided to output_type . text\\_output\\_function.py *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/output/index.html#output-functions", "page": "output/index.html", "source_site": "pydantic_ai"}
{"title": "Output modes", "anchor": "output-modes", "heading_level": 3, "md_text": "Pydantic AI implements three different methods to get a model to output structured data: 1. [Tool Output](index.html#tool-output), where tool calls are used to produce the output. 2. [Native Output](index.html#native-output), where the model is required to produce text content compliant with a provided JSON schema. 3. [Prompted Output](index.html#prompted-output), where a prompt is injected into the model instructions including the desired JSON schema, and we attempt to parse the model's plain-text response as appropriate. #### Tool Output In the default Tool Output mode, the output JSON schema of each output type (or function) is provided to the model as the parameters schema of a special output tool. This is the default as it's supported by virtually all models and has been shown to work very well. If you'd like to change the name of the output tool, pass a custom description to aid the model, or turn on or off strict mode, you can wrap the type(s) in the [ ToolOutput ](../api/output/index.html#pydantic_ai.output.ToolOutput) marker class and provide the appropriate arguments. Note that by default, the description is taken from the docstring specified on a Pydantic model or output function, so specifying it using the marker class is typically not necessary. To dynamically modify or filter the available output tools during an agent run, you can define an agent-wide prepare_output_tools function that will be called ahead of each step of a run. This function should be of type [ ToolsPrepareFunc ](../api/tools/index.html#pydantic_ai.tools.ToolsPrepareFunc), which takes the [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext) and a list of [ ToolDefinition ](../api/tools/index.html#pydantic_ai.tools.ToolDefinition), and returns a new list of tool definitions (or None to disable all tools for that step). This is analogous to the [ prepare_tools function](../tools-advanced/index.html#prepare-tools) for non-output tools. tool\\_output.py 1. If we were passing just Fruit and Vehicle without custom tool names, we could have used a union: output_type=Fruit Vehicle . However, as ToolOutput is an object rather than a type, we have to use a list. *(This example is complete, it can be run \"as is\")* #### Native Output Native Output mode uses a model's native \"Structured Outputs\" feature (aka \"JSON Schema response format\"), where the model is forced to only output text matching the provided JSON schema. Note that this is not supported by all models, and sometimes comes with restrictions. For example, Anthropic does not support this at all, and Gemini cannot use tools at the same time as structured output, and attempting to do so will result in an error. To use this mode, you can wrap the output type(s) in the [ NativeOutput ](../api/output/index.html#pydantic_ai.output.NativeOutput) marker class that also lets you specify a name and description if the name and docstring of the type or function are not sufficient. native\\_output.py 1. This could also have been a union: output_type=Fruit Vehicle . However, as explained in the \"Type checking considerations\" section above, that would've required explicitly specifying the generic parameters on the Agent constructor and adding # type: ignore to this line in order to be type checked correctly. *(This example is complete, it can be run \"as is\")* #### Prompted Output In this mode, the model is prompted to output text matching the provided JSON schema through its [instructions](../agents/index.html#instructions) and it's up to the model to interpret those instructions correctly. This is usable with all models, but is often the least reliable approach as the model is not forced to match the schema. While we would generally suggest starting with tool or native output, in some cases this mode may result in higher quality outputs, and for models without native tool calling or structured output support it is the only option for producing structured outputs. If the model API supports the \"JSON Mode\" feature (aka \"JSON Object response format\") to force the model to output valid JSON, this is enabled, but it's still up to the model to abide by the schema. Pydantic AI will validate the returned structured data and tell the model to try again if validation fails, but if the model is not intelligent enough this may not be sufficient. To use this mode, you can wrap the output type(s) in the [ PromptedOutput ](../api/output/index.html#pydantic_ai.output.PromptedOutput) marker class that also lets you specify a name and description if the name and docstring of the type or function are not sufficient. Additionally, it supports an template argument lets you specify a custom instructions template to be used instead of the [default](../api/profiles/index.html#pydantic_ai.profiles.ModelProfile.prompted_output_template). prompted\\_output.py 1. This could also have been a union: output_type=Vehicle Device . However, as explained in the \"Type checking considerations\" section above, that would've required explicitly specifying the generic parameters on the Agent constructor and adding # type: ignore to this line in order to be type checked correctly. *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/output/index.html#output-modes", "page": "output/index.html", "source_site": "pydantic_ai"}
{"title": "Custom JSON schema", "anchor": "structured-dict", "heading_level": 3, "md_text": "If it's not feasible to define your desired structured output object using a Pydantic BaseModel , dataclass, or TypedDict , for example when you get a JSON schema from an external source or generate it dynamically, you can use the [ StructuredDict() ](../api/output/index.html#pydantic_ai.output.StructuredDict) helper function to generate a dict[str, Any] subclass with a JSON schema attached that Pydantic AI will pass to the model. Note that Pydantic AI will not perform any validation of the received JSON object and it's up to the model to correctly interpret the schema and any constraints expressed in it, like required fields or integer value ranges. The output type will be a dict[str, Any] and it's up to your code to defensively read from it in case the model made a mistake. You can use an [output validator](index.html#output-validator-functions) to reflect validation errors back to the model and get it to try again. Along with the JSON schema, you can optionally pass name and description arguments to provide additional context to the model:", "url": "https://ai.pydantic.dev/output/index.html#structured-dict", "page": "output/index.html", "source_site": "pydantic_ai"}
{"title": "Output validators", "anchor": "output-validator-functions", "heading_level": 3, "md_text": "Some validation is inconvenient or impossible to do in Pydantic validators, in particular when the validation requires IO and is asynchronous. Pydantic AI provides a way to add validation functions via the [ agent.output_validator ](../api/agent/index.html#pydantic_ai.agent.Agent.output_validator) decorator. If you want to implement separate validation logic for different output types, it's recommended to use [output functions](index.html#output-functions) instead, to save you from having to do isinstance checks inside the output validator. If you want the model to output plain text, do your own processing or validation, and then have the agent's final output be the result of your function, it's recommended to use an [output function](index.html#output-functions) with the [ TextOutput marker class](index.html#text-output). Here's a simplified variant of the [SQL Generation example](../examples/sql-gen/index.html): sql\\_gen.py *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/output/index.html#output-validator-functions", "page": "output/index.html", "source_site": "pydantic_ai"}
{"title": "Image output", "anchor": "image-output", "heading_level": 2, "md_text": "Some models can generate images as part of their response, for example those that support the [Image Generation built-in tool](../builtin-tools/index.html#image-generation-tool) and OpenAI models using the [Code Execution built-in tool](../builtin-tools/index.html#code-execution-tool) when told to generate a chart. To use the generated image as the output of the agent run, you can set output_type to [ BinaryImage ](../api/messages/index.html#pydantic_ai.messages.BinaryImage). If no image-generating built-in tool is explicitly specified, the [ ImageGenerationTool ](../api/builtin_tools/index.html#pydantic_ai.builtin_tools.ImageGenerationTool) will be enabled automatically. image\\_output.py *(This example is complete, it can be run \"as is\")* If an agent does not need to always generate an image, you can use a union of BinaryImage and str . If the model generates both, the image will take precedence as output and the text will be available on [ ModelResponse.text ](../api/messages/index.html#pydantic_ai.messages.ModelResponse.text): image\\_output\\_union.py", "url": "https://ai.pydantic.dev/output/index.html#image-output", "page": "output/index.html", "source_site": "pydantic_ai"}
{"title": "Streamed Results", "anchor": "streamed-results", "heading_level": 2, "md_text": "There two main challenges with streamed results: 1. Validating structured responses before they're complete, this is achieved by \"partial validation\" which was recently added to Pydantic in [pydantic/pydantic#10748](https://github.com/pydantic/pydantic/pull/10748). 2. When receiving a response, we don't know if it's the final response without starting to stream it and peeking at the content. Pydantic AI streams just enough of the response to sniff out if it's a tool call or an output, then streams the whole thing and calls tools, or returns the stream as a [ StreamedRunResult ](../api/result/index.html#pydantic_ai.result.StreamedRunResult). Note As the run_stream() method will consider the first output matching the output_type to be the final output, it will stop running the agent graph and will not execute any tool calls made by the model after this \"final\" output. If you want to always run the agent graph to completion and stream all events from the model's streaming response and the agent's execution of tools, use [ agent.run_stream_events() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream_events) ([docs](../agents/index.html#streaming-all-events)) or [ agent.iter() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.iter) ([docs](../agents/index.html#streaming-all-events-and-output)) instead. ### Streaming Text Example of streamed text output: streamed\\_hello\\_world.py 1. Streaming works with the standard [ Agent ](../api/agent/index.html#pydantic_ai.agent.Agent) class, and doesn't require any special setup, just a model that supports streaming (currently all models support streaming). 2. The [ Agent.run_stream() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream) method is used to start a streamed run, this method returns a context manager so the connection can be closed when the stream completes. 3. Each item yield by [ StreamedRunResult.stream_text() ](../api/result/index.html#pydantic_ai.result.StreamedRunResult.stream_text) is the complete text response, extended as new data is received. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* We can also stream text as deltas rather than the entire text in each item: streamed\\_delta\\_hello\\_world.py 1. [ stream_text ](../api/result/index.html#pydantic_ai.result.StreamedRunResult.stream_text) will error if the response is not text. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* Output message not included in messages The final output message will **NOT** be added to result messages if you use .stream_text(delta=True) , see [Messages and chat history](../message-history/index.html) for more information. ### Streaming Structured Output Here's an example of streaming a user profile as it's built: streamed\\_user\\_profile.py *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* As setting an output_type uses the [Tool Output](index.html#tool-output) mode by default, this will only work if the model supports streaming tool arguments. For models that don't, like Gemini, try [Native Output](index.html#native-output) or [Prompted Output](index.html#prompted-output) instead. ### Streaming Model Responses If you want fine-grained control of validation, you can use the following pattern to get the entire partial [ ModelResponse ](../api/messages/index.html#pydantic_ai.messages.ModelResponse): streamed\\_user\\_profile.py 1. [ stream_responses ](../api/result/index.html#pydantic_ai.result.StreamedRunResult.stream_responses) streams the data as [ ModelResponse ](../api/messages/index.html#pydantic_ai.messages.ModelResponse) objects, thus iteration can't fail with a ValidationError . 2. [ validate_response_output ](../api/result/index.html#pydantic_ai.result.StreamedRunResult.validate_response_output) validates the data, allow_partial=True enables pydantic's [ experimental_allow_partial flag on TypeAdapter ](https://docs.pydantic.dev/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.validate_json). *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/output/index.html#streamed-results", "page": "output/index.html", "source_site": "pydantic_ai"}
{"title": "Streaming Text", "anchor": "streaming-text", "heading_level": 3, "md_text": "Example of streamed text output: streamed\\_hello\\_world.py 1. Streaming works with the standard [ Agent ](../api/agent/index.html#pydantic_ai.agent.Agent) class, and doesn't require any special setup, just a model that supports streaming (currently all models support streaming). 2. The [ Agent.run_stream() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream) method is used to start a streamed run, this method returns a context manager so the connection can be closed when the stream completes. 3. Each item yield by [ StreamedRunResult.stream_text() ](../api/result/index.html#pydantic_ai.result.StreamedRunResult.stream_text) is the complete text response, extended as new data is received. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* We can also stream text as deltas rather than the entire text in each item: streamed\\_delta\\_hello\\_world.py 1. [ stream_text ](../api/result/index.html#pydantic_ai.result.StreamedRunResult.stream_text) will error if the response is not text. *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* Output message not included in messages The final output message will **NOT** be added to result messages if you use .stream_text(delta=True) , see [Messages and chat history](../message-history/index.html) for more information.", "url": "https://ai.pydantic.dev/output/index.html#streaming-text", "page": "output/index.html", "source_site": "pydantic_ai"}
{"title": "Streaming Structured Output", "anchor": "streaming-structured-output", "heading_level": 3, "md_text": "Here's an example of streaming a user profile as it's built: streamed\\_user\\_profile.py *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )* As setting an output_type uses the [Tool Output](index.html#tool-output) mode by default, this will only work if the model supports streaming tool arguments. For models that don't, like Gemini, try [Native Output](index.html#native-output) or [Prompted Output](index.html#prompted-output) instead.", "url": "https://ai.pydantic.dev/output/index.html#streaming-structured-output", "page": "output/index.html", "source_site": "pydantic_ai"}
{"title": "Streaming Model Responses", "anchor": "streaming-model-responses", "heading_level": 3, "md_text": "If you want fine-grained control of validation, you can use the following pattern to get the entire partial [ ModelResponse ](../api/messages/index.html#pydantic_ai.messages.ModelResponse): streamed\\_user\\_profile.py 1. [ stream_responses ](../api/result/index.html#pydantic_ai.result.StreamedRunResult.stream_responses) streams the data as [ ModelResponse ](../api/messages/index.html#pydantic_ai.messages.ModelResponse) objects, thus iteration can't fail with a ValidationError . 2. [ validate_response_output ](../api/result/index.html#pydantic_ai.result.StreamedRunResult.validate_response_output) validates the data, allow_partial=True enables pydantic's [ experimental_allow_partial flag on TypeAdapter ](https://docs.pydantic.dev/latest/api/type_adapter/#pydantic.type_adapter.TypeAdapter.validate_json). *(This example is complete, it can be run \"as is\"  you'll need to add asyncio.run(main()) to run main )*", "url": "https://ai.pydantic.dev/output/index.html#streaming-model-responses", "page": "output/index.html", "source_site": "pydantic_ai"}
{"title": "Examples", "anchor": "examples", "heading_level": 2, "md_text": "The following examples demonstrate how to use streamed responses in Pydantic AI: * [Stream markdown](../examples/stream-markdown/index.html) * [Stream Whales](../examples/stream-whales/index.html)", "url": "https://ai.pydantic.dev/output/index.html#examples", "page": "output/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAI", "anchor": "openai", "heading_level": 1, "md_text": "## Install To use OpenAI models or OpenAI-compatible APIs, you need to either install pydantic-ai , or install pydantic-ai-slim with the openai optional group: pipuv ## Configuration To use OpenAIChatModel with the OpenAI API, go to [platform.openai.com](https://platform.openai.com/) and follow your nose until you find the place to generate an API key. ## Environment variable Once you have the API key, you can set it as an environment variable: You can then use OpenAIChatModel by name: Or initialise the model directly with just the model name: By default, the OpenAIChatModel uses the OpenAIProvider with the base_url set to https://api.openai.com/v1 . ## Configure the provider If you want to pass parameters in code to the provider, you can programmatically instantiate the [OpenAIProvider](../../api/providers/index.html#pydantic_ai.providers.openai.OpenAIProvider) and pass it to the model: ## Custom OpenAI Client OpenAIProvider also accepts a custom AsyncOpenAI client via the openai_client parameter, so you can customise the organization , project , base_url etc. as defined in the [OpenAI API docs](https://platform.openai.com/docs/api-reference). custom\\_openai\\_client.py You could also use the [ AsyncAzureOpenAI ](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/switching-endpoints) client to use the Azure OpenAI API. Note that the AsyncAzureOpenAI is a subclass of AsyncOpenAI . ## OpenAI Responses API Pydantic AI also supports OpenAI's [Responses API](https://platform.openai.com/docs/api-reference/responses) through the You can use [ OpenAIResponsesModel ](../../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIResponsesModel) by name: Or initialise the model directly with just the model name: You can learn more about the differences between the Responses API and Chat Completions API in the [OpenAI API docs](https://platform.openai.com/docs/guides/migrate-to-responses). ### Built-in tools The Responses API has built-in tools that you can use instead of building your own: * [Web search](https://platform.openai.com/docs/guides/tools-web-search): allow models to search the web for the latest information before generating a response. * [Code interpreter](https://platform.openai.com/docs/guides/tools-code-interpreter): allow models to write and run Python code in a sandboxed environment before generating a response. * [Image generation](https://platform.openai.com/docs/guides/tools-image-generation): allow models to generate images based on a text prompt. * [File search](https://platform.openai.com/docs/guides/tools-file-search): allow models to search your files for relevant information before generating a response. * [Computer use](https://platform.openai.com/docs/guides/tools-computer-use): allow models to use a computer to perform tasks on your behalf. Web search, Code interpreter, and Image generation are natively supported through the [Built-in tools](../../builtin-tools/index.html) feature. File search and Computer use can be enabled by passing an [ openai.types.responses.FileSearchToolParam ](https://github.com/openai/openai-python/blob/main/src/openai/types/responses/file_search_tool_param.py) or [ openai.types.responses.ComputerToolParam ](https://github.com/openai/openai-python/blob/main/src/openai/types/responses/computer_tool_param.py) in the openai_builtin_tools setting on [ OpenAIResponsesModelSettings ](../../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIResponsesModelSettings). They don't currently generate [ BuiltinToolCallPart ](../../api/messages/index.html#pydantic_ai.messages.BuiltinToolCallPart) or [ BuiltinToolReturnPart ](../../api/messages/index.html#pydantic_ai.messages.BuiltinToolReturnPart) parts in the message history, or streamed events; please submit an issue if you need native support for these built-in tools. file\\_search\\_tool.py #### Referencing earlier responses The Responses API supports referencing earlier model responses in a new request using a previous_response_id parameter, to ensure the full [conversation state](https://platform.openai.com/docs/guides/conversation-state?api-mode=responses#passing-context-from-the-previous-response) including [reasoning items](https://platform.openai.com/docs/guides/reasoning#keeping-reasoning-items-in-context) are kept in context. This is available through the openai_previous_response_id field in [ OpenAIResponsesModelSettings ](../../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIResponsesModelSettings). By passing the provider_response_id from an earlier run, you can allow the model to build on its own prior reasoning without needing to resend the full message history. ##### Automatically referencing earlier responses When the openai_previous_response_id field is set to 'auto' , Pydantic AI will automatically select the most recent provider_response_id from message history and omit messages that came before it, letting the OpenAI API leverage server-side history instead for improved efficiency. ## OpenAI-compatible Models Many providers and models are compatible with the OpenAI API, and can be used with OpenAIChatModel in Pydantic AI. Before getting started, check the [installation and configuration](index.html#install) instructions above. To use another OpenAI-compatible API, you can make use of the base_url and api_key arguments from OpenAIProvider : Various providers also have their own provider classes so that you don't need to specify the base URL yourself and you can use the standard <PROVIDER>_API_KEY environment variable to set the API key. When a provider has its own provider class, you can use the Agent(\"<provider>:<model>\") shorthand, e.g. Agent(\"deepseek:deepseek-chat\") or Agent(\"openrouter:google/gemini-2.5-pro-preview\") , instead of building the OpenAIChatModel explicitly. Similarly, you can pass the provider name as a string to the provider argument on OpenAIChatModel instead of building instantiating the provider class explicitly. #### Model Profile Sometimes, the provider or model you're using will have slightly different requirements than OpenAI's API or models, like having different restrictions on JSON schemas for tool definitions, or not supporting tool definitions to be marked as strict. When using an alternative provider class provided by Pydantic AI, an appropriate model profile is typically selected automatically based on the model name. If the model you're using is not working correctly out of the box, you can tweak various aspects of how model requests are constructed by providing your own [ ModelProfile ](../../api/profiles/index.html#pydantic_ai.profiles.ModelProfile) (for behaviors shared among all model classes) or [ OpenAIModelProfile ](../../api/profiles/index.html#pydantic_ai.profiles.openai.OpenAIModelProfile) (for behaviors specific to OpenAIChatModel ): ### DeepSeek To use the [DeepSeek](https://deepseek.com) provider, first create an API key by following the [Quick Start guide](https://api-docs.deepseek.com/). You can then set the DEEPSEEK_API_KEY environment variable and use [ DeepSeekProvider ](../../api/providers/index.html#pydantic_ai.providers.deepseek.DeepSeekProvider) by name: Or initialise the model and provider directly: You can also customize any provider with a custom http_client : ### Ollama Pydantic AI supports both self-hosted [Ollama](https://ollama.com/) servers (running locally or remotely) and [Ollama Cloud](https://ollama.com/cloud). For servers running locally, use the http://localhost:11434/v1 base URL. For Ollama Cloud, use https://ollama.com/v1 and ensure an API key is set. You can set the OLLAMA_BASE_URL and (optionally) OLLAMA_API_KEY environment variables and use [ OllamaProvider ](../../api/providers/index.html#pydantic_ai.providers.ollama.OllamaProvider) by name: Or initialise the model and provider directly: 1. For Ollama Cloud, use the base_url='https://ollama.com/v1' and set the OLLAMA_API_KEY environment variable. ### Azure AI Foundry To use [Azure AI Foundry](https://ai.azure.com/) as your provider, you can set the AZURE_OPENAI_ENDPOINT , AZURE_OPENAI_API_KEY , and OPENAI_API_VERSION environment variables and use [ AzureProvider ](../../api/providers/index.html#pydantic_ai.providers.azure.AzureProvider) by name: Or initialise the model and provider directly: ### OpenRouter To use [OpenRouter](https://openrouter.ai), first create an API key at [openrouter.ai/keys](https://openrouter.ai/keys). You can set the OPENROUTER_API_KEY environment variable and use [ OpenRouterProvider ](../../api/providers/index.html#pydantic_ai.providers.openrouter.OpenRouterProvider) by name: Or initialise the model and provider directly: ### Vercel AI Gateway To use [Vercel's AI Gateway](https://vercel.com/docs/ai-gateway), first follow the [documentation](https://vercel.com/docs/ai-gateway) instructions on obtaining an API key or OIDC token. You can set the VERCEL_AI_GATEWAY_API_KEY and VERCEL_OIDC_TOKEN environment variables and use [ VercelProvider ](../../api/providers/index.html#pydantic_ai.providers.vercel.VercelProvider) by name: Or initialise the model and provider directly: ### Grok (xAI) Go to [xAI API Console](https://console.x.ai/) and create an API key. You can set the GROK_API_KEY environment variable and use [ GrokProvider ](../../api/providers/index.html#pydantic_ai.providers.grok.GrokProvider) by name: Or initialise the model and provider directly: ### MoonshotAI Create an API key in the [Moonshot Console](https://platform.moonshot.ai/console). You can set the MOONSHOTAI_API_KEY environment variable and use [ MoonshotAIProvider ](../../api/providers/index.html#pydantic_ai.providers.moonshotai.MoonshotAIProvider) by name: Or initialise the model and provider directly: ### GitHub Models To use [GitHub Models](https://docs.github.com/en/github-models), you'll need a GitHub personal access token with the models: read permission. You can set the GITHUB_API_KEY environment variable and use [ GitHubProvider ](../../api/providers/index.html#pydantic_ai.providers.github.GitHubProvider) by name: Or initialise the model and provider directly: GitHub Models supports various model families with different prefixes. You can see the full list on the [GitHub Marketplace](https://github.com/marketplace?type=models) or the public [catalog endpoint](https://models.github.ai/catalog/models). ### Perplexity Follow the Perplexity [getting started](https://docs.perplexity.ai/guides/getting-started) guide to create an API key. ### Fireworks AI Go to [Fireworks.AI](https://fireworks.ai/) and create an API key in your account settings. You can set the FIREWORKS_API_KEY environment variable and use [ FireworksProvider ](../../api/providers/index.html#pydantic_ai.providers.fireworks.FireworksProvider) by name: Or initialise the model and provider directly: ### Together AI Go to [Together.ai](https://www.together.ai/) and create an API key in your account settings. You can set the TOGETHER_API_KEY environment variable and use [ TogetherProvider ](../../api/providers/index.html#pydantic_ai.providers.together.TogetherProvider) by name: Or initialise the model and provider directly: ### Heroku AI To use [Heroku AI](https://www.heroku.com/ai), first create an API key. You can set the HEROKU_INFERENCE_KEY and (optionally ) HEROKU_INFERENCE_URL environment variables and use [ HerokuProvider ](../../api/providers/index.html#pydantic_ai.providers.heroku.HerokuProvider) by name: Or initialise the model and provider directly: ### Cerebras To use [Cerebras](https://cerebras.ai/), you need to create an API key in the [Cerebras Console](https://cloud.cerebras.ai/). You can set the CEREBRAS_API_KEY environment variable and use [ CerebrasProvider ](../../api/providers/index.html#pydantic_ai.providers.cerebras.CerebrasProvider) by name: Or initialise the model and provider directly: ### LiteLLM To use [LiteLLM](https://www.litellm.ai/), set the configs as outlined in the [doc](https://docs.litellm.ai/docs/set_keys). In LiteLLMProvider , you can pass api_base and api_key . The value of these configs will depend on your setup. For example, if you are using OpenAI models, then you need to pass https://api.openai.com/v1 as the api_base and your OpenAI API key as the api_key . If you are using a LiteLLM proxy server running on your local machine, then you need to pass http://localhost:<port> as the api_base and your LiteLLM API key (or a placeholder) as the api_key . To use custom LLMs, use custom/ prefix in the model name. Once you have the configs, use the [ LiteLLMProvider ](../../api/providers/index.html#pydantic_ai.providers.litellm.LiteLLMProvider) as follows: ### Nebius AI Studio Go to [Nebius AI Studio](https://studio.nebius.com/) and create an API key. You can set the NEBIUS_API_KEY environment variable and use [ NebiusProvider ](../../api/providers/index.html#pydantic_ai.providers.nebius.NebiusProvider) by name: Or initialise the model and provider directly: ### OVHcloud AI Endpoints To use OVHcloud AI Endpoints, you need to create a new API key. To do so, go to the [OVHcloud manager](https://ovh.com/manager), then in Public Cloud > AI Endpoints > API keys. Click on Create a new API key and copy your new key. You can explore the [catalog](https://endpoints.ai.cloud.ovh.net/catalog) to find which models are available. You can set the OVHCLOUD_API_KEY environment variable and use [ OVHcloudProvider ](../../api/providers/index.html#pydantic_ai.providers.ovhcloud.OVHcloudProvider) by name: If you need to configure the provider, you can use the [ OVHcloudProvider ](../../api/providers/index.html#pydantic_ai.providers.ovhcloud.OVHcloudProvider) class:", "url": "https://ai.pydantic.dev/models/openai/index.html#openai", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "Install", "anchor": "install", "heading_level": 2, "md_text": "To use OpenAI models or OpenAI-compatible APIs, you need to either install pydantic-ai , or install pydantic-ai-slim with the openai optional group: pipuv", "url": "https://ai.pydantic.dev/models/openai/index.html#install", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "Configuration", "anchor": "configuration", "heading_level": 2, "md_text": "To use OpenAIChatModel with the OpenAI API, go to [platform.openai.com](https://platform.openai.com/) and follow your nose until you find the place to generate an API key.", "url": "https://ai.pydantic.dev/models/openai/index.html#configuration", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "Environment variable", "anchor": "environment-variable", "heading_level": 2, "md_text": "Once you have the API key, you can set it as an environment variable: You can then use OpenAIChatModel by name: Or initialise the model directly with just the model name: By default, the OpenAIChatModel uses the OpenAIProvider with the base_url set to https://api.openai.com/v1 .", "url": "https://ai.pydantic.dev/models/openai/index.html#environment-variable", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "Configure the provider", "anchor": "configure-the-provider", "heading_level": 2, "md_text": "If you want to pass parameters in code to the provider, you can programmatically instantiate the [OpenAIProvider](../../api/providers/index.html#pydantic_ai.providers.openai.OpenAIProvider) and pass it to the model:", "url": "https://ai.pydantic.dev/models/openai/index.html#configure-the-provider", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "Custom OpenAI Client", "anchor": "custom-openai-client", "heading_level": 2, "md_text": "OpenAIProvider also accepts a custom AsyncOpenAI client via the openai_client parameter, so you can customise the organization , project , base_url etc. as defined in the [OpenAI API docs](https://platform.openai.com/docs/api-reference). custom\\_openai\\_client.py You could also use the [ AsyncAzureOpenAI ](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/switching-endpoints) client to use the Azure OpenAI API. Note that the AsyncAzureOpenAI is a subclass of AsyncOpenAI .", "url": "https://ai.pydantic.dev/models/openai/index.html#custom-openai-client", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAI Responses API", "anchor": "openai-responses-api", "heading_level": 2, "md_text": "Pydantic AI also supports OpenAI's [Responses API](https://platform.openai.com/docs/api-reference/responses) through the You can use [ OpenAIResponsesModel ](../../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIResponsesModel) by name: Or initialise the model directly with just the model name: You can learn more about the differences between the Responses API and Chat Completions API in the [OpenAI API docs](https://platform.openai.com/docs/guides/migrate-to-responses). ### Built-in tools The Responses API has built-in tools that you can use instead of building your own: * [Web search](https://platform.openai.com/docs/guides/tools-web-search): allow models to search the web for the latest information before generating a response. * [Code interpreter](https://platform.openai.com/docs/guides/tools-code-interpreter): allow models to write and run Python code in a sandboxed environment before generating a response. * [Image generation](https://platform.openai.com/docs/guides/tools-image-generation): allow models to generate images based on a text prompt. * [File search](https://platform.openai.com/docs/guides/tools-file-search): allow models to search your files for relevant information before generating a response. * [Computer use](https://platform.openai.com/docs/guides/tools-computer-use): allow models to use a computer to perform tasks on your behalf. Web search, Code interpreter, and Image generation are natively supported through the [Built-in tools](../../builtin-tools/index.html) feature. File search and Computer use can be enabled by passing an [ openai.types.responses.FileSearchToolParam ](https://github.com/openai/openai-python/blob/main/src/openai/types/responses/file_search_tool_param.py) or [ openai.types.responses.ComputerToolParam ](https://github.com/openai/openai-python/blob/main/src/openai/types/responses/computer_tool_param.py) in the openai_builtin_tools setting on [ OpenAIResponsesModelSettings ](../../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIResponsesModelSettings). They don't currently generate [ BuiltinToolCallPart ](../../api/messages/index.html#pydantic_ai.messages.BuiltinToolCallPart) or [ BuiltinToolReturnPart ](../../api/messages/index.html#pydantic_ai.messages.BuiltinToolReturnPart) parts in the message history, or streamed events; please submit an issue if you need native support for these built-in tools. file\\_search\\_tool.py #### Referencing earlier responses The Responses API supports referencing earlier model responses in a new request using a previous_response_id parameter, to ensure the full [conversation state](https://platform.openai.com/docs/guides/conversation-state?api-mode=responses#passing-context-from-the-previous-response) including [reasoning items](https://platform.openai.com/docs/guides/reasoning#keeping-reasoning-items-in-context) are kept in context. This is available through the openai_previous_response_id field in [ OpenAIResponsesModelSettings ](../../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIResponsesModelSettings). By passing the provider_response_id from an earlier run, you can allow the model to build on its own prior reasoning without needing to resend the full message history. ##### Automatically referencing earlier responses When the openai_previous_response_id field is set to 'auto' , Pydantic AI will automatically select the most recent provider_response_id from message history and omit messages that came before it, letting the OpenAI API leverage server-side history instead for improved efficiency.", "url": "https://ai.pydantic.dev/models/openai/index.html#openai-responses-api", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "Built-in tools", "anchor": "built-in-tools", "heading_level": 3, "md_text": "The Responses API has built-in tools that you can use instead of building your own: * [Web search](https://platform.openai.com/docs/guides/tools-web-search): allow models to search the web for the latest information before generating a response. * [Code interpreter](https://platform.openai.com/docs/guides/tools-code-interpreter): allow models to write and run Python code in a sandboxed environment before generating a response. * [Image generation](https://platform.openai.com/docs/guides/tools-image-generation): allow models to generate images based on a text prompt. * [File search](https://platform.openai.com/docs/guides/tools-file-search): allow models to search your files for relevant information before generating a response. * [Computer use](https://platform.openai.com/docs/guides/tools-computer-use): allow models to use a computer to perform tasks on your behalf. Web search, Code interpreter, and Image generation are natively supported through the [Built-in tools](../../builtin-tools/index.html) feature. File search and Computer use can be enabled by passing an [ openai.types.responses.FileSearchToolParam ](https://github.com/openai/openai-python/blob/main/src/openai/types/responses/file_search_tool_param.py) or [ openai.types.responses.ComputerToolParam ](https://github.com/openai/openai-python/blob/main/src/openai/types/responses/computer_tool_param.py) in the openai_builtin_tools setting on [ OpenAIResponsesModelSettings ](../../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIResponsesModelSettings). They don't currently generate [ BuiltinToolCallPart ](../../api/messages/index.html#pydantic_ai.messages.BuiltinToolCallPart) or [ BuiltinToolReturnPart ](../../api/messages/index.html#pydantic_ai.messages.BuiltinToolReturnPart) parts in the message history, or streamed events; please submit an issue if you need native support for these built-in tools. file\\_search\\_tool.py #### Referencing earlier responses The Responses API supports referencing earlier model responses in a new request using a previous_response_id parameter, to ensure the full [conversation state](https://platform.openai.com/docs/guides/conversation-state?api-mode=responses#passing-context-from-the-previous-response) including [reasoning items](https://platform.openai.com/docs/guides/reasoning#keeping-reasoning-items-in-context) are kept in context. This is available through the openai_previous_response_id field in [ OpenAIResponsesModelSettings ](../../api/models/openai/index.html#pydantic_ai.models.openai.OpenAIResponsesModelSettings). By passing the provider_response_id from an earlier run, you can allow the model to build on its own prior reasoning without needing to resend the full message history. ##### Automatically referencing earlier responses When the openai_previous_response_id field is set to 'auto' , Pydantic AI will automatically select the most recent provider_response_id from message history and omit messages that came before it, letting the OpenAI API leverage server-side history instead for improved efficiency.", "url": "https://ai.pydantic.dev/models/openai/index.html#built-in-tools", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenAI-compatible Models", "anchor": "openai-compatible-models", "heading_level": 2, "md_text": "Many providers and models are compatible with the OpenAI API, and can be used with OpenAIChatModel in Pydantic AI. Before getting started, check the [installation and configuration](index.html#install) instructions above. To use another OpenAI-compatible API, you can make use of the base_url and api_key arguments from OpenAIProvider : Various providers also have their own provider classes so that you don't need to specify the base URL yourself and you can use the standard <PROVIDER>_API_KEY environment variable to set the API key. When a provider has its own provider class, you can use the Agent(\"<provider>:<model>\") shorthand, e.g. Agent(\"deepseek:deepseek-chat\") or Agent(\"openrouter:google/gemini-2.5-pro-preview\") , instead of building the OpenAIChatModel explicitly. Similarly, you can pass the provider name as a string to the provider argument on OpenAIChatModel instead of building instantiating the provider class explicitly. #### Model Profile Sometimes, the provider or model you're using will have slightly different requirements than OpenAI's API or models, like having different restrictions on JSON schemas for tool definitions, or not supporting tool definitions to be marked as strict. When using an alternative provider class provided by Pydantic AI, an appropriate model profile is typically selected automatically based on the model name. If the model you're using is not working correctly out of the box, you can tweak various aspects of how model requests are constructed by providing your own [ ModelProfile ](../../api/profiles/index.html#pydantic_ai.profiles.ModelProfile) (for behaviors shared among all model classes) or [ OpenAIModelProfile ](../../api/profiles/index.html#pydantic_ai.profiles.openai.OpenAIModelProfile) (for behaviors specific to OpenAIChatModel ): ### DeepSeek To use the [DeepSeek](https://deepseek.com) provider, first create an API key by following the [Quick Start guide](https://api-docs.deepseek.com/). You can then set the DEEPSEEK_API_KEY environment variable and use [ DeepSeekProvider ](../../api/providers/index.html#pydantic_ai.providers.deepseek.DeepSeekProvider) by name: Or initialise the model and provider directly: You can also customize any provider with a custom http_client : ### Ollama Pydantic AI supports both self-hosted [Ollama](https://ollama.com/) servers (running locally or remotely) and [Ollama Cloud](https://ollama.com/cloud). For servers running locally, use the http://localhost:11434/v1 base URL. For Ollama Cloud, use https://ollama.com/v1 and ensure an API key is set. You can set the OLLAMA_BASE_URL and (optionally) OLLAMA_API_KEY environment variables and use [ OllamaProvider ](../../api/providers/index.html#pydantic_ai.providers.ollama.OllamaProvider) by name: Or initialise the model and provider directly: 1. For Ollama Cloud, use the base_url='https://ollama.com/v1' and set the OLLAMA_API_KEY environment variable. ### Azure AI Foundry To use [Azure AI Foundry](https://ai.azure.com/) as your provider, you can set the AZURE_OPENAI_ENDPOINT , AZURE_OPENAI_API_KEY , and OPENAI_API_VERSION environment variables and use [ AzureProvider ](../../api/providers/index.html#pydantic_ai.providers.azure.AzureProvider) by name: Or initialise the model and provider directly: ### OpenRouter To use [OpenRouter](https://openrouter.ai), first create an API key at [openrouter.ai/keys](https://openrouter.ai/keys). You can set the OPENROUTER_API_KEY environment variable and use [ OpenRouterProvider ](../../api/providers/index.html#pydantic_ai.providers.openrouter.OpenRouterProvider) by name: Or initialise the model and provider directly: ### Vercel AI Gateway To use [Vercel's AI Gateway](https://vercel.com/docs/ai-gateway), first follow the [documentation](https://vercel.com/docs/ai-gateway) instructions on obtaining an API key or OIDC token. You can set the VERCEL_AI_GATEWAY_API_KEY and VERCEL_OIDC_TOKEN environment variables and use [ VercelProvider ](../../api/providers/index.html#pydantic_ai.providers.vercel.VercelProvider) by name: Or initialise the model and provider directly: ### Grok (xAI) Go to [xAI API Console](https://console.x.ai/) and create an API key. You can set the GROK_API_KEY environment variable and use [ GrokProvider ](../../api/providers/index.html#pydantic_ai.providers.grok.GrokProvider) by name: Or initialise the model and provider directly: ### MoonshotAI Create an API key in the [Moonshot Console](https://platform.moonshot.ai/console). You can set the MOONSHOTAI_API_KEY environment variable and use [ MoonshotAIProvider ](../../api/providers/index.html#pydantic_ai.providers.moonshotai.MoonshotAIProvider) by name: Or initialise the model and provider directly: ### GitHub Models To use [GitHub Models](https://docs.github.com/en/github-models), you'll need a GitHub personal access token with the models: read permission. You can set the GITHUB_API_KEY environment variable and use [ GitHubProvider ](../../api/providers/index.html#pydantic_ai.providers.github.GitHubProvider) by name: Or initialise the model and provider directly: GitHub Models supports various model families with different prefixes. You can see the full list on the [GitHub Marketplace](https://github.com/marketplace?type=models) or the public [catalog endpoint](https://models.github.ai/catalog/models). ### Perplexity Follow the Perplexity [getting started](https://docs.perplexity.ai/guides/getting-started) guide to create an API key. ### Fireworks AI Go to [Fireworks.AI](https://fireworks.ai/) and create an API key in your account settings. You can set the FIREWORKS_API_KEY environment variable and use [ FireworksProvider ](../../api/providers/index.html#pydantic_ai.providers.fireworks.FireworksProvider) by name: Or initialise the model and provider directly: ### Together AI Go to [Together.ai](https://www.together.ai/) and create an API key in your account settings. You can set the TOGETHER_API_KEY environment variable and use [ TogetherProvider ](../../api/providers/index.html#pydantic_ai.providers.together.TogetherProvider) by name: Or initialise the model and provider directly: ### Heroku AI To use [Heroku AI](https://www.heroku.com/ai), first create an API key. You can set the HEROKU_INFERENCE_KEY and (optionally ) HEROKU_INFERENCE_URL environment variables and use [ HerokuProvider ](../../api/providers/index.html#pydantic_ai.providers.heroku.HerokuProvider) by name: Or initialise the model and provider directly: ### Cerebras To use [Cerebras](https://cerebras.ai/), you need to create an API key in the [Cerebras Console](https://cloud.cerebras.ai/). You can set the CEREBRAS_API_KEY environment variable and use [ CerebrasProvider ](../../api/providers/index.html#pydantic_ai.providers.cerebras.CerebrasProvider) by name: Or initialise the model and provider directly: ### LiteLLM To use [LiteLLM](https://www.litellm.ai/), set the configs as outlined in the [doc](https://docs.litellm.ai/docs/set_keys). In LiteLLMProvider , you can pass api_base and api_key . The value of these configs will depend on your setup. For example, if you are using OpenAI models, then you need to pass https://api.openai.com/v1 as the api_base and your OpenAI API key as the api_key . If you are using a LiteLLM proxy server running on your local machine, then you need to pass http://localhost:<port> as the api_base and your LiteLLM API key (or a placeholder) as the api_key . To use custom LLMs, use custom/ prefix in the model name. Once you have the configs, use the [ LiteLLMProvider ](../../api/providers/index.html#pydantic_ai.providers.litellm.LiteLLMProvider) as follows: ### Nebius AI Studio Go to [Nebius AI Studio](https://studio.nebius.com/) and create an API key. You can set the NEBIUS_API_KEY environment variable and use [ NebiusProvider ](../../api/providers/index.html#pydantic_ai.providers.nebius.NebiusProvider) by name: Or initialise the model and provider directly: ### OVHcloud AI Endpoints To use OVHcloud AI Endpoints, you need to create a new API key. To do so, go to the [OVHcloud manager](https://ovh.com/manager), then in Public Cloud > AI Endpoints > API keys. Click on Create a new API key and copy your new key. You can explore the [catalog](https://endpoints.ai.cloud.ovh.net/catalog) to find which models are available. You can set the OVHCLOUD_API_KEY environment variable and use [ OVHcloudProvider ](../../api/providers/index.html#pydantic_ai.providers.ovhcloud.OVHcloudProvider) by name: If you need to configure the provider, you can use the [ OVHcloudProvider ](../../api/providers/index.html#pydantic_ai.providers.ovhcloud.OVHcloudProvider) class:", "url": "https://ai.pydantic.dev/models/openai/index.html#openai-compatible-models", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "DeepSeek", "anchor": "deepseek", "heading_level": 3, "md_text": "To use the [DeepSeek](https://deepseek.com) provider, first create an API key by following the [Quick Start guide](https://api-docs.deepseek.com/). You can then set the DEEPSEEK_API_KEY environment variable and use [ DeepSeekProvider ](../../api/providers/index.html#pydantic_ai.providers.deepseek.DeepSeekProvider) by name: Or initialise the model and provider directly: You can also customize any provider with a custom http_client :", "url": "https://ai.pydantic.dev/models/openai/index.html#deepseek", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "Ollama", "anchor": "ollama", "heading_level": 3, "md_text": "Pydantic AI supports both self-hosted [Ollama](https://ollama.com/) servers (running locally or remotely) and [Ollama Cloud](https://ollama.com/cloud). For servers running locally, use the http://localhost:11434/v1 base URL. For Ollama Cloud, use https://ollama.com/v1 and ensure an API key is set. You can set the OLLAMA_BASE_URL and (optionally) OLLAMA_API_KEY environment variables and use [ OllamaProvider ](../../api/providers/index.html#pydantic_ai.providers.ollama.OllamaProvider) by name: Or initialise the model and provider directly: 1. For Ollama Cloud, use the base_url='https://ollama.com/v1' and set the OLLAMA_API_KEY environment variable.", "url": "https://ai.pydantic.dev/models/openai/index.html#ollama", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "Azure AI Foundry", "anchor": "azure-ai-foundry", "heading_level": 3, "md_text": "To use [Azure AI Foundry](https://ai.azure.com/) as your provider, you can set the AZURE_OPENAI_ENDPOINT , AZURE_OPENAI_API_KEY , and OPENAI_API_VERSION environment variables and use [ AzureProvider ](../../api/providers/index.html#pydantic_ai.providers.azure.AzureProvider) by name: Or initialise the model and provider directly:", "url": "https://ai.pydantic.dev/models/openai/index.html#azure-ai-foundry", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "OpenRouter", "anchor": "openrouter", "heading_level": 3, "md_text": "To use [OpenRouter](https://openrouter.ai), first create an API key at [openrouter.ai/keys](https://openrouter.ai/keys). You can set the OPENROUTER_API_KEY environment variable and use [ OpenRouterProvider ](../../api/providers/index.html#pydantic_ai.providers.openrouter.OpenRouterProvider) by name: Or initialise the model and provider directly:", "url": "https://ai.pydantic.dev/models/openai/index.html#openrouter", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "Vercel AI Gateway", "anchor": "vercel-ai-gateway", "heading_level": 3, "md_text": "To use [Vercel's AI Gateway](https://vercel.com/docs/ai-gateway), first follow the [documentation](https://vercel.com/docs/ai-gateway) instructions on obtaining an API key or OIDC token. You can set the VERCEL_AI_GATEWAY_API_KEY and VERCEL_OIDC_TOKEN environment variables and use [ VercelProvider ](../../api/providers/index.html#pydantic_ai.providers.vercel.VercelProvider) by name: Or initialise the model and provider directly:", "url": "https://ai.pydantic.dev/models/openai/index.html#vercel-ai-gateway", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "Grok (xAI)", "anchor": "grok-xai", "heading_level": 3, "md_text": "Go to [xAI API Console](https://console.x.ai/) and create an API key. You can set the GROK_API_KEY environment variable and use [ GrokProvider ](../../api/providers/index.html#pydantic_ai.providers.grok.GrokProvider) by name: Or initialise the model and provider directly:", "url": "https://ai.pydantic.dev/models/openai/index.html#grok-xai", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "MoonshotAI", "anchor": "moonshotai", "heading_level": 3, "md_text": "Create an API key in the [Moonshot Console](https://platform.moonshot.ai/console). You can set the MOONSHOTAI_API_KEY environment variable and use [ MoonshotAIProvider ](../../api/providers/index.html#pydantic_ai.providers.moonshotai.MoonshotAIProvider) by name: Or initialise the model and provider directly:", "url": "https://ai.pydantic.dev/models/openai/index.html#moonshotai", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "GitHub Models", "anchor": "github-models", "heading_level": 3, "md_text": "To use [GitHub Models](https://docs.github.com/en/github-models), you'll need a GitHub personal access token with the models: read permission. You can set the GITHUB_API_KEY environment variable and use [ GitHubProvider ](../../api/providers/index.html#pydantic_ai.providers.github.GitHubProvider) by name: Or initialise the model and provider directly: GitHub Models supports various model families with different prefixes. You can see the full list on the [GitHub Marketplace](https://github.com/marketplace?type=models) or the public [catalog endpoint](https://models.github.ai/catalog/models).", "url": "https://ai.pydantic.dev/models/openai/index.html#github-models", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "Perplexity", "anchor": "perplexity", "heading_level": 3, "md_text": "Follow the Perplexity [getting started](https://docs.perplexity.ai/guides/getting-started) guide to create an API key.", "url": "https://ai.pydantic.dev/models/openai/index.html#perplexity", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "Fireworks AI", "anchor": "fireworks-ai", "heading_level": 3, "md_text": "Go to [Fireworks.AI](https://fireworks.ai/) and create an API key in your account settings. You can set the FIREWORKS_API_KEY environment variable and use [ FireworksProvider ](../../api/providers/index.html#pydantic_ai.providers.fireworks.FireworksProvider) by name: Or initialise the model and provider directly:", "url": "https://ai.pydantic.dev/models/openai/index.html#fireworks-ai", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "Together AI", "anchor": "together-ai", "heading_level": 3, "md_text": "Go to [Together.ai](https://www.together.ai/) and create an API key in your account settings. You can set the TOGETHER_API_KEY environment variable and use [ TogetherProvider ](../../api/providers/index.html#pydantic_ai.providers.together.TogetherProvider) by name: Or initialise the model and provider directly:", "url": "https://ai.pydantic.dev/models/openai/index.html#together-ai", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "Heroku AI", "anchor": "heroku-ai", "heading_level": 3, "md_text": "To use [Heroku AI](https://www.heroku.com/ai), first create an API key. You can set the HEROKU_INFERENCE_KEY and (optionally ) HEROKU_INFERENCE_URL environment variables and use [ HerokuProvider ](../../api/providers/index.html#pydantic_ai.providers.heroku.HerokuProvider) by name: Or initialise the model and provider directly:", "url": "https://ai.pydantic.dev/models/openai/index.html#heroku-ai", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "Cerebras", "anchor": "cerebras", "heading_level": 3, "md_text": "To use [Cerebras](https://cerebras.ai/), you need to create an API key in the [Cerebras Console](https://cloud.cerebras.ai/). You can set the CEREBRAS_API_KEY environment variable and use [ CerebrasProvider ](../../api/providers/index.html#pydantic_ai.providers.cerebras.CerebrasProvider) by name: Or initialise the model and provider directly:", "url": "https://ai.pydantic.dev/models/openai/index.html#cerebras", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "LiteLLM", "anchor": "litellm", "heading_level": 3, "md_text": "To use [LiteLLM](https://www.litellm.ai/), set the configs as outlined in the [doc](https://docs.litellm.ai/docs/set_keys). In LiteLLMProvider , you can pass api_base and api_key . The value of these configs will depend on your setup. For example, if you are using OpenAI models, then you need to pass https://api.openai.com/v1 as the api_base and your OpenAI API key as the api_key . If you are using a LiteLLM proxy server running on your local machine, then you need to pass http://localhost:<port> as the api_base and your LiteLLM API key (or a placeholder) as the api_key . To use custom LLMs, use custom/ prefix in the model name. Once you have the configs, use the [ LiteLLMProvider ](../../api/providers/index.html#pydantic_ai.providers.litellm.LiteLLMProvider) as follows:", "url": "https://ai.pydantic.dev/models/openai/index.html#litellm", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "Nebius AI Studio", "anchor": "nebius-ai-studio", "heading_level": 3, "md_text": "Go to [Nebius AI Studio](https://studio.nebius.com/) and create an API key. You can set the NEBIUS_API_KEY environment variable and use [ NebiusProvider ](../../api/providers/index.html#pydantic_ai.providers.nebius.NebiusProvider) by name: Or initialise the model and provider directly:", "url": "https://ai.pydantic.dev/models/openai/index.html#nebius-ai-studio", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "OVHcloud AI Endpoints", "anchor": "ovhcloud-ai-endpoints", "heading_level": 3, "md_text": "To use OVHcloud AI Endpoints, you need to create a new API key. To do so, go to the [OVHcloud manager](https://ovh.com/manager), then in Public Cloud > AI Endpoints > API keys. Click on Create a new API key and copy your new key. You can explore the [catalog](https://endpoints.ai.cloud.ovh.net/catalog) to find which models are available. You can set the OVHCLOUD_API_KEY environment variable and use [ OVHcloudProvider ](../../api/providers/index.html#pydantic_ai.providers.ovhcloud.OVHcloudProvider) by name: If you need to configure the provider, you can use the [ OVHcloudProvider ](../../api/providers/index.html#pydantic_ai.providers.ovhcloud.OVHcloudProvider) class:", "url": "https://ai.pydantic.dev/models/openai/index.html#ovhcloud-ai-endpoints", "page": "models/openai/index.html", "source_site": "pydantic_ai"}
{"title": "Advanced Tool Features", "anchor": "advanced-tool-features", "heading_level": 1, "md_text": "This page covers advanced features for function tools in Pydantic AI. For basic tool usage, see the [Function Tools](../tools/index.html) documentation. ## Tool Output Tools can return anything that Pydantic can serialize to JSON, as well as audio, video, image or document content depending on the types of [multi-modal input](../input/index.html) the model supports: function\\_tool\\_output.py *(This example is complete, it can be run \"as is\")* Some models (e.g. Gemini) natively support semi-structured return values, while some expect text (OpenAI) but seem to be just as good at extracting meaning from the data. If a Python object is returned and the model expects a string, the value will be serialized to JSON. ### Advanced Tool Returns For scenarios where you need more control over both the tool's return value and the content sent to the model, you can use [ ToolReturn ](../api/messages/index.html#pydantic_ai.messages.ToolReturn). This is particularly useful when you want to: * Provide rich multi-modal content (images, documents, etc.) to the model as context * Separate the programmatic return value from the model's context * Include additional metadata that shouldn't be sent to the LLM Here's an example of a computer automation tool that captures screenshots and provides visual feedback: advanced\\_tool\\_return.py * ** return_value **: The actual return value used in the tool response. This is what gets serialized and sent back to the model as the tool's result. * ** content **: A sequence of content (text, images, documents, etc.) that provides additional context to the model. This appears as a separate user message. * ** metadata **: Optional metadata that your application can access but is not sent to the LLM. Useful for logging, debugging, or additional processing. Some other AI frameworks call this feature \"artifacts\". This separation allows you to provide rich context to the model while maintaining clean, structured return values for your application logic. ## Custom Tool Schema If you have a function that lacks appropriate documentation (i.e. poorly named, no type information, poor docstring, use of \\*args or \\*\\*kwargs and suchlike) then you can still turn it into a tool that can be effectively used by the agent with the [ Tool.from_schema ](../api/tools/index.html#pydantic_ai.tools.Tool.from_schema) function. With this you provide the name, description, JSON schema, and whether the function takes a RunContext for the function directly: Please note that validation of the tool arguments will not be performed, and this will pass all arguments as keyword arguments. ## Dynamic Tools Tools can optionally be defined with another function: prepare , which is called at each step of a run to customize the definition of the tool passed to the model, or omit the tool completely from that step. A prepare method can be registered via the prepare kwarg to any of the tool registration mechanisms: * [ @agent.tool ](../api/agent/index.html#pydantic_ai.agent.Agent.tool) decorator * [ @agent.tool_plain ](../api/agent/index.html#pydantic_ai.agent.Agent.tool_plain) decorator * [ Tool ](../api/tools/index.html#pydantic_ai.tools.Tool) dataclass The prepare method, should be of type [ ToolPrepareFunc ](../api/tools/index.html#pydantic_ai.tools.ToolPrepareFunc), a function which takes [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext) and a pre-built [ ToolDefinition ](../api/tools/index.html#pydantic_ai.tools.ToolDefinition), and should either return that ToolDefinition with or without modifying it, return a new ToolDefinition , or return None to indicate this tools should not be registered for that step. Here's a simple prepare method that only includes the tool if the value of the dependency is 42 . As with the previous example, we use [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) to demonstrate the behavior without calling a real model. tool\\_only\\_if\\_42.py *(This example is complete, it can be run \"as is\")* Here's a more complex example where we change the description of the name parameter to based on the value of deps For the sake of variation, we create this tool using the [ Tool ](../api/tools/index.html#pydantic_ai.tools.Tool) dataclass. customize\\_name.py *(This example is complete, it can be run \"as is\")* ### Agent-wide Dynamic Tools In addition to per-tool prepare methods, you can also define an agent-wide prepare_tools function. This function is called at each step of a run and allows you to filter or modify the list of all tool definitions available to the agent for that step. This is especially useful if you want to enable or disable multiple tools at once, or apply global logic based on the current context. The prepare_tools function should be of type [ ToolsPrepareFunc ](../api/tools/index.html#pydantic_ai.tools.ToolsPrepareFunc), which takes the [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext) and a list of [ ToolDefinition ](../api/tools/index.html#pydantic_ai.tools.ToolDefinition), and returns a new list of tool definitions (or None to disable all tools for that step). Note The list of tool definitions passed to prepare_tools includes both regular function tools and tools from any [toolsets](../toolsets/index.html) registered on the agent, but not [output tools](../output/index.html#tool-output). To modify output tools, you can set a prepare_output_tools function instead. Here's an example that makes all tools strict if the model is an OpenAI model: agent\\_prepare\\_tools\\_customize.py *(This example is complete, it can be run \"as is\")* Here's another example that conditionally filters out the tools by name if the dependency ( ctx.deps ) is True : agent\\_prepare\\_tools\\_filter\\_out.py *(This example is complete, it can be run \"as is\")* You can use prepare_tools to: * Dynamically enable or disable tools based on the current model, dependencies, or other context * Modify tool definitions globally (e.g., set all tools to strict mode, change descriptions, etc.) If both per-tool prepare and agent-wide prepare_tools are used, the per-tool prepare is applied first to each tool, and then prepare_tools is called with the resulting list of tool definitions. ## Tool Execution and Retries When a tool is executed, its arguments (provided by the LLM) are first validated against the function's signature using Pydantic. If validation fails (e.g., due to incorrect types or missing required arguments), a ValidationError is raised, and the framework automatically generates a [ RetryPromptPart ](../api/messages/index.html#pydantic_ai.messages.RetryPromptPart) containing the validation details. This prompt is sent back to the LLM, informing it of the error and allowing it to correct the parameters and retry the tool call. Beyond automatic validation errors, the tool's own internal logic can also explicitly request a retry by raising the [ ModelRetry ](../api/exceptions/index.html#pydantic_ai.exceptions.ModelRetry) exception. This is useful for situations where the parameters were technically valid, but an issue occurred during execution (like a transient network error, or the tool determining the initial attempt needs modification). Raising ModelRetry also generates a RetryPromptPart containing the exception message, which is sent back to the LLM to guide its next attempt. Both ValidationError and ModelRetry respect the retries setting configured on the Tool or Agent . ### Parallel tool calls & concurrency When a model returns multiple tool calls in one response, Pydantic AI schedules them concurrently using asyncio.create_task . If a tool requires sequential/serial execution, you can pass the [ sequential ](../api/tools/index.html#pydantic_ai.tools.ToolDefinition.sequential) flag when registering the tool, or wrap the agent run in the [ with agent.sequential_tool_calls() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.sequential_tool_calls) context manager. Async functions are run on the event loop, while sync functions are offloaded to threads. To get the best performance, *always* use an async function *unless* you're doing blocking I/O (and there's no way to use a non-blocking library instead) or CPU-bound work (like numpy or scikit-learn operations), so that simple functions are not offloaded to threads unnecessarily. Limiting tool executions You can cap tool executions within a run using [ UsageLimits(tool_calls_limit=...) ](../agents/index.html#usage-limits). The counter increments only after a successful tool invocation. Output tools (used for [structured output](../output/index.html)) are not counted in the tool_calls metric. ## See Also * [Function Tools](../tools/index.html) - Basic tool concepts and registration * [Toolsets](../toolsets/index.html) - Managing collections of tools * [Deferred Tools](../deferred-tools/index.html) - Tools requiring approval or external execution * [Third-Party Tools](../third-party-tools/index.html) - Integrations with external tool libraries", "url": "https://ai.pydantic.dev/tools-advanced/index.html#advanced-tool-features", "page": "tools-advanced/index.html", "source_site": "pydantic_ai"}
{"title": "Tool Output", "anchor": "function-tool-output", "heading_level": 2, "md_text": "Tools can return anything that Pydantic can serialize to JSON, as well as audio, video, image or document content depending on the types of [multi-modal input](../input/index.html) the model supports: function\\_tool\\_output.py *(This example is complete, it can be run \"as is\")* Some models (e.g. Gemini) natively support semi-structured return values, while some expect text (OpenAI) but seem to be just as good at extracting meaning from the data. If a Python object is returned and the model expects a string, the value will be serialized to JSON. ### Advanced Tool Returns For scenarios where you need more control over both the tool's return value and the content sent to the model, you can use [ ToolReturn ](../api/messages/index.html#pydantic_ai.messages.ToolReturn). This is particularly useful when you want to: * Provide rich multi-modal content (images, documents, etc.) to the model as context * Separate the programmatic return value from the model's context * Include additional metadata that shouldn't be sent to the LLM Here's an example of a computer automation tool that captures screenshots and provides visual feedback: advanced\\_tool\\_return.py * ** return_value **: The actual return value used in the tool response. This is what gets serialized and sent back to the model as the tool's result. * ** content **: A sequence of content (text, images, documents, etc.) that provides additional context to the model. This appears as a separate user message. * ** metadata **: Optional metadata that your application can access but is not sent to the LLM. Useful for logging, debugging, or additional processing. Some other AI frameworks call this feature \"artifacts\". This separation allows you to provide rich context to the model while maintaining clean, structured return values for your application logic.", "url": "https://ai.pydantic.dev/tools-advanced/index.html#function-tool-output", "page": "tools-advanced/index.html", "source_site": "pydantic_ai"}
{"title": "Advanced Tool Returns", "anchor": "advanced-tool-returns", "heading_level": 3, "md_text": "For scenarios where you need more control over both the tool's return value and the content sent to the model, you can use [ ToolReturn ](../api/messages/index.html#pydantic_ai.messages.ToolReturn). This is particularly useful when you want to: * Provide rich multi-modal content (images, documents, etc.) to the model as context * Separate the programmatic return value from the model's context * Include additional metadata that shouldn't be sent to the LLM Here's an example of a computer automation tool that captures screenshots and provides visual feedback: advanced\\_tool\\_return.py * ** return_value **: The actual return value used in the tool response. This is what gets serialized and sent back to the model as the tool's result. * ** content **: A sequence of content (text, images, documents, etc.) that provides additional context to the model. This appears as a separate user message. * ** metadata **: Optional metadata that your application can access but is not sent to the LLM. Useful for logging, debugging, or additional processing. Some other AI frameworks call this feature \"artifacts\". This separation allows you to provide rich context to the model while maintaining clean, structured return values for your application logic.", "url": "https://ai.pydantic.dev/tools-advanced/index.html#advanced-tool-returns", "page": "tools-advanced/index.html", "source_site": "pydantic_ai"}
{"title": "Custom Tool Schema", "anchor": "custom-tool-schema", "heading_level": 2, "md_text": "If you have a function that lacks appropriate documentation (i.e. poorly named, no type information, poor docstring, use of \\*args or \\*\\*kwargs and suchlike) then you can still turn it into a tool that can be effectively used by the agent with the [ Tool.from_schema ](../api/tools/index.html#pydantic_ai.tools.Tool.from_schema) function. With this you provide the name, description, JSON schema, and whether the function takes a RunContext for the function directly: Please note that validation of the tool arguments will not be performed, and this will pass all arguments as keyword arguments.", "url": "https://ai.pydantic.dev/tools-advanced/index.html#custom-tool-schema", "page": "tools-advanced/index.html", "source_site": "pydantic_ai"}
{"title": "Dynamic Tools", "anchor": "tool-prepare", "heading_level": 2, "md_text": "Tools can optionally be defined with another function: prepare , which is called at each step of a run to customize the definition of the tool passed to the model, or omit the tool completely from that step. A prepare method can be registered via the prepare kwarg to any of the tool registration mechanisms: * [ @agent.tool ](../api/agent/index.html#pydantic_ai.agent.Agent.tool) decorator * [ @agent.tool_plain ](../api/agent/index.html#pydantic_ai.agent.Agent.tool_plain) decorator * [ Tool ](../api/tools/index.html#pydantic_ai.tools.Tool) dataclass The prepare method, should be of type [ ToolPrepareFunc ](../api/tools/index.html#pydantic_ai.tools.ToolPrepareFunc), a function which takes [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext) and a pre-built [ ToolDefinition ](../api/tools/index.html#pydantic_ai.tools.ToolDefinition), and should either return that ToolDefinition with or without modifying it, return a new ToolDefinition , or return None to indicate this tools should not be registered for that step. Here's a simple prepare method that only includes the tool if the value of the dependency is 42 . As with the previous example, we use [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) to demonstrate the behavior without calling a real model. tool\\_only\\_if\\_42.py *(This example is complete, it can be run \"as is\")* Here's a more complex example where we change the description of the name parameter to based on the value of deps For the sake of variation, we create this tool using the [ Tool ](../api/tools/index.html#pydantic_ai.tools.Tool) dataclass. customize\\_name.py *(This example is complete, it can be run \"as is\")* ### Agent-wide Dynamic Tools In addition to per-tool prepare methods, you can also define an agent-wide prepare_tools function. This function is called at each step of a run and allows you to filter or modify the list of all tool definitions available to the agent for that step. This is especially useful if you want to enable or disable multiple tools at once, or apply global logic based on the current context. The prepare_tools function should be of type [ ToolsPrepareFunc ](../api/tools/index.html#pydantic_ai.tools.ToolsPrepareFunc), which takes the [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext) and a list of [ ToolDefinition ](../api/tools/index.html#pydantic_ai.tools.ToolDefinition), and returns a new list of tool definitions (or None to disable all tools for that step). Note The list of tool definitions passed to prepare_tools includes both regular function tools and tools from any [toolsets](../toolsets/index.html) registered on the agent, but not [output tools](../output/index.html#tool-output). To modify output tools, you can set a prepare_output_tools function instead. Here's an example that makes all tools strict if the model is an OpenAI model: agent\\_prepare\\_tools\\_customize.py *(This example is complete, it can be run \"as is\")* Here's another example that conditionally filters out the tools by name if the dependency ( ctx.deps ) is True : agent\\_prepare\\_tools\\_filter\\_out.py *(This example is complete, it can be run \"as is\")* You can use prepare_tools to: * Dynamically enable or disable tools based on the current model, dependencies, or other context * Modify tool definitions globally (e.g., set all tools to strict mode, change descriptions, etc.) If both per-tool prepare and agent-wide prepare_tools are used, the per-tool prepare is applied first to each tool, and then prepare_tools is called with the resulting list of tool definitions.", "url": "https://ai.pydantic.dev/tools-advanced/index.html#tool-prepare", "page": "tools-advanced/index.html", "source_site": "pydantic_ai"}
{"title": "Agent-wide Dynamic Tools", "anchor": "prepare-tools", "heading_level": 3, "md_text": "In addition to per-tool prepare methods, you can also define an agent-wide prepare_tools function. This function is called at each step of a run and allows you to filter or modify the list of all tool definitions available to the agent for that step. This is especially useful if you want to enable or disable multiple tools at once, or apply global logic based on the current context. The prepare_tools function should be of type [ ToolsPrepareFunc ](../api/tools/index.html#pydantic_ai.tools.ToolsPrepareFunc), which takes the [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext) and a list of [ ToolDefinition ](../api/tools/index.html#pydantic_ai.tools.ToolDefinition), and returns a new list of tool definitions (or None to disable all tools for that step). Note The list of tool definitions passed to prepare_tools includes both regular function tools and tools from any [toolsets](../toolsets/index.html) registered on the agent, but not [output tools](../output/index.html#tool-output). To modify output tools, you can set a prepare_output_tools function instead. Here's an example that makes all tools strict if the model is an OpenAI model: agent\\_prepare\\_tools\\_customize.py *(This example is complete, it can be run \"as is\")* Here's another example that conditionally filters out the tools by name if the dependency ( ctx.deps ) is True : agent\\_prepare\\_tools\\_filter\\_out.py *(This example is complete, it can be run \"as is\")* You can use prepare_tools to: * Dynamically enable or disable tools based on the current model, dependencies, or other context * Modify tool definitions globally (e.g., set all tools to strict mode, change descriptions, etc.) If both per-tool prepare and agent-wide prepare_tools are used, the per-tool prepare is applied first to each tool, and then prepare_tools is called with the resulting list of tool definitions.", "url": "https://ai.pydantic.dev/tools-advanced/index.html#prepare-tools", "page": "tools-advanced/index.html", "source_site": "pydantic_ai"}
{"title": "Tool Execution and Retries", "anchor": "tool-retries", "heading_level": 2, "md_text": "When a tool is executed, its arguments (provided by the LLM) are first validated against the function's signature using Pydantic. If validation fails (e.g., due to incorrect types or missing required arguments), a ValidationError is raised, and the framework automatically generates a [ RetryPromptPart ](../api/messages/index.html#pydantic_ai.messages.RetryPromptPart) containing the validation details. This prompt is sent back to the LLM, informing it of the error and allowing it to correct the parameters and retry the tool call. Beyond automatic validation errors, the tool's own internal logic can also explicitly request a retry by raising the [ ModelRetry ](../api/exceptions/index.html#pydantic_ai.exceptions.ModelRetry) exception. This is useful for situations where the parameters were technically valid, but an issue occurred during execution (like a transient network error, or the tool determining the initial attempt needs modification). Raising ModelRetry also generates a RetryPromptPart containing the exception message, which is sent back to the LLM to guide its next attempt. Both ValidationError and ModelRetry respect the retries setting configured on the Tool or Agent . ### Parallel tool calls & concurrency When a model returns multiple tool calls in one response, Pydantic AI schedules them concurrently using asyncio.create_task . If a tool requires sequential/serial execution, you can pass the [ sequential ](../api/tools/index.html#pydantic_ai.tools.ToolDefinition.sequential) flag when registering the tool, or wrap the agent run in the [ with agent.sequential_tool_calls() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.sequential_tool_calls) context manager. Async functions are run on the event loop, while sync functions are offloaded to threads. To get the best performance, *always* use an async function *unless* you're doing blocking I/O (and there's no way to use a non-blocking library instead) or CPU-bound work (like numpy or scikit-learn operations), so that simple functions are not offloaded to threads unnecessarily. Limiting tool executions You can cap tool executions within a run using [ UsageLimits(tool_calls_limit=...) ](../agents/index.html#usage-limits). The counter increments only after a successful tool invocation. Output tools (used for [structured output](../output/index.html)) are not counted in the tool_calls metric.", "url": "https://ai.pydantic.dev/tools-advanced/index.html#tool-retries", "page": "tools-advanced/index.html", "source_site": "pydantic_ai"}
{"title": "Parallel tool calls & concurrency", "anchor": "parallel-tool-calls-concurrency", "heading_level": 3, "md_text": "When a model returns multiple tool calls in one response, Pydantic AI schedules them concurrently using asyncio.create_task . If a tool requires sequential/serial execution, you can pass the [ sequential ](../api/tools/index.html#pydantic_ai.tools.ToolDefinition.sequential) flag when registering the tool, or wrap the agent run in the [ with agent.sequential_tool_calls() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.sequential_tool_calls) context manager. Async functions are run on the event loop, while sync functions are offloaded to threads. To get the best performance, *always* use an async function *unless* you're doing blocking I/O (and there's no way to use a non-blocking library instead) or CPU-bound work (like numpy or scikit-learn operations), so that simple functions are not offloaded to threads unnecessarily. Limiting tool executions You can cap tool executions within a run using [ UsageLimits(tool_calls_limit=...) ](../agents/index.html#usage-limits). The counter increments only after a successful tool invocation. Output tools (used for [structured output](../output/index.html)) are not counted in the tool_calls metric.", "url": "https://ai.pydantic.dev/tools-advanced/index.html#parallel-tool-calls-concurrency", "page": "tools-advanced/index.html", "source_site": "pydantic_ai"}
{"title": "See Also", "anchor": "see-also", "heading_level": 2, "md_text": "* [Function Tools](../tools/index.html) - Basic tool concepts and registration * [Toolsets](../toolsets/index.html) - Managing collections of tools * [Deferred Tools](../deferred-tools/index.html) - Tools requiring approval or external execution * [Third-Party Tools](../third-party-tools/index.html) - Integrations with external tool libraries", "url": "https://ai.pydantic.dev/tools-advanced/index.html#see-also", "page": "tools-advanced/index.html", "source_site": "pydantic_ai"}
{"title": "Toolsets", "anchor": "toolsets", "heading_level": 1, "md_text": "A toolset represents a collection of [tools](../tools/index.html) that can be registered with an agent in one go. They can be reused by different agents, swapped out at runtime or during testing, and composed in order to dynamically filter which tools are available, modify tool definitions, or change tool execution behavior. A toolset can contain locally defined functions, depend on an external service to provide them, or implement custom logic to list available tools and handle them being called. Toolsets are used (among many other things) to define [MCP servers](../mcp/client/index.html) available to an agent. Pydantic AI includes many kinds of toolsets which are described below, and you can define a [custom toolset](index.html#building-a-custom-toolset) by inheriting from the [ AbstractToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset) class. The toolsets that will be available during an agent run can be specified in four different ways: * at agent construction time, via the [ toolsets ](../api/agent/index.html#pydantic_ai.agent.Agent.__init__) keyword argument to Agent , which takes toolset instances as well as functions that generate toolsets [dynamically](index.html#dynamically-building-a-toolset) based on the agent [run context](../api/tools/index.html#pydantic_ai.tools.RunContext) * at agent run time, via the toolsets keyword argument to [ agent.run() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run), [ agent.run_sync() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_sync), [ agent.run_stream() ](../api/agent/index.html#pydantic_ai.agent.AbstractAgent.run_stream), or [ agent.iter() ](../api/agent/index.html#pydantic_ai.agent.Agent.iter). These toolsets will be additional to those registered on the Agent * [dynamically](index.html#dynamically-building-a-toolset), via the [ @agent.toolset ](../api/agent/index.html#pydantic_ai.agent.Agent.toolset) decorator which lets you build a toolset based on the agent [run context](../api/tools/index.html#pydantic_ai.tools.RunContext) * as a contextual override, via the toolsets keyword argument to the [ agent.override() ](../api/agent/index.html#pydantic_ai.agent.Agent.iter) context manager. These toolsets will replace those provided at agent construction or run time during the life of the context manager toolsets.py 1. The [ FunctionToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.FunctionToolset) will be explained in detail in the next section. 2. We're using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here because it makes it easy to see which tools were available on each run. 3. This extra_toolset will be ignored because we're inside an override context. *(This example is complete, it can be run \"as is\")* ## Function Toolset As the name suggests, a [ FunctionToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.FunctionToolset) makes locally defined functions available as tools. Functions can be added as tools in three different ways: * via the [ @toolset.tool ](../api/toolsets/index.html#pydantic_ai.toolsets.FunctionToolset.tool) decorator * via the [ tools ](../api/toolsets/index.html#pydantic_ai.toolsets.FunctionToolset.__init__) keyword argument to the constructor which can take either plain functions, or instances of [ Tool ](../api/tools/index.html#pydantic_ai.tools.Tool) * via the [ toolset.add_function() ](../api/toolsets/index.html#pydantic_ai.toolsets.FunctionToolset.add_function) and [ toolset.add_tool() ](../api/toolsets/index.html#pydantic_ai.toolsets.FunctionToolset.add_tool) methods which can take a plain function or an instance of [ Tool ](../api/tools/index.html#pydantic_ai.tools.Tool) respectively Functions registered in any of these ways can define an initial ctx: RunContext argument in order to receive the agent [run context](../api/tools/index.html#pydantic_ai.tools.RunContext). The add_function() and add_tool() methods can also be used from a tool function to dynamically register new tools during a run to be available in future run steps. function\\_toolset.py 1. We're using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here because it makes it easy to see which tools were available on each run. *(This example is complete, it can be run \"as is\")* ## Toolset Composition Toolsets can be composed to dynamically filter which tools are available, modify tool definitions, or change tool execution behavior. Multiple toolsets can also be combined into one. ### Combining Toolsets [ CombinedToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.CombinedToolset) takes a list of toolsets and lets them be used as one. combined\\_toolset.py 1. We're using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here because it makes it easy to see which tools were available on each run. *(This example is complete, it can be run \"as is\")* ### Filtering Tools [ FilteredToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.FilteredToolset) wraps a toolset and filters available tools ahead of each step of the run based on a user-defined function that is passed the agent [run context](../api/tools/index.html#pydantic_ai.tools.RunContext) and each tool's [ ToolDefinition ](../api/tools/index.html#pydantic_ai.tools.ToolDefinition) and returns a boolean to indicate whether or not a given tool should be available. To easily chain different modifications, you can also call [ filtered() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.filtered) on any toolset instead of directly constructing a FilteredToolset . filtered\\_toolset.py 1. We're using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here because it makes it easy to see which tools were available on each run. *(This example is complete, it can be run \"as is\")* ### Prefixing Tool Names [ PrefixedToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.PrefixedToolset) wraps a toolset and adds a prefix to each tool name to prevent tool name conflicts between different toolsets. To easily chain different modifications, you can also call [ prefixed() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.prefixed) on any toolset instead of directly constructing a PrefixedToolset . combined\\_toolset.py 1. We're using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here because it makes it easy to see which tools were available on each run. *(This example is complete, it can be run \"as is\")* ### Renaming Tools [ RenamedToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.RenamedToolset) wraps a toolset and lets you rename tools using a dictionary mapping new names to original names. This is useful when the names provided by a toolset are ambiguous or would conflict with tools defined by other toolsets, but [prefixing them](index.html#prefixing-tool-names) creates a name that is unnecessarily long or could be confusing to the model. To easily chain different modifications, you can also call [ renamed() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.renamed) on any toolset instead of directly constructing a RenamedToolset . renamed\\_toolset.py 1. We're using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here because it makes it easy to see which tools were available on each run. *(This example is complete, it can be run \"as is\")* ### Dynamic Tool Definitions [ PreparedToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.PreparedToolset) lets you modify the entire list of available tools ahead of each step of the agent run using a user-defined function that takes the agent [run context](../api/tools/index.html#pydantic_ai.tools.RunContext) and a list of [ ToolDefinition s](../api/tools/index.html#pydantic_ai.tools.ToolDefinition) and returns a list of modified ToolDefinition s. This is the toolset-specific equivalent of the [ prepare_tools ](../tools-advanced/index.html#prepare-tools) argument to Agent that prepares all tool definitions registered on an agent across toolsets. Note that it is not possible to add or rename tools using PreparedToolset . Instead, you can use [ FunctionToolset.add_function() ](index.html#function-toolset) or [ RenamedToolset ](index.html#renaming-tools). To easily chain different modifications, you can also call [ prepared() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.prepared) on any toolset instead of directly constructing a PreparedToolset . prepared\\_toolset.py 1. We're using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here because it makes it easy to see which tools were available on each run. ### Requiring Tool Approval [ ApprovalRequiredToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.ApprovalRequiredToolset) wraps a toolset and lets you dynamically [require approval](../deferred-tools/index.html#human-in-the-loop-tool-approval) for a given tool call based on a user-defined function that is passed the agent [run context](../api/tools/index.html#pydantic_ai.tools.RunContext), the tool's [ ToolDefinition ](../api/tools/index.html#pydantic_ai.tools.ToolDefinition), and the validated tool call arguments. If no function is provided, all tool calls will require approval. To easily chain different modifications, you can also call [ approval_required() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.approval_required) on any toolset instead of directly constructing a ApprovalRequiredToolset . See the [Human-in-the-Loop Tool Approval](../deferred-tools/index.html#human-in-the-loop-tool-approval) documentation for more information on how to handle agent runs that call tools that require approval and how to pass in the results. approval\\_required\\_toolset.py 1. We're using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here because it makes it easy to specify which tools to call. *(This example is complete, it can be run \"as is\")* ### Changing Tool Execution [ WrapperToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.WrapperToolset) wraps another toolset and delegates all responsibility to it. It is is a no-op by default, but you can subclass WrapperToolset to change the wrapped toolset's tool execution behavior by overriding the [ call_tool() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.call_tool) method. logging\\_toolset.py 1. All docs examples are tested in CI and their their output is verified, so we need LOG to always have the same order whenever this code is run. Since the tools could finish in any order, we sleep an increasing amount of time based on which number tool call we are to ensure that they finish (and log) in the same order they were called in. 2. We use [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here as it will automatically call each tool. *(This example is complete, it can be run \"as is\")* ## External Toolset If your agent needs to be able to call [external tools](../deferred-tools/index.html#external-tool-execution) that are provided and executed by an upstream service or frontend, you can build an [ ExternalToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.ExternalToolset) from a list of [ ToolDefinition s](../api/tools/index.html#pydantic_ai.tools.ToolDefinition) containing the tool names, arguments JSON schemas, and descriptions. When the model calls an external tool, the call is considered to be [\"deferred\"](../deferred-tools/index.html#deferred-tools), and the agent run will end with a [ DeferredToolRequests ](../api/output/index.html#pydantic_ai.output.DeferredToolRequests) output object with a calls list holding [ ToolCallPart s](../api/messages/index.html#pydantic_ai.messages.ToolCallPart) containing the tool name, validated arguments, and a unique tool call ID, which are expected to be passed to the upstream service or frontend that will produce the results. When the tool call results are received from the upstream service or frontend, you can build a [ DeferredToolResults ](../api/tools/index.html#pydantic_ai.tools.DeferredToolResults) object with a calls dictionary that maps each tool call ID to an arbitrary value to be returned to the model, a [ ToolReturn ](../tools-advanced/index.html#advanced-tool-returns) object, or a [ ModelRetry ](../api/exceptions/index.html#pydantic_ai.exceptions.ModelRetry) exception in case the tool call failed and the model should [try again](../tools-advanced/index.html#tool-retries). This DeferredToolResults object can then be provided to one of the agent run methods as deferred_tool_results , alongside the original run's [message history](../message-history/index.html). Note that you need to add DeferredToolRequests to the Agent 's or agent.run() 's [ output_type ](../output/index.html#structured-output) so that the possible types of the agent run output are correctly inferred. For more information, see the [Deferred Tools](../deferred-tools/index.html#deferred-tools) documentation. To demonstrate, let us first define a simple agent *without* deferred tools: deferred\\_toolset\\_agent.py Next, let's define a function that represents a hypothetical \"run agent\" API endpoint that can be called by the frontend and takes a list of messages to send to the model, a list of frontend tool definitions, and optional deferred tool results. This is where ExternalToolset , DeferredToolRequests , and DeferredToolResults come in: deferred\\_toolset\\_api.py 1. As mentioned in the [Deferred Tools](../deferred-tools/index.html#deferred-tools) documentation, these toolsets are additional to those provided to the Agent constructor 2. As mentioned in the [Deferred Tools](../deferred-tools/index.html#deferred-tools) documentation, this output_type overrides the one provided to the Agent constructor, so we have to make sure to not lose it 3. We don't include an user_prompt keyword argument as we expect the frontend to provide it via messages Now, imagine that the code below is implemented on the frontend, and run_agent stands in for an API call to the backend that runs the agent. This is where we actually execute the deferred tool calls and start a new run with the new result included: deferred\\_tools.py 1. Imagine that this returns the frontend [ navigator.language ](https://developer.mozilla.org/en-US/docs/Web/API/Navigator/language). *(This example is complete, it can be run \"as is\")* ## Dynamically Building a Toolset Toolsets can be built dynamically ahead of each agent run or run step using a function that takes the agent [run context](../api/tools/index.html#pydantic_ai.tools.RunContext) and returns a toolset or None . This is useful when a toolset (like an MCP server) depends on information specific to an agent run, like its [dependencies](../dependencies/index.html). To register a dynamic toolset, you can pass a function that takes [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext) to the toolsets argument of the Agent constructor, or you can wrap a compliant function in the [ @agent.toolset ](../api/agent/index.html#pydantic_ai.agent.Agent.toolset) decorator. By default, the function will be called again ahead of each agent run step. If you are using the decorator, you can optionally provide a per_run_step=False argument to indicate that the toolset only needs to be built once for the entire run. dynamic\\_toolset.py 1. We're using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here because it makes it easy to see which tools were available on each run. 2. We're using the agent's dependencies to give the toggle tool access to the active via the RunContext argument. 3. This shows the available tools *after* the toggle tool was executed, as the \"last model request\" was the one that returned the toggle tool result to the model. *(This example is complete, it can be run \"as is\")* ## Building a Custom Toolset To define a fully custom toolset with its own logic to list available tools and handle them being called, you can subclass [ AbstractToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset) and implement the [ get_tools() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.get_tools) and [ call_tool() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.call_tool) methods. If you want to reuse a network connection or session across tool listings and calls during an agent run, you can implement [ __aenter__() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.__aenter__) and [ __aexit__() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.__aexit__). ## Third-Party Toolsets ### MCP Servers Pydantic AI provides two toolsets that allow an agent to connect to and call tools on local and remote MCP Servers: 1. MCPServer : the [MCP SDK-based Client](../mcp/client/index.html) which offers more direct control by leveraging the MCP SDK directly 2. FastMCPToolset : the [FastMCP-based Client](../mcp/fastmcp-client/index.html) which offers additional capabilities like Tool Transformation, simpler OAuth configuration, and more. ### LangChain Tools If you'd like to use tools or a [toolkit](https://python.langchain.com/docs/concepts/tools/#toolkits) from LangChain's [community tool library](https://python.langchain.com/docs/integrations/tools/) with Pydantic AI, you can use the [ LangChainToolset ](../api/ext/index.html#pydantic_ai.ext.langchain.LangChainToolset) which takes a list of LangChain tools. Note that Pydantic AI will not validate the arguments in this case -- it's up to the model to provide arguments matching the schema specified by the LangChain tool, and up to the LangChain tool to raise an error if the arguments are invalid. You will need to install the langchain-community package and any others required by the tools in question. ### ACI.dev Tools If you'd like to use tools from the [ACI.dev tool library](https://www.aci.dev/tools) with Pydantic AI, you can use the [ ACIToolset ](../api/ext/index.html#pydantic_ai.ext.aci.ACIToolset) [toolset](index.html) which takes a list of ACI tool names as well as the linked_account_owner_id . Note that Pydantic AI will not validate the arguments in this case -- it's up to the model to provide arguments matching the schema specified by the ACI tool, and up to the ACI tool to raise an error if the arguments are invalid. You will need to install the aci-sdk package, set your ACI API key in the ACI_API_KEY environment variable, and pass your ACI \"linked account owner ID\" to the function.", "url": "https://ai.pydantic.dev/toolsets/index.html#toolsets", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "Function Toolset", "anchor": "function-toolset", "heading_level": 2, "md_text": "As the name suggests, a [ FunctionToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.FunctionToolset) makes locally defined functions available as tools. Functions can be added as tools in three different ways: * via the [ @toolset.tool ](../api/toolsets/index.html#pydantic_ai.toolsets.FunctionToolset.tool) decorator * via the [ tools ](../api/toolsets/index.html#pydantic_ai.toolsets.FunctionToolset.__init__) keyword argument to the constructor which can take either plain functions, or instances of [ Tool ](../api/tools/index.html#pydantic_ai.tools.Tool) * via the [ toolset.add_function() ](../api/toolsets/index.html#pydantic_ai.toolsets.FunctionToolset.add_function) and [ toolset.add_tool() ](../api/toolsets/index.html#pydantic_ai.toolsets.FunctionToolset.add_tool) methods which can take a plain function or an instance of [ Tool ](../api/tools/index.html#pydantic_ai.tools.Tool) respectively Functions registered in any of these ways can define an initial ctx: RunContext argument in order to receive the agent [run context](../api/tools/index.html#pydantic_ai.tools.RunContext). The add_function() and add_tool() methods can also be used from a tool function to dynamically register new tools during a run to be available in future run steps. function\\_toolset.py 1. We're using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here because it makes it easy to see which tools were available on each run. *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/toolsets/index.html#function-toolset", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "Toolset Composition", "anchor": "toolset-composition", "heading_level": 2, "md_text": "Toolsets can be composed to dynamically filter which tools are available, modify tool definitions, or change tool execution behavior. Multiple toolsets can also be combined into one. ### Combining Toolsets [ CombinedToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.CombinedToolset) takes a list of toolsets and lets them be used as one. combined\\_toolset.py 1. We're using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here because it makes it easy to see which tools were available on each run. *(This example is complete, it can be run \"as is\")* ### Filtering Tools [ FilteredToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.FilteredToolset) wraps a toolset and filters available tools ahead of each step of the run based on a user-defined function that is passed the agent [run context](../api/tools/index.html#pydantic_ai.tools.RunContext) and each tool's [ ToolDefinition ](../api/tools/index.html#pydantic_ai.tools.ToolDefinition) and returns a boolean to indicate whether or not a given tool should be available. To easily chain different modifications, you can also call [ filtered() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.filtered) on any toolset instead of directly constructing a FilteredToolset . filtered\\_toolset.py 1. We're using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here because it makes it easy to see which tools were available on each run. *(This example is complete, it can be run \"as is\")* ### Prefixing Tool Names [ PrefixedToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.PrefixedToolset) wraps a toolset and adds a prefix to each tool name to prevent tool name conflicts between different toolsets. To easily chain different modifications, you can also call [ prefixed() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.prefixed) on any toolset instead of directly constructing a PrefixedToolset . combined\\_toolset.py 1. We're using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here because it makes it easy to see which tools were available on each run. *(This example is complete, it can be run \"as is\")* ### Renaming Tools [ RenamedToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.RenamedToolset) wraps a toolset and lets you rename tools using a dictionary mapping new names to original names. This is useful when the names provided by a toolset are ambiguous or would conflict with tools defined by other toolsets, but [prefixing them](index.html#prefixing-tool-names) creates a name that is unnecessarily long or could be confusing to the model. To easily chain different modifications, you can also call [ renamed() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.renamed) on any toolset instead of directly constructing a RenamedToolset . renamed\\_toolset.py 1. We're using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here because it makes it easy to see which tools were available on each run. *(This example is complete, it can be run \"as is\")* ### Dynamic Tool Definitions [ PreparedToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.PreparedToolset) lets you modify the entire list of available tools ahead of each step of the agent run using a user-defined function that takes the agent [run context](../api/tools/index.html#pydantic_ai.tools.RunContext) and a list of [ ToolDefinition s](../api/tools/index.html#pydantic_ai.tools.ToolDefinition) and returns a list of modified ToolDefinition s. This is the toolset-specific equivalent of the [ prepare_tools ](../tools-advanced/index.html#prepare-tools) argument to Agent that prepares all tool definitions registered on an agent across toolsets. Note that it is not possible to add or rename tools using PreparedToolset . Instead, you can use [ FunctionToolset.add_function() ](index.html#function-toolset) or [ RenamedToolset ](index.html#renaming-tools). To easily chain different modifications, you can also call [ prepared() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.prepared) on any toolset instead of directly constructing a PreparedToolset . prepared\\_toolset.py 1. We're using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here because it makes it easy to see which tools were available on each run. ### Requiring Tool Approval [ ApprovalRequiredToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.ApprovalRequiredToolset) wraps a toolset and lets you dynamically [require approval](../deferred-tools/index.html#human-in-the-loop-tool-approval) for a given tool call based on a user-defined function that is passed the agent [run context](../api/tools/index.html#pydantic_ai.tools.RunContext), the tool's [ ToolDefinition ](../api/tools/index.html#pydantic_ai.tools.ToolDefinition), and the validated tool call arguments. If no function is provided, all tool calls will require approval. To easily chain different modifications, you can also call [ approval_required() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.approval_required) on any toolset instead of directly constructing a ApprovalRequiredToolset . See the [Human-in-the-Loop Tool Approval](../deferred-tools/index.html#human-in-the-loop-tool-approval) documentation for more information on how to handle agent runs that call tools that require approval and how to pass in the results. approval\\_required\\_toolset.py 1. We're using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here because it makes it easy to specify which tools to call. *(This example is complete, it can be run \"as is\")* ### Changing Tool Execution [ WrapperToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.WrapperToolset) wraps another toolset and delegates all responsibility to it. It is is a no-op by default, but you can subclass WrapperToolset to change the wrapped toolset's tool execution behavior by overriding the [ call_tool() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.call_tool) method. logging\\_toolset.py 1. All docs examples are tested in CI and their their output is verified, so we need LOG to always have the same order whenever this code is run. Since the tools could finish in any order, we sleep an increasing amount of time based on which number tool call we are to ensure that they finish (and log) in the same order they were called in. 2. We use [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here as it will automatically call each tool. *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/toolsets/index.html#toolset-composition", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "Combining Toolsets", "anchor": "combining-toolsets", "heading_level": 3, "md_text": "[ CombinedToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.CombinedToolset) takes a list of toolsets and lets them be used as one. combined\\_toolset.py 1. We're using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here because it makes it easy to see which tools were available on each run. *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/toolsets/index.html#combining-toolsets", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "Filtering Tools", "anchor": "filtering-tools", "heading_level": 3, "md_text": "[ FilteredToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.FilteredToolset) wraps a toolset and filters available tools ahead of each step of the run based on a user-defined function that is passed the agent [run context](../api/tools/index.html#pydantic_ai.tools.RunContext) and each tool's [ ToolDefinition ](../api/tools/index.html#pydantic_ai.tools.ToolDefinition) and returns a boolean to indicate whether or not a given tool should be available. To easily chain different modifications, you can also call [ filtered() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.filtered) on any toolset instead of directly constructing a FilteredToolset . filtered\\_toolset.py 1. We're using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here because it makes it easy to see which tools were available on each run. *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/toolsets/index.html#filtering-tools", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "Prefixing Tool Names", "anchor": "prefixing-tool-names", "heading_level": 3, "md_text": "[ PrefixedToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.PrefixedToolset) wraps a toolset and adds a prefix to each tool name to prevent tool name conflicts between different toolsets. To easily chain different modifications, you can also call [ prefixed() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.prefixed) on any toolset instead of directly constructing a PrefixedToolset . combined\\_toolset.py 1. We're using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here because it makes it easy to see which tools were available on each run. *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/toolsets/index.html#prefixing-tool-names", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "Renaming Tools", "anchor": "renaming-tools", "heading_level": 3, "md_text": "[ RenamedToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.RenamedToolset) wraps a toolset and lets you rename tools using a dictionary mapping new names to original names. This is useful when the names provided by a toolset are ambiguous or would conflict with tools defined by other toolsets, but [prefixing them](index.html#prefixing-tool-names) creates a name that is unnecessarily long or could be confusing to the model. To easily chain different modifications, you can also call [ renamed() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.renamed) on any toolset instead of directly constructing a RenamedToolset . renamed\\_toolset.py 1. We're using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here because it makes it easy to see which tools were available on each run. *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/toolsets/index.html#renaming-tools", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "Dynamic Tool Definitions", "anchor": "preparing-tool-definitions", "heading_level": 3, "md_text": "[ PreparedToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.PreparedToolset) lets you modify the entire list of available tools ahead of each step of the agent run using a user-defined function that takes the agent [run context](../api/tools/index.html#pydantic_ai.tools.RunContext) and a list of [ ToolDefinition s](../api/tools/index.html#pydantic_ai.tools.ToolDefinition) and returns a list of modified ToolDefinition s. This is the toolset-specific equivalent of the [ prepare_tools ](../tools-advanced/index.html#prepare-tools) argument to Agent that prepares all tool definitions registered on an agent across toolsets. Note that it is not possible to add or rename tools using PreparedToolset . Instead, you can use [ FunctionToolset.add_function() ](index.html#function-toolset) or [ RenamedToolset ](index.html#renaming-tools). To easily chain different modifications, you can also call [ prepared() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.prepared) on any toolset instead of directly constructing a PreparedToolset . prepared\\_toolset.py 1. We're using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here because it makes it easy to see which tools were available on each run.", "url": "https://ai.pydantic.dev/toolsets/index.html#preparing-tool-definitions", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "Requiring Tool Approval", "anchor": "requiring-tool-approval", "heading_level": 3, "md_text": "[ ApprovalRequiredToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.ApprovalRequiredToolset) wraps a toolset and lets you dynamically [require approval](../deferred-tools/index.html#human-in-the-loop-tool-approval) for a given tool call based on a user-defined function that is passed the agent [run context](../api/tools/index.html#pydantic_ai.tools.RunContext), the tool's [ ToolDefinition ](../api/tools/index.html#pydantic_ai.tools.ToolDefinition), and the validated tool call arguments. If no function is provided, all tool calls will require approval. To easily chain different modifications, you can also call [ approval_required() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.approval_required) on any toolset instead of directly constructing a ApprovalRequiredToolset . See the [Human-in-the-Loop Tool Approval](../deferred-tools/index.html#human-in-the-loop-tool-approval) documentation for more information on how to handle agent runs that call tools that require approval and how to pass in the results. approval\\_required\\_toolset.py 1. We're using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here because it makes it easy to specify which tools to call. *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/toolsets/index.html#requiring-tool-approval", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "Changing Tool Execution", "anchor": "changing-tool-execution", "heading_level": 3, "md_text": "[ WrapperToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.WrapperToolset) wraps another toolset and delegates all responsibility to it. It is is a no-op by default, but you can subclass WrapperToolset to change the wrapped toolset's tool execution behavior by overriding the [ call_tool() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.call_tool) method. logging\\_toolset.py 1. All docs examples are tested in CI and their their output is verified, so we need LOG to always have the same order whenever this code is run. Since the tools could finish in any order, we sleep an increasing amount of time based on which number tool call we are to ensure that they finish (and log) in the same order they were called in. 2. We use [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here as it will automatically call each tool. *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/toolsets/index.html#changing-tool-execution", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "External Toolset", "anchor": "external-toolset", "heading_level": 2, "md_text": "If your agent needs to be able to call [external tools](../deferred-tools/index.html#external-tool-execution) that are provided and executed by an upstream service or frontend, you can build an [ ExternalToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.ExternalToolset) from a list of [ ToolDefinition s](../api/tools/index.html#pydantic_ai.tools.ToolDefinition) containing the tool names, arguments JSON schemas, and descriptions. When the model calls an external tool, the call is considered to be [\"deferred\"](../deferred-tools/index.html#deferred-tools), and the agent run will end with a [ DeferredToolRequests ](../api/output/index.html#pydantic_ai.output.DeferredToolRequests) output object with a calls list holding [ ToolCallPart s](../api/messages/index.html#pydantic_ai.messages.ToolCallPart) containing the tool name, validated arguments, and a unique tool call ID, which are expected to be passed to the upstream service or frontend that will produce the results. When the tool call results are received from the upstream service or frontend, you can build a [ DeferredToolResults ](../api/tools/index.html#pydantic_ai.tools.DeferredToolResults) object with a calls dictionary that maps each tool call ID to an arbitrary value to be returned to the model, a [ ToolReturn ](../tools-advanced/index.html#advanced-tool-returns) object, or a [ ModelRetry ](../api/exceptions/index.html#pydantic_ai.exceptions.ModelRetry) exception in case the tool call failed and the model should [try again](../tools-advanced/index.html#tool-retries). This DeferredToolResults object can then be provided to one of the agent run methods as deferred_tool_results , alongside the original run's [message history](../message-history/index.html). Note that you need to add DeferredToolRequests to the Agent 's or agent.run() 's [ output_type ](../output/index.html#structured-output) so that the possible types of the agent run output are correctly inferred. For more information, see the [Deferred Tools](../deferred-tools/index.html#deferred-tools) documentation. To demonstrate, let us first define a simple agent *without* deferred tools: deferred\\_toolset\\_agent.py Next, let's define a function that represents a hypothetical \"run agent\" API endpoint that can be called by the frontend and takes a list of messages to send to the model, a list of frontend tool definitions, and optional deferred tool results. This is where ExternalToolset , DeferredToolRequests , and DeferredToolResults come in: deferred\\_toolset\\_api.py 1. As mentioned in the [Deferred Tools](../deferred-tools/index.html#deferred-tools) documentation, these toolsets are additional to those provided to the Agent constructor 2. As mentioned in the [Deferred Tools](../deferred-tools/index.html#deferred-tools) documentation, this output_type overrides the one provided to the Agent constructor, so we have to make sure to not lose it 3. We don't include an user_prompt keyword argument as we expect the frontend to provide it via messages Now, imagine that the code below is implemented on the frontend, and run_agent stands in for an API call to the backend that runs the agent. This is where we actually execute the deferred tool calls and start a new run with the new result included: deferred\\_tools.py 1. Imagine that this returns the frontend [ navigator.language ](https://developer.mozilla.org/en-US/docs/Web/API/Navigator/language). *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/toolsets/index.html#external-toolset", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "Dynamically Building a Toolset", "anchor": "dynamically-building-a-toolset", "heading_level": 2, "md_text": "Toolsets can be built dynamically ahead of each agent run or run step using a function that takes the agent [run context](../api/tools/index.html#pydantic_ai.tools.RunContext) and returns a toolset or None . This is useful when a toolset (like an MCP server) depends on information specific to an agent run, like its [dependencies](../dependencies/index.html). To register a dynamic toolset, you can pass a function that takes [ RunContext ](../api/tools/index.html#pydantic_ai.tools.RunContext) to the toolsets argument of the Agent constructor, or you can wrap a compliant function in the [ @agent.toolset ](../api/agent/index.html#pydantic_ai.agent.Agent.toolset) decorator. By default, the function will be called again ahead of each agent run step. If you are using the decorator, you can optionally provide a per_run_step=False argument to indicate that the toolset only needs to be built once for the entire run. dynamic\\_toolset.py 1. We're using [ TestModel ](../api/models/test/index.html#pydantic_ai.models.test.TestModel) here because it makes it easy to see which tools were available on each run. 2. We're using the agent's dependencies to give the toggle tool access to the active via the RunContext argument. 3. This shows the available tools *after* the toggle tool was executed, as the \"last model request\" was the one that returned the toggle tool result to the model. *(This example is complete, it can be run \"as is\")*", "url": "https://ai.pydantic.dev/toolsets/index.html#dynamically-building-a-toolset", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "Building a Custom Toolset", "anchor": "building-a-custom-toolset", "heading_level": 2, "md_text": "To define a fully custom toolset with its own logic to list available tools and handle them being called, you can subclass [ AbstractToolset ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset) and implement the [ get_tools() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.get_tools) and [ call_tool() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.call_tool) methods. If you want to reuse a network connection or session across tool listings and calls during an agent run, you can implement [ __aenter__() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.__aenter__) and [ __aexit__() ](../api/toolsets/index.html#pydantic_ai.toolsets.AbstractToolset.__aexit__).", "url": "https://ai.pydantic.dev/toolsets/index.html#building-a-custom-toolset", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "Third-Party Toolsets", "anchor": "third-party-toolsets", "heading_level": 2, "md_text": "### MCP Servers Pydantic AI provides two toolsets that allow an agent to connect to and call tools on local and remote MCP Servers: 1. MCPServer : the [MCP SDK-based Client](../mcp/client/index.html) which offers more direct control by leveraging the MCP SDK directly 2. FastMCPToolset : the [FastMCP-based Client](../mcp/fastmcp-client/index.html) which offers additional capabilities like Tool Transformation, simpler OAuth configuration, and more. ### LangChain Tools If you'd like to use tools or a [toolkit](https://python.langchain.com/docs/concepts/tools/#toolkits) from LangChain's [community tool library](https://python.langchain.com/docs/integrations/tools/) with Pydantic AI, you can use the [ LangChainToolset ](../api/ext/index.html#pydantic_ai.ext.langchain.LangChainToolset) which takes a list of LangChain tools. Note that Pydantic AI will not validate the arguments in this case -- it's up to the model to provide arguments matching the schema specified by the LangChain tool, and up to the LangChain tool to raise an error if the arguments are invalid. You will need to install the langchain-community package and any others required by the tools in question. ### ACI.dev Tools If you'd like to use tools from the [ACI.dev tool library](https://www.aci.dev/tools) with Pydantic AI, you can use the [ ACIToolset ](../api/ext/index.html#pydantic_ai.ext.aci.ACIToolset) [toolset](index.html) which takes a list of ACI tool names as well as the linked_account_owner_id . Note that Pydantic AI will not validate the arguments in this case -- it's up to the model to provide arguments matching the schema specified by the ACI tool, and up to the ACI tool to raise an error if the arguments are invalid. You will need to install the aci-sdk package, set your ACI API key in the ACI_API_KEY environment variable, and pass your ACI \"linked account owner ID\" to the function.", "url": "https://ai.pydantic.dev/toolsets/index.html#third-party-toolsets", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "MCP Servers", "anchor": "mcp-servers", "heading_level": 3, "md_text": "Pydantic AI provides two toolsets that allow an agent to connect to and call tools on local and remote MCP Servers: 1. MCPServer : the [MCP SDK-based Client](../mcp/client/index.html) which offers more direct control by leveraging the MCP SDK directly 2. FastMCPToolset : the [FastMCP-based Client](../mcp/fastmcp-client/index.html) which offers additional capabilities like Tool Transformation, simpler OAuth configuration, and more.", "url": "https://ai.pydantic.dev/toolsets/index.html#mcp-servers", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "LangChain Tools", "anchor": "langchain-tools", "heading_level": 3, "md_text": "If you'd like to use tools or a [toolkit](https://python.langchain.com/docs/concepts/tools/#toolkits) from LangChain's [community tool library](https://python.langchain.com/docs/integrations/tools/) with Pydantic AI, you can use the [ LangChainToolset ](../api/ext/index.html#pydantic_ai.ext.langchain.LangChainToolset) which takes a list of LangChain tools. Note that Pydantic AI will not validate the arguments in this case -- it's up to the model to provide arguments matching the schema specified by the LangChain tool, and up to the LangChain tool to raise an error if the arguments are invalid. You will need to install the langchain-community package and any others required by the tools in question.", "url": "https://ai.pydantic.dev/toolsets/index.html#langchain-tools", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
{"title": "ACI.dev Tools", "anchor": "aci-tools", "heading_level": 3, "md_text": "If you'd like to use tools from the [ACI.dev tool library](https://www.aci.dev/tools) with Pydantic AI, you can use the [ ACIToolset ](../api/ext/index.html#pydantic_ai.ext.aci.ACIToolset) [toolset](index.html) which takes a list of ACI tool names as well as the linked_account_owner_id . Note that Pydantic AI will not validate the arguments in this case -- it's up to the model to provide arguments matching the schema specified by the ACI tool, and up to the ACI tool to raise an error if the arguments are invalid. You will need to install the aci-sdk package, set your ACI API key in the ACI_API_KEY environment variable, and pass your ACI \"linked account owner ID\" to the function.", "url": "https://ai.pydantic.dev/toolsets/index.html#aci-tools", "page": "toolsets/index.html", "source_site": "pydantic_ai"}
